<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.12,148.91,269.48,15.49">UMass at TREC 2003: HARD and QA</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_wRgahmc">
					<orgName type="full">Advanced Research and Development Activity</orgName>
				</funder>
				<funder ref="#_QjJCfG4">
					<orgName type="full">SPAWARSYSCEN-SD</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.21,181.39,97.12,10.76"><forename type="first">Nasreen</forename><surname>Abduljaleel</surname></persName>
						</author>
						<author>
							<persName coords="1,275.32,181.39,128.90,10.76"><forename type="first">Andres</forename><surname>Corrada-Emmanuel</surname></persName>
						</author>
						<author>
							<persName coords="1,412.93,181.39,23.98,10.76"><forename type="first">Qi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName coords="1,190.08,195.34,64.62,10.76"><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName coords="1,262.57,195.34,71.15,10.76"><forename type="first">Courtney</forename><surname>Wade</surname></persName>
						</author>
						<author>
							<persName coords="1,362.92,195.34,58.70,10.76"><forename type="first">James</forename><surname>Allan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">The Center for Intelligent Information</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.12,148.91,269.48,15.49">UMass at TREC 2003: HARD and QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD59D5135857D2E809C58F11B12D3496</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrieval (CIIR) at UMass Amherst participated in two tracks for TREC 2003: High Accuracy Retrieval from Documents (HARD) and Question Answering (QA).</p><p>• In the HARD track, we developed document metadata to correspond to query metadata requirements; implemented clarification forms based on query expansion, passage retrieval, and clustering; and retrieved variable length passages deemed most likely to be relevant. This work is discussed at length in Section 1.</p><p>• In the QA track, we focused on retrieving passages that were likely to contain the answer to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">HARD track 1.Overview</head><p>The goal of the High Accuracy Retrieval from Documents track was to explore techniques for improving the accuracy of the top-ranked documents in response to a query. We participated in all three aspects of the problem:</p><p>• We mapped query metadata values to document metadata values that we assigned. We then adjusted the ranking of documents depending on whether their metadata matched the query metadata.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• We generated clarification forms to tease more information out of the searcher. We tried several types of clarification forms, including providing a list of keywords that might appear in relevant documents, a list of top-ranking clusters that might contain relevant documents, and a list of passages that might appear in relevant documents.</p><p>• We explored passage-level retrieval of documents to see if we could pinpoint the relevant portions of documents.</p><p>In the final analysis, all runs using metadata or clarification forms failed to outperform our best baseline run. We interpret this as an indictment of the track and of our effort. As with most new TREC tracks, the HARD track was slow to get started, had problems being clearly defined, and had poor training data. In addition, several engineering bottlenecks delayed our initial work and prevented us from moving as rapidly as we had originally intended. The rest of this section discusses what we did. We first describe the baseline runs that we generated for comparison. The same section presents the mechanism behind our passage retrieval runs. In Section 1.3 we discuss the types of clarification forms that we used and, in Section 1.4, how we used the responses. We outline how we used query and document metadata in Section 1.5 and how it was incorporated into the ranking in Section 1.6. We discuss our results in Section 1.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Baseline Runs 1.2.1 Standard Document Retrieval Baseline</head><p>Two of our document retrieval baseline runs (ciirtbas and ciirtdbas) used relevance modeling (method 2) <ref type="bibr" coords="2,469.67,149.38,16.59,8.97" target="#b9">[10]</ref> as implemented in Lemur. <ref type="foot" coords="2,100.14,159.76,3.48,6.28" target="#foot_0">1</ref> This relevance model was built using the top 75 terms from the top 50 documents. Ciirtbas uses only the topic title as the query and ciirtdbas uses the topic title and description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">Passage-Inspired Document Retrieval Baseline</head><p>The two other baseline runs (ciirtpsgbas and ciirtdpsgbas) re-ranked documents from the standard document retrieval baselines based on the best passage from each document.</p><p>Passage retrieval was performed using passage language models <ref type="bibr" coords="2,340.94,242.41,10.57,8.97" target="#b0">[1]</ref>. The passages were ranked according to querylikelihood. P (q|P ) = n i=1 P (q i |P ) where P is a passage. The results were smoothed using interpolation with a collection model and a Dirichlet prior.</p><p>Each document was divided into passages of 150 words, with an overlap of 75 words. The best passage for every document in the initial ranked list (ciirtbas and ciirtdbas) was determined. These "top" passages were then ranked by their log-likelihood. In the final list(ciirtpsgbas or ciirtdpsgbas), each passage was replaced with its corresponding document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.3">Passage Retrieval Baseline</head><p>Our final baseline (ciirtp) was a ranked list of passages. Passages were retrieved using the method described in the previous section. However for this run, the top 10 passages from each document were considered, as opposed to only one in the previous case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Clarification Forms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1">Top-term clarification form</head><p>The system first employs a simple language modeling approach <ref type="bibr" coords="2,314.03,437.17,16.58,8.97" target="#b12">[13]</ref> to retrieve the top 1000 documents, and then constructs a relevance model <ref type="bibr" coords="2,135.08,449.13,16.59,8.97" target="#b9">[10]</ref> from those documents. The top 30 terms selected by the relevance model were used for the top-term clarification form. A snapshot of the form is shown in Figure <ref type="figure" coords="2,306.74,461.09,3.73,8.97" target="#fig_0">1</ref>. In this form, we ask the LDC annotators to mark the terms that are relevant to the query as "Good", the terms that are non-relevant to the query as "Bad", and leave the terms that they can't judge as "Unknown" which is the default option. The text in the parenthesis after each term is a sample context in which the term appears in the retrieved documents. In addition, we also ask the annotators to suggest if there are any terms other than the ones already shown in the form that they think might occur in the relevant documents.</p><p>In the responses we have obtained from LDC on this clarification form, there is an average of 12.2 good terms marked among the 30 terms provided per test topic, and the maximum number of good terms marked for any topic is 22 while the minimum is 3. Table <ref type="table" coords="2,147.69,544.77,4.97,8.97" target="#tab_0">1</ref> summarizes the term statistics per topic. Only one test topic had suggested terms other than the 30 provided. In our submission runs that made use of this clarification form, we expanded the original query with the good terms (suggested terms are also considered as good terms) and performed retrieval. Retrieval results are discussed in section 1.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">Other clarification forms (CF)</head><p>Besides the top-term clarification form, we also generated other clarification forms, namely cluster-by-size CF, cluster-bydistribution CF, and passage CF.  • Cluster-by-size CF. The system first retrieves the top 200 documents using a simple language modeling approach <ref type="bibr" coords="4,86.10,123.97,15.26,8.97" target="#b12">[13]</ref>. The group-average hierarchic clustering algorithm <ref type="bibr" coords="4,314.62,123.97,11.61,8.97" target="#b3">[4]</ref> is then applied to the retrieved documents set to obtain clusters. The similarity between documents is determined by the cosine similarity between their term vectors, and the threshold is set to 0.6. Clusters are ranked by their size with the largest cluster at the top of the ranked list. Top 15 clusters are provided, and both the headline of the centroid document and the top 10 terms for each cluster are shown in the CF form. The LDC annotators are asked to mark whether each cluster is good, bad, or unknown. If none of clusters are marked good, the annotators are encouraged to suggest some terms that they think might occur in the relevant documents. In the responses we obtained from LDC on this form, on average, there are 5.5 clusters that are marked "good" , 5.1 marked "bad", and 4.4 marked "unknown", out of the total of 15 clusters provided per test topic. Among the 50 test topics, there are two topics for which all 15 provided clusters are judged "good", four topics for which all 15 clusters are judged "bad", and four topics that have only "unknown" clusters. Only one topic had suggested terms other than the ones displayed in the form. Table <ref type="table" coords="4,362.06,243.53,4.97,8.97" target="#tab_1">2</ref> shows the statistics on clusters per topic. • Cluster-by-distribution CF. This CF is generated in a similar fashion to the cluster-by-size CF but with a different cluster ranking mechanism. After the clusters are formed by the group-average clustering algorithm, a simple cluster language model is constructed from the clusters and a query language model is constructed using information from the query. A Kullback-Leibler (KL) divergence score is computed between each cluster model and the query model, and is used as the basis for the ranking of clusters. Clusters and related information are displayed in the same format as the cluster-by-size CF.</p><p>• Passage CF. The system uses the relevance model <ref type="bibr" coords="4,291.60,427.07,16.59,8.97" target="#b9">[10]</ref> for retrieval of top 1000 documents, and then segments the documents into passages and ranks the passages. The top 10 passages are presented in the clarification form, again to be marked as "good", "bad", or "unknown" based on their relevance to the query.</p><p>However, due to the tight schedule of the LDC annotators, only the top-term CF and the cluster-by-size CF were filled out for all test topics. After comparing retrieval performance on the training data using top-term CF and cluster-by-size CF, we selected the top-term CF to be used in our final submission runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Incorporating Clarification Forms</head><p>One of our runs (ciirtcftt) used information gathered from the top-term clarification form. We issued the original query with all of the terms marked "good" added, and with all of the additional suggested terms. In our training experiments, we found that this method actually harmed results, although we did not have enough data to determine whether this could be expected to hold in general. We experimented with several methods for using the terms marked "bad" to improve retrieval, but none produced promising results on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Metadata</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.1">Statistics on the metadata of the test topics</head><p>We summarize in the following tables the statistics on the metadata of the 50 test topics. The first column of each Overview, Administrative, I-Reaction, Other Some of the above document metadata categories are different from the query metadata categories. It was not clear how to assign values to some of the query metadata categories, when starting from a top-ranked document for a query. Two such categories were "Purpose" and "Familiarity". Instead, we chose metadata categories "Time" and "Expertise" as indicators of "Purpose" and "Familiarity", respectively.</p><p>Familiarity mapping. Initially we built a familiarity classifier using support vector machines (SVM) <ref type="bibr" coords="5,490.86,520.98,10.57,8.97" target="#b6">[7]</ref>, to classify documents into one of the four categories: expert, amateur, novice, and child. However, by using ten-fold cross validation on 629 instances, the classifier only yielded an average accuracy of 71%. To insure the availability of sufficient number of documents for retrieval for each familiarity level, we instead developed the following method.</p><p>To determine which document should map to which familiarity metadata value, we compute the readability indices and then map the scores. Readability/reading level describes the ease with which a document can be read or understood <ref type="bibr" coords="5,536.23,580.76,10.57,8.97" target="#b2">[3]</ref>. While familiarity and reading level are different, we made an assumption that materials that are more difficult to understand would be more appropriate for a user that is more familiar with a topic. We have computed five readability indices for each document, namely, Dale-Chall <ref type="bibr" coords="5,206.27,616.63,10.57,8.97" target="#b1">[2]</ref>, FOG <ref type="bibr" coords="5,245.42,616.63,10.57,8.97" target="#b4">[5]</ref>, Holquist <ref type="bibr" coords="5,298.95,616.63,10.57,8.97" target="#b5">[6]</ref>, Flesch-Kincaid <ref type="bibr" coords="5,379.00,616.63,10.78,8.97" target="#b7">[8,</ref><ref type="bibr" coords="5,392.35,616.63,7.19,8.97" target="#b8">9]</ref>, and SMOG (the Simplified Measure of Gobbledygook) <ref type="bibr" coords="5,138.16,628.59,15.25,8.97" target="#b11">[12]</ref>. The statistics of document readability scores across the whole collection is given in table <ref type="table" coords="5,526.33,628.59,3.73,8.97" target="#tab_3">3</ref>. By manually checking how well each of the readability indices is able to differentiate various documents of known levels, we selected SMOG as the final measure. From figure <ref type="figure" coords="6,128.04,197.57,3.73,8.97">2</ref>, we see that a majority of the test topics specify familiarity values 2 and 3. To insure the quality of retrieval, we made a corresponding mapping that allowed most documents to fall under those two categories. The mapping scheme is shown in table 4. For example, if a document has a SMOG readability score of 9, it is mapped to familiarity=2. If a test topic also specifies the same familiarity value, this document will then be included in the pool of 155,372 documents to perform retrieval for that topic. There are a total of 372,219 documents in the collection out of which 128 documents have no familiarity score because they have no actual contents. Genre classification. We built a genre classifier using support vector machines. There are total of 615 training documents for this metadata from annotation, out of which 373 were annotated as "overview", 172 as "administrative," 31 as "ireaction," 21 as "reaction," and 17 as "other." We applied ten-fold cross validation and obtained an average accuracy of 90%. However, the number of training instances is small thus the quality of the classifier can not be reliably determined. When we applied this classifier on the 10 NIST-provided training topics, it was found that 89% of the retrieved documents were classified as "overview" and the remaining 11% were classified as "administrative". There were no instances that were classified as "reaction" or "i-reaction." Because of the potential impact that the classification results might have on the final retrieval, we decided not to use it in our final submission. Instead, we resort to more conservative measures discussed in section 1.6.1.</p><p>Purpose classification. A purpose classifier was built, again using SVMs, to classify documents into either "background" or "details," because the third value "answer" can be handled together with the granularity metadata. The documents that were tagged as "current" or "future" for metadata Time used in annotation are considered having a Purpose metadata value of "details", and those tagged "historical" for Time map to "background" for Purpose. Out of the 567 annotated documents for this metadata only 6 were given the value "background." A classifier trained on this data is clearly not reliable. Due to time constraints, we did not use this metadata in our submission runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Incorporating Metadata 1.6.1 Genre</head><p>As discussed above, attempts at more sophisticated learning-based methods of assigning genres to documents were unsuccessful so our submitted runs rely on ad hoc rules for re-ranking documents. In the runs that used the genre metadata field (ciirtmda and ciirtmdap), documents were ranked using baseline methodology and then re-ranked according to the following rules:</p><p>1. If the query metadata field was overview, any document from the Federal Register or Congressional Record was moved five places down the ranked list.</p><p>2. If the query metadata field was reaction, any document from Xinghua English or the Federal Register was moved five places down the ranked list.</p><p>3. If the query metadata field was i-reaction, any document not from Xinghua English was moved five places down the ranked list.</p><p>4. If the query metadata field was administrative, any document not from the Congressional Record or Federal Register was moved to the end of the ranked list. Because of the size of the CR and FR document collections, this means that only CR and FR documents were ever returned in the top 1000 documents.</p><p>These rules were derived largely from our own experience working with the documents and due to the limited training data, we were generally unable to test their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.2">Familiarity</head><p>For all runs incorporating familiarity metadata (ciirtmda and ciirtmdap), we tagged every document with an integer familiarity score in the range 1 to 5 as explained in section 1.5.2. After each document was ranked using the baseline methodology, the rankings were shifted slightly by subtracting a δ value from the rank of each document. We set δ = 2 -|(query familiarity score) -(document familiarity score)|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7">Granularity</head><p>Two of our runs (ciirtmdgp and ciirtmdap) incorporated query granularity by retrieving passages of appropriate size. For queries with Granularity value "Sentence" and " Passage", passages of length 50 words and 150 words, respectively, were retrieved. For the value "Any", ranked lists for passages of size 50 and 150 were merged, after normalization, with the ranked list of documents from ciirtmda. A simple measure of "novelty" was used to reduce overlap between different sized passages from the same document. If a passage or document had more than a 75higher up in the ranked list, it was removed from the ranking. The resulting list was submitted as ciirtmdgp.</p><p>The entries in the merged list were reranked using the familiarity and genre information as indicated in the previous section. The "novelty" measure was also applied and the reranked list was submitted as ciirtmdap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7.1">Related Text</head><p>Only one of our runs (ciirtrt) used the related text query metadata. Because the related text included relatively large amounts of text (often entire articles), we were concerned that simply appending the related text to the original query might dwarf the query terms. To compensate for this our new query was set to the original query repeated 100 times with the related text appended. Again, we did not have sufficient data to test this model so the number 100 is arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7.2">Summary of Submitted Runs</head><p>We submitted 10 runs in all, including five baseline runs and five other runs that incorporated different combinations of metadata and clarification form data. Table <ref type="table" coords="7,246.81,551.87,4.97,8.97" target="#tab_4">4</ref> summarizes each of the runs submitted. The first five lines give some information about the baseline runs and the last five lines show what techniques each of the other runs used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.8.1">Document-Level Evaluation</head><p>Table <ref type="table" coords="8,85.84,334.94,4.97,8.97" target="#tab_6">5</ref> shows document-level evaluation results for all ten runs submitted. The top line shows the results for ciirtbas, our best-performing baseline run. Boldface numbers indicate runs that were significantly different (using the Student's t-test at the .05 significance level). Each significance test was performed against the ciirtbas entry in the same column. None of our runs performed significantly better than our ciirtbas baseline. One run, ciirtmda, had better hard average precision and hard precision at 20 documents than the baseline. However, the improvement was not statistically significant.</p><p>For 20 of the 48 queries, adding the "good" terms from the clarification form to the original query improved average precision and for 19 of the queries precision at 20 documents retrieved improved. Using genre and familiarity improved the average precision of 16 queries but only improved precision at 20 documents retrieved for two of the queries. We should note here that 4 of the 48 queries had genre of "any" and familiarity "unknown" so they stood no chance of improving when techniques to incorporate genre and familiarity metadata were incorporated. Using the related text metadata to augment the query resulted in better average precision for 16 queries and better precision at 20 documents retrieved for 11 queries. This means that the use of the chosen metadata and our clarification form did help improve retrieval in some cases but not in general.</p><p>Advance discussion on the track mailing list led us to believe that only Congressional Record and Federal Register could be considered relevant using HARD relevance evaluation criteria on queries with genre equal to "Administrative." To take advantage of this known fact we designed our system only to retrieve those two types of documents for "Administrative" queries. Interestingly, however, for the two "Administrative" queries in the testing set (HARD-069 and HARD-176), there were several documents from the New York Times and Xinghua English collections that were marked relevant. As a result, our system did worse on both of these queries than it did using the baseline method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.8.2">Passage-Level Evaluation</head><p>Table <ref type="table" coords="9,86.65,306.52,4.97,8.97" target="#tab_7">6</ref> shows the results of passage-level evaluation for each of the submitted runs. As in Table <ref type="table" coords="9,466.97,306.52,3.73,8.97" target="#tab_6">5</ref>, boldface numbers indicate runs that were significantly different (using the Student's t-test at the .05 significance level). Each significance test was performed against the ciirtdbas entry in the same column.</p><p>Results are similar to those in the previous section. None of the runs performed significantly better than the baseline. The baseline run with highest R-Precision is ciirtdbas. Three of the submitted runs, ciirtmdap, ciirtmdgp and ciirtrt, gave higher R-Precision and Passage Precision @ 20 docs than the best baseline run, but the differences were not statistically significant.</p><p>Using granularity information to retrieve passages of different size improved R-precision for 12 queries. Of the remaining queries, 19 had the granularity value "Document". The R-precision for these queries was unaffected by incorporating granularity. For 6 other queries, no relevant documents were retrieved in the baseline run, ciirtbas. Since this list was the starting point for passage retrieval, R-Precision remained at 0. For 6 queries, R-precision was adversely affected by the use of granularity values. Therefore, using metadata values helps some queries, but overall, it does not significantly affect performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Answering track</head><p>In the QA track, we developed a dynamic passaging retrieval system to identify passages likely to contain answers. CIIR last participated in the QA task in TREC <ref type="bibr" coords="10,265.34,146.24,32.62,8.97">9 (2000)</ref>. At that time we fielded the Marsha system <ref type="bibr" coords="10,479.21,146.24,15.26,8.97" target="#b10">[11]</ref>. This system was based on an INQUERY document retrieval engine followed by the application of a series of heuristics rules to identify 250-byte long passages in the retrieved documents that were likely to contain the desired answers. This year's passage sub-task in the QA track has allowed us to participate once again utilizing our current approach to passage retrieval with language models. We developed a dynamic passaging system that retrieved document passages based on the simplest implementation of language models: cross-entropy between bag-of-word models for a question and a candidate passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Theoretical QA model</head><p>Our system used a simple approach to passage retrieval for a question. We constructed a MLE model for the question by treating the words in the question independently, the so-called bag-of-words (BOW) model. A collection-smoothed, with a Dirichlet-prior was used to create a BOW model for a candidate passage. The candidate passage was then ranked by the cross-entropy between its model and the unsmoothed model of the question:</p><formula xml:id="formula_0" coords="10,243.74,320.33,124.51,28.90">H(q|p) = - n i=1 P i (q) ln P i (p)</formula><p>where q denotes the question model, p the passage model and n is the number of unique words in the question. This measure is rank-equivalent to the more familiar query-likelihood formula. But we utilize this alternative formulation because it will allow us to build in the future a Bayesian classifier in combination with the use of Relevance Models where the cross-entropy is calculated between a relevance model and the passage model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">QA System implementation</head><p>Passage retrieval was done in two stages. The initial stage consisted of document retrieval using the #sum INQUERY operator. The retrieval used the an index that had INQUERY-stopped and Krovetz-stemmed the AQUAINT collection. Similar stopping and stemming was applied to the questions. We should point out that this step was done solely for the purposes of time savings. As we will comment later on this section, skipping this document retrieval step and going directly to the passage retrieval step produces essentially the same performance.</p><p>The top 60 documents (as determined by tuning on TREC 2002 questions) were then selected for the passage retrieval phase. Documents were passaged on-the-fly and sliding windows that moved forward one word at a time while guaranteeing the 250-byte evaluation limit were ranked according to the cross-entropy formula detailed above. Only one-passage per document was allowed to appear on the ranked list.</p><p>We tested whether skipping the document retrieval phase would have gained us any performance benefit by looking at the rank-1 measure on the TREC 2002 question set and retrieving 1KB passages. Skipping the document retrieval and doing passage retrieval on the whole AQUAINT collection gave us a rank-1 performance of 38.2%. The two-step retrieval gave us the same rank-1 performance.</p><p>The system did no processing to recognize NIL-answer questions so the NIL token was never returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">QA Results</head><p>The evaluation metric (rank-1 correct) for this year was 20.1%. This compares favourably with our expectations of 19.2%</p><p>obtained by tuning on the TREC 2002 question set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>We participated in the HARD and QA tracks. In both cases, we believe that our results are good, though we had hoped for better. In the case of HARD, we look forward to trying again with a more mature track, based on the lessons learned and with the training data collected this year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,160.57,489.65,290.57,8.97;3,75.60,139.74,460.80,334.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A snapshot of the top-term clarification form for test topic 033.</figDesc><graphic coords="3,75.60,139.74,460.80,334.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,145.39,588.22,320.87,59.22"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the terms that are marked "Good", "Bad", and "Unknown".</figDesc><table coords="3,219.88,599.43,172.16,48.02"><row><cell></cell><cell cols="3">Average Max. Min.</cell></row><row><cell>Good terms</cell><cell>12.2</cell><cell>22</cell><cell>3</cell></row><row><cell>Bad terms</cell><cell>5.4</cell><cell>20</cell><cell>0</cell></row><row><cell>Unknown terms</cell><cell>6.4</cell><cell>21</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,141.37,265.45,328.89,69.19"><head>Table 2 .</head><label>2</label><figDesc>Statistics of the clusters that are marked "Good", "Bad", and "Unknown".</figDesc><table coords="4,215.73,286.61,180.46,48.02"><row><cell></cell><cell cols="3">Average Max. Min.</cell></row><row><cell>Good clusters</cell><cell>5.5</cell><cell>15</cell><cell>0</cell></row><row><cell>Bad clusters</cell><cell>5.1</cell><cell>15</cell><cell>0</cell></row><row><cell>Unknown clusters</cell><cell>4.4</cell><cell>15</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,206.77,677.40,198.27,8.97"><head>Table 3 .</head><label>3</label><figDesc>Statistics of document readability scores.</figDesc><table coords="6,177.34,110.83,257.17,72.72"><row><cell></cell><cell>Min.</cell><cell>Max.</cell><cell cols="3">Median Mean Std. Dev.</cell></row><row><cell>SMOG</cell><cell>3</cell><cell>153.00</cell><cell>10.94</cell><cell>11.99</cell><cell>6.39</cell></row><row><cell>Holquist</cell><cell cols="2">7.12 5509.0</cell><cell>31.77</cell><cell>47.20</cell><cell>105.64</cell></row><row><cell>Dale-Chall</cell><cell cols="2">3.69 978.26</cell><cell>4.55</cell><cell>5.48</cell><cell>5.99</cell></row><row><cell>Flesch-Kincaid</cell><cell>0</cell><cell>7665.9</cell><cell>9.04</cell><cell>15.95</cell><cell>47.23</cell></row><row><cell>FOG</cell><cell cols="2">0.03 7867.0</cell><cell>13.03</cell><cell>20.35</cell><cell>48.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,142.61,280.54,326.42,56.14"><head>Table 4 .</head><label>4</label><figDesc>Mapping between document SMOG score and familiarity metadata value</figDesc><table coords="6,159.21,301.00,293.32,35.67"><row><cell>Familiarity value</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Range of SMOG score</cell><cell>&lt;=7</cell><cell cols="3">(7, 10] (10, 14] (14, 19]</cell><cell>&gt;19</cell></row><row><cell cols="4">Num. of docs mapped 35185 155372 143053</cell><cell>61418</cell><cell>20725</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,61.20,112.02,461.26,194.96"><head>Table 4 .</head><label>4</label><figDesc>Summary of submitted runs.</figDesc><table coords="8,61.20,133.90,461.26,173.08"><row><cell>RUNID</cell><cell></cell><cell>QUERY</cell><cell cols="2">METADATA</cell><cell cols="3">CLAR. FORM RETURNS</cell></row><row><cell></cell><cell cols="4">title descrip. gran. purp. genre famil. rltd. text</cell><cell>top terms</cell><cell cols="2">docs psgs.</cell></row><row><cell>ciirtbas</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*</cell></row><row><cell>ciirtdbas</cell><cell>*</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell>*</cell></row><row><cell>ciirtpsgbas</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*</cell></row><row><cell>ciirtdpsgbas</cell><cell>*</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell>*</cell></row><row><cell>ciirtp</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*</cell></row><row><cell>ciirtmda</cell><cell>*</cell><cell></cell><cell>*</cell><cell>*</cell><cell></cell><cell>*</cell></row><row><cell>ciirtmdap</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell></cell><cell>*</cell><cell>*</cell></row><row><cell>ciirtmdgp</cell><cell>*</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell>*</cell><cell>*</cell></row><row><cell>ciirtrt</cell><cell>*</cell><cell></cell><cell></cell><cell>*</cell><cell></cell><cell>*</cell></row><row><cell>ciirtcftt</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell>*</cell><cell>*</cell></row><row><cell>1.8 Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,96.45,112.02,418.74,158.37"><head>Table 5 .</head><label>5</label><figDesc>Document-level evaluation of all runs. Standard deviations are shown in parenthesis.</figDesc><table coords="9,96.45,133.90,418.74,136.49"><row><cell>RUNID</cell><cell cols="4">SOFT AVG. PREC. HARD AVG. PREC. SOFT PREC. @ 20 HARD PREC. @ 20</cell></row><row><cell>ciirtbas</cell><cell>0.3518 (0.2777)</cell><cell>0.3091 (0.2737)</cell><cell>0.5365 (0.3681)</cell><cell>0.4250 (0.3341)</cell></row><row><cell>ciirtdbas</cell><cell>0.3314 (0.2508)</cell><cell>0.2969 (0.2452)</cell><cell>0.5198 (0.3462)</cell><cell>0.4156 (0.3239)</cell></row><row><cell>ciirtpsgbas</cell><cell>0.3052 (0.2371)</cell><cell>0.2529 (0.2279)</cell><cell>0.4719 (0.3334)</cell><cell>0.3438 (0.2822)</cell></row><row><cell cols="2">ciirtdpsgbas 0.3029 (0.2106)</cell><cell>0.2640 (0.2073)</cell><cell>0.4708 (0.2988)</cell><cell>0.3708 (0.2837)</cell></row><row><cell>ciirtp</cell><cell>0.3049 (0.2367)</cell><cell>0.2526 (0.2275)</cell><cell>0.4719 (0.3334)</cell><cell>0.3438 (0.2822)</cell></row><row><cell>ciirtmda</cell><cell>0.3500 (0.2811)</cell><cell>0.3136 (0.2815)</cell><cell>0.5344 (0.3689)</cell><cell>0.4260 (0.3361)</cell></row><row><cell>ciirtmdap</cell><cell>0.3056 (0.2540)</cell><cell>0.2682 (0.2497)</cell><cell>0.5031 (0.3593)</cell><cell>0.3844 (0.3094)</cell></row><row><cell>ciirtmdgp</cell><cell>0.3036 (0.2519)</cell><cell>0.2662 (0.2472)</cell><cell>0.5031 (0.3593)</cell><cell>0.3844 (0.3094)</cell></row><row><cell>ciirtrt</cell><cell>0.3430 (0.2644)</cell><cell>0.3016 (0.2644)</cell><cell>0.5469 (0.3809)</cell><cell>0.4250 (0.3639)</cell></row><row><cell>ciirtcftt</cell><cell>0.3146 (0.2669)</cell><cell>0.2942 (0.2668)</cell><cell>0.5448 (0.3900)</cell><cell>0.4469 (0.3810)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,124.93,471.90,361.77,158.38"><head>Table 6 .</head><label>6</label><figDesc>Passage-level evaluation of all runs. Standard deviations are shown in parenthesis.</figDesc><table coords="9,142.12,493.78,327.53,136.49"><row><cell>RUNID</cell><cell>R-PRECISION</cell><cell>PASSAGE PREC. @ 20</cell><cell>F-MEASURE</cell></row><row><cell>ciirtdbas</cell><cell>0.2301 (0.221)</cell><cell>0.282 (0.3441)</cell><cell>0.209 (0.2061)</cell></row><row><cell>ciirtbas</cell><cell cols="2">0.2261 (0.2326) 0.2801 (0.3585)</cell><cell>0.2063 (0.2001)</cell></row><row><cell>ciirtpsgbas</cell><cell>0.1853 (0.2076)</cell><cell>0.226 (0.2984)</cell><cell>0.1576 (0.1622)</cell></row><row><cell cols="3">ciirtdpsgbas 0.1942 (0.1871) 0.2474 (0.3234)</cell><cell>0.1627 (0.1522)</cell></row><row><cell>ciirtp</cell><cell cols="2">0.2093 (0.1913) 0.2563 (0.3279)</cell><cell>0.0929 (0.0878)</cell></row><row><cell>ciirtmda</cell><cell cols="2">0.2261 (0.2306) 0.2805 (0.3434)</cell><cell>0.211 (0.2102)</cell></row><row><cell>ciirtmdap</cell><cell>0.2542 (0.2367)</cell><cell>0.292 (0.3686)</cell><cell>0.1943 (0.2035)</cell></row><row><cell>ciirtmdgp</cell><cell cols="2">0.2541 (0.2367) 0.2917 (0.3687)</cell><cell>0.1943 (0.2035)</cell></row><row><cell>ciirtrt</cell><cell cols="2">0.2327 (0.2371) 0.2842 (0.3312)</cell><cell>0.2105 (0.2127)</cell></row><row><cell>ciirtcftt</cell><cell cols="2">0.2229 (0.2235) 0.3144 (0.4012)</cell><cell>0.1974 (0.2038)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,75.55,654.99,105.69,7.17"><p>http://www-2.cs.cmu.edu/ lemur/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknoweldgments</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part by <rs type="funder">SPAWARSYSCEN-SD</rs> grant number <rs type="grantNumber">N66001-02-1-8903</rs>, and in part by <rs type="funder">Advanced Research and Development Activity</rs> under contract number <rs type="grantNumber">MDA904-01-C-0984</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QjJCfG4">
					<idno type="grant-number">N66001-02-1-8903</idno>
				</org>
				<org type="funding" xml:id="_wRgahmc">
					<idno type="grant-number">MDA904-01-C-0984</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,77.78,305.85,472.51,8.97;11,77.80,317.80,54.45,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,293.85,305.85,189.46,8.97">Answer passage retrieval for question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corrada-Emmanuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Murdock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,489.76,305.85,60.53,8.97;11,77.80,317.80,25.34,8.97">CIIR Technical Report</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,77.78,337.72,464.33,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,206.88,337.72,141.31,8.97">A formula for predicting readability</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Chall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,354.39,337.72,112.63,8.97">Education Research Bulletin</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="37" to="54" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,77.78,357.65,270.78,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,161.00,357.65,44.33,8.97">Readability</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gilliland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972. 1972</date>
			<publisher>University of London Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,77.78,377.58,472.51,8.97;11,77.79,389.53,214.79,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,273.09,377.58,277.21,8.97;11,77.79,389.53,54.18,8.97">Hierarchic agglomerative clustering methods for automatic document classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Griths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,135.84,389.53,99.10,8.97">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="175" to="205" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,77.78,409.45,270.53,8.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,160.72,409.45,124.49,8.97">The Technique of Clear Writing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952">1952</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,77.78,429.38,472.53,8.97;11,77.79,441.33,381.32,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,167.00,429.38,383.31,8.97;11,77.79,441.33,216.63,8.97">A determination of whether the Dale-Chall readability formula may be revised to evaluate more validly the readability of high school science materials</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Holquist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
		<respStmt>
			<orgName>Colorado State University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="11,77.78,461.26,472.52,8.97;11,77.79,473.22,146.01,8.97" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<title level="m" coord="11,162.94,461.26,229.76,8.97">Learning to Classify Text Using Support Vector Machines</title>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D dissertation</note>
</biblStruct>

<biblStruct coords="11,77.78,493.14,472.55,8.97;11,77.79,505.10,469.96,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,378.09,493.14,172.24,8.97;11,77.79,505.10,94.35,8.97">Derivation of new readability formulas for Navy enlisted personnel</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Fishburn</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Chissom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,178.76,505.10,97.57,8.97;11,302.05,505.10,107.32,8.97">Naval Air Station Memphis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">75</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="1975">1975</date>
			<pubPlace>Millington, Tennessee</pubPlace>
		</imprint>
	</monogr>
	<note>Research Branch Report</note>
</biblStruct>

<biblStruct coords="11,77.78,525.02,472.54,8.97;11,77.79,536.98,285.23,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,350.03,525.02,143.50,8.97">Computer readability editing system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aagard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>O'hara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,500.38,525.02,49.93,8.97;11,77.79,536.98,161.30,8.97">IEEE Transactions on Professional Communications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,82.76,556.90,467.58,8.97;11,77.79,568.85,472.53,8.97;11,77.79,580.81,315.15,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,237.07,556.90,135.72,8.97">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,142.16,568.85,408.16,8.97;11,77.79,580.81,82.11,8.97">Proceedings of the 24th annual international ACM-SIGIR conference on research and development in information retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 24th annual international ACM-SIGIR conference on research and development in information retrieval<address><addrLine>New Orleans, Louisiana; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,82.76,600.74,467.52,8.97;11,77.79,612.69,173.69,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,176.92,600.74,218.29,8.97">Evaluating Question Answering Techniques in Chinese</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,402.08,600.74,148.20,8.97;11,77.79,612.69,91.36,8.97">Proceedings of the Human Language Technology Conference</title>
		<meeting>the Human Language Technology Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,82.76,632.62,432.09,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,180.61,632.62,170.46,8.97">SMOG grading -a new readability formula</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mclaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,357.81,632.62,74.62,8.97">Journal of Reading</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="639" to="646" />
			<date type="published" when="1969">1969. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,82.76,652.54,467.56,8.97;11,77.79,664.50,271.04,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,261.12,652.54,211.67,8.97">A hidden Markov model information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,490.65,652.54,59.67,8.97;11,77.79,664.50,213.80,8.97">Proceedings of the 22nd annual international ACM SIGIR conference</title>
		<meeting>the 22nd annual international ACM SIGIR conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
