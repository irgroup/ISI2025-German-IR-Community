<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.98,61.54,284.07,14.42">QUALIFIER in TREC-12 QA Main Task</title>
				<funder>
					<orgName type="full">NUS Joint-Lab</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,86.34,90.98,46.62,10.80"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>yangh@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>3 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,142.03,90.98,47.27,10.80"><forename type="first">Hang</forename><surname>Cui</surname></persName>
							<email>cuihang@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>3 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,197.72,90.98,109.60,10.80"><forename type="first">Mstislav</forename><surname>Maslennikov</surname></persName>
							<email>maslenni@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>3 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.06,90.98,46.41,10.80"><forename type="first">Long</forename><surname>Qiu</surname></persName>
							<email>qiul@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>3 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.04,90.98,67.72,10.80"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>3 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,448.00,90.98,77.64,10.80"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>3 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.98,61.54,284.07,14.42">QUALIFIER in TREC-12 QA Main Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E08CADE5EA8927267679ECC46BAA38F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a question answering system and its various modules to solve definition, factoid and list questions defined in the TREC12 Main task. In particular, we tackle the factoid QA task by Event-based Question Answering. Each QA event comprises of elements describing different facets like time, location, object, action etc. By analyzing the external knowledge from pre-retrieved TREC documents, Web documents, WordNet and Ontology to discover the QA event structure, we explore the inherent associations among QA elements and then obtain the answers. There are three subsystems working parallel to handle definition, factoid, and list questions separately. We highlight the shared modules, fine-grained named entity recognition, anaphora resolution and canonicalization co-reference resolution, among the three subsystems as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open domain Question Answering (QA) is a complex research area formed as a distinctive combination of Information Retrieval (IR), Information Extraction (IE), and Natural Language Processing (NLP). The basic problem that QA poses is: given a question and a large text corpus, return an "answer" rather than relevant "documents". QA has received tremendous interest recently with the emergence of many commercial products, and research papers in various communities <ref type="bibr" coords="1,524.87,342.00,29.42,9.02;1,57.60,353.46,18.04,9.02">(SIGIR 2003</ref><ref type="bibr" coords="1,75.64,353.46,48.26,9.02">, ACL 2003</ref><ref type="bibr" coords="1,123.90,353.46,72.23,9.02">, ACMMM 2003)</ref>. In TREC-11 <ref type="bibr" coords="1,258.99,353.46,66.03,9.02">(Voorhees 2002)</ref>, we explored the use of external resources like the Web and WordNet to extract terms that are highly correlated with the query, and use them to perform linear query expansion <ref type="bibr" coords="1,57.60,376.50,77.43,9.02">(Yang&amp;Chua,2002)</ref>. While the technique has been found to be effective, we found that there is a need to perform structured analysis on the knowledge obtained from the Web/WordNet to further improve the performance.</p><p>This year, we model the factoid questions by Event-based Question Answering <ref type="bibr" coords="1,379.77,405.48,71.93,9.02" target="#b5">(Yang et al. 2003)</ref>. Questions often refer to several aspects/elements of the events, such as Location, Time, Subject, Object, Quantity, Description and Action, etc. For most QA events, there are inherent associations among their elements. We thus perform Event Mining to discover and then incorporate the knowledge of event structure systematically for more effective QA.</p><p>Our system, named QUALIFIER (QUestion Answering by LexIcal FabrIc and External Resources), includes modules to perform detailed question analysis, QA event construction, answer justification, fine-grained named entity recognition, anaphora resolution, canonicalization co-reference, and successive constraint relaxation.</p><p>During question parsing, detailed question classes, answer types, original query terms and NLP roles of the query terms are analyzed. We derive detailed question class ontology that corresponds to fine-grained named entities. This enables us to extract exact answer from the candidate sentences more accurately. All the questions are treated equally during the stage of detailed question analysis, and then passed to three different subsystems to handle definition, factoid and list questions separately. The original query terms can be used directly to locate potential answer candidates in the corpus. However, one major problem is that those terms do not provide sufficient evidence to retrieve the answer candidates. This is known as the semantic gap between the query space and document space. In order to bridge this gap, we use the knowledge of both the Web and lexical resources to expand the original query. The new query therefore contains terms that are related to the local context in the Web and the lexical context in WordNet. Finally, we structure the query and use it to search for answer candidates through the MG system <ref type="bibr" coords="1,205.84,601.44,80.42,9.02" target="#b2">(Witten et al. 1999)</ref>. Answer candidate sentences are selected from the top returned documents and are ranked based on association rules obtained from QA Event analysis. Named entity recognition, answer justification, canonicalization resolution and answer selection are done to extract the final answer while successive constraint relaxation is used as an auto-feedback loop to boost the answer coverage.</p><p>We will describe the three subsystems one by one in the following sections. Figure <ref type="figure" coords="1,393.28,653.45,5.01,9.02">1</ref> illustrates the flow of the main system. For factoid and list questions, they are solved similarly and definition questions are handled differently in answer extraction. Our system performs event-based question answering for factoid questions in a few steps: external knowledge acquisition, QA event construction, query formulation, element association mining and answer processing. First, it extracts several sets of words (known elements) from the original question and employs a rule-based question classifier to identify the answer target (unknown element type). During the knowledge acquisition stage, it integrates the knowledge of the pre-retrieved TREC documents, Web, WordNet, and our manually constructed Ontology to extract additional evidences for the query. Second, it performs event construction to discover different facets or elements of events and employs the knowledge of events to perform query formulation. Third, given the newly formulated query, it employs the MG tool to search for top ranked documents in the corpus. Fourth, for the top ranked documents, it identifies the relevant passages by exploiting the associations among event elements. After performing element association mining, it computes the Answer Event Score (AES) and uses it to rank the passages from the relevant documents in the corpus. Answer justification module reinforces the confidence of the returned correct answers and filters out some unreasonable ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">QA Event Mining</head><p>In our previous work, we modeled the world and lexical knowledge from the Web and WordNet to support effective QA. Basically, we performed the structured analysis of the external knowledge to extract the QA event structure. First, we used the original query terms in the questions to retrieve the top N w documents by using the Web search engine and then extracted the terms that are highly correlated to the original query terms. Second, we used WordNet to adjust the term weights as well as to introduce new lexical related terms. Third, we computed the lexical, co-occurrence, and distance correlations between terms and used these as the basis to induce event elements by unsupervised semantic grouping. For example, given the question, "What Spanish explorer discovered the Mississippi River?", we could get the event structure as shown in Figure <ref type="figure" coords="2,139.66,560.94,3.76,9.02">2</ref>. Finally, we used this event structure to formulate boolean query to retrieve relevant documents from the QA corpus, and employed a featured-based approach to perform answer selection and extraction.</p><p>Figure <ref type="figure" coords="2,249.37,731.16,3.91,9.02">2</ref>: Example for QA Event Structure</p><p>After extracting the event structure, we employ Event Mining to study the relationships among the event elements. In this approach, we extract important association rules among the elements by using data mining techniques. Given a QA event E i , we define X, Y as two sets of event elements. Event mining studies the rules of the form X → Y, where "→" means "implies", and Y is the possible answer candidate set. In order to avoid too many misleading rules, we restrict the relationships before generating the rules:</p><formula xml:id="formula_0" coords="3,93.60,121.75,168.64,37.46">if X ⊆ Y , ignore X → Y. if cardinality(Y) &gt; 1, ignore X → Y. if Y∩ {element original }≠∅, ignore X → Y.</formula><p>Also, we define Event Mining as follows:</p><p>Event mining studies the association rules of the form X → Y, where X, Y are QA event element sets, X ∩ Y =∅, and Y∩ {element original }=∅.</p><p>There are 4 major steps in the event-based QA approach and we are going to elaborate the details in the following subsections.</p><p>a) Extract the QA event from the Web and WordNet for a question. b) Mine the event in a) to generate the useful association rules among the event elements. c) Rank the passages in the relevant documents from the QA corpus by matching them with the association rules. d) Extract the answer phrase from the top passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">QA Event Generation</head><p>We explore the use of semantic grouping to structurally utilize the external knowledge extracted from the Web, WordNet, Ontology and pre-retrieved documents from TREC. The terms extracted from the relevant web snippets, WordNet, preretrieved TREC documents are put into a vector called K q , which is the basis for semantic grouping since it is most likely to contain the facts/terms about the QA entity or QA event. Given any two distinct terms t i , t j , in the K q , we compute (a)</p><formula xml:id="formula_1" coords="3,57.60,378.42,428.07,10.67">Lexical correlation R l (t i , t i );(b) Co-occurrence correlation ) , ( j i co t t R ; (c) Distance correlation ) , ( j i d t t R</formula><p>. With the term association measures R l , R co , and R d , we employ an unsupervised clustering algorithm <ref type="bibr" coords="3,422.96,393.60,73.39,9.02">(yang et al. 2003)</ref>.to derive the semantic groups of the terms in K q , which are expected to match with the event structure. Figure <ref type="figure" coords="3,462.29,405.06,5.01,9.02">2</ref> shows the QA event structure developed after performing knowledge modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Association Rule Mining and Answer Extraction</head><p>After constructing the events from the textual data, event-mining techniques are applied to discover association rules within the QA events. We perform the rule mining in two steps: association rule generation and selection. Association rules are in the form of X → Y as we mentioned earlier. Basically, we need to find all the combinations of the event elements to form the sets X and Y. To do this, we need to assign Y to contain only one of the QA event elements. For the rest of the elements, we use them to form different X in various sizes and combinations. We then store all of the X → Y rules in an association rule bank B i for QA event E i . The association rule generation algorithm is given as follows.</p><p>Usually for a QA event with n elements, the number of association rules ℵ rules is:</p><formula xml:id="formula_2" coords="3,155.22,531.06,382.06,206.98">ℵ rules = ∑ - = - * 1 1 1 n k n k C n (1) Algorithm Association_Rule_Mining (E i ) Input: QA event E i and event element set S i Output: Association rule bank B i of QA event E i 1. for each element e j in S i 2. Y = { e j } 3. G j = S i -Y 4.</formula><p>for (k=1; k=cardinality(G j ); k++) 5.</p><p>select k elements from G j to form a collection of size-k element sets R (k) ; 6.</p><p>for each size-k element set r m</p><formula xml:id="formula_3" coords="3,155.22,632.47,180.06,47.89">(k) in R (k) 7. X = r m (k) 8. add X→Y into rule bank B i 9. Return B i = {X→Y|X ∈∪ k R (k) };</formula><p>The number of these rules is potentially large and hence it is necessary to prune away some rules that do not have "interesting" information. Note that not all the rules are reliable. This is because:</p><p>• Some association rules are not complete. For example, above algorithm will discover both rule X → Y and rule Z → Y, where X ⊆ Z. Then X → Y is not as complete as Z → Y. Thus we have to decide which one should be preserved to extract more accurate answers.</p><p>• The rules appear fewer times may not be so significant for a certain QA event. The more often a rule appears in the document collection, the more important it might be.</p><p>In order to identify the interesting association rules among the event elements, we need to measure the "usefulness" of the rules. The rules containing more information and involving more event elements are considered to be more important. We consider the typical Support measure in data mining (KDD) literature <ref type="bibr" coords="4,348.28,183.66,72.69,9.02" target="#b1">(Dörre et al. 1999</ref>) and the reliability of the event elements in X. Therefore, the first measure Support (X → Y) is given as:</p><formula xml:id="formula_4" coords="4,96.48,210.98,440.80,25.71">( ) ( ) X X Y X d anded original window w exp ) ( ℵ * + ℵ * + ℵ ∧ β α (2)</formula><p>where α + β =1, ( )</p><formula xml:id="formula_5" coords="4,127.02,244.33,40.54,12.84">X original ℵ</formula><p>is the number of elements containing the original question terms in X, ( )</p><formula xml:id="formula_6" coords="4,476.88,244.33,46.18,12.84">X anded exp ℵ</formula><p>is the number of elements containing the expanded query terms in X,</p><formula xml:id="formula_7" coords="4,314.76,259.69,39.76,12.72">) ( Y X d w ∧</formula><p>is the number of passages containing both X and Y, and window ℵ is the total number of passages in the documents.</p><p>Another measure is Confidence, which helps to filter out both false information introduced by unreliable data source and the conflicting rules. Confidence (X → Y) as defined in data mining:</p><formula xml:id="formula_8" coords="4,168.48,323.78,368.79,25.86">) ( ) ( X d Y X d w w ∧ ( 3 )</formula><p>We select the best association rules for each event element based on these two measures, which indicates the "usefulness" of the event element relationships. These association rules are combined with event element matching to rank the passages. We compute the Answer Event Score (AES) for each passage from the relevant documents in the QA corpus. It is defined as:</p><formula xml:id="formula_9" coords="4,96.60,411.70,440.68,35.70">ele N i i i i ele N rule Confidence rule Support M M r ∑ = + + 1 ))) ( ) ( ( * ( (4)</formula><p>where ele M is the number of matched event elements;</p><formula xml:id="formula_10" coords="4,281.04,457.26,125.09,10.36">) ( i rule Support ( ) ( i rule Confidence</formula><p>) is the support (confidence) for the matched rule i; M i is set to 1 when rule i is present in this passage, otherwise it is set to 0; N r is the total number of association rules, and N ele is the total number of event elements for the question.</p><p>The good passages we have now describe the same event as the question. Hence, once we have these passages, we can either use the event element in Y as the answer or extract the answer from the passages. In order to extract the exact answers from the passages, we first apply named entity tagging on the top ranked passages and the event association rules. All the named entities whose type match with answer target are selected as answer candidates and ranked based on AES score of the passage where they extracted from, number and significance of the association rules that they match. The ranking formula for answers candidate j is defined as:</p><formula xml:id="formula_11" coords="4,132.54,576.93,404.74,28.73">∑ = + j Y i i j rule Support P AES 1 ) ( ) (<label>( 5 )</label></formula><p>Where Y j is the number of top association rules whose Y is matched with j;</p><formula xml:id="formula_12" coords="4,382.44,615.58,53.15,9.66">) ( i rule Support</formula><p>is the support for the rule i; AES(Pj) is the AES score of the passage where answer candidate j extracted from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fine-grained Named Entity Recognition</head><p>With the set of the top passages obtained after document retrieval and sentence ranking, QUALIFIER performs fine-grained named entity tagging before extracting the string that matches the question class (or answer target or unknown element) as the answer. The system adopts a rule-based algorithm to detect the named entities by utilizing the lexical and semantic features such as capitalization, punctuation, context, part-of-speech tagging and phrase chunking.</p><p>The input text is going through the preprocessing stage. We split sentences and remove all characters, which potentially cannot be seen or may risk the following process (mainly, those are non-ASCII characters). In addition, we extract some simple types on this stage, like NUM_PERCENT or COD_URL. In order to avoid the problems with calculations, the program may transfer the text presentation of numbers to numerical. For example, it will convert "one hundred eleven" to 111. The output is represented in the XML format, which contains some marked named entities. We then use the shallow parser by the company Infogistics. The program performs part-of-speech tagging and extracts noun groups and verb groups.</p><p>The named entity extraction module is the core for the whole system. The heuristic rules allow creating user-defined types. Also, they support the regular expression style for features of words. Example of the possible rule: person_title_np = listi_personWord src_, hum_Cap2+ src_, $set(HUM_PERSON/2) This rule sets the type for each noun phrase, which contains some known person title (academic, academician etc.). The assigned type is person_title_np for this particular phrase. $set(HUM_PERSON/2) means, that the second position (positions are started from 0) should be settled to HUM_PERSON.</p><p>Interpretation of those rules starts from the lists initialization. Each document is parsed sentence by sentence. During the parsing stage, each word is represented in the form of token with several features. The output of the interpreter may involve several competing types for some named entity. We also use the following heuristics to choose the rule in such situation:</p><p>1) Longer length. We preferred to extracted longer named entities 2) Ontology. If the rule is found in some resource (e.g., confirmed list of persons), then we assigned this type of rule; 3) Handcrafted priorities, which disambiguates the named entities correctly in most of the cases.</p><p>Our NE recognizer supports 51 types of NEs, which support 2 levels of granularity. Top levels of classification are human (HUM), location (LOC), number (NUM), object (OBJ), time (TME) and code (COD). The module extracts named entities based on the set of heuristic rules and lists for the semantic categories. We added some new types of fine-grained NE into our old system. QUALIFIER detects fine-grained named entities using a two-tiered hierarchy as shown in Table <ref type="table" coords="5,511.47,302.94,3.75,9.02" target="#tab_1">1</ref>. In last year's TREC evaluation for NUM questions, we got the NE tagging for NUMBER only 17 correct out of 29 instances, or an accuracy of 58.6%, which is the lowest rate among all the question types. In order to improve QUALIFIER's ability to recognize NUMBER, we added a range converter module into NE recognizer. We unify numbers close to each other into ranges other than the absolute surface numeric value appearing in the corpus. E.g. "5000000" should be the same range as "5.1 million".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Anaphora Resolution</head><p>In order to maximize recall of the system, which is crucial for list questions, we perform anaphora resolution for the retrieved document by MG system before we proceed to passage selection.</p><p>Firstly, we make use of Charniak's parser <ref type="bibr" coords="5,234.14,549.42,69.30,9.02" target="#b0">(Charniak, 2000)</ref> to generate the parse tree for each sentence. The Parse Tree Walker extracts two lists. One list contains all the NPs in the text and the other contains all the anaphors. Their agreement features, head-argument/head-adjunct information and all the salience factors except sentence recency are annotated. All the pleonastic pronouns are filtered out. Their antecedent is assigned as null.</p><p>For each lexical anaphor, it forms a pair with each item in a subset of the NPs (currently the implementation only considers NPs contained within three sentences proceeding the anaphor and those in the sentence where the anaphor resides). These antecedent candidate-anaphor pairs are examined by the Anaphora Binding Algorithm. For third person pronouns, similarly, the syntactic filter is applied on the candidate pairs consisting one pronoun and one NP from the set satisfying the same positional criteria. In both cases a morphological filter is applied, checking the agreement features compatibility.</p><p>For the candidates identified earlier, their salience weights are computed and they are ranked by an arbitrator accordingly.</p><p>The candidate with the highest weight is proposed as the actual antecedent. In case of tie, the one closer to the anaphor is favored. Figure <ref type="figure" coords="5,124.26,687.90,5.01,9.02">3</ref> shows the structure of the anaphora resolution process. In this way, we could identify the wrong answer "50000", which is what the surface text shown. Only when we know certain constraint to indicate the actual range of Maryland's population, we know that the surface answer is wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">List Questions</head><p>List question answering is quite similar to what we did for factoid questions. However, we allow the answer extraction module to return multiple answers and remove duplicated answer candidates with the help of abbreviation co-reference. The exact answers are exacted based on patterns and its ranking. Each of these answers is passed to answer justification module for confirmation.</p><p>List questions are processed similarly to factoid questions. However, we allow the answer extraction module to return multiple answers and remove duplicated answer candidates with the help of abbreviation co-reference. In general, our method is more data-driven because we want it to be domain independent. However, we still utilized some heuristics to enhance the process.</p><p>From a list question, we obtain the same or less amount information comparing to factoid questions since the constraint from the list question itself will be more relax. Under this situation, possible answers for a list question could be in any number. It's hard for us to decide when the right time to stop is. The only way to abase this uncertainty is to perform exhaustive search or near exhaustive search. These patterns could mean surface text patterns, commonly used cue words.</p><p>There are some examples:</p><p>• &lt;same_type_NE&gt;, &lt;same_type_NE&gt; and &lt;same_type_NE&gt; + verb … • … include: &lt;same_type_NE&gt;, &lt;same_type_NE&gt;, &lt;same_type_NE&gt; … • "list of …"</p><p>• "top" + number + adj-superlative • "alphabetical list" of …</p><p>• "following list…"</p><p>Due to the characteristics of definitional questions, i.e. informative and stress on completeness, we treat answering definitional questions as an integrated process of information retrieval and summarization. We utilize techniques from information retrieval as anti-noise mechanism and make use of summarization techniques to avoid redundancy in the results.</p><p>The pipeline system can be divided into 2 modules: sentence ranking, and sentence selection plus summary generation.</p><p>Figure <ref type="figure" coords="7,86.24,124.56,5.01,9.02" target="#fig_1">4</ref> shows an overview of the definition question subsystem. Due to the heterogeneity of the news articles, many of the input documents are actually not related to the search target. We applied a document filter to the get only those documents containing all terms of the search target are labeled as "relevant".</p><p>We then applied anaphora resolution to sentences from all "relevant" documents to replace appropriate pronouns with the search target. Finally, those sentences containing any part of the search target and their contextual sentences (one sentence proceeding and following them respectively) are sifted as "positive set". All other sentences in the input documents are considered as "negative set".</p><p>As for sentence ranking, we utilized evidence from two sources, namely the input documents and the Web. Basically, we use sentence frequency (the number of sentences containing the word) as the main metric to measure the importance of each word. The sentences in "negative set" are used to provide "negative examples". In other words, a word is considered important if it appears often in the sentences of "positive set" while occurs rarely in the "negative set". This can be done in a TFIDF-like fashion:</p><p>) </p><formula xml:id="formula_13" coords="7,155.28,443.24,382.00,19.57">+ × + = ∑ ∈ (6)</formula><p>In order to account for the diversity of the news articles, we use the Web as a supplementary source. The Web evidence is derived from the snippets obtained using the queries constructed from the sentences of the "positive set" containing any part of the search target. Specifically, the expansion terms are selected by:</p><formula xml:id="formula_14" coords="7,94.62,515.07,442.65,20.90">) _ ( ) ( ) , _ ( 1 log( # ) ( ) ( term sch f w f w term sch Co Sentences Total w SF w Weight Exp + + × = ) ( 7 )</formula><p>The weight of the sentence for Web evidence can be expressed as:</p><formula xml:id="formula_15" coords="7,94.62,565.59,442.66,22.61">∑ + × + = w Positive Web w SF Corpus ntences PositiveSe w SF Web s Weight ) ) ( _ # 1 log( )) ( _ 1 log( ) (<label>( 8 )</label></formula><p>These two weight values are linearly combined to represent the sentence weight for the search target.</p><formula xml:id="formula_16" coords="7,94.62,612.63,442.65,13.37">Web Corpus Weight Weight s Weight ⋅ - + ⋅ = ) 1 ( ) ( λ λ<label>( 9 )</label></formula><p>After sentence ranking, we have a list of ordered sentences with the most relevant sentences to the search target being placed in the top of the list. We made use of summarization techniques to accomplish sentence selection because sentences from news articles are likely to contain duplicated content. We employed a variation of the Maximal Marginal Relevance (MMR) to select sentences from the list while avoiding redundancy between the summary sentences: ( 1 1 ) d) Go to Step c) till the length limit of the target summary is satisfied.</p><p>In general, our method is more data-driven because we aim it to be domain independent. However, we still utilized some heuristics to enhance the process. For example, if a sentence containing "&lt;Target&gt;, a ……" or "&lt;Target&gt;, which is the ……", it is likely to be a good definitional sentence for the target. Thus in the sentence ranking and content selection processes, we employed a set of heuristic rules. In content selection process, we applied heuristics to selecting meaningful fragments of the summary sentences to construct the final answers for the search target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We submitted 3 runs for TREC 2003 QA main task. They are mmlnus03r1 (definition run 1 + factoid run 1 + list run 1), mmlnus03r2 (definition run 2 + factoid run 1 + list run 1) and mmlnus03r3 (definition run 3 + factoid run 2 + list run 1). The first run mmlnus03r1 emphasized more on recall (answer coverage) while the third run mmlnus03r3 more on precision (answer accuracy). The second run was a hybrid to test the balance between recall and precision.</p><p>The 2 factoid runs focus on precision and recall each. Our first run maximizes recall (answer coverage) by using anaphora resolution, abbreviation co-reference, and successive constraint relaxation to find as many answers as possible without answer justification. In the second run, however, we focused on precision (answer correctness), answer justification plays a rather important role and successive constraint relaxation keeps the recall at a satisfactory level. Table <ref type="table" coords="8,481.79,334.56,5.01,9.02" target="#tab_3">2</ref> gives the scores for our factoid runs.</p><p>The 3 definition runs that we submitted to TREC balance precision and recall. We empirically set the length of the summary for people and objects. Our first run maximizes recall by using full sentences. We have the same length limit for summary in run 2, while text fragments are extracted by heuristics instead of using the full sentences. In the third run (to maximize precision), fewer sentences were extracted for summary and text fragments extraction was also applied. The performance of our definition runs is given in Table <ref type="table" coords="8,267.89,409.56,3.77,9.02" target="#tab_4">3</ref>.</p><p>Only one run of list questions is submitted. Its average precision over 37 questions is given in Table <ref type="table" coords="8,462.57,427.08,3.75,9.02" target="#tab_5">4</ref>.</p><p>Table <ref type="table" coords="8,85.87,444.54,5.01,9.02" target="#tab_6">5</ref> summarizes the overall performance for the three submitted runs. It shows that nulmmlr2 gives the best performance, which uses strict constraints and focuses on answer accuracy. We develop three subsystems for this year's TREC. Definition questions answering combines techniques from information retrieval as anti-noise mechanism and make use of summarization techniques to avoid redundancy in the results. Factoid question and list questions take the advantage of Event-based QA, which employs the intuition that there exists implicit knowledge that connects an answer to a question, to extract the correct answer. Association rules are mined to get the relationship among the QA event elements. The whole system consists many modules, including detailed question analysis, QA event construction, event mining, document retrieval, passage retrieval, answer extraction, answer justification, finegrained named entity recognition, anaphora resolution, canonicalization co-reference, and successive constraint relaxation.</p><p>We also manually constructed Ontology to lever the performance of our NE and answer justification modules.</p><p>We are currently refining our approach in several directions. First, we are trying to formulate a formal proof of our QA event hypothesis. Second, we are working towards an online question answering system. Our longer-term research plan includes Interactive QA, and the handling of more difficult analysis and opinion question types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,243.90,313.44,124.20,9.02;6,145.44,57.60,321.06,247.50"><head>Figure</head><label></label><figDesc>Figure 3: Anaphora Resolution</figDesc><graphic coords="6,145.44,57.60,321.06,247.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,214.50,283.74,183.02,9.02"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of Definition Subsystem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,57.60,688.56,249.44,9.02;7,57.60,700.08,172.74,9.02;7,57.60,711.60,153.04,9.02;7,75.60,719.41,242.43,12.92;8,77.64,66.72,133.34,9.02;8,305.40,74.50,14.49,8.09;8,329.28,60.88,9.99,8.09;8,322.02,60.88,3.50,8.09;8,304.98,60.88,13.49,8.09;8,252.78,67.42,9.99,8.09;8,236.76,67.42,12.49,8.09;8,215.22,67.42,12.99,8.09;8,291.42,65.00,12.04,6.30;8,284.40,65.00,2.72,6.30;8,299.58,74.50,4.50,8.09;8,339.54,60.88,2.99,8.09;8,325.68,60.88,2.25,8.09;8,318.66,60.88,2.99,8.09;8,263.04,67.42,2.99,8.09;8,249.42,67.42,2.99,8.09;8,230.34,67.42,4.50,8.09;8,276.96,57.94,7.11,12.23;8,286.74,62.75,4.98,8.56;8,268.50,64.52,4.93,11.01;8,525.60,66.72,16.72,9.02;8,77.64,85.92,476.77,9.02;8,75.60,97.44,210.05,9.02;8,339.72,115.46,3.01,8.12;8,335.64,122.54,3.01,8.12;8,326.34,122.54,3.01,8.12;8,268.56,122.54,4.51,8.12;8,232.38,122.54,4.51,8.12;8,308.10,109.51,4.51,8.12;8,266.04,109.51,4.51,8.12;8,246.30,109.51,4.51,8.12;8,218.52,115.45,4.51,8.12;8,204.90,115.45,14.54,8.12;8,166.20,115.45,3.01,8.12;8,152.28,115.45,2.26,8.12;8,145.20,115.45,3.01,8.12;8,293.76,126.13,3.51,6.32;8,187.32,112.44,7.14,12.27;8,190.08,123.44,5.39,8.59;8,182.40,123.44,5.00,8.59;8,238.26,119.63,4.95,11.05;8,224.28,112.55,4.95,11.05;8,171.24,112.55,4.95,11.05;8,195.66,125.71,7.80,6.32;8,187.02,125.71,2.73,6.32;8,178.38,125.71,4.68,6.32;8,298.50,126.13,26.13,6.32;8,284.88,126.13,7.41,6.32;8,329.76,122.54,6.02,8.12;8,275.10,122.54,10.03,8.12;8,244.44,122.54,22.07,8.12;8,314.58,109.52,10.03,8.12;8,272.46,109.52,33.59,8.12;8,253.38,109.52,10.53,8.12;8,156.00,115.45,10.03,8.12;8,148.62,115.45,3.51,8.12;8,131.58,115.45,13.54,8.12"><head></head><label></label><figDesc>a) All sentences are ordered in descending order by weights. b) Add the first sentence to the summary. c) Examine the following sentences.If Weight(stc)-Weight(next_stc) &lt; β avg_sim(stc) continue; ) is the average similarity of the sentence stc and the sentences already in the summary. The similarity is defined as the measure of their word overlapping as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,57.60,320.52,485.36,109.88"><head>Table 1 : Partial List of Fine-Grained Named Entities</head><label>1</label><figDesc></figDesc><table coords="5,57.60,338.40,485.36,92.00"><row><cell>HUMAN:</cell><cell>Basic, Organization, Person</cell></row><row><cell>TIME:</cell><cell>Basic, Day, Month, Year</cell></row><row><cell cols="2">LOCATION: Basic, Body, City, Continent, Country, County, Island, Lake, Mountain, Ocean, Planet, Province, River,</cell></row><row><cell></cell><cell>Town</cell></row><row><cell>NUMBER:</cell><cell>Basic, Age, Area, Count, Degree, Distance, Frequency, Money, Percent, Period, Range, Size, Speed</cell></row><row><cell>CODE</cell><cell>URL, Telephone, Post code, email address, Product index</cell></row><row><cell>OBJECT:</cell><cell>Basic, Animal, Breed, Color, Currency, Entertainment, Game, Language, Music, Plant, Profession,</cell></row><row><cell></cell><cell>Religion, War, Works</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,141.60,473.70,326.71,110.84"><head>Table 2 : Performance over 413 Factoid Questions in TREC-12</head><label>2</label><figDesc></figDesc><table coords="8,141.60,491.52,326.71,93.02"><row><cell></cell><cell># correct</cell><cell cols="2">232 Accuracy</cell><cell>0.562</cell></row><row><cell></cell><cell># unsupported</cell><cell>24</cell><cell>Precision of recognizing NIL</cell><cell>0.160</cell></row><row><cell>nusmmlr1</cell><cell># inexact</cell><cell>13</cell><cell>Recall of recognizing NIL</cell><cell>0.400</cell></row><row><cell>nusmmlr2</cell><cell># wrong</cell><cell>144</cell><cell></cell><cell></cell></row><row><cell></cell><cell># correct</cell><cell cols="2">225 Accuracy</cell><cell>0.545</cell></row><row><cell></cell><cell># unsupported</cell><cell>20</cell><cell>Precision of recognizing NIL</cell><cell>0.158</cell></row><row><cell>nusmmlr3</cell><cell># inexact</cell><cell>12</cell><cell>Recall of recognizing NIL</cell><cell>0.767</cell></row><row><cell></cell><cell># wrong</cell><cell>156</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,170.16,605.16,271.68,50.90"><head>Table 3 : Performance over 50 Definition Questions in TREC-12</head><label>3</label><figDesc></figDesc><table coords="8,221.28,623.04,177.95,33.02"><row><cell>Average F score for definition question</cell><cell>nusmmlr1 0.441 nusmmlr2 0.473</cell></row><row><cell></cell><cell>nusmmlr3 0.432</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,183.18,676.68,245.60,50.84"><head>Table 4 : Performance over 37 List Questions in TREC-12</head><label>4</label><figDesc></figDesc><table coords="8,221.40,694.50,169.24,33.02"><row><cell>nusmmlr1</cell><cell>Average precision</cell><cell>0.568</cell></row><row><cell>nusmmlr2</cell><cell>Average recall</cell><cell>0.264</cell></row><row><cell>nusmmlr3</cell><cell>Average F1</cell><cell>0.317</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,57.60,60.06,384.69,64.13"><head>Table 5 : Overall Performance of 3 Submitted Runs in TREC-12</head><label>5</label><figDesc></figDesc><table coords="9,264.06,77.94,78.95,33.02"><row><cell>nusmmlr1</cell><cell>0.471</cell></row><row><cell>nusmmlr2</cell><cell>0.479</cell></row><row><cell>nusmmlr3</cell><cell>0.460</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank <rs type="person">Dr. Jimin Liu</rs>, <rs type="person">Ms Jing Xiao</rs> and <rs type="person">Mr. Chun-Keat Koh</rs> for their great efforts in the earlier development for the named entity recognition module. Also, we would like to thank our student assistants <rs type="person">Yee-Fan Tan</rs>, <rs type="person">Victor Goh</rs> and <rs type="person">Shi-Yong Neo</rs> for constructing Ontology and conducting various experiments. This work was supported in part by the <rs type="person">A*Star</rs> and <rs type="funder">NUS Joint-Lab</rs> funding.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,57.60,365.88,496.80,9.02;9,66.60,377.34,343.91,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,149.32,365.88,149.61,9.02">A maximum-entropy-inspired parser</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,325.45,365.88,228.95,9.02;9,66.60,377.34,73.65,9.02">Proceedings of the first 1st Conference of the North American Chapter</title>
		<meeting>the first 1st Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,57.60,394.86,496.89,9.02;9,69.12,406.38,485.25,9.02;9,69.12,417.84,108.64,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,277.90,394.86,254.56,9.02">Text mining: finding nuggets in mountains of textual data</title>
		<author>
			<persName coords=""><forename type="first">Jochen</forename><surname>Dörre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Gerstl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roland</forename><surname>Seiffert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,69.12,406.38,485.25,9.02;9,69.12,417.84,60.29,9.02">Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining (SIGKDD&apos;1999</title>
		<meeting>the fifth ACM SIGKDD international conference on Knowledge discovery and data mining (SIGKDD&apos;1999</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="398" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,57.60,435.36,361.58,9.02;9,57.60,452.88,213.64,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,222.83,435.36,81.34,9.02">Managing Gigabytes</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<ptr target="http://www.infogistics.com/posdemo.htm" />
	</analytic>
	<monogr>
		<title level="j" coord="9,57.60,452.88,40.99,9.02">Infogistics</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,57.60,470.34,496.95,9.02;9,69.12,481.86,185.83,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,152.12,470.34,228.21,9.02">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,403.51,470.34,151.04,9.02;9,69.12,481.86,142.69,9.02">the Proceedings of the Eleventh Text REtrieval Conference (TREC&apos;2002)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,57.60,499.38,496.89,9.02;9,69.12,510.84,297.94,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,180.84,499.38,352.99,9.02">The Integration of Lexical Knowledge and External Resources for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,69.12,510.84,237.47,9.02">the Proceedings of the Eleventh Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,57.60,528.36,496.75,9.02;9,69.12,539.88,485.25,9.02;9,69.12,551.34,271.03,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,261.27,528.36,293.07,9.02;9,69.12,539.88,82.85,9.02">Structured Use of External Knowledge for Event-based Open Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,180.00,539.88,374.37,9.02;9,69.12,551.34,266.52,9.02">the Proceedings of the Twenty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2003)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
