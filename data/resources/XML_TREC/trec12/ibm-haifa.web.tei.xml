<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.64,84.00,341.94,21.32;1,156.00,107.76,299.34,21.32;1,206.76,131.52,197.98,21.32">Juru at TREC 2003 -Topic Distillation using Query-Sensitive Tuning and Cohesiveness Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.16,163.43,55.94,10.65"><forename type="first">Einat</forename><surname>Amitay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.00,163.43,60.36,10.65"><forename type="first">David</forename><surname>Carmel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.52,163.43,59.71,10.65"><forename type="first">Adam</forename><surname>Darlow</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.20,163.43,84.44,10.65"><forename type="first">Michael</forename><surname>Herscovici</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.28,175.43,64.44,10.65"><forename type="first">Ronny</forename><surname>Lempel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.88,175.43,48.21,10.65"><forename type="first">Aya</forename><surname>Soffer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.40,199.19,53.96,10.65"><forename type="first">Reiner</forename><surname>Kraft</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.36,199.19,50.64,10.65"><forename type="first">Jason</forename><surname>Zien</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.64,84.00,341.94,21.32;1,156.00,107.76,299.34,21.32;1,206.76,131.52,197.98,21.32">Juru at TREC 2003 -Topic Distillation using Query-Sensitive Tuning and Cohesiveness Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B8320D95B91C7977439EA3F118BEFD8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the third year that our group participates in TREC's Web track, the second year in the topic distillation task. Our experiments last year, as well as those of other participants, indicated that sophisticated link-based measures did not significantly improve search results in comparison to standard text-based relevance scoring. We thus focused our experiments this year on improving the ranking algorithms of our core search engine, Juru, and on developing measures that are good indicators of topical pages.</p><p>In particular, realizing that one ranking flavor does not fit all queries <ref type="bibr" coords="1,453.48,371.21,13.11,10.80" target="#b2">[3]</ref> [6], we developed a method, which fine tunes the parameters governing the ranking formula based on the nature of the query. This novel ranking method, called the QUEry Sensitive Tuner (or QUEST), tunes the ranking parameters according to the query type. QUEST classifies queries into "informational" vs. "navigational" by considering both the query's length and the expected number of documents containing all query terms (edf). For queries with a few expected results, each document's score is primarily determined according to the document's textual score, i.e. its similarity to the query. On the other hand, for queries with many expected results, document scores are determined by considering additional factors such as anchor-text data, number of in-links, etc.</p><p>In addition, we continued experimenting with some of the topic distillation filters we introduced last year <ref type="bibr" coords="1,205.20,539.81,11.92,10.79" target="#b1">[2]</ref>, as well as with a new cohesiveness filter. The cohesiveness filter tries to identify pages that focus on the desired topic in contrast to pages than just mention it in passing, or which mention it in the context of a broader topic. This is achieved by identifying pages in which the query terms are uniformly distributed over the entire page.</p><p>The rest of this paper is organized as follows: Section 2 describes the QUEST algorithm and the query parameters tuned by the algorithm according to the query type. In section 3 we describe the cohesiveness filter that tries to distill pages that focus on the desired topic. Section 4 describes the results of the official runs submitted to TREC. Section 5 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QUEry Sensitive Tuner (QUEST)</head><p>The QUEST algorithm tunes the query parameters according to the query's characteristics, which in turn imply its type. QUEST classifies queries into "informational" vs. "navigational" by considering both the query's length and the expected number of documents containing all query terms (edf). The main rationale is that for short queries with many expected results (large edf), standard IR techniques based on textual scores cannot discriminate between topical and non-topical pages, therefore more factors, especially static scores and anchor-text scores associated with the documents, should be used in order to distill the best results. On the other hand, for long queries with few expected results, document's static scores, which are independent of the query, should only take a secondary role, as standard IR techniques are expected to return satisfactory results.</p><p>After query classification, the query parameters are tuned according to the query type. For "navigational" queries, parameters are set such that the number of in-links per page has stronger effect on the page's final score than for "informational" queries for which the textual score is dominant in determining the final score. Similarly, the anchor text associated with these in-links is weighted more heavily in navigational queries compared to informational ones. QUEST does not, however, assume just the two extremes; rather it tunes the parameters on a sliding scale ranging from purely navigational to purely informational.</p><p>We now describe in more details the QUEST algorithm. QUEST treats separately queries containing one, two and three+ terms. For each query length, it maintains a threshold on the query edf. In addition, it also maintains two sets of values for several ranking parameters, one set for informational queries and one set for navigational queries. A query with an edf lower than the threshold is classified as "informational" and its parameters are set using the informational set of parameters. A query with an edf higher then the threshold is considered "navigational" and its parameters are set using the navigational set of parameters. See Section 2.2 for details on the calculation of the edf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Parameters tuned by QUEST</head><p>QUEST tunes three sets of parameters as described below: I. Boosts for different token types. The tokens of a document are classified into several types, and the significance of a token and its contribution to the document's textual score is determined by the boost associated with its type. Thus, the occurrence of tokens with a high boost in the document's content significantly affects its textual score, while tokens with a low boost contribute much less. The token types include: a. Textual tokens: tokens extracted from the document's raw text which are differentiated into: i. Title tokens -extracted from the document's title. ii. Strong tokens -extracted from the document's headers. iii. Mid tokens -extracted from the document's emphasized text (colored, bold, etc.). iv. Regular tokens -all the rest. b. Anchor tokens: tokens extracted from the anchor text of the document's in-links. These tokens are differentiated according to the relation between the source and target of the link: i. Different site anchor: anchor tokens where the source site differs from the target site ii. Same site anchor: anchor tokens where the anchor and the target pages are from the same site but in different directories. iii. Same dir anchor: anchor tokens where the source and the target pages reside in the same directory.</p><p>c. URL tokens: tokens extracted from the document's URL.</p><p>d. Snippet tokens: Tokens extracted from the document's snippet. We compute for each document a snippet based on its anchors, using the method described in <ref type="bibr" coords="3,272.52,264.77,12.01,10.80" target="#b0">[1]</ref>.</p><p>For informational queries, textual tokens are given the highest boosts while for navigational queries anchor tokens, URL tokens, and snippet tokens receive higher boosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Lexical Affinity weight (LA-Weight).</head><p>Our ranking algorithm takes into account lexical affinities common to the query and the document, in addition to simple query terms. Lexical affinities are pairs of closely related terms frequently found in proximity to each other <ref type="bibr" coords="3,355.20,381.41,11.92,10.80" target="#b6">[7]</ref>. Each query term a simple keyword or a lexical affinity, contributes to the textual score of the document according to its term frequency and to its inverse document frequency (following the tf-idf formula). The LA-weight determines the relative contribution of lexical-affinities to the document's score compared to simple keywords. Experiments have shown that the LA-weight should be smaller for longer queries <ref type="bibr" coords="3,216.60,459.29,12.01,10.80" target="#b3">[4]</ref>. In accordance, QUEST assigns a lower LA-weight for informational queries as compared to navigational queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Static Score coefficient:</head><p>The final score of a document is computed by linearly combining its textual score with a static score. The static score is based on the number of its in-links. The Static Score coefficient determines the relative weight of the static score with respect to the weight of the textual score of the document. QUEST assigns a higher value to the static score coefficients for navigational queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Approximating the expected document frequency (edf) per query</head><p>The main feature used by QUEST for query classification is the expected document frequency edf. For one-term queries, the document frequency (df) can be precisely determined since the df of each term is stored within the index. For multi-term queries, the edf must be approximated since the only way to derive the precise edf is to process the query.</p><p>Given a query with k terms q = q 1 ..q k . The edf of the query is approximated based on the df values of the individual query terms. Assuming independence between query terms, the number of documents containing all of the query terms can be estimated by multiplying the occurrence probability of all query terms. The occurrence probability of a query term q i can be approximated by Pr(q i ) = df(q i )/|D|, where df(q i ) is the document frequency of term q i and |D| is the total number of documents in the collection. Thus, the edf of a query q with k independent terms is:</p><formula xml:id="formula_0" coords="4,251.88,135.41,108.21,42.83">| | | | ) ( ) ( 1 D D q df q edf k k i i</formula><p>Since query terms are usually not independent, but are rather expected to co-appear in documents, we heuristically multiply the above by the number of query terms k:</p><formula xml:id="formula_1" coords="4,258.00,234.05,93.98,42.95">1 1 | | ) ( ) ( k k i i D q df k q edf</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Cohesiveness Filter</head><p>The relevance score computed above finds good individual candidates for topical pages. However, given that the goal of the topic distillation task is to find a set of topical pages, we apply some additional filters that influence the final ranking. The goal of these topic distillation filters is to identify pages that exhibit features of a good topical page, and to boost their query relevance score. We applied the following sequence of filters to the initial search results: 1) duplicate-elimination filter, 2) sitecompression filter, and 3) the new cohesiveness filter. The first two filters were already reported last year in <ref type="bibr" coords="4,245.76,425.71,12.01,10.78" target="#b1">[2]</ref>. The new cohesiveness filter tries to identify pages that focus on the desired topic in contrast to pages that just mention it in passing, or which mention it in the context of a broader topic. This is achieved by identifying pages in which the query terms are uniformly distributed over the entire page.</p><p>More specifically, for each document in the result set we measure the uniformity of the query terms along the document's content. This is done by measuring the entropy of the occurrence distribution of the query terms within the document. The entropy is maximal when the term occurrences are uniformly distributed over the document's content. The entropy is minimal when all term occurrences are close to each other. We conjecture that the larger the entropy of the term distribution, the higher its uniformity.</p><p>Given a query term t with a list of positions o 1 , o 2 ,...,o k within document d of length |d|, the entropy of the term occurrence distribution within d is measured by:</p><formula xml:id="formula_2" coords="4,131.04,633.77,348.50,28.56">) | log(| ) | (| ) log( ) ( log ) , ( 1 2 1 1 1 k k i i k i i i o d o d o o o o o o d t entropy</formula><p>The cohesiveness of the document d for query q is defined by the weighted average entropy of q's query terms within d:</p><formula xml:id="formula_3" coords="5,205.68,72.31,199.94,20.07">q t d t entropy t idf q d ss cohesivene ) , ( * ) ( ) , (</formula><p>The cohesiveness filter computes for each document in the result set a new score based on its previous score and its cohesiveness. It then re-ranks the search results based on the new score. The new score is a linear combination of the previous score and the cohesiveness score. The cohesiveness filter weight determines the relative weight between the two scores. This weight is set by QUEST according to the query type. For purely informational queries this weight is low, while for purely navigational queries the weight is high.</p><p>The cohesiveness filter is especially useful for queries with high frequency terms. In such cases, the cohesiveness filter will prefer pages where the query terms occur throughout the entire document over pages where query terms appear only in part of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We used the Juru search engine <ref type="bibr" coords="5,270.24,306.19,13.22,10.78" target="#b4">[5]</ref> to index and search the pages in the ".gov" domain. Each page was indexed based on its content as well as its anchor descriptions, its URL, and its snippet (see Section 2). Each page is scored by a linear combination of its textual score and its link topology score (a static score). The static score of page p is based on the number of links n pointing to p:</p><formula xml:id="formula_4" coords="5,247.44,385.04,116.31,25.51">otherwise N n N n p St / 0 . 1 ) (</formula><p>The constant N determines an upper bound on a page's in-link number; each page with more than N in-links receives the maximum static score of 1. The N parameter is also set by QUEST according to the query type, low value for informational queries and high value for navigational queries.</p><p>The combined scores are used to rank the set of pages. The top 200 pages are reranked using the sequence of filters described above designed to guarantee a mixture of good sources in the top-10 list returned by the system. The top 100 pages were submitted to TREC.</p><p>We submitted 5 runs for the topic distillation task. The JuruFull run scored pages based on both a textual and a topological score. The query parameters were tuned separately for each query using the QUEST algorithm as described above, and all filters were invoked on the search results. In the JuruNoAnchor run we zeroed the boosts of all anchor tokens, thus, textual ranking is based only on the document content. In the JuruNoCohes run the cohesive filter was ignored by zeroing the cohesiveness filter weight. In JuruNoQueryDiff the QUEST algorithm was ignored by fixing the values of the ranking parameters for all the queries. In JuruNoSS the document static scores were ignored.</p><p>Table <ref type="table" coords="5,142.20,685.27,5.62,10.78">1</ref> shows the average P@10 and average R-precision of our runs and the average-best and median P@10 of all participants. While the results of all our runs are much higher than the median, the results are somewhat disappointing. For 16 topics JuruFull could not find pages marked relevant by the assesors in its top 10 results, among them 3 topics for which all participants completely failed. On the other hand, for 7 topics JuruFull achieved the best result among all participants. Both the JuruNoAnchor and the JuruNoSS runs achieved significantly lower results than the other runs, indicating the significance of link analysis, contradicting our findings from previous year about the relatively insignificance of link analysis for the topic distillation task. There was however no difference between the runs applying QUEST and the cohesiveness filter, and thus the experiments we hoped to achieve by participating in this task are inconclusive.  <ref type="table" coords="6,137.28,240.84,4.67,9.67">1</ref> --Average P@10 and R-precision of our runs and the average-best and median P@10 of all participants.</p><p>Figure <ref type="figure" coords="6,142.92,284.35,5.62,10.78" target="#fig_0">1</ref> shows the difference between P@10 of our runs and the median P@10 of all participants. For almost all topics (except 3 for the JuruFull run) our runs achieved a better result than the median.</p><p>-0.  The difference between P@10 of the best result, some of our runs, and the median P@10 of all participants</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>Our experiments this year focused on improving the ranking algorithm of our core search engine, and on developing measures that are good indicators of topical pages. We experimented with the QUEST algorithm that tunes the query parameters according to the query's characteristics. We also experimented with the cohesiveness filter that tries to find topical pages by identifying those in which the query terms are uniformly distributed over the entire page. Our results demonstrate that link analysis and anchor-text data slightly improved the results this year, in contrast to last year. However, our results do not indicate any advantage for QUEST or the cohesiveness filter. One reason for this is the apparent disparity between our understanding and the assessors understanding of the notion of a 'topical page". The topic distillation task, in our opinion, is still not well defined. Consequently, our system in several cases returned many good pages (according to our judgment) that were rejected by the assessors as non-relevant. We believe that QUEST and cohesiveness can indeed make a difference -more exhaustive experiments are needed to study their effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,115.68,494.11,379.99,10.81;6,232.80,507.19,145.85,10.78"><head>Figure 1</head><label>1</label><figDesc>Figure1--The difference between P@10 of the best result, some of our runs, and the median P@10 of all participants</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,135.96,163.27,365.07,10.78;7,110.52,176.23,192.65,10.78" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Paris</surname></persName>
		</author>
		<title level="m" coord="7,266.28,163.36,234.75,10.56;7,110.52,176.23,127.48,10.78">Automatically Summarizing Web Sites -Is There A Way Around It? CIKM 2000</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="173" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,135.96,194.83,364.87,10.78;7,110.52,207.67,390.49,10.78;7,110.52,220.63,374.21,10.78" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,475.20,194.92,25.63,10.56;7,110.52,207.76,163.40,10.56">Topic Distillation with Knowledge Agents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,297.00,207.67,204.01,10.78;7,110.52,220.63,114.50,10.78">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,135.96,239.23,364.97,10.78;7,110.52,252.19,55.49,10.78" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,220.68,239.32,118.36,10.56">A taxonomy of web search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,347.04,239.23,91.31,10.78">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,135.96,270.79,364.92,10.78;7,110.52,283.75,390.53,10.78;7,110.52,296.71,111.05,10.78" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Chong</surname></persName>
		</author>
		<title level="m" coord="7,291.96,270.79,208.93,10.78;7,110.52,283.75,142.14,10.78">The GURU system in TREC-6. The Sixth Text Retrieval Conference (TREC-6)</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="535" to="540" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,135.96,315.43,364.97,10.78;7,110.52,328.39,390.43,10.78;7,110.52,341.35,390.34,10.78;7,110.52,354.31,92.09,10.78" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,147.84,328.48,242.88,10.56">Juru at TREC 10 -Experiments with Index Pruning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,413.88,328.39,87.07,10.78;7,110.52,341.35,154.39,10.78">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,135.96,372.91,364.97,10.78;7,110.52,385.87,390.46,10.78;7,110.52,398.83,330.17,10.78" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,259.44,373.00,237.33,10.56">Query type classification for web document retrieval</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,110.52,385.87,390.46,10.78;7,110.52,398.83,171.56,10.78">Proceedings of the 26th annual international ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,135.96,417.43,365.02,10.78;7,110.52,430.39,390.18,10.78;7,110.52,443.35,390.29,10.78;7,110.52,456.43,25.37,10.78" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,278.04,417.52,222.94,10.56;7,110.52,430.48,139.31,10.56">Full text indexing based on lexical relations: An application: Software libraries</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Smadja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,270.00,430.39,230.70,10.78;7,110.52,443.35,308.96,10.78">Proceedings of the 12th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 12th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1989">1989. 1989</date>
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
