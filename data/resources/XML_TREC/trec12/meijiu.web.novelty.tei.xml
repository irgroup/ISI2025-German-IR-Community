<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.92,136.81,399.61,12.24">Meiji University Web and Novelty Track Experiments at TREC 2003</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.42,172.66,67.33,9.19"><forename type="first">Ryosuke</forename><surname>Ohgaya</surname></persName>
							<email>ohgaya@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.63,172.66,82.46,9.19"><forename type="first">Akiyoshi</forename><surname>Shimmura</surname></persName>
							<email>sinmura@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.84,172.66,70.09,9.19"><forename type="first">Tomohiro</forename><surname>Takagi</surname></persName>
							<email>takagi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.32,242.69,58.71,9.19"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.92,136.81,399.61,12.24">Meiji University Web and Novelty Track Experiments at TREC 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">091CA2CD819B9DB7C69FDBD46E555487</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This year we participated in TREC for the first time. We submitted runs for Novelty track and the topic distillation task of Web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Conceptual Fuzzy Sets</head><p>To represent the meaning of a word, we proposed conceptual fuzzy sets (CFS) <ref type="bibr" coords="1,432.72,417.77,11.33,9.19" target="#b1">[1]</ref> <ref type="bibr" coords="1,444.05,417.77,11.33,9.19" target="#b2">[2]</ref>. In CFS, the meaning of a word is represented by the distribution of the activation of other words and dynamically changes reflecting context. The image of CFS is shown in Figure <ref type="figure" coords="1,350.66,452.75,3.82,9.19" target="#fig_0">1</ref>.</p><p>We used two different implementation of CFS in each track.</p><p>In Figure <ref type="figure" coords="1,134.75,706.61,3.80,9.19" target="#fig_0">1</ref>, white surrounding concepts explain the centered gray concept. The strength of the links </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Web Track</head><p>We submitted five runs for the topic distillation task. Our system is based on vector space model with tf-idf weighting. To create a document vector, we used the contents of a target page and those of its neighboring pages in the run meijihil3, meijihil4 and meijihil5.</p><p>Searching procedure is:</p><p>1. Expand query using conceptual fuzzy sets (in meijihil2, meijihil4 and meijihil5) 2. Calculate similarities 3. Rerank search results based on out-degree (in meijihil5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Aggregate pages from the same server into one</head><p>Table <ref type="table" coords="2,117.95,400.25,5.11,9.19" target="#tab_0">1</ref> shows the description of each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Using the contents of inlinks and outlinks</head><p>In the World Wide Web, a web page and its neighboring pages are likely to be on the same topic. We evaluated whether incorporating the contents of neighboring pages in that of a target page improve search accuracy. We create the document vector of a target page as follows:</p><p>1. Create the word vector of each page using only its contents with tf-idf weighting.</p><p>2. Aggregate the word vector of the target page and those of its neighboring pages: </p><formula xml:id="formula_0" coords="3,165.12,134.62,140.34,28.52">+ = k d j d d i k j i V V V V γ β α</formula><p>where i d V is the word vector of the target page, j d V is the word vector of a page that is linking to the target page and k d V is the word vector of a page that is linked to by the target page.</p><p>In the run meijihilw3, meijihilw4 and meijihilw5, we set α , β and γ to be 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Query expansion using conceptual fuzzy sets</head><p>We used CFS to expand queries. To construct CFS, we need a dictionary in which the meanings of words are represented by other words and their degree of relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Dictionary for conceptual fuzzy sets</head><p>To create the dictionary, we used a method proposed in <ref type="bibr" coords="3,323.85,353.57,11.92,9.19" target="#b3">[3]</ref> in which overlapping clusters of terms are generated based on co-occurrence (Actually, documents and other related information are also clustered simultaneously with terms, but we used only term clusters for the dictionary). A term cluster is composed of a representative word and related words with their degrees of relationship and is considered as a word vector that represents the concept of the word. We refer to this word vector as concept vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Expansion procedure</head><p>The similarities between the input vector and each concept vector are calculated using cosine measure:</p><formula xml:id="formula_1" coords="3,126.54,495.53,75.86,35.80">i i C q C q i V V V V S ⋅ =</formula><p>where q V is the input vector and i C V is the i th concept vector.</p><p>The expanded query vector is the weighted sum of the concept vectors:</p><formula xml:id="formula_2" coords="3,124.26,578.90,70.71,30.50">∑ ⋅ = ′ i C i q i V S V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Similarity calculation</head><p>We used cosine measure to calculate the similarity between input vector and each document vector. Document structure and proximity of query terms are also used: a document gets an additional score if the query terms appeared in title (&lt;title&gt;) or headings (&lt;hn&gt;) field or if the query terms appeared closely in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Out-degree reranking</head><p>A key resource is expected to have links to many relevant pages. Thus we reranked initial search results based on out-degree as follows:</p><formula xml:id="formula_3" coords="4,82.74,205.72,257.90,52.92">∑ + = ′ n j d d d j i i Sim n Sim m Si 1 α where i d Sim is the initial score of the document i d , j d</formula><p>Sim is the initial score of the document j d that is pointed to by i d and n is the number of outlinks in i d . This technique is used in the run meijihilw5 and α is set to be 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Site aggregation</head><p>Initial search results often give higher rank to pages from the same server. We simply merged them into one that has the shortest URL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Results</head><p>Results are shown in Table <ref type="table" coords="4,205.55,406.07,3.82,9.19" target="#tab_0">1</ref>. Query expansion and reranking failed to improve R-precision and P@10.</p><p>Incorporating the contents of neighboring pages on the other hand showed some improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Novelty Track</head><p>In Novelty Track, our main challenge is conceptual expansion of profiles and sentences. Expanding them using CFS can calculate similarities more correctly than only using word frequency.</p><p>We regarded sentences as very short documents, and converted them to word vectors. In the conversion phase, we removed stop words, stemmed words using Porter's algorithm and assigned weights to them using tf-idf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Conceptual Expansion</head><p>We constructed the network shown in Figure <ref type="figure" coords="4,278.51,633.65,5.11,9.19">2</ref> to implement CFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept vector i C (fragment of the concept description) is created by clustering documents in</head><p>Reuters corpus. The weights between concept layer and output layer are also trained using Reuters corpus.</p><p>An input is expanded as follows:</p><p>1. Calculate similarities between input vector X and each concept vector i C :</p><formula xml:id="formula_4" coords="5,123.60,167.06,93.16,50.37">i i i C X C X S ⋅ = 2.</formula><p>Expanded vector Y is calculated by propagating the similarities:</p><formula xml:id="formula_5" coords="5,165.48,221.91,79.43,27.49">∑ × = i i ij j S a Y ) (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relevant Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Relevancy Detection System Description</head><p>To identify relevant sentences, we used an information-filtering-based approach. Initial profiles, which are made with the topic descriptions, are expanded conceptually. If the cosine similarity between an Figure <ref type="figure" coords="5,243.25,430.01,3.83,9.19">2</ref>. Network structure for CFS Figure <ref type="figure" coords="5,219.36,718.73,3.81,9.19" target="#fig_4">3</ref>. Architecture of relevancy detection system expanded profile and the word vector of a sentence exceeds a threshold, the sentence is regarded as relevant. Figure <ref type="figure" coords="6,152.29,155.15,5.11,9.19" target="#fig_4">3</ref> shows the architecture of our relevancy detection system. The title, description and narrative field were used to adjust profiles. Only the topic profile was expanded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Threshold Learning</head><p>In this system, we must set an appropriate threshold to distinguish Relevant sentences from Non-Relevant ones. The threshold was trained by using the corpus of TREC2002 Novelty Track (min_qrels.relevant and max_qrels.relevant). We adopted the threshold where the F measure was maximized.</p><p>The number of New sentences in a Relevant sentence set decreases inevitably as the recall becomes low. Therefore, the threshold where the recall is 0.7 was also used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">System Variation</head><p>We had three system variations to identify Relevant sentences as shown in Table <ref type="table" coords="6,429.01,365.21,3.81,9.19">2</ref>. The profiles were expanded by CFS in R1 and R2, but were not expanded in R3 to compare accuracy with R1 and R2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">New Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Novelty Detection System Description</head><p>To identify new sentences, we used two measures: sentence score and redundancy score. 1) For calculating sentence score, we used N-window-idf to consider the time window. Local sentence score is calculated by using document frequency for the past N documents. 2) Redundancy score of a sentence is the maximum similarity between the sentence and ones judged to be new in the past. Figure <ref type="figure" coords="6,463.89,601.67,5.11,9.19" target="#fig_3">4</ref> shows the architecture of our novelty detection system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.1.">Sentence Score</head><p>The sentence score is calculated based on sentence weight proposed by Zechner <ref type="bibr" coords="6,430.30,671.68,10.87,9.19" target="#b4">[4]</ref>. We improved it so that it might take novel feature. If news documents are streaming in chronological order, they have the feature that a specific topic concentrates in a small range. Therefore, in order to judge novelty, it is Table 2. Relevancy detection system variation effective not to consider globally, but to consider locally. We used local rarity of a word to use this feature. It is calculable using N-window-idf which is document frequency in past N documents. By using N-window-idf, weights of frequent words decrease and sentence weights represent local information.</p><formula xml:id="formula_6" coords="7,125.52,186.91,242.14,64.52">) ( ) ( ) ( i i i t idf window N t tf s ore SentenceSc - - × = ∑ ) ( log ) ( t df window N N t idf window N - - = - -</formula><p>where tf(t i ) is the frequency of the word t i in the sentence s, N is the window size, and N-window-df(t) is the document frequency of the word t in past N documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.2.">Redundancy Score</head><p>To calculate the redundancy score, we used maximum similarity of sentences which are already identified as novelty. The similarity is calculated by cosine measure. We used the sentence score and the redundancy score to identify the novelty. We thought that novelty sentences must have higher information weight and differ from pre-selected novelty sentences. Therefore, we combined these scores:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Parameter Setting</head><p>To identify new sentences, we had to set up three parameters: 1) window size: N 2) ratio of sentence score to redundancy score: λ 3) threshold for judging whether the input sentence is New or not: θ</p><p>We set the widow size to 200 based on the number of sentences to a news document. λ and θ were determined by learning using Trec2002 Novelty Track data (min_qrels.new, max_qrels.new) as well as Relevancy Detection System. We adopted λ and θ from which F measure becomes the maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">System Variation</head><p>Four variations were prepared (Table <ref type="table" coords="8,246.82,365.21,3.69,9.19">3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Result and Discussion</head><p>We submitted for Task 1-4. Table <ref type="table" coords="8,232.14,645.29,4.56,9.19">4</ref>-5 shows the results. In the Relevancy Detection phase, the validity of expansion by CFS has been shown. Moreover, we presented the validity of N-window-idf, which considered locality, in the Novelty phase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,253.20,666.29,96.74,9.19;1,152.76,511.44,298.18,141.48"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Image of CFS</figDesc><graphic coords="1,152.76,511.44,298.18,141.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,456.36,468.65,3.90,10.53;7,447.06,468.65,3.90,10.53;7,348.78,468.65,3.90,10.53;7,325.80,468.65,5.85,10.53;7,322.86,468.65,3.90,10.53;7,308.10,468.65,3.90,10.53;7,298.80,468.65,3.90,10.53;7,197.40,468.65,3.90,10.53;7,188.10,468.65,3.90,10.53;7,451.44,468.65,4.55,10.53;7,420.42,468.65,26.64,10.53;7,363.00,468.65,57.83,10.53;7,303.18,468.65,4.55,10.53;7,283.08,468.65,15.60,10.53;7,230.94,468.65,52.63,10.53;7,192.48,468.65,4.55,10.53;7,178.26,468.65,9.75,10.53;7,125.94,468.65,52.64,10.53;7,354.24,464.88,6.41,14.32;7,332.88,464.88,6.41,14.32;7,314.28,464.88,6.41,14.32;7,222.36,464.88,6.41,14.32;7,204.30,464.88,6.41,14.32;7,341.46,464.21,6.41,15.14;7,213.60,464.21,6.41,15.14;7,92.46,487.79,312.16,9.19"><head></head><label></label><figDesc>If the NoveltyScore exceeds a threshold, the sentence is regarded as novelty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,194.70,710.03,205.11,9.19;7,155.04,513.18,284.28,190.50"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Architecture of novelty detection system</figDesc><graphic coords="7,155.04,513.18,284.28,190.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,208.74,400.24,177.97,9.19;8,92.94,522.77,419.52,9.19;8,82.74,540.28,429.85,9.19;8,82.74,557.81,429.89,9.19;8,82.74,575.27,429.81,9.19;8,82.74,592.78,21.25,9.19"><head>Table 3 .</head><label>3</label><figDesc>Novelty detection system variation In N-window-idf column, [O] means N-Window-idf is used to calculate sentence scores and [X] means basic idf is used instead of N-Window-idf. The df values of basic idf were calculated using about 810,000 news documents in Reuters corpus. In Expand column, [O] means CFS expansion is used to calculate redundancy scores, [X] means expansion is not used and [-] means redundancy scores are not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,128.22,164.52,338.55,152.46"><head></head><label></label><figDesc></figDesc><graphic coords="9,128.22,164.52,338.55,152.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,112.38,438.76,378.26,117.13"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results of submitted runs</figDesc><table coords="2,112.38,456.71,378.26,99.19"><row><cell>Run</cell><cell>Query expansion</cell><cell>Inlinks &amp; Outlinks</cell><cell>Reranking</cell><cell>R-Prec</cell><cell>P@10</cell></row><row><cell>meijihil1</cell><cell></cell><cell></cell><cell></cell><cell>0.0918</cell><cell>0.0920</cell></row><row><cell>meijihil2</cell><cell>O</cell><cell></cell><cell></cell><cell>0.0614</cell><cell>0.0700</cell></row><row><cell>meijihil3</cell><cell></cell><cell>O</cell><cell></cell><cell>0.0902</cell><cell>0.1060</cell></row><row><cell>meijihil4</cell><cell>O</cell><cell>O</cell><cell></cell><cell>0.0687</cell><cell>0.0700</cell></row><row><cell>meijihil5</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>0.0523</cell><cell>0.0620</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.86,521.95,78.78,12.24" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.58,537.29,412.08,9.19;9,82.74,549.05,429.82,9.19;9,82.74,560.81,74.83,9.19" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,345.65,537.29,167.00,9.19;9,82.74,549.05,199.21,9.19">Conceptual Fuzzy Sets as a Meaning Representation and their Inductive Construction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Imura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ushida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,294.30,549.05,180.83,9.19">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="929" to="945" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,97.20,572.57,415.45,9.19;9,82.74,584.27,337.64,9.19" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,312.19,572.57,200.47,9.19;9,82.74,584.27,43.56,9.19">Multilayered Reasoning by Means of Conceptual Fuzzy Sets</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Imura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ushida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,136.92,584.27,176.33,9.19">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="97" to="111" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,100.02,596.03,412.58,9.19;9,82.74,607.79,356.38,9.19" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,160.13,596.03,233.17,9.19">A Method of Cluster-Based Indexing of Textual Data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,407.34,596.03,105.26,9.19;9,82.74,607.79,294.99,9.19">Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING 2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,99.97,619.55,412.73,9.19;9,82.74,631.25,429.81,9.19;9,82.74,643.01,124.19,9.19;9,228.24,149.93,148.05,9.19" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,160.59,619.55,352.11,9.19;9,82.74,631.25,79.36,9.19">Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,174.66,631.25,337.89,9.19;9,82.74,643.01,41.52,9.19">Proceedings of the 16th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 16th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,223.20,342.47,148.04,9.19" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,258.51,342.47,112.72,9.19">Result of Task 2 and Task 4</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
