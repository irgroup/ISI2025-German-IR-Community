<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,196.07,132.97,219.11,15.49;1,138.41,154.89,334.42,15.49">The University of Amsterdam at the TREC 2003 Question Answering Track</title>
				<funder ref="#_DBEzaf3 #_KtXgAMS #_uVbnhWD #_t9EGtMc #_aRPSBT2">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_6FgWggC">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_g5Et3nw #_wBSvYAb #_BRfUnSn #_EP5hTGf">
					<orgName type="full">NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.23,187.37,84.75,10.76"><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
						</author>
						<author>
							<persName coords="1,275.93,187.37,69.42,10.76"><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,357.31,187.37,74.71,10.76;1,432.02,185.07,1.49,7.86"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<email>christof@umiacs.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Now at the Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<addrLine>3161 A.V. Williams Building</addrLine>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.61,202.03,90.19,10.76"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<author>
							<persName coords="1,279.75,202.03,88.01,10.76"><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
						</author>
						<author>
							<persName coords="1,379.72,202.03,53.92,10.76;1,433.64,199.73,2.24,8.07"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Now at Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Language &amp; Inference Technology Group</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,196.07,132.97,219.11,15.49;1,138.41,154.89,334.42,15.49">The University of Amsterdam at the TREC 2003 Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2BCE750ADA4557E543C467C6340138F0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the TREC 2003 Question Answering track. We explain the ideas underlying our approaches to the task, report on our results, provide an error analysis, and give a summary of our findings so far.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim for our participation in the Question Answering track at TREC 2003 was to experiment with a new multistream architecture, in which we implemented 6 separate subsystems that each try to answer questions in different ways. We also wanted to experiment with a dedicated biography question module that is currently in development.</p><p>Our experiments exploited the home-grown FlexIR document retrieval system <ref type="bibr" coords="1,169.92,475.94,10.58,8.97" target="#b8">[9]</ref>. The main goal underlying FlexIR's design is to facilitate flexible experimentation with a wide variety of retrieval components and techniques; we used FlexIR's implementations of the Lnu.ltc weighting scheme, various language models, as well as the Okapi scheme.</p><p>Current Question Answering (QA) systems, as reflected by the TREC QA track participants, can be divided into two categories: knowledge-intensive systems, that make use of various linguistic tools for the question answering process, and redundancy-based systems, that rely on very high volumes of data (in many cases, the Web) cs.biu.ac.il. and take a more shallow approach to text analysis. Until last year, we were focused on the first approach, concentrating our QA efforts exclusively on Tequesta <ref type="bibr" coords="1,504.02,308.20,15.77,8.97" target="#b9">[10,</ref><ref type="bibr" coords="1,523.48,308.20,11.83,8.97" target="#b10">11]</ref>. This approach may be successful for some types of questions, but for others more shallow approaches seem more beneficial. This year we expanded our QA work and implemented a multi-stream approach. While maintaining Tequesta as one of the approaches, we developed additional systems that compete which each other to find the correct answer. These systems, or "streams," employ a range of redundancy-based and knowledge-intensive techniques. We took part in the main QA task and in the passage QA task. For our participation in the main task we employed our new multi-stream architecture; for the passage task we relied on the Tequesta stream only.</p><p>The rest of this paper is organized as follows. In two (largely self-contained) sections we describe our work for the main task and the passage task. Finally, we summarize our findings in a concluding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Main Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Description</head><p>We now describe the approach we adopted for the main QA task; we devote separate subsections to factoid questions on the one hand, and list questions and definition questions on the other. The system consists of 6 separate QA streams and a final answer selection module that combines the results of all streams and produces the final answers. An important benefit of this architecture is easy modification, maintenance, and testing of the dif-ferent subsystems as well as easy integration of multiple sources of information. Evaluation of the contribution of each stream to the entire QA process becomes a relatively simple task too. We now describe the streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table Lookup</head><p>. This stream uses specialized knowledge bases constructed by preprocessing the collection, similar in spirit to <ref type="bibr" coords="2,118.09,208.07,10.58,8.97" target="#b3">[4]</ref>. The stream exploits the fact that certain types of information (such as country capitals, abbreviations, and names of political leaders) tend to occur in a small number of fixed patterns. When a question type indicates that the question might potentially have an answer in these tables, a lookup is performed in the appropriate table and answers found are assigned high confidence.</p><p>We hand-crafted a small number of regular expressions for extracting information about the categories listed in Table <ref type="table" coords="2,109.33,316.06,3.74,8.97" target="#tab_0">1</ref>. For instance, the "Location" category concerns geographic information of the following type "Amu Darya, river, Turkmenistan, XIE19990811.0277," where the first field indicates a location, the second its type, the third a country or region in which it is located, and the fourth the identifier for the document from which it was extracted. "Geography" contains similar information, but without the type; "Leaders" has information of the following kind "Dutch, Foreign Minister, Jozias van Aartsen, XIE19991027.0270", and "Roles" generalizes this to also include other roles besides government-related ones. When a question is classified as possibly having an answer in a table, we first identify the question keywords that will be used in the table search. Next, a line matching all of the words in the order they appeared in the question is searched; if no line matches, we look again for a line containing all words, this time in any word order. If there is still no match, we start removing words from the list of words to match; the order of removal is based on the fre-quency of words in the language (i.e., common words are removed first) and part-of-speech tags (e.g., superlatives like fastest, largest are removed last). We do this until some threshold is reached (percentage of lookup words out of total keywords in the question). When a matching line is found, we return the text in the column that is declared to contain the information required as the answer.</p><p>Pattern Matching. This stream exploits the fact that in some cases, the contextual format of an answer to a question can be back-generated from the question itself. For example, an answer to a question such as 2257. What is the richest country in the world? will possibly match the pattern &lt;Capitalized-Words&gt;(,| is) the richest country in the world. In these cases, the position of the answer within the context is also known when generating the context pattern; in the given example, it would be the capitalized word or words (and indeed, in document XIE19980302.0146, this pattern matches against ". . . Although the United States is the richest country in the world, 20 percent of its population . . . ").</p><p>The Pattern Matching stream consists of three stages: Generation, Document Prefetch and Matching. In the Generation stage, the question is analyzed and possible answer patterns are generated. For questions like 2347. Where is Mount Olympus? the question type and focus (both provided by the question classifier) are sufficient for generating a number of answer patterns. For other questions (e.g., 2375. What date did Thomas Jefferson die?) we also use a set of manually created rules based on part-of-speech tags of the question words and a dictionary of word forms, in order to rewrite the question into declarative forms (e.g., Thomas Jefferson (died|dies) (on|in) &lt;answer&gt;). In the Prefetch stage, for each generated pattern a query containing words from it is formed, and documents are retrieved from the collection using the query. In the final stage, the patterns are matched against the retrieved documents, and answers are extracted from the matches.</p><p>Two variations of this stream were implemented, Web Pattern Matching and Collection Pattern Matching. For the first variation the text collection was the Web, and for the second, the local AQUAINT corpus. For the prefetch stage we used the top-ranking documents from Google (for the Web variation) and all matching documents retrieved using a boolean query to our document retrieval engine FlexIR against the AQUAINT corpus.</p><p>Ngram Mining. This stream, similar in spirit to <ref type="bibr" coords="3,276.06,127.96,10.79,8.97" target="#b1">[2,</ref><ref type="bibr" coords="3,289.85,127.96,7.19,8.97" target="#b2">3]</ref>, constructs a weighted list of queries for each question using a shallow reformulation process, similar to the Pattern Match stream. The queries are then sent to a large document collection; we implemented two variations for this stream, Web Ngram Mining and Collection Ngram Mining, using the Web and the local AQUAINT corpus, respectively. For Web searches, we used Google, and for local searches FlexIR, with the Lnu.ltc weighting scheme. Then, we looked at word ngrams in the relevant retrieved document paragraphs (for the Web we used the snippets provided by Google, and for the collection we used a window of 200 bytes around the query). The ngrams were ranked according to the weight of the query that generated them, their frequency in the paragraphs, their NE type, the proximity to the query keywords and more parameters, and the top-ranking ngrams were considered answer candidates. To find justification for the answer in the local corpus, we constructed a query with keywords from the question and the answer, and considered the top-ranking document for this query to be the justification, this time using an Okapi model as this tends to do well on early high precision in our experience.</p><p>Tequesta. As mentioned before, this is a stream that implements a linguistically informed approach to QA. We defer a discussion of this stream to Section 3 where we describe our strategy for the passage task.</p><p>Many components are shared by all streams, including a locally developed named entity tagger and the following: Question Classifier. An incoming question is first analyzed for its type (e.g., date-of-birth), expected answer type (e.g., location) and focus (the core of the question, used e.g., for answer pattern generation). Currently our system recognizes 37 question types. The question analysis is based on surface and part-of-speech patterns. We also use hierarchical relations in WordNet to identify semantic classes of question focus words (e.g., this allows us to assign the type person-ident to the question 1943. What is the name of Ling Ling's mate?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web</head><p>Ranking. The answer candidates produced by the streams have different confidence levels, generated by stream-specific parameters and measuring methods. To compare these levels, a uniform way of ranking the candi-dates was required. To this end we implemented a search engine hit count module, similar to <ref type="bibr" coords="3,452.57,139.92,10.58,8.97" target="#b5">[6]</ref>.</p><p>Answer Selection. Each of our streams produces a pool of answer candidates, with normalized confidence scores. After filtering the candidates to remove obvious noise, we create a joint pool of answers, adjusting each candidate's score by a factor that reflects the past performance of its stream on questions of the same type. We tried different ways of assigning these stream/question-type weights: manually (i.e., based on human intuition about how good different streams perform on different questions) and automatic (using Machine Learning to find weights that optimize the performance of the system on a training set of questions) <ref type="bibr" coords="3,354.41,292.67,10.58,8.97" target="#b4">[5]</ref>. In the joint pool of answer candidates we identify identical or similar (small edit distance) answers, merge and add their confidence scores. Finally, a candidate with the highest score is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List and Definition Questions</head><p>Because of time constraints, we were unable to implement a proper module for handling list questions. All list questions were automatically rewritten into factoids using rule-based transformations (e.g., 2097. Which countries were visited by first lady Hillary Clinton? was transformed to Which country was visited by first lady Hillary Clinton?) and fed to our multi-stream QA system. The top N candidate answers to this factoid question were submitted as answers to the original list question. We experimented with different values of N (10 and 20 in our official runs) and with different numbers of retrieved documents used during answer selection (both for collectionand web-based QA streams).</p><p>In contrast to list questions, we did invest a serious effort in developing a component for handling definition questions. More precisely, we piggybacked on ongoing inhouse activities aimed at developing a QA system for handling "biography oriented" definitions on the web <ref type="bibr" coords="3,330.80,593.65,15.27,8.97" target="#b12">[13]</ref>. The main steps in our handling of definition questions are Question Analysis (very similar to the analysis carried out for factoids), Answer Retrieval (always from external resources), Answer Filtering, and Answer Justification (very similar to the justification performed for externally found answers to factoid questions).</p><p>For concept definition questions we followed a WordNet-based strategy as discussed in the literature <ref type="bibr" coords="4,281.55,127.96,15.27,8.97" target="#b11">[12]</ref>. Given a question that asks for a definition of a concept, we simply consult WordNet. As our primary strategy for handling person definition questions, we also consulted an external resource. The main resource used is biography.com. However, in many cases no biography could be found in this resource. In such cases we backed off to using Google, with queries obtained by combining the name of the person in question with varying subsets of a predefined set of hand-crafted features (including "born", "graduated", "suffered", etc.) For questions asking for definitions of organizations the latter was the strategy used (with a set of "organization features"). As a final fallback option for each type of definition question, if the use of the strategies mentioned earlier returned no satisfactory results, we simply submitted &lt;question term&gt; is a to Google and mined the snippets returned. This method worked surprisingly well for questions like 2385. What is the Kama Sutra?.</p><p>Given a set of candidate answer snippets, we performed two more steps before carrying out the final answer justification step: we separated junk snippets from valuable snippets and we identified snippets whose content is very similar. We addressed the first step by analyzing the distances between query terms submitted to the search engine and the sets of features, and by means of shallow syntactic aspects of the different features such as sentence boundaries. To address the second step we developed a snippet similarity metric based on edit distance, stemming, stopword removal, and keyword overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Runs</head><p>We submitted 3 runs. These runs used the exact same strategies and settings for definition questions. They did differ in their settings for factoids and list questions.</p><p>Here's a brief description: UAmsT03M1 For factoids, the answer selection module used automatically learned stream/question weights; answers coming only from external sources (streams based on Web) were justified against the AQUAINT collection using the Okapi model. For each list question the top 10 answers to its factoid counterpart were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UAmsT03M2</head><p>For factoids, the weights for answer selection were learned automatically; external answers were discarded. For list questions the number of collection and web documents used for answer mining was increased, and the top 20 answers were submitted for each question.</p><p>UAmsT03M3 Manually assigned weights were used for answer selection; external answers were discarded.</p><p>The number of documents for answering list questions was as in UAmsT03M2, but only top 10 answers were submitted.</p><p>Our three runs allowed us to compare the impact of justification, and the impact of using manually assigned versus learned weights for our answer selection. For the list questions we wanted to evaluate the effect of using more data and of giving more answers on the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>Table <ref type="table" coords="4,334.63,363.53,4.98,8.97" target="#tab_1">2</ref> gives the detailed results of our system for the 413 factoid questions: accuracy and the number of correct (R), unsupported (U), inexact (X) and wrong (W) answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Error Analysis</head><p>Analyzing errors made by a QA system is a complex task. In <ref type="bibr" coords="4,322.63,522.48,10.58,8.97" target="#b6">[7]</ref>, such an analysis is based on examination of the outputs of every module in the process separately, and attributing the error to the first malfunctioning module. In many cases an error in one of the earlier stages of the QA pipeline (for example, the question classification module) does indeed cause cascaded errors later. But when diagnosing a system with multiple independent approaches such as ours, this does not necessarily hold; we found incorrectly classified questions which were still answered correctly due to the redundancy-based modules, and many other counter-examples to the "cascaded errors" assumption. Therefore, we chose to examine the incorrect answers produced by the system, and associate each of them with a main error type, the dominant reason for producing this incorrect answer. Table <ref type="table" coords="5,109.94,151.87,4.98,8.97" target="#tab_2">3</ref> shows the most common error types for run UAmsT03M1; for each incorrectly answered question (counting inexact or unsupported answers as incorrect), we examined the candidate answer list produced by our system (the list contains less than 10 candidates on average). If the correct answer was in this list, we classified the error types of the candidates that received a higher rank than it; otherwise, we classified the top 3 ranking candidates. Since we examined more than one answer per question, there may be multiple error types for a specific question. A brief explanation of main error types follows:</p><p>• Answer Selection errors describe an incorrect answer with the correct named entity or concept type which typically appears in relevant documents. An example is the answer George Bush for 2391. What president created social security? -the answer type is correct, and is very frequent in relevant documents (both in the local collection and on the web).</p><p>• Named-Entity errors result from an incorrect classification of a phrase as a named entity which matches the expected answer type. An example is the answer Springsteen for 2001. What rock band sang "A Whole Lotta Love"?.</p><p>• Question Classification errors are cascaded errors originating from an incorrect question type assigned to the question at an early stage of the QA pipeline, or failure to assign any question type to it.</p><p>• Justification errors are correct answers which were obtained using external resources, and were not projected correctly to the local corpus.</p><p>• Unit Errors are answers of the correct named entity type, but incorrect granularity (i.e. state instead of city) or out of range (according to world-knowledge). An example is the answer about 2 billion dollar for 2302. How much did the first Barbie cost? (referring to profits rather than costs).</p><p>While our analysis revealed many technical issues that need to be addressed -such as over-tiling of ngrams, resulting in inexact answers (Colombia country South America instead of Colombia) -most of the errors stem from the shallow answer-selection techniques used by our system. We currently use mostly frequency counts and proximity measures to select the answer candidates; this works for questions which have a large amount of relevant documents, but for other questions deeper analysis is required. An alternative approach, still relying on redundancy methods, is to expand the number of retrieved documents using query expansion methods (both for the local corpus and the web) -an approach which is also almost not used in our system. Our main conclusion is that while we continue to see redundancy-based methods as our basic strategy for QA, shallow NLP and reasoning methods should be selectively used throughout the process, especially when the number of retrieved documents is low.</p><p>A few more remarks are worth making. First, although the run with the automatically learned weights for answer selection from multiple streams (UAmsT03M2) outperformed the run with manually assigned weights (UAmsT03M3), our subsequent experiments revealed that whereas a small difference exists, it is not statistically significant. However, both runs improve significantly over a baseline system with equal weights to all streams.</p><p>We also evaluated the contribution of different streams to the performance of the system on the factoids (using unofficial answer patterns). Table <ref type="table" coords="5,448.62,522.48,4.98,8.97" target="#tab_4">4</ref> gives the results (the number of "correct" answers, i.e., those that match the patterns) for the whole system, for separate streams and for the system with one of the streams turned off. As expected, each of the six streams answered some questions correctly and more interestingly, each stream contributed to the overall performance of the system. The two "worst" performing streams (predictably, collection-based pattern matching and ngram mining) brought one more answer each either at the top rank or in the top 5. Surprisingly, the "winner" among the streams is equivocal: while  answer candidates in the top 5. Table <ref type="table" coords="6,106.91,326.66,4.98,8.97" target="#tab_5">5</ref> gives the combined results for the 3 QA tasks (accuracy for factoids, F score for list and definition questions) and the final scores of our runs. The results for the list questions suggest that using more retrieved documents for answer extraction and submitting more answer candidates hurts performance: the increase in recall does not compensate for the drop in precision.</p><p>Turning to definition questions now, recall that there is no difference between the three runs listed in Table <ref type="table" coords="6,295.66,501.36,4.98,8.97" target="#tab_5">5</ref> as far as definition questions are concerned, despite the different scores in the table. The differences are due to inconsistencies in the judgments provided by NIST. Table <ref type="table" coords="6,295.66,537.23,4.98,8.97" target="#tab_6">6</ref> provides a breakdown of the scores for the different types of definition questions; the highest scores are obtained for person definitions, which reflects the fact that those are the type of definition questions in which we put most work. As an aside, in our submission we found no answer for 19 of the 50 definition questions. If we compute the F score not over all 50 question but only over questions with a positive F score, we obtain an average of 0.527. In post-submission experiments we changed the subsets of features we use in the queries sent to Google as well as the number of queries/subsets we use. The snippetssimilarity threshold was also tuned in order to filter more snippets. This resulted in a reduction of unanswered definition questions to 6 instead of 19. Using our own (unofficial) assessment, this yielded an F score of 0.688. Those changes also reflected in the average answer length. After the parameters tuning the average length was half of the average TREC submission answer, improving precision and contributing to the F score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Conclusions for the Main Task</head><p>Our general conclusion on answering factoid questions is that our new multi-stream approach helped answer considerably more questions than our "old" single-stream Tequesta system. This year's questions seem much harder than those of previous years. A preliminary error analysis shows that retrieval, named entity recognition, and answer selection all require further attention. Our main conclusion on answering definition questions is that external dictionary-like resources are crucial, but a feature-based approach offers an effective strategy if such resources are absent or too sparse. Following an analysis of our TREC results, we investigated the use of trainable text classifiers as a pre-processing stage instead of the features vectors, treating the web as a 'noisy' external knowledge source, and using the text classifier to filter out the noise. Initial results show that using text classifiers greatly improves the F score and the coherence of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Passage Task</head><p>The aim of the passage task was to return an excerpt from a document rather than an exact answer. Excerpts had to be unmodified snippets from a document in the AQUAINT collection, and were not allowed to be longer than 250 characters. For the passage task only the factoid questions from the main task were used, i.e., list and definition questions were not included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Description</head><p>For the passage task, we used a modification of the Tequesta question answering system, which has remained largely unchanged since TREC 2002 <ref type="bibr" coords="7,236.93,170.55,15.77,8.97" target="#b9">[10,</ref><ref type="bibr" coords="7,259.17,170.55,11.83,8.97" target="#b10">11]</ref>. We dropped the exact answer output feature, and included some of the context surrounding the answer identified by Tequesta. We added the use of minimal span weighting for identifying documents that are likely to contain an answer to a given question. We used minimal matching spans as the snippets in which to find the exact answer. Minimal span weighting takes the positions of matching terms into account, but does so in a more flexible way than passage-based retrieval; see <ref type="bibr" coords="7,254.58,278.15,11.62,8.97" target="#b7">[8]</ref> for details. Intuitively, a minimal matching span is the smallest text excerpt from a document that contains all terms which occur in the query and the document. More formally, given a query q and a document d, the function term at pos d (p) returns the term occurring at position p in d. A matching span (ms) is a set of positions that contains at least one position of each matching term, i.e. Minimal span weighting depends on three factors.</p><p>1. document similarity: The document similarity is computed using the Lnu.ltc weighting scheme Buckley et al. <ref type="bibr" coords="7,133.67,510.58,11.62,8.97" target="#b0">[1]</ref> for the whole document. Similarity scores are normalized with respect to the maximal similarity score for a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">span size ratio:</head><p>The span size ratio is the number of unique matching terms in the span over the total number of tokens in the span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">matching term ratio:</head><p>The matching term ratio is the number of unique matching terms over the number of unique terms in the query, after stop word removal.</p><p>The msw score is the sum of two weighted components: the normalized original retrieval status value (RSV), which measures global similarity and the spanning factor which measures local similarity. Given a query q, the original retrieval status values are normalized with respect to the highest retrieval status value for that query:</p><p>RSV n (q, d) = RSV(q, d) max d RSV(q, d) .</p><p>The spanning factor is the product of two components: the span size ratio, which is weighted by α, and the matching term ratio, which is weighted by β. Global and local similarity are weighted by λ. The optimal values of the three parameters λ, α, and β were found to be λ = 0.4, α = 1/8, and β = 1 by empirical means. Parameter estimation was done using the TREC-9 data collection only, but it proved to be the best parameter setting for all collections. The final retrieval status value (RSV') based on minimal span weighting is defined as follows, where | • | is the number of elements in a set: If |q ∩ d| &gt; 1 (that is, if the document and the query have more than one term in common), then</p><formula xml:id="formula_0" coords="7,310.61,349.56,229.22,42.62">RSV'(q, d) = λ • RSV n (q, d) + (1 -λ) • |q ∩ d| 1 + max(mms) -min(mms) α • |q ∩ d| |q| β .</formula><p>If |q ∩ d| = 1 then RSV'(q, d) = RSV n (q, d).</p><p>Given a minimal matching span, the document analysis component of Tequesta tries to identify a phrase which is of the appropriate type. All phrases that are of the appropriate type are considered candidate answers. Tequesta selects answers by considering the frequency of a candidate answer and relying on linking a candidate answer to the question by proximity. Hence, all candidate answers are weighted equally. But there is one exception. If the question is of type what-np, candidate answers that are in a WordNet hypernym relationship with the question focus receive a higher weight than candidate answers that are identified by means of the fallback strategy.</p><p>Once an answer has been selected, the corresponding minimal matching span from which the answer has been extracted is returned as the answer passage, trimmed down to 250 characters if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Runs, Results and Conclusion for the Passage Task</head><p>We submitted one run to the passage task, run id UAmsT03P1. The results are shown in Table <ref type="table" coords="7,487.37,665.94,3.74,8.97" target="#tab_7">7</ref>. (R) stands for passages that contained a correct and exact answer, (U) for passages that contained the correct answer, but were not supported by the corresponding document, and (W) stands for wrong answers. The passage track does not make a distinction between exact and inexact (X) answers, as in the main task. Here, an inexact answer is simply judged wrong (W). The results were quite disappointing. At this point we are not sure what caused this rather bad performance. Before submitting this year's run to the passage track, we conducted some experiments on the question sets from previous TRECs, and these results were substantially better. Therefore, one explanation could be that this year's question set was much harder than the previous ones, but a more detailed error analysis remains to be done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have described our participation in the TREC 2003 Question Answering Track. This year, our work was largely motivated by our move to a new, multi-stream architecture. Although a further and more detailed analysis of the performance of the system remains to be done, our preliminary results show that different approaches to the QA process do produce answers to different question types. Our combined use of external resources and handcrafted feature sets proved to be a successful approach for answering definition questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,79.67,377.92,17.20,6.63;7,98.48,373.79,46.47,8.97;7,144.94,378.08,3.69,6.63;7,149.51,373.79,48.00,8.97;7,81.96,385.74,218.68,9.99;7,72.00,397.70,228.64,8.97;7,83.48,409.65,217.16,9.99;7,72.00,421.61,228.64,9.99;7,72.00,433.56,228.64,8.97;7,72.00,445.52,228.64,9.99;7,72.00,457.47,220.92,9.99"><head></head><label></label><figDesc>p∈ms term at pos d (p) = q ∩ d. Then, given a matching span ms, let b d (the beginning of the excerpt) be the minimal value in ms, i.e., = min(ms), and e d (the end of the excerpt) be the maximal value in ms, i.e., e d = max(ms). A matching span ms is a minimal matching span (mms) if there is no other matching span ms with b d = min(ms ), e d = max(ms ), such that b d = b d or e d = e d , and b d ≤ b d ≤ e d ≤ e d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,78.38,459.60,215.89,107.17"><head>Table 1 :</head><label>1</label><figDesc>Facts extracted from the AQUAINT corpus.</figDesc><table coords="2,78.38,470.63,215.89,96.14"><row><cell>Category</cell><cell># Facts Category</cell><cell># Facts</cell></row><row><cell>Abbreviations</cell><cell>31737 Birthdates</cell><cell>9156</cell></row><row><cell>Capitals</cell><cell>1273 Currencies</cell><cell>231</cell></row><row><cell>Dates</cell><cell>9331 Deathdates</cell><cell>1510</cell></row><row><cell>Geography</cell><cell>70363 Height</cell><cell>15603</cell></row><row><cell>Inhabitants</cell><cell>2025 Languages</cell><cell>853</cell></row><row><cell>Leaders</cell><cell>18073 Locations</cell><cell>1348</cell></row><row><cell>Manners of death</cell><cell>857 Organizations</cell><cell>98758</cell></row><row><cell>Roles</cell><cell>396558</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,316.98,409.64,215.89,52.37"><head>Table 2 :</head><label>2</label><figDesc>Results for the QA track (factoid questions).</figDesc><table coords="4,316.98,420.67,215.89,41.34"><row><cell>Run identifier</cell><cell cols="2">Accuracy R</cell><cell>U</cell><cell>X</cell><cell>W</cell></row><row><cell>UAmsT03M1</cell><cell>0.136</cell><cell cols="4">56 22 32 303</cell></row><row><cell>UAmsT03M2</cell><cell>0.145</cell><cell cols="4">60 20 26 307</cell></row><row><cell>UAmsT03M3</cell><cell>0.128</cell><cell cols="4">53 24 30 306</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,78.38,293.52,215.89,74.62"><head>Table 3 :</head><label>3</label><figDesc>Frequent Error Types in UAmsT03M1.</figDesc><table coords="5,78.38,304.88,215.89,63.26"><row><cell>Error Type</cell><cell>Frequency</cell></row><row><cell>Answer Selection</cell><cell>134 (38%)</cell></row><row><cell>Named-Entity</cell><cell>78 (22%)</cell></row><row><cell>Question Classification</cell><cell>67 (19%)</cell></row><row><cell>Justification</cell><cell>58 (16%)</cell></row><row><cell>Unit Error</cell><cell>53 (15%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,310.61,642.03,228.64,32.88"><head></head><label></label><figDesc>Table Lookup allows the system to answer 15 questions more, Web Ngrams accounts for more (35 vs. 19) unique correct</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,78.38,127.08,215.89,164.35"><head>Table 4 :</head><label>4</label><figDesc>Contribution of different streams.</figDesc><table coords="6,78.38,138.11,215.89,153.32"><row><cell>Configuration</cell><cell cols="2"># correct # correct in top 5</cell></row><row><cell>All streams</cell><cell>98</cell><cell>165</cell></row><row><cell>Collection ngrams</cell><cell>39</cell><cell>42</cell></row><row><cell>Without collection ngrams</cell><cell>98</cell><cell>164</cell></row><row><cell>Web ngrams</cell><cell>65</cell><cell>115</cell></row><row><cell>Without Web ngrams</cell><cell>89</cell><cell>130</cell></row><row><cell>Collection patterns</cell><cell>39</cell><cell>39</cell></row><row><cell>Without collection patterns</cell><cell>97</cell><cell>165</cell></row><row><cell>Web patterns</cell><cell>51</cell><cell>59</cell></row><row><cell>Without Web patterns</cell><cell>94</cell><cell>163</cell></row><row><cell>Table lookup</cell><cell>71</cell><cell>77</cell></row><row><cell>Without table lookup</cell><cell>83</cell><cell>146</cell></row><row><cell>Tequesta</cell><cell>63</cell><cell>102</cell></row><row><cell>Without Tequesta</cell><cell>91</cell><cell>140</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,78.38,373.68,215.89,52.37"><head>Table 5 :</head><label>5</label><figDesc>Results for the QA track.</figDesc><table coords="6,78.38,384.71,215.89,41.34"><row><cell>Run identifier</cell><cell cols="4">A (Fact) F (List) F (Def) Overall</cell></row><row><cell>UAmsT03M1</cell><cell>0.136</cell><cell>0.054</cell><cell>0.315</cell><cell>0.160</cell></row><row><cell>UAmsT03M2</cell><cell>0.145</cell><cell>0.042</cell><cell>0.308</cell><cell>0.160</cell></row><row><cell>UAmsT03M3</cell><cell>0.128</cell><cell>0.035</cell><cell>0.292</cell><cell>0.146</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,78.38,619.74,215.89,30.46"><head>Table 6 :</head><label>6</label><figDesc>Breakdown of F scores for definition questions.</figDesc><table coords="6,78.38,630.77,215.89,19.43"><row><cell>Run identifier</cell><cell cols="2">Concept Person</cell><cell>Org.</cell><cell>Overall</cell></row><row><cell>UAmsT03M1</cell><cell>0.150</cell><cell>0.392</cell><cell>0.268</cell><cell>0.315</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,78.38,187.78,215.89,30.46"><head>Table 7 :</head><label>7</label><figDesc>Results for the QA passage track</figDesc><table coords="8,78.38,198.80,215.89,19.43"><row><cell>Run identifier</cell><cell cols="2">Accuracy R U</cell><cell>W</cell></row><row><cell>UAmsT03P1</cell><cell>0.111</cell><cell cols="2">46 6 361</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thank you to <rs type="person">Börkur Sigurbjörnsson</rs> for useful suggestions and discussion. We thank <rs type="person">Raffaella Bernardi</rs> for help on the question classifier and table extraction components, and <rs type="person">Karin Müller</rs> and <rs type="person">Detlef Prescher</rs> for help with our named entity recognizer. Many thanks to <rs type="person">Henry Chinaski</rs> for sanity checks and inspiration. <rs type="person">Valentin Jijkoun</rs>, <rs type="person">Gilad Mishne</rs>, and <rs type="person">Stefan Schlobach</rs> were supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project number <rs type="grantNumber">220-80-001</rs>. <rs type="person">Christof Monz</rs> was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">612-13-001</rs> and <rs type="grantNumber">220-80-001</rs>. <rs type="person">Maarten de Rijke</rs> was supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">612.000.207</rs>, and <rs type="grantNumber">612.066.302</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6FgWggC">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_g5Et3nw">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_wBSvYAb">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_BRfUnSn">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_EP5hTGf">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_DBEzaf3">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_KtXgAMS">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_uVbnhWD">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_t9EGtMc">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_aRPSBT2">
					<idno type="grant-number">612.066.302</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,330.52,193.54,208.72,8.07;8,330.52,204.50,208.73,8.07;8,330.52,215.46,128.78,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,476.04,193.54,63.20,8.07;8,330.52,204.50,119.57,8.07">New retrieval approaches using SMART: TREC 4</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,466.59,204.50,72.66,8.07;8,330.52,215.46,102.19,8.07">The Fourth Text REtrieval Conference (TREC-4)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,226.42,208.72,8.07;8,330.52,237.37,208.72,8.07;8,330.52,248.33,208.73,8.07;8,330.52,259.29,101.11,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,475.43,226.42,63.81,8.07;8,330.52,237.37,101.09,8.07">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,459.93,248.33,79.32,8.07;8,330.52,259.29,16.14,8.07">Proceedings of SIGIR 2001</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>SIGIR 2001</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,270.25,208.72,8.07;8,330.52,281.21,208.72,8.07;8,330.52,292.17,208.72,8.07;8,330.52,303.13,101.11,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,523.03,270.25,16.21,8.07;8,330.52,281.21,156.02,8.07">Web question answering: Is more always better?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,460.35,292.17,78.89,8.07;8,330.52,303.13,16.14,8.07">Proceedings of SIGIR 2002</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bennett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</editor>
		<meeting>SIGIR 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,314.09,208.72,8.07;8,330.52,325.05,208.72,8.07;8,330.52,336.00,205.53,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,514.07,314.09,25.18,8.07;8,330.52,325.05,208.72,8.07;8,330.52,336.00,94.36,8.07">Offline strategies for online question answering: Answering questions before they are asked</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fleischman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,441.59,336.00,90.43,8.07">Proceedings of ACL 2003</title>
		<meeting>ACL 2003</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,346.96,208.72,8.07;8,330.52,357.92,208.72,8.07;8,330.52,368.88,99.36,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,435.80,346.96,103.44,8.07;8,330.52,357.92,173.17,8.07">Answer selection in a multistream open domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,523.22,357.92,16.03,8.07;8,330.52,368.88,72.68,8.07">Proceedings of ECIR&apos;04</title>
		<meeting>ECIR&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,379.84,208.72,8.07;8,330.52,390.80,208.72,8.07;8,330.52,401.76,208.72,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,511.59,379.84,27.65,8.07;8,330.52,390.80,208.72,8.07;8,330.52,401.76,21.13,8.07">Is it the right answer? exploiting web redundancy for answer validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,366.57,401.76,88.97,8.07">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,412.72,208.72,8.07;8,330.52,423.68,208.72,8.07;8,330.52,434.64,208.72,8.07;8,330.52,445.59,56.04,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,330.52,423.68,208.72,8.07;8,330.52,434.64,95.74,8.07">Performance issues and error analysis in an open-domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,434.58,434.64,76.89,8.07">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="154" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,456.55,208.72,8.07;8,330.52,467.51,175.97,8.07" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,368.74,456.55,170.51,8.07;8,330.52,467.51,10.28,8.07">From Document Retrieval to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,330.52,478.47,208.72,8.07;8,330.52,489.43,208.72,8.07;8,330.52,500.39,208.72,8.07;8,330.52,511.35,181.15,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,425.04,478.47,114.20,8.07;8,330.52,489.43,208.72,8.07;8,330.52,500.39,38.19,8.07">Shallow morphological analysis in monolingual information retrieval for Dutch, German and Italian</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,398.27,511.35,68.58,8.07">Proceedings CLEF</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,522.31,208.72,8.07;8,330.52,533.27,208.72,8.07;8,330.52,544.22,157.46,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,442.86,522.31,96.38,8.07;8,330.52,533.27,189.12,8.07">Tequesta: The University of Amsterdam&apos;s textual question answering system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,330.52,544.22,79.51,8.07">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,555.18,208.72,8.07;8,330.52,566.14,208.72,8.07;8,330.52,577.10,208.72,8.07;8,330.52,588.06,131.25,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,474.53,555.18,64.72,8.07;8,330.52,566.14,77.09,8.07">The University of Amsterdam at TREC</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,397.60,577.10,141.65,8.07;8,330.52,588.06,46.45,8.07">The Eleventh Text REtrieval Conference (TREC 2002)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
			<biblScope unit="page" from="603" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,599.02,208.72,8.07;8,330.52,609.98,208.72,8.07;8,330.52,620.94,122.01,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,481.61,599.02,57.63,8.07;8,330.52,609.98,156.85,8.07">Use of Wordnet hypernyms for answering what-is questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,506.04,609.98,33.20,8.07;8,330.52,620.94,44.07,8.07">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.52,631.90,208.72,8.07;8,330.52,642.85,203.11,8.07" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,366.23,631.90,173.02,8.07;8,330.52,642.85,34.47,8.07">Definitional question answering using trainable classifiers</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.Sc thesis</note>
</biblStruct>

<biblStruct coords="8,330.52,653.81,208.73,8.07;8,330.52,664.77,152.44,8.07" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="8,487.54,653.81,51.71,8.07;8,330.52,664.77,77.47,8.07">The Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
