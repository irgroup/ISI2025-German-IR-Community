<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.40,121.21,412.25,12.11">Rutgers&apos; HARD and Web Interactive Track Experiences at TREC 2003</title>
				<funder>
					<orgName type="full">National Science Foundation (NSF</orgName>
				</funder>
				<funder ref="#_uvNMrcD">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,116.40,138.61,42.82,8.95"><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
							<email>belkin@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.60,138.61,32.05,8.95"><forename type="first">D</forename><surname>Kelly</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.00,138.61,33.82,8.95"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.09,138.61,31.32,8.95"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.34,138.61,43.46,8.95"><forename type="first">G</forename><surname>Muresan</surname></persName>
							<email>muresan@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.58,138.61,44.46,8.95"><forename type="first">M.-C</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.20,138.61,40.57,8.95"><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
							<email>xjyuan@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,429.77,138.61,51.54,8.95"><forename type="first">X.-M</forename><surname>Zhang</surname></persName>
							<email>xzhang]@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.40,121.21,412.25,12.11">Rutgers&apos; HARD and Web Interactive Track Experiences at TREC 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9BD51FCC9BC816EBE78837CB5C49E5F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year, members of our group, the Information Interaction Laboratory at Rutgers, SCILS, participated in the HARD track, and in the Interactive Sub-track of the Web track. Since there were no points of commonality between the two separate investigations, we describe and present the results and conclusions for each separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>The HARD Track 2.1</p><p>Introduction and hypotheses The goal of our work in the HARD track was to test techniques for using knowledge about various aspects of the information seeker's context to improve IR system performance. We were particularly concerned with such knowledge which could be gained through implicit sources of evidence, rather than explicit questioning of the information seeker. We therefore did not submit any clarification form 1 , preferring to rely on the categories of supplied metadata concerning the user which we believed could, at least in principle, be inferred from user behavior, either in the past or during the current information seeking episode. To this end, based on the training data supplied and our previous research, we attempted to test the following hypotheses: H1: People who are familiar with a topic will want to see documents which are detailed and terminologically specific; people who are unfamiliar with a topic will want to see general and relatively simple documents. This we operationalized by promoting the value of documents which scored toward the unreadable end of readability scales for people highly familiar with the topic, and by promoting the value of documents which scored toward the easily readable end of the scales for people unfamiliar with the topic. H2: Different document genres can be identified by their vocabularies . This we operationalized by constructing language models for all the retrieved documents for each training topic and for just the completely relevant documents for each topic. We then identified words which occurred with greater than expected probability, based on the entire topic language model, in the relevant documents, for all topics which had the same genre. These words were considered to be indicators of the genre. We added the words associated with a particular genre to queries f or topics which requested that genre. H3: Certain document sources will be relevant, or not, to different desired genres. This we operationalized by promoting documents from certain sources to the top of the retrieved list for topics with some genres, by removing documents from some sources entirely from the retrieved list for topics with some genres, and by demoting the value of documents from some sources in the retrieved list for topics with some genres. H4: If there are texts which the information searcher has identified as relevant to the topic, using them as the basis for automatic query expansion will improve retrieval performance. This was operationalized by choosing terms for query expansion from the relevant texts, based on a combined ranking formula. H5: If the desired granularity of the retrieval result is passage, then the retrieved documents should be ranked on the basis of their best passage, rather than on the document as a whole. This was operationalized by using the InQuery best passage ranking function. Our official submission was with queries constructed on the basis of hypotheses 2, 4 and 5. Our basic IR system was InQuery, version 3.2, obtained from the Center for Intelligent Information Retrieval, University of Massachusetts ( http://ciir.cs.umass.edu) using its default indexing, query processing and retrieval algorithms. The queries for our baseline run were constructed using both title and description fields from the topics, and were just the weighted sum of the stemmed, non-stoplist words from the title and description fields. These queries were then used as the basis for our experimental runs, with them, or their results, modified according to the metadata, as described in section 2.2, below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>How metadata about the searcher was used The experimental condition of the HARD track was for each site to submit at least one baseline run for the set of 50 (eventually 48) topics, using only the title and (optionally) description fields for query construction. The results of the 1 See Allan, this volume, for detailed information about the goals and conditions of the HARD track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>baseline run(s) were compared with the results from one or more experimental runs, which made use of the searcher metadata that was supplied, and of a clarification form submitted to the searcher, asking for whatever information each site thought would be useful in improving search results. We used only the supplied metadata, for the reasons stated in section 2.1, and especially because we were interested in how to make initial queries better, rather than in how to conduct a dialogue with a searcher. There were five categories of searcher metadata for each topic (not all topics had values for all five): Purpose, Genre, Familiarity, Granularity and Related text(s), which were intended to represent aspects of the searcher's context which might be useful in tailoring retrieval to the individual, and the individual situation. We made the assumption that at least some of these categories would be available to the IR system prior to (or in conjunction with) the specific search session, either through explicit or implicit evidence. Therefore, for us the HARD track experimental condition was designed to test whether knowledge of these contextual characteristics, and our specific ways of using that knowledge, would result in better retrieval pe rformance than a good IR system without such knowledge. We understood that there would be, in general, two ways in which to take account of the metadata. One would be to modify the initial query from the (presumed) searcher, before submitting it for search; the other would be to search with the initial query, and then to modify (i.e. re-rank) the results before showing them to the searcher. We used both of these techniques in taking account of the different types of metadata. Knowledge of the purpose of a search (i.e. the searcher's general goal) has long been understood to be important for human search intermediaries in tailoring a search to the specific user (cf. <ref type="bibr" coords="2,350.14,304.81,52.05,8.95" target="#b1">Belkin, 1984)</ref>. Whether such knowledge could be used effectively in a direct end-user IR system is still an open question. Unfortunately, we were unable to investigate this issue in this experiment. One reason for this is that the training data that were supplied in the HARD track did not have sufficient variety on this characteristic for us to investigate different hypotheses about how to take account of it; another is that the types of purpose that were identified did not immediately suggest how they could be used. Desired genre for the results of a search has also been identified as potentially significant in improving search performance (e.g. <ref type="bibr" coords="2,136.20,375.01,123.82,8.95" target="#b9">Rauber &amp; Müller-Kögler, 2001)</ref>. In this case, we had two hypotheses. One was general: that the genre of a document could be identified by its vocabulary. This hypothesis we operationalized in the following way. For the training data, we constructed a language model<ref type="foot" coords="2,279.00,394.80,3.22,5.79" target="#foot_0">2</ref> based on the top 100 documents retrieved by our basic query for each topic, and a language model based on all of the documents which were evaluated as both topically relevant, and satisfying all of the metadata conditions with respect to that topic. We then identified those words which appeared with a significantly higher probability in relevant documents than in all retrieved documents, for each topic associated with each specific genre. We also identified those words which were significant in the relevant documents, but had a low probability of being generated by the language model of the retrieved documents. Using these two lists, and given the nature of the metadata, we were able to identify some words which seemed to be indicative of the genre class, Overview. These words <ref type="bibr" coords="2,156.66,475.81,350.70,8.95">: one, two, three, year, last, more, total, average, historically, spanning, surveyed, trends ;</ref> were added to the baseline queries for all topics which specified Genre as Overview, using the InQuery "or" operator.</p><p>The second hypothesis for genre was based on specifics of the HARD collection. The HARD database consists of the AP Newswire, the New York Times, the Xinghua newspaper (in translation), the Federal Register and the Congressional Record. We noted that documents satisfying the Genre category of Administrative were almost certainly to be found in the Federal Register or the Congressional Record. For such topics, we therefore submitted the basic query, and increased the value on which the document rank was based (the Retrieval Status Value -RSV) for all Congressional Record and Federal Register documents as follows:</p><formula xml:id="formula_0" coords="2,170.40,571.81,114.60,8.95">new RSV = 1 + original RSV</formula><p>(1) This had the effect of placing all CR and FR documents at the top of t he retrieved list, in their original order with respect to one another. We also noted that the Genre category Reaction would almost certainly never be satisfied by a document from the Federal Register collection, and was most likely to be satisfied by documents from news databases. We therefore deleted all Federal Register documents from the results lists for topics with Genre = Reaction, and demoted the value of Congressional Record documents according to the following formula:</p><p>new RSV = original RSV -0.5(original RSV)</p><p>(2) Familiarity with a topic has been identified as having a significant impact on relevance assessments and on how interactive IR searches are conducted (e.g. <ref type="bibr" coords="2,232.41,669.61,78.97,8.95">Kelly &amp; Cool, 2002)</ref>, and it is easy to imagine various ways in which familiarity would impact understanding and usefulness of a document to a person. We hypothesized that people familiar with a topic would not only be able to read and understand technical and detailed documents on the topic, but that they also would prefer those to more general documents on the topic. On the other hand, people who are unfamiliar with a topic might prefer more general documents, and might not be able to comprehend technical ones. Failing any better ideas, we decided to use readability as a measure of techni cality/generality; the less readable, the more technical, the more readable, the more general. Although there was insufficient variety on this characteristic in the training data for our hypothesis to be tested on it, we did compute the readability of a systematic random sample of the HARD collection. This led us to an additional hypothesis: that some documents are too simple to read or too unreadable to be of use to anyone searching in this collection. We therefore implemented the following procedure for taking account of familiarity. The readability of each of the top 1200 documents retrieved by a query to the collection was computed, using three widely used measures. The measures were Fog index, Flesch reading ease score, and Flesch-Kincaid grade level score, computed using algorithms implemented in the PERL programming language by Kim Ryan in 2000<ref type="foot" coords="3,454.80,209.40,3.22,5.79" target="#foot_1">3</ref> . All documents which had all three readability scores at or below, or at or above extreme outlier values for the collection as a whole (as estimated by our sample of the collection) were discarded from the results. Then, for all topics which had a readability level of 4 (meaning very familiar), the RSV was increased for documents which had a readability score greater than (meaning less readable) or equal to 3 standard deviations above the mean as follows:</p><p>new RSV = original RSV + 0.2(original RSV)</p><p>(3) For all topics which had a familiarity level of 1 (meaning no familiarity), the RSV for documents which had a readability score less than (meaning very readable) or equal to 3 standard deviations below the mean were promoted according to equation (3). Granularity of response was a category of metadata to which we paid relatively little attention, primarily because we did not have the capability for effective passage and sentence-level retrieval. However, we made the assumptions that documents with highly relevant passages might have those passages near the beginning of the document, or that such passages would be easy to spot in the document. Then we addressed the Granularity category of Passage by submitting the queries for all such topics using InQuery's passage-level ranking of retrieval results rather than whole-documentbased ranking, with a passage length of 200 words, approximating a paragraph. Finally, we used the Related Text metadata as the basis for query expansion (QE) of the baseline queries for all topics which specified related texts. We did not use these texts for query term re-weighting, and we simply added the QE terms to the basic we ighted sum query. The terms added to a query were determined by using three different QE termranking measures on the set of relevant texts, combining the rankings according to the median rank, and then selecting the top 10. We decided on this method based on results reported by <ref type="bibr" coords="3,329.40,436.81,150.71,8.95">Carpineto, Romano &amp; Giannini (2002)</ref> , which suggest that using different QE ranking techniques and then combining them leads to better retrieval performance than using any single QE ranking technique. We ranked according to the following three formulae: Baseline query + relevant text QE + Overview words + passage-level ranking = results list 1</p><formula xml:id="formula_1" coords="3,100.20,473.41,39.31,8.95">rank = t ,</formula><p>Results list 1 + Administrative re-ranking + Reaction re -ranking + Familiarity re-ranking = final result list Unfortunately, for a variety of reasons, we were able to complete this process only as far as results list 1 in time for the official submission. This is the basis for the results reported below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">HARD results</head><p>Our baseline results were rather good, and substantially above the median of the experimental results for all systems. This is likely to be a result of our using both title and description for our queries; it seems likely that most other sites used title only, or title plus some form of pseudo-relevance feedback or other query expansion technique. Of more interest, of course, are our experimental results.</p><p>With respect to experimental results from all sites participating in the HARD track, Rutgers did quite well. Figure <ref type="figure" coords="4,512.18,108.01,4.97,8.95">1</ref> indicates, for each topic, the amount above or below the median value of the Rutgers results for both R-precision and average precision.</p><p>-.2000 .0000</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2000</head><p>.4000</p><p>.6000</p><p>.8000 1.0000 <ref type="table" coords="4,118.80,347.41,312.06,7.97" target="#tab_9">1 2 3 4 5 6 7 8 910 11 1213 14 15 16 17 18 1920 21 22 23 24 25 2627 28 29 30 31 32 33 34 35 36 37 38 39 4041 42 43 44 45 46 4748 49</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-Precision Avg. Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Difference between median values and Rutgers results for R-precision and Average Precision</head><p>Table <ref type="table" coords="4,89.48,418.81,4.97,8.95">1</ref> shows roughly the same data, indicating how many times the Rutgers results were best, above the median, at the median (M), and below the median for three performance measures (Rutgers was not worst for any topics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure</head><p>Best Above M At M Below M Rel. Ret. @ 10 10 23 13 2 R-precision 4 32 8 4</p><p>Average Precision 3 39 3 3 Table <ref type="table" coords="4,90.51,517.21,4.97,8.95">1</ref> . Rutgers' results compared to all results for experimental run. Unfortunately, this comparison to everyone else does not really tell the full story. In fact, since the goal of the HARD track is to use metadata to improve over the baseline, it is much more important to look at that comparison. Here, things do not look so good. In fact, as table 2 indicates, performance on almost all measures was slightly lower for our experimental run (called Rutmeta) than for our baseline run (called rutbase2), when summarizing over all topics. Although the differences are clearly not significant, they are somewhat disheartening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Precision @ 10 R-precision Avg. Precision Rel. Ret. rutbase2 0.4750 0.3451 0.3186 3736 Rutmeta 0.4750 0.3308 0.3019 3728 Table <ref type="table" coords="4,90.51,648.61,4.97,8.95">2</ref> . Mean values of performance measures for baseline and experimental Rutgers runs. Fortunately, this again does not tell the whole tale. If the results are compared on a topic-by-topic basis, and cumulated as in table 3, then we see that for three out of the four measures, the baseline did better than the experimental run a few times, but for average precision, the experimental run did better on 26 out of the 48 topics, and was equal for three.<ref type="foot" coords="4,513.60,683.40,3.22,5.79" target="#foot_2">4</ref>   <ref type="table" coords="5,91.79,155.41,3.82,8.95">3</ref>. Topic-by-topic comparison of performance between baseline and experimental runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rel</head><p>Although we do not have results which can conclusively indicate what effect each of our different techniques had on performance, we can look at some aspects of this issue. The various topics had different combinations of metadata that we used in our official experimental run, so that there are instances of each technique used separately, and the techniques used in various combinations. Table <ref type="table" coords="5,252.12,217.81,4.97,8.95" target="#tab_2">4</ref> indicates the effect of the different techniques by displaying for how many of the four evaluation measures using the particular metadata technique, or combination of metadata techniques, the technique did better, the same as, or worse than just the baseline. Entries in the 3 leftmost data columns indicate some advantage to having used the metadata; the fourth and fifth data columns indicate no real difference between metadata and baseline, and the sixth and seventh data columns indicate a distinct disadvantage to using the metadata. These data suggest that there was some overall advantage to enhancing the baseline query by using relevant text query expansion in combination with overview query expansion, and, if we disregard the "no difference" values, that using metadata had an overall advantage of better performance on 19 topics compared to 15 topics with better performance in the baseline condition. We still need to figure out how it happened that a topic to whose query we thought we had done nothing turned out to perform worse in the experimental condition than in the baseline. We also did several runs in which we tested the effect of applying only one category of metadata at a time to the baseline run. The results are displayed in Table <ref type="table" coords="5,251.40,564.61,3.94,8.95">5</ref>, where it is easy to see that using Overview query expansion and our version of Passage retrieval had no effect. However, both Genre using the source, and Query expansion had positive effects on performance. Although the differences in performance levels are typically not great for these two, the number of topics positively affected by these two treatments was substantially greater on several of the measures.</p><formula xml:id="formula_2" coords="5,108.60,343.81,381.81,32.35">Metadata 3 or 4 &gt; 2 &gt; 2 = 2 &gt; 1 = 1 &lt; 3 or 4 = 2 &gt; 2 &lt; 2 &lt; 2 = 3 or 4 &lt; Total None (3) 1! 4 QE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4</head><p>Discussion and conclusions on the HARD results Although the average performance of our official run using metadata is somewhat lower than our baseline run, more detailed analysis suggests that we did indeed gain some advantage from using the metadata to modify the baseline queries, in some respects. In particular, performance as measured by average precision was improved for well over half the topics, and there appears to be some advantage to the relevance feedback-like query expansion techniques. The language model-based genre technique did not work well, however. Of course, the ways in which we used the metadata to modify rankings and queries were quite ad hoc, and without real theoretical justification, which could go some way toward explaining negative results. We are still not in a position to evaluate properly the effects of each of the techniques which we have proposed on retrieval performance, nor of their complete combination, nor are we abl e to respond with any level of confidence to our initial hypotheses. We intend to perform further studies in which we compare all of the different techniques, and vary their parameters, in order to address this problem. topics for which the condition had better results. When the two add to less than the total topics, all others were equal. Table <ref type="table" coords="6,91.79,262.81,3.82,8.95">5</ref>. Performance of single metadata treatments compared to baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The Web Interactive Track 3.1</p><p>Introduction and hypotheses This year the interactive TREC experiment was set up as part of the Web track and was built around the topic distillation task: finding a list of key resources for a particular topic, concentrating solely on websites as resources<ref type="foot" coords="6,509.40,328.20,3.22,5.79" target="#foot_3">5</ref> . In the interactive sub-track, the searchers' task was to construct such a resource list for each of a set of broad topics, through interaction with an information access system<ref type="foot" coords="6,276.00,350.40,3.22,5.79" target="#foot_4">6</ref> . The purpose of the experiment was to investigate whether the human capacity to interpret and summarize can beat machine algorithms at the topic distillation task. Apart from the direct comparisons of results, the observation of the human searchers' behavior could potentially offer clues to improving topic distillation algorithms. We investigated the role that the layout of search results plays in supporting human searchers executing topic distillation tasks. Success was measured in terms of accuracy and precision, operationalized as coverage and overlap, so the searcher was expected to find documents that provide information on as many distinct aspects of the assigned topic as possible, with as little overlap between them as possible. Our hypothesis was that using the structure of the domain and of the document corpus in order to organize the search output, would help identify aspects of the search topic in different sub-domains of the document collection, would reduce the searchers' cognitive load and would produce better results than the classic hit list. We tested this hypothesis by using two user interfaces for the Panoptic search engine, one with a simple list output, and the second with documents clustered based on common URL elements. The experimental (or hierarchic) interface, depicted in Figure <ref type="figure" coords="6,306.60,504.01,4.97,8.95" target="#fig_0">2</ref> and described in Box 1, grouped the search results based on c ommonality of URL parts (sub-domain and path) and displayed them in a one level tree. The groups of hits were ranked based on the Panoptic rank of their top document; the Panoptic ranks were also used to sort hits within each group. The structured layout determined us to take two design decisions that go against common Web search engine result arrangements. Firstly, we reckoned that "More results" or "Next page" would be either ambiguous or confusing, so we did not provide such functionality. Instead, the sets of search results contained 30 hits, which was considered sufficient for the topic distillation task: if no relevant document can be found in the top 30 hits, then a query formulation is probably more appropriate than a request for more hits. Secondly, also in order to avoid confusion, the actual ranks were not displayed in the hierarchic output, but the subjects were explained the ranking scheme. The baseline (or linear) interface was almost identical, the only difference being the layout of the 30 hits: they were displayed in a list, with the ranking provided by Panoptic. For consistency, the ranks were not displayed, but the subjects were told that documents at the top of the list were more likely to be relevant. We used the neutral version of Panoptic, so that the subjects' task would not be supported by a topic distillation algorithm; judging the relevance of retrieved documents and the completion of the topic distillation task was entirely based on the subjects' effort. Apart from the measures of coverage and overlap, provided by NIST based on the assessors' relevance judgments, we planned to use a set of objective measures that indicate search effort such as time required to complete the task, number of iterations (or queries submitted), number of documents seen 7 , selected 8 and viewed 9 , number of documents saved during the interaction and number of documents kept 10 . We also prepared questionnaires in order to measure subjective measures of success such as user satisfaction and perception of success, and to investigate the correlation between success and measures such as familiarity or expertise with the topic, search expertise etc. We were also interested in continuing previous years' investigation by looking at the effect that the query formulation panel and the instructions provided to the subject have on the syntax, length and specificity of the queries submitted. As time and resource constraints did not allow us to build another two user interfaces and run more subjects, we have no rigorously tested results. However, observations of the subjects and comparisons to last year's experiment allowed us to draw some anecdotal conclusions. 7 Document surrogates seen while scrolling through the search results. 8 Documents selected from the set of search results output by Panoptic. These are a subset of the set of documents seen. 9 Documents either selected from the hit set, or obtained by following links in the Document Viewer, or by editing the URL and loading the specified webpage, if available in the .gov collection. 10 Saved documents could be unsaved if the subject found better documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>The Interactive experiment We had 16 subjects, volunteers mostly recruited from among Library and Information Science students. Eight were female and four male, and the ages were evenly distributed in the range 18-47. They all displayed a high level of experience with computers (6.44, 0.96), with WWW browsers (6.31, 1.01), with search engines (6.25, 0.93), and displayed a high level of confidence in being able to find information (6.00, 1.10) 11 . While this gave us confidence that the subject would easily learn and adapt to our user interfaces, it also made impossible any comparison between people with different levels of expertise. The experimental design was established by NIST, so the reader is referred to the relevant webpage 12 , which also details the topics. Each subject conducted eight searches, four on the baseline and four on the experimental system. The order of systems and topics was rotated as described in the experimental design to minimize the effect of learning and tiredness on the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data analysis and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.1</head><p>Objective measures Each set of documents saved by each subject, while searching on each of the eight topics, was judged by two NIST assessors and given two scores by each: coverage (of the different aspects of the topic), ranging from 1 (very good) to 5 (very bad) and overlap (between saved documents), ranging from 1 (none) to 5 (way too much). Although there were significant differences in the reviewers' judgments, the conclusions drawn from comparing the linear and the hierarchic system were consistent. Even though a t-test failed to find a statistically significant difference, the data summarized in Table <ref type="table" coords="8,89.40,564.61,4.97,8.95" target="#tab_4">6</ref> indicates a tendency of linear display to be more conducive to better coverage and of hierarchal display to be more conducive to less overlap.  11 The values in parentheses represent mean values and standard deviation on a 7-point Likert scale. 12 http://www.ted.cmis.csiro.au/TRECInt/guidelines.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box 1. Description of the user interfaces:</head><p>The Task Panel allows the subject to start a task (which opens a log file), displays the text of the task, including the topic, keeps track of the time, and allows the subject to end the task (which closes the log file). The Query Panel encourages the subject to describe the information problem and provides sufficient space for several sentences. The Search Results Panel displays the output from the Panoptic search engine, each hit being represented by a URL, a document title, and a summary. The subject can scroll and select documents for viewing in the Document Viewer. For improved usability, color coding is used to mark the currently selected document (visible by switching to the Document Viewer by clicking on the appropriate tab), the already saved documents and the already viewed documents. The Document Viewer displays the full text of the selected document, allows the user to follow hyperlinks, to specify a URL and to request the loading of the specified document; it also allows the user to save the current document or to go back to a previously displayed document. The Saved Documents Panel displays the URL and title of the saved documents. If the user clicks on one of the items in the panel, the corresponding document is displayed in a new window, above the Document Viewer, for comparison with the current document, so that the user can decide if there is overlap between the documents and which document is better. A saved documents can be unsaved if the user finds a better one as replacement, or reviews its relevance in view of the information retrieved.</p><p>A possible explanation of this result is that users of the baseline interface have no structure to support their exploration of the search results and therefore have to scan a larger number of documents to be satisfied with what they find. While more time-consuming and more cognitively demanding, this process has the potential to give a better coverage of a topic. On the other hand, the users of the hierarchic system have the option to direct their browsing at different subdomains of the collection; once the user gets familiar with this kind of output, it is expected that the user would do more analysis, deciding what areas to explore, and less browsing, the result being less "direct interaction" and less overlap between content of saved documents. Scanning all the documents in a collection has the potential for complete coverage of a topic, but is obviously not feasible; recall needs to be balanced by precision or effort. The slight increase in coverage shown by the linear system needs to be considered in the context of effort, measured in terms of time t aken to search, number of iterations, and number of documents seen, selected and viewed. These measures are compared in Table <ref type="table" coords="9,411.60,223.21,3.94,8.95">7</ref>.  <ref type="table" coords="9,91.20,363.61,4.08,8.95">7</ref>: Interaction measures by display modes Even if the difference is not statistically significant, the data in this table indicates a tendency that appears to confirm our hypothesis and expectations: the hierarchic system is conducive to less interaction. Based on previous years' experiments, which indicated a negative correlation between user satisfaction with a system and the amount of interaction <ref type="bibr" coords="9,182.87,410.41,77.67,8.95">(Belkin et al, 2003a)</ref>, the results from our objective measures would predict that the users would prefer the hierarchic system. Other experimental results have indicated that users like to have control over the interaction <ref type="bibr" coords="9,108.34,433.81,115.26,8.95" target="#b8">(Koenemann &amp; Belkin, 1996)</ref>; this provides another reason for us to expect the hierarchic system to be favored by users, as it allows the searcher more navigational control. Let us see if our subjective measures confirm our expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Subjective measures 3.3.2.1 Direct comparison</head><p>The exit questionnaires provide a direct comparison between the two systems: the subjects were asked which system they found easier to learn, easier to use, which system they felt supported the task better, and which system they liked more overall. The results are as shown in The results show that most of the subjects (11) perceive no difference between the linear and the hierarchic system with respect to which one is easier to learn to use. On the other three questions most of the subjects (8, 9, 10 respectively) preferred the hierarchical system. A Chi -Square test indicates that the skewness of the distribution of subject perception is statistically significant in terms of ease to learn (? 2 (2, N=16) = 9.875, p&lt;.01) and overall preference (? 2 (2, N=16) = 6.500, p&lt;.05) and not quite significant in the other cases. We can conclude that the systems are perceived as similar in ease to learn and that people prefer the hierarchic output. Apart from the layout of the display, the two systems were identical, which explains that the subjects found no real difference in learning to use them and in using them. As expected, they clearly preferred the hierarchic display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2.2</head><p>Indirect comparison An indirect comparison between systems was provided by answers to questionnaires administered after a subject finished using a system. The questions focused on the searchers' perception of the system with regard to ease to learn, ease to use, understanding of how to use the system, and usefulness in helping accomplish the search tasks. The subjects answered by assigning scores on a 1 -7 Likert scale, and these scores obtained by the systems were compared by a t-test. No statistical difference was observed overall (t(30) = -.048, p&gt;.05), or in terms of ease to learn (t(30) = -.425, p &gt;.05), ease to use (t(30) = -.116, p&gt;.05), clarity of the conceptual model (t(30) = .227, p&gt;.05) or usefulness for the search task (t(30) = -.374, p&gt;.05). Another indirect comparison between systems was provided by answers to questionnaires administered after each of the eight searches. The questions focused on the subjects' perception of the task completion and the quality level that was achieved on each task:</p><p>-"Do you think the resource list you just constructed focuses on the topic well?" -"Do you think the resource list you just constructed provides a good coverage of the topic?" -"Do you think the resource list you just constructed will be helpful for those people who are interested in this topic?" The results in Table <ref type="table" coords="10,145.20,373.21,4.97,8.95" target="#tab_8">9</ref> indicate that the sets of documents saved with the linear system tend to be slightly better, which correlates with the slightly better coverage observed in the objective measures. Ho wever, a t-test (t(126) =.324, p&gt;.05) shows no significant. Table <ref type="table" coords="10,89.40,410.41,10.37,8.95" target="#tab_9">10</ref> shows the correlations between a few task-related factors and the subjects' subjective search performance (as measured by a scale constructed from their responses to the post -search questions on the extent to which the compiled list is helpful to others , covers the topic, and focuses on the topic). Data on these factors were collected both before and after each search. The results show that all of them are highly correlated with the subjects' subjective search performance. This suggests that these factors may be critical to impact the subjects' search performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.1</head><p>Query formulation Last year we investigated two query-formulation modes. In the former, the experimenter and the text displayed in the user interface encouraged users to submit keywords. In the latter experimental mode, the subjects were specifically asked to use sentences to describe their information need and were provided sufficient space to do so. The experimenters' insistence and their demonstration of describing information problems in sentences, combined with the parenthetical statement "(the more you say, the better the results are likely to be)" had effect on the subjects' behavior: they did follow the instructions and did write sentences. These sentences proved to provide longer queries and fewer iterations, and to generate more satisfaction with the search outcome <ref type="bibr" coords="10,335.35,694.21,81.45,8.95">(Belkin et al., 2003b)</ref>. This year, the Query Panel of our user interface was nearly identical to that from the second mode of last year and provided the same amount of space, suitable for writing several sentences. The difference was that the parenthetical statement was removed from the Query Panel, which was reduced to "Describe your information problem" and the subjects were not specifically asked to write sentences. The result: no subjects generated any sentences. Very familiar with Web searching, the subjects seemed to enter "Google mode": they ignored the instruction from the screen and typed instead keywords, as they are used to. Consequently, the query length distribution (mean 3.04 (st.dev. 1.25) including stopwords and 2.72 (0.87) without stopwords) was surprisingly low compared to the expectations created by last year's experiment. Another explanation for this behavior may be related to the fact that, unlike last year, the topic descriptions were rather "naively" constructed, in order to be appropriate for the automatic tasks of the Web track. The essential topic keywords were present in the topic description, so most users copied and pasted the keywords into the query box, rather than having to generate them based on a problem and context description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.2</head><p>User comments In the exit interviews, many subjects praised the capacity of the hierarchic organization to separate the different subdomains of the collection and therefore different aspects of the topic at hand. The structured output save d them from having to mentally organize the hits and judge the overlap between their content; this was perceived as saving both time and cognitive effort. Such comments confirm our intuition that a structured display should support a structurebased task such as topic distillation. Another feature mentioned often was the Saved Results panel, which helped users keep track of documents saved and allowed them to do side-by-side comparison between the currently examined document and already saved documents in order to compare their quality and the degree of content overlap. Some comments indicated the need to improve the usability of the interfaces and the clarity of the underlying conceptual model. Despite the pre-experiment tutorial, some subjects did not notice the difference between the linear and the hierarchic display, as the indentation of the hierarchy was not seen as significant; others thought that the order of the hits in the display was random, rather than based on some probability of relevance score. There were several complaints concerning the experiment settings: the constraint to limit the search to .gov documents invalidated many hyper-links , which frustrated most subjects; the time limit (10 minutes) put pressure on searchers and potentially generated un-natural behavior; thinking aloud impaired some users' ability to concentrate on the test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5</head><p>Conclusions on the interactive experiment Although it does not produce better coverage than the linear interface, the hierarchic interface seems to be conducive to less effort for the searcher: fewer iterations, shorter search sessions, fewer documents seen, selected and viewed. With regards to subjective measures, users perceived the hierarchic one as easier to use and better at supporting the topic distillation task. These results were not statistically significant. What was statistically significant is that the subjects perceived the two systems equally easy to learn and that they prefer the hierarchic display. One advantage of the structured output, as suggested by the objective measures and highlighted by the users' comments, is the support for investigating different sub-domains of a document collection and consequently different aspects of a topic. The searcher does not need to make a cognitive effort to separate the search results into sub-domain, so the layout makes the interaction easier and more pleasant and more accurately supports the searcher's judgment on task completion. This correlates with results obtained by CSIRO at TREC 2002: although the motivation to use a hierarchic organization was somewhat different, the structure imposed on the search output improved the retrieval performance in the case of complicated tasks, when relevant information needs to be gathered from various parts of the document collection <ref type="bibr" coords="11,64.80,560.41,82.79,8.95" target="#b6">(Craswell et al, 2002)</ref>. One direction in which we intend to continue our investigation is in displaying more than one levels of the hierarchic structure of webpages. While experiments with Cha-Cha<ref type="foot" coords="11,286.20,583.80,6.22,5.79" target="#foot_5">13</ref> have shown promise, we are interested in whether combining navigation by browsing the hierarchic structure and following links to other parts of the hierarchy would help or confuse users. 4</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,64.80,564.61,177.62,8.95;7,64.80,243.80,420.60,305.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The experimental user interface.</figDesc><graphic coords="7,64.80,243.80,420.60,305.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,64.80,367.21,446.50,172.15"><head>Table 4 .</head><label>4</label><figDesc>Effect of application of different metadata information to baseline queries.</figDesc><table coords="5,100.80,367.21,389.57,91.15"><row><cell>only</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell></cell><cell>6</cell><cell>18</cell></row><row><cell>Passage only</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>Overview only</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>1</cell><cell>3</cell></row><row><cell>QE + P</cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell></cell><cell>1</cell><cell>3</cell><cell>6</cell></row><row><cell>QE + O</cell><cell>5</cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>9</cell></row><row><cell>P + O</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>QE + P + O</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>5</cell></row><row><cell>TOTALS</cell><cell>11</cell><cell>2</cell><cell>6</cell><cell>6</cell><cell>5</cell><cell>1</cell><cell>14</cell><cell>48</cell></row></table><note coords="5,100.20,474.61,411.10,8.95;5,100.20,485.41,276.06,8.95;5,100.20,497.41,398.49,8.95;5,100.20,508.21,382.45,8.95;5,100.20,519.61,91.80,8.95"><p>Each column indicates the number of topics for which using metadata resulted in the specified number of evaluation measures being better, equal to, or worse than the baseline . &gt; means Meta better than baseline; = means Meta same as baseline &lt; means Meta worse than baseline QE is query expansion; Passage (P) is ranking by best passage; Overview (O) is adding "overview vocabulary" to queries.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,64.80,598.81,409.86,70.15"><head>Table 6 :</head><label>6</label><figDesc>Search results judged by expert reviewers</figDesc><table coords="8,387.60,598.81,42.72,8.95"><row><cell>Hierarchy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,64.80,520.81,403.76,88.75"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table coords="9,64.80,551.41,403.76,58.15"><row><cell></cell><cell>Linear</cell><cell>Hierarchical</cell><cell>No difference</cell></row><row><cell>Easier to learn to use</cell><cell>4</cell><cell>1</cell><cell>11</cell></row><row><cell>Easier to use</cell><cell>3</cell><cell>8</cell><cell>5</cell></row><row><cell>Support your tasks better</cell><cell>3</cell><cell>9</cell><cell>4</cell></row><row><cell>Like the best overall</cell><cell>2</cell><cell>10</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,64.80,612.61,192.65,8.95"><head>Table 8 :</head><label>8</label><figDesc>Direct system comparison (frequencies)   </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,64.80,294.01,384.05,57.55"><head>Table 9 :</head><label>9</label><figDesc>Subjects' perception of search results</figDesc><table coords="10,283.20,294.01,165.65,8.95"><row><cell>Linear</cell><cell>Hierarchy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,64.80,480.61,426.05,126.55"><head>Table 10 :</head><label>10</label><figDesc>Correlation of subjective assessment of search performance with task-related factors</figDesc><table coords="10,384.00,480.61,106.85,20.35"><row><cell>Correlation with subjective</cell></row><row><cell>search performance</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,70.89,729.01,325.79,8.95"><p>Using the language modeling toolkit at http://mi.eng.cam.ac.uk/~prc14/toolkit.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,68.40,729.01,423.71,8.95"><p>Available online: http://aspn.activestate.com/ASPN/CodeDoc/Lingua-EN-Fathom/Fathom.html#SYNOPSIS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,70.68,729.01,292.37,8.95"><p>For four topics, there was no metadata used at all, so these are not counted.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,70.80,717.61,219.67,8.95"><p>http://es.cmis.csiro.au/TRECWeb/guidelines_2003.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="6,70.80,729.01,214.87,8.95"><p>http://www.ted.cmis.csiro.au/TRECInt/guidelines.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_5" coords="11,74.40,729.01,108.70,8.95"><p>http://cha-cha.berkeley.edu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Many thanks to <rs type="person">Colleen Cool</rs> for her work on the HARD study, to <rs type="person">Michael Coviello</rs> for his contributions to the interactive experiment, and to our volunteer subjects. The research reported here was supported in part by a <rs type="grantName">Rutgers Research Council Grant</rs> to <rs type="person">X.-M. Zhang</rs>, and by <rs type="funder">NSF</rs> grant No. <rs type="grantNumber">9911942</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the <rs type="funder">National Science Foundation (NSF</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uvNMrcD">
					<idno type="grant-number">9911942</idno>
					<orgName type="grant-name">Rutgers Research Council Grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,64.80,122.41,254.48,8.95" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,91.27,122.41,224.34,8.95">J (this volume) Overview of the TREC 2003 HARD track</title>
		<author>
			<persName coords=""><forename type="first">Allan</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,136.21,457.89,8.95;12,64.80,147.61,36.30,8.95" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,142.80,136.21,164.31,8.95">Cognitive models and information transfer</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,315.60,136.21,135.48,8.95">Social Science Information Studies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="129" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,162.01,452.90,8.95;12,64.80,173.41,457.88,8.95;12,64.80,184.21,252.86,8.95" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,64.80,173.41,197.95,8.95">Interaction and query length in interactive retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,414.00,173.41,108.68,8.95;12,64.80,184.21,45.54,8.95">The Eleventh Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>GPO</publisher>
			<date type="published" when="2002">2003 a. 2002</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,198.61,445.51,8.95;12,64.80,209.41,449.55,8.95" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,483.30,198.61,27.01,8.95;12,64.80,209.41,194.22,8.95">b) Query Length in Interactive Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,277.80,209.41,108.06,8.95">Proceedings of SIGIR 2003</title>
		<meeting>SIGIR 2003<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,223.21,453.64,8.95;12,64.80,235.21,309.45,8.95" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,261.33,223.21,257.11,8.95;12,64.80,235.21,47.35,8.95">Improving retrieval feedback with multiple term-ranking function combination</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ro Mano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Giannini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,118.80,235.21,180.67,8.95">ACM Transactions on Information Systems , v</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="290" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,249.01,427.20,8.95;12,64.80,260.41,465.49,8.95;12,64.80,271.21,50.93,8.95" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,247.96,249.01,229.94,8.95">Cha-Cha: A System for Organizing Intranet Search Results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,64.80,260.41,371.40,8.95">Proceedings of the 2nd USENIX Symposium on Internet Technologies and SYSTEMS (USITS)</title>
		<meeting>the 2nd USENIX Symposium on Internet Technologies and SYSTEMS (USITS)<address><addrLine>Boulder, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-10-11">1999. October 11-14, 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,285.61,467.94,8.95;12,64.80,296.41,436.92,8.95" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,378.60,285.61,154.14,8.95;12,64.80,296.41,25.57,8.95">TREC11 Web and Interactive Tracks at CSIRO</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Upstill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,243.60,296.41,105.47,8.95">Proceedings of TREC2002</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>TREC2002<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>GPO</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,311.41,459.85,8.95;12,64.80,322.21,385.99,8.95" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,176.60,311.41,226.65,8.95">Effects of topic familiarity on information search behavior</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,420.00,311.41,104.65,8.95;12,64.80,322.21,243.19,8.95">Proceedings of the Second ACM/IEEE-CS Joint Conference on Digital Libraries -JCDL</title>
		<meeting>the Second ACM/IEEE-CS Joint Conference on Digital Libraries -JCDL<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="74" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,336.61,463.61,8.95;12,64.80,347.41,308.49,8.95" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,223.50,336.61,304.91,8.95;12,64.80,347.41,67.78,8.95">A Case for Interaction: A Study of Interactive Information Retrieval Behavior and Effectiveness</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koenemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,148.80,347.41,93.57,8.95">Proceedings of CHI &apos;96</title>
		<meeting>CHI &apos;96<address><addrLine>New-York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,64.80,361.81,467.56,8.95;12,64.80,372.61,403.29,8.95" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,221.95,361.81,219.26,8.95">Integrating automatic genre analysis into digital libraries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Müller-Kögler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,457.80,361.81,74.56,8.95;12,64.80,372.61,265.39,8.95">Proceedings of the First ACM/IEEE-CS Joint Conference on Digital Libraries -JCDL</title>
		<meeting>the First ACM/IEEE-CS Joint Conference on Digital Libraries -JCDL<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
