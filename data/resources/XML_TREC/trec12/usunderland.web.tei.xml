<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.16,109.83,388.59,12.72;1,220.80,126.57,137.35,12.72">Towards a Sense Based Document Representation for Internet Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.00,166.40,98.20,10.84"><forename type="first">Christopher</forename><surname>Stokoe</surname></persName>
							<email>christopher.stokoe@sund.ac.uk</email>
						</author>
						<author>
							<persName coords="1,365.16,166.40,47.15,10.84"><forename type="first">John</forename><surname>Tait</surname></persName>
							<email>john.tait@sund.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Informatics Centre</orgName>
								<orgName type="institution">University of Sunderland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">St Peters Campus</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Informatics Centre St Peters Campus</orgName>
								<orgName type="institution">University of Sunderland</orgName>
								<address>
									<postCode>+44 (0)191 515, 2712</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.16,109.83,388.59,12.72;1,220.80,126.57,137.35,12.72">Towards a Sense Based Document Representation for Internet Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2DD3915E73537728E4BDF728F545458E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe an attempt to use word sense as an alternate text representation within an information retrieval system in order to enhance retrieval effectiveness. A performance comparison between a term and sense based system was carried out indicating increased retrieval effectiveness using a sense based representation. These increases come about by using a retrieval strategy designed to down rank documents containing query terms identified as being used in an infrequent sense.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction.</head><p>Lexical ambiguity has long been considered as having a negative impact on the performance of information retrieval (IR) systems. Despite a number of studies <ref type="bibr" coords="1,166.43,468.51,15.15,8.50" target="#b9">[10,</ref><ref type="bibr" coords="1,181.58,468.51,7.57,8.50" target="#b4">5,</ref><ref type="bibr" coords="1,189.15,468.51,7.57,8.50" target="#b7">8,</ref><ref type="bibr" coords="1,196.72,468.51,7.57,8.50" target="#b6">7,</ref><ref type="bibr" coords="1,204.30,468.51,7.57,8.50" target="#b8">9]</ref> into ambiguity and IR to date only two have demonstrated significant performance increases. In the first, Sh√ºtze and Pederson <ref type="bibr" coords="1,203.58,502.05,11.35,8.50" target="#b7">[8]</ref> used the computationally expensive approach of clustering co-occurrences within the collection. Each of the clusters in which a given word was found was considered a unique "word use" with each word use arguably representing an individual sense of the word. The second study by Stokoe, Oakes and Tait <ref type="bibr" coords="1,127.74,591.51,11.34,8.50" target="#b8">[9]</ref> took advantage of the skewed frequency distribution in test collections observed by Krovetz and Croft <ref type="bibr" coords="1,212.80,613.89,11.96,8.50" target="#b4">[5]</ref> to create a sense based retrieval strategy that downranked documents which contained infrequent senses of a word. An additional property of this approach was that in cases of inaccurate disambiguation the technique degrades gracefully to at worst the baseline performance of a term model.</p><p>One perceived failing of both of these studies was their evaluation setting. Both showed a comparable performance increase when contrasting TF*IDF ranking with those achieved using sense frequency (SF*IDF). Although this is a first step to demonstrating the worth of a sense based document representation it is clear that most modern information systems rely on a combination of techniques to assign rank. This is most clearly demonstrated when we compare the performance of the Stokoe, Oakes and Tait work against the Web Track submissions for TREC 9. A baseline ranking produced using only TF*IDF was considerably below the average performance achieved by systems at the evaluation. Given this we must question whether the performance increases demonstrated by this technique are simply an artefact of the low performance of the baseline retrieval method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hypothesis.</head><p>Given that, at a low level we see a sense based representation outperforming a term based model, it is our belief that this increased performance can carry over to a modern web retrieval system. In order to demonstrate this we undertook to construct a "full featured" term based topic distillation system and to compare its performance against an identical system which used a sense based model. Our aims were as follows:</p><p>1) Produce a term based system with average or above performance. 2) Produce a corresponding sense based system. 3) Compare and contrast the performance of the term based vs. sense based system.</p><p>For our disambiguation we used the Sunderland University Disambiguation System (SUDS) running in the configuration described in Stokoe, Oakes and Tait for comparability. In terms of the topic distillation techniques to be used we selected a number of common ranking algorithms that were utilised in the 2002 evaluation.</p><p>3. Experimental Methodology.</p><p>All the experimental work was carried out using a 1 GHz Pentium 3 with 398Mb of memory running Linux. All documents were striped of their headers and HTML tags and an initial term based inverted index of the .GOV collection was produced reducing the collection from 21GB to 5.3GB on disk. Total processing time for the production of this index was 11hrs 23 mins. The full text of each document in the collection was also made available for subsequent processing. For each query all the documents containing the query terms were identified from the index and were then subsequently ranked in accordance with our retrieval strategy. These same documents were subsequently disambiguated and reranked using a sense based representation.</p><p>4. Automated Disambiguation.</p><p>The disambiguation system we used (SUDS) is based on a statistical language model constructed from the manually sense tagged Brown1 part of the Semcor corpus. Briefly, it uses a statistical analysis of collocation, cooccurrence and occurrence frequency in order to assign sense. A more detailed explanation of SUDS can be found in Stokoe, Oakes and Tait. SUDS normally work's using a context window consisting of the sentence encapsulating the target word. However in cases where this information is not available E.g. queries, SUDS relies on occurrence frequency stats to perform sense tagging. Therefore one of the underlying assumptions behind SUDS use in IR is that query terms will rarely be seen as examples of a term being used in an infrequent sense. SUDS overall accuracy is reported at 62.1% when evaluated using the Brown2 part of SemCor, this is representative of the current state of the art systems <ref type="bibr" coords="2,187.66,656.30,12.14,8.50" target="#b1">[2]</ref>. However more notably it outperforms bare frequency tagging by 8.2%. Given that frequency only tagging treat's all terms in the collection as being nonpolysemous, and in turn represents the best possible accuracy you could achieve by ignoring sense. This indicates that SUDS can provide a more accurate representation of a collection than simply ignoring sense given that it is more accurate than frequency only tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Topic Distillation.</head><p>Our strategy for topic distillation was based on the increasingly popular link analysis theory initially proposed by <ref type="bibr" coords="2,402.13,364.10,58.88,8.50">Kleinberg [3]</ref>. This approach uses the notion that a key resource can either be an authority or a hub. Authorities are considered to be highly relevant documents of the type often in-linked by hubs which are inversely documents that contain significant outlinks to authorities. Kleinberg proposed that by exploring the link topology of the WWW using a connectivity analysis mechanism one could make inferences on the relevance of a given document based on its linkages. Despite a number of key studies into the performance of link analysis as a ranking mechanism there remain some questions as to its effectiveness. In general the technique has demonstrated comparable performance to traditional statistical retrieval models. However in some cases reduced performance has occurred. These performance drops are generally perceived to be as a result of evaluation using a static collection with a high number of documents that contain out-links to documents not contained in the collection.</p><p>Given that traditional statistical ranking had performed reasonable favourably at TREC 2002 <ref type="bibr" coords="2,297.60,643.70,11.34,8.50" target="#b0">[1]</ref> we used a combination of ranking algorithms to identify authorities. The linkages between documents in our result set were then analysed in order to inflate the rankings of hubs by identifying those pages that had a significant number of out-links to other pages that were judged relevant by our system. Although we collected information about in-links to a given document this was not used in our eventual ranking algorithm as we were unable to identify a way to use it which demonstrated increased retrieval effectiveness. Additionally our system tracked the number of pages form each unique domain which appeared in our final rankings allowing us to manipulate the number of results from each site that the system returned. This was eventually used in the "UNIQUE" runs in order to evaluate system performance where only the highest ranking page from a given site was returned. This was a common strategy at TREC 2002 where several groups <ref type="bibr" coords="3,186.06,253.65,11.40,8.50" target="#b0">[1]</ref> manipulated the number of unique sites that appeared in the top 10 retrieved results. Our sense based retrieval experiments followed exactly the same algorithm (see section 6) however terms were replaced by WordNet sense tags. Therefore each of our sense-based runs has a corresponding term-based baseline for comparison.</p><p>6. Retrieval Algorithm.</p><p>Our retrieval strategy used a number of common techniques associated with the vector space model of retrieval. Each query was stop worded to leave just the content terms and for each document which contained one or more of these terms we calculated an authority_rank using the following features:</p><p>1) TF*IDF Using the well known ranking algorithm (1) presented by <ref type="bibr" coords="3,140.98,510.86,82.94,8.50">Salton and McGill[6]</ref>. With rank being assigned based on the sum of the weights of each term in the query.</p><p>( )</p><formula xml:id="formula_0" coords="3,88.26,550.66,180.10,39.63">Ô£∑ Ô£∏ Ô£∂ Ô£¨ Ô£≠ Ô£´ = = ) ( * ) ( ) ( * ) , ( , i id i i i w Nf Nf LOG W N w IDF d w TF d w W (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Cosine similarity (title)</head><p>A vector based comparison of the document title against the query. This was carried out using a cosine similarity measure <ref type="bibr" coords="3,223.23,647.19,10.33,8.50" target="#b1">(2)</ref>. As seen in Salton and McGill <ref type="bibr" coords="3,171.79,658.41,10.38,8.50" target="#b5">[6]</ref>.</p><formula xml:id="formula_1" coords="3,88.44,676.12,179.09,43.60">‚àë ‚àë ‚àë √ó √ó = k k k k k k k q t q t Q D 2 2 ) ( ) ( ) ( ) , ( œÉ (2)</formula><p>3) Cosine similarity (body) A vector based comparison of the document body and the query carried out using the similarity algorithm (2) we previously used on the document title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Boolean Weighting</head><p>A weighting modifier applied based on testing whether a document is binary 'AND' complete for a given query. I.e. contains all of the content terms.</p><p>The rank assigned by each of these techniques was then normalised between 0..1 using max / min normalisation. Table <ref type="table" coords="3,399.18,287.24,4.87,8.50" target="#tab_0">1</ref> shows the weightings used in the max / min combination, added bias was given to documents that contained query terms in their &lt;title&gt;&lt;/title&gt; tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><formula xml:id="formula_2" coords="3,297.60,343.76,133.98,60.10">Weight TF*IDF 1 TITLE_SIM 2 BODY_SIM 1 Boolean 1</formula><p>Additionally for each document we calculated a hub_rank based on the sum of the authority_rank's assigned to any outward links contained in that document. In order to calculate the final page rank the hub_rank for a document was normalised and added to the authority_rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results.</head><p>We submitted a total of four topic distillation runs for evaluation. Runtags beginning with SB indicate sense based runs while those beginning with TB indicate term based ones. Firstly if we examine the performance of the baseline term based runs (Table <ref type="table" coords="3,373.92,617.49,4.05,8.50" target="#tab_1">2</ref>) we can see that TBBASE outperformed TBUNIQUE with regard to P@10. This demonstrates that returning only the highest ranked document from each website reduces overall system performance. Given that one key feature of topic distillation has always been to identify a suitable entry point to a relevant website it is interesting to note that our system gains performance when returning multiple pages from the same site. On several occasions our TBBASE run demonstrates increased precision @10 by returning multiple ranked relevant pages from a single domain. If we consider the performance (precision @ 10) of our best baseline run compared with the median of all runs submitted to the evaluation (Figure <ref type="figure" coords="4,249.03,278.07,4.02,8.50" target="#fig_3">2</ref>) we can see that this run has average or above performance on all but five of the fifty topics.</p><p>In addition we showed above average performance on thirteen of the topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Median</head><p>Having established that the TBBASE run is representative of the current state of the art we can contrast its performance with the equivalent sense based run (SBBASE). The graph in Figure <ref type="figure" coords="4,152.59,590.67,4.87,8.50" target="#fig_4">3</ref> shows the precision of both the TBBASE and SBBASE runs plotted for the 11 standard points of recall. We can see that the sense based run demonstrates increased precision compared with the term only model particularly in the mid-recall range. In addition if we compare the average precision of both runs 0.1166 (TBBASE) and 0.1259 (SBBASE) we note a small increase when using sense's rather than terms. There is an insignificant increase in Precision @ 5 (0.004) achieved using senses however the term and sense models performed identically when we compare the official measure of precision @ 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion.</head><p>The main aim of our participation in TREC was to assess whether automated word sense disambiguation could be used to improve retrieval effectiveness. We developed a combination topic distillation system that used several traditional techniques and merged their rankings. A comparison between our term based system and the median of all runs in the Web Track topic distillation stream was performed. This demonstrated that our system was representative of average system performance levels seen at the evaluation. A comparison between the performance of our algorithm using a term and sense based representation was performed. The results of this evaluation demonstrate a clear performance gain from using sense information to represent documents. Although our sense based system failed to demonstrate increased precision @ 10 significant gains in recall were made without a corresponding drop in precision. In addition we do see increased avg. precision using a sense based representation and increased R-Precision. This increase was notable given that it was achieved using a disambiguation algorithm that has significantly lower levels of accuracy than those commonly associated with humans who perform the same task. One possible explanation for the success of this approach is that it exploits the skewed frequency distribution known to exist in large collections of natural language. The   work also offers anecdotal evidence to suggest there is a bias towards queries using polysemous terms in a frequently observed sense.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,297.66,248.18,192.55,7.63;2,303.78,258.38,180.03,7.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between the precision of our WSD algorithm compared to baseline frequency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,92.16,519.50,183.39,7.63;4,159.12,529.76,49.43,7.63"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Difference from Median in Precision @ 10 per Topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,309.18,291.98,169.38,7.63;4,316.50,302.18,154.77,7.63"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision -Recall for TBBASE and SBBASE @ 11 Standard Points of Recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,299.58,414.50,188.68,27.91"><head>Table 1 : Weighting bias applied to each technique when merging the rankings using max / min combination.</head><label>1</label><figDesc></figDesc><table coords="4,87.54,108.93,185.10,70.84"><row><cell>Run Tag</cell><cell>R-</cell><cell>Avg.</cell><cell>P@10</cell></row><row><cell></cell><cell>Precision</cell><cell>Precision</cell><cell></cell></row><row><cell>TBBASE</cell><cell>0.1333</cell><cell>0.1166</cell><cell>0.1020</cell></row><row><cell cols="2">TBUNIQUE 0.1278</cell><cell>0.0948</cell><cell>0.0880</cell></row><row><cell>SBBASE</cell><cell>0.1283</cell><cell>0.1259</cell><cell>0.1020</cell></row><row><cell cols="2">SBUNIQUE 0.1407</cell><cell>0.1114</cell><cell>0.0940</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,79.14,195.68,200.59,17.83"><head>Table 2 : R-Precision, Avg. Precision, and Precision @ 10 for all runs.</head><label>2</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.06,186.63,166.31,8.50;5,105.06,197.79,166.29,8.50;5,105.06,208.95,166.23,8.50;5,105.06,220.17,136.90,8.50" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,220.96,186.63,50.41,8.50;5,105.06,197.79,142.83,8.50">Overview of the TREC 2002 Web Track</title>
		<author>
			<persName coords=""><forename type="first">N;</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,105.06,208.95,166.23,8.50;5,105.06,220.17,81.04,8.50">proceedings of the Eleventh Text Retrieval Conference</title>
		<meeting>the Eleventh Text Retrieval Conference<address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,235.23,166.23,8.50;5,105.06,246.39,64.78,8.50;5,195.45,246.39,42.67,8.50;5,263.72,246.39,7.57,8.50;5,105.06,257.55,166.23,8.50;5,105.06,268.77,166.27,8.50;5,105.06,279.93,99.37,8.50" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,110.46,246.39,59.38,8.50;5,195.45,246.39,37.93,8.50">SENSEVAL-2: Overview</title>
		<author>
			<persName coords=""><forename type="first">P;</forename><surname>Edmounds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cotton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>in proceedings of the Second International workshop on Evaluating Word Sense Disambiguation Systems</note>
</biblStruct>

<biblStruct coords="5,105.06,295.05,166.23,8.50;5,105.06,306.21,166.31,8.50;5,105.06,317.37,55.87,8.50" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,176.21,295.05,95.07,8.50;5,105.06,306.21,49.97,8.50">Hubs, authorities, and communities</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,163.64,306.21,103.51,8.50">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4es</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,332.43,166.29,8.50;5,105.06,343.65,138.50,8.50" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,169.14,332.43,102.21,8.50;5,105.06,343.65,35.93,8.50">Information Storage and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Korfhage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,358.71,166.23,8.50;5,105.06,369.93,166.30,8.50;5,105.06,381.09,166.21,8.50;5,105.06,392.25,93.77,8.50" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,242.01,358.71,29.28,8.50;5,105.06,369.93,150.72,8.50">Lexical Ambiguity and Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R;</forename><surname>Krovetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,105.06,381.09,166.21,8.50;5,105.06,392.25,32.36,8.50">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,407.37,24.82,8.50;5,145.95,407.37,9.79,8.50;5,171.87,407.37,30.55,8.50;5,218.50,407.37,17.31,8.50;5,251.89,407.37,19.41,8.50;5,105.06,418.53,166.22,8.50;5,105.06,429.69,152.86,8.50" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,109.09,418.53,162.19,8.50;5,105.06,429.69,35.93,8.50">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G;</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw &amp; Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,444.75,166.25,8.50;5,105.06,455.97,166.25,8.50;5,105.06,467.19,75.67,8.50" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,205.83,444.75,65.47,8.50;5,105.06,455.97,44.03,8.50">Retrieving with good sense</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,167.79,455.97,85.57,8.50">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="69" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,482.25,166.18,8.50;5,105.06,493.35,166.23,8.50;5,105.06,504.57,166.42,8.50;5,105.06,515.79,166.22,8.50;5,105.06,526.95,95.35,8.50" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,109.29,493.35,162.01,8.50;5,105.06,504.57,26.39,8.50">Information Retrieval Based on Word Senses</title>
		<author>
			<persName coords=""><forename type="first">H;</forename><surname>Sh√ºtze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">O</forename><surname>Pederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,149.22,504.57,122.25,8.50;5,105.06,515.79,166.22,8.50;5,105.06,526.95,35.62,8.50">proceedings of the Symposium on Document Analysis and Information Retrieval</title>
		<meeting>the Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,542.01,166.25,8.50;5,105.06,553.17,26.41,8.50;5,148.56,553.17,21.07,8.50;5,186.72,553.17,59.90,8.50;5,263.71,553.17,7.57,8.50;5,105.06,564.39,166.25,8.50;5,105.06,575.61,166.26,8.50;5,105.06,586.71,63.78,8.50" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,110.34,553.17,21.13,8.50;5,148.56,553.17,21.07,8.50;5,186.72,553.17,59.90,8.50;5,263.71,553.17,7.57,8.50;5,105.06,564.39,138.00,8.50">Word sense disambiguation in information retrieval revisited</title>
		<author>
			<persName coords=""><forename type="first">C;</forename><surname>Stokoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oakes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,105.06,575.61,166.26,8.50">proceedings of ACM SIGIR Conference</title>
		<meeting>ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.06,601.83,166.22,8.50;5,105.06,612.99,166.22,8.50;5,105.06,624.21,166.28,8.50;5,105.06,635.37,146.42,8.50" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,248.37,601.83,22.92,8.50;5,105.06,612.99,166.22,8.50;5,105.06,624.21,53.62,8.50">Using WordNet to disambiguate word sense for text retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorehees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,181.97,624.21,89.37,8.50;5,105.06,635.37,72.67,8.50">proceedings of ACM SIGIR Conference</title>
		<meeting>ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
