<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.60,101.79,298.68,15.58">Overview of the TREC 2003 Novelty Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.88,134.26,62.83,10.87"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.22,134.26,78.78,10.87"><forename type="first">Donna</forename><surname>Harman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.60,101.79,298.68,15.58">Overview of the TREC 2003 Novelty Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">17F82CBD33B7D5DC1815B85B9E2757CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The novelty track was first introduced in TREC 2002. Given a TREC topic and an ordered list of documents, systems must find the relevant and novel sentences that should be returned to the user from this set. This task integrates aspects of passage retrieval and information filtering. This year, rather than using old TREC topics and documents, we developed fifty new topics specifically for the novelty track. These topics were of two classes: "events" and "opinions". Additionally, the documents were ordered chronologically, rather than according to a retrieval status value. There were four tasks which provided systems with varying amounts of relevance or novelty information as training data. Fourteen groups participated in the track this year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The novelty track was introduced as a new track last year <ref type="bibr" coords="1,94.17,460.89,9.98,9.96" target="#b4">[5]</ref>. The basic task is as follows: given a topic and an ordered set of relevant documents segmented into sentences, return sentences that are both relevant to the topic and novel given what has already been seen. This task models an application where a user is skimming a set of documents, and the system highlights new, on-topic information.</p><p>There are two problems that participants must solve in the novelty track. The first is identifying relevant sentences, which is essentially a passage retrieval task. Sentence retrieval differs from document retrieval because there is much less text to work with, and identifying a relevant sentence may involve examining the sentence in the context of those surrounding it. We have specified the unit of retrieval as the sentence in order to standardize the task across a variety of passage retrieval approaches, as well as to simplify the evaluation.</p><p>The second problem is that of identifying those relevant sentences that contain new information. The operational definition of "new" is information that has not appeared previously in this topic's set of documents. In other words, we allow the system to assume that the user is most concerned about finding new information in this particular set of documents, and is tolerant of reading information he already knows because of his background knowledge. Since each sentence adds to the user's knowledge, and later sentences are to be retrieved only if they contain new information, novelty retrieval resembles a filtering task.</p><p>To allow participants to focus on the filtering and passage retrieval aspects separately, this year the track offered four tasks. The base task was to identify all relevant and novel sentences in the documents. The other tasks provided varying amounts of relevant and novel sentences as training data. Some groups which chose to focus on passage retrieval alone did only relevant sentence retrieval in the first task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Input Data</head><p>Last year, the track used 50 topics from TRECs 6, 7, and 8, along with relevant documents in rank order according to a top-performing manual TREC run. The assessors' judgments for those topics were remarkable in that almost no sentences were judged to be relevant, despite the documents themselves being relevant. As a consequence, nearly every relevant sentence was novel. This was due in large part to assessor disagreement (the assessors were not the original topic authors) and drift (the document judgments were all made several years ago).</p><p>To both solve the assessor drift problem and to achieve greater redundancy in the test data, this year we constructed fifty new topics on a collection of three contemporaneous newswires. For each topic, the assessor composed the topic, selected 25 relevant documents by searching the collection, and labeled the relevant and novel sentences in the documents.</p><p>As an added twist, 28 of the topics concerned events such as the bombing at the 1996 Olympics in Atlanta, while the remaining topics focused on opinions about controversial subjects such as cloning, gun control, and same-sex marriages. The topic type was indicated in the topic description by a &lt;toptype&gt; tag.</p><p>The documents for the novelty track were taken from the AQUAINT collection. This collection is unique in that it contains three news sources from overlapping time periods: New York Times News Service <ref type="bibr" coords="2,107.13,182.13,44.71,9.96">(Jun 1998</ref><ref type="bibr" coords="2,156.03,182.13,51.65,9.96">-Sep 2000)</ref>, AP (also <ref type="bibr" coords="2,260.22,182.13,40.75,9.96">Jun 1998</ref><ref type="bibr" coords="2,72.00,194.01,50.69,9.96">-Sep 2000)</ref>, and Xinhua News Service <ref type="bibr" coords="2,248.49,194.01,43.62,9.96">(Jan 1996</ref><ref type="bibr" coords="2,295.83,194.01,5.03,9.96;2,72.00,206.01,42.18,9.96">-Sep 2000)</ref>. We intended that this collection would exhibit greater redundancy and thus less novel information, increasing the realism of the task. The assessors, in creating their topics, searched the AQUAINT collection using WebPRISE, NIST's IR system, and collected 25 documents which they deemed to be relevant to the topic.</p><p>Once selected, the documents were ordered chronologically. (Chronological ordering is achieved trivially in the AQUAINT collection by sorting document IDs.) This is a significant change from last year's task, in which they were ordered according to retrieval status value in a particular TREC ad hoc run. Last year's ordering was motivated by the idea of seeking novel information in a ranked list of documents, whereas this year, the task more closely resembles reading new documents over time. This approach seems to make more sense when working with news articles, since background information tends to occur more completely in earlier articles and is summarized more briefly as time goes on and new information is reported. With relevance ranking, one can identify novel sentences but there is no sense of which document should come first.</p><p>The documents were then split into sentences, each sentence receiving an identifier, and all sentences were concatenated together to produce the document set for a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>This year, there were four tasks: Task 1. Given the set of 25 relevant documents for the topic, identify all relevant and novel sentences. (This was the same as last year's task.)</p><p>Task 2. Given the relevant sentences in all 25 documents, identify all novel sentences.</p><p>Task 3. Given the relevant and novel sentences in the first 5 documents only, find the relevant and novel sentences in the remaining 20 documents.</p><p>Task 4. Given the relevant sentences from all 25 documents, and the novel sentences from the first 5 documents, find the novel sentences in the last 20 documents.</p><p>These four tasks allowed the participants to test their approaches to novelty detection given different levels of training: none, partial, or complete relevance information, and none or partial novelty information.</p><p>Participants were provided with the topics, the set of sentence-segmented documents, and the chronological order for those documents. For tasks 2-4, training data in the form of relevant and novel "sentence qrels" were also given. The data were released and results were submitted in stages to limit "leakage" of training data between tasks. Depending on the task, the system was to output the identifiers of sentences which the system determined to contain relevant and/or novel relevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Creation of truth data</head><p>Judgments were created by having NIST assessors manually perform the task. From the concatenated document set, the assessor selected the relevant sentences, then selected those relevant sentences that were novel. Each topic was independently judged by two different assessors, the topic author and a "secondary" assessor, so that the effects of different human opinions could be assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of truth data</head><p>Since the novelty task requires systems to automatically select the same sentences that were selected manually by the assessors, it is important to analyze the characteristics of the manually-created truth data in order to better understand the system results. In particular, there were several concerns raised by the peculiarities of last year's data.</p><p>1. What percentage of the sentences were marked relevant, and how does this vary across topics and across assessors?</p><p>2. Did the quantity of relevant and new information improve from last year? In particular, are more sentences relevant, and are fewer relevant sentences novel? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel sentences</head><p>Percentage of relevant sentences 3. How different are the results of the secondary assessor from the primary assessor who authored the topic and selected the documents?</p><p>4. Is there any difference between "event topics" and "opinion topics", in terms of amounts of relevant and new information?</p><p>Table <ref type="table" coords="3,110.62,461.49,5.03,9.96" target="#tab_0">1</ref> shows the number of relevant and novel sentences selected for each topic by each of the two assessors who worked on that topic. The column marked "assr-1" precedes the results for the primary assessor, whereas "assr-2" precedes those of the secondary assessor. The column marked "rel" is the number of sentences selected as relevant; the next column, "%total", is the percentage of the total set of sentences for that topic that were selected as relevant. The column marked "new" gives the number of sentences selected as novel; the next column, "%rel", is the percentage of relevant sentences that were marked novel. The column "sents" gives the total number of sentences for that topic, and "type" indicates whether the topic is about an event (E) or about opinions on a subject (O).</p><p>One of the most striking aspects of Table <ref type="table" coords="3,268.04,652.77,5.03,9.96" target="#tab_0">1</ref> is the difference in relevant and new percentages from last year. The median percentage of relevant sentences is 37.56%, compared with about 2% last year. For novel sentences, the median is 65.91%, compared with 93% last year. Figure <ref type="figure" coords="3,412.22,390.33,5.03,9.96" target="#fig_1">1</ref> illustrates the range of relevant and novel sentences, and compares it to the 2002 data. Whereas last year, almost no sentences were selected as relevant, and as a result nearly every relevant sentence was novel, this year the distributions of relevant and novel sentences are much more reasonable.</p><p>The analysis of assessor effects is complicated by the fact that only four of the seven assessors (B, C, D, and E) acted as both primary and secondary assessors. Assessor A only judged as a primary assessor, and assessors F and G only judged as secondary assessors (i.e., they judged other assessors topics, but did not author their own).</p><p>As we might expect, there is a large effect from the assessors. For relevant sentence selection, this effect is more significant than either topic type or judgment round. The four assessors who judged topics in both rounds (B, C, D, and E) were quite different from each other, but judged similarly from the first round to the second. For novel sentences, it's a different story; differences between assessors are more pronounced in the first round, but in the second they are all quite similar to each other. Overall, the number of novel sentences selected is more uniform across Last year, we found that the assessors tended to pick consecutive groups of sentences as relevant, despite being instructed otherwise. This year, we did not restrict them from selecting consecutive sentences, instead allowing them to select whatever they felt was necessary. As might be expected, this along with the greater amount of relevant sentences chosen resulted in a much higher occurrence of consecutive relevant sentences. On average, 84% of relevant sentences were selected immediately adjacent to another relevant sentence. The median length of a string of consecutive relevant sentences was 2; the mean was 4.252 sentences.</p><p>Overall, there was not a large difference between the primary and secondary assessor in terms of the number of relevant and novel sentences selected. Figure <ref type="figure" coords="5,88.43,291.45,17.73,9.96" target="#fig_4">3(a)</ref> shows that the secondary assessors tended to be a little more restrictive in their judgments, but this difference is not statistically significant. This implies that the marked difference in judgment patterns we see between this year and last is not only due to an assessor effect. Having more recent documents and topics, and allowing the assessors to select the relevant documents, probably also played a role.</p><p>There is a larger difference between event and opinion topics. Figure <ref type="figure" coords="5,153.30,399.93,4.56,9.96" target="#fig_4">3</ref>(b) illustrates this. Opinion topics tended to have a lower percentage of relevant and a higher percentage of novel sentences than events. The higher percentage of novel sentences is actually due to the lower percentage of relevant sentences. The difference is statistically significant for relevant sentences, but not for novel ones.</p><p>While it may be the case that having multiple news sources from the same time period increased redundancy over last year's topics, having stories from two or three wires did not make a significant difference in the number of novel sentences. Only one topic <ref type="bibr" coords="5,72.00,544.29,17.73,9.96" target="#b9">(10)</ref> drew stories from a single news source; all others involved either two or three sources. On average, 63.61% of relevant sentences were novel for topics with two sources, and 64.73% for those with three. Both of these are less than the new percentage for topic 10 (83.25%), but with only one topic we can't make any conclusions.</p><p>To summarize, the topics and judgments are much improved over last year. While there are differences in judging between the two assessment rounds, and between the different topic types, once again differences between assessors are dominant. Differences are more marked for relevant sentence selection than for novelty, indicating that there is a real difference between these two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scoring</head><p>The sentences selected manually by the NIST assessors were considered the truth data. In contrast to last year, where concerns about assessors selecting groups of sentences for context drove the evaluation to use the assessor with the fewest selected relevant sentences (the so-called "minimum assessor"), this year the judgments by the topic author were taken as the truth data. The judgments by the secondary assessor were taken as a human baseline performance in the task.</p><p>Because relevant and novel sentences are returned as an unranked set in the novelty track, we cannot use traditional measures of ranked retrieval effectiveness such as mean average precision. The track guidelines specified the F measure as the primary evaluation measure for the track. The F measure (from van Rijsbergen's E measure) is itself derived from set precision and recall. For the novelty track, the "set" in question is the set of retrieved sentences (rather than documents as in the retrieval case). Relevant and novel sentence retrieval are evaluated separately. Let M be the number of matched sentences, i.e., the number of sentences selected by both the assessor and the system, A be the number of sentences selected by the assessor, and S be the number of sentences selected by the system. Then sentence set recall is M/A and precision is M/S.</p><p>As previous filtering tracks have demonstrated, setbased recall and precision do not average well, especially when the assessor set sizes vary widely across topics. Consider the following example as an illustration of the problems. One topic has hundreds of relevant sentences and the system retrieves 1 relevant sentence. The second topic has 1 relevant sentence and the system retrieves hundreds of sentences. The average for both recall and precision over these two topics is approximately .5 (the scores on the first topic are 1.0 for precision and essentially 0.0 for recall, and the scores for the second topic are the reverse), even though the system did precisely the wrong thing. While most real submissions won't exhibit this extreme behavior, the fact remains that recall and precision averaged over a set of topics is not a good diagnostic indicator of system performance. There is also the problem of how to define precision when the system returns no sentences (S = 0). Not counting that question in the evaluation for that run means differ-    ent systems are evaluated over different numbers of topics, while defining precision to be either 1 or 0 is extreme. (The average scores given in Appendix A defined precision to be 0 when S = 0 since that seems the least evil choice.)</p><p>To avoid these problems, the primary measure for novelty track runs is the F measure. This measure is a function of set recall and precision, together with a parameter β which determines the relative importance of recall and precision. A β value of 1, indicating equal weight, is used in the novelty track. F β=1 is given as:</p><formula xml:id="formula_0" coords="8,153.72,490.17,64.40,23.52">F = 2 × P × R P + R</formula><p>Alternatively, this can be formulated as</p><formula xml:id="formula_1" coords="8,72.00,537.33,231.40,23.52">F = 2 × (# relevant sentences retrieved) (# retrieved sentences) + (# relevant sentences)</formula><p>For any choice of β, F lies in the range [0, 1], and the average of the F measure is meaningful even when the judgment sets sizes vary widely. For example, the F measure in the scenario above is essentially 0, an intuitively appropriate score for such behavior. Using the F measure also deals with the problem of what to do when the system returns no sentences since recall is 0 and the F measure is legitimately 0 regardless of what precision is defined to be.</p><p>Note, however, that two runs with equal F scores do not indicate equal precision and recall. Figure <ref type="figure" coords="8,295.86,688.65,5.03,9.96" target="#fig_5">4</ref> illustrates the shape of the F measure in precisionrecall space. An F score of 0.5, for example, can reflect a range of precision and recall scores. Thus, two runs with equal F scores may be performing quite differently, and a difference in F scores can be due to changes in precision, recall, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants</head><p>Table 2 lists the 14 groups that participated in the TREC 2003 novelty track. All but one group attempted the first task, and nearly every group tried every task. The rest of this section contains short summaries submitted by most of the groups about their approaches to the novelty task. For more details, please refer to the group's complete paper in the proceedings.</p><p>In general, most groups took a similar approach to the problem. Relevant sentences were selected by measuring similarity to the topic, and novel sentences by dissimilarty to past sentences. As can be seen from the following descriptions, there is a tremendous variation in how "the topic" and "past sentences" are modeled, and in how similarity is computed when sentences are involved. Many groups tried variations on term expansion to improve sentence similarity, some with more success than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CCS/University of Maryland [1]</head><p>For the 2003 DUC task of forming a summary based on the relevant and novel sentences, we tested a system based on a Hidden Markov Model (HMM). In this work, we use variations of this system on the tasks of the TREC Novelty Track. Our information retrieval system couples a query handler, a document clusterer, and a summary generator with a convenient user interface. Our summarization system uses an HMM to find relevant sentences in a document. The HMM has two types of states, corresponding to relevant and non-relevant sentences. The observation sequence scored by the HMM is composed of the number of signature terms and topic terms contained in each sentence. A signature term is a term that statistically occurs more frequently in the document set than in the document collection at large, and a subject term is a signature term which also occurs in the headlines or subject lines of a document. The counts of these terms are normalized within a document to have a mean of zero and variance of one. We determine the relevant sentences in a document based on the HMM posterior probability of each sentence being relevant. In particular, we choose the number of sentences to maximize the expected utility, which for TREC is simply the F1 score. Several methods were explored to find a subset of the relevant sentences that has good coverage but low redundancy. In our multi-document summarization system, we used the QR algorithm on term-sentence matrices. For this work, we explored the use of the singular value decomposition as well as two variants of the QR algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Chinese Academy of Sciences (ICT) [11]</head><p>The novelty track can be treated as a binary classification problem: relevant sentences vs. irrelevant sentences, or new vs. non-new. In this way, we applied variants of techniques that have been employed for text categorization problem. To retrieve the relevant sentences, we compute the similarity between the topic and sentences using vector space model. The features for each topic are obtained by employing χ 2 statistic and each feature is also weighted using the χ 2 statistic. If the similarity exceeds a certain threshold, the sentence is considered as relevant. In addition, we try several techniques in an attempt to improve the performance. One is that the narrative section in the topic is analyzed to obtain the negative features and negative vector of the topic. We determine the relevance by adding similarity between the negative vector and sentence as a negative factor.</p><p>The second, the threshold for different docs in each topic is dynamically adjusted according to the doc density, rather than fixed in the whole period. We have implemented the KNN algorithm and Winnow algorithm for classifying the sentences into relevant and irrelevant sentences in the novelty task 3. To detect the new sentences from the relevant sentences, we try several methods, such as Maximum Marginal Relevance (MMR) measure, Winnow algorithm and word overlapping within sentences. What's more, we attempt to detect novelty by computing semantic distance between sentences using WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Chinese Academy of Sciences (NLPR) [6]</head><p>For finding relevant sentences, we use a new statistical model called "Term Similarity Tree" to make the process of query expansion more flexible and controllable. Then, relevant feedback is used for additional modification for queries. Serveral different methods for similarity computing are developed to improve the performance. They are "simple window", "dynamic window", "active window". The key notion is that the window-based method can ensure that the closer the query words in sentences, the higher the similarity value. Finally, dynamic thresholds are used for different topics, which usually brings 1% increase of average F measure. For finding new sentences, We define a value called "New Information Degree" (NID) to present whether a sentence includes new information related to the former sentences. If the value of NID is big, this sentence is reserved, or it will be discard. There are two different ways to define NID of the latter sentence related to the former sentence.</p><p>One is based on idf value of terms and the other is based on bi-gram sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CL Research [8]</head><p>The CL Research system parses and processes text into an XML representation, tagging the text with discourse, noun, verb, and preposition characteristics. The topic characterizations (titles, descriptions, and/or narratives) and the relevant documents provided by NIST were processed in this way. Componential analysis of the degree to which topic characterizations corresponded to sentences was used as the basis for determining relevance, using various scoring metrics. Similar componential analysis was used to compare each relevant sentence with all those that preceded it in order to assess novelty. Several variables were used as the basis for different runs under the different tasks (which also provided prior information that could be exploited), providing useful experimental results that will inform selection among alternatives for approaching the novelty task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">IRIT-SIG [2]</head><p>In TREC 2003, IRIT improved the strategy that was introduced in TREC 2002. A sentence is considered as relevant if it matches the topic with a certain level of coverage. This coverage depends on the category of the terms used in the texts. Three types of terms were defined for TREC 2002 highly relevant, lowly relevant and non-relevant (like stop words). In TREC 2003 we introduced a new class of terms: highly non-relevant terms. Terms from this category are extracted from the narrative parts of the queries that describe what will be a non-relevant document. A negative weight can be assigned to these words. With regard to the novelty part, a sentence is considered as novel if its similarity with each of the previously processed and selected-as-novel sentences does not exceed a certain threshold. In addition, this sentence should not be too similar to a virtual sentence made of the n best-matching sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">University of Southern California-ISI</head><p>To identify opinion sentences, we used unigrams to indicate subjectivity. In addition to three baseline algorithms, we employed two sets of subjectivityindicating words (either positive or negative valence, with appropriate strengths). One set was collected manually and extended with WordNet synonyms. The other was learned automatically from the Wall Street Journal. The words' relative scores and the algorithm's cutoff parameters were determined in a series of experiments. To our surprise the TREC results showed that one of our baselines (indicating that every sentence carries an opinion) actually beat the algorithm using the manually collected words. To identify event sentences, we adopted a standard IR procedure, treating each sentence as a separate document. For each event topic, we used all its non-stop words as query to extract event sentences. Again, the cutoff parameter was determined by experiment. We were happy to see that this method worked relatively well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">LexiClone [4]</head><p>For the sake of convenience we decided that on the word-per-word level, any language is about 58-59 percent nouns, 20 percent verbs and 20 percent adjectives. Except for prepositions, conjunctions, interjections, pronouns and other parts of speech that make up the remaining 1-2 percent, the rest of the language is a combination of these three dominant elements (or can be reduced to them). LexiClone establishes all possible combinations of nouns, verbs and adjectives for each sentence. We call these combinations "triads". (Actually, a triad is a smallest possible "key" phrase from a sentence.) After that we find sentences that have triads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Meiji University [9]</head><p>For identifying relevant sentences, we employed following information-filtering-based approach. We regarded sentences as very short documents. Initial profiles, which are made from topic descriptions, are expanded conceptually. Conceptual fuzzy sets, which we proposed previously, are used for conceptual expansion. If the cosine similarity between the expanded profile and a word vector of each sentence exceeds a threshold, the sentence is regarded as relevant. For identifying new sentences, we considered two measures; sentence score and redundancy score. 1) For calculating a sentence score, we used Nwindow-idf as a time window. Local sentence score is calculable by using document frequency of past N documents. 2) Redundancy score is the maximum value of the similarity with the sentence judged to be novel in the past.</p><p>5.9 National Taiwan University <ref type="bibr" coords="11,269.19,115.11,20.99,11.97" target="#b11">[12]</ref> According to the results of TREC 2002, we realized the major challenge issue of recognizing relevant sentences is a lack of information used in similarity computation among sentences. In TREC 2003, NTU attempts to find relevant and novel information based on variants of employing information retrieval (IR) system. We call this methodology IR with reference corpus, which can also be considered an information expansion of sentences. A sentence is considered as a query of a reference corpus, and similarity between sentences is measured in terms of the weighting vectors of document lists ranked by IR systems. Basically, we looked for relevant sentences by comparing their results on a certain information retrieval system. Two sentences are regarded as similar if they are related to the similar document lists returned by IR system. In novelty parts, similar analysis is used to compare each relevant sentence with all those that preceded it to find out novelty. An effectively dynamic threshold setting which is based on what percentage of relevant sentences is within a relevant document is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Tsinghua University [14]</head><p>Research in IR group of Tsinghua University on this year's novelty track mainly focused on four aspects:</p><p>(1) unsupervised relevance judgment, where QE and pseudo relevance feedback has been used. (2) efficient sentence redundancy computing: we used unsymmetrical sentence "overlap" metric, sub-topic redundancy elimination and sentence clustering. (3) supervised sentence classification, where a SVM classifier has been used and got encouraging results; (4) supervised redundancy threshold learning. A new IR system named TMiner has been built on which all experiments have been performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">University of Iowa [3]</head><p>Our approach is basically the same as that used last year. We use new named entity and noun phrase triggering, guarded by a dual threshold of sentence similarity and full-document similarity. If the full document is sufficiently similar and the current sentence is sufficiently similar, the number of newly-detected named entities and noun phrases is compared against a minimum threshold and if the minimum is met, the current sentence is declared to be novel. The named entities used include persons, organizations and place names. Relevance is simple term similarity.</p><p>5.12 University of Maryland Baltimore County <ref type="bibr" coords="11,429.79,156.39,14.15,11.97" target="#b6">[7]</ref> To find the relevant sentences, we used a method comprising of query expansion and sentence clustering. In the query expansion step, we experimented with two methods, one was to determine highly cooccurring terms by means of a SVD analysis and, the other was by determining meaningful terms as obtained by a language analysis of the narrative section for each topic. The sentences, per topic, were clustered and the top clusters were selected based on similarity scores of the cluster centroids and the expanded query. All the sentences from the selected clusters are chosen as the relevant sentences.</p><p>To find the novel sentences, we experimented with two methods. One, based on a text summarization method, was clustering relevant sentences and choosing one sentence each from the selected clusters to make up the set of novel sentences. In the second method, using a sentence-sentence similarity matrix (of relevant sentences), the dissimilarity between sentences was used to determine novel sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.13">University of Michigan [10]</head><p>First we used the MEAD summarization software to compute scores for each sentence on features such as length, position, word overlap with query, title and description. Since we trained maximum entropy classifiers, these scores were then discretized. Once the MEAD features were calculated, discretized and formatted, we used the maxent-2.1.0 software to train our models for novel and relevant sentences.</p><p>For tasks 1 and 3, once the maxent models had been trained for classifying novel and relevant sentences and were used to produce a ranked list of sentences as to how likely they were to be novel or relevant, we then chose differing percentage cut offs for each run in an attempt to maximize recall and precision on our devtest data set. For tasks 2 and 4, we noted that the F-measure for a baseline algorithm of submitting all relevant sentences as being novel was quite high. Therefore, we focused on trying various discretizations of our feature scores in order to improve the classifier's performance on the devtest set <ref type="bibr" coords="12,72.00,98.37,43.88,9.96">Figures 5,</ref><ref type="bibr" coords="12,119.96,98.37,7.79,9.96" target="#b6">7,</ref><ref type="bibr" coords="12,131.83,98.37,7.79,9.96" target="#b7">8,</ref><ref type="bibr" coords="12,143.82,98.37,25.06,9.96" target="#b8">and 9</ref> show the average F scores in each task. Task 1 scores are shown alongside the "scores" of the secondary assessor, who may be considered to have been performing this task. Within the margin of error of human disagreement, these lines can be thought of as representing the best possible performance. The best systems are performing at this level. Nine runs have novelty F scores of 0 because those runs did not return any novel sentences.</p><p>Tasks 1 and 3 show novelty retrieval performance closely tracking relevant retrieval performance. Only a few runs near the bottom of the performance range did better at retrieving novel sentences than relevant ones. This seems somewhat surprising, since while the retrieved set of relevant sentences places a bound on recall for the novel set (since only retrieved sentences can be labeled novel), any level of precision is possible, and thus there isn't any reason why F novel shouldn't exceed F relevant . However, to achieve this most systems would have had to make a very large improvement in precision when retrieving novel sentences.</p><p>As stated previously, sometimes it can be hard to understand what the F score means in terms of the actual behavior of each run. Figure <ref type="figure" coords="12,237.47,387.57,5.03,9.96">6</ref> shows the F scores for task 1, along with each run's corresponding average recall and precision. Note for example the run ISIALL03 (run #11 on the x axis), which retrieved only relevant sentences, and retrieved all of them; for this run, average recall was 1.0 but precision was 0.41. It is very interesting to note that average recall seems to correlate more closely to the F scores, although F is defined to be a harmonic mean between the two. This may mean that within each run, recall was more consistent across topics than was precision.</p><p>The scores for tasks 2-4 show how many of the systems can take advantage of training data, both for relevance and novelty. Comparing the graph of tasks 2 and 3, we can see that having more relevance information dramatically improves novelty retrieval effectiveness. Moreover, comparing tasks 2 and 4, we can see that having relevant sentences is more valuable than having novel sentences for training, since the top systems do not improve from task 2 to task 4.</p><p>The graphs for tasks 2 and 4 compare the runs against a baseline system which merely returns all the relevant sentences (provided as training data in these tasks) as novel. The best systems are performing above this baseline, indicating that they are being somewhat selective in what they return as novel.</p><p>Event topics were easier than opinion topics. Figure <ref type="figure" coords="12,328.91,86.49,10.06,9.96" target="#fig_11">10</ref> illustrates this phenomenon in task 1. Relevant sentence retrieval scores are on the left, novelty retrieval scores on the right. The graphs show the overall average along with the averages for event and opinion topics for each run. Nearly every run did better at events than opinions; the exceptions are UMBC and NTU for relevant sentences, and NTU and one IRIT run for novel sentences.</p><p>As the systems receive more relevant sentences as training data, they improve on opinion topics. In task 3 (where systems received some relevant and novel training data), all systems perform as well or better on event topics than on opinions. However, in tasks 2 and 4, where the systems receive complete relevance information, the situation is reversed: all systems do better on opinion topics. Clearly, the systems are less able to identify relevant sentences in opinion topics, but if they know which ones are relevant, they do better on opinion topics than on events. Having a small amount of relevant sentence training data (as in task 3) is not sufficient to boost a system's overall performance.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.00,334.05,467.55,9.96;3,72.00,346.05,205.77,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Percentage of relevant and novel sentences (both primary and secondary assessors), compared to 2002 (both minimum and maximum assessors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,249.48,615.33,112.93,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Assessor effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,163.08,674.01,285.60,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Differences between assessment rounds and topic types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,72.00,288.09,228.78,9.96;8,72.00,300.09,228.90,9.96;8,72.00,311.97,155.50,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The F measure, plotted according to its precision and recall components. The lines show contours at intervals of 0.1 points of F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="14,119.52,337.41,372.92,9.96"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Scores for Task 1, along with the "average score" of the secondary assessor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="15,96.17,260.07,7.98,11.99;15,96.17,228.26,7.98,11.99;15,96.17,196.44,7.98,11.99;15,96.17,164.63,7.98,11.99;15,96.17,132.82,7.98,11.99;15,262.41,100.62,108.52,9.31;15,75.46,207.58,7.98,5.27;15,75.46,184.08,7.98,21.09;15,128.06,131.21,300.75,35.92;15,433.95,175.92,3.36,7.98;15,442.45,183.45,11.85,10.14;15,459.44,207.57,37.34,16.24"><head></head><label></label><figDesc>* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="15,245.52,675.57,120.96,9.96"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Scores for Task 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="16,109.92,381.81,392.03,9.96"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Scores for Task 4, against a baseline of returning all relevant sentences as novel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="16,72.00,647.73,467.72,9.96;16,72.00,659.73,344.50,9.96"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Scores for task 1, broken down by topic type. Runs are along the X axis; the run names have been omitted for readability, but the runs are in the same order as in Figure5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,78.00,81.45,425.26,625.44"><head>Table 1 :</head><label>1</label><figDesc>Analysis of relevant and novel sentences by topic</figDesc><table coords="4,78.00,98.73,425.26,608.16"><row><cell cols="13">Topic type sents assr-1 rel %total new %rel assr-2 rel %total new %rel</cell></row><row><cell>N1</cell><cell>O</cell><cell>880</cell><cell>A</cell><cell cols="2">184 20.91</cell><cell cols="2">151 82.07</cell><cell>F</cell><cell></cell><cell>51.93</cell><cell cols="2">265 57.99</cell></row><row><cell>N2</cell><cell>E</cell><cell>500</cell><cell>D</cell><cell>78</cell><cell>15.6</cell><cell>43</cell><cell>55.13</cell><cell>B</cell><cell></cell><cell>34.0</cell><cell>58</cell><cell>34.12</cell></row><row><cell>N3</cell><cell>E</cell><cell>932</cell><cell>C</cell><cell cols="2">596 63.95</cell><cell cols="2">331 55.54</cell><cell>E</cell><cell></cell><cell>26.61</cell><cell cols="2">152 61.29</cell></row><row><cell>N4</cell><cell>E</cell><cell>928</cell><cell>B</cell><cell>438</cell><cell>47.2</cell><cell>265</cell><cell>60.5</cell><cell>D</cell><cell></cell><cell>12.18</cell><cell>72</cell><cell>63.72</cell></row><row><cell>N5</cell><cell>E</cell><cell>1662</cell><cell>B</cell><cell cols="2">259 15.58</cell><cell cols="2">219 84.56</cell><cell>G</cell><cell></cell><cell>17.63</cell><cell cols="2">246 83.96</cell></row><row><cell>N6</cell><cell>E</cell><cell>424</cell><cell>B</cell><cell cols="2">317 74.76</cell><cell>233</cell><cell>73.5</cell><cell>C</cell><cell></cell><cell>69.34</cell><cell cols="2">192 65.31</cell></row><row><cell>N7</cell><cell>E</cell><cell>306</cell><cell>E</cell><cell>95</cell><cell>31.05</cell><cell>79</cell><cell>83.16</cell><cell>D</cell><cell>99</cell><cell>32.35</cell><cell>61</cell><cell>61.62</cell></row><row><cell>N8</cell><cell>E</cell><cell>659</cell><cell>D</cell><cell cols="2">158 23.98</cell><cell cols="2">107 67.72</cell><cell>G</cell><cell></cell><cell>52.96</cell><cell cols="2">210 60.17</cell></row><row><cell>N9</cell><cell>E</cell><cell>637</cell><cell>B</cell><cell cols="2">160 25.12</cell><cell>62</cell><cell>38.75</cell><cell>F</cell><cell></cell><cell>41.29</cell><cell cols="2">205 77.95</cell></row><row><cell>N10</cell><cell>E</cell><cell>257</cell><cell>C</cell><cell cols="2">191 74.32</cell><cell cols="2">159 83.25</cell><cell>F</cell><cell></cell><cell>67.7</cell><cell cols="2">139 79.89</cell></row><row><cell>N11</cell><cell>E</cell><cell>393</cell><cell>C</cell><cell cols="2">148 37.66</cell><cell cols="2">108 72.97</cell><cell>G</cell><cell></cell><cell>33.08</cell><cell cols="2">107 82.31</cell></row><row><cell>N12</cell><cell>O</cell><cell>1044</cell><cell>C</cell><cell cols="2">729 69.83</cell><cell cols="2">579 79.42</cell><cell>D</cell><cell>76</cell><cell>7.28</cell><cell>62</cell><cell>81.58</cell></row><row><cell>N13</cell><cell>O</cell><cell>941</cell><cell>A</cell><cell cols="2">205 21.79</cell><cell cols="2">166 80.98</cell><cell>F</cell><cell></cell><cell>48.57</cell><cell cols="2">198 43.33</cell></row><row><cell>N14</cell><cell>O</cell><cell>1129</cell><cell>D</cell><cell>93</cell><cell>8.24</cell><cell>42</cell><cell>45.16</cell><cell>G</cell><cell></cell><cell>16.92</cell><cell cols="2">122 63.87</cell></row><row><cell>N15</cell><cell>O</cell><cell>649</cell><cell>C</cell><cell cols="2">522 80.43</cell><cell cols="2">421 80.65</cell><cell>B</cell><cell></cell><cell>58.24</cell><cell cols="2">214 56.61</cell></row><row><cell>N16</cell><cell>E</cell><cell>500</cell><cell>E</cell><cell>179</cell><cell>35.8</cell><cell cols="2">119 66.48</cell><cell>F</cell><cell></cell><cell>54.8</cell><cell cols="2">183 66.79</cell></row><row><cell>N17</cell><cell>O</cell><cell>1106</cell><cell>B</cell><cell cols="2">792 71.61</cell><cell cols="2">488 61.62</cell><cell>G</cell><cell></cell><cell>65.46</cell><cell cols="2">524 72.38</cell></row><row><cell>N18</cell><cell>O</cell><cell>1238</cell><cell>B</cell><cell cols="2">537 43.38</cell><cell cols="2">429 79.89</cell><cell>D</cell><cell>98</cell><cell>7.92</cell><cell>47</cell><cell>47.96</cell></row><row><cell>N19</cell><cell>O</cell><cell>867</cell><cell>D</cell><cell>62</cell><cell>7.15</cell><cell>37</cell><cell>59.68</cell><cell>C</cell><cell></cell><cell>48.79</cell><cell cols="2">253 59.81</cell></row><row><cell>N20</cell><cell>O</cell><cell>886</cell><cell>D</cell><cell>69</cell><cell>7.79</cell><cell>41</cell><cell>59.42</cell><cell>B</cell><cell></cell><cell>25.73</cell><cell cols="2">169 74.12</cell></row><row><cell>N21</cell><cell>O</cell><cell>932</cell><cell>E</cell><cell cols="2">340 36.48</cell><cell cols="2">194 57.06</cell><cell>G</cell><cell></cell><cell>34.01</cell><cell>265</cell><cell>83.6</cell></row><row><cell>N22</cell><cell>O</cell><cell>841</cell><cell>D</cell><cell>84</cell><cell>9.99</cell><cell>52</cell><cell>61.9</cell><cell>F</cell><cell></cell><cell>47.68</cell><cell cols="2">295 73.57</cell></row><row><cell>N23</cell><cell>O</cell><cell>896</cell><cell>E</cell><cell cols="2">346 38.62</cell><cell cols="2">254 73.41</cell><cell>D</cell><cell></cell><cell>14.96</cell><cell>86</cell><cell>64.18</cell></row><row><cell>N24</cell><cell>O</cell><cell>968</cell><cell>E</cell><cell cols="2">160 16.53</cell><cell>76</cell><cell>47.5</cell><cell>B</cell><cell></cell><cell>22.73</cell><cell cols="2">101 45.91</cell></row><row><cell>N25</cell><cell>O</cell><cell>701</cell><cell>D</cell><cell>19</cell><cell>2.71</cell><cell>16</cell><cell>84.21</cell><cell>C</cell><cell></cell><cell>42.94</cell><cell cols="2">249 82.72</cell></row><row><cell>N26</cell><cell>O</cell><cell>911</cell><cell>C</cell><cell cols="2">661 72.56</cell><cell cols="2">283 42.81</cell><cell>E</cell><cell></cell><cell>19.54</cell><cell cols="2">137 76.97</cell></row><row><cell>N27</cell><cell>O</cell><cell>962</cell><cell>C</cell><cell cols="2">730 75.88</cell><cell cols="2">577 79.04</cell><cell>E</cell><cell></cell><cell>28.38</cell><cell cols="2">229 83.88</cell></row><row><cell>N28</cell><cell>O</cell><cell>978</cell><cell>B</cell><cell cols="2">371 37.93</cell><cell cols="2">261 70.35</cell><cell>E</cell><cell></cell><cell>11.15</cell><cell>80</cell><cell>73.39</cell></row><row><cell>N29</cell><cell>O</cell><cell>861</cell><cell>D</cell><cell>65</cell><cell>7.55</cell><cell>52</cell><cell>80.0</cell><cell>B</cell><cell>69</cell><cell>8.01</cell><cell>39</cell><cell>56.52</cell></row><row><cell>N30</cell><cell>O</cell><cell>900</cell><cell>C</cell><cell cols="2">445 49.44</cell><cell cols="2">307 68.99</cell><cell>G</cell><cell></cell><cell>55.22</cell><cell cols="2">386 77.67</cell></row><row><cell>N31</cell><cell>O</cell><cell>1220</cell><cell>C</cell><cell cols="2">985 80.74</cell><cell cols="2">752 76.35</cell><cell>D</cell><cell>74</cell><cell>6.07</cell><cell>48</cell><cell>64.86</cell></row><row><cell>N32</cell><cell>O</cell><cell>1078</cell><cell>B</cell><cell cols="2">216 20.04</cell><cell>100</cell><cell>46.3</cell><cell>C</cell><cell></cell><cell>63.45</cell><cell cols="2">475 69.44</cell></row><row><cell>N33</cell><cell>E</cell><cell>680</cell><cell>C</cell><cell cols="2">526 77.35</cell><cell cols="2">376 71.48</cell><cell>G</cell><cell></cell><cell>64.85</cell><cell cols="2">297 67.35</cell></row><row><cell>N34</cell><cell>E</cell><cell>1030</cell><cell>E</cell><cell cols="2">475 46.12</cell><cell cols="2">217 45.68</cell><cell>D</cell><cell></cell><cell>10.29</cell><cell>78</cell><cell>73.58</cell></row><row><cell>N35</cell><cell>E</cell><cell>399</cell><cell>E</cell><cell cols="2">221 55.39</cell><cell>77</cell><cell>34.84</cell><cell>B</cell><cell></cell><cell>63.41</cell><cell>95</cell><cell>37.55</cell></row><row><cell>N36</cell><cell>E</cell><cell>355</cell><cell>C</cell><cell cols="2">167 47.04</cell><cell cols="2">162 97.01</cell><cell>F</cell><cell></cell><cell>67.32</cell><cell cols="2">183 76.57</cell></row><row><cell>N37</cell><cell>E</cell><cell>547</cell><cell>D</cell><cell>76</cell><cell>13.89</cell><cell>52</cell><cell>68.42</cell><cell>G</cell><cell></cell><cell>48.08</cell><cell cols="2">196 74.52</cell></row><row><cell>N38</cell><cell>O</cell><cell>1127</cell><cell>D</cell><cell cols="2">140 12.42</cell><cell>96</cell><cell>68.57</cell><cell>F</cell><cell></cell><cell>22.36</cell><cell>188</cell><cell>74.6</cell></row><row><cell>N39</cell><cell>E</cell><cell>590</cell><cell>B</cell><cell cols="2">211 35.76</cell><cell cols="2">128 60.66</cell><cell>E</cell><cell></cell><cell>37.46</cell><cell cols="2">151 68.33</cell></row><row><cell>N40</cell><cell>E</cell><cell>533</cell><cell>B</cell><cell>307</cell><cell>57.6</cell><cell cols="2">183 59.61</cell><cell>E</cell><cell></cell><cell>39.21</cell><cell cols="2">145 69.38</cell></row><row><cell>N41</cell><cell>E</cell><cell>672</cell><cell>C</cell><cell cols="2">535 79.61</cell><cell cols="2">403 75.33</cell><cell>B</cell><cell></cell><cell>44.2</cell><cell>99</cell><cell>33.33</cell></row><row><cell>N42</cell><cell>E</cell><cell>1119</cell><cell>C</cell><cell cols="2">400 35.75</cell><cell>340</cell><cell>85.0</cell><cell>B</cell><cell></cell><cell>37.0</cell><cell cols="2">140 33.82</cell></row><row><cell>N43</cell><cell>E</cell><cell>224</cell><cell>E</cell><cell cols="2">122 54.46</cell><cell>67</cell><cell>54.92</cell><cell>C</cell><cell></cell><cell>63.39</cell><cell cols="2">116 81.69</cell></row><row><cell>N44</cell><cell>E</cell><cell>585</cell><cell>C</cell><cell cols="2">432 73.85</cell><cell cols="2">266 61.57</cell><cell>D</cell><cell>55</cell><cell>9.4</cell><cell>34</cell><cell>61.82</cell></row><row><cell>N45</cell><cell>E</cell><cell>1054</cell><cell>E</cell><cell cols="2">262 24.86</cell><cell cols="2">124 47.33</cell><cell>F</cell><cell></cell><cell>39.09</cell><cell cols="2">254 61.65</cell></row><row><cell>N46</cell><cell>E</cell><cell>549</cell><cell>E</cell><cell cols="2">322 58.65</cell><cell>123</cell><cell>38.2</cell><cell>C</cell><cell></cell><cell>62.3</cell><cell cols="2">165 48.25</cell></row><row><cell>N47</cell><cell>E</cell><cell>601</cell><cell>C</cell><cell cols="2">420 69.88</cell><cell cols="2">290 69.05</cell><cell>B</cell><cell></cell><cell>23.63</cell><cell>86</cell><cell>60.56</cell></row><row><cell>N48</cell><cell>E</cell><cell>1075</cell><cell>D</cell><cell>98</cell><cell>9.12</cell><cell>53</cell><cell>54.08</cell><cell>E</cell><cell></cell><cell>22.79</cell><cell cols="2">156 63.67</cell></row><row><cell>N49</cell><cell>E</cell><cell>684</cell><cell>D</cell><cell cols="2">209 30.56</cell><cell>66</cell><cell>31.58</cell><cell>C</cell><cell></cell><cell>32.89</cell><cell cols="2">147 65.33</cell></row><row><cell>N50</cell><cell>E</cell><cell>810</cell><cell>E</cell><cell cols="2">400 49.38</cell><cell>200</cell><cell>50.0</cell><cell>C</cell><cell></cell><cell>59.88</cell><cell cols="2">182 37.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,84.48,81.45,442.85,206.64"><head>Table 2 :</head><label>2</label><figDesc>Organizations participating in the TREC 2003 Novelty Track</figDesc><table coords="9,317.88,98.37,209.45,21.96"><row><cell>Runs submitted</cell></row><row><cell>Run prefix Task 1 Task 2 Task 3 Task 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,74.20,105.39,435.22,214.90"><head>Task 1, Relevant and Novel F Scores</head><label></label><figDesc></figDesc><table coords="14,74.20,138.26,435.22,182.03"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>o +</cell><cell cols="2">Relevant New 2nd Assr Rel 2nd Assr New</cell></row><row><cell>F score</cell><cell>+</cell><cell>+ +</cell><cell>+</cell><cell>+</cell><cell>+ + + +</cell><cell>+</cell><cell>+ +</cell><cell>+ + +</cell><cell>+</cell><cell>+ + + + + + +</cell><cell>+</cell><cell>+</cell><cell>+ + +</cell><cell>+ + + + + + + +</cell><cell cols="2">+ + + + + +</cell><cell>+ + + +</cell></row><row><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell>+ +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell>+ + +</cell><cell>+</cell></row><row><cell></cell><cell cols="17">THUIRnv0315 ISIDSCm203 UIowa03Nov01 THUIRnv0311 MeijiHilF13 MeijiHilF14 UIowa03Nov02 THUIRnv0312 THUIRnv0313 THUIRnv0314 ISIALL03 ISIDSm203 IRITfb1MtmIb MeijiHilF11 MeijiHilF12 IRITf2bis IRITfNegR2 NLPR03n1w1 MeijiHilF15 IRITnb1MtmI4 IRITnip2bis ICT03NOV1DTH ICT03NOV1SQR ICT03NOV1BSL clr03n1n2 NLPR03n1f1 ICT03NOV1NAR ISIRAND03 ICT03NOV1XTD NLPR03n1f2 NLPR03n1w2 clr03n1d NLPR03n1w3 clr03n1n3 ccsumlaqr clr03n1t ccsumrelqr ccsumrelsvd ccsummeoqr ccsummeosvd lexiclone03 NTU11 NTU12 NTU13 NTU14 NTU15 umich1 umbcrun2 umbcrun3 umbcrun1 umich5 umich4 umich3 umich2 ISINONE03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,73.87,132.55,435.78,527.91"><head></head><label></label><figDesc>Figure 7: Scores for Task 2, against a baseline of returning all relevant sentences as novel.</figDesc><table coords="15,73.87,132.55,435.78,527.91"><row><cell></cell><cell></cell><cell cols="44">+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + Return all rel sents: 0.774</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>o + *</cell><cell cols="8">All topics Events Opinions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>+ +</cell><cell>+ + + + +</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell></cell><cell></cell><cell cols="2">NLPR03n2s1</cell><cell>NLPR03n2s2</cell><cell>NTU21</cell><cell cols="2">NTU23</cell><cell cols="2">NTU22</cell><cell cols="2">NLPR03n2d2</cell><cell cols="2">NLPR03n2d1</cell><cell>NLPR03n2d3</cell><cell>ccsumt2pqr</cell><cell cols="2">UIowa03Nov04</cell><cell cols="2">umbcnew2</cell><cell cols="2">clr03n2</cell><cell cols="2">ccsumt2qr</cell><cell>ccsumt2svdqr</cell><cell>ICT03NOV2LPA</cell><cell cols="2">Irito</cell><cell cols="2">MeijiHilF23</cell><cell cols="2">UIowa03Nov03</cell><cell cols="2">MeijiHilF24</cell><cell>ccsum2svdpqr</cell><cell>umbcnew3</cell><cell cols="2">UIowa03Nov05</cell><cell cols="2">IritMtm4</cell><cell cols="2">IritMtm5</cell><cell cols="2">Irit1</cell><cell>Irit5q</cell><cell>MeijiHilF22</cell><cell>MeijiHilF21</cell><cell>THUIRnv0323</cell><cell>ICT03NOV2LPP</cell><cell>THUIRnv0321</cell><cell>THUIRnv0322</cell><cell>ICT03NOV2CUR</cell><cell>ICT03NOV2SQR</cell><cell>ICT03NOV2PNK</cell><cell>UIowa03Nov06</cell><cell>UIowa03Nov07</cell><cell>NTU25</cell><cell>NTU24</cell><cell>umich21</cell><cell>umich22</cell><cell>umich23</cell><cell>umich24</cell><cell>umich25</cell><cell>umbcnew1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="24">Task 3, Relevant and Novel F Scores</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>o +</cell><cell>Relevant New</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F score</cell><cell>0.2 0.4</cell><cell cols="21">+ + + + + + + + + + +</cell><cell></cell><cell>+</cell><cell cols="10">+ + + + +</cell><cell cols="6">+ + +</cell><cell cols="4">+ +</cell><cell>+ +</cell><cell>+ + + + + +</cell><cell>+ +</cell><cell>+</cell><cell>+ + + + +</cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>NLPR03n3d1</cell><cell cols="2">NLPR03n3s1</cell><cell cols="2">NLPR03n3d3</cell><cell cols="2">THUIRnv0331</cell><cell cols="2">NLPR03n3s2</cell><cell cols="2">THUIRnv0332</cell><cell cols="2">THUIRnv0333</cell><cell cols="2">NLPR03n3d2</cell><cell cols="2">THUIRnv0334</cell><cell cols="2">UIowa03Nov08</cell><cell cols="2">UIowa03Nov09</cell><cell cols="2">ICT03NOV3KNS</cell><cell cols="2">umich33</cell><cell cols="2">umich34</cell><cell cols="2">umich32</cell><cell cols="2">umich31</cell><cell cols="2">clr03n3f01</cell><cell cols="2">ICT03NOV3WND</cell><cell cols="2">ICT03NOV3IKK</cell><cell cols="2">ICT03NOV3KNN</cell><cell cols="2">umich35</cell><cell cols="2">clr03n3f02</cell><cell>MeijiHilF31</cell><cell>MeijiHilF32</cell><cell>ccsum3pqr</cell><cell>ccsum3qr</cell><cell>ccsum3svdpqr</cell><cell>ICT03NOV3WN3</cell><cell>clr03n3f03</cell><cell>clr03n3f04</cell><cell>MeijiHilF33</cell><cell>MeijiHilF34</cell><cell>clr03n3f05</cell><cell>NTU34</cell><cell>NTU32</cell><cell>NTU33</cell><cell>NTU35</cell><cell>NTU31</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,331.43,397.29,208.62,9.96;12,331.44,409.29,208.40,9.96;12,331.44,421.17,118.45,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,331.44,409.29,181.01,9.96">From TREC to DUC to TREC again</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,331.44,421.17,96.99,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,331.43,443.61,208.35,9.96;12,331.44,455.61,180.09,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,439.10,443.61,100.68,9.96;12,331.44,455.61,39.75,9.96">TREC novelty track at IRIT-SIG</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dkaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,393.08,455.61,96.99,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,331.43,477.93,208.42,9.96;12,331.44,489.93,208.53,9.96;12,331.44,501.93,208.38,9.96;12,331.44,513.81,201.77,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,507.60,489.93,32.37,9.96;12,331.44,501.93,208.38,9.96;12,331.44,513.81,62.01,9.96">Experiments in novelty, genes and questions at the University of Iowa</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eichmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Arens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sehgal</surname></persName>
		</author>
		<editor>Voorhees and Harman</editor>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,331.43,536.25,208.51,9.96;12,331.44,548.13,208.52,9.96;12,331.44,560.13,188.73,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,384.32,536.25,155.63,9.96;12,331.44,548.13,208.52,9.96;12,331.44,560.13,49.02,9.96">The role and meaning of predicative and non-predicative definitions in the search for information</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">S</forename><surname>Geller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,401.71,560.13,96.99,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,331.43,582.57,208.60,9.96;12,331.44,594.45,208.49,9.96;12,331.44,606.45,208.47,9.96;12,331.44,618.45,208.35,9.96;12,331.44,630.33,208.45,9.96;12,331.44,642.33,118.81,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,410.69,582.57,129.34,9.96;12,331.44,594.45,54.91,9.96">Overview of the TREC 2002 novelty track</title>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,394.76,606.45,145.15,9.96;12,331.44,618.45,208.35,9.96;12,331.44,630.33,102.87,9.96">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002), NIST Special Publication 500-251</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002), NIST Special Publication 500-251<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">November 2002</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,331.43,664.65,208.42,9.96;12,331.44,676.65,208.47,9.96;12,331.44,688.65,40.41,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,465.99,664.65,73.85,9.96;12,331.44,676.65,107.95,9.96">NLPR at TREC 2003: Novelty and robust</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,459.36,676.65,80.56,9.96;12,331.44,688.65,18.83,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.51,74.49,208.54,9.96;13,92.52,86.49,208.50,9.96;13,92.52,98.37,208.27,9.96;13,92.52,110.37,118.33,9.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="13,207.03,98.37,75.45,9.96">UMBC at trec 12</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kallukar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Cost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajavaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shanbhag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bhatkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ogle</surname></persName>
		</author>
		<editor>Voorhees and Harman</editor>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.51,130.29,208.37,9.96;13,92.52,142.29,208.37,9.96;13,92.52,154.17,56.71,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,171.88,130.29,129.00,9.96;13,92.52,142.29,123.90,9.96">Use of metadata for question answering and novelty tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,241.43,142.29,59.46,9.96;13,92.52,154.17,35.25,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.51,174.09,208.28,9.96;13,92.52,186.09,208.36,9.96;13,92.52,197.97,201.89,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,278.73,174.09,22.07,9.96;13,92.52,186.09,208.36,9.96;13,92.52,197.97,62.19,9.96">Meiji university web and novelty track experiements at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ohgaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shimmura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,175.96,197.97,96.99,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.50,218.01,208.38,9.96;13,92.52,229.89,208.25,9.96;13,92.52,241.89,118.33,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,92.52,229.89,188.68,9.96">The University of Michigan at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,92.52,241.89,96.99,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.50,261.81,208.43,9.96;13,92.52,273.69,208.39,9.96;13,92.52,285.69,193.62,9.96" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<title level="m" coord="13,168.44,273.69,132.47,9.96;13,92.52,285.69,172.16,9.96">TREC 2003 novelty and web track at ICT. In Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.50,305.61,208.32,9.96;13,92.52,317.61,208.38,9.96;13,92.52,329.49,208.38,9.96;13,92.52,341.49,56.71,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,284.52,305.61,16.31,9.96;13,92.52,317.61,208.38,9.96;13,92.52,329.49,121.14,9.96">Approach of information retrieval with reference corpus to novelty detection</title>
		<author>
			<persName coords=""><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,240.84,329.49,60.06,9.96;13,92.52,341.49,35.25,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.50,361.41,208.26,9.96;13,92.52,373.41,208.21,9.96;13,92.52,385.29,197.68,9.96" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="13,281.71,361.41,19.06,9.96;13,92.52,373.41,208.21,9.96;13,92.52,385.29,80.73,9.96">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,92.50,405.21,208.46,9.96;13,92.52,417.21,208.37,9.96;13,92.52,429.21,154.91,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,92.52,417.21,208.37,9.96;13,92.52,429.21,14.75,9.96">THUIR at TREC 2003: Novelty, robust and web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,128.98,429.21,96.99,9.96">Voorhees and Harman</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
