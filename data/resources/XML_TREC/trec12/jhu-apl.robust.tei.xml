<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.46,87.91,325.01,12.19">Combining Methods for the TREC 2003 Robust Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,233.46,117.21,63.55,8.74"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>mayfield@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.58,117.21,61.95,8.74"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.46,87.91,325.01,12.19">Combining Methods for the TREC 2003 Robust Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">028182A9609F34F91D219FDAA3E45F42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The Johns Hopkins University Applied Physics Laboratory (JHU/APL) focused on the Robust Retrieval Track at this year's conference. In the past we have investigated the use of alternate methods for tokenization and applied character n-grams, with success, to tasks in ad hoc, filtering, and crosslanguage tracks.</p><p>For ranked retrieval, we have come to rely on a statistical language model to compute query/document similarity values. Hiemstra and de Vries describe such a linguistically motivated probabilistic model and explain how it relates to both the Boolean and vector space models <ref type="bibr" coords="1,227.10,366.09,10.65,8.74" target="#b3">[4]</ref>. The model has also been cast as a rudimentary Hidden Markov Model <ref type="bibr" coords="1,102.68,389.07,10.65,8.74" target="#b6">[7]</ref>. Although the model does not explicitly incorporate inverse document frequency, it does favor documents that contain more of the rare query terms. The similarity measure can be computed as</p><formula xml:id="formula_0" coords="1,73.80,443.57,196.96,27.34">Sim(q,d) = α⋅ f (t,d) + (1-α) ⋅ f (t,C) ( ) t ∈q ∏ f (t,q )</formula><p>Equation 1. Similarity calculation.</p><p>where α is the probability that a query word is generated by a document-specific model, and (1-α) is the probability that it is generated by a generic language model. f(t,C) denotes the mean relative document frequency of term t. We have observed that aggregate performance using this model is fairly insensitive to the precise value of α that is used; however, higher values of alpha tend to result in selecting documents that contain a greater number of the query terms.</p><p>Earlier work on the Query Track, held during the TREC-8 and TREC-9 evaluations, showed that different query formulations could result in substantially different retrieval performance on individual queries (see Buckley <ref type="bibr" coords="1,210.55,684.15,10.53,8.74" target="#b0">[1]</ref>). For example, deleting an important query term, adding an important word that was not initially present, the use of idioms in topic statements, and deleterious effects caused by inappropriate stemming (over or under aggressive) were each shown to demonstrably alter precision on particular topics. Furthermore, multiple reports have appeared in the literature suggesting that a combination of evidence from multiple, disparate approaches can be beneficial (e.g., Savoy <ref type="bibr" coords="1,495.21,266.55,10.52,8.74" target="#b8">[9]</ref>).</p><p>From this we conclude that a scheme based on multiple document representations and multiple similarity metrics might exhibit increased robustness in query performance, and possibly higher aggregate performance as well. How different methods can best be combined is not clear. In this paper we report on our efforts attempting to: (1) merge disparate run files; and (2) devise an automated technique for learning query-specific run weights that can be used to create a single, robust run.</p><p>We built several indices to compare different tokenization methods. We have begun investigating the use of part-of-speech tagging and entity-tagging to transform the term space, with the hope of capturing semantic distinctions (e.g., bat, a noun, versus bat, the verb, or, Washington, the place, instead of Washington, a person); however, we did not use these indices in this year's Robust Track. Summary information for the indices that we used is shown in Table <ref type="table" coords="1,388.46,519.32,3.77,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disparate Retrieval Approaches</head><p>In the previous section we enumerated a number of alternatives to tokenization that resulted in different index data files being created for the collection. In many cases different tokenizations lead to similar overall performance. For example, on the query "health food" the use of case-normalized and unnormalized words should produce similar ranked lists. On the other hand, the query "NRA" will likely produce different results for case-sensitive words and character 4-grams.</p><p>We We were unable to consider each of these, primarily due to a lack of time to implement each method or to empirically evaluate each method with each index. We ended up using seven different indexes and 11 different retrieval methods (listed below). We then sought to combine or select from the 77 runs produced. With some risk of overtraining, we measured our performance on the 150 queries used in the TREC-6, TREC-7, and TREC-8 evaluations. We restricted ourselves to the use of only the 'Description' portion of topic statements; however, we did submit one official run using 'Title', 'Description', and 'Narrative' sections expecting that it would better contribute to the relevance pools.</p><p>For each of the 7 indexes, we created runs that computed document relevance by: o [4 runs] Adjusting alpha values in our statistical language model of retrieval. We considered values of 0.2, 0.5, 0.8, and 0.9. We thought that higher alpha values would lead to better precision at low recall levels. in a binary independence model, both with and without relevance feedback (as described above). We used values of 1.2 for k1, 500 for k3, 0.6 for b, and we assumed that the top 8 documents for each query were relevant and all others were not, for the purpose of term weighting.</p><p>We considered several metrics for robust performance, but chiefly examined the percentage of topics with at least one relevant document in the top 10 ranks (TopTen) and the area under the curve when topical average precision is averaged over a number of the worst scoring topics and plotted as a function of the number of worst topics examined (MAP-Hardest). We focused on the hardest 25% of topics, as suggested in the track guidelines. For whatever number of topics is considered, MAP-Hardest is most effected by the most difficult topics. If the worst 12 topics are examined, then about three-quarters of the weight is given to the hardest 6 topics and only about one-quarter of the weight is given to the next 6 hardest topics. This puts a premium on doing as well as possible on the absolute hardest topics. We also considered high mean average precision (averaged over all topics) to be desirable, but were primarily concerned with 'robust' measures of performance.</p><p>We were interested to know how well each combination might do. In particular, we wondered how much improvement could be obtained given an oracle that could perfectly select the single-best method to apply for each individual query. Using the known relevance judgments for the TREC-6 through TREC-8 topics we determined that a method that selected the single best method (from our set of 77) for each topic could improve each of the performance measures appreciably. If this was not the case, for example, if our different methods did not exhibit large variations in retrieval performance, then any machine-learning approach to select a query-specific method would be doomed to failure.</p><p>Of our 77 runs, the one with the highest mean average precision (over all topics) on the training set used stems as indexing terms with the statistical language model (with alpha=0.2) and with relevance feedback. However, when alpha=0.5 mean average precision was about the same, and the robust measures were improved. We compare these runs and an oracular 'best' run using the robust metrics in Table <ref type="table" coords="3,97.35,338.85,3.76,8.74" target="#tab_3">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting a Single Method</head><p>We would like to predict which tokenization and scoring methods will prove most effective given a particular query. As several years of training data are available for this collection, we are inclined to adopt a supervised learning approach. For this study we applied Support Vector Machines (SVMs) <ref type="bibr" coords="3,255.23,583.11,10.64,8.74" target="#b4">[5]</ref>. For any learning approach to succeed we need to identify features that might discriminate high performing retrieval configurations from low performing ones.</p><p>We the highest ranked and rank (5, 10, 20, 100, 500) documents using the 'stems-lm5-rf' run o Run plus index: the percentage of unique terms to the total number of terms observed in documents from a specified range using the 'stems-lm5-rf' run (ranges considered included 0-10, 11-50, 51-100, and 201-300); the percentage of query terms to total terms observed in the ranked documents of a given range -as described above; and, the mean RIDF value of all terms occurring in the documents of the ranges described above. Using these features we attempted to train a support vector machine with a cubic kernel for each of our 77 runs. For each of the queries in a 100-query training set we used the top 10 scoring runs as positive examples and the bottom scoring runs as negative examples. We hoped the SVM could distinguish methods likely to achieve high mean average precision (i.e., good performance) and those unlikely to do so.</p><p>Unfortunately the SVM was not generally able to learn this distinction. The training algorithm converged, but essentially memorized the data. It may be that our set of features was inadequate and that more semantically laden features are essential to such a task. Or possibly, many more training exemplars are required for this approach to succeed. We next turned our attention to an approach based on combining results from multiple runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging Multiple Methods</head><p>As an alternative to selecting a single method based on features about the query, or weighting several methods predicted to perform well, we examined combination of multiple methods to produce a single ranked list for each topic. Combination of disparate techniques has occasionally led to improved performance in TREC-style evaluations; many consider that the constituent runs should be chosen to maximize orthogonality with respect to one another. That is, it is hoped that they make independent mistakes and that run combination will reinforce selection of good documents and lower the ranks of documents that only appear to be of high quality using a single method. In the past we have used combination to reasonable advantage; we found that combination of n-gram based runs with stem or word runs can confer a 10% relative advantage in mean average precision <ref type="bibr" coords="4,145.02,289.05,10.65,8.74" target="#b5">[6]</ref>.</p><p>Our preferred method for merging multiple runs is to first normalize score values for each individual run and then create an ordering based on these normalized scores. We view scores as masses, and divide individual scores by the sum of the masses of the top k documents (we use k=1000). Because our probabilistic calculations are typically performed in log space, and scores are therefore negative, we achieve the desired effect by using the reciprocal of a document's score as its mass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other Approaches</head><p>We thought up several other schemes that were not implemented for the evaluation.</p><p>One was to build a tagger that processes query words and assigns them to different categories. For example, some words are clearly query-specific, such as "find documents that"; others are modifiers of a key concept, like 'international' in 'international organized crime"; still others are keywords, like 'black bears' in "black bear attacks". This technique is unproven, but recent successes in tagging applications might be applied to the 1000+ available topics from past TREC, CLEF, and NTCIR conferences. Terms in different categories could be treated (e.g., weighted) differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Official Submissions</head><p>We submitted five runs described below. aplrob03a was a combination of two runs, one using stems and one using character 5-grams. Title, Description, and Narrative topic fields were used here, but only the Description field was used for our other official runs. Relevance feedback was applied and alpha was 0.5. We thought that this method would maximize mean average precision over all topics. This run typifies our traditional processing. aplrob03b was designed to maximize the number of topics with a relevant document appearing in the top ranks (say up to rank 20 or so). We filled 'slots' by examining several run files and selecting what we though to be good documents from different methods. Some of the documents were selected based on which run files worked well on the training set; others were based by clustering the top 50 ranked documents (for a given run) and selecting the highest ranking document from each of 5 clusters.</p><p>We used a quadratic implementation of agglomerative clustering that started with the 50 individual documents and repeatedly combined clusters, one at a time. Our distance metric was Sim(c1,c2) + Sim(c2,c1) -Max(cardinality(c1), cardinality(c2)). Our language model similarity metric is not symmetric, so we added the similarity between 'query' and 'document' (both clusters of documents) and the similarity between 'document' and 'query'. We then subtracted the cardinality of the larger of the two clusters; this was done in an attempt to even the distribution of the number of documents per cluster. We also ignored both very common and very rare terms when clustering. This method found a relevant document in the top 10 ranks for 134 of 150 topics in the training set.</p><p>To achieve reasonable performance in mean average precision as well, we then extended this list of topranked documents using run aplrob03d (described below), taking care to remove duplicate documents when building the ranked list. aplrob03c was an attempt to maximize MAP-Hardest. We combined results using all 77 runs as input. On the training data this method performed the best, achieving 0.0103 on the MAP-Hardest metric. This is nearly double the performance obtained with a single, well-performing run, but well below our 0.0364 theoretical maximum. aplrob03d is analogous to aplrob03a, but different in using only the 'Description' portion of the topic statements. Like aplrob03a, we expected this run to achieve good mean average precision over all topics and to serve as a baseline for our other runs.</p><p>Finally, aplrob03e was an overtraining run that sought to optimize TopTen. On the training data, we were able to use the qrels to build a run by selecting the ith rank of the jth run for all topics. This method found a relevant document (in the top 10 ranks) for 143 out of the 150 training topics. Its use on novel data is questionable, but it is possible that the different runs selected represent a method for finding orthogonal methods. The following runs/ranks were employed:</p><p>stems-okapi 1 stems-logreg 3 stems-slm8 5 words-logreg 4 case-words-slm5 3 4-grams-okapi-rf 2 phrases-logreg 3 5-grams-slm8 5 stems-slm5 5 5-grams-okapi 2 4-grams-slm8-rf 5 words-nostop 5 4-grams-slm9 4</p><p>Examining Table <ref type="table" coords="5,149.92,327.39,5.01,8.74" target="#tab_5">3</ref> (below), we observe that run aplrob03c achieved a 0.0040 improvement in MAP-Hardest over our description-only baseline, aplrob03d; this is a 50% increase over the baseline. We interpret this as mild support for the hypothesis that combination of a very large number of methods can improve robustness. For the TopTen measure, run aplrob03e achieved a 90% success rate vs. 78% using our baseline method, a 15.4% relative improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We attempted to determine whether selection or combination of diverse methods can improve the robustness of query processing. Our attempts to select preferred methods dynamically (i.e., on a query-by-query basis) failed; however, we have not fully investigated this line of work. We did discover that combination of 77 runs (7 tokenizations and 11 similarity metrics) led to our best results for the MAP-Hardest measure. A priori selection of several diverse methods seemed to optimize the TopTen measure, though the small number of topics leaves it difficult to determine whether this result is valid. These results are based only on an examination of 'description-only' runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,324.00,542.30,216.01,133.14"><head>Table 1 .</head><label>1</label><figDesc>Index statistics for the Robust Track collection.</figDesc><table coords="1,336.42,576.94,191.09,98.51"><row><cell></cell><cell></cell><cell># terms</cell><cell>index size</cell></row><row><cell>words</cell><cell>w</cell><cell>554751</cell><cell>373 MB</cell></row><row><cell cols="2">words preserving case c</cell><cell>698786</cell><cell>410 MB</cell></row><row><cell>stems (Snowball)</cell><cell>s</cell><cell>455803</cell><cell>320 MB</cell></row><row><cell>4-grams</cell><cell>4</cell><cell>251694</cell><cell>1.39 GB</cell></row><row><cell>5-grams</cell><cell>5</cell><cell>1485406</cell><cell>2.22 GB</cell></row><row><cell>6-grams</cell><cell>6</cell><cell>6030289</cell><cell>3.22 GB</cell></row><row><cell>words + phrases</cell><cell cols="2">p 19141479</cell><cell>1.97 GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,342.00,109.04,198.11,273.88"><head></head><label></label><figDesc>Pseudo relevance feedback was not applied.</figDesc><table coords="2,342.00,118.80,198.11,264.12"><row><cell>o [3 runs] Adjusting alpha values as</cell></row><row><cell>mentioned above; however, relevance</cell></row><row><cell>feedback was applied, selecting 60 'terms'</cell></row><row><cell>(whatever those terms might be (e.g., n-</cell></row><row><cell>grams, words, stems), from 20 top-ranked</cell></row><row><cell>documents. Alpha values of 0.2, 0.5, and 0.8</cell></row><row><cell>were considered. We imagine that it might</cell></row><row><cell>also be useful to use other parameter settings</cell></row><row><cell>for automated feedback, such as, different</cell></row><row><cell>methods for isolating feedback terms,</cell></row><row><cell>different numbers of expansion terms, or</cell></row><row><cell>different numbers of presumed positive and</cell></row><row><cell>negative documents.</cell></row><row><cell>o [1 run] We used our statistical language</cell></row><row><cell>model with alpha = 0.5 without feedback,</cell></row><row><cell>and without stopword removal. Normally we</cell></row><row><cell>represent documents using all terms, but</cell></row><row><cell>omit query terms at run-time that have a</cell></row><row><cell>relative document frequency greater than</cell></row><row><cell>0.2.</cell></row><row><cell>o [1 run] Using the logistic regression retrieval</cell></row><row><cell>model described by UC Berkeley</cell></row><row><cell>o [2 runs] Using Okapi BM25 term weighting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,72.00,361.82,216.08,130.00"><head>Table 2 .</head><label>2</label><figDesc>Comparing an oracle-based run and two high-performing methods on the training set.</figDesc><table coords="3,72.00,384.94,216.08,106.89"><row><cell></cell><cell cols="3">TopTen MAP-Hardest MAP</cell></row><row><cell>stems-lm2-rf</cell><cell>0.7467</cell><cell>0.0052</cell><cell>0.2513</cell></row><row><cell>stems-lm5-rf</cell><cell>0.8067</cell><cell>0.0061</cell><cell>0.2489</cell></row><row><cell>oracular</cell><cell>0.9600</cell><cell>0.0364</cell><cell>0.3587</cell></row><row><cell cols="4">From Table 2 we observe that an oracle-based run</cell></row><row><cell cols="4">can improve mean average precision by</cell></row><row><cell cols="4">approximately 40% and MAP-Hardest by roughly</cell></row><row><cell>600%.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,72.00,453.74,216.04,107.22"><head>Table 3 .</head><label>3</label><figDesc>Performance of officially submitted methods on all 100 topics. Run aplrob03d is a baseline for comparison against other methods.</figDesc><table coords="5,77.28,488.38,205.48,72.59"><row><cell></cell><cell cols="4">Fields TopTen MAP-Hardest MAP</cell></row><row><cell cols="2">aplrob03a TDN</cell><cell>0.8900</cell><cell>0.0238</cell><cell>0.2998</cell></row><row><cell>aplrob03b</cell><cell>D</cell><cell>0.8500</cell><cell>0.0113</cell><cell>0.2522</cell></row><row><cell>aplrob03c</cell><cell>D</cell><cell>0.8200</cell><cell>0.0120</cell><cell>0.2521</cell></row><row><cell>aplrob03d</cell><cell>D</cell><cell>0.7800</cell><cell>0.0080</cell><cell>0.2726</cell></row><row><cell>aplrob03e</cell><cell>D</cell><cell>0.9000</cell><cell>0.0096</cell><cell>0.2535</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,342.02,151.30,198.00,7.85;5,324.00,161.68,184.41,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,392.65,151.30,93.05,7.85">The TREC-9 Query Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,495.98,151.30,44.05,7.85;5,324.00,161.68,175.78,7.85">Proceedings of the Ninth Text REtrieval Conference (TREC-9</title>
		<meeting>the Ninth Text REtrieval Conference (TREC-9</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,342.02,180.20,197.96,9.96;5,324.00,190.58,128.34,9.96;5,452.34,189.23,4.68,6.64;5,461.70,190.58,78.30,9.96;5,324.00,200.90,216.02,9.96;5,324.00,211.22,194.07,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,406.83,182.32,63.73,7.85">One term or two?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,366.18,190.58,86.16,9.96;5,452.34,189.23,4.68,6.64;5,461.70,190.58,78.30,9.96;5,324.00,200.90,216.02,9.96;5,324.00,211.22,119.51,9.96">Proceedings of the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-95)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<meeting>the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-95)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,342.01,234.04,198.01,7.85;5,324.00,244.36,215.86,7.85;5,324.00,254.74,216.05,7.85;5,324.00,265.06,158.76,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,471.17,234.04,68.85,7.85;5,324.00,244.36,215.86,7.85;5,324.00,254.74,73.28,7.85">Full Text Retrieval based on Probabilistic Equations with Coefficients fitted by Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,410.79,254.74,129.26,7.85;5,324.00,265.06,78.47,7.85">Proceedings of the Second Text REtrieval Conference</title>
		<meeting>the Second Text REtrieval Conference</meeting>
		<imprint>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,342.00,285.76,197.97,7.85;5,324.00,296.08,216.04,7.85;5,324.00,306.46,216.10,7.85;5,324.00,316.78,39.17,7.85" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,473.06,285.76,66.91,7.85;5,324.00,296.08,216.04,7.85;5,324.00,306.46,56.35,7.85">Relating the new language models of information retrieval to the traditional retrieval models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<idno>TR-CTIT-00-09</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct coords="5,342.03,337.48,197.95,7.85;5,324.00,347.80,216.00,7.85;5,324.00,358.12,216.01,7.85;5,324.00,368.50,62.62,7.85" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,399.11,337.48,140.87,7.85;5,324.00,347.80,216.00,7.85;5,324.00,358.12,30.94,7.85">Making large-Scale SVM Learning Practical Advances in Kernel Methods -Support Vector Learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<editor>B. Schölkopf and C. Burges and A. Smola</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,341.95,389.14,198.04,7.85;5,324.00,399.52,216.02,7.85;5,324.00,409.84,216.00,7.85;5,324.00,420.16,215.99,7.85;5,324.00,430.54,216.01,7.85;5,324.00,440.86,182.05,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,457.94,389.14,82.05,7.85;5,324.00,399.52,216.02,7.85;5,324.00,409.84,49.73,7.85">JHU/APL Experiments at CLEF-2001: Translation Resources and Score Normalization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,488.14,409.84,51.86,7.85;5,324.00,420.16,215.99,7.85;5,324.00,430.54,216.01,7.85;5,324.00,440.86,47.05,7.85">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum (CLEF-2001)</title>
		<editor>
			<persName><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="193" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,342.03,461.56,198.00,7.85;5,324.00,471.88,215.99,7.85;5,324.00,482.26,216.04,7.85;5,324.00,492.58,216.03,7.85;5,324.00,502.90,145.06,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,535.27,461.56,4.75,7.85;5,324.00,471.88,196.84,7.85">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,324.00,482.26,216.04,7.85;5,324.00,492.58,216.03,7.85;5,324.00,502.90,39.81,7.85">the Proceedings of the 22nd International Conference on Research and Development in Information Retrieval (SIGIR-99)</title>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,342.01,523.60,197.99,7.85;5,324.00,533.92,216.00,7.85;5,324.00,544.30,216.02,7.85;5,324.00,554.62,216.00,7.85;5,324.00,564.94,82.72,7.85" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,519.13,523.60,20.88,7.85;5,324.00,533.92,216.00,7.85;5,324.00,544.30,58.67,7.85">Okapi at TREC-7: automatic ad hoc, filtering, VLC and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,351.26,554.62,188.75,7.85;5,324.00,564.94,41.59,7.85">Proceedings of the Seventh Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference</meeting>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,342.02,585.64,198.02,7.85;5,324.00,595.96,216.00,7.85;5,324.00,606.34,214.28,7.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,393.19,585.64,146.85,7.85;5,324.00,595.96,152.75,7.85">Cross-language information retrieval: experiments based on CLEF 2000 corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,486.74,595.96,53.25,7.85;5,324.00,606.34,101.43,7.85">In Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="115" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
