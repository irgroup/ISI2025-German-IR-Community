<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,202.92,75.91,205.99,12.19">WIDIT in TREC-2003 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,164.40,98.73,49.16,8.74"><forename type="first">Kiduk</forename><surname>Yang</surname></persName>
							<email>kiyang@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science Indiana University Bloomington</orgName>
								<address>
									<postCode>47405</postCode>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,393.66,98.73,58.66,8.74"><forename type="first">Dan</forename><surname>Albertson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Library and Information Science Indiana University Bloomington</orgName>
								<address>
									<postCode>47405</postCode>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,202.92,75.91,205.99,12.19">WIDIT in TREC-2003 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1CDA550506AE8E28E57B67BD53C11D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Web IR experiment of TREC, otherwise known as the Web track, investigated in its initial stages the strategies for the same ad-hoc retrieval task as was done previously with plain text documents. Although many TREC participants explored methods of leveraging non-textual sources of information such as hyperlinks and document structure, the general consensus among the early Web track participants was that link analysis and other non-textual methods did not perform as well as the content-based retrieval methods fine-tuned over the years <ref type="bibr" coords="1,194.34,256.29,91.29,8.74" target="#b10">(Hawking et al., 1999;</ref><ref type="bibr" coords="1,288.60,256.29,87.94,8.74" target="#b11">Hawking et al., 2000;</ref><ref type="bibr" coords="1,379.51,256.29,103.78,8.74" target="#b8">Gurrin &amp; Smeaton, 2001;</ref><ref type="bibr" coords="1,486.26,256.29,35.77,8.74;1,90.00,267.81,62.69,8.74" target="#b15">Savoy &amp; Rasolofo, 2001)</ref>.</p><p>There have been many speculations as to why link analysis, which showed much promise in previous research and has been so readily embraced by commercial Web search engines, did not prove useful in Web track experiments. Most such speculations point to potential problems with Web track's earlier test collections, from the inadequate link structure of truncated Web data <ref type="bibr" coords="1,424.48,313.77,97.55,8.74" target="#b14">(Savoy &amp; Picard, 1998;</ref><ref type="bibr" coords="1,90.00,325.29,102.05,8.74">Singhal &amp; Kazkiel, 2001)</ref>, and relevance judgments that penalize the link analysis by not counting the hub pages as relevant <ref type="bibr" coords="1,161.46,336.81,115.57,8.74" target="#b20">(Voorhees &amp; Harman, 2000)</ref> and boost the content analysis by counting multiple relevant pages from the same site as relevant <ref type="bibr" coords="1,240.59,348.27,105.89,8.74">(Singhal &amp; Kazkiel, 2001)</ref>, to unrealistic queries that are too detailed and specific to be representative of real world Web searches <ref type="bibr" coords="1,333.13,359.79,108.25,8.74" target="#b17">(Singhal &amp; Kaszkiel, 2001)</ref>.</p><p>In an effort to address the criticism and problems associated with the early Web track experiments, TREC abandoned the ad-hoc Web retrieval task in 2002 in favor of topic distillation and named page finding task and replaced its earlier Web test collection of randomly selected Web pages with a larger and potentially higher quality domain-specific collection<ref type="foot" coords="1,313.26,403.63,3.24,5.65" target="#foot_0">1</ref> . The topic distillation task in TREC-2002 is described as finding a short, comprehensive list of pages that are good information resources, and the named page finding tasks is described as finding a specific page whose name is described by the query <ref type="bibr" coords="1,90.00,440.25,121.22,8.74" target="#b9">(Hawking &amp; Craswell, 2002;</ref><ref type="bibr" coords="1,215.40,440.25,116.58,8.74" target="#b5">Craswell &amp; Hawking, 2003)</ref>. Adjustment of the Web track environment brought forth renewed interest in retrieval approaches that leverage Web-specific sources of evidences such as link structure and document structure.</p><p>For the home page finding task, where the objective is to find the entry page of a specific site described by the query, Web page's URL characteristics, such as its type and length, as well as the anchor text of Web page's inlinks proved to be useful sources of information to be leveraged <ref type="bibr" coords="1,469.52,497.79,52.58,8.74;1,90.00,509.25,62.04,8.74" target="#b9">(Hawking &amp; Craswell, 2002)</ref>. In the named page finding task, which is similar to home page finding task except that the target page described by the query is not necessarily the entry point of a Web site but any specific page on the Web, the use of anchor text still proved to be an effective strategy but the use of URL characteristics did not work well as it did in the home page finding task <ref type="bibr" coords="1,340.55,543.75,119.89,8.74" target="#b5">(Craswell &amp; Hawking, 2003)</ref>. In the topic distillation task, anchor text still seemed to be a useful resource, especially as a mean to boost the performance of content-based methods via fusion (i.e. result merging), although the level of its usefulness fell much below that achieved in named page finding tasks <ref type="bibr" coords="1,347.72,578.25,122.09,8.74" target="#b9">(Hawking &amp; Craswell, 2002;</ref><ref type="bibr" coords="1,474.29,578.25,47.78,8.74;1,90.00,589.77,63.93,8.74" target="#b5">Craswell &amp; Hawking, 2003)</ref>. Various site compression strategies, which attempt to select the "best" pages of a given site, was another common theme in the topic distillation task, once again demonstrating the importance of fine-tuning the retrieval system according to the task at hand <ref type="bibr" coords="1,339.26,612.75,81.55,8.74" target="#b0">(Amitay et al., 2003;</ref><ref type="bibr" coords="1,423.48,612.75,72.80,8.74" target="#b24">Zhang et al., 2003)</ref>. It is interesting to note that link analysis (e.g. <ref type="bibr" coords="1,263.91,624.27,115.45,8.74">PageRank, HITS variations)</ref> has not yet proven itself to be an effective strategy and the content-based method seems to be still the most dominant factor in the Web track. In fact, the two best results in TREC-2002 topic distillation task were achieved by the baseline systems that used only the content-based methods <ref type="bibr" coords="1,291.90,658.77,78.86,8.74">(MacFarlane, 2003;</ref><ref type="bibr" coords="1,373.26,658.77,73.96,8.74" target="#b24">Zhang et al., 2003)</ref>.</p><p>In our earlier studies <ref type="bibr" coords="2,225.12,74.61,60.81,8.74" target="#b21">(Yang, 2002a;</ref><ref type="bibr" coords="2,291.91,74.61,56.63,8.74" target="#b22">Yang, 2002b)</ref>, where we investigated various fusion approaches for ad-hoc retrieval using the WT10g collection, we found that simplistic approach that combine the results of content-and link-based retrieval results did not enhance retrieval performance in general. TREC participants in recent Web track environment, however, found that use of non-textual information such as hyperlinks, document structure, and URL could be beneficial for specific tasks such as topic distillation and named/home page finding tasks. We believe that this is not only due to the change in the retrieval environment (i.e. test collection, retrieval task) but also the result of more dynamic approach to combining multiple sources of evidence than straightforward result merging. Thus, our focus in TREC-2003 Web track was in exploring fusion strategies that utilize various information sources in a dynamic manner to optimize retrieval for specific search environment. For our experiment, we used the experimental fusion retrieval system called WIDIT<ref type="foot" coords="2,303.12,187.45,3.24,5.65" target="#foot_1">2</ref> to combine content and link information, and then reranked the combined result based on heuristics arrived at from dynamic system tuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">WIDIT</head><p>Basic approach of WIDIT in the Web track consisted of four main phases: indexing, searching, result merging, and reranking. Indexing phase involved indexing various sources of evidence to generate multiple indexes, which was followed by the searching phase that produced multiple result sets from using different query formulations against multiple indexes. The result sets were combined using weighted sum formula, after which a reranking heuristics were applied to optimize the ranking of the merged results. The overview of WIDIT system architecture is displayed in Figure <ref type="figure" coords="2,341.45,310.35,3.75,8.74">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing Module</head><p>WIDIT preprocessed documents by removing HTML tags and stopwords and applying the simple plural remover (Frakes &amp; Baeza-Yates, 1992)<ref type="foot" coords="2,248.04,359.89,3.24,5.65" target="#foot_2">3</ref> . The stopwords consisted of non-meaningful words such as words in a standard stopword list, non-alphabetical words, words consisting of more than 25 or less than 3 characters, and words that contain 3 or more repeated characters. Hyphenated words were split into parts before applying the stopword exclusion, and acronyms and abbreviations were kept as index terms<ref type="foot" coords="2,484.32,394.39,3.24,5.65" target="#foot_3">4</ref> . In addition to extracting body text terms (i.e. terms between &lt;body&gt; and &lt;/body&gt; tags), WIDIT extracted terms from document title, meta keywords and descriptions, and "emphasized" text (e.g. text with &lt;b&gt;, &lt;em&gt;, &lt;font&gt;, &lt;u&gt;, &lt;h1&gt; tags) as well as extracting terms from the anchor texts of incoming links. Thus, WIDIT created three sets of term indexes: first based on document content (i.e. body index), second based on document structure (header index), and third based on link structure (anchor index).</p><p>In order to enable incremental indexing as well as to scale up to larger collections, each of the indexes consisted of smaller subcolllections, which were created and searched in parallel. The whole collection term statistics were derived after the creation of the subcollections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval Module</head><p>The retrieval component of WIDIT was based on a Vector Space Model (VSM) using the SMART lengthnormalized term weights as was implemented in IRIS <ref type="bibr" coords="2,320.18,540.33,120.27,8.74" target="#b23">(Yang &amp; Maglaughlin, 2000)</ref>. Documents were ranked in decreasing order of the inner product of document and query vectors,</p><formula xml:id="formula_0" coords="2,204.96,585.63,36.83,8.74">, (<label>1</label></formula><formula xml:id="formula_1" coords="2,127.44,575.42,118.24,29.84">) ik t k k i d q ∑ = = 1 T d q</formula><p>where q k is the weight of term k in the query, d ik is the weight of term k in document i, and t is the number of terms in the index. SMART Lnu weights with the slope of 0.3 were used for document terms <ref type="bibr" coords="3,485.34,86.13,36.74,8.74;3,90.00,97.59,49.17,8.74" target="#b3">(Buckley et al., 1997)</ref>, and SMART ltc weights <ref type="bibr" coords="3,254.01,97.59,91.63,8.74" target="#b2">(Buckley et al., 1995)</ref> were used for query terms. Lnu weights attempt to match the probability of retrieval given a document length with the probability of relevance given that length <ref type="bibr" coords="3,159.71,120.63,82.41,8.74" target="#b16">(Singhal et al., 1996)</ref>.</p><p>Two sets of queries, one resulting from simple stop and stemming, and another with phrases, acronyms and abbreviations extracted, were applied against three sets of document indexes<ref type="foot" coords="3,458.76,141.43,3.24,5.65" target="#foot_4">5</ref> to produce six sets of retrieval results<ref type="foot" coords="3,180.24,152.95,3.24,5.65" target="#foot_5">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. WIDIT System Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fusion Module</head><p>In post-retrieval fusion, where multiple sets of search results are combined after retrieval time, two of the most common fusion formulas are Similarity Merge <ref type="bibr" coords="3,308.34,563.07,85.15,8.74" target="#b7">(Fox &amp; Shaw, 1995;</ref><ref type="bibr" coords="3,397.13,563.07,44.45,8.74" target="#b13">Lee, 1997)</ref> and Weighted Sum <ref type="bibr" coords="3,90.00,574.53,87.82,8.74" target="#b1">(Bartell et al., 1994;</ref><ref type="bibr" coords="3,182.85,574.53,71.90,8.74" target="#b18">Thompson, 1990)</ref>. The similarity merge formula multiplies the sum of fusion component scores for a document by the number of fusion components that retrieved the document (i.e. overlap), based on the assumption that documents with higher overlap are more likely to be relevant. Instead of relying on overlap, the weighted sum formula sums fusion component scores weighted with the relative contributions of the fusion components that retrieved them, which is typically estimated based on training data. Both formulas compute the fusion score of a document by a linear combination of fusion component scores.</p><p>In our earlier study <ref type="bibr" coords="4,206.22,74.61,56.71,8.74" target="#b22">(Yang, 2002b)</ref>, similarity merge approach proved ineffective when combining content-and link-based results, so this time we tried three variations of the weighted sum fusion formula, which were shown to be more effective in combining fusion components that are dissimilar <ref type="bibr" coords="4,461.51,97.59,56.22,8.74" target="#b21">(Yang, 2002a)</ref>. Equation (2) describes the simple Weight Sum (WS) formula, which sums the normalized system scores multiplied by system contribution weights. Equation (3) describes the Overlap Weight Sum (OWS) formula, which multiplies the WS score by overlap. Equation (4) describes the Weighted Overlap Weighted Sum (WOWS) formula, which multiplies the WS score by overlap weighted by system contributions:</p><formula xml:id="formula_2" coords="4,118.80,175.33,194.98,14.21">FS WS = ∑(w i *NS i ), (<label>2</label></formula><formula xml:id="formula_3" coords="4,118.80,178.83,198.88,24.94">) FS OWS = ∑(w i *NS i *olp), (<label>3</label></formula><formula xml:id="formula_4" coords="4,118.80,193.05,198.88,24.94">) FS WOWS = ∑(w i *NS i * w i *olp),<label>(4)</label></formula><p>where: FS = fusion score of a document, w i = weight of system i, NS i = normalized score of a document by system i, = (S i -S min ) / (S max -S min ) olp = number of systems that retrieved a given document.</p><p>The normalized document score, NS i , is computed by <ref type="bibr" coords="4,308.56,305.25,119.27,8.74">Lee's min-max formula (1996</ref><ref type="bibr" coords="4,434.74,305.25,21.52,8.74">Lee's min-max formula ( , 1997))</ref>, where S i is the retrieval score of a given document and S max and S min are the maximum and minimum document scores by method i.</p><p>One of the main challenges in using the weighted fusion formula lies in determination of the optimum weights for each system (w i ). We assessed various weight combinations<ref type="foot" coords="4,424.86,353.05,3.24,5.65" target="#foot_6">7</ref> (e.g. 0.9 for body text, 0.08 for header text, 0.01 for anchor text) with the training data of past Web track results to tune our fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Reranking Module</head><p>In order to optimize retrieval performance in top ranks, fusion results were reranked based on combinations of site compression technique and content-link evidence ranking heuristic. The site compression involved clustering results by sites, sorting sites by the highest document score of each site, and floating the top n documents from top m sites to the top ranks. The content-link evidence ranking heuristic consisted of a set of ranking and document score boosting rules arrived at by dynamic tuning process involving interactive retrieval and manual system tuning in real time. The dynamic tuning process was applied to the best single and best fusion systems to "tune" the ranking heuristic.</p><p>The dynamic tuning component of WIDIT produces retrieval results that display individual scores for each source of evidence such as inter/intrasite in/outdegree, phrase/proximity match counts in body/header/anchor texts, and query term matches in URL as well as ranking and retrieval scores before/after fusion and site compression. We performed a series of dynamic tuning sessions using training data, which involved repeated cycles of retrieval and tuning the reranking heuristic based on real time evaluation of retrieval results. In contrast to the static tuning of fusion formulas, dynamic tuning process, though ad-hoc, allows tuning of systems with numerous parameters by leveraging human intelligence. The main components of content-link evidence ranking heuristic we used were inter/intrasite in/outdegree (e.g. boost score if large outdegree for topic distillation, boost score if large indegree for home/named page finding), phrase/proximity match (e.g. boost ranking if phrase match in title or anchor text), and query term match in URL (e.g. boost to top 10 rank if acronym match in URL). The reranking heuristics for home/named page finding task also involved the query classification component, which assigned different emphasis on evidence sources according to the query type. The queries were classified into either named page or home page based on term occurrence patterns observed in training queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Topic Distillation vs. Home/Named Page Finding Tasks</head><p>WIDT systems for topic distillation and home/named page finding runs shared the indexing and searching modules, but used different fusion formulas arrived at from different training data. The reranking module of topic distillation systems employed both site compression and content-link evidence reranking heurisitic, but home/named page finding systems used only the reranking heuristics that included the query classification component and emphasized the acronym matching.</p><p>The training data for topic distillation systems were 2002 topic distillation topics and relevance judgments, and home/named page finding system used 2002 named page finding topics and relevance judgments as training data. Unfortunately, both training data were problematic. The topic distillation task in TREC-2003 was about finding relevant "home pages" given a broad query, which introduced bias towards home page finding approaches not present in the training data. As the name indicates, home/named page finding task in TREC-2003 combined home page and named page finding tasks, only half of which were present in the training data used. In addition to suboptimal training data, the topic distillation systems were trained based on precision at rank 10 (P@10), whereas the main evaluation metric for topic distillation in TREC-2003 was changed to R-precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>In the topic distillation test runs using training data, the best single system was widittdb1 (P@10 = 0.168), which used simple query and body text index. The best fusion runs, which used fusion weights of 0.9, 0.01, 0.09 for body, header, and anchor text respectively, improved the baseline (widittdb1) results by 8% (P@10 = 0.182). The reranking of fusion results improved the baseline results by 17% (P@10 = 0.196). In the named page finding test runs, same baseline system resulted in mean reciprocal rank (MRR) of 0.471, which fusion with weights 0.8, 0.19, and 0.01 improved by 10% (MRR = 0.520) but reranking failed to improve (MRR = 0.471).</p><p>The official run results were comparable to test run results in that fusion improved retrieval performance in both tasks and reranking further enhanced performance in topic distillation runs according to P@10. The official home/named page finding runs, which consisted of the baseline and the best fusion run were comparable to test runs in that the fusion run (MRR = 0.400) improved the baseline result by 10% (MRR = 0.362). The official topic distillation runs showed 21% improvement by fusion (P@10 = 0.092) and 29% improvement by reranking (P@10 = 0.098) over baseline (P@10 = 0.076).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>TREC System Rankings By official TREC system rankings, which ranked all TREC topic distillation runs by R-precision, WIDIT appeared to perform rather poorly when compared with other TREC systems (Table <ref type="table" coords="5,477.97,477.09,3.61,8.74" target="#tab_0">1</ref>). By Rprecision, the best WIDIT run, which was the baseline run, was ranked 71 of 107 systems (18 of 23 groups). When we reranked the systems using P@10, however, WIDIT runs were ranked much higher (Table <ref type="table" coords="5,119.13,511.59,3.62,8.74" target="#tab_1">2</ref>). In fact, the best WIDIT run, which was the reranked fusion run, ranked 28 of 107 systems (12 of 23 groups). When ranked by mean average precision (Table <ref type="table" coords="5,347.17,523.05,3.62,8.74" target="#tab_2">3</ref>), WIDIT ranked 38 of 107 systems (11 of 23 groups). Table <ref type="table" coords="5,168.77,534.57,5.01,8.74" target="#tab_3">4</ref> and<ref type="table" coords="5,194.67,534.57,3.77,8.74" target="#tab_4">5</ref>, which shows system rankings for home/named page finding runs, reflects the poor performance level of WIDIT<ref type="foot" coords="5,227.52,543.91,3.24,5.65" target="#foot_7">8</ref> , which we tried to compensate for in post-submission runs described in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Post-submission Runs</head><p>After receiving the results of the official TREC runs, we conducted post-submission analysis to discover and address the shortcomings of the submitted systems. In retrospect, it was easy to see the negative effects of improper system tuning that resulted from biased training data as well as system overtuning that penalized system rankings based on the different evaluation measure than one used in training. To compensate for these shortcomings, we conducted another cycle of dynamic tuning iterations that involved the adjustments in reranking and query classification heuristics and the implementation of home page identification method. The WIDIT dynamic tuning component was modified to produce ranking by both P@10 and R-precision, so that system tuning could be optimized for both evaluation measures. Home page identification was based on URL typing <ref type="bibr" coords="6,338.79,546.57,74.77,8.74" target="#b19">(Tomlinson, 2003;</ref><ref type="bibr" coords="6,416.30,546.57,74.21,8.74" target="#b12">Kraaij et al., 2002)</ref>, where Web pages are classified into categories of root, subroot, path, and file. The root page was defined as URL with zero slash counts or URL that ends with home page file name (e.g. index.htm, default.htm) and 1 slash count. Subroot page was defined as home page ending with 2 slash count, and path page was defined as home page ending with 3 or more slash count. The file page was defined as URL that meets none of these conditions. Based on the observations from post-submission analysis, which suggested the strong performance of fusion results in top ranks as well as the importance of home page finding approaches, following rank boosting rules were added to the reranking heuristic: The query classification heuristics for home/named page finding task was also improved by adding additional rules such as classifying queries that ends in all capitalized words as home page queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post-submission Results</head><p>The post-submission topic distillation run results that introduced home page finding bias and tuned the system for both P@10 and R-precision improved system ranking by MRP from 71 to 23 (Table <ref type="table" coords="7,480.50,137.85,3.61,8.74" target="#tab_5">6</ref>), system ranking by avgP@10 from 28 to 25 (Table <ref type="table" coords="7,267.51,149.37,3.61,8.74" target="#tab_6">7</ref>), and system ranking by MAP from 38 to 10 (Table <ref type="table" coords="7,489.60,149.37,3.61,8.74" target="#tab_7">8</ref>). The post-submission home/named page finding run results, however, were little different from official results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discussion</head><p>In this year's topic distillation task, there were on the average 10.32 relevant documents per topic (5 topics with 1 relevant document, 3 topics with 2 relevant documents, 20 topics with 5 or fewer relevant documents, 33 topics with 10 or fewer relevant documents). The best WIDIT topic distillation run results had 17 topics with zero P@10, 23 topics with zero R-Precision, and only 4 topics with relevant documents at top 20 ranks. The key question for WIDIT topic distillation runs, therefore, is why they performed so poorly with topics with few relevant documents. Whether this is due to some WIDIT specific factors or it is a TREC system-wide phenomenon remains to be seen.</p><p>In the home/named page finding task, WIDIT exhibited suboptimal performance, which we attribute largely to incomplete training data (e.g. omission of home page finding training data) based on the following observation: the best WIDIT home/named page run had 24 home page topics with first correct answer beyond rank 100, compared to 6 named page topics with first correct answer beyond rank 100.</p><p>Overall, we believe fusion is a promising area of investigation for Web IR. Our results show that exploiting the richness of Web search environment by combining multiple sources of evidence via result merging and dynamic system tuning can enhance retrieval performance in the topic distillation task. As for the home/named page finding task, we suspect our approach was hampered by incomplete training data, which will be investigated in a follow-up study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,118.80,647.29,109.71,12.26;6,118.80,659.47,257.09,12.26;6,118.80,671.71,298.50,12.26;6,118.80,683.95,135.16,12.26"><head>•</head><label></label><figDesc>Keep top 5 ranks static • Boost the rank of potential home pages (root, subroot, path) • Boost the rank of file type page with two or more query terms in URL • Stop if top 20 ranks are filled</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,90.00,186.96,431.58,292.98"><head></head><label></label><figDesc></figDesc><graphic coords="3,90.00,186.96,431.58,292.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,580.59,327.59,74.56"><head>Table 1 .</head><label>1</label><figDesc>Topic Distillation ranking by Mean R-Precision (MRP)</figDesc><table coords="5,90.00,598.29,327.59,56.86"><row><cell></cell><cell>MRP</cell><cell>MAP</cell><cell>avgP@10</cell></row><row><cell>Best TREC system</cell><cell>0.1636</cell><cell>0.1543</cell><cell>0.1240</cell></row><row><cell>Median TREC system</cell><cell>0.0699</cell><cell>0.0896</cell><cell>0.0700</cell></row><row><cell>Best WIDIT system</cell><cell>0.0736</cell><cell>0.1016</cell><cell>0.0760</cell></row><row><cell>Worst TREC system 9</cell><cell>0.0181</cell><cell>0.0250</cell><cell>0.0160</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,74.61,327.57,74.56"><head>Table 2 .</head><label>2</label><figDesc>Topic Distillation ranking by Mean Precision at rank 10 (avgP@10)</figDesc><table coords="6,90.00,92.31,327.57,56.86"><row><cell></cell><cell>MRP</cell><cell>MAP</cell><cell>avgP@10</cell></row><row><cell>Best TREC system</cell><cell>0.1485</cell><cell>0.1387</cell><cell>0.1280</cell></row><row><cell>Best WIDIT system</cell><cell>0.0626</cell><cell>0.0787</cell><cell>0.0980</cell></row><row><cell>Median TREC system</cell><cell>0.0871</cell><cell>0.1057</cell><cell>0.0807</cell></row><row><cell>Worst TREC system</cell><cell>0.0181</cell><cell>0.0250</cell><cell>0.0160</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.00,163.83,327.64,74.62"><head>Table 3 .</head><label>3</label><figDesc>Topic Distillation ranking by Mean Average Precision (MAP)</figDesc><table coords="6,90.00,181.59,327.64,56.86"><row><cell></cell><cell>MRP</cell><cell>MAP</cell><cell>avgP@10</cell></row><row><cell>Best TREC system</cell><cell>0.1636</cell><cell>0.1543</cell><cell>0.1240</cell></row><row><cell>Best WIDIT system</cell><cell>0.0736</cell><cell>0.1016</cell><cell>0.0760</cell></row><row><cell>Median TREC system</cell><cell>0.0699</cell><cell>0.0896</cell><cell>0.0700</cell></row><row><cell>Worst TREC system</cell><cell>0.0230</cell><cell>0.0222</cell><cell>0.0200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,253.11,281.68,74.56"><head>Table 4 .</head><label>4</label><figDesc>Home/Name Page ranking by Mean Reciprocal Rank (MRR)</figDesc><table coords="6,90.00,270.81,251.74,56.86"><row><cell></cell><cell>MRR</cell><cell>avgS@10</cell></row><row><cell>Best TREC system</cell><cell>0.727</cell><cell>89.3</cell></row><row><cell>Median TREC system</cell><cell>0.496</cell><cell>64.3</cell></row><row><cell>Best WIDIT system</cell><cell>0.400</cell><cell>66.3</cell></row><row><cell>Worst TREC system</cell><cell>0.065</cell><cell>8.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.00,342.33,326.63,74.62"><head>Table 5 .</head><label>5</label><figDesc>Home/Name Page ranking by Mean Success Rate at rank 10 (avgS@10)</figDesc><table coords="6,90.00,360.09,251.67,56.86"><row><cell></cell><cell>MRR</cell><cell>avgS@10</cell></row><row><cell>Best TREC system</cell><cell>0.727</cell><cell>89.3</cell></row><row><cell>Median TREC system</cell><cell>0.465</cell><cell>68.3</cell></row><row><cell>Best WIDIT system</cell><cell>0.400</cell><cell>66.3</cell></row><row><cell>Worst TREC system</cell><cell>0.065</cell><cell>8.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,90.00,189.63,327.59,74.56"><head>Table 6 .</head><label>6</label><figDesc>Post-submission Topic Distillation ranking by Mean R-Precision (MRP)</figDesc><table coords="7,90.00,207.33,327.59,56.86"><row><cell></cell><cell>MRP</cell><cell>MAP</cell><cell>avgP@10</cell></row><row><cell>Best TREC system</cell><cell>0.1636</cell><cell>0.1543</cell><cell>0.1240</cell></row><row><cell>Best WIDIT system</cell><cell>0.1139</cell><cell>0.1281</cell><cell>0.0980</cell></row><row><cell>Median TREC system</cell><cell>0.0699</cell><cell>0.0896</cell><cell>0.0700</cell></row><row><cell>Worst TREC system</cell><cell>0.0181</cell><cell>0.0250</cell><cell>0.0160</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,90.00,278.85,377.29,74.56"><head>Table 7 .</head><label>7</label><figDesc>Post-submission Topic Distillation ranking by Mean Precision at rank 10 (avgP@10)</figDesc><table coords="7,90.00,296.55,327.57,56.86"><row><cell></cell><cell>MRP</cell><cell>MAP</cell><cell>avgP@10</cell></row><row><cell>Best TREC system</cell><cell>0.1485</cell><cell>0.1387</cell><cell>0.1280</cell></row><row><cell>Best WIDIT system</cell><cell>0.0626</cell><cell>0.1216</cell><cell>0.0981</cell></row><row><cell>Median TREC system</cell><cell>0.0871</cell><cell>0.1057</cell><cell>0.0807</cell></row><row><cell>Worst TREC system</cell><cell>0.0181</cell><cell>0.0250</cell><cell>0.0160</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,90.00,368.07,353.60,74.62"><head>Table 8 .</head><label>8</label><figDesc>Post-submission Topic Distillation ranking by Mean Average Precision (MAP)</figDesc><table coords="7,90.00,385.83,327.64,56.86"><row><cell></cell><cell>MRP</cell><cell>MAP</cell><cell>avgP@10</cell></row><row><cell>Best TREC system</cell><cell>0.1636</cell><cell>0.1543</cell><cell>0.1240</cell></row><row><cell>Best WIDIT system</cell><cell>0.1139</cell><cell>0.1281</cell><cell>0.0980</cell></row><row><cell>Median TREC system</cell><cell>0.0699</cell><cell>0.0896</cell><cell>0.0700</cell></row><row><cell>Worst TREC system</cell><cell>0.0230</cell><cell>0.0222</cell><cell>0.0200</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,95.28,690.22,426.70,7.85;1,90.00,700.54,432.12,7.85;1,90.00,710.92,237.52,7.85"><p>Current test collection of the Web track (i.e. .GOV) consists of 1.25 million Web pages (19 gigabytes) from .gov domain, which is larger, less diverse and likely to be of higher quality than the previous collection (i.e. WT10g), which was a 10 gigabyte subset of the Web crawl from Internet Archive.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,95.82,646.84,426.18,7.85;2,90.00,657.16,432.07,7.85;2,90.00,667.54,432.12,7.85;2,90.00,677.86,235.20,7.85"><p>WIDIT (Web Information Discovery Integrated Tool; http://widit.slis.indiana.edu/), which extends IRIS research (http://ils.unc.edu/iris/) at the University of North Carolina, is an experimental IR system with a suite of modular retrieval tools designed for easy integration of multiple Web IR approaches. WIDIT is currently being developed in the School of Library and Information Science at Indiana University.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,95.28,689.20,426.65,7.85;2,90.00,699.58,77.05,7.85"><p>The simple plural remover was chosen to speed up indexing time and to minimize the overstemming effect of more aggressive stemmers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,95.28,710.92,415.80,7.85"><p>Acronym and abbreviation identification was based on simple pattern matching of punctuations and capitalizations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,95.28,678.88,426.77,7.85;3,90.00,689.20,375.18,7.85"><p>Body text index consisted of title and body text terms. Anchor text index consisted of title and inlink anchor text terms. Header text index consisted of title, meta keywords and descriptions, and emphasized text terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,95.28,700.54,426.70,7.85;3,90.00,710.92,298.67,7.85"><p>In practice, retrieval for each document index consisted of parallel searches of 46 subcollections using the whole collection term weights, whose results were merged and sorted by document score.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,95.28,699.40,321.78,7.85"><p>Eight weight combinations for each fusion formula (24 systems per task) were examined.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,95.70,686.13,426.35,8.74;5,90.00,697.59,163.39,8.74"><p>WIDIT fusion run ranked 48 of 75 systems (13 of 19 groups) by MRR, and 41 of 75 (13 of 19 groups) systems by mean success rate at rank 10.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="5,95.76,710.13,328.58,8.74"><p>Outlier was excluded to keep the system performance comparisons in perspective.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,90.00,98.73,421.11,8.74;8,90.00,110.25,330.50,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,371.43,98.73,139.68,8.74;8,90.00,110.25,26.47,8.74">Topic Distillation with Knowledge Agents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,123.36,110.25,200.24,8.74">Proceedings of the11th Text Retrieval Conference</title>
		<meeting>the11th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2003. 2002</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,133.23,427.90,8.74;8,90.00,144.75,396.91,8.74;8,90.00,156.27,39.17,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,311.00,133.23,206.91,8.74;8,90.00,144.75,29.88,8.74">Automatic combination of multiple ranked retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Bartell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,126.66,144.75,360.25,8.74;8,90.00,156.27,35.25,8.74">Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,179.25,415.90,8.74;8,90.00,190.71,119.22,8.74;8,209.16,188.53,5.76,5.65;8,217.44,190.71,169.96,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,331.43,179.25,174.47,8.74;8,90.00,190.71,119.22,8.74;8,209.16,188.53,5.76,5.65;8,217.44,190.71,102.29,8.74">Automatic query expansion using SMART: TREC 3. Proceeding of the 3 rd Text Rerieval Conference</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,213.75,428.25,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,278.33,213.75,239.92,8.74">Using query zoning and correlation within SMART: TREC</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.77,225.21,87.33,8.74;8,181.14,223.03,5.04,5.65;8,188.70,225.21,189.27,8.74" xml:id="b4">
	<monogr>
		<title level="m" coord="8,102.54,225.21,78.56,8.74;8,181.14,223.03,5.04,5.65;8,188.70,225.21,146.24,8.74">Proceeding of the 5 th Text REtrieval Conference (TREC-5)</title>
		<meeting>eeding of the 5 th Text REtrieval Conference (TREC-5)</meeting>
		<imprint>
			<biblScope unit="page" from="105" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,248.25,399.18,8.74;8,489.18,246.07,5.04,5.65;8,496.74,248.25,17.14,8.74;8,90.00,259.71,172.24,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,238.61,248.25,156.78,8.74">Overview of the TREC-2002 web track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,401.64,248.25,87.54,8.74;8,489.18,246.07,5.04,5.65;8,496.74,248.25,17.14,8.74;8,90.00,259.71,139.51,8.74">Proceedings of the 11 th Text Retrieval Conference (TREC 2002)</title>
		<meeting>the 11 th Text Retrieval Conference (TREC 2002)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="86" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,282.75,415.19,8.74;8,90.00,294.21,147.22,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,293.52,282.75,207.50,8.74">Information retrieval: Data structures &amp; algorithms</title>
		<editor>Frakes, W. B., &amp; Baeza-Yates, R.</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,317.25,423.03,8.74;8,90.00,328.71,128.90,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,229.14,317.25,133.89,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,371.94,317.25,141.09,8.74;8,90.00,328.71,85.87,8.74">Proceeding of the3rd Text Rerieval Conference (TREC-3)</title>
		<meeting>eeding of the3rd Text Rerieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,351.69,402.91,8.74;8,90.00,363.21,121.35,8.74;8,211.38,361.03,5.04,5.65;8,218.94,363.21,187.75,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,238.00,351.69,254.92,8.74;8,90.00,363.21,31.24,8.74">Dublin City University experiments in connectivity analysis for TREC-9</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,128.95,363.21,82.41,8.74;8,211.38,361.03,5.04,5.65;8,218.94,363.21,105.01,8.74">Proceedings of the 9 th Text Retrieval Conference</title>
		<meeting>the 9 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,386.19,401.61,8.74;8,491.64,384.01,5.04,5.65;8,499.20,386.19,17.21,8.74;8,90.00,397.71,169.71,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,241.22,386.19,156.90,8.74">Overview of the TREC-2001 web track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,404.16,386.19,87.45,8.74;8,491.64,384.01,5.04,5.65;8,499.20,386.19,17.21,8.74;8,90.00,397.71,85.32,8.74">Proceedings of the 10 th Text Retrieval Conference</title>
		<meeting>the 10 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2002. 2001</date>
			<biblScope unit="page" from="25" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,420.69,429.37,8.74;8,90.00,432.21,129.15,8.74;8,219.18,430.03,5.04,5.65;8,226.74,432.21,114.45,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,370.68,420.69,148.69,8.74;8,90.00,432.21,40.16,8.74">Results and challenges in web search evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thistlewaite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,136.68,432.21,82.47,8.74;8,219.18,430.03,5.04,5.65;8,226.74,432.21,71.65,8.74">Proceedings of the 8 th WWW Conference</title>
		<meeting>the 8 th WWW Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,455.19,392.64,8.74;8,90.00,466.71,82.47,8.74;8,172.50,464.53,5.04,5.65;8,180.06,466.71,187.82,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,337.09,455.19,141.80,8.74">Overview of the TREC-8 web track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,90.00,466.71,82.47,8.74;8,172.50,464.53,5.04,5.65;8,180.06,466.71,105.07,8.74">Proceedings of the 8 th Text Retrieval Conference</title>
		<meeting>the 8 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="131" to="148" />
		</imprint>
	</monogr>
	<note>TREC-8</note>
</biblStruct>

<biblStruct coords="8,90.00,489.69,420.22,8.74;8,90.00,501.21,415.07,8.74;8,90.00,512.67,65.02,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,299.39,489.69,210.84,8.74;8,90.00,501.21,25.51,8.74">The Importance of Prior Probabilities for Entry Page Search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,124.74,501.21,380.33,8.74;8,90.00,512.67,35.25,8.74">Proceedings of the 25th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,535.65,385.25,8.74;8,90.00,547.17,311.65,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,167.74,535.65,172.13,8.74">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,349.14,535.65,126.11,8.74;8,90.00,547.17,269.31,8.74">Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,570.15,431.34,8.74;8,90.00,581.67,133.05,8.74;8,223.08,579.49,5.04,5.65;8,230.64,581.67,187.65,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,212.73,570.15,308.61,8.74;8,90.00,581.67,44.06,8.74">Report on the TREC-8 Experiment: Searching on the Web and in Distributed Collections</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,140.58,581.67,82.47,8.74;8,223.08,579.49,5.04,5.65;8,230.64,581.67,104.93,8.74">Proceedings of the 8 th Text Retrieval Conference</title>
		<meeting>the 8 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
	<note>TREC-8</note>
</biblStruct>

<biblStruct coords="8,90.00,604.65,423.12,8.74;8,90.00,616.17,130.83,8.74;8,220.86,613.99,5.04,5.65;8,228.42,616.17,187.65,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,226.54,604.65,286.58,8.74;8,90.00,616.17,42.00,8.74">Report on the TREC-9 experiment: Link-based retrieval and distributed collections</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,138.36,616.17,82.47,8.74;8,220.86,613.99,5.04,5.65;8,228.42,616.17,104.93,8.74">Proceedings of the 9 th Text Retrieval Conference</title>
		<meeting>the 9 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="579" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,639.15,423.73,8.74;8,90.00,650.61,340.65,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,278.48,639.15,156.26,8.74">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,441.30,639.15,72.43,8.74;8,90.00,650.61,309.59,8.74">Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,673.65,419.59,8.74;8,90.00,685.11,204.74,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,236.03,673.65,206.66,8.74">A case study in Web search using TREC algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kaszkiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,449.34,673.65,60.25,8.74;8,90.00,685.11,162.00,8.74">Proceedings of the 11th International WWW Conference</title>
		<meeting>the 11th International WWW Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,74.61,431.59,8.74;9,90.00,86.13,333.85,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,182.45,74.61,339.14,8.74;9,90.00,86.13,97.04,8.74">A combination of expert opinion approach to probabilistic information retrieval, part 1: The conceptual model</title>
		<author>
			<persName coords=""><forename type="middle">P</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,196.62,86.13,157.25,8.74">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="382" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,109.11,411.16,8.74;9,90.00,120.63,307.28,8.74" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="9,180.62,109.11,320.54,8.74;9,90.00,120.63,264.37,8.74">Robust, Web and Genomic Retrieval with Hummingbird SearchServer at TREC 2003. The 12th Text REtrieval Conference (TREC 2003) Notebook</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="372" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,143.61,430.99,8.74;9,90.00,155.13,5.01,8.74;9,95.04,152.95,5.04,5.65;9,102.60,155.13,172.75,8.74" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,238.34,143.61,200.83,8.74">Overview of the Eighth Text Retrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,445.98,143.61,75.01,8.74;9,90.00,155.13,5.01,8.74;9,95.04,152.95,5.04,5.65;9,102.60,155.13,144.67,8.74">Proceedings of the 8 th Text Retrieval Conference (TREC-8)</title>
		<meeting>the 8 th Text Retrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,178.11,400.96,8.74;9,90.00,189.63,365.46,8.74" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="9,167.54,178.11,323.42,8.74">Combining Text-, Link-, and Classification-based Retrieval Methods to Enhance</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002a</date>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina)</orgName>
		</respStmt>
	</monogr>
	<note>Information Discovery on the Web. (Doctoral Dissertation</note>
</biblStruct>

<biblStruct coords="9,90.00,212.61,416.28,8.74;9,90.00,224.07,10.05,8.74;9,100.08,221.89,5.04,5.65;9,110.10,224.07,196.60,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,165.61,212.61,256.65,8.74">Combining Text-and Link-based Retrieval Methods for Web IR</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,431.34,212.61,74.94,8.74;9,90.00,224.07,10.05,8.74;9,100.08,221.89,5.04,5.65;9,110.10,224.07,102.17,8.74">Proceedings of the 10 th Text Rerieval Conference</title>
		<meeting>the 10 th Text Rerieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2002b. 2001</date>
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,247.11,304.93,8.74;9,394.98,244.93,5.04,5.65;9,402.54,247.11,102.15,8.74;9,90.00,258.57,80.38,8.74" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="9,241.68,247.11,60.76,8.74">IRIS at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Maglaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,312.48,247.11,82.46,8.74;9,394.98,244.93,5.04,5.65;9,402.54,247.11,102.15,8.74;9,90.00,258.57,37.27,8.74">Proceedings of the 8 th Text Rerieval Conference (TREC-8)</title>
		<meeting>the 8 th Text Rerieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="645" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,90.00,281.61,429.75,8.74;9,90.00,293.07,168.81,8.74;9,258.84,290.89,5.04,5.65;9,266.34,293.07,201.92,8.74" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="9,424.39,281.61,95.36,8.74;9,90.00,293.07,74.49,8.74">THU TREC 2002: Web Track Experiments</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.42,293.07,87.39,8.74;9,258.84,290.89,5.04,5.65;9,266.34,293.07,105.02,8.74">Proceedings of the 11 th Text Retrieval Conference</title>
		<meeting>the 11 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="591" to="594" />
		</imprint>
	</monogr>
	<note>TREC 2002</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
