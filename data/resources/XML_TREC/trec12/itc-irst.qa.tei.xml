<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,227.88,109.47,140.75,12.30;1,222.36,125.19,151.78,12.30">ITC-irst at TREC-2003: the DIOGENE QA system</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.76,150.82,69.45,8.72"><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
							<email>kouylekov@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo</settlement>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.44,150.82,75.35,8.72"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
							<email>magnini@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo</settlement>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.88,150.82,53.60,8.72"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
							<email>negri@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo</settlement>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.68,150.82,54.89,8.72"><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
							<email>tanev@itc.it</email>
							<affiliation key="aff0">
								<orgName type="department">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo</settlement>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,227.88,109.47,140.75,12.30;1,222.36,125.19,151.78,12.30">ITC-irst at TREC-2003: the DIOGENE QA system</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB2F68192C6F646488AE1F5C46391F68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a new version of the DIOGENE Question Answering (QA) system developed at ITC-Irst. The recent updates here presented are targeted to the participation to TREC-2003 and meet the specific requirements of this year's QA main task. In particular, extending the backbone already developed for our participation to the last two editions of the QA track, special attention was paid to deal with the principal novelty factors of the new challenge, namely the introduction of the so-called definition and list questions. Moreover, we experimented with a first attempt to integrate parsing as a deeper linguistic analysis technique to find similarities between the syntactic structure of the input questions and the retrieved text passages. The outcome of such experiments, as well as the variations of the system's architecture and the results achieved at TREC-2003 will be presented in the following sections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The new version of DIOGENE described in this paper results from recent improvements to the well-tested backbone built in the framework of our participation to the last two editions of the TREC QA main task (see <ref type="bibr" coords="1,202.56,445.66,106.77,9.67" target="#b2">Magnini et al., 2001 and</ref><ref type="bibr" coords="1,312.48,445.66,96.34,9.67" target="#b3">Magnini et al., 2002a)</ref> and to the first edition of the multiple language QA track at CLEF 2003 <ref type="bibr" coords="1,318.24,458.02,85.56,9.67" target="#b5">(Negri et al., 2003)</ref>. This year, due to the availability of a relatively stable and reliable version of the system, most of the work concentrated on handling the new question classes introduced to complicate the TREC QA main task, namely the definition and list questions. To this aim, a specific module for definition questions (e.g. "Who is Aaron Copland?", "What are fractals?") has been created, which relies both on a set of specific hand-crafted answer patterns, and on the evaluation of the answer candidates through Web-based statistical techniques. Furthermore, as for list questions (e.g. "Which past and present NFL players have the last name of Johnson?"), the system was tuned by considering as correct answers all the candidates ranked over an experimentally determined threshold by the statistical answer validation component already described in <ref type="bibr" coords="1,305.76,568.78,96.35,9.67" target="#b3">(Magnini et al., 2002a)</ref>. Besides the ad hoc improvements specifically targeted to the TREC-2003 QA main task, some preliminary experiments were also carried out with the long-term goal of integrating parsing as a core technique to improve system's performance. More specifically, such integration is intended to improve part of the DIOGENE's basic components that usually fail when dealing with particular kinds of questions. For instance, Answer-Type Identification will benefit from the capability of finding more precisely the head of the (sometimes rather complex) NPs that follow the WH-word, as in "What Boston Red Sox infielder was given his father's first name, with the letters reversed?", or "What country singer's first album was titled "Storms of Life"?". Moreover, the introduction of parsing in the QA loop is a crucial step to refine the whole Answer Extraction process. With regard to this issue, while in the last year's version of DIOGENE this process was carried out only by considering the presence in a paragraph of named entities matching the answer type category, in the new version of the system we tried to consider the syntactic similarity between the input questions and the retrieved text passages as a further clue for candidate answers' selection. Being the extraction of answer candidates a critical issue, and one of the weakest modules in last year's version of DIOGENE, our experiments on parsing were mainly focused on this direction. The expected result was not only the improvement of system's performance over the types of questions it was already capable to deal with, but also some improvement over types of questions that the previous version of the system could not handle at all. As an example, this is the case of the very frequent (and apparently simple) questions whose answer is not a named entity (such as "What instrument did Louis Armstrong play?", and "What color hair did Thomas Jefferson have before gray?") and the "HOW-DID" questions (such as "How did Jimi Hendrix die?"), which represent a challenging direction for future developments. Starting from these general premises, this paper will mainly describe the novelties and the experiments carried out to develop this year's version of DIOGENE. In particular, after a short overview of the system's architecture (Section 2), we will focus on the new module developed to handle definition questions (Section 3), and on the experiments carried out to use parsing as a technique to refine the answer extraction process (Section 4). Finally, Section 5 will conclude the paper presenting the results achieved by DIOGENE at TREC-2003, as well as some final remarks about strengths and weaknesses of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DIOGENE's Architecture</head><p>The overall system's architecture (depicted in Figure <ref type="figure" coords="2,335.16,366.22,4.49,9.67">1</ref>) relies on the rather standard threecomponents backbone used for the participation to the last two editions of the TREC QA main task. Such a backbone relies on a question processing component, which is in charge of the linguistic analysis of input questions, a search component, which performs the query composition and the document retrieval, and an answer extraction component, which extracts the final answer from the retrieved text passages (see <ref type="bibr" coords="2,246.84,427.78,98.38,9.67" target="#b3">(Magnini et al., 2002a)</ref> for further details). Within this rather conservative framework, the automatic answer validation technique developed last year still plays a crucial role. The algorithm, fully described in <ref type="bibr" coords="2,380.28,452.50,97.45,9.67" target="#b3">(Magnini et al., 2002a)</ref>, relies on discovering relations between a question and the answer candidates by mining the Web or a large text corpus for their co-occurrence tendency. Summarizing, the answer validation process is carried out through the following steps:</p><p>1. Compute the set of representative keywords Kq and Ka both from the question and the answer. 2. From the extracted keywords compute a set of validation patterns (i.e. textual expressions where the question and the answer keywords co-occur closely). 3. Submit the validation patterns to the Web. 4. Estimate an answer relevance score (ARS) considering the number of retrieved documents. The ARS is calculated on the basis of the number of hits (i.e. retrieved pages) by means of a statistical co-occurrence metric called corrected conditional probability <ref type="bibr" coords="2,404.16,600.22,94.83,9.67" target="#b4">(Magnini et al., 2002b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= =</head><p>Such a general formula had to be specified to deal with the new kinds of questions presented in this year's edition of the TREC QA main task. In particular, while factoid questions were handled with the original ARS calculation formula, list questions required the experimental definition of a relative threshold to select a larger number of answers, and definition questions required the development of ad-hoc validation patterns. While the experimental setting of a threshold to capture relevant answers to an input list question is a relatively easy task, let's focus on the more interesting and challenging issue of answering definition questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answering Definition Questions</head><p>In this year's TREC QA main task competition 10% of the questions belonged to the type definition. However, according to the evaluation scheme adopted, these questions contributed to the overall score up to 25%, thus forcing participants to invest on this particular aspect of research in QA. Our strategy relies on using patterns to extract the best text fragments where definitions are likely to occur (we call them "definition fragments") and then going to the Web to measure the co-occurrence between the question focus (i.e. the entity for which a definition is sought, such as "golden parachute" in the question "What is a golden parachute?") and the most important part of the definition (i.e. the so-called "definition core"), usually represented by an NP contained in the definition fragment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Extraction and ranking of DEFINITION-FRAGMENTs</head><p>At the beginning of the process we use a small set of manually defined lexical patterns to extract and rank definition-like fragments. Being these patterns weighted, our technique resorts to calculating a score for every candidate fragment summing the weights from the matching. For instance, the most used patterns for the extraction of candidate DEFINITION-FRAGMENTs are the following:</p><formula xml:id="formula_0" coords="3,87.96,703.90,329.48,22.03">FOCUS {"who"|"what"|"which"} {"is"|"was"} DEFINITION-FRAGMENT nonPrep FOCUS {"is"|"was"} DEFINITION-FRAGMENT nonPrep FOCUS (DEFINITION-FRAGMENT) nonPrep FOCUS, DEFINITION-FRAGMENT DEFINITION-FRAGMENT "known as" FOCUS DEFINITION-FRAGMENT "called" FOCUS</formula><p>where "DEFINITION-FRAGMENT" stands for the part we take for further processing and "nonPrep" stands for any word which is not a preposition. We also used, as further clues, the presence of hypernyms of the focus and words from the WordNet gloss, in case the focus is found in WordNet hierarchy. The following step consists of sorting all the DEFINITION-FRAGMENTs according to their score and passing them to the next module which is in charge of querying the Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Extraction and validation of definition cores (DEF-COREs)</head><p>At this stage of the process, we consider as possible appropriate answers to a definition question all the noun phrases that appear close to the question focus in the DEFINITION-FRAGMENTs retrieved from the document collection. For example, given the DEFINITION-FRAGMENT: "… The Italian skier Alberto Tomba won the World Cup in 1993…" the corresponding candidate answer phrases will be "Italian skier" and "World Cup". We think that such noun phrases, to which we refer with the term DEF-CORE, represent the core of a good definition or at least an introduction to it. DEF-COREs are extracted from the candidate passages by means of the shallow parser Scoll <ref type="bibr" coords="4,248.40,367.06,60.38,9.67" target="#b0">(Abney, 1996)</ref>. Once the DEF-CORES have been extracted, their validation (i.e. the calculation of the corresponding Answer Relevance Score) is carried out automatically by means of the statistical answer validation technique outlined in Section 2. However, such technique allows that only one simple validation pattern, namely the generic pattern [Kq NEAR Ka], is considered. By definition, this pattern will lead to the number of pages where the keywords from the question (Kq) appear close to the keywords from the answer (Ka). However, for each specific question type it is possible to define one or more validation patterns which are much more efficient than the generic validation pattern. In particular, for the definition questions we can use the following list of more precise validation patterns (where Kq and Ka have been respectively substituted with FOCUS and DEF-CORE):</p><p>FOCUS "is" {""|"a"|"an"|"the"} DEF-CORE FOCUS "was" {""|"a"|"an"|"the"} DEF-CORE FOCUS "means" {""|"a"|"an"|"the"} DEF-CORE FOCUS "stands for" {""|"a"|"an"|"the"} DEF-CORE FOCUS "known as" {""|"a"|"an"|"the"} DEF-CORE These patterns are intended to present the typical lexical context used by an English speaker to introduce common notions when giving a definition for an entity. Moreover, even though some of them show a limited applicability with respect to some possible definition questions (e.g. patterns like "means" and "stands for" can not be applied to validate questions whose focus is a person name), all of them are completely domain independent. During the development we considered also other kinds of patterns, but we decided not to use them as they didn't bring enough statistically relevant improvements to the performance. In order a DEF-CORE to be taken into consideration, at least for one of the patterns the search engine should return relevant documents; this way, the number of pages where the focus and the DEF-COREs co-occurr is the combined number of the pages for all the patterns. During the validation, the DEF-COREs are striped from determiners and are tested with all possible determiners. Often the DEF-COREs contain too many adjectives that make them receive zero score when retrieving relevant documents. In this case we calculate the score for any of the sub-phrases of the DEF-CORE and take the maximum score obtained. In light of these assumptions, the Answer Relevance Score (ARS) measure is specified in the following way: </p><formula xml:id="formula_1" coords="5,487.32,187.64,8.73,9.38">es</formula><formula xml:id="formula_2" coords="5,297.84,171.92,136.61,33.74">* ) ( * ) ( )) , (<label>(</label></formula><formula xml:id="formula_3" coords="5,109.56,168.51,307.36,37.94">) ( ) | ( ) ( 3 / 2 3 / 2 - - = - - = -</formula><p>which gives a score to the candidate DEF-COREs through the statistic measure of their cooccurrence with the focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion, comments and future work</head><p>The proposed algorithm for extracting answers to definition questions gave us rather promising results. Even though the overall score for the definition part of the question set was not very high, with an average F score of 0.317 for the fifty definition questions of the competition, we think that the evaluation scheme that we have presented gives an appropriate framework for answering such type of questions. Our analysis of the results shows that on 76% of the questions the system has provided a correct answer. The major problems came from the relatively low recall (only 38% with respect to the vital nuggets selected by the NIST assessors as ideal answers). This is probably due to the fact that the methodology that we presented is more oriented towards canonical definition, rather than important facts and events related to the question focus which was the main objective of the TREC organizers when creating the appropriate nuggets. This leads us to the general conclusion that we need a more linguistically oriented approach, more focused on deep analysis of candidate answers. Another problem of our approach is related to the velocity with which the search engine returns the relevant documents. However our opinion is that using a large source of information as the Web is important to extract good answers to definition questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment: Adding Parsing in the QA Loop</head><p>This year, we integrated in DIOGENE's architecture an algorithm for graph matching between syntactic structures in order to add structural-semantic criteria to the answer validation process which up to now was entirely based on techniques exploiting the Web redundancy. For every candidate answer, the graph matching algorithm gives a score which reflects structural, lexical and semantic similarities between its syntactic context and the question. The main assumption behind the use of a parser for answer validation is that often the question and the syntactic context of the answer have similar structures. Resorting to this assumption, besides our short-term goal of improving the answer validation process, our experiments represent a preliminary step towards the long-term goal of dealing with questions whose answer is not a named entity. Given two parse trees, the main scope of the graph matching algorithm here presented is to find the best mapping among the two, considering similarities among their lexical content. In the following explanation we will present the graph matching algorithm using as an example the question-answer_passage pair: Question #1920: "When was 'Cold Mountain' written?" Answer passage: "When the 'Cold Mountain' began rising to the top of bestseller lists in 1997…".</p><p>Our algorithm proceeds through the following steps: 0. The syntactic structures both of the question Q(V Q ,E Q ) (vertices V Q and edges E Q ) and the candidate answer passage, CA(V CA ,E CA ), are found. To this aim, we use dependency parse trees produced by the RASP <ref type="bibr" coords="6,277.92,145.42,117.58,9.67">(Carrol and Briscoe, 2002)</ref> parsing toolkit (in reality the structures are not trees but rather directed acyclic graphs). 1. An association graph is built A(V A ,E A ) with a set of vertices V A and edges E A . Such association graph generalizes the structure of both input graphs -Q and CA.</p><p>1.1. Every vertex from the association graph A has two corresponding vertices from both graphs Q and CA. From any two vertices Mountain" generate in the corresponding association graph a vertex with the same label. Moreover, being both of them verbs, the vertices "written" from Q and "rising" from CA generate a vertex labeled with "V" in A. In the model we have adopted, two words which have the same part of speech can also be generalized if they have a common hypernym in WordNet.</p><formula xml:id="formula_4" coords="6,123.00,216.92,385.53,34.76">Q Q V v ∈ and CA CA V v ∈ we may form a vertex A A V v ∈ if the</formula><p>1.2 We put an edge between any two vertices 1 A v and 2 A v from V A if their corresponding vertices from Q and CA are linked in the same direction. Formally this means:</p><formula xml:id="formula_5" coords="6,125.16,396.46,384.62,39.90">CA Q Q CA CA CA A A A CA Q A CA Q A E v v AND E v v E v v THEN v v generalize v AND v v generalize v IF ∈ ∈ ⇔ ∈ = = ) ,<label>( ) , ( ) , ( ) , ( ) , ( 2</label></formula><formula xml:id="formula_6" coords="6,136.92,398.53,323.36,28.94">1 2 1 2 1 2 2 2 1 1 1</formula><p>2. We have defined a function weight which gives a score to every syntactic structure obtained via generalization of two structures. In this way we can define for every substructure of A, a weight which is based on the number of edges and vertices and the labels they have. The best match is defined as the highest weighted sub-graph of A in which no vertices share common corresponding vertex in Q or CA. Considering the table in Figure <ref type="figure" coords="6,500.52,490.66,3.98,9.67" target="#fig_0">2</ref>, the last condition can be formulated in the following way: all the vertices in the matching sub-graph of A have to be distributed on different columns and rows. We call the resulting sub-graph the best matching graph of Q and CA.</p><p>3. From the question form we define the possible syntactic position of the exact answer (in Figure <ref type="figure" coords="6,136.92,564.46,5.37,9.67" target="#fig_0">2</ref> the position is denoted with X). If the candidate exact answer matches the position X, then it takes the score of the matching between Q and CA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Calculating the weights</head><p>As it was mentioned before, when two vertices are generalized in one vertex in the association graph, we assign a generalizing label to this association vertex. According to the differences in the labels of the vertices which have been generalized, we assign a score to the association vertex.</p><p>For every vertex in the association graph ) , (</p><formula xml:id="formula_7" coords="6,274.32,673.13,107.69,16.06">CA Q A v v generalize v =</formula><p>we calculate its weight by means of the following heuristically defined parameters: Using these definitions, we can define the weight of any syntactic sub-graph A'(V',E') of the association graph A(V,E) in the following way: Finally, we calculate the weight of every candidate answer passage by calculating the weight of the best matching graph between the sentence where it appears and the question. The algorithm gives this score only if the candidate answer matches the answer position in the question (denoted by X in Figure <ref type="figure" coords="8,153.48,157.78,3.86,9.67" target="#fig_0">2</ref>).</p><formula xml:id="formula_8" coords="7,121.92,401.18,210.41,34.50">∈ ∈ = ' 2 ' ) ( . ) ( ) ' , ' ( ' ( V v E e v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Problems and discussion on the syntactic graph matching</head><p>We did not have enough time to precisely define the parameters of the syntactic graph matching described so far; therefore, the application of the syntactic validation criteria gave no improvement in the overall result. Although, we did not perform complete error analysis, the following weak points of the current implementation can be pinpointed: 1. We are still not able to identify the position of the expected answer (X in Figure <ref type="figure" coords="8,476.16,274.18,4.43,9.67" target="#fig_0">2</ref>) with enough precision. 2. Calculation of the weights is not refined and parameters are only intuitively defined. For example, it would be much better to define the weight of the vertices considering their IDF value in a corpus. 3. Syntactic and lexical transformations can be integrated in the algorithm in order to make the matching of the structures more flexible (for instance, considering nominalization of verbs and active-passive transformations could improve our method). 4. Anaphora and ellipsis should be resolved before applying the syntactic structure matching; unfortunately, these techniques were not implemented in this version. 5. We did not normalize the question by translating it in affirmative form; this also influences the precision of the approach. However, our empirical observations show that structural similarities between the question and the candidate answer passages often exist, and can be identified by inexact graph matching techniques. Therefore, fine tuning of the parameters of the matching algorithm will be necessary to identify these similarities and use them to improve both our answer ranking criteria and the overall answer extraction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Conclusion</head><p>DIOGENE's performance has been evaluated over the three runs submitted to the TREC-2003 QA main task (see Table <ref type="table" coords="8,179.04,544.30,3.82,9.67" target="#tab_5">1</ref>). As for the 413 factoid questions, the total number of wrong (W), unsupported (U), inexact (X) and right (R) answers, together with the overall accuracy, the precision and the recall of recognizing NIL answers are reported for each run. Results for the list and the definition questions are only presented in terms of the average F-measure scores achieved over the total number of questions (respectively 37 and 50).</p><p>All these results have been obtained using the same overall architecture, but varying the validation technique to answer factoid and list questions. In particular: irstqa2003w, our best run, has been obtained relying on the Web as the unique resource to accomplish automatic evaluation as in last year's best performing version of DIOGENE.</p><p>irstqa2003p results from the combination of the scores provided by the Web-based answer validation methodology and the graph-matching technique described in Section 4. Unfortunately, this did not bring any improvement to the system's performance. This is probably due to the weaknesses of the approach already mentioned in the same section. Nevertheless, in light of our empirical observations, parsing and other deeper linguistic analysis techniques (e.g. anaphora resolution, temporal and spatial reasoning, etc.) are deemed necessary to deal with the general QA problem and, more specifically, with the increasing difficulty level of the TREC competition.</p><p>irstqa2003d, surprisingly the worst result, has been obtained combining the Web-based answer validation technique with metrics that take into account the density of the query keywords within the retrieved passages. Our surprise, partially motivated by the higher difficulty of this year's TREC questions, comes from the fact that the same combined validation technique proved to be the most successful in the recent multiple-language QA track at CLEF-2003 (see <ref type="bibr" coords="9,258.12,367.06,120.32,9.67">Negri et al. 2003 for details)</ref>.</p><p>A general conclusion that can be drawn in light of these results is that statistical approaches are relatively easy to implement and prove to be effective for some of the QA subtasks such as answer validation, allowing systems to reach reasonable performances with a limited effort. However, as they are limited to the statistically relevant knowledge that we can acquire from the Web or from an off-line corpus, deeper linguistic techniques seem to be a crucial step towards higher flexibility, coverage, and effectiveness of any QA system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,209.40,719.38,177.58,9.67"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Matching among syntactic trees</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,123.00,239.22,385.53,47.50"><head></head><label></label><figDesc>lexical or part-of-speech tag labels on Q</figDesc><table coords="6,123.00,239.22,385.53,47.50"><row><cell>v and CA v are consistent</cell></row><row><cell>and can be generalized. In such case the label on A v is a generalization of the labels on</cell></row><row><cell>Q v and CA v . For example, in Figure 2 the vertices from Q and CA labeled with "Cold</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,87.96,106.76,425.02,239.86"><head></head><label></label><figDesc>We define the weights of the edge e A in the association graph as:</figDesc><table coords="7,90.00,106.76,422.98,239.86"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">•</cell><cell cols="6">, 300</cell><cell cols="2">if</cell><cell cols="5">the</cell><cell>lexical</cell><cell>part</cell><cell>of</cell><cell>v</cell><cell>C</cell><cell>A</cell><cell>and</cell><cell>v</cell><cell>Q</cell><cell>is</cell><cell>equal</cell><cell>and</cell><cell>they</cell><cell>both</cell><cell>represent</cell><cell>names</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">•</cell><cell cols="6">, 100</cell><cell cols="2">if</cell><cell cols="5">the</cell><cell>lexical</cell><cell>part</cell><cell>of</cell><cell>v</cell><cell>CA</cell><cell>and</cell><cell>v</cell><cell>Q</cell><cell>is</cell><cell>equal</cell><cell>and</cell><cell>it</cell><cell>is</cell><cell>not</cell><cell>a</cell><cell>name</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">•</cell><cell cols="5">100</cell><cell cols="2">*</cell><cell>k</cell><cell cols="2">,</cell><cell cols="3">if</cell><cell>v</cell><cell>CA</cell><cell>and</cell><cell>v</cell><cell>Q</cell><cell>have</cell><cell>a</cell><cell>common</cell><cell>hypernym</cell><cell>in</cell><cell>WordNet</cell></row><row><cell>weight</cell><cell>(</cell><cell>v</cell><cell>A</cell><cell>)</cell><cell>=</cell><cell cols="15">and is k case hypernym this in</cell><cell>words number the d generalize from defined the</cell><cell>of</cell><cell>vertices</cell><cell>between</cell><cell>the</cell><cell>common</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">•</cell><cell>1</cell><cell cols="3">, 5 .</cell><cell></cell><cell cols="2">if</cell><cell cols="2">v</cell><cell cols="3">CA</cell><cell>and</cell><cell>v</cell><cell>Q</cell><cell>have</cell><cell>the</cell><cell>same</cell><cell>part</cell><cell>of</cell><cell>speech</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">•</cell><cell cols="3">, 0</cell><cell>if</cell><cell></cell><cell></cell><cell cols="6">none</cell><cell>of</cell><cell>the</cell><cell>above</cell><cell>conditions</cell><cell>hold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>•</cell><cell cols="3">, 2</cell><cell cols="2">if</cell><cell cols="6">both</cell><cell cols="3">associatio</cell><cell>n</cell><cell>vertices</cell><cell>it</cell><cell>connects</cell><cell>represent</cell><cell>a</cell><cell>lexical</cell><cell>item</cell><cell>which</cell><cell>is</cell><cell>not</cell></row><row><cell cols="4">e weight A (</cell><cell>)</cell><cell>=</cell><cell>• a</cell><cell cols="14">-only word one stop if , 1</cell><cell>of</cell><cell>the</cell><cell>vertices</cell><cell>is</cell><cell>a</cell><cell>lexical</cell><cell>item</cell><cell>which</cell><cell>is</cell><cell>not</cell><cell>a</cell><cell>stop</cell><cell>-</cell><cell>word</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>•</cell><cell cols="2">, 0</cell><cell></cell><cell cols="2">if</cell><cell></cell><cell cols="6">none</cell><cell cols="2">of</cell><cell>the</cell><cell>above</cell><cell>conditions</cell><cell>hold</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,87.96,569.38,424.01,79.27"><head>Table 1 :</head><label>1</label><figDesc>DIOGENE at TREC-2003    </figDesc><table coords="8,87.96,569.38,424.01,66.68"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Factoid</cell><cell></cell><cell></cell><cell>List</cell><cell>Definition</cell><cell>FINAL</cell></row><row><cell></cell><cell>W</cell><cell>U</cell><cell>X</cell><cell>R</cell><cell>Accuracy</cell><cell>NIL</cell><cell>NIL</cell><cell>Average</cell><cell>Average</cell><cell>SCORE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prec.</cell><cell>Rec.</cell><cell>F</cell><cell>F</cell></row><row><cell>irstqa2003w</cell><cell>300</cell><cell cols="2">10 6</cell><cell>97</cell><cell cols="3">0.235 0.121 0.267</cell><cell>0.076</cell><cell>0.317</cell><cell>0.216</cell></row><row><cell>irstqa2003p</cell><cell>305</cell><cell cols="2">11 5</cell><cell>92</cell><cell cols="3">0.223 0.132 0.167</cell><cell>0.074</cell><cell>0.315</cell><cell>0.209</cell></row><row><cell>irstqa2003d</cell><cell>343</cell><cell cols="2">4 4</cell><cell>62</cell><cell cols="3">0.150 0.111 0.067</cell><cell>0.067</cell><cell>0.318</cell><cell>0.171</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.96,513.94,420.59,9.67;9,99.12,526.30,110.62,9.67" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,140.52,513.94,180.24,9.67">Partial Parsing via Finite-State Cascades</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,330.36,513.94,178.19,9.67;9,99.12,526.30,79.53,9.67">Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop</title>
		<meeting>the ESSLLI &apos;96 Robust Parsing Workshop</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,538.66,420.69,9.67;9,99.12,550.90,10.65,9.67;9,109.80,548.66,5.33,6.14;9,118.92,550.90,389.61,9.67;9,99.12,563.14,31.13,9.67" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,192.12,538.66,227.21,9.67">High Precision Extraction of Grammatical Relations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,427.32,538.66,81.33,9.67;9,99.12,550.90,10.65,9.67;9,109.80,548.66,5.33,6.14;9,118.92,550.90,314.71,9.67">Proceedings of the 19 th International Conference on Computational Linguistics (COLING-2002)</title>
		<meeting>the 19 th International Conference on Computational Linguistics (COLING-2002)<address><addrLine>Taipei; Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,575.50,420.56,9.67;9,99.12,587.86,409.49,9.67;9,99.12,600.22,31.13,9.67" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,294.24,575.50,214.29,9.67;9,99.12,587.86,29.25,9.67">Multilingual Question/Answering: the DIOGENE System</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,137.04,587.86,280.28,9.67">Proceedings of the Tenth Text Retrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text Retrieval Conference (TREC-10)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,612.46,420.70,9.67;9,99.12,624.70,409.43,9.67;9,99.12,637.06,236.57,9.67" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,324.48,612.46,184.18,9.67;9,99.12,624.70,183.53,9.67">Mining Knowledge from Repeated Cooccurrences: DIOGENE at TREC-2002</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,296.52,624.70,212.03,9.67;9,99.12,637.06,108.99,9.67">Proceedings of the Eleventh Text Retrieval Conference (TREC-2002)</title>
		<meeting>the Eleventh Text Retrieval Conference (TREC-2002)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,649.42,420.45,9.67;9,96.72,661.78,252.45,9.67;9,349.20,659.54,5.21,6.14;9,357.24,661.78,151.29,9.67;9,96.72,674.02,305.09,9.67" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,318.72,649.42,189.69,9.67;9,96.72,661.78,151.46,9.67">Is It the Right Answer? Exploiting Web Redundancy for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,255.48,661.78,93.69,9.67;9,349.20,659.54,5.21,6.14;9,357.24,661.78,151.29,9.67;9,96.72,674.02,182.86,9.67">Proceedings of the 40 th Annual Meeting of the Association for Computational Linguistics (ACL-2002)</title>
		<meeting>the 40 th Annual Meeting of the Association for Computational Linguistics (ACL-2002)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.96,686.38,420.71,9.67;9,99.12,698.62,409.37,9.67;9,99.12,710.86,68.45,9.67" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,245.88,686.38,262.79,9.67;9,99.12,698.62,48.43,9.67">Bridging Languages for Question Answering: DIOGENE at CLEF-2003</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.24,698.62,293.65,9.67">Proceedings of the Cross Language Evaluation Forum (CLEF-2003)</title>
		<meeting>the Cross Language Evaluation Forum (CLEF-2003)<address><addrLine>Trondheim; Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
