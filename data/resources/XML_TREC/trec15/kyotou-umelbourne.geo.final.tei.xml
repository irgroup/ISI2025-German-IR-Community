<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.06,115.29,421.85,15.13;1,245.60,137.21,120.81,15.13">Combining Vector-Space and Word-based Aspect Models for Passage Retrieval</title>
				<funder ref="#_WrfXj8W">
					<orgName type="full">MEXT (Ministry of Education, Culture, Sports, Science and Technology), Japan</orgName>
				</funder>
				<funder ref="#_zDy5qj7">
					<orgName type="full">Bioinformatics Education Program &quot;Education and Research Organization for Genome Information Science&quot;</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Research Council&apos;s Center for Perceptive and Intelligent Machines in Complex Environments</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.52,171.01,71.10,10.51;1,294.61,169.15,1.41,7.44"><forename type="first">Raymond</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bioinformatics Center</orgName>
								<orgName type="department" key="dep2">Institute for Chemical Research</orgName>
								<orgName type="institution">Kyoto Univer-sity</orgName>
								<address>
									<postCode>611-0011</postCode>
									<settlement>Gokasho, Uji</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.96,171.01,41.95,10.51"><forename type="first">Ngoc</forename><surname>Vo</surname></persName>
						</author>
						<author>
							<persName coords="1,367.89,171.01,20.59,10.51;1,388.48,169.15,1.88,7.44"><surname>Anh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The Uni-versity of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.21,184.96,90.63,10.51;1,290.79,183.09,1.41,7.44"><forename type="first">Ichigaku</forename><surname>Takigawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bioinformatics Center</orgName>
								<orgName type="department" key="dep2">Institute for Chemical Research</orgName>
								<orgName type="institution">Kyoto Univer-sity</orgName>
								<address>
									<postCode>611-0011</postCode>
									<settlement>Gokasho, Uji</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.14,184.96,92.64,10.51;1,411.79,183.09,1.41,7.44"><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bioinformatics Center</orgName>
								<orgName type="department" key="dep2">Institute for Chemical Research</orgName>
								<orgName type="institution">Kyoto Univer-sity</orgName>
								<address>
									<postCode>611-0011</postCode>
									<settlement>Gokasho, Uji</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.06,115.29,421.85,15.13;1,245.60,137.21,120.81,15.13">Combining Vector-Space and Word-based Aspect Models for Passage Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B75580793DA8223BF7A89107FBC7789E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report summarizes the work done at Kyoto University and the University of Melbourne for the TREC 2006 Genomics Track. The single task for this year was to retrieve passages from a biomedical document collection. We devised a system made of two parts to deal with this problem. The first part was an existing IR system based on the vector-space model. The second part was a newly developed probabilistic word-based aspect model for identifying passages within relevant documents (or paragraphs).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Kyoto University and the University of Melbourne participated together for the TREC 2006 Genomics Track. The task was to retrieve passages from a full-text HTML collection of biomedical journals. A "passage" was defined as a sequence of words which do not cross any paragraph boundaries.</p><p>We developed a system comprised of two main parts for passage retrieval. The first is an information retrieval (IR) system which constructs an index for the document collection and returns results according to given queries. The IR system uses a vector-space model (VSM).</p><p>The second part employs a probabilistic word-based aspect model (AM) to score a query against a passage by including both exact and inexact word matches. The following example exemplifies the motivation for our work on this part. If word w 1 occurs often with w 2 and w 2 is usually found near w 3 , then any two of these words should contribute some positive value to the overall passage score. However, of the three possible scores, the score of w 1 and w 3 should be the lowest. Thus, we assign a score to two words even if they do not match exactly but are usually found within the same paragraph. A preliminary "training" phase is required to derive these scores. In the absence of a separate training set, we used the test collection itself for this purpose.</p><p>These two parts of our system are illustrated in Figure <ref type="figure" coords="1,341.45,601.41,5.45,9.59">1</ref> and separated between off-line processing Figure <ref type="figure" coords="1,103.20,614.96,4.39,9.59">1</ref>(a) and on-line processing Figure <ref type="figure" coords="1,256.14,614.96,16.72,9.59">1(b)</ref>.</p><p>This report is structured as follows. A description of our system is provided in the next section, with particular emphasis on our use of the aspect model since it is new. Then, in Section 3, we give information on our three officially submitted runs. Ten additional runs were performed just prior to and after TREC in order to compare with the submitted results. These runs are reported next in Section 4. Finally, we summarize our findings in Section 5. Figure <ref type="figure" coords="2,103.52,208.44,4.24,9.59">1</ref>: Our system includes a vector-space model and an aspect model, which are both required for offline pre-processing and on-line querying. The purpose of the dashed lines are described later in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>A description of each part of our system follows. Before beginning with the vector-space model, some points relevant to the entire system (both the vector-space model and the aspect model) are covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System-wide Features</head><p>Since certain biological terms appear in various combinations of mixed upper/lower case and hyphenation, we applied biological term normalization with other techniques commonly found in information retrieval systems.</p><p>First, we restrict words to contain either alphabetic characters or digits, but not a mix of both. Punctuation characters such as hyphens are considered to be non-word characters by our system. As a result, both "Nurr-77" and "Nurr77" are separated into the two words "Nurr" and "77". Furthermore, every part of our system makes use of case-folding and stemming using the Lovins algorithm <ref type="bibr" coords="2,441.01,425.73,63.18,9.59" target="#b7">[Lovins, 1968]</ref>. All of these transformations are performed on both the query terms and the document collection during both index creation and querying.</p><p>A second feature which we call synonym expansion was applied only to query terms. Synonym expansion combines existing information in the query and several external databases to derive lists of words which are similar to the query term. Each query was provided as two parts: a gene and a biological function<ref type="foot" coords="2,506.80,491.46,3.99,7.01" target="#foot_0">1</ref> . First, if an alternate term is given in parentheses, then it is assumed to be a synonym. For example, the gene for query #162 was given as "APC (adenomatous polyposis coli)". Then, additional synonyms were obtained by looking up certain on-line databases.</p><p>If the term is a gene then it was first expanded using the Biomedical abbreviation server<ref type="foot" coords="2,473.59,545.66,3.99,7.01" target="#foot_1">2</ref>  <ref type="bibr" coords="2,480.94,547.67,59.05,9.59;2,72.00,561.22,23.48,9.59" target="#b2">[Chang et al., 2002]</ref>. The first abbreviation which was an exact match and scored as "Excellent" was returned. In addition, gene names were also expanded using Entrez Gene <ref type="bibr" coords="2,307.79,574.77,96.75,9.59" target="#b8">[Maglott et al., 2005]</ref> from the NCBI web site<ref type="foot" coords="2,518.40,572.75,3.99,7.01" target="#foot_2">3</ref> . If multiple candidates were available, the first one was selected. As for biological functions, they were all expanded using the Medical Subject Headings (MeSH terms) <ref type="bibr" coords="2,339.91,601.87,86.59,9.59" target="#b9">[Nelson et al., 2004]</ref>, also from the NCBI web site in the same way.</p><p>Since synonym expansion relied on multiple sources, duplicates in the enlarged query were removed. Synonym expansion can increase the number of words in each query greatly, depending on the query and the number of synonyms found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vector-space Model</head><p>The vector-space model (VSM) is employed in the first stage of our retrieval process. The actual version of VSM used is the impact-based ranking approach. Like in the traditional VSM, the approach represents a text item (where an item can be a document, a paragraph, or a query) by a vector in n-dimension space, where n is the number of distinct terms in the collection.</p><p>There are, however, some differences between impact ranking and traditional VSM. First, in the impact ranking approach, all vector coordinates are integers between 1 and 8 (as oppose to floating point values in traditional VSM). Second, the similarity score between two vectors is now calculated as the inner vector product, but not as the cosine of the angle between them, as in conventional practice. The motivations behind these differences as well as the details of the retrieval approach, are described in <ref type="bibr" coords="3,423.96,205.05,99.88,9.59" target="#b0">Anh and Moffat [2005]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deriving Scores</head><p>The aspect model (also, latent semantic analysis) has been proposed by others to associate words to documents <ref type="bibr" coords="3,100.35,269.40,91.70,9.59" target="#b6">[Hofmann et al., 1998</ref>]. In particular, the aspect model maps words and documents to a k-dimensional space using the singular value decomposition (SVD) of co-occurrence tables. By selecting k such that it is less than the number of words or documents, we end up with both words and documents being related to each other through the k latent states, or clusters. Probabilistic latent semantic analysis (PLSA) <ref type="bibr" coords="3,492.36,310.05,47.56,9.59;3,72.00,323.60,25.45,9.59" target="#b5">[Hofmann, 2001]</ref> adds a probabilistic model to earlier work by employing an iterative approach using the Expectation-Maximization (EM) algorithm <ref type="bibr" coords="3,207.41,337.15,98.88,9.59" target="#b3">[Dempster et al., 1977]</ref>.</p><p>In these earlier works, the starting point was a co-occurrence table of documents against words. If m denotes the number of documents and n refers to the number of unique words in the collection, then this implies an m by n matrix. In our work, we modify it so that we have an n by n table. This resembles earlier work in the field of document clustering <ref type="bibr" coords="3,249.98,391.34,115.58,9.59" target="#b1">[Borko and Bernick, 1962]</ref>, except that we retain the methodology employed by Hofmann. So, the score between two different words w x and w y is the sum of their scores across all k clusters, where k n:</p><formula xml:id="formula_0" coords="3,220.76,445.13,319.24,33.36">p(w x , w y ) = ∑ z∈Z p(w x |z)p(w y |z)p(z),<label>(1)</label></formula><p>In this formula, Z is the set of clusters. The parameters of this aspect model can be estimated using the EM algorithm by iterating between the following E-step and M-step:</p><formula xml:id="formula_1" coords="3,88.94,531.18,451.06,49.83">E-step: p(z|w x , w y ) = p(w x |z)p(w y |z)p(z) ∑ z ∈Z p(w x |z )p(w y |z )p(z )<label>(2)</label></formula><p>M-step:</p><formula xml:id="formula_2" coords="3,193.78,605.80,346.22,65.31">p(w x |z) = p(w y |z) = ∑ wy∈W n(w x , w y )p(z|w x , w y ) (3) p(z) = ∑ wx∈W ∑ wy∈W n(w x , w y )p(z|w x , w y ), (<label>4</label></formula><formula xml:id="formula_3" coords="3,535.76,649.08,4.24,9.59">)</formula><p>where W is the set of unique words in the document collection and n(w x , w y ) is the number of cooccurrences of w x and w y within a paragraph across the entire collection. Initial values are generated at random using a uniform distribution.</p><p>The output from the word-based aspect model (AM) is a set of scores p(w x , w y ) such that p(w x , w y ) = p(w y , w x ), where w x = w y . For the remainder of this report, we denote these scores as co-occurrence scores, as opposed to co-occurrence counts (n(w x , w y )). The score of a word with itself was purposely excluded from our aspect model so that it could be defined as some constant, as explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Passage Extraction</head><p>Since the document collection is in HTML format, HTML "p" tags were used to separate each document into paragraphs. However, rules were needed to further divide paragraphs into sections. In our implementation, punctuation marks indicated the boundary between two sections.</p><p>Passages were formed from one or more consecutive sections and each was scored against a query through the pair-wise comparison of words. We denote a passage of s words as r 1 , ..., r s and a query of t words as q 1 , ..., q t . The score between two words r i and q j (for</p><formula xml:id="formula_4" coords="4,233.06,234.72,288.13,52.92">1 ≤ i ≤ s and 1 ≤ j ≤ t) is defined as: score(r i , q j ) = { m if r i = q j m if r i = q j .</formula><p>(5)</p><p>When two words match exactly, then a constant score of m is contributed to the final score for that passage. In the case of a non-match, the score for that passage still increases, but by m. If the co-occurrence scores obtained through the score derivation step of the aspect model is used, then m = p(r i , q j ). Thus, m is constant, while m varies depending on the co-occurrence scores of the two words. Several things are worth noting with respect to Equation ( <ref type="formula" coords="4,243.75,351.33,3.86,9.59">5</ref>). As m increases in size compared to p(r i , q j ) the effectiveness of the co-occurrence scores diminishes. If m = 0, then the score for the passage is essentially a count of the number of query terms that appear in the passage.</p><p>In our experiments, we investigated several values for m and m. For m, we evaluated three values ranging from m = p max to m = 1, where p max is the maximum score across all word-pairs. Based on how the scores are derived from probabilities, it is obvious that p max 1. As for m, we also considered the case of m = 0.</p><p>The formula to score an entire passage r against the query q is: score</p><formula xml:id="formula_5" coords="4,218.26,472.47,317.50,35.60">(r, q) = ∑ s i α w i × ( ∑ s i ∑ t j (score(r i , q j ))) s × t . (<label>6</label></formula><formula xml:id="formula_6" coords="4,535.76,490.64,4.24,9.59">)</formula><p>As Equation ( <ref type="formula" coords="4,152.85,518.42,4.24,9.59" target="#formula_5">6</ref>) shows, we adjusted the scoring mechanism so that the length of the query and the passage under consideration are taken into account. Moreover, a parameter α w i is calculated on a paragraphby-paragraph basis as follows:</p><formula xml:id="formula_7" coords="4,130.70,570.50,409.30,65.28">α w i = frequency of w i in paragraph P words in paragraph P × ( 1 -log number of documents document frequency of w i ) (7) = f P,w i |P | × ( 1 -log |D| f D,w i ) . (<label>8</label></formula><formula xml:id="formula_8" coords="4,535.76,617.01,4.24,9.59">)</formula><p>Our motivation for this definition of α w i was to use the passage and query lengths, the word frequency in the current paragraph, and the IDF factor to affect the scoring mechanism. Furtherwork is necessary in order to determine the merits of the terms in Equation ( <ref type="formula" coords="4,316.57,674.23,3.86,9.59" target="#formula_7">8</ref>). As a first step, a few additional runs were done after TREC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions</head><p>Three official runs were submitted to TREC. Ten additional ones were executed prior to and after TREC which were based on the submitted run which performed the best. Details about these additional runs are deferred to the next section.</p><p>We applied two types of indexing with the vector-space model for the submitted runs: document-level and paragraph-level indexing. For the document-level, each relevant article is expanded into its constituent paragraphs, prior to passage extraction. This is indicated as a dashed line in Figure <ref type="figure" coords="5,443.98,275.74,16.72,9.59">1(b)</ref>. The VSM score attributed to each paragraph is equal to the score of the document from which it is taken. For paragraphlevel indexing, the constituent paragraphs of every article are fed into the VSM for indexing, as shown by the dashed line in Figure <ref type="figure" coords="5,187.67,316.39,16.23,9.59">1(a)</ref>. The output from the VSM to the passage extraction (PE) system is a set of relevant paragraphs. Regardless of the indexing level used, the PE system of the aspect model obtains a set of paragraphs and outputs a list of relevant passages. The score of each passage for final ranking is determined by combining the score given by the VSM and the PE.</p><p>As part of the Genomics Track this year, the collection of 162,259 documents included the set of 12,641,127 legal spans (see <ref type="bibr" coords="5,196.52,384.14,83.07,9.59" target="#b4">Hersh et al. [2006]</ref> for further details). A legal span is a section of text which excludes any HTML "p" tags. Thus, a legal span is equivalent to the paragraphs described above.</p><p>Score derivation by the aspect model was fixed so that the same set of scores is used for all of our runs. The space required by score derivation is O(kn 2 + kn + k) where n is the number of unique words and k is the number of clusters. In order to reduce memory requirements, we pre-selected words based on their document frequency. In the 162,259 documents, there were 1,299,308 unique words using our definition of a "word" from Section 2.1. By choosing words that appeared in at least 1% of the document collection, the lexicon size was reduced to 13,895 unique words. Co-occurrence scores were determined using k = 128 clusters and the EM algorithm iterated until the maximum likelihood did not change by more than 1%. The maximum score across all word pairs (p max ) was 0.00102.</p><p>The maximal number of sections per paragraph was limited to 1,000 due to time constraints. Such paragraphs were usually found as part of bibliographies in articles due to the higher concentration of punctuation marks.</p><p>The main characteristics that differed between the official runs are given below and summarized in Table <ref type="table" coords="5,98.68,573.83,4.09,9.59" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PARAGRAPH (kyoto1)</head><p>The document collection was indexed at the paragraph-level by the IR system and the top 500 results (paragraphs) for each query were given to PE. When a query word is equal to a passage word, m = 1 in Equation ( <ref type="formula" coords="5,260.70,626.43,3.86,9.59">5</ref>). Otherwise, m is equal to the co-occurrence scores. Since the final list of results has 1,000 results per query, multiple passages are selected from a single paragraph. The ranking of the results is determined by giving equal weight to the vector-space model and the aspect model.</p><p>DOCUMENT-1 (kyoto20) A document-level run using the top 1,000 documents. The paragraphs of every document in the top 1,000 were considered by the aspect model. passage word (a match), a score of m = 2 × p max was used for Equation ( <ref type="formula" coords="6,418.73,324.41,3.86,9.59">5</ref>). Ranking was done using only the scores provided by the aspect model.</p><p>DOCUMENT-2 (kyoto2) Identical to DOCUMENT-1 except that synonyms were expanded only by the aspect model and not the VSM. This is the only run where the IR system did not make use of synonym expansion.</p><p>The mean average precision results from our submission are shown in Table <ref type="table" coords="6,428.10,411.72,4.09,9.59" target="#tab_1">2</ref>. Of the three submitted runs, PARAGRAPH performed the best. This led us to believe that document-level indexing does not perform well for this task and that more granularity is required in the form of paragraph-level indexing.</p><p>The running time of PARAGRAPH is given in Table <ref type="table" coords="6,334.12,452.37,5.45,9.59" target="#tab_2">3</ref> as both real and user time in minutes. Both "Querying" and "Passage extraction" refer to the time required to process all 28 queries. Two different architectures were used for our experiments. The VSM was executed on a 3.06 GHz Intel Xeon with 8 GB RAM, while score derivation and passage extraction were run on a 3.6 GHz Intel Xeon (dual processor) with 8 GB RAM and 8 MB cache.</p><p>Since this use of the aspect model is new and generally untested, its execution time is noticeably high. It is expected that a more careful implementation can reduce the running time significantly. The difference between the real and user time for the score derivation phase of the aspect model is due in part to excessive disk swapping because of its high memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional Runs</head><p>The performance of PARAGRAPH among our three submitted runs motivated us to concentrate our investigation on this run. In particular, we modified a parameter at a time to help determine which parameters have a noticeable effect on our system's performance. Overall, ten additional runs were performed. A description of each run is given next with respect to PARAGRAPH. In all of these additional runs, paragraph-level indexing was used with synonym expansion. This information is summarized in Table <ref type="table" coords="6,450.16,674.75,4.09,9.59">4</ref>.</p><p>PARAGRAPH As our reference run using paragraph-level indexing and synonym expansion, 500 paragraphs were taken by the PE system. Only paragraphs with 1,000 sections were considered. In Run Documents Passage Equation ( <ref type="formula" coords="7,378.44,74.66,4.24,9.59">5</ref>) Equation ( <ref type="formula" coords="7,461.23,81.44,4.24,9.59" target="#formula_5">6</ref>) Weight Limit m m (VSM:PE) PARAGRAPH 500 1,000 1 p(r i , q j ) Yes</p><formula xml:id="formula_9" coords="7,83.93,99.64,431.06,148.42">1 2 : 1 2 AM 1,000 1,000 1,000 1 p(r i , q j ) Yes 1 2 : 1 2 NO AM 500 500 N/A N/A N/A N/A N/A NO AM 1,000 1,000 N/A N/A N/A N/A N/A NO SECTION LIMIT 500 ∞ 1 p(r i , q j ) Yes 1 2 : 1 2 MATCH PMAX 500 1,000 p max p(r i , q j ) Yes 1 2 : 1 2 MATCH 2PMAX 500 1,000 2 × p max p(r i , q j ) Yes 1 2 : 1 2 NON MATCH 0 500 1,000 1 0 Yes 1 2 : 1 2 NO ALPHA 500 1,000 1 p(r i , q j ) No 1 2 : 1 2 RANK WEIGHT VSM 500 1,000 1 p(r i , q j ) Yes 1 : 0 RANK WEIGHT PE 500 1,000 1 p(r i , q j ) Yes 0 : 1</formula><p>Table <ref type="table" coords="7,99.38,260.51,4.24,9.59">4</ref>: Additional runs after the release of relevance judgements. In all cases, paragraph-level indexing and synonym expansion were employed, just like PARAGRAPH.</p><p>Equation ( <ref type="formula" coords="7,145.75,306.94,3.86,9.59">5</ref>), m = 1 and m = p(r i , q j ). Equation ( <ref type="formula" coords="7,331.12,306.94,4.24,9.59" target="#formula_5">6</ref>) was used exactly as shown and equal weight was given to both the VSM and the PE scores. This information is summarized in the first rows of Table <ref type="table" coords="7,125.96,334.04,5.45,9.59" target="#tab_0">1</ref> and<ref type="table" coords="7,152.60,334.04,30.78,9.59">Table 4</ref>.</p><p>AM 1,000 The top 1,000 paragraphs are used by the aspect model.</p><p>NO AM 500 The aspect model was not used. So, each paragraph output through the querying process became a passage. Since there is one passage for each paragraph, the number of passages returned for each query is only 500.</p><p>NO AM 1,000 Similar to NO AM 500, except that 1,000 paragraphs were output.</p><p>NO SECTION LIMIT There was no limit on the number of sections allowed in a paragraph.</p><p>MATCH PMAX For exact matches, m = p max = 0.00102.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATCH 2PMAX</head><p>For exact matches, m = 2 × p max .</p><p>NON MATCH 0 When a query term and a passage term does not match, no score is added to the passage score. That is, m = 0.</p><p>NO ALPHA The first term in the numerator of Equation ( <ref type="formula" coords="7,337.52,553.35,4.24,9.59" target="#formula_5">6</ref>) is removed. In other words,</p><formula xml:id="formula_10" coords="7,471.73,544.21,57.19,21.14">∑ s i α w i = 1.</formula><p>RANK WEIGHT VSM The final ranking is determined entirely by the VSM scores.</p><p>RANK WEIGHT PE The final ranking is determined entirely by the aspect model scores.</p><p>The mean average precision results from these runs are given in Table <ref type="table" coords="7,393.42,622.98,4.09,9.59" target="#tab_3">5</ref>. For each type of MAP, our best score is shown in bold font. As these results show, there was no single run which gave the best results for all three measures.</p><p>These additional runs can be summarized as follows, with respect to our baseline, PARAGRAPH:</p><p>• As expected, by increasing the number of paragraphs given to the PE system from 500 to 1,000, effectiveness improves regardless of whether or not the aspect model is used (see the first four rows of Table <ref type="table" coords="7,137.78,712.78,3.94,9.59" target="#tab_3">5</ref>). • Removing the restriction that only paragraphs with 1,000 sections or less are processed does not yield significant results (NO SECTION LIMIT), despite the fact that some results in the gold standard are found in bibliographies. We believe the reason for is our system's inability to properly find these relevant passages in bibliographies.</p><p>• Setting m to 0 reduces the effectiveness of our system slightly for all 3 measures (NON MATCH 0). In other words, use of the co-occurrence scores (p(r i , q j )) improves retrieval. This is encouraging since it means the word-based aspect model is helpful in ranking documents and locating passages.</p><p>• In contrast, reducing m to a value similar to the maximum co-occurrence score is detrimental to all three measures (MATCH PMAX and MATCH 2PMAX). Unlike the NON MATCH 0 run where the difference between m and m increases, in these two runs, the difference decreases. So, we hypothesize that the difference between m and m should be large, but as NON MATCH 0 shows, m should not be 0.</p><p>• Omitting the first term of the numerator of Equation ( <ref type="formula" coords="8,348.02,463.61,4.24,9.59" target="#formula_5">6</ref>) reduces all three measures, but passage retrieval is affected to a lesser degree (NO ALPHA). Thus, a term which makes use of the IDF factor and paragraph word frequency is useful. However, this run has not proven that this form of the equation is ideal.</p><p>• Finally, determining the final ranking by either the VSM or PE degrades performance, except when only the PE system is used. In this case, passage retrieval is the best out of all our runs.</p><p>In general, paragraph-level indexing is better than document-level indexing (see Table <ref type="table" coords="8,460.59,558.58,3.94,9.59" target="#tab_1">2</ref>). For paragraphlevel indexing, document retrieval and aspect retrieval were best when no passage extraction mechanism was employed (NO AM 1,000). However, to attain better passage retrieval scores over our reference run (PARAGRAPH), we can simply use the scores from the aspect model (RANK WEIGHT PE). An insufficient number of runs were made to extrapolate the effect from combining factors behind the runs. For example, while it is expected that RANK WEIGHT PE can be improved by simply using 1,000 paragraphs instead of 500, this has not been demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>In this report, we have described our contribution to the TREC 2006 Genomics Track. The main task was passage retrieval and our solution was a system made of an IR system based on the VSM and a newly developed word-based aspect model. We initially submitted three runs and then performed ten additional ones to further evaluate our system. In general, the use of co-occurrence scores gives slight improvements for all three measures (see the NON MATCH 0 run in Table <ref type="table" coords="9,343.19,102.84,3.94,9.59">4</ref>), which is encouraging.</p><p>However, as our system performed within the bottom half of all systems which submitted runs to the Genomics Track this year, much work is expected. Various parameters and equations need to be further evaluated and the results in Section 4 is only the beginning. The parameters for the aspect model have not been thoroughly examined. In particular, while the reduction of the vocabulary to 13,895 words is necessary due to space limitations, its overall effect is unknown. Moreover, the importance of the number of clusters <ref type="bibr" coords="9,72.00,183.90,44.19,9.82">(k = 128)</ref> has not yet been properly assessed.</p><p>More importantly, our system is actually comprised of three parts: an IR system which returns a set of relevant documents (or paragraphs), an aspect model which derives co-occurrence scores, and a passage extraction system which locates potential passages and scores them. Problems in even one part of this entire system could be problematic. For example, even though our work focusses on the coupling of an information retrieval system with a word-based aspect model, ineffectiveness in the passage extraction system in isolating passages for scoring could have a significant effect on our system's performance. All of these aspects need to be considered as part of our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,82.11,114.39,31.89,6.72;2,82.11,123.83,30.27,6.72;2,117.84,157.96,62.96,6.72;2,206.34,157.96,43.19,6.72;2,256.40,136.17,17.36,6.72;2,143.45,127.60,27.45,6.72;2,205.95,90.42,31.48,6.72;2,213.02,80.64,18.97,6.72;2,256.25,90.32,47.83,6.72;2,256.25,99.76,21.39,6.72;2,316.11,102.07,23.81,6.72;2,501.00,88.76,22.20,6.72;2,501.29,98.20,31.08,6.72;2,351.84,157.99,62.96,6.72;2,440.34,157.51,43.19,6.72;2,412.21,81.69,25.83,6.72;2,406.06,90.60,35.52,6.72;2,375.76,105.85,29.46,6.72;2,441.99,101.59,28.26,6.72;2,439.32,111.23,31.48,6.72;2,369.07,132.82,71.23,6.72;2,77.98,172.42,222.04,9.59;2,77.98,185.97,46.42,9.59;2,311.98,172.42,222.00,9.59;2,311.98,185.97,68.30,9.59"><head></head><label></label><figDesc>On-line processing includes querying and passage extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,121.38,74.66,369.51,87.05"><head>Table 1 :</head><label>1</label><figDesc>Submitted runs to TREC.</figDesc><table coords="5,121.38,74.66,369.51,65.18"><row><cell>Run</cell><cell>Indexing level</cell><cell>IR synonym</cell><cell>Documents</cell><cell>Equation (5) m</cell><cell>Weight (VSM:PE)</cell></row><row><cell>PARAGRAPH</cell><cell>paragraph</cell><cell>yes</cell><cell>500</cell><cell>1</cell><cell>1 2 : 1 2</cell></row><row><cell cols="2">DOCUMENT-1 document</cell><cell>yes</cell><cell>1,000</cell><cell>2 × p max</cell><cell>0 : 1</cell></row><row><cell cols="2">DOCUMENT-2 document</cell><cell>no</cell><cell>1,000</cell><cell>2 × p max</cell><cell>0 : 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,393.16,703.14,146.73,9.59"><head>Table 2 :</head><label>2</label><figDesc>When a query word is equal to a Mean average precision for the three submitted runs.</figDesc><table coords="6,195.41,74.66,221.45,179.05"><row><cell>Run</cell><cell cols="3">Document Passage Aspect</cell></row><row><cell>PARAGRAPH</cell><cell>0.2248</cell><cell></cell><cell>0.0248 0.1217</cell></row><row><cell>DOCUMENT-1</cell><cell>0.1231</cell><cell></cell><cell>0.0075 0.0610</cell></row><row><cell>DOCUMENT-2</cell><cell>0.1297</cell><cell></cell><cell>0.0071 0.0692</cell></row><row><cell>Phase</cell><cell cols="3">Real time User time</cell></row><row><cell cols="2">Vector-space model</cell><cell></cell></row><row><cell cols="2">Index construction</cell><cell>25</cell><cell>25</cell></row><row><cell>Querying</cell><cell></cell><cell>&lt; 1</cell><cell>&lt; 1</cell></row><row><cell>Aspect model</cell><cell></cell><cell></cell></row><row><cell cols="2">Deriving scores</cell><cell>244</cell><cell>61</cell></row><row><cell cols="2">Passage extraction</cell><cell>651</cell><cell>632</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,266.99,467.93,36.68"><head>Table 3 :</head><label>3</label><figDesc>Real and user time (in minutes) for PARAGRAPH with respect to executing the vector-space and aspect models. The computer architecture used differed between the two models; see the text for further details.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,175.73,74.66,260.47,181.89"><head>Table 5 :</head><label>5</label><figDesc>Mean average precision for the ten additional runs.</figDesc><table coords="8,176.83,74.66,258.61,159.03"><row><cell>Run</cell><cell cols="2">Document Passage Aspect</cell></row><row><cell>PARAGRAPH</cell><cell>0.2248</cell><cell>0.0248 0.1217</cell></row><row><cell>AM 1,000</cell><cell>0.2369</cell><cell>0.0258 0.1235</cell></row><row><cell>NO AM 500</cell><cell>0.2203</cell><cell>0.0103 0.1232</cell></row><row><cell>NO AM 1,000</cell><cell>0.2372</cell><cell>0.0117 0.1246</cell></row><row><cell>NO SECTION LIMIT</cell><cell>0.2246</cell><cell>0.0231 0.1204</cell></row><row><cell>NON MATCH 0</cell><cell>0.2231</cell><cell>0.0244 0.1210</cell></row><row><cell>MATCH PMAX</cell><cell>0.1459</cell><cell>0.0083 0.0911</cell></row><row><cell>MATCH 2PMAX</cell><cell>0.1558</cell><cell>0.0091 0.0955</cell></row><row><cell>NO ALPHA</cell><cell>0.1497</cell><cell>0.0215 0.0386</cell></row><row><cell>RANK WEIGHT VSM</cell><cell>0.1744</cell><cell>0.0131 0.0348</cell></row><row><cell>RANK WEIGHT PE</cell><cell>0.2067</cell><cell>0.0261 0.1081</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.14,677.56,320.65,7.88"><p>During the conference, we realized that this extra information was not part of the queries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.14,689.62,177.53,6.72"><p>http://abbreviation.stanford.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,88.14,700.80,150.63,6.72"><p>http://www.ncbi.nlm.nih.gov/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements: This work was supported in part by the <rs type="funder">Bioinformatics Education Program "Education and Research Organization for Genome Information Science"</rs> and the <rs type="institution">Kyoto University</rs> <rs type="programName">21st Century COE Program</rs> "<rs type="programName">Knowledge Information Infrastructure for Genome Science"</rs> with support from <rs type="funder">MEXT (Ministry of Education, Culture, Sports, Science and Technology), Japan</rs>. The second author was supported by the <rs type="funder">Australian Research Council's Center for Perceptive and Intelligent Machines in Complex Environments</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zDy5qj7">
					<orgName type="program" subtype="full">21st Century COE Program</orgName>
				</org>
				<org type="funding" xml:id="_WrfXj8W">
					<orgName type="program" subtype="full">Knowledge Information Infrastructure for Genome Science&quot;</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,72.00,430.84,468.00,8.76;9,82.91,442.80,346.99,8.76" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,175.31,430.84,180.12,8.76">Simplified similarity scoring using term ranks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.06,430.93,166.94,8.55;9,82.91,442.89,252.80,8.55">Proc. 28th ACM International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>28th ACM International Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,462.08,422.70,8.76" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,179.81,462.08,137.22,8.76">Automatic document classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Borko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,324.24,462.17,75.57,8.55">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="162" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,481.37,468.01,8.76;9,82.91,493.32,371.13,8.76" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,249.00,481.37,250.15,8.76">Creating an online dictionary of abbreviations from MEDLINE</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,509.26,481.46,30.75,8.55;9,82.91,493.41,194.60,8.55">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="612" to="620" />
			<date type="published" when="2002-12">November-December 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,512.61,468.01,8.76;9,82.91,524.56,188.15,8.76" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,228.90,512.61,261.03,8.76">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,498.69,512.70,41.32,8.55;9,82.91,524.65,110.10,8.55">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,543.84,468.01,8.76;9,82.91,555.80,211.98,8.76" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,308.70,543.84,149.46,8.76">TREC 2006 genomics track overview</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Rekapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,478.90,543.93,61.11,8.55;9,82.91,555.89,138.52,8.55">Proc. 15th Text Retrieval Conference (TREC 2006)</title>
		<meeting>15th Text Retrieval Conference (TREC 2006)</meeting>
		<imprint>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,575.08,467.99,8.76;9,82.91,587.04,96.83,8.76" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,127.55,575.08,252.61,8.76">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,388.80,575.17,72.24,8.55">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001-02">January-February 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,606.32,369.46,8.76" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,241.98,606.32,104.63,8.76">Learning from dyadic data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="466" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,625.61,468.00,8.76;9,82.91,637.56,52.30,8.76" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,130.50,625.61,155.78,8.76">Development of a stemming algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Lovins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,296.71,625.70,223.16,8.55">Mechanical Translation and Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,656.85,467.99,8.76;9,82.91,668.80,154.43,8.76" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,281.69,656.85,193.28,8.76">Entrez gene: gene-centered information at NCBI</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maglott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ostell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">D</forename><surname>Pruitt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tatusova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,484.68,656.94,55.31,8.55;9,82.91,668.89,34.70,8.55">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2005-01">January 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,688.09,467.94,8.76;9,82.91,700.04,457.09,8.76;9,82.91,712.00,52.30,8.76" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,364.37,688.09,175.57,8.76;9,82.91,700.04,192.80,8.76">The MeSH translation maintenance system: Structure, interface design, and implementation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schopen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-L</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arluk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,299.59,700.13,209.66,8.55">Proc. 11th World Congress on Medical Informatics</title>
		<meeting>11th World Congress on Medical Informatics</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="67" to="69" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
