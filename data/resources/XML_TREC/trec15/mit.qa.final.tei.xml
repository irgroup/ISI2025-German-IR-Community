<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.28,94.61,343.36,12.62">Question Answering Experiments and Resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.00,128.13,60.92,10.52"><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.81,128.13,93.83,10.52"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.96,128.13,64.88,10.52"><forename type="first">Sue</forename><surname>Felshin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.94,128.13,78.52,10.52"><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,476.96,128.13,40.52,10.52"><forename type="first">Ben</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,161.04,143.51,81.70,10.52"><forename type="first">Federico</forename><surname>Mora</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.96,140.62,79.93,13.41"><forename type="first">Ã–zlem</forename><surname>Uzuner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.02,143.51,144.61,10.52"><forename type="first">Michael</forename><surname>Mcgraw-Herdeg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,122.84,157.46,88.69,10.52"><forename type="first">Natalie</forename><surname>Cheung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.96,157.46,54.51,10.52"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.46,157.46,77.34,10.52"><forename type="first">Alexey</forename><surname>Radul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.78,157.46,60.84,10.52"><forename type="first">Yuan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,448.55,157.46,88.51,10.52"><forename type="first">Gabriel</forename><surname>Zaccak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.28,94.61,343.36,12.62">Question Answering Experiments and Resources</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">928E7B16BD3354E6E86FA6B3ECD932DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction MIT CSAIL's entries for the TREC Question Answering track <ref type="bibr" coords="1,216.56,266.31,83.80,9.57" target="#b5">(Voorhees, 2006)</ref> explored the effects of new document retrieval and duplicate removal strategies for 'list' and 'other' questions, established a baseline for other systems in the interactive task, and focused on question analysis and paraphrasing, rather than incorporation of external knowledge, in the factoid task. Many of the individual subsystems are largely unchanged from last year.</p><p>We found that document retrieval strategy has an influence on performance in the different kinds of tasks later in the pipeline. Our other changes from last year did not immediately yield clear lessons. We present a question analysis data set and interannotator agreement indicators for the ciQA task that we hope will spur further evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Factoid Question Answering</head><p>Our emphasis for factoid answering this year was on identifying answers within the collection, rather than finding answers on the Web and projecting them onto the collection.</p><p>For several years, systems have thrived on the latter strategy. The Aranea system <ref type="bibr" coords="1,301.30,608.06,20.15,9.57;1,103.18,621.61,79.75,9.57" target="#b2">(Lin and Katz, 2003)</ref>, developed here four years ago, uses Web search engines (Google, Yahoo, Ask) to find many short snippets that appear with the question, and uses redundancy and heuristics about answer types to rank answers, which it then reranks based on occurrences in the source text. The major advantage is that the Web may have the answer phrased in a manner that is easier for text processing sys-tems to identify. However, the best answer on the Web may not be the best answer in the collection of interest, and thus using this as the only or dominant strategy may be a weakness.</p><p>In renewing our focus on the target collection, we addressed issues of document retrieval, question analysis, and paraphrasebased answer extraction.</p><p>In document retrieval, we experimented with external relevance feedback using Google and Wikipedia search results, and with examining only high-confidence documents.</p><p>We used our START system for question analysis, and made improvements to its handling of sentential topics, anaphora, and answer type identification. START also produces an assertion, in which the question is transformed into a statement with a variable marker and its type at the position of the linguistic trace.</p><p>We developed a new system, SmartQA <ref type="bibr" coords="1,394.40,515.26,74.40,9.57" target="#b3">(Loreto, 2006)</ref>, to select answers from text based on how well they match START's assertion. SmartQA uses the Stanford parser <ref type="bibr" coords="1,448.08,555.91,108.64,9.57;1,338.46,569.46,26.06,9.57" target="#b1">(Klein and Manning, 2003)</ref> to parse both the assertion and each candidate sentence, and a set of structural transformations and scoring heuristics that yield a final score for each sentence. We reranked these answers based on redundancy information from the Web and answer type heuristics, by passing them through Aranea as a last stage of processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">List Question Answering</head><p>Our baseline list question answering strategy remained the same as in previous years: we look for phrases matching the expected an-swer type, in contexts surrounding hot spots of question keywords or their synonyms. This year we added minor enhancements in the answer type analysis and duplicate removal mechanisms, and submitted runs with and without these enhancements.</p><p>We experimented with viewing list and factoid questions in analogous ways, processing the list question into an assertion using START as we do for factoid questions, and taking SmartQA's top k answers as the answer to the list question. In the end, however, we submitted only one run containing SmartQA results, and these simply added the top answer from SmartQA if it was not yet present in the otherwise normally generated list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Other Question Answering</head><p>An important part of the 'Other' task, even emphasized by its name, is that it should not repeat information, either from the previously asked questions, or within its own list of answers. Our 'novelty' component last year in this task did as much harm as good-removing many redundant answers, but also removing answers that would have been marked as containing a nugget. This year we compared various other strategies for duplicate removal, and submitted the best one based on our previous tests.</p><p>Our 'other' system simply reranks all the sentences in its input based on how well they match the topic phrase, and on how little they overlap with previously given information. Thus, unlike in the factoid task, it is not necessarily a good retrieval strategy to aim for very high recall whatever the precision. We tested a document retrieval method that placed more emphasis on precision, and found an improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Complex Interactive Question Answering</head><p>For the complex interactive question answering task, we attempted to provide a baseline for others based on last year's results. The ciQA organizers had suggested during the organizing meeting at TREC2005 that our sys-tem, which had the best performance on the Relationship task last year, would serve as a good baseline for evaluation this year. We therefore ran the same system this year, modified only in that it was tuned for the best performance on last year's data set.</p><p>For the interactive component, the organizers had suggested that one simple form of feedback might be simply asking the assessors which responses from the non-interactive engine had good answers, and later returning all and only those answers. This is almost the strategy we followed, except that, for the purpose of having ground truth for as many responses as possible, we also filled in previously below-cutoff responses, up to the character limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Results</head><p>Our results indicate that the different ways of using retrieved documents were sensitive to the characteristics of the retrieved set. Factoid question answering, which is relatively rich in information about the expected answer, benefitted from higher recall in the document retrieval component, though only slightly, whereas 'other' question answering, which simply looks for representative sentences, benefitted from the more focused set of documents in the strict retrieval condition. (Figure <ref type="figure" coords="2,326.09,498.04,4.85,9.57">1</ref>)</p><p>Our unchanged complex interactive question answering runs showed similar performance as last year, but many systems now exceeded this performance, showing an overall improvement in the state of the art over last year. (Figure <ref type="figure" coords="2,374.67,579.96,4.85,9.57">2</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Retrieval</head><p>Underlying each component of our question answering system is keyword-based document retrieval using Lucene<ref type="foot" coords="2,393.87,652.62,4.23,6.99" target="#foot_0">1</ref> . Last year, we found that various keyword backoff mechanisms did not retrieve documents with higher recall than the default Lucene baseline. This year we experimented with obtaining relevance feedback</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factoid</head><p>List Defn csail01 default docs: 0.154 list06a: .122 default+novelty: .106, .116 csail02 (same as csail01) list06b: .125 strict+novelty: .124, .140 csail03 strict docs: 0.149 list06+SQ 1 : .120 strict+edit-dist. clus.: .117, .142</p><p>Figure <ref type="figure" coords="3,138.93,156.54,4.24,9.57">1</ref>: Main task results: document retrieval and average accuracy for each factoid run; method and average accuracy for each list run; document retrieval+choice of duplicate removal method, average F score and average pyramid score for each definition run. Figure <ref type="figure" coords="3,137.87,276.23,4.24,9.57">2</ref>: ciQA results: the result of our optimized relationship 2005 system on this new task, along with the results after returning those responses that assessors marked correct in interaction, plus previously below-cutoff responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>from Wikipedia and Google results, as well as a "strict" method for returning a more focused set of documents.</p><p>For query expansion, we used Wikipedia synonymy and Web-based relevance feedback. For each topic-question pair p, we found the top ten Google snippets, and for each topic we found the first paragraph. To each of the words in these texts, we assigned a relevance r(w), based on the frequency of that word in the corpus c(w), of the topic and question within the corpus c(p), and their intersection c(w, p), such that 2 r(w</p><formula xml:id="formula_0" coords="3,108.55,498.92,206.33,42.90">) = 1 - max(log(c(w)), log(c(p))) -log(c(w, p)) log(|A|) -min(log(c(w)), log(c(p)))</formula><p>Given this "similarity to the topic and question", we chose a cutoff, above which we added the resulting words to the query.</p><p>We also used Wikipedia's redirect structure to find "synonyms" for words and phrases in the topic and question. For example, if the question contains "TWA800", and we know that "TWA Flight 800" redirects to the same Wikipedia page, then we use that as an expansion.</p><p>The strict condition sought to impose a minimum relevancy cutoff on the document retrieval task. To do so, we restricted document 2 |A| is the size of the AQUAINT corpus.</p><p>retrieval to the subset of documents containing all keyword tokens in the topic phrase. To improve recall while maintaining this strict relevancy, we used the Wikipedia synonyms for each keyword as a bag-of-words expansion. We then ranked the resulting document subset according to the Lucene scores on the keywords in question, which were further expanded to include the terms from the Web-based context feedback process.</p><p>Because we used both expansion as described above and this strict cutoff for the alternate runs, the two are confounded. However, because we used the topic and the question in the expansion, we found few cases where expansion made a difference, so we attribute most of the change to the strictness parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Analysis</head><p>We used START's question analysis module to turn questions into assertions for further processing. These assertions can be more easily matched against sentences of evidence than can the questions themselves. The Stanford parser, trained primarily on statements, was used in the following stage to obtain parses of both the assertion and the candidate sentences, to help score each candidate. .608 .304 .508 .264</p><p>Figure <ref type="figure" coords="4,90.87,160.33,4.24,9.57">3</ref>: Question analysis performance: START finds the correct assertion in about six of ten cases, and correctly resolves all references in the question in about half of those.</p><p>To evaluate the question analysis component, we also annotated the correct assertions for all questions in the TREC 2004 and TREC 2005 data sets, and for most of the TREC15 data set. Annotation was performed by one person unfamiliar with the details of the START system, and in cases where it did not match START's output, was adjudicated with two others, until agreement was reached. At the time of the TREC 2006 submission, START generated an assertion matching our ground truth for 61% of the annotated TREC2005 cases. On its "test set", the TREC2006 data, START correctly generated 51% of the ground truth assertions. (See Figure <ref type="figure" coords="4,86.59,438.14,4.24,9.57">3</ref>.)</p><p>The annotation involved two kinds of assertions, one "almost correct" which is a close restatement of the original question, and one "complete" in which references to the topic and to any previous questions or answers are resolved.</p><p>START handles question-to-question and question-to-topic coreference using information about gender, animacy, proper/common distinction, number, discourse salience, and coreferentiality of partial names, and of synonyms or hypernyms.</p><p>START attempts to identify the focus of each question and provide it as the answer type. We have generated ground truth for these focus answer types, but have not yet vetted them as well as we have the assertions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">'Other' Questions</head><p>The 'other' question task asks us for relevant information not yet presented, and CSAIL's approach is, as in previous years, to look for the most representative sentences from the relevant collection of sentences that have the least overlap with information already presented. We have captured the notion of 'least overlap' in a keyword-based "novelty" algorithm <ref type="bibr" coords="4,317.77,170.29,86.06,9.57" target="#b0">(Katz et al., 2005)</ref>.</p><p>Experiments with the novelty algorithm showed that it helped as much as it hurt: while it removed many sentences that did not contain nuggets, it also removed enough sentences that did to offset the score gain from shorter responses. We implemented alternate strategies based on edit distance and on Bleu <ref type="bibr" coords="4,479.18,265.43,21.22,9.57;4,287.43,278.98,81.48,9.57" target="#b4">(Papineni et al., 2001)</ref>  Our Bleu-based method uses the popular machine translation metric to detect similar pairs of sentences (with or without prior stopword removal) and removes the less novel of the pair. Novelty finds the sentence that maximizes weighted keyword overlap with informa-</p><p>tion not yet presented and minimizes weighted keyword overlap with information already presented. The DB condition includes sentences that were previously identified as definitional contexts of the topic.</p><p>Because the edit distance clustering ap-proach showed the best results on this data set, we chose to devote one run to seeing if the edit distance-based duplicate removal would do better than the novelty algorithm. It did, but only very slightly, as predicted by the preliminary results. (See Figure <ref type="figure" coords="5,243.55,156.74,4.24,9.57">1</ref>.) A much greater difference came from the use of the strict document retrieval-returning fewer documents, the most relevant ones. This reflects the fact that, unlike in factoid question answering, we have very little extra information to use in selecting good answers-we select the most representative answers. The representative answers from a smaller, more focused collection can then be expected to contain more nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Complex Interactive Questions</head><p>In the complex interactive task, we submitted a run using our unmodified relationship 2005 system, with its best parameter settings for that task. Unlike the University of Maryland baseline, which only used the template fillers for sentence retrieval, we attempted to use the narrative, as we had in the relationship task. Our results thus represent a baseline of sentence selection after narrative question analysis.</p><p>We used two types of interactive forms. Both are based on the best answers from our non-interactive system. One type of form presents all of those answers and asks the assessor to check off those answers that they consider "good"-relevant, correct, etc. The second type of form offers a finer-grained interface: it again presents every answer, but instead of checkboxes for entire answers, it allows the assessor to click on individual words in the responses to designate those words as either being "good" themselves or belonging to a "good" phrase.</p><p>On receiving the results from both of these kinds of forms, we simply submitted, as the beginning of each of our interactive submissions, just those responses that either were marked "good," or contained any words or phrases that were marked "good," respectively. Since this necessarily decreases the overall sizes of our responses to each question, we filled the responses out to the limit of 7000 characters by adding additional responses that our noninteractive system had ranked below the responses already seen by assessors.</p><p>By submitting much the same responses for assessment three separate times, we are able to look closely at the agreement between judgements, as well as the agreement between the assessor clicking "good" items under time pressure, vs. doing so in the formal assessment environment. These results are summarized in Figure <ref type="figure" coords="5,374.43,254.45,4.24,9.57">5</ref>. Low kappa scores for nuggets vs. clicks occur because most clicks do not correspond to a nugget assignment. The kappa scores for assignment of nuggets, between 0.75 and 0.86, can be interpreted as inter-annotator agreement for this task, and thus as guidance for the likely significance of comparisons between systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Contributions</head><p>Our TREC entry this year showed a proof-ofconcept factoid system in which the question is converted to an assertion, and the assertion then parsed and matched against parsed candidate answer sentences, to find candidate answers. We have created a dataset of correct assertion versions of the TREC questions for the past several years, which may be useful for other systems attempting to parse questions or to learn correct reference resolution. We have begun to create a data set of question focuses for each question, which may help others with expected answer type identification.</p><p>We tested the effects of different priorities in document retrieval as applied to the factoid and 'other' tasks, finding that there may be an interaction.</p><p>We established a baseline for ciQA performance, and gave indications of the extent of inter-annotator agreement on ciQA judging.</p><p>We look forward to working with other participants interested in using these resources for their own evaluations.  Figure <ref type="figure" coords="6,85.51,184.64,4.24,9.57">5</ref>: ciQA evaluation consistency: for each comparison, the Îº for agreement between nuggets assigned, the number of responses in common over which we evaluated that Îº, and finally the Îº for clicks, either between clicks and nuggets assigned in the cases of comparison between the original submission and the indicated form responses (Îº &lt; 0.1), or for the comparison between the two forms (Îº = 0.65).</p><p>In the run descriptions, orig indicates csail1, the initial output of our relationship system on the question set, sentences indicates csailif1, the responses to forms where entire sentences could be selected, words indicates csailif2, the responses to forms where individual words could be selected.</p><p>The difference between the comparison of forms directly (third column) and the comparison of their agreeing subset with the original (fourth column), reflects the contribution of the previously unseen responses. On average, 11.7 unseen responses made it into each final response set, constituting a third of the length, but they yielded only ten new nuggets consistently assigned overall, or one tenth of the assigned nuggets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,194.71,88.88,21.20,9.60"><head></head><label></label><figDesc>orig</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,287.43,278.98,227.36,285.46"><head></head><label></label><figDesc>overlap score, and tested the results on the TREC 2005 'other' task. The results are presented in Figure 4.</figDesc><table coords="4,287.43,333.61,227.36,230.82"><row><cell>Type</cell><cell cols="2">Vital Okay avg. F-measure</cell></row><row><cell>Lucene only</cell><cell></cell><cell></cell></row><row><cell>none</cell><cell>146</cell><cell>153 0.1409 Â± 0.0298</cell></row><row><cell cols="2">edit-distance 144</cell><cell>153 0.1612 Â± 0.0331</cell></row><row><cell>novelty</cell><cell>79</cell><cell>120 0.1447 Â± 0.0390</cell></row><row><cell>bleu</cell><cell>142</cell><cell>145 0.1426 Â± 0.0343</cell></row><row><cell>bleu + stop</cell><cell>143</cell><cell>145 0.1431 Â± 0.0344</cell></row><row><cell>DB + Lucene</cell><cell></cell><cell></cell></row><row><cell>none</cell><cell>165</cell><cell>180 0.0928 Â± 0.0263</cell></row><row><cell cols="2">edit-distance 164</cell><cell>180 0.1122 Â± 0.0304</cell></row><row><cell>novelty</cell><cell>78</cell><cell>101 0.1107 Â± 0.0304</cell></row><row><cell>bleu</cell><cell>143</cell><cell>144 0.1084 Â± 0.0308</cell></row><row><cell cols="3">Figure 4: A comparison of algorithms for re-</cell></row><row><cell cols="3">moving duplicates in the 'other' task. Edit-</cell></row><row><cell cols="3">distance uses clustering with an edit distance</cell></row><row><cell cols="3">metric to group and remove duplicate nuggets.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,304.02,731.98,117.68,7.47"><p>http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,52.16,411.63,218.27,8.74;6,63.07,422.59,207.36,8.74;6,63.07,433.55,207.36,8.74;6,63.07,444.51,207.36,8.74;6,63.07,455.47,207.35,8.74;6,63.07,466.43,108.89,8.74;6,188.67,466.43,81.75,8.74;6,63.07,477.39,207.36,8.74;6,63.07,488.34,109.41,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,144.57,455.47,125.84,8.74;6,63.07,466.43,104.31,8.74">External knowledge sources for question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Borchardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Brownell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Louis-Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephan</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angela</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,205.26,466.43,65.17,8.74;6,63.07,477.39,207.36,8.74;6,63.07,488.34,54.15,8.74">Proceedings of the 14th Annual Text REtrieval Conference (TREC2005)</title>
		<meeting>the 14th Annual Text REtrieval Conference (TREC2005)</meeting>
		<imprint>
			<date type="published" when="2005-11">2005. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,52.16,507.82,218.27,8.74;6,63.07,518.78,207.36,8.74;6,63.07,529.74,207.36,8.74;6,63.07,540.70,169.83,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,255.20,507.82,15.22,8.74;6,63.07,518.78,121.05,8.74">Accurate unlexicalized parsing</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,208.35,518.78,62.08,8.74;6,63.07,529.74,207.36,8.74;6,63.07,540.70,114.45,8.74">Proceedings of the 41st annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,52.16,560.18,218.27,8.74;6,63.07,571.14,207.36,8.74;6,63.07,582.10,207.36,8.74;6,63.07,593.06,207.36,8.74;6,63.07,604.02,207.36,8.74;6,63.07,614.98,137.46,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,213.47,560.18,56.96,8.74;6,63.07,571.14,207.36,8.74;6,63.07,582.10,182.80,8.74">Question answering from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,63.07,593.06,207.36,8.74;6,63.07,604.02,207.36,8.74;6,63.07,614.98,83.08,8.74">Proceedings of the 12th International Conference on Information and Knowledge Management (CIKM 2003)</title>
		<meeting>the 12th International Conference on Information and Knowledge Management (CIKM 2003)</meeting>
		<imprint>
			<date type="published" when="2003-11">2003. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,52.16,634.46,218.26,8.74;6,63.07,645.42,207.36,8.74;6,63.07,656.37,161.40,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,144.54,634.46,125.87,8.74;6,63.07,645.42,99.25,8.74">Exploiting syntactic relations for question answering</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-09">2006. September</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT Infolab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="6,52.16,675.85,218.27,8.74;6,63.07,686.81,207.36,8.74;6,63.07,697.77,207.36,8.74;6,63.07,708.73,207.36,8.74;6,63.07,719.69,207.36,8.74;6,63.07,730.65,75.75,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,172.96,686.81,97.47,8.74;6,63.07,697.77,203.20,8.74">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,77.13,708.73,193.29,8.74;6,63.07,719.69,207.36,8.74;6,63.07,730.65,46.63,8.74">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002)</meeting>
		<imprint>
			<date type="published" when="2001-07">2001. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,287.43,394.43,218.27,8.74;6,298.34,405.39,110.87,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,384.91,394.43,120.79,8.74;6,298.34,405.39,106.71,8.74">Overview of the TREC 2006 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
