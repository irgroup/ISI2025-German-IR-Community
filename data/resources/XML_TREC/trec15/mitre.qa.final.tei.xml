<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,202.92,60.90,206.33,15.22">MITRE&apos;s Qanda at TREC-15</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,272.52,82.04,67.19,9.57"><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
							<email>john@mitre.org</email>
						</author>
						<title level="a" type="main" coord="1,202.92,60.90,206.33,15.22">MITRE&apos;s Qanda at TREC-15</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92D7A6BEB8279A1A4D29A9B8955A0B6C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Qanda is MITRE's TREC-style question answering system. In recent years, we have been able to apply only a small effort to the TREC QA activity, approximately six person-weeks this year. (Accordingly, much of this discussion is plagiarized from prior system descriptions.) We made a number of small improvements to the system this year, including expanding our use of Wordnet. The system's information retrieval wrapper now performs iterative query relaxation in order to improve document retrieval. We also experimented with an ad hoc means of "boosting" the maximum entropy model used to score candidate answers in order to improve its ranking ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">TREC-15 system description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Underlying architecture</head><p>Qanda uses a general computational infrastructure for human language technology called the Annotation Management System (AMS). AMS is a flexible library for pairwise interactions between language processors, based on the Catalyst infrastructure used in previous versions of Qanda <ref type="bibr" coords="1,223.80,451.64,68.30,9.57">(Burger 2004</ref><ref type="bibr" coords="1,292.10,451.64,4.96,9.57;1,54.12,464.36,102.62,9.57">, Burger &amp; Mardis 2002</ref><ref type="bibr" coords="1,156.74,464.36,99.65,9.57" target="#b10">, Nyberg et al. 2004</ref>). AMS provides an extensible wrapper between a consistent internal programming model for language processors and the wide range of ways the language processor can be invoked, as well as the wide range of possible annotation formats and storage types. Philosophically, it is similar to IBM's UIMA infrastructure <ref type="bibr" coords="1,244.92,540.20,51.84,9.57;1,54.12,552.68,54.37,9.57" target="#b6">(Ferrucci &amp; Lally 2004)</ref>, without the benefits and drawbacks associated with the strong programming assumptions that UIMA makes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Major system components</head><p>Qanda has a by now banal QA architecture, which proceeds in several phases. Questions are analyzed for expected answer types, documents are retrieved using an IR system and are then processed by various taggers to find entities of the expected types in contexts that match the question. Qanda is composed of several dozen components connected via AMSbelow we describe each of the major phases in turn.</p><p>• Common question and document processing: This consists of several steps: tokenization, sentence boundary detection, part of speech tagging <ref type="bibr" coords="1,333.24,182.60,94.21,9.57" target="#b11">(Ratnaparkhi 1996)</ref>, morphological analysis <ref type="bibr" coords="1,333.24,195.32,96.37,9.57" target="#b9">(Minnen et al. 2001)</ref>, fixed phrase tagging (see below), and entity tagging. For the latter, we use Phrag <ref type="bibr" coords="1,361.80,220.52,85.33,9.57">(Burger et al. 2002)</ref>, an HMM-based tagger, to identify named persons, locations and organizations, as well as temporal expressions.</p><p>• Question analysis: After the common initial phase of analysis, questions are chunked and parsed, and salient features of the meaning of the question are extracted. See Section 2 below for more detail. (Retrieved documents do not have this level of analysis applied to them.)</p><p>• Document retrieval: AMS components have been written for several IR engines. These take the results of the question analysis and formulate a series of queries for each question-Section 3 explains this in more detail.</p><p>• Passage processing: After the retrieved documents pass through the common analysis phase, Qanda identifies lexical relations between the words in each sentence and those in the question (see Section 4). It also assigns a preliminary score to each sentence by summing the log-IDF (inverse document frequency) of words common to the sentence and the question. Those sentences with a low score are not processed by most of the system, improving the efficiency of more expensive downstream components.</p><p>• Fixed repertoire taggers: In addition to named entity tagging, we have a simple facility for constructing AMS taggers from fixed word-and phrase-lists. Some of these are used in question analysis to help determine the expected answer type. Others re-tag many named locations more specifically as cities, states/provinces, and countries. Qanda also identifies various other (nearly) closed classes such as precious metals, birthstones, several animal categories (e.g., state bird), and so on (although these latter are less relevant to the more recent TREC QA incarnations).</p><p>• Numeric tagging: A fixed repertoire tagger is run on the retrieved passages to identify words and phrases denoting units of measure, and then a simple pattern-based tagger combines these with numeric expressions to identify full-fledged measure phrases, as well as currency, percentages and other numeric phrases.</p><p>• Overlap: The question is compared to each sentence, and a number of overlap features are computed, some in terms of).</p><p>• Answer collection and ranking: Candidates are identified and merged, a number of features are collected, and a score is computed (Section 4).</p><p>• Answer selection: A final component down-selects the candidates and generates the actual answer strings. For factoid questions, this is simply the highest-scoring phrasal candidate, but definition and list questions require other processing, as detailed in Section 5.</p><p>All of these components communicate by consuming and producing stand-off annotations via AMS. A separate declarative facility is used to indicate which components are interested in consuming which annotations, and AMS arranges for the components to be connected appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Question analysis</head><p>In previous TREC evaluations, Qanda performed a limited analysis of the questions. We tagged for partof-speech and named entities, and also applied a simple fixed-repertoire tagger that maps head words to answer types in Qanda's ontology, using a set of approximately 6000 words and phrases, some extracted heuristically from WordNet, some identified by hand. As of last year, the system includes a detailed parsing phase using MITRE's conditional random field chunker Carafe <ref type="bibr" coords="2,188.28,538.52,71.28,9.57">(Wellner, 2005)</ref> and the Pro3Gres dependency parser from the University of Zurich <ref type="bibr" coords="2,86.04,563.72,93.32,9.57" target="#b12">(Schneider et al. 2004</ref>).</p><p>Perhaps due to the scarcity of questions in standard corpora, many of our corpus-based tools require a repair phase to address some of the more egregious misinterpretations of questions as declarative statements. For instance, it is not uncommon for a part of speech tagger trained on declarative data to attempt to tag questions like Who does John love? as if John love is a noun-noun compound. We find analogous problems in chunking and parsing as well, which we are able to correct to some extent using simple heuristics.</p><p>Once these tagging phases are complete, Qanda's question analysis component uses a set of structural heuristics to identify the following aspects of each question:</p><p>• Anchor: the object that the answer refers to. The answer may be the anchor, or it may be a property (e.g., length, color) or name of the anchor. The anchor will have a type and supertype from Qanda's (rather simple) ontology, e.g., PERSON and AGENT. The supertype is used as a backoff for some statistics.</p><p>• Property: the property, if any, of the anchor that is the actual answer, e.g., the height of a mountain.</p><p>Properties also have a type and supertype in Qanda's ontology.</p><p>• Name: the name, if any, of the anchor that is the actual answer. This case can arise in questions which require descriptive answers, as in Who is Henry Kissinger?</p><p>• Answer restriction: an open-domain phrase from the question that describes the anchor, e.g., first woman in space.</p><p>• Superlative: Relevant adjectives from the question restriction, e.g., first, or fastest.</p><p>• Event: the main event in the question, if any; typically the main verb, unless it is simply be.</p><p>• Salient entity: What the question is "about". Typically a named entity, this corresponds roughly to the classical notion of topic, e.g., Matterhorn in What is the height of the Matterhorn?</p><p>• Geographical and temporal restriction: Phrases that can be interpreted as restricting the question's geophysical domain, or time period, e.g., in America, or in the nineteenth century, respectively.</p><p>These features are emitted as annotations on the question, and are then available for down-stream components to consume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Document retrieval</head><p>The results of question analysis are passed to a document retrieval component-for TREC this was an AMS wrapper around the Java-based Lucene engine (Apache 2002). This formulates an IR query from the question components described above with the goal of retrieving documents likely to answer the question. An ideal document would contain every term from the question, in fact, every question component as a full phrase. This is, of course, usually too much to hope for, so Qanda begins with a restrictive query, but then relaxes it until a target number of documents is retrieved, typically 50. Question components are relaxed in a heuristic order that we arrived at through trial and error, and new documents are added to the retrieval set until the target number is reached. This is similar to the first "feedback cycle" performed by some of LCC's TREC QA systems <ref type="bibr" coords="3,220.68,145.40,76.14,9.57;3,54.12,158.12,23.65,9.57" target="#b10">(Moldovan et al. 2002)</ref>.</p><p>For example, for question 143.4:</p><p>Who is the senior vice president of the American Enterprise Institute ?</p><p>Qanda formulates the following initial query:</p><p>+"senior vice president" +"American Enterprise Institute"</p><p>In Lucene's query syntax, the quotes indicate that only the complete phrases should match, while the plus signs require the phrases to be present. Such a query retrieves only documents containing both exact phrases-there are apparently only four such documents in the AQUAINT collection, so another round of retrieval is attempted, with a weaker query:</p><p>senior vice president "senior vice president" +"American Enterprise Institute"</p><p>This query still requires the topic phrase, as that is deemed most important, but has weakened the requirements on the other phrase-the words can appear individually, and in fact none of the words need appear in a retrieved document. The entire phrase is still retained as optional, as Lucene will more highly rank documents containing the phrase.</p><p>The second, weakened query retrieves 350 documents, and the system greedily adds novel documents to the result set from the original query. When the target number of documents is reached, the cycle of query relaxation stops, and the documents are passed to the rest of the system. In this case, if necessary, a fully weakened query would have been used:</p><p>senior vice president "senior vice president" American Enterprise Institute "American Enterprise Institute"</p><p>Lucene's query syntax also allows weighted terms, and in actuality, Qanda uses different weights for each question component, even when weakened. Like the relaxation order, these weights were arrived at heuristically, and only a small portion of the full query space is explored by Qanda. Ideally, we would like to automatically acquire the weights, relaxation order, etc., so as to optimally traverse this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Answer ranking</head><p>The retrieved documents are then examined by a number of taggers and other processors. As indicated in the overview, most components of Qanda skip sentences that do not sufficiently match the question, based on an IDF-weighted overlap threshold. This year we lowered this threshold substantially so that, effectively, every sentence containing at least one content word from the question is fully processed. Qanda collects candidate answers by gathering phrasal annotations from all of the semantic taggers, and identifies a number of features for each candidate. These are combined using a conditional maximumentropy model trained from past TREC QA data sets.</p><p>Several TREC participants have used this approach, e.g., <ref type="bibr" coords="3,336.36,252.44,99.38,9.57" target="#b7">Ittycheriah et al (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer candidate features</head><p>Many of the features used in the maxent model reflect particular kinds of overlap between the question and the context in which the candidate answer is found:</p><p>• Context IDF Overlap: Described above.</p><p>• Context Unigram and Bigram Overlaps: Raw counts of words/bigrams<ref type="foot" coords="3,449.16,364.46,3.48,6.17" target="#foot_0">1</ref> in common with the question.</p><p>• Context Question Component Overlaps: Raw counts of words from various components of the question (see Section 2).</p><p>• Context Wordnet Overlap: Raw counts of words that could be synonyms, hypernyms, etc. of questions words. Count features for most Wordnet relations (Fellbaum 1998) are used.</p><p>A number of features are computed based on the candidate itself, or its location in the context sentence:</p><p>• Candidate Overlap: Raw count of words in common between the candidate itself and the question, to bias against entities from the question being chosen as answers.</p><p>• Candidate Overlap Distance: Number of characters between the candidate and the closest (content) question word in the context.</p><p>• Candidate Question Component Distances: Number of characters between the candidate and various components of the question found in the candidate context.</p><p>Candidates from the same document with the same textual realizations are merged, with the combined candidate retaining the best value for each feature. This is the extent of Qanda's candidate combinationno coreference is currently performed. We use several cross-candidate features:</p><p>• Merge Count: (log of) count of identical candidates merged together.</p><p>• Answer similarity: Average character-level similarity between this candidate and all others.</p><p>The latter feature allows textually similar candidates to "vote" for each other, allowing, for example, January, 1964 and Jan 64 to support each other without requiring any explicit coreference. We have also used such an approach to combine answers from multiple QA systems <ref type="bibr" coords="4,110.04,579.80,125.17,9.57" target="#b3">(Burger &amp; Henderson, 2003)</ref>.</p><p>A number of boolean features are also computed that compare the question's expected answer type with the semantic type of the candidate:</p><p>• Type Same: True if the candidate and expected answer types are identical.</p><p>• Type Consistent: True if the candidate's type is "similar" to the expected answer type.</p><p>• Type-Pair: This is a series of features corresponding to selected pairs of consistent types (see below).</p><p>For the most part, candidates are only considered for a question if their types are consistent. For example, Where questions lead to an expected answer type of LOCATION, which is consistent with LOCATION, CITY and COUNTRY candidates; How much questions lead to QUANTITY, consistent with PERCENTAGE.</p><p>Ideally, Qanda would consider all candidates for all questions, but, if nothing else, performance considerations justify limiting this. We do not even represent all consistent pairs as explicit features.</p><p>Instead, we use a small set of approximately 20 combinations chosen by hand, as indicated in Figure <ref type="figure" coords="4,549.72,245.96,4.11,9.57">1</ref>. These represent particular biases or preferences that we feel justified in trying to acquire from the training data. In addition, some of these pairwise features represent exceptions to the consistency requirement, e.g., PERSON is not consistent with COUNTRY, but we wish to consider such candidates anyway, as the latter can sometimes answer questions such Who started the six-day war? Similarly, we wish to consider certain named entity types as candidates, even when question analysis was unsuccessful in divining an expected answer type (unknown).</p><p>After all of the (merged) candidates have been acquired, most of the raw feature values described above are normalized with respect to the maximum across all candidates for a particular question, resulting in values between 0 and 1. We have previously found that features normalized in this way are more commensurate across questions <ref type="bibr" coords="4,470.76,479.72,82.45,9.57" target="#b8">(Light et al. 2001)</ref>. This year we also explored unit Gaussian normalization, as well as quantile normalization, but found results to be inconclusive. All of our official TREC runs simply used max-value-per-question normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum entropy models</head><p>The normalized features are combined using the weights assigned by a maxent model during training. This year, we trained the model using the question sets from TREC 1999 through 2004, including the 2001 list questions and the 25 AQUAINT definition evaluation questions. Last year's questions <ref type="bibr" coords="4,470.04,645.08,61.92,9.57">(TREC 2005)</ref> were used as a development set, although for our final run (MITRE2006D) we included this development data in the training. We used the MegaM package <ref type="bibr" coords="4,522.60,683.00,35.28,9.57;4,315.24,695.72,25.68,9.57">(Daumé 2004)</ref> to train these models.</p><p>In previous years we have struggled with a number of issues involving training this part of the system, for This year we spent some time exploring the problems involved in using a discriminative model such as maxent to rank candidates. We encountered a number of cases of feature additions or other modification to the system that decreased the maxent model's estimation error, but had negative effects on its ranking ability. This is frustrating, but perhaps understandable-MegaM is choosing feature weights to maximize the likelihood of all of the data, but we are in fact only interested in the candidate that ranks highest using the resulting probability estimate.</p><p>For many questions, there are multiple correct answerdocument pairs-some are harder than others for the model to "justify". In a sense, we would like the model to focus on those correct answers that it can more easily rank highly, possibly at the expense of other correct, but more "difficult" candidates, in the context of a particular question. MegaM in fact has a simply facility for weighting some instances more than others, and we used this to perform a crude form of "answer boosting". Our procedure is thus: we first train a maxent model from the training data, and then use this to rank all of the training answers for each question. Then, within a question, we weight the top n instances with a weight w (greater than 1), and retrain the model. This second model is then used at runtime.</p><p>Again, the intuition is to force the model to focus on those correct candidates that can already be ranked fairly highly. Similarly, we wish to focus on those incorrect candidates that are confusable with the best correct candidates. We are willing to "sacrifice" poorly ranked candidates if at least one correct candidate per question can be ranked highly. We typically used values of 100 and 2 for n and w, respectively, and found that this ad hoc procedure could consistently improve our factoid scores by roughly ten percent, relative. Our TREC runs, A and C differ only in whether this boosting was performed (see Figure <ref type="figure" coords="5,105.72,694.52,4.02,9.57" target="#fig_1">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Definition questions</head><p>Qanda has no real facility for processing definition (other) questions as such. Instead, we leverage our factoid question processing, which for the most part only considers named and other entities as candidate answers. Of course, very few definition answers correspond directly to named entities, per se, but we have noticed that certain kinds of named entities were involved with some definition answers, as indicated in the example below:</p><p>Who is Gunter Blobel?</p><p>Is at Rockefeller University 1999 Nobel prize in Medicine was born in 1936 was born in Waltersdorf, Silesia, Germany</p><p>In a sense, while named entities alone might not constitute good definition nuggets, they form the "kernel" of many nuggets. Qanda's question analysis component can already identify the semantic type of the definition target (e.g., PERSON, above). Since definition answers do not need to be exact, we allow Qanda to consider certain entity types as pseudoanswers to definition questions. Then, at the end, the actual definition text is constructed from the matrix sentences in which these pseudo-answers are found (see Section 6).</p><p>We used the type-pair features described in Section 4 to license certain combinations of definition target type and candidate type, as shown in Figure <ref type="figure" coords="5,549.96,441.08,4.11,9.57" target="#fig_0">2</ref>. Additionally, we inject some non-entity candidates using crude heuristics for identifying short fragments occurring in appositional contexts. Our hope is that the type-pair features, as well as the candidate count feature, allow the system to find some definition answers. As training data, we use the explicit other questions from recent TRECs, the AQUAINT  definition questions, and a number of questions from previous years that we determined were essentially definition questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Final answer generation</head><p>Most of Qanda's processing is independent of whether the question is factoid, list, or other. One exception is the fragment pseudo-answers generated for definition questions, another is that the question type is, in fact, available as a feature to the maxent scoring model. Otherwise, however, the system performs the same processing on all questions, until the very last stage, actual answer string generation. Special processing is required to generate both definition (other) and list answers.</p><p>List questions generate a set of short (factoid) answers, while definition questions are a set of full sentences containing the candidate pseudo-answers described above. Earlier versions of Qanda simply picked the top n candidate answers, with fixed cutoffs for list and definitions.</p><p>For several years, we have used something slightly more sophisticated-the system determines this cutoff dynamically so as to maximize the expected score for each question.</p><p>The basic idea takes advantage of Qanda's candidate evaluation mechanism-since the maxent model produces probability estimates for the correctness of each individual answer, we can use these to reason about the expected value of the score an entire answer set might receive. Our algorithm for generating list and definition answers is thus to greedily add each of the ranked candidates to an answer set in turn, stopping when the expected score appears to decrease.</p><p>The expected score of an answer set of course depends on the scoring metric to be used. Both list and definition questions are scored with variants of Fmeasure, the weighted harmonic mean of precision and recall:</p><formula xml:id="formula_0" coords="6,89.59,564.09,121.54,84.38">! F = " 2 + 1 ( ) PR " 2 P + R</formula><p>P is precision, the fraction of our generated answers that are correct, while R is recall, the fraction of all possible correct answers that we generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head><p>" is a weight used to place more emphasis on either precision or recall. For list questions, NIST weighted P and R evenly, and so the evaluation simply reduces to the following:</p><p>!</p><formula xml:id="formula_1" coords="6,149.00,701.38,52.53,28.38">F list = 2c n + r</formula><p>For definition questions,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head><p>" is set to three, and precision is approximated with a length penalty:</p><formula xml:id="formula_2" coords="6,329.61,88.62,164.69,116.77">! F def = 10 ˆ P R 9 ˆ P + R ˆ P = min(1.0, 100c /l) R = c /r</formula><p>In both F def and F list , c is the number of correct answers-either the number of distinct list answers judged correct, or the number of correct nuggets found in a definition answer. r is the total number of correct answers possible, according to the NIST assessors. For list questions, n is the total number of answers generated, and for definition questions, l is the total length of an answer.<ref type="foot" coords="6,404.52,248.78,3.48,6.17" target="#foot_1">2</ref> </p><p>Based on these equations, Qanda can estimate the expected score of an answer set. We estimate c as the sum of the maxent scores for the answers in the set. It remains to estimate r, the number of correct answers possible. This is, of course, difficult,<ref type="foot" coords="6,498.36,318.14,3.48,6.17" target="#foot_2">3</ref> and so we simply use a fixed value, in this case the means from last year's data.</p><formula xml:id="formula_3" coords="6,405.24,364.52,35.16,22.90">r list = 12 r def = 4</formula><p>For list questions, we add each factoid-style answer to the answer set in turn, incrementing n by 1 with each, as long as F list increases. Similarly, for definition questions, we add each pseudo-answers matrix sentence to the answer set, incrementing the length l appropriately, as long as F def continues to increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Submitted runs and results</head><p>This year we submitted three variant runs (see Figure <ref type="figure" coords="6,315.24,511.88,3.94,9.57" target="#fig_1">3</ref>). Run A is from a basic system, with the features described above. Run C differs only in that the candidate scoring model has been "boosted" as described in Section 4-the roughly ten percent improvement is similar to what we saw with our development data. It is unclear why the evaluation list questions appear to be unaffected by the boosting. Run D differs mainly in that our development set has been added to the training set, namely, last year's questions.</p><p>This did not include, however, the definition questions, perhaps accounting for run D's loss in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>As well as the usual description of this year's system architecture, we have discussed Qanda's question analysis and our use of maximum entropy models for answer selection, in particular a method for boosting such models to better support TREC's winner-take-all evaluation. We believe that this ad hoc method has some connections to minimum-risk annealing <ref type="bibr" coords="7,267.00,329.96,29.88,9.57;7,54.12,342.44,72.48,9.57" target="#b5">(Smith &amp; Eisner, 2006)</ref> and would like to explore this in the future. We described the query relaxation technique that Qanda uses in an attempt to improve the document retrieval phase of the system-we would like to use a more principled exploration of the query space for retrieval. Finally, we presented our approach to generating definition and list answer sets by maximizing the expected score each set will receive.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,326.28,711.08,220.92,9.57;5,375.48,723.80,122.28,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Type-pair features used in evaluating definition pseudo-answers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,65.40,166.52,225.00,9.57;7,68.28,179.24,219.49,9.57"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results for three MITRE runs, as well as median and best across all 2006 submissions</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,318.60,717.49,239.40,8.72;3,315.24,729.01,66.78,8.72"><p>All of the "raw count" features described in this section omit stop words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,318.60,667.57,239.40,8.72;6,315.24,679.09,100.62,8.72"><p>Qanda ignores the distinction between inessential and essential correct nuggets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,318.60,690.61,239.40,8.72;6,315.24,702.13,242.69,8.72;6,315.24,713.65,243.00,8.72;6,315.24,724.93,102.78,8.72"><p>David Lewis (personal communication) has suggested using the sum of scores over all answer candidates for a particular question as an estimate for r, but we have found this to worsen our results.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,54.12,471.32,197.76,9.57;7,54.12,484.04,241.26,9.57" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,220.38,471.32,31.50,9.57;7,54.12,484.04,83.06,9.57">Jakarta Lucene-Overview</title>
		<ptr target="http://jakarta.apache.org/lucene/" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Apache Software Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.12,502.52,239.10,9.57;7,54.12,515.24,196.20,9.57;7,54.12,527.96,198.60,9.57;7,54.12,540.44,212.04,9.57;7,54.12,553.16,89.40,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,145.85,515.24,104.47,9.57;7,54.12,527.96,43.05,9.57">Data-intensive question answering</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,121.08,527.96,131.64,9.57;7,54.12,540.44,144.65,9.57">Proceedings the Tenth Text REtrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC-10)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.12,571.88,210.54,9.57;7,54.12,584.60,227.51,9.57;7,54.12,597.08,216.84,9.57;7,54.12,609.80,163.02,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,128.47,584.60,153.16,9.57;7,54.12,597.08,44.45,9.57">Statistical named entity recognizer adaptation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">C</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">T</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,121.56,597.08,149.40,9.57;7,54.12,609.80,121.97,9.57">Proceedings of the Conference on Natural Language Learning</title>
		<meeting>the Conference on Natural Language Learning<address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.12,628.52,217.80,9.57;7,54.12,641.00,234.12,9.57;7,54.12,653.72,230.16,9.57;7,54.12,666.44,207.00,9.57;7,54.12,678.92,119.82,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,225.12,628.52,46.80,9.57;7,54.12,641.00,144.94,9.57">Exploiting diversity for answering questions</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,222.12,641.00,66.12,9.57;7,54.12,653.72,230.16,9.57;7,54.12,666.44,109.80,9.57">Proceedings of the Human Language Technology Conference of the North American Chapter</title>
		<meeting>the Human Language Technology Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.12,697.64,233.52,9.57;7,54.12,710.36,234.60,9.57;7,54.12,723.08,223.74,9.57;7,315.24,56.84,231.12,9.57;7,315.24,69.56,219.90,9.57;7,315.24,82.04,163.80,9.57;7,315.24,100.76,201.96,9.57;7,315.24,113.48,182.70,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,224.84,697.64,62.80,9.57;7,54.12,710.36,89.71,9.57;7,419.94,56.84,126.43,9.57;7,315.24,69.56,149.21,9.57;7,459.00,100.76,58.20,9.57;7,315.24,113.48,123.49,9.57">Notes on CG and LM-BFGS optimization of logistic regression</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Mardis</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/~hdaume/megam/ChristianeFellbaum" />
	</analytic>
	<monogr>
		<title level="m" coord="7,166.44,710.36,122.28,9.57;7,54.12,723.08,219.05,9.57">AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">2002. 2004. 1998</date>
		</imprint>
	</monogr>
	<note>WordNet: An Electronic Lexical Database</note>
</biblStruct>

<biblStruct coords="7,315.24,132.20,241.44,9.57;7,315.24,144.68,213.24,9.57;7,315.24,157.40,210.12,9.57;7,315.24,170.12,222.12,9.57;7,315.24,182.60,194.16,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,509.96,132.20,46.72,9.57;7,315.24,144.68,192.10,9.57">Minimumrisk annealing for training log-linear models</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,315.24,157.40,210.12,9.57;7,315.24,170.12,222.12,9.57;7,315.24,182.60,187.96,9.57">Proceedings of the International Conference on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL</title>
		<meeting>the International Conference on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.24,201.32,237.36,9.57;7,315.24,214.04,202.68,9.57;7,315.24,226.52,219.48,9.57;7,315.24,239.24,22.38,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,460.96,201.32,91.64,9.57;7,315.24,214.04,202.68,9.57;7,315.24,226.52,113.52,9.57">Building an example application with the Unstructured Information Management Architecture</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,440.52,226.52,94.20,9.57">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.24,257.96,224.70,9.57;7,315.24,270.68,235.26,9.57;7,315.24,283.16,241.44,9.57;7,315.24,295.88,208.86,9.57" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,350.54,270.68,190.40,9.57">IBM&apos;s statistical question answering system</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,326.52,283.16,230.16,9.57;7,315.24,295.88,46.28,9.57">Proceedings of the Tenth Text REtrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC-10)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.24,314.60,240.78,9.57;7,315.24,327.08,216.36,9.57;7,315.24,339.80,199.20,9.57;7,315.24,352.52,78.06,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,350.27,327.08,181.33,9.57;7,315.24,339.80,94.81,9.57">Analyses for elucidating current question answering technology</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,433.56,339.80,80.88,9.57;7,315.24,352.52,54.12,9.57">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.24,371.00,238.62,9.57;7,315.24,383.72,212.70,9.57;7,315.24,396.44,164.46,9.57" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,320.23,383.72,198.57,9.57">Applied morphological processing of English</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Guido Minnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darren</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,315.24,396.44,135.95,9.57">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.24,415.16,238.92,9.57;7,315.24,427.64,210.00,9.57;7,315.24,440.36,208.44,9.57;7,315.24,453.08,241.32,9.57;7,315.24,465.56,22.86,9.57;7,315.24,484.28,221.16,9.57;7,315.24,497.00,241.55,9.57;7,315.24,509.72,234.84,9.57;7,315.24,522.20,199.50,9.57" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,397.36,427.64,127.88,9.57;7,315.24,440.36,208.44,9.57;7,315.24,453.08,28.48,9.57;7,392.41,497.00,164.38,9.57;7,315.24,509.72,88.09,9.57">Performance issues and error analysis in an open-domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Paca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Mardis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,367.56,453.08,189.00,9.57;7,315.24,465.56,17.15,9.57;7,427.56,509.72,122.52,9.57;7,315.24,522.20,44.77,9.57">Proceedings of the 40th Annual Meeting of ACL</title>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Maybury</surname></persName>
		</editor>
		<meeting>the 40th Annual Meeting of ACL</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2002">2002. 2004</date>
		</imprint>
	</monogr>
	<note>New Directions in Question Answering</note>
</biblStruct>

<biblStruct coords="7,315.24,540.92,220.92,9.57;7,315.24,553.64,197.76,9.57;7,315.24,566.12,231.00,9.57;7,315.24,578.84,53.34,9.57" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,446.22,540.92,89.94,9.57;7,315.24,553.64,92.28,9.57">A maximum entropy part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,430.68,553.64,82.32,9.57;7,315.24,566.12,231.00,9.57;7,315.24,578.84,48.49,9.57">Proceedings of the Empirical Methods in Natural Language Processing Conference</title>
		<meeting>the Empirical Methods in Natural Language Processing Conference</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.24,597.56,218.46,9.57;7,315.24,610.04,220.92,9.57;7,315.24,622.76,192.84,9.57;7,315.24,635.48,211.98,9.57;7,315.24,654.20,218.52,9.57;7,315.24,666.68,208.08,9.57;7,315.24,679.40,209.28,9.57;7,315.24,692.12,209.58,9.57" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,346.93,610.04,189.23,9.57;7,315.24,622.76,31.03,9.57;7,483.69,654.20,50.07,9.57;7,315.24,666.68,208.08,9.57;7,315.24,679.40,72.52,9.57">Leveraging machine readable dictionaries in discriminative sequence models</title>
		<author>
			<persName coords=""><forename type="first">Gerold</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Dowdall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,357.96,622.76,150.12,9.57;7,315.24,635.48,96.92,9.57;7,411.72,679.40,112.80,9.57;7,315.24,692.12,204.12,9.57">Proceedings of Language Resources and Evaluation Conference (LREC)</title>
		<editor>
			<persName><forename type="first">Geneva</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</editor>
		<meeting>Language Resources and Evaluation Conference (LREC)</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2006</date>
		</imprint>
	</monogr>
	<note>Workshop on Recent Advances in Dependency Grammar</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
