<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.68,81.21,334.03,18.37;1,109.56,106.05,376.32,18.37">Towards Practical PPM Spam Filtering: Experiments for the TREC 2006 Spam Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.08,145.92,88.97,12.74"><forename type="first">Andrej</forename><surname>Bratko</surname></persName>
							<email>andrej.bratko@klika.si</email>
						</author>
						<author>
							<persName coords="1,250.80,145.92,90.36,12.74"><forename type="first">Bogdan</forename><surname>Filipič</surname></persName>
							<email>bogdan.filipic@ijs.si</email>
						</author>
						<author>
							<persName coords="1,377.68,145.92,70.89,12.74"><forename type="first">Blaž</forename><surname>Zupan</surname></persName>
							<email>blaz.zupan@fri.uni-lj.si</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<addrLine>Tržaška cesta 25</addrLine>
									<postCode>SI-1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Intelligent Systems</orgName>
								<address>
									<addrLine>1 Jozef Stefan Institute Jamova 39</addrLine>
									<postCode>SI-1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.68,81.21,334.03,18.37;1,109.56,106.05,376.32,18.37">Towards Practical PPM Spam Filtering: Experiments for the TREC 2006 Spam Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">40C53D9182253B59036C53DB3456BB57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes our participation in the TREC 2006 spam track. We submitted a single filter for the evaluation, based on the Prediction by Partial Matching compression scheme, a method that performed well in the previous TREC evaluation. A major focus of our effort was to improve efficiency of the method, particularly in terms of memory consumption, in order to establish whether compressionbased filters are in fact a viable solution for practical applications. Our system exhibited fair performance, despite the fact that the filtering techniques remained virtually unchanged from the previous evaluation. We did not investigate methods for tackling delayed user feedback. A very simple strategy of training on most recent examples was used for the active learning task, and found to work surprisingly well given its simplicity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Text REtrieval Conference (TREC) is an annual event aimed at encouraging and supporting research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. Since 2005, a track on spam filtering is included in the TREC framework, with the goal of providing a standard evaluation of current and proposed spam filtering approaches. To facilitate the evaluation effort, a collection of corpora, both public and private, is compiled annually by the track organizers.</p><p>Our group from the Jozef Stefan Institute in Ljubljana, Slovenia ("Institut Jožef Stefan" -IJS) participated in TREC 2005, using a spam filter based on statistical data compression algorithms <ref type="bibr" coords="2,242.95,39.55,125.15,9.69" target="#b1">(Bratko and Filipič, 2005)</ref>. This unconventional filtering approach featured prominently in the results of the TREC 2005 evaluation. Despite the favorable performance, practical implementations of a compressionbased spam filter are yet to emerge, with the notable exception of the "bit-entropy" filter included in CRM114 <ref type="bibr" coords="2,232.10,93.79,106.44,9.69">(Yerazunis, Yerazunis)</ref>. We believe that the main obstacle to adopting such methods in practice is the large amount of memory that is typically required to train such models, and, to a lesser extent, the difficulties in implementing persistent versions of such models.</p><p>This paper describes the system submitted to the TREC 2006 spam track by the IJS group. We submitted a single filter configuration for both online filtering and active learning tasks, which featured essentially the same methods that were used for TREC 2005. However, the algorithms were re-implemented in a package that comes much closer to what would be expected of a production quality spam filter in terms of speed and memory efficiency.</p><p>The remainder of this paper is structured as follows. In the following section, we describe the filtering methods employed by the IJS filter. We discuss issues related to developing an efficient implementation of the Prediction by Partial Matching compression algorithm in Section 3. In Section 4, we review the results of the TREC evaluation. Finally, we outline some plans for future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Our online spam filtering submission employed the same compression-based filtering methods that were used by the IJS system at TREC 2005. We did not investigate methods for tackling delayed user feedback and used a very simple strategy for the active learning task: Training on the most recent examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Online Filtering</head><p>Our spam filtering approach is based on statistical data compression models. Such models can be used to estimate the probability of a sequences of characters based on previous observations. The system submitted for evaluation at TREC 2006 is based on the Prediction by Partial Matching compression algorithm, a characterlevel Markov model that is described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The Prediction by Partial Matching Algorithm</head><p>The Prediction by Partial Matching (PPM) compression algorithm <ref type="bibr" coords="2,436.85,545.11,58.38,9.69;2,100.08,558.67,66.67,9.69" target="#b2">(Cleary and Witten, 1984)</ref> is among the best-performing compression schemes for lossless text compression. Essentially, the PPM algorithm is a back-off smoothing method for finite-order Markov models, similar to back-off models used in natural language processing.</p><p>It is convenient to assume that an order-k PPM model stores a table of all subsequences up to length k that occur anywhere in the training text. For each such subsequence (or context ), the frequency counts of characters that immediately follow it is maintained. When predicting the character x i at position i in a sequence x n 1 , statistics associated with the preceding k characters x i-1 i-k (the context of x i ) are used for prediction. If this particular context has not appeared in the training text, a shorter suffix of the sequence leading up to position i is looked up, until a context with non-zero statistics is found.</p><p>If the target character has appeared in this context in the training text, its relative frequency within the context is used for prediction. This probability is discounted by a small amount to reserve some probability mass, which is distributed among characters not seen in the current context, according to a lower-order model, that is, according to statistics associated with a shorter context. The procedure is applied recursively until all characters receive a non-zero probability. If necessary, a default model of order -1 is used, which always predicts a uniform distribution among all possible characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Using the PPM Algorithm for Spam Filtering</head><p>For classification tasks, we build one PPM model from the training data of each class, and use the trained models to evaluate the sequence of characters contained in the target document. The classification outcome is determined by the model that assigns the greatest probability to the target document (the document probability is simply the product of the probabilities of all characters contained in the document).</p><p>Our filter uses an order-6 PPM-D model <ref type="bibr" coords="3,318.88,293.11,74.24,9.69" target="#b5">(Howard, 1993)</ref> in combination with the exclusion principle <ref type="bibr" coords="3,212.52,306.67,69.80,9.69" target="#b6">(Sayood, 2000)</ref>. The models of spam and legitimate email are used adaptively, that is, the models are updated after evaluating each character probability as if the sequence of characters leading up to the current position was part of the training data. Such models were found to perform well in previous experiments <ref type="bibr" coords="3,161.76,360.91,97.22,9.69" target="#b0">(Bratko et al., 2006)</ref>. To produce scores that are comparable among different-length documents, the log odds of the message being spam is divided by the message length to produce the final "spamminess score".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Active Learning</head><p>We used a very basic strategy for the active learning task: We decided to train on the most recent messages in the email stream first, in the hope that these recent examples better represent future data. The motivation for this approach is based on the study by <ref type="bibr" coords="3,180.26,482.47,132.63,9.69" target="#b4">Cormack and Bratko (2006)</ref>, which indicates a that large amount of locality is present in the chronological order of messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Description</head><p>Our system was designed as a client-server application. The server maintains the compression models required for classification in main memory and updates the models incrementally with each new training example. The system performs very limited message preprocessing, which primarily includes MIME-decoding and stripping messages of all non-textual message parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PPM Implementation</head><p>Most of our efforts for TREC 2006 were devoted to implementing an efficient compression-based spam filter, since efficiency was the main shortcoming of our previous system. In this section, we describe some technical details of this effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Optimizing Memory for PPM Models</head><p>Our PPM implementation is based on the suffix tree data structure <ref type="bibr" coords="4,422.21,137.59,67.98,9.69" target="#b8">(Weiner, 1973)</ref>. The suffix tree is a tree data structure, used to represent all suffixes of a given sequence. The data structure requires no more than 2n tree nodes for this purpose, where n is the length of the indexed string. In practice, the size of suffix trees required for our PPM model is much smaller, primarily due to the fact that we only need to store suffixes of length not greater than k + 1, where k is the order of the PPM model.</p><p>In our implementation, sibling nodes at each level of the suffix tree are stored consecutively as sorted arrays. Our implementation requires 7 bytes of memory per node. Leaves of the suffix tree may store the corresponding suffixes, or contain pointers back into the training sequences to the same effect. The former approach is slightly faster, while the latter is better suited for higher-order models and for tasks that require indexing of strings or files. Memory buffers in which the model is stored take one of |Σ| different sizes, where |Σ| is the cardinality of the alphabet (i.e. the maximum arity of internal nodes).<ref type="foot" coords="4,279.60,324.80,4.23,7.97" target="#foot_0">1</ref> We implemented custom memory allocators for our suffix tree implementation, which eliminate allocation overhead by taking advantage of the fact that all possible buffer sizes are known in advance.</p><p>Statistics required by the PPM algorithm must be stored at every node of the suffix tree. For a binary classification model, an additional 8 bytes are needed to store the statistical counters required by PPM at each node. These counters could trivially be quantized to a fraction of this size to further reduce memory usage. However, incremental training would not always be reversible in this case, since the counters may require scaling to prevent overflows. At 15 bytes per node, memory usage of our PPM filter peaks at 410 MB after training on the full TREC 2005 public corpus (92,189 messages; MIME decoded), which is comfortably within limits for the TREC evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Persistent Models</head><p>The memory requirements of our PPM model may still be unacceptable for very large datasets or for environments where memory is limited (e.g. for desktop use). To this end, our suffix tree implementation may also use file-based allocators. Such allocators return temporary copies of the actual buffers and release the cached copies when they are no longer needed (possibly writing any modifications back to file storage). The memory requirements of file-based models are minimal, since no more than k paths of the whole suffix tree are kept in memory at any time, where k is the order of the PPM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Filtering Speed</head><p>The training and classification time complexity of our PPM implementation is O(kn), where n is the length of the target document, in characters, and k is the order of the PPM model. Some optimizations further reduce this complexity in practice, following Ukkonen's online suffix tree construction algorithm <ref type="bibr" coords="5,446.01,100.87,49.00,9.69;5,100.08,114.43,25.88,9.69" target="#b7">(Ukkonen, 1995)</ref> (specifically, nodes before the "active point" of Ukkonen's boundary path are skipped). Unfortunately, Ukkonen's linear time suffix tree construction algorithm cannot be fully applied in our setting, since statistics must be updated also for nodes that are not visited by Ukkonen's algorithm (specifically, this applies to nodes beyond the "end point" of Ukkonen's boundary path). Our PPM-based filter requires just under 1.5 hours to complete an online filtering run on the TREC 2005 public corpus (92,189 messages; includes MIME decoding) on a 2.0 GHz Intel Pentium M processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Probabilistic Sequence Modeling Library</head><p>The PPM algorithm submitted for the TREC 2006 evaluation is part of a larger sequence modeling toolkit developed and maintained by the authors (the Probabilistic Sequence Modeling Library -PSMLib). A wrapper around this software is available online as a C++ library.<ref type="foot" coords="5,262.20,301.16,4.23,7.97" target="#foot_2">2</ref> A Python extension module which exposes the same functionality to Python is also available.<ref type="foot" coords="5,320.16,314.72,4.23,7.97" target="#foot_3">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we comment on the results of the TREC evaluation that are most relevant to our system. We also present measurements related to the efficiency of our filter in terms of filtering speed and memory consumption. The full official results of the TREC evaluation are reported in the spam track results section of the TREC 2006 proceedings.<ref type="foot" coords="5,219.00,431.60,4.23,7.97" target="#foot_4">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Online Filtering</head><p>The performance of our system on the online filtering task was satisfactory. Figure <ref type="figure" coords="5,100.08,501.43,5.45,9.69">1</ref> shows the results of the online filtering evaluation with immediate feedback on the TREC public corpus (admittedly, this is a trial in which our filter performed relatively well compared to most other experiments). Overall, the system was among the better performers in all tasks involving immediate feedback, but appears to be more sensitive to delayed feedback than the majority of the other systems. It is not clear whether other participants employed methods to harness the information contained in unlabeled examples, or whether the PPM method is more sensitive to delayed feedback in general. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Active Learning</head><p>Our simple active learning strategy worked surprisingly well, and the official results show no indication that our approach was inferior to any of the more sophisticated methods that were used by other participants. Figure <ref type="figure" coords="6,359.69,329.59,5.45,9.69" target="#fig_0">2</ref> shows how filtering performance on the two public corpora relates to the number of training examples when using random sampling and our "most recent first" strategy.  Using the most recent messages for training is clearly beneficial for up to 1600 training messages. On the Chinese corpus, performance levels off at this point, even when additional training data is provided. Similarly, performance cannot be improved significantly by adding training data for the English corpus, however, choosing the most recent examples hurts performance at certain sampling points (not shown in the graph). This is possibly due to an anomaly in the English corpus that is also reflected in the results of the online filtering runs in Figure <ref type="figure" coords="7,442.62,93.79,4.23,9.69">1</ref>, in which performance of all filters drops after processing the first 25,000 messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency</head><p>Table <ref type="table" coords="7,130.19,161.11,5.45,9.69" target="#tab_2">1</ref> compares the efficiency of memory-based and disk-based models for online filtering on the SpamAssassin corpus. Although the disk-based version of the filter is more than 30 times slower, the filtering rate of 3.2 messages per second appears to be satisfactory for desktop use. We found that the efficiency of disk-based models drops considerably as the size of the model grows. A better caching mechanism may help improve performance for such large model files. filtering experiments on the SpamAssassin corpus. The corpus contains 6034 messages with a total size of 13,883,063 bytes (messages are truncated to 2500 bytes). Each message is first classified, then trained on. The experiment was performed on a 2.0 GHz Intel Pentium-M processor with 2 GB of main memory and a 7200 RPM portable disk drive with 8 MB of disk cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our system exhibited fair performance in the TREC 2006 evaluation, despite the fact that the filtering techniques employed by the IJS system remained virtually unchanged from the previous TREC evaluation. We are satisfied with this result, bearing in mind that the efficiency of our system was improved substantially, bringing compression-based spam filters a step closer to becoming a viable solution for real world spam filtering applications. A very crude estimate is that memory usage was reduced to about a quarter of what was required by our previous system, while filtering speed was improved about two-fold. Further improvements, particularly in terms of memory consumption, would be possible if we were willing to sacrifice some of the flexibility offered by the system (such as, for example, decremental learning).</p><p>Unfortunately, the development effort required for technical improvements of our system hindered our research towards possible enhancements in terms of filtering performance. We wish to remedy the lack of progress in this area at TREC 2007.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,70.92,580.15,453.62,10.62;6,70.92,595.27,453.44,9.69;6,70.92,609.67,453.62,9.69;6,70.92,624.19,100.01,9.69"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Active learning results on the TREC public English (left) and Chinese (right) corpora. The area under the ROC curve (%) is plotted as a function of the number of training messages for the "most recent first" active learning strategy, and random sampling with an equal number of training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,70.92,38.57,453.69,218.99"><head></head><label></label><figDesc>Online filtering performance on the TREC public English corpus (figure reproduced from<ref type="bibr" coords="6,96.13,234.31,76.41,9.69" target="#b3">(Cormack, 2006)</ref>). AUC curves (left) and ROC learning curves (right) of the best performing filters of all nine groups that participated in TREC 2006 are shown.</figDesc><table coords="6,70.92,38.57,451.18,192.08"><row><cell></cell><cell></cell><cell></cell><cell>ROC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ROC Learning Curve</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>% Spam Misclassification (logit scale)</cell><cell>10.00 1.00 0.10</cell><cell>oflS1pei tufS1pei ijsS1pei CRMS1Fpei hubS1pei tamS4pei hitS1pei bptS2pei dalS1pei</cell><cell></cell><cell></cell><cell>(1-ROCA)% (logit scale)</cell><cell>0.10 50.00 1.00 10.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">oflS1pei tufS1pei ijsS1pei CRMS1Fpei hubS1pei tamS4pei hitS1pei bptS2pei dalS1pei</cell></row><row><cell></cell><cell>50.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell>0.10</cell><cell>1.00</cell><cell>10.00</cell><cell>50.00</cell><cell>0</cell><cell>5000</cell><cell>10000</cell><cell>15000</cell><cell>20000</cell><cell>25000</cell><cell>30000</cell><cell>35000</cell><cell>40000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>% Ham Misclassification (logit scale)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Messages</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,70.92,268.51,453.55,65.46"><head>Table 1 :</head><label>1</label><figDesc>Storage Total time CPU time Peak mem. Peak disk Msg/sec KB/sec Efficiency of memory-based and file-based adaptive order-6 PPM-D models in online</figDesc><table coords="7,93.12,286.03,401.81,24.21"><row><cell>Memory</cell><cell>56 sec.</cell><cell>55 sec.</cell><cell>43.6 MB</cell><cell>0.0 MB</cell><cell>107.7</cell><cell>242.1</cell></row><row><cell>Disk</cell><cell>1892 sec.</cell><cell>1393 sec.</cell><cell>9.0 MB</cell><cell>41.6 MB</cell><cell>3.2</cell><cell>7.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,88.80,640.59,69.19,9.96"><p>Typically, |Σ| =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="4,160.80,641.28,201.97,8.85"><p>256, corresponding to all 1-byte symbol codes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="5,88.80,618.24,244.87,9.08"><p>Available at http://ai.ijs.si/andrej/psmslib.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="5,88.80,630.12,244.87,9.08"><p>Available at http://ai.ijs.si/andrej/psmslib.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="5,88.80,642.12,363.07,9.08"><p>The TREC proceedings are available online at http://trec.nist.gov/pubs.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,100.08,63.91,395.18,9.69;8,111.84,77.47,383.16,9.69;8,111.84,91.03,137.90,9.69" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,468.52,63.91,26.74,9.69;8,111.84,77.47,235.26,9.69">Spam filtering using statistical data compression models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Filipič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,356.77,77.47,138.24,9.68;8,111.84,91.03,41.33,9.68">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2673" to="2698" />
			<date type="published" when="2006-12">2006. Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,114.55,394.62,9.69;8,111.84,128.11,383.32,9.69;8,111.84,141.67,268.98,9.69" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,324.26,114.55,170.44,9.69;8,111.84,128.11,288.28,9.69">Spam filtering using character-level markov models: Experiments for the TREC 2005 Spam Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Filipič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,421.93,128.11,73.23,9.68;8,111.84,141.67,169.58,9.68">Proc. 14th Text REtrieval Conference (TREC 2005)</title>
		<meeting>14th Text REtrieval Conference (TREC 2005)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">2005, November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,165.19,395.17,9.69;8,111.84,178.75,383.38,9.69;8,111.84,192.31,74.09,9.69" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,314.94,165.19,180.31,9.69;8,111.84,178.75,147.64,9.69">Data compression using adaptive coding and partial string matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,269.88,178.75,225.34,9.68">IEEE Transactions on Communications COM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="396" to="402" />
			<date type="published" when="1984-04">1984. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,215.71,394.92,9.69;8,111.84,229.27,318.66,9.69" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,278.28,215.71,164.97,9.69">TREC 2006 Spam Track overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,470.29,215.71,24.71,9.68;8,111.84,229.27,219.27,9.68">Proc. 15th Text REtrieval Conference (TREC 2006)</title>
		<meeting>15th Text REtrieval Conference (TREC 2006)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">2006, November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,252.79,395.02,9.69;8,111.84,266.35,383.21,9.69;8,111.84,279.91,19.11,9.69" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,327.29,252.79,167.80,9.69;8,111.84,266.35,32.99,9.69">Batch and online spam filter comparison</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,169.80,266.35,175.20,9.68">Conference on Email and Anti-Spam</title>
		<meeting><address><addrLine>Mountain View, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">2006, July. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,303.43,395.00,9.69;8,111.84,316.99,371.98,9.69" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,214.68,303.43,280.40,9.68;8,111.84,316.99,78.43,9.68">The Design and Analysis of Efficient Lossless Data Compression Systems</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Providence, Rhode Island</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct coords="8,100.08,340.51,323.20,9.69" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,194.40,340.51,160.32,9.68">Introduction to Data Compression</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,364.03,394.87,9.69;8,111.84,377.59,19.23,9.69" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,201.28,364.03,164.16,9.69">On-line construction of suffix trees</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,374.77,364.03,60.49,9.68">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,401.11,394.85,9.69;8,111.84,414.67,253.27,9.69" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,190.01,401.11,167.66,9.69">Linear pattern matching algorithms</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,379.93,401.11,115.00,9.68;8,111.84,414.67,167.66,9.68">14th Annual Symposium on Switching and Automata Theory</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,100.08,438.19,395.12,9.69;8,111.84,451.63,170.08,9.68" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,173.16,438.19,322.04,9.68;8,111.84,451.63,164.98,9.68">CRM114 Revealed Or How I learned To Stop Worrying and Trust My Automatic Monitoring Systems</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yerazunis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
