<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,202.32,30.86,207.30,12.98">AnswerFinder at TREC 2006</title>
				<funder ref="#_tzpAq8w">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.60,59.90,71.61,10.82"><forename type="first">Diego</forename><surname>Mollá</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.16,59.90,112.00,10.82"><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,397.25,59.90,73.24,10.82"><forename type="first">Luiz</forename><surname>Pizzato</surname></persName>
							<email>pizzato@ics.mq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,202.32,30.86,207.30,12.98">AnswerFinder at TREC 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">133CD26E19674975C393564910DEB948</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the AnswerFinder question answering system and its participation in the TREC 2006 question answering competition. This year there have been several improvements in the AnswerFinder system, although most of them in the implementation sphere. The actual functionality used this year is almost exactly the same as last year, but many bugs are fixed and the efficiency of the system has improved much. This allows for more extensive parameter tuning. Here we will also present an error analysis of the current AnswerFinder system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the fourth year the AnswerFinder project is participating in the TREC question answering track. The underlying research question has been the same over the years: what shallow semantic representations are best suitable for representing the meaning of questions and sentences in the context of question answering?</p><p>The underlying framework of the An-swerFinder system has remained the same (or nearly the same). The system implements a step-wise reduction of data. This means that when AnswerFinder receives a question, the document selection phase selects a small number of relevant documents, out of which a smaller number of relevant sentences are selected during the sentence selection phase and finally within these sentences a selection of possible answers is selected. The best of these answers is then returned to the user.</p><p>So far, the focus has mainly been on the sentence selection and the answer extraction phases. In the sentence selection phase, several sentence selection methods have been implemented. Special attention has been paid to metrics that abstract over the exact word order used in sentences and questions. This is done by considering grammatical relations or more semantically oriented metrics.</p><p>In the answer extraction phase, a baseline approach based on the extraction of named entities from the expected answer type was combined with a rule-based method based on shallow semantics. We used logical graphs, and the answer extraction rules were learnt automatically from a small corpus of questions and answer sentences where the exact answers were manually annotated. Some initial experiments on this method were tried in the 2005 system. In the 2006 system, the method was reimplemented from scratch to increase speed.</p><p>For the 2005 competition, the system had been completely redesigned and rewritten <ref type="bibr" coords="1,315.00,369.86,146.46,10.91">(Mollá and van Zaanen, 2006)</ref>. Based on our experiences from last year, we have decided to again rewrite large parts. Mainly third party tools that were integrated in the 2005 system introduced many problems, including memory leaks and random crashes. By isolating these problematic components and rewriting parts of these, the 2006 system is much more stable and much faster.</p><p>The rest of the article is structured as follows. We will first describe an overview of the AnswerFinder system in Sections 2 and 3, followed by a discussion on AnswerFinder in this year's competition. Next, we will do an error analysis of the system, followed by conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AnswerFinder overview</head><p>AnswerFinder is a question answering system mainly developed for research in the use of shallow semantic representations of text. The underlying rationale behind this research is the aim for reducing the impact of paraphrases in text. There are different ways to say the same thing (or something very similar). When looking for answers to a question, it is necessary to reduce the impact of this.</p><p>The system consists of several phases, each reducing the amount of data that needs to be processed. This allows the system to perform more complex and computationally intensive algorithms further down the pipeline. Figure <ref type="figure" coords="2,291.65,75.02,5.45,10.91">1</ref> gives a graphical representation of the system. Each of the phases will be described in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Document Selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Analysis</head><p>Sentence Selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Type</head><p>Answer Selection Final Answer(s)</p><p>Figure <ref type="figure" coords="2,121.21,283.82,4.22,10.91">1</ref>: AnswerFinder system overview Document selection When the system receives a question from the user, it will first search for documents relevant to the question in the full document collection. This is called the document selection phase. The aim in this phase is to only select relevant documents and thereby removing most of the document collection for further processing.</p><p>In previous competitions, we have always relied on the preselected documents provided by NIST. NIST provides the ranking of the top 1,000 documents retrieved by the PRISE search engine when using the target as the query. Last year, we selected the best 50 documents from the list of relevant documents.</p><p>This year, instead of having a list of documents per topic, as generated by NIST, we have experimented with a list of documents per question. The idea behind this is that the document selection is better tuned to the question. The list was generated using a document retrieval (DR) system built using the indexing and retrieval methods of Xapian<ref type="foot" coords="2,196.32,600.50,4.23,7.29" target="#foot_0">1</ref> toolkit.</p><p>The list of relevant documents for each question is found by taking the words in the question as search keywords in the DR system. Additionally, since not every question contains the topic words, the queries for the DR stage were extended using the words of the topic.</p><p>The introduction of the question words in the query may, in some cases, introduce undesirable information that could cause a query drift. To avoid this, we implemented another document selection system that performs a filtering process that only includes a document when it was found by the DR and was in the original NIST list.</p><p>Sentence selection Once the relevant documents are selected during the document selection phase, the sentence selection phase is started. The documents are segmented, resulting in a set of sentences. The size of this set of sentences is reduced by only selecting sentences that are considered relevant to the question.</p><p>Several sentence selection methods have been implemented. Each of the methods rely on a distance metric between a question and a sentence. The distance is computed for each of the sentences and based on that metric the best n are selected.</p><p>The simplest metric is word overlap. The score of a sentence with respect to a question is the number of words that can be found in both. To reduce the impact of function words (such as "the" and "and"), we do not take these into account. The idea behind this metric is that a sentence should say something about one of the content words of the question. The answer will probably not be in the sentence if the sentence is about something else.</p><p>The next metric is called grammatical relation overlap. The underlying idea is the same as that of word overlap. However, instead of simply counting words in both question and sentence, the number of grammatical relations <ref type="bibr" coords="2,315.00,483.74,100.74,10.91" target="#b0">(Carroll et al., 1998)</ref> are counted. Grammatical relations are dependency relations computed by the Connexor Functional Dependency Parser <ref type="bibr" coords="2,315.00,521.42,158.95,10.91" target="#b5">(Tapanainen and Järvinen, 1997)</ref>. In addition to having the same words in question and sentence, they must also be in a same relation to each other.</p><p>Additionally, previous versions of An-swerFinder also implemented logic form overlap. Logical forms can be generated from the grammatical relations that are output by Connexor.</p><p>They are shallow semantic representations, which abstract away even more over the grammatical realisation of the actual sentence and question. The advantage of using this representation is that the impact of paraphrasing is reduced.</p><p>However, the computation of the overlap was done using SICStus Prolog, which seemed to introduce memory leaks.</p><p>Instead of using the flat logical forms, in the current version we use a different representation of the same information. Last year, we introduced logical graph overlap. The implementation used then was slow and this year, the algorithm that computes the overlap between the graphs has been completely redesigned. The format of the logical graphs and the method used is presented in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question analysis</head><p>Once the relevant sentences are selected, the actual answer selection process starts. The extraction of possible answers is discussed in the next section. After the answers are extracted, an answer selection step is performed. This selects the actual answer (one or more) that will be returned to the user. During the answer selection step, information from the question is used. The question is analysed during the question analysis phase for this information.</p><p>The question analysis phase looks for the kind of answer the question is asking for. The question is matched against a list of regular expressions. Depending on which regular expression matches, it indicates the question type. For example, if the regular expression "Where" matches the question, the answer should be a location.</p><p>Answer selection From the small set of relevant sentences, possible answers are extracted.</p><p>Again, several answer extraction methods have been implemented. At the moment, only the logical graph overlap sentence selection method finds possible answers while computing the similarity score of a sentence with respect to a question (the previous AnswerFinder system also used the logical form overlap, which found possible answers as well).</p><p>In addition to the answers found by the sentence selection method, we find named entities in sentences and keep these as possible answers. We are currently investigating the requirements for the used named entity recogniser <ref type="bibr" coords="3,252.61,588.02,44.33,10.91;3,72.00,600.62,43.96,10.91" target="#b3">(Mollá et al., 2006)</ref>. However, for this competition, we used GATE's ANNIE named entity recogniser<ref type="foot" coords="3,289.08,612.02,4.23,7.29" target="#foot_1">2</ref> , like in previous competitions.</p><p>Finally, all found possible answers are considered. The scores associated with the answers are computed. The logical graph overlap metric assigns a score to the answer and answers found as named entities receive a score of one.</p><p>The answers are now matched against the question type. The answer has to be of a type that fits with what the question is asking for. Within these answers, the one with the highest score is returned. For non-factoid questions, all answers that match the correct question type are returned. If no such answer exists, the question type restriction is removed and all possible answers are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Logical Graphs</head><p>The main goal of using logical graphs is to provide a graph representation of the shallow semantics of the sentences. By using a graph representation it becomes possible to use graphbased algorithms to compute the similarity between sentences and to find the answer to a question <ref type="bibr" coords="3,359.53,253.10,63.28,10.91" target="#b3">(Mollá, 2006)</ref> Relations Relations act as links between concepts. To facilitate the production of the</p><p>LGs we have decided to use relation labels that represent verb argument positions. Thus, the relation 1 indicates the link to the first argument of a verb (that is, what is usually a subject). The relation 2 indicates the link to the second argument of a verb (usually the direct object), and so forth. Furthermore, relations introduced by prepositions are labelled with the prepositions themselves. Our relations are therefore close to the syntactic structure.</p><p>By using graphs, the task of finding the common elements between a question and a text sentence is reduced to the task of finding the maximum common subgraph (MCS) between the corresponding LGs. The similarity between two sentences can be computed as the size of the resulting MCS.</p><p>To find the exact answer we use the MCS to automatically learn question-answering rules. The method is basically the same as described in our entry to TREC 2005 and is presented in Figure <ref type="figure" coords="3,364.46,660.14,4.22,10.91" target="#fig_0">2</ref>. Given a question with LG q, an answer candidate sentence with LG s and the actual answer with LG a, the rules learnt have the following structure: R p is the MCS of q and s, that is, M CS(q, s). R e is the path between the projection of R p in s and the actual answer a.</p><p>R a is the graph representation of the exact answer, that is, a.</p><p>FOR every question/answerSentence pair q = the graph of the question s = the graph of the answer sentence a = the graph of the exact answer FOR every overlap O between q and s FOR every path P between O and a Build a rule R of the form The rules learnt this way are generalised so that they can be applied to unseen data. The process to generalise rules takes advantage of the two kinds of vertices. Basically, relation vertices represent names of relations and we considered these to be important in the rule. Consequently relation vertices are left unmodified in the generalised rule. Concept vertices are generalised by replacing them with generic variables, except for a specific set of "stop concepts" which are left unmodified. The list of stop concepts is very small: and, or, not, nor, if, otherwise, have, be, become, do, make</p><formula xml:id="formula_0" coords="4,113.88,189.73,36.61,32.78">R p = O R e = P R a = a</formula><p>The only difference between the system presented in TREC 2006 and that of TREC 2005 is the implementation of the algorithm that computes the MCS, which is based on the algorithm developed by <ref type="bibr" coords="4,137.65,509.54,154.00,10.91" target="#b4">(Myaeng and López-López, 1992</ref>) and summarised in Figure <ref type="figure" coords="4,208.70,522.02,4.22,10.91">3</ref>. The basic approach of the algorithm is to reduce the size of the graphs in a preliminary stage by merging some of the vertices of the related association graph. The problem of finding the MCS of two graphs is NP-complete but, given that the graphs are relatively small, the algorithm is fast enough for the task of learning and applying the QA rules.</p><p>The system used in TREC 2006 applies the above method to logical graphs, but the methodology is independent of the actual graph formalism. We are exploring the use of other graph representations, such as syntactic dependencies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Build a new association graph G ′</head><p>A by taking each set of IN (x,y) and OU T (x,y) as a vertex and the compatibility between vertices as an edge 6. An MCS is a clique of G ′ A Figure <ref type="figure" coords="4,354.85,226.34,4.22,10.91">3</ref>: Sketch of the MCS algorithm by <ref type="bibr" coords="4,330.13,238.82,159.17,10.91" target="#b4">(Myaeng and López-López, 1992)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TREC 2006</head><p>This year, our aim was to consolidate the implementation we are current working on. The results generated for last year's competition required a lot of manual restarting of the system due to memory leaks and bugs. This year we concentrated on removing the components that created these problems. The current system is much faster and much more stable.</p><p>Unfortunately, while running the final test experiments before submitting the actual results for this year, we found a bug in the system in the final answer selection phase. Due to time constraints, we could not fix this bug in time, which meant that the submitted results were generated by the system with this bug.</p><p>It turns out that when a sentence contains several possible answers, all of these answers are counted as many times as there are answers in that sentence. This means that the system has a high preference for answers that occur in sentences that also contain many other (possibly incorrect) answers. In practice, even fewer sentences were considered than were returned by the sentence selection phase.</p><p>We will now describe the parameters of the system that were used to generate the results submitted to the TREC QA track and investigate how the results would be different when the bug had been fixed. Next, we will take a look at the error analysis of the system on the TREC 2005 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameters</head><p>We have submitted three runs: lf10w10g5, lf10w10g5l5, and lf10w20g5l5. These names already indicate the most important settings of the system. The run tags can be divided into different components. The first part of the run "lf" indicates that we have used our own document selection method. The "l" stands for Luiz (Pizzato), who implemented the information retrieval part and the "f" stands for filtered. His system filters the preselected documents provided by NIST.</p><p>The rest of the tags are composed of sentence selection methods. The "w" stands for word overlap sentence selection and the number indicates the number of selected sentences based on that metric. Similarly, "g" stands for grammatical relation overlap and "l" is the logical graph overlap sentence selection method.</p><p>Finding possible answers is done using logical graph overlap (when that sentence selection method is used) and by applying the ANNIE named entity recogniser to the remaining sentences after sentence selection. The best answer is then selected as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>AnswerFinder currently concentrates on answering factoid questions and therefore, we will look at the results of these kinds of questions. List type questions are answered by returning all answers that are found in the sentences that are selected and match the question type. "Other" type questions are found by treating "What is topic ?" as a list question. We do not expect these types of questions to return very useful answers.</p><p>Even when considering only factoid questions, the results of the submitted runs are very disappointing. Accuracies of 0.072, 0.042, and 0.040 are reported for runs lf10w10g5, lf10w10g5l5, and lf10w20g5l5, respectively.</p><p>To be able to compare the submitted results to the results of the system with the bug fixed (and with the same settings), we have used Ken Litkowski's patterns to compute the results of the runs. The results are shown in Table <ref type="table" coords="5,288.50,609.26,4.22,10.91" target="#tab_2">1</ref>. The "submitted" part in the table contains the results we submitted to the competition, while "fixed" are the results with the bug fixed.</p><p>The parameters used in the submitted runs have been selected by performing a simple parameter search by hand on the 2005 question answering track data. Due to time constraints, only low values of the parameters have been tried. Although one would expect that a larger number of selected sentences would perform better, initial parameter tuning indicated that this is not the case. In the next section, we will investigate this further.</p><p>The fixed results are actually more like what we would expect. Fixing the bug increased the results. However, the systems with logical graphs are clearly outperformed by the one without. The main reason for that is the way answers are combined. Logical graphs can generate answers with much higher scores than those generated from named entities. This indicates that a better method for consolidating the scores of the answers needs to be designed. Finally, as expected, the system that selects the best sentences from 20 sentences selected based on word overlap outperforms that which is based on 10 sentences.</p><p>You have to keep in mind that we did our parameter tuning beforehand and we used the system with the bug in it. We cannot expect the parameters to be optimal. Further parameter tuning may improve the results greatly with the current system. In fact, one would expect that returning more (&gt; 20) sentences after sentence selection may result in higher scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Error analysis</head><p>In this section, we will take a closer look at where the system looses the correct answers. We will investigate all the phases in turn, starting with the document selection phase, followed by the sentence selection phases, and finally we quickly look at the answer selection phase.</p><p>We can evaluate the results of the system after each phase by taking the full output of that phase as an answer. For example, after document selection, we concatenate the contents of all selected documents and pretend that that is the answer. We can then compute the score using Ken Litkowski's patterns (as substring matching) and take that as the upper bound of the percentage of questions that can still be answered.</p><p>After each phase, we expect the score to go down. The aim is to reduce the amount of data as much as possible on one hand and keep the score as high as possible on the other hand. In other words, we want to reduce the data that does not contain the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Document selection</head><p>The first phase in AnswerFinder is the document selection phase. From the 1,033,461 documents contained in the Aquaint corpus, only a small selection can be handled in the next phases due to time and memory constraints.</p><p>In Figure <ref type="figure" coords="6,131.05,496.22,4.22,10.91" target="#fig_1">4</ref>, we show the percentage of questions that can still be answered after document selection with different selection methods. PRISE is the system provided by NIST, whereas Xapian is the system developed for An-swerFinder. The combination of PRISE and Xapian is the list of documents found by the Xapian system that are also found in the PRISE list. The last system is used the current An-swerFinder system.</p><p>As could be expected, increasing the number of documents for further processing will increase the upper bound of the number of questions that can be answered. The combined Xapian and PRISE systems outperform the PRISE system by itself. However, after about 50 documents, all upper bounds do not increase much anymore, whereas the amount of data that needs to be considered in the next phase does increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentence selection</head><p>The first two curves of Figure <ref type="figure" coords="6,454.81,357.14,5.45,10.91" target="#fig_2">5</ref> indicate the percentage of questions that can still be answered against the number of sentences selected using word overlap. Here, we see that overall, we loose several percent of answerable questions with respect to document selection. This is obvious, since we remove a lot of sentences from the set of sentences contained in the selected documents.</p><p>Selecting from sentences from 50 documents generates better results than selecting from 10 documents. However, this difference is relatively small when only a small number of sentences are selected. This difference can perhaps be explained from the fact that the maximum score when selecting 10 documents is just over 80% whereas after selecting 50 documents still over 90% possible questions can be answered.</p><p>The third curve of Figure <ref type="figure" coords="6,458.91,572.30,5.45,10.91" target="#fig_2">5</ref> indicates what happens when we select 50 documents, from these select 100 sentences using word overlap and from these 100 sentences, select sentences based on grammatical relation overlap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Answer selection</head><p>Once the sentence selection phase has been done, the actual answers are found. Named entities are found in the relevant sentences and if logical graph sentence selection was used, the found answers in these are taken into account as well.</p><p>Table <ref type="table" coords="7,114.00,445.82,5.45,10.91" target="#tab_3">2</ref> illustrates the results of this phase. The first column contains the percentage of questions that can still be answered after sentence selection. The next column has the percentage of questions that can be answered using the named entities and answers found by the logical graphs. The third column gives the results when the question type restriction is applied and the final column gives the actual system output.</p><p>The results of the experiments including the logical graphs show some interesting aspects of the system. The first two runs perform similarly up to the point where question information is used to select answers. This is interesting. Apparently, the logical graphs do not add many more new answers. However, the scoring of the answers found by the logical graphs is typically higher than that of the named entities. The fact that the full system results are quite similar indicates that the answers found by logical graphs are still quite good. A better integration of the scores of the named entities and logical graphs could possibly further increase the results.</p><p>Interestingly, selecting the best 5 sentences from 20 sentences selected based on word overlap gives worse results. Apparently selecting sentences using grammatical relations should only be performed on relatively small collections. The rest of the third system simply percolates the slightly lower results.</p><p>From the final results, we see that in this case, we lost about half of the answers (that were still possible after document selection) to sentence selection. Finding possible answers seems to work quite well, although we still lost over 10%. The question type hardly lost us answerable questions, but the final selection should be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and future work</head><p>The current implementation of AnswerFinder is efficient and stable. All of the phases have different methods, which can be used in combination with each other.</p><p>The interaction between the algorithms in the different phases needs to be investigated further. In this article, we have included an error analysis, which indicates that choosing the parameters for each of the phases is not trivial. The choice for parameters is driven by two conflicting goals: reducing the amount of data and keeping the number of questions that can be answered as high as possible. We need to measure the impact of selecting parameters on these two different goals to come to the best setting for the whole system. At the moment, selecting the exact answers from the set of possible answers is the phase where most answers are lost. This is to be expected as it is in a sense the most difficult phase. The final reduction to an exact answer has to be made. Whereas previous phases are allowed to include some spurious text for further processing, the final phase does not have this luxury. This is the phase that we will focus our work on for the next competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,104.88,234.02,159.38,10.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning of graph rules</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,204.48,252.26,202.74,10.91"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results after document selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,207.96,252.26,195.77,10.91"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results after sentence selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,315.00,253.10,225.07,85.55"><head></head><label></label><figDesc>. A logical graph (LG) is a directed, bipartite graph with two types of vertices, concepts and relations.</figDesc><table /><note coords="3,315.00,302.66,224.86,10.91;3,336.84,315.14,203.23,10.91;3,336.84,327.74,99.39,10.91"><p>Concepts Examples of concepts are objects dog, table, events and states run, love, and properties red, quick.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,335.52,24.85,189.57,124.29"><head></head><label></label><figDesc>1. Find the association graph G A 2. For each vertex (x, y) in G A , find the set of incoming vertices IN (x,y) and outgoing vertices OU T (x,y) in the original graphs 3. Split IN (x,y) and OU T (x,y) into sets of compatible vertices 4. Propagate elements of IN (x,y) by exploring the value of IN (z,w) when (x, y) is in IN (z,w) for active vertices of G A ; do similar treatment with OU T (x,y)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,315.00,31.34,224.75,99.23"><head>Table 1 :</head><label>1</label><figDesc>Results according to Ken Litkowski's</figDesc><table coords="5,315.00,43.94,221.61,86.63"><row><cell cols="2">patterns on TREC2006 data</cell><cell></cell></row><row><cell></cell><cell>submitted</cell><cell>fixed</cell></row><row><cell></cell><cell cols="2">correct exact correct exact</cell></row><row><cell>lf10w10g5</cell><cell>8.5% 7.8%</cell><cell>9.8% 7.9%</cell></row><row><cell>lf10w10g5l5</cell><cell>5.4% 4.1%</cell><cell>6.8% 5.3%</cell></row><row><cell>lf10w20g5l5</cell><cell>4.9% 3.9%</cell><cell>7.0% 5.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,107.04,31.34,397.87,75.83"><head>Table 2 :</head><label>2</label><figDesc>Results from sentence selection to final system output on TREC2005 data</figDesc><table coords="8,165.96,43.70,280.04,63.47"><row><cell></cell><cell cols="2">sentence possible</cell><cell>with</cell><cell>full</cell></row><row><cell></cell><cell cols="4">selection answers question type system</cell></row><row><cell>lf10w10g5</cell><cell>44.7%</cell><cell>30.8%</cell><cell>28.2%</cell><cell>8.9%</cell></row><row><cell>lf10w10g5l5</cell><cell>44.7%</cell><cell>30.8%</cell><cell>28.2%</cell><cell>8.2%</cell></row><row><cell>lf10w20g5l5</cell><cell>43.8%</cell><cell>29.6%</cell><cell>27.0%</cell><cell>7.6%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.56,699.78,98.31,8.27"><p>http://www.xapian.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,88.56,699.78,84.27,8.27"><p>http://gate.ac.uk/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is funded by the <rs type="funder">Australian Research Council</rs>, <rs type="grantName">ARC Discovery Grant</rs> no. <rs type="grantNumber">DP0450750</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tzpAq8w">
					<idno type="grant-number">DP0450750</idno>
					<orgName type="grant-name">ARC Discovery Grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,392.30,225.08,10.91;8,82.92,404.90,214.19,10.91;8,82.92,417.50,166.23,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,143.06,404.90,154.05,10.91;8,82.92,417.50,70.21,10.91">Parser evaluation: a survey and a new proposal</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Sanfilippo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,176.28,417.50,66.58,10.91">Proc. LREC98</title>
		<meeting>LREC98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,430.46,224.83,10.91;8,82.92,443.06,214.02,10.91;8,82.92,455.66,213.95,10.91;8,82.92,468.14,93.04,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,82.92,443.06,141.04,10.91">Answerfinder at TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,272.16,455.66,24.71,10.91;8,82.92,468.14,55.01,10.91">Proc. TREC 2005</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>TREC 2005</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,481.22,225.08,10.91;8,82.92,493.82,214.06,10.91;8,82.92,506.30,214.18,10.91;8,82.92,518.90,92.55,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,157.44,493.82,139.54,10.91;8,82.92,506.30,90.91,10.91">Named entity recognition for question answering</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pizzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,203.64,506.30,93.46,10.91;8,82.92,518.90,20.09,10.91">Proceedings ALTW 2006</title>
		<meeting>ALTW 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">pages</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,531.98,67.00,10.91;8,154.69,531.98,24.63,10.91;8,195.13,531.98,101.94,10.91;8,82.92,544.58,213.95,10.91;8,82.92,557.06,214.13,10.91;8,82.92,569.66,213.87,10.91;8,82.92,582.14,59.79,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,195.13,531.98,101.94,10.91;8,82.92,544.58,156.82,10.91">Learning of graphbased question answering rules</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,272.16,544.58,24.71,10.91;8,82.92,557.06,214.13,10.91;8,82.92,569.66,209.12,10.91">Proc. HLT/NAACL 2006 Workshop on Graph Algorithms for Natural Language Processing</title>
		<meeting>HLT/NAACL 2006 Workshop on Graph Algorithms for Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,595.22,224.94,10.91;8,82.92,607.82,214.17,10.91;8,82.92,620.30,214.15,10.91;8,82.92,632.90,214.04,10.91;8,82.92,645.50,99.64,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,112.92,607.82,184.16,10.91;8,82.92,620.30,129.42,10.91">Conceptual graph matching: a flexible algorithm and experiments</title>
		<author>
			<persName coords=""><forename type="first">Sung</forename><forename type="middle">H</forename><surname>Myaeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aurelio</forename><surname>López-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,224.76,620.30,72.31,10.91;8,82.92,632.90,214.04,10.91;8,82.92,645.50,42.58,10.91">Journal of Experimentation and Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,658.58,225.06,10.91;8,82.92,671.06,213.95,10.91;8,82.92,683.66,77.79,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,288.88,658.58,8.18,10.91;8,82.92,671.06,162.12,10.91">A non-projective dependency parser</title>
		<author>
			<persName coords=""><forename type="first">Pasi</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Järvinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,272.16,671.06,24.71,10.91;8,82.92,683.66,71.39,10.91">Proc. ANLP-97. ACL</title>
		<meeting>ANLP-97. ACL</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
