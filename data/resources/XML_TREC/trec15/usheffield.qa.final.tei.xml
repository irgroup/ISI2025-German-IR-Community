<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.88,82.56,420.46,15.49">The University of Sheffield&apos;s TREC 2006 Q&amp;A Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.04,119.99,96.36,10.76"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
							<email>m.greenwood@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.16,119.99,77.74,10.76"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
							<email>m.stevenson@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.96,119.99,89.97,10.76"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
							<email>r.gaizauskas@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.88,82.56,420.46,15.49">The University of Sheffield&apos;s TREC 2006 Q&amp;A Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46C050BC8CF0263CE5BED74B22B1B53A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a natural language processing group (NLP) our original approach to question answering was linguistically motivated culminating in the development of the QA-LaSIE system <ref type="bibr" coords="1,292.80,248.80,100.64,8.97" target="#b6">(Humphreys et al., 1999)</ref>. In its original form QA-LaSIE would only propose answers which were linked via syntactic/semantic relations to the information missing from the question (for example "Who released the Internet worm?" is missing a person). While the answers proposed by the system were often correct, the system was frequently unable to suggest any answer. The next version of the system loosened the requirement for a link between question and answer which improved performance <ref type="bibr" coords="1,336.48,296.56,115.64,8.97" target="#b10">(Scott and Gaizauskas, 2000)</ref>. There are still a number of open questions from the development of the QA-LaSIE system: does the use of parsing and discourse interpretation to determine links between questions and proposed answers result in better performance than simpler systems which adopt a shallower approach? Is it simply that the performance of our parser is below the level at which it could contribute to question answering? Are there questions which can only be answered using deep linguistic techniques? With the continued development of a second QA system at Sheffield which uses shallower techniques <ref type="bibr" coords="1,382.08,356.32,98.12,8.97" target="#b2">(Gaizauskas et al., 2005)</ref> we believe that we are now in a position to investigate these and related questions. Our entries to the 2006 TREC QA evaluation are designed to help us answer some of these questions and to investigate further the possible benefits of linguistic processing over shallower techniques.</p><p>The remainder of this paper is organised as follows. Firstly the framework in which our systems are developed is described in section 2 along with the QA system components. Section 3 describes the configurations and aims of our evaluation runs. Section 4 discusses the official evaluation results of our submitted runs in relation to the research questions outlined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QA Framework</head><p>To simplify both the development and evaluation of our multiple approaches to question answering the separate components are hosted within a newly developed framework. This framework abstracts away from specific implementations of the components within a QA system. This allows us to develop multiple competing components which can easily be substituted for one another within a QA system. The framework also allows us to provide a number of common processing resources which not only simplifies the development of the more complex components, but also ensures the consistent lower level treatment of texts to allow for valid performance comparisons between competing components. The remainder of this section details both the common processing available as well as the component implementations used by the runs submitted for evaluation which are described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Common Text Processing</head><p>There are a number of language processing tools that most (if not all) question answering systems will require. These include relatively low level tasks, such as tokenization and sentence splitting, as well as more complex task, such as named entity recognition (or as we refer to the more abstract case -semantic entity tagging). The framework provides these resources by utilizing the GATE framework <ref type="bibr" coords="1,184.20,655.24,105.92,8.97" target="#b0">(Cunningham et al., 2002)</ref>, also developed within the NLP group at Sheffield. Specifically we utilize extended versions of the ANNIE components to allow tokenization, sentence splitting, part-of-speech tagging, morphological analysis, gazetteer lookup and semantic entity recognition. This common set of processing components ensures that the same basic information can be extracted from the examined texts irrespective of the approach to question answering used. This makes comparisons between differing QA approaches easier to carry out and their results more reliable.</p><p>Whilst these components provide a solid foundation for our QA systems it does not address the problem of there being multiple ways of representing identical pieces of information. The answers to many questions can be represented in many ways and as most QA systems rely, at least in part, on the frequency of occurrence of competing candidate answers (see e.g. <ref type="bibr" coords="1,511.08,743.92,44.25,8.97;1,56.64,755.80,26.84,8.97" target="#b7">Light et al. (2001)</ref> the ability to accurately compare candidate answers is important. To this end all dates and numbers are normalised to a standard format. Dates are converted to a standard numerical format including resolving partial or descriptive dates (such as today or tomorrow) against the date of the newswire article. Numbers, both isolated and within measurements, are converted to a plain numeric form, e.g. 3000, 3,000, and three thousand are all represented as 3000.</p><p>While such normalisation helps with the comparison of answers, and ultimately, in the ranking used to determine the most likely answer, it does not address issues of ambiguity in proposed answers. For example dates provided as answers can often be incomplete and ambiguous simply because of the way they appear in text. A newswire article discussing a recent or ongoing event will simply give the date as the day and month without the year as this is implicitly specified by the date of the document. We solve this specific issue by ensuring that when a date is proposed as an answer it is expanded to a full non-ambiguous form (i.e. contains a day, month and year) from the information present in the supported document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval with Lucene</head><p>All three of our runs use the open-source Lucene<ref type="foot" coords="2,252.84,191.56,3.49,6.28" target="#foot_0">1</ref> IR engine to index and access the AQUAINT collection. Each document is split into separate paragraphs using the embedded SGML paragraph tags. All remaining SGML tags are then removed and each paragraph, after having been stemmed <ref type="bibr" coords="2,234.36,217.00,56.12,8.97" target="#b8">(Porter, 1980)</ref> and stopwords removed, is added to the Lucene index along with the unique document ID and associated date.</p><p>Our current approaches to answering factoid and list questions use the same approach to retrieve relevant documents for further processing. The question and target are first combined to form a single IR query (using the approach documented by <ref type="bibr" coords="2,70.08,269.80,98.36,8.97" target="#b2">Gaizauskas et al. (2005)</ref>) and then this query is used to retrieve the twenty most relevant passages from the AQUAINT collection. The use of only the top twenty passages is based on a number of experiments <ref type="bibr" coords="2,406.80,281.80,97.57,8.97" target="#b1">(Gaizauskas et al., 2003;</ref><ref type="bibr" coords="2,506.40,281.80,49.17,8.97;2,56.64,293.80,23.48,8.97" target="#b5">Greenwood, 2006)</ref> carried out using combinations of various IR engines and QA systems which all suggest that while retrieving more text means greater coverage (i.e. the percentage of questions for which at least one relevant passage is retrieved <ref type="bibr" coords="2,503.16,305.68,52.26,8.97;2,56.64,317.68,69.01,8.97" target="#b9">(Roberts and Gaizauskas, 2004</ref>)) there comes a point at which the larger volumes of text actually inhibit the ability of answer extraction components to extract correct answers. Whilst this approach maximises our answer extraction performance (and hence the end-to-end performance of our QA systems) the coverage of the retrieved passages is approximately 52% <ref type="bibr" coords="2,477.48,341.56,78.08,8.97" target="#b5">(Greenwood, 2006)</ref> which means that the maximum accuracy the QA systems could achieve would also be 52% (i.e. we cannot answer a question if we do not retrieve any answer bearing documents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answering Factoid and List Questions</head><p>Once relevant documents have been retrieved we use two main approaches to answering factoid questions: a linguistic approach called QA-LaSIE and a shallow semantic tagging approach. Both can also be used to answer list questions simply by returning more than one answer<ref type="foot" coords="2,183.48,434.68,3.49,6.28" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">QA-LaSIE</head><p>QA-LaSIE has traditionally been our main approach to question answering and it has been used in each TREC QA evaluation in which we have participated. QA-LaSIE performs partial syntactic and semantic analysis of questions and candidate answer bearing documents and then performs matching over a derived logical form representation. The system has been described in detail in past TREC proceedings <ref type="bibr" coords="2,191.64,516.88,101.96,8.97" target="#b4">(Greenwood et al., 2002)</ref> and does not differ substantially to the version used in the 2005 TREC evaluation <ref type="bibr" coords="2,128.16,528.88,97.15,8.97" target="#b2">(Gaizauskas et al., 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Semantic Tagger</head><p>For TREC 2003 <ref type="bibr" coords="2,125.52,573.76,101.36,8.97" target="#b1">(Gaizauskas et al., 2003)</ref> we introduced a simple baseline system for answering factoid and list questions based around the premise that an answer to a question will be an entity from a fixed set of semantic types. Since its introduction this system has improved to the point where it is now consistently outperforming QA-LaSIE and is no longer considered a baseline system.</p><p>This system consists of two main components: a rule based question analyser and a semantic tagger based answer extraction component. The development of this system is documented in some detail by <ref type="bibr" coords="2,378.48,638.44,76.52,8.97" target="#b5">Greenwood (2006)</ref> for a brief overview see <ref type="bibr" coords="2,56.64,650.44,94.89,8.97" target="#b2">Gaizauskas et al. (2005)</ref>.</p><p>When using this approach to answer list questions we were previously heavily penalised for not returning any answers to questions for which the semantic type of the expected answer could not be accurately determined. In the 2005 evaluation <ref type="bibr" coords="2,56.64,691.24,97.52,8.97" target="#b2">(Gaizauskas et al., 2005)</ref> we experimented with a very simple system which guessed answers based on frequency of occurrence of base noun phrases. This system has also been incorporated into the framework and is again used to answer list questions when the semantic tagging approach fails to find any answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">PhDef</head><p>This component, which is used for answering other questions (previously referred to as "The Bare Target + Filter + Reduce Approach"), has not changed substantially from that used in the TREC 2005 evaluation <ref type="bibr" coords="3,422.16,160.36,100.88,8.97" target="#b2">(Gaizauskas et al., 2005)</ref>. For a detailed description of the development of this system see <ref type="bibr" coords="3,289.56,172.24,74.02,8.97" target="#b5">Greenwood (2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">DefSys</head><p>This component, which is used for answering other questions (previously referred to as "Target Enrichment + Filter Approach"), has not changed substantially from that used in the TREC 2005 evaluation <ref type="bibr" coords="3,394.92,229.12,97.16,8.97" target="#b2">(Gaizauskas et al., 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Run Configurations</head><p>The framework described in section 2 allows us to build QA systems by specifying the components to use for the different stages. For example a factoid QA systems consists of an IR engine followed by one or more answer extraction components.</p><p>In this way we configured two main runs for the main task of the 2006 TREC QA evaluation<ref type="foot" coords="3,425.64,302.20,3.49,6.28" target="#foot_2">3</ref> :</p><p>shef06qal This run used QA-LaSIE to answer factoid and list questions using documents retrieved from AQUAINT using Lucene. It uses Lucene and the DefSys component to answer the other questions.</p><p>shef06sem This run used the semantic tagging approach to answer factoid and list questions using documents retrieved from AQUAINT using Lucene. If this approach fails for list questions then the answers are guessed as described earlier in Section 2.3. It uses Lucene and the PhDef component to answer the other questions.</p><p>shef06ss This run is identical to shef06sem apart from the fact that factoid and list questions are first attempted using the surface matching patterns approach previously documented by <ref type="bibr" coords="3,333.36,416.32,138.70,8.97" target="#b3">Greenwood and Gaizauskas (2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The official results from our submitted runs are given in Table <ref type="table" coords="3,318.48,467.20,3.77,8.97" target="#tab_0">1</ref>. It is clear from these results that the semantic tagging approach to answering factoid and list questions greatly outperformed the linguistically motivated QA-LaSIE system. In contrast, the two approaches to answering other questions, PhDef and DefSys, show little difference in performance.</p><p>As yet no further analysis of the output of our approaches to question answering with respect to this evaluation have been carried out, and so it would be premature to speculate as to why there is such a difference in performance between the approaches to answering factoid questions. What is clear is that as the two approaches have access to the same information within the documents the semantic tagging approach is much better at selecting the correct answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,188.88,45.04,234.08,62.39"><head>Table 1 :</head><label>1</label><figDesc>Summary of official results from main task submissions.</figDesc><table coords="3,191.16,45.04,229.70,45.93"><row><cell>Tag</cell><cell>Factoid</cell><cell>List</cell><cell cols="2">Other Combined</cell></row><row><cell>shef06qal</cell><cell>0.057</cell><cell cols="2">0.029 0.127</cell><cell>0.071</cell></row><row><cell>shef06sem</cell><cell>0.171</cell><cell cols="2">0.106 0.126</cell><cell>0.134</cell></row><row><cell>shef06ss</cell><cell>0.171</cell><cell cols="2">0.106 0.128</cell><cell>0.134</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,68.64,733.20,134.98,7.05"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,68.64,744.56,486.46,7.17;2,68.64,753.92,348.55,7.17"><p>If less than ten answers are found then all are returned as answers to the list question otherwise the first ten answers are returned along with any others which have a score above 0.08 (chosen by empirical testing over questions from previous TREC evaluations).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,68.64,712.76,486.49,7.17;3,68.64,722.12,486.58,7.17;3,68.64,731.60,486.54,7.17;3,68.64,741.08,486.34,7.17;3,68.64,750.56,107.83,7.17"><p>A third run was submitted and is also documented here. This run is important only in testing one very small component which affected the answer returned for only four factoid questions in the entire test set, hence the results do not differ significantly from the shef06sem run. Our main aim was to compare the two main approaches to QA, linguistically motivated or shallow semantic tagging, and as such this third run does not contribute to the debate. The difference in the evaluation of other questions between the runs shef06sem and shef06ss is due to differences in the human judgements as the submissions were identical.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,56.64,605.68,498.85,8.97;3,56.64,617.68,498.70,8.97;3,56.64,629.56,171.69,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,408.72,605.68,146.77,8.97;3,56.64,617.68,269.81,8.97">GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications</title>
		<author>
			<persName coords=""><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Tablan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,346.56,617.68,208.78,8.97">Proceedings of the 40th Anniversary Meeting of the</title>
		<meeting>the 40th Anniversary Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,56.64,649.48,498.70,8.97;3,56.64,661.48,446.25,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,539.76,649.48,15.58,8.97;3,56.64,661.48,223.54,8.97">The University of Sheffield&apos;s TREC 2003 Q&amp;A Experiments</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Sargaison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,298.68,661.48,199.83,8.97">Proceedings of the 12th Text REtrieval Conference</title>
		<meeting>the 12th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,56.64,681.40,498.70,8.97;3,56.64,693.28,446.25,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,539.76,681.40,15.58,8.97;3,56.64,693.28,223.54,8.97">The University of Sheffield&apos;s TREC 2005 Q&amp;A Experiments</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henk</forename><surname>Harkema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Atheesh</forename><surname>Sanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,298.68,693.28,199.83,8.97">Proceedings of the 14th Text REtrieval Conference</title>
		<meeting>the 14th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,56.64,45.64,498.73,8.97;4,56.64,57.64,498.66,8.97;4,56.64,69.64,218.01,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,278.04,45.64,277.33,8.97;4,56.64,57.64,129.37,8.97">Using a Named Entity Tagger to Generalise Surface Matching Text Patterns for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,205.44,57.64,349.86,8.97;4,56.64,69.64,38.59,8.97">Proceedings of the Workshop on Natural Language Processing for Question Answering (EACL03)</title>
		<meeting>the Workshop on Natural Language Processing for Question Answering (EACL03)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-04-14">2003. April 14</date>
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,56.64,89.56,498.66,8.97;4,56.64,101.44,204.33,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,322.92,89.56,215.26,8.97">The University of Sheffield TREC 2002 Q&amp;A System</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,56.64,101.44,199.95,8.97">Proceedings of the 11th Text REtrieval Conference</title>
		<meeting>the 11th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,56.64,121.36,498.81,8.97;4,56.64,133.36,304.77,9.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,182.64,121.36,141.25,8.97">Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Greenwood</surname></persName>
		</author>
		<ptr target="http://www.dcs.shef.ac.uk/˜mark/nlp/pubs/" />
		<imprint>
			<date type="published" when="2006-10">2006. October 2006</date>
		</imprint>
		<respStmt>
			<orgName>The University of Sheffield. Available</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="4,56.64,153.28,498.71,8.97;4,56.64,165.28,245.49,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,395.28,153.28,160.07,8.97;4,56.64,165.28,27.33,8.97">University of Sheffield TREC-8 Q &amp; A System</title>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,102.84,165.28,194.91,8.97">Proceedings of the 8th Text REtrieval Conference</title>
		<meeting>the 8th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,56.64,185.20,498.78,8.97;4,56.64,197.08,202.05,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,335.76,185.20,219.66,8.97;4,56.64,197.08,45.25,8.97">Analysis for Elucidating Current Question Answering Technology</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,109.68,197.08,123.19,8.97">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,56.64,217.00,330.69,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,142.20,217.00,135.19,8.97">An Algorithm for Suffix Stripping</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,284.76,217.00,33.15,8.97">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,56.64,236.92,498.88,8.97;4,56.64,248.92,221.37,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,227.76,236.92,259.69,8.97">Evaluating Passage Retrieval Approaches for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,505.92,236.92,49.60,8.97;4,56.64,248.92,217.47,8.97">Proceedings of 26th European Conference on Information Retrieval</title>
		<meeting>26th European Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,56.64,268.84,498.61,8.97;4,56.64,280.84,88.77,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="4,231.24,268.84,191.37,8.97">University of Sheffield TREC-9 Q &amp; A System</title>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,444.48,268.84,110.77,8.97;4,56.64,280.84,84.39,8.97">Proceedings of the 9th Text REtrieval Conference</title>
		<meeting>the 9th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
