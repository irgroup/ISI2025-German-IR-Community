<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.30,115.51,372.40,19.27;1,221.82,140.05,151.65,19.27">SVM-Based Spam Filter with Active and Online Learning</title>
				<funder ref="#_K45HmCC">
					<orgName type="full">Key Project of Chinese Ministry of Education &amp; Microsoft Asia Research Centre</orgName>
				</funder>
				<funder ref="#_kQQWywW #_S9YuR4C">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.72,187.40,50.93,8.90"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
							<email>qwang@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.06,187.40,34.26,8.90"><forename type="first">Yi</forename><surname>Guan</surname></persName>
							<email>guanyi@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.75,187.40,63.78,8.90"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<email>wangxl@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.30,115.51,372.40,19.27;1,221.82,140.05,151.65,19.27">SVM-Based Spam Filter with Active and Online Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">11AACCC0C329C2D24704B314D89D3BFD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A realistic classification model for spam filtering should not only take account of the fact that spam evolves over time, but also that labeling a large number of examples for initial training can be expensive in terms of both time and money. This paper address the problem of separating legitimate emails from unsolicited ones with active and online learning algorithm, using a Support Vector Machines (SVM) as the base classifier. We evaluate its effectiveness using a set of goodness criteria on TREC2006 spam filtering benchmark datasets, and promising results are reported.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The well-documented problem of unsolicited email, or spam, is currently of serious and escalating concern.</p><p>To date most research in the area of spam detection has focused on some tasks like non-stationarity of the data source, severe sampling bias in the training data, and non-uniformity of misclassification costs. Unfortunately, the researches on spam filtering rarely takes account of the fact that spam should evolve over time and we should adopt an efficient strategy to accelerate the learning process.</p><p>In this paper, we explicitly address the necessity and feasibility of the adaptive learning strategy to spam filtering task. We present and evaluate a classification model for spam filtering with active and online learning algorithm. The research focuses on the use of Support Vector Machines (SVMs) as our base classifier, since their demonstrated robustness and ability to handle large feature spaces makes them particularly attractive for this task. Based on SVMs, an active learning algorithm that adopts informative-ness combined with diversity as selection criterion and an online learning strategy that only use the historical SV samples and the incremental training samples in re-training are studied, and their performance is evaluated on TREC2006 spam filtering benchmark datasets. Advantages of directly accounting for active and online learning methods are demonstrated.</p><p>In the remainder of this paper, we first give a short survey of related work, and then describe the active and online algorithm in detail. After that, we present our results of applying the algorithm to email classification. Finally, we give our conclusions and discuss the prospects for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The spam filtering problem has traditionally been presented as an instance of a text categorization problem with the categories being spam and ham. In reality, the structure of email is richer than that of flat text, with meta-level features such as the fields found in MIME compliant messages. Researchers have recently acknowledged this, setting the problem in a semi-structured document classification framework. Several solutions have been proposed to overcome the spam problem. Among the proposed methods, much interest has focused on the machine learning techniques in spam filtering. They include rule learning <ref type="bibr" coords="1,470.88,642.55,7.38,5.50" target="#b0">[1]</ref> , Naive Bayes <ref type="bibr" coords="1,111.24,653.71,6.74,5.50" target="#b1">[2,</ref><ref type="bibr" coords="1,119.94,653.71,5.34,5.50" target="#b2">3]</ref> , decision trees <ref type="bibr" coords="1,185.82,653.71,7.38,5.50" target="#b4">[4]</ref> , support vector machines <ref type="bibr" coords="1,296.88,653.71,7.44,5.50" target="#b5">[5]</ref> or combinations of different learners <ref type="bibr" coords="1,455.58,653.71,7.32,5.50" target="#b6">[6]</ref> . The basic and common concept of these inductive approaches is that using a classifier to filter out spam and the classifier is learned from training data rather than constructed by hand.</p><p>Usually spam filtering task is a continuous work with email sequence increasing in size, there is a need to scale up the learning algorithms to handle more training data. And since it is time consuming to retrain the classifier whenever a new example is added to the training set, it is more efficient from a computational point of view to minimize the number of labeled examples to learn a function at a certain accuracy level.</p><p>We can use Support Vector Machines (SVMs) and adopt the active and online learning techniques as possible solutions to these problems. SVMs have worked well for the incremental model learning <ref type="bibr" coords="2,476.88,122.29,6.79,5.50" target="#b7">[7,</ref><ref type="bibr" coords="2,485.22,122.29,5.34,5.50" target="#b8">8]</ref> and have shown impressive performance in the active learning <ref type="bibr" coords="2,341.76,133.51,7.32,5.50" target="#b9">[9]</ref> applications for its nice properties of summarizing data in the form of support vectors. The active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model. Tong and Koller <ref type="bibr" coords="2,450.96,156.43,10.50,5.50" target="#b10">[10]</ref> control the labeling effort and accelerate the learning process by using the current SVM classifier to query the instance closest to the decision hyperplane in terms of version space bisection. Brinker <ref type="bibr" coords="2,394.74,178.81,10.56,5.50" target="#b11">[11]</ref> first incorporate diversity criterion in active learning to maximize the training utility of a batch. The online learning is attractive for solving the problem of dynamic nature of data and drifting concepts. The elegant solution to online SVM learning is the incremental SVM which provides a framework for exact online learning. Syed <ref type="bibr" coords="2,476.16,213.08,11.39,5.95" target="#b12">[12]</ref> et al. proposed an incremental SVM learning algorithm, which uses only the historical SV samples and the incremental training samples in re-training. All non-SV samples are discarded after previous training. Xiao <ref type="bibr" coords="2,107.22,248.36,11.39,5.95" target="#b13">[13]</ref> et al. proposed a different incremental learning approach for SVM based on the boosting idea.</p><p>These scalability and accelerative learning task are almost confined to the text classification task and seldom discussed in spam filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Active and Online Learning for Spam Filtering Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Brief Introduction of SVM</head><p>The main idea of Support Vector Machine is to construct a nonlinear kernel function to map the data from the input space into a possibly high-dimensional feature space and then generalize the optimal hyper-plane with maximum margin between the two classes.</p><p>Given a T-element training set {(x i ,y i ):x i ∈R D ,y i ∈{-1,1},i=1,…,T} a linear SVM classifier: ( )</p><formula xml:id="formula_0" coords="2,157.98,376.01,329.83,27.13">i i i i i i i i F x a y x x b x a y x b x w b = ⋅ + = ⋅ + = ⋅ + ∑ ∑<label>(1)</label></formula><p>Where a i ≥0, b is a bias term and D is the dimension of the input space; • is a dot-product operator, and w is the normal vector of the classification hyperplane. Typically, the multipliers a i have non-zero values only for a small subset of the training set, which is called the support set and its elements the support vectors. The optimal hyperplane is found such as to maximize the classification margin, given by 2/‖w‖ 2 , where w denotes the normal vector of the hyperplane. The soft-margin optimization task is formulated as:</p><formula xml:id="formula_1" coords="2,203.88,493.22,283.93,45.42">2 1 min : 2 : ( ) 1 p i i i i imize C w subject to y w x b ξ ξ + ⋅ + ≥ - ∑<label>(2)</label></formula><p>Where C i ≥0, p≥0 and ξ i =(1-y(w•x+b));(z) + =z for z≥0 and is equal to 0 otherwise; The slack variables ξ i take non-zero values only for bound support vectors, i.e., points that are misclassified or lie inside the classifier's margin. Note that in eq.( <ref type="formula" coords="2,294.92,573.38,4.01,8.90" target="#formula_1">2</ref>) the accuracy over the training set is balanced by the "smoothness" of the solution. We will consider the case of p=1, for which a number of highly efficient computational methods have been developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Active Learning for Spam Filtering</head><p>So far we have considered learning strategies in which data is acquired passively. However, SVMs construct a hypothesis using a subset of the data containing the most informative patterns and thus they are good candidates for active or selective sampling techniques which seek out these patterns. Suppose the data is initially unlabeled, a good heuristic algorithm would predominantly request the labels for those patterns which will become support vectors. Active selection would be particularly important for practical situations in which the process of labeling data is expensive or the dataset is large and unlabeled.</p><p>Tong <ref type="bibr" coords="2,118.38,711.56,11.45,5.95" target="#b10">[10]</ref> proposed the Simple algorithm, which iteratively chooses h unlabeled instances closest to the separating hyperplane to solicit user feedback. Based on this algorithm, a first spam filter f can be trained on the labeled email pool L. Then the f is applied to the unlabeled e-mail pool U to compute each unlabeled e-mail's distance to the separating hyperplane. The h unlabeled e-mails closest to the hyperplane and relatively apart are chosen as the next batch of samples for conducting pool-queries.</p><p>However, the Simple algorithm may choose too many similar queries, which impairs the learning performance. We adopted diversity measure presented by Brinker <ref type="bibr" coords="3,371.16,157.22,11.15,5.95" target="#b11">[11]</ref> to construct batches of new training examples and enforces selected examples to be diverse with respect to their angles. The main idea is to select a collection of emails close to the classification hyperplane, while at the same time maintaining their diversity. The diversity of email is measured by the angles. For example, suppose x i has a normal vector equal to (x i ). The angle between two hyperplanes h i and h j , corresponding to the email instances x i and x j , can be written in terms of the kernel operator K:</p><p>The angle-diversity algorithm starts with an initial hyperplane h i trained by the given labeled email set L. Then, for each unlabeled email x i , it computes the distance to the classification hyperplane h i . The angle between the unlabeled x i and the current set S is defined as the maximal angle from x i to any other x j in set S. This angle measures how diverse the resulting S would be, if x i were to be chosen as a sample. Algorithm angle-diversity introduces a parameter to balance two components: the distance from emails to the classification hyperplane and the diversity of angles among different emails. Incorporating the trade-off factor, the final score for the unlabeled x i can be written as Where function f computes the distance to the hyperplane, function K is the kernel operator, and S is the training set. After that, the algorithm selects as the sample the unlabeled email that enjoys the smallest score in U. The algorithm repeats the above steps h times to select h emails. In practice, the complete method uses a weighted sum of diversity and hyperplane distance, controlled by a parameter λ, where λ= 0 is the equivalent of focusing solely on diversity and λ= 1 is the same as Simple. We determined that λ= 0.5 worked well for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online Learning for Spam Filtering</head><p>Online learning is an important domain in machine learning with interesting theoretical properties and practical applications. Online learning is performed in a sequence of trials. At trial t the algorithm first receives an instance x t ∈ R n and is required to predict the label associated with that instance. After the online learning algorithm has predicted the label, the true label is revealed and the algorithm pays a unit cost if its prediction is wrong. The ultimate goal of the algorithm is to minimize the total number of prediction mistakes it makes along its run. To achieve this goal, the algorithm may update its prediction mechanism after each trial so as to be more accurate in later trials.</p><p>In this paper, we assume that the prediction of the algorithm at each trial is determined by a SVMs classifier. Usually in SVMs only a small portion of samples have non-zero  α  i coefficients, whose corresponding x i (support vectors) and y i fully define the decision function. Therefore, the SV set can fully describe the classification characteristics of the entire training set. Because the training process of SVM involves solving a quadratic programming problem, the computational complexity of training process is much higher than a linear complexity. Hence, if we train the SVM on the SV set instead of the whole training set, the training time can be reduced greatly without much loss of the classification precision. This is the main idea of online learning algorithm.</p><p>Given that only a small fraction of training emails end up as support vectors, the support vector algorithm is able to summarize the data space in a very concise manner. The training <ref type="bibr" coords="3,449.76,700.70,11.45,5.95" target="#b12">[12]</ref> would use only the historical SV samples and the incremental training samples in re-training. All non-SV samples are discarded after previous training. Figure <ref type="figure" coords="3,304.56,726.74,5.11,8.90">1</ref> shows the incremental training procedure.</p><formula xml:id="formula_2" coords="3,152.40,236.22,330.85,173.30">( ) ( ) ( , ) cos( ( , )) ( ) ( ) ( , ) ( , ) i j i j i j i j i i i j x x K x x h h x x K x x K x x Φ ⋅Φ ∠ = = Φ Φ (3) ( , ) ( ) (1 ) (max ) ( , ) ( , ) j i j i x S i i j j k x x f x K x x K x x λ λ ∈ ⋅ + -⋅<label>(4)</label></formula><p>Let us call the training email set as TR, so we trained an initial spam filter on TR. Then we took the support vectors chosen from TR, called SV 1 , and classify the instances x 0 in trial sequence. After querying the true label of x i , we add x i to SV 1 if the filter's prediction is wrong. Again we ran the SVM training algorithm on SV 1 + x 0 . Now the SVs chosen from the training of SV 1 + x 0 were taken, let us call these SV 2 . Again the training and testing were done. This incremental step was repeated until all the trials are used. The algorithm can be illustrated in algorithm 1 as follows.</p><p>The model obtained by this method should be the same or similar to what would have been obtained using all the data together to train. The reason for this is that the SVMs algorithm will preserve the essential class boundary information seen so far in a very compact form as support vectors, which would contribute appropriately to deriving the new concept in the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluations and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head><p>In this section, we report the test results on four email datasets provided by TREC 2006 spam track. The basic statistics for all four datasets are given in Table <ref type="table" coords="4,324.45,537.44,3.82,8.90" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ham Spam Total</head><p>Trec06p The experiment is evaluated by a number of criteria that were used for the official evaluation:</p><p>• hm%: Ham Misclassification Rate, the fraction of ham messages labeled as spam.</p><p>• sm%: Spam Misclassification Rate, the fraction of spam messages labeled as ham.</p><p>• 1-ROCA: Area above the Receiver Operating Characteristic (ROC) curve.</p><p>• Lam%: logistic average misclassification percentage defined as, lam% = logit -1 (logit(hm%)/2 + logit(sm%)/2), where logit(x) = log(x/(1-x)) and logit -1 (x) = e x /(1+e x ). The spam track includes two approaches to measuring the filter's learning curve: (1)   <ref type="formula" coords="5,180.63,112.88,3.97,8.90" target="#formula_1">2</ref>) cumulative (1-ROCA)% is given as a function of the number of messages processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experiment I: The Impact of Active Learning Method</head><p>The first experiment was designed to find whether the active learning strategy perform similarly on two-class spam filtering task with Multivariate Performance Measures like 1-ROCA in mind. To this end, the spam filter is first requested to select a sequence of messages from the first 90% of the corpus as "teach me" examples. That is, the filter is trained on the correct classification for these, and only these examples. Secondly, from time to time (after 100, 200, 400, etc. "teach me" examples) the filter is asked to classify the remaining 10% of the corpus, one-at-a-time, in order. The performance of active selection (AL) and random adding training examples (RL) are compared.</p><p>The graphs in Figure <ref type="figure" coords="5,189.18,629.84,4.54,8.90" target="#fig_1">2</ref>-3 illustrate the performance of our active and random learning method on trec06p and trec06c respectively. We can make two useful observations when putting the results together (Figures <ref type="figure" coords="5,160.18,653.30,21.87,8.90" target="#fig_1">2 to 3</ref>). The first thing we notice with the number of training examples increasing we observed higher generalization accuracy for the active learning strategy and the random strategy in all our experiments. This corresponds to the intuition that the more training examples, the more prediction power a classifier can produce. The second thing is that active selection can minimize the number of labeled examples that are necessary to learn a classification function at a certain accuracy level, especially when there are comparatively few support vectors to find. In figure <ref type="figure" coords="5,381.37,711.44,3.81,8.90" target="#fig_1">2</ref>, there are altogether 34,040 examples to be learned. The AL filter performance promotes very fast that the AL.3200 achieves the  comparable performance with AL.25600. While the RL filter works best until the RL.25600 finished. In figure <ref type="figure" coords="6,125.88,137.54,3.82,8.90" target="#fig_0">3</ref>, same trend is still obviously. There are altogether 58,158 examples to be learned. The best performance obtained at AL12800 and RL.51200 respectively. Since the difference in performance is much stable between different datasets as seen in the graphs, we may conclude that the random selection cannot provide the same level of spam detection provided by the active learning filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Experiment II: The Impact of Online Learning Method</head><p>This experiment run in a controlled environment simulating personal spam filter use to verify the online learning performance on the official corpus. The system was first presented a sequence of messages, one message at a time, and was required to return a "spamminess" score (a real number representing the estimated likelihood that the message is spam). After that the correct classification was presented. Two kinds of online learning strategy are tested respectively, the ideal and delayed user feedback. In ideal user feedback, the filter retrains the model immediately after each classification. And in the delayed feedback, the filter will not be retained immediately. That is random-sized sequences of email messages will be classified without any intervening "train" commands and the corresponding "train" commands for these messages will follow, with no intervening "classify" commands. The length of the sequences will be randomly generated with an exponential distribution.</p><p>A summary of the results achieved with Ideal user feedback (Ideal) against Delayed user feedback (Delayed) learning algorithm data on the official corpus is listed in the Table <ref type="table" coords="6,421.92,337.04,3.86,8.90" target="#tab_3">2</ref>. Overall, the Ideal system outperformed the other system on most of the evaluation corpora in the 1-ROCA criterion. However, the tradeoff between ham misclassification and spam misclassification varies considerably for different datasets.    The ROC and ROC learning curves for the Ideal and Delay on the trec06p and trec06c result are depicted in Figure <ref type="figure" coords="7,169.81,306.26,4.03,8.90" target="#fig_3">4</ref><ref type="figure" coords="7,177.88,306.26,4.03,8.90">5</ref>. The left figures depicts the ROC curve of the Ideal and Delay system, it provide a convenient graphical display of the trade-off between true and false positive classification rates for spam filtering task. We can interpret this curve as a comparison of the Ideal filter performance across the entire range of class distributions and error costs. The performance improves the further the curve is near to the upper left corner of the plot. The left figures show that the Ideal system dominates the other curve over most regions. The right graph shows how the above the ROC curve changes with an increasing number of examples. The performance improves the further the curve is near to the lower left corner of the plot. Note that the 1-ROCA statistic is plotted in log-scale. All filters learn fast at the start, but performance levels off at around 10,000 messages. This behavior may be attributed to the implicit bias in the learning algorithms, data or concept drift. It remains to be seen whether a more refined model pruning strategy would help the filter to continue improving beyond this mark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hm%</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Our system reached the anticipated goal in the TREC evaluation. The TREC results confirmed our intuition that active and online learning offer a number of advantages over random selection and delay-retraining spam filtering methods. By using active learning algorithm, the spam filter can faster attain a level of generalization accuracy in terms of the number of labeled examples. Also, the online learning algorithm, whereby only subsets of the data are to be considered at any one time and results subsequently combined, can make the retraining process much faster and avoid the much storage cost. Thus the filter algorithm can be scaled up to handle extremely large data sets.</p><p>This active and online spam filter achieved the best ranking filter overall in the 1-ROCA statistic for most of the datasets in the official evaluation. Despite the encouraging results at TREC, we believe there is much room for improvement in our system. To most users spam is a nuisance, while the loss of legitimate email is much more serious, so the filter should define an optimal threshold to one that rejects a maximum amount of spam while passing all legitimate emails. In future work, we intend to employ a suitable mechanism for dynamically adapting the filtering threshold. And while traditional SVMs only use the error rate, not the application specific performance measure like ROC-Area, as the performance measure to optimize model, it is likely to produce suboptimal results. An interesting avenue for future research would be to employ a strategy which would directly optimize SVMs for 1-ROCA measure that promotes spam filter performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,126.00,614.63,325.44,9.19;5,52.32,437.52,237.66,168.60"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. ROC curves for AL (left) and RL (right) filters on the trec06c corpus.</figDesc><graphic coords="5,52.32,437.52,237.66,168.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,135.00,427.73,326.66,9.19;5,288.72,437.52,238.43,168.06"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. ROC curves for AL (left) and RL (right) filters on the trec06p corpus.</figDesc><graphic coords="5,288.72,437.52,238.43,168.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,98.76,461.42,12.45,8.90;6,144.24,458.06,12.83,6.36;6,134.22,466.46,32.79,6.36;6,189.90,458.06,12.78,6.36;6,179.82,466.46,32.88,6.36;6,235.56,458.06,12.83,6.36;6,225.54,466.46,32.79,6.36;6,281.16,458.06,12.78,6.36;6,271.08,466.46,32.87,6.36;6,326.76,458.06,12.83,6.36;6,315.78,466.46,34.70,6.36;6,372.42,458.06,12.78,6.36;6,361.44,466.46,34.71,6.36;6,418.20,458.06,20.10,6.36;6,402.84,466.46,50.84,6.36;6,475.56,458.06,20.10,6.36;6,460.20,466.46,50.80,6.36;6,99.00,480.74,11.90,8.90;6,144.24,477.38,12.83,6.36;6,134.22,485.78,32.79,6.36;6,189.90,477.38,12.78,6.36;6,179.82,485.78,32.88,6.36;6,235.56,477.38,12.83,6.36;6,225.54,485.78,32.79,6.36;6,281.16,477.38,12.78,6.36;6,271.08,485.78,32.87,6.36;6,326.76,477.38,12.83,6.36;6,315.78,485.78,34.70,6.36;6,372.42,477.38,12.78,6.36;6,361.44,485.78,34.71,6.36;6,418.20,477.38,20.10,6.36;6,402.72,485.78,51.09,6.36;6,475.56,477.38,20.10,6.36;6,460.08,485.78,51.09,6.36;6,97.62,497.72,410.23,8.90;6,87.42,509.48,420.50,8.90;6,87.42,521.18,420.33,8.90;6,87.42,532.94,422.89,8.90;6,87.42,544.70,277.89,8.90"><head></head><label></label><figDesc>1013 -1.4940)A comparison with the statistics in Table2reveals that our online system disproportionately favored classification into the class that contains more training examples. The online system did not vary the filtering threshold dynamically, but rather kept it fixed at a spamminess score. The results indicate that adjusting the threshold with respect to the number of ham and spam training examples or, alternatively, with respect to previous performance statistics, would be beneficial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,83.10,711.57,430.21,9.48;6,290.22,554.70,213.21,151.44"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. ROC curves (left) and learning curves (right) for Ideal and Delayed filters on trec06p corpus.</figDesc><graphic coords="6,290.22,554.70,213.21,151.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,87.42,119.71,420.43,607.30"><head>Table 1 .</head><label>1</label><figDesc>piecewise approximation and logistic regression are used to model hm% and sm% as a function of the number of Basic statistics for the evaluation datasets</figDesc><table coords="4,114.72,119.71,332.05,263.38"><row><cell>Trials Sequence</cell><cell>X0</cell><cell>X1</cell><cell>X2</cell><cell>...</cell><cell>Xn</cell></row><row><cell>SV1</cell><cell>SV2</cell><cell>SV3</cell><cell></cell><cell>SVn-1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Final</cell></row><row><cell>TR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Concept</cell></row><row><cell></cell><cell>Target</cell><cell>Target</cell><cell></cell><cell>Target</cell><cell></cell></row><row><cell></cell><cell>Concept1</cell><cell>Concept2</cell><cell></cell><cell cols="2">ConceptN-1</cell></row><row><cell cols="6">Online SVM Spam Filter: 1) Initialization Seed SVM classifier with a few examples of each class (spam and ham) Train an initial SVM filters 2) Online Learning -Classify x Figure 1. The Incremental Training Procedure</cell></row></table><note coords="4,191.88,377.90,1.90,5.95;4,145.38,384.92,107.32,9.67;4,145.38,395.60,294.64,9.67;4,114.72,406.01,58.37,9.19;4,140.28,416.96,104.00,9.61"><p>i -Query the true label of x i -If the filter's prediction is wrong, retrain SVM filters based on SV i + x i 3) Finishing Repeat until x n is finished</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,216.96,376.22,147.27,8.90"><head>Table 2 .</head><label>2</label><figDesc>Misclassification Summary</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported by <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">60435020</rs>, <rs type="grantNumber">60504021</rs>) and <rs type="funder">Key Project of Chinese Ministry of Education &amp; Microsoft Asia Research Centre</rs> (<rs type="grantNumber">01307620</rs>).</p></div>
<div><head>References</head><p>Figure 5. ROC curves (left) and learning curves (right) for Ideal and Delayed filters on trec06c corpus.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kQQWywW">
					<idno type="grant-number">60435020</idno>
				</org>
				<org type="funding" xml:id="_S9YuR4C">
					<idno type="grant-number">60504021</idno>
				</org>
				<org type="funding" xml:id="_K45HmCC">
					<idno type="grant-number">01307620</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.81,125.67,400.00,8.50;8,87.42,136.77,238.60,8.50" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,165.24,125.67,144.84,8.50">Learning Rules that Classify E-mail</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,330.48,125.67,177.33,8.50;8,87.42,136.77,158.03,8.50">Proceedings of AAAI Spring Symposium on Machine Learning in Information Access</title>
		<meeting>AAAI Spring Symposium on Machine Learning in Information Access<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,147.99,399.97,8.50;8,87.42,159.15,420.45,8.50;8,87.42,170.37,415.16,8.50" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,441.18,147.99,66.62,8.50;8,87.42,159.15,144.12,8.50">An Evaluation of Naive Bayesian Anti-Spam Filtering</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutsias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">V</forename><surname>Chandrinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,251.10,159.15,256.77,8.50;8,87.42,170.37,291.49,8.50">Proceedings of the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning (ECML)</title>
		<meeting>the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning (ECML)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,181.53,400.05,8.50" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<title level="m" coord="8,320.46,181.53,183.14,8.50">A Bayesian Approach to Filtering Junk E-Mail</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.42,192.69,391.88,8.50" xml:id="b3">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="8,97.98,192.69,263.52,8.50">Proceedings of AAAI Workshop on Learning for Text Categorization</title>
		<meeting>AAAI Workshop on Learning for Text Categorization<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.82,203.91,399.94,8.50;8,87.42,215.07,358.00,8.50" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,226.44,203.91,182.37,8.50">Boosting Trees for Anti-Spam Email Filtering</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Marquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,423.60,203.91,84.16,8.50;8,87.42,215.07,143.12,8.50">Proceedings of Euro. Conference Recent Advances in NLP</title>
		<meeting>Euro. Conference Recent Advances in NLP<address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
	<note>RANLP-2001</note>
</biblStruct>

<biblStruct coords="8,107.83,226.23,399.98,8.50;8,87.42,237.45,418.26,8.50" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,242.22,226.23,265.59,8.50;8,87.42,237.45,88.85,8.50">SVM-based Filtering of E-mail Spam with Content-specific Misclassification Costs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alspector</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,193.32,237.45,217.16,8.50">Proceedings of the TextDM&apos;01 Workshop on Text Mining</title>
		<meeting>the TextDM&apos;01 Workshop on Text Mining<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,248.67,400.07,8.50;8,87.42,259.77,420.48,8.50;8,87.42,270.99,283.88,8.50" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,87.42,259.77,221.51,8.50">Stacking Classifiers for Anti-Spam Filtering of E-mail</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sakkis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karkaletsis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stamatopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,330.30,259.77,177.60,8.50;8,87.42,270.99,161.04,8.50">Proceedings of the 6th Conf. on Empirical Methods in Natural Language Processing</title>
		<meeting>the 6th Conf. on Empirical Methods in Natural Language Processing<address><addrLine>Pittsburgh,USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="44" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.82,282.21,399.98,8.50;8,87.42,293.37,398.28,8.50" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,166.20,282.21,276.22,8.50">On-Line Support Vector Machines for Function Approximation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<idno>LSI-02-11-R). 2002</idno>
		<imprint/>
		<respStmt>
			<orgName>Catalunya: Department of Software, Universitat Politecnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,107.81,304.53,399.99,8.50;8,87.42,315.75,189.16,8.50" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,240.30,304.53,180.05,8.50">Incremental and Decremental Support Vector</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,439.86,304.53,67.93,8.50;8,87.42,315.75,123.44,8.50">Advances Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="409" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.86,326.91,399.95,8.50;8,87.42,338.13,420.41,8.50;8,87.42,349.29,103.31,8.50" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,243.00,326.91,264.81,8.50;8,87.42,338.13,139.96,8.50">Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,251.64,338.13,217.41,8.50">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="705" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,360.45,400.02,8.50;8,87.42,371.40,325.48,8.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,245.46,360.45,262.39,8.50;8,87.42,371.67,52.63,8.50">Support Vector Machine Active Learning with Applications to Text Classification</title>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,146.10,371.67,167.02,8.50">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.82,382.77,400.04,8.50;8,87.42,393.99,420.34,8.50;8,87.42,405.21,28.33,8.50" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,173.04,382.77,299.23,8.50">Incorporating Diversity in Active Learning with Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,493.26,382.77,14.60,8.50;8,87.42,393.99,290.97,8.50">The Twentieth International Conference on Machine Learning (ICML-2003)</title>
		<meeting><address><addrLine>Washington, DC USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.81,416.37,400.04,8.50;8,87.42,427.53,420.42,8.50;8,87.42,438.75,189.42,8.50" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,251.88,416.37,213.25,8.50">Incremental Learning with Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,486.90,416.37,20.95,8.50;8,87.42,427.53,420.42,8.50;8,87.42,438.75,40.94,8.50">Proc. Workshop on Support Vector Machines at the International Joint Conference on Artificial Intelligence (IJCAI-99)</title>
		<meeting>Workshop on Support Vector Machines at the International Joint Conference on Artificial Intelligence (IJCAI-99)<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="352" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.83,449.91,400.02,8.50;8,87.42,461.13,322.24,8.50" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,224.34,449.91,204.26,8.50">An Approach to Incremental SVM learning algorism</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,446.34,449.91,61.51,8.50;8,87.42,461.13,280.62,8.50">Proceeding. the 12th International Conference on Tools with Artificial Intelligence: 2000</title>
		<meeting>eeding. the 12th International Conference on Tools with Artificial Intelligence: 2000</meeting>
		<imprint>
			<biblScope unit="page" from="268" to="273" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
