<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.75,113.91,291.32,18.15">FDUQA on TREC2006 QA Track</title>
				<funder ref="#_wveZ9Zb #_8vCuJhz">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.00,139.45,55.54,9.41"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
							<email>zhouyaqian@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.75,139.45,64.17,9.41"><forename type="first">Xiaofeng</forename><surname>Yuan</surname></persName>
							<email>xfyuan@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.50,139.45,51.99,9.41"><forename type="first">Junkuo</forename><surname>Cao</surname></persName>
							<email>jkcao@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.77,139.45,70.11,9.41"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.24,139.45,37.48,9.41"><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<email>ldwu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.75,113.91,291.32,18.15">FDUQA on TREC2006 QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C1BFB81FE6F8C2DA45F1642E26A2A08</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this year's QA Track, we participant in the main task and do not take part in the ciQA task. The main task is essentially the same as the single task from 2004, in that the test set consists of a set of question series where each series asks for information regarding a particular target. In order to better answer the questions in the series, we try to improve our anaphora resolution within question series.</p><p>For factoid questions, we use the system that submits the RUN-A in TREC 2005 <ref type="bibr" coords="1,464.09,296.95,41.80,9.41;1,87.75,311.20,19.01,9.41" target="#b3">[Wu et al. 2005</ref>]. Therefore we won't describe the factoid system in this paper.</p><p>For list questions, we get a lot of improvements, the most important of which are answer type classification, document searching, answer ranking and answer filtering.</p><p>For definition question, we still focus on utilizing the existing definitions in the Web knowledge bases. And also applied the method of relative terms extraction to extract reliable information associated with target for getting web definition directly by question target is becoming a bottleneck.</p><p>In the following, Section 2 will describe question series anaphora resolution. Section 3,and 4 will describe our algorithms for list and definition questions separately. Section 5 will present our results in TREC 2006.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Question Series Anaphora Resolution</head><p>Because the test set consists of a set of question series where each series asks for information regarding a particular target, it is necessary to do anaphora resolution.</p><p>There are three kinds of anaphora in the question series: coreference, bridging and zero anaphora. In our system we only resolve the coreference. And considering target as additional information when the question sentence has the other two kinds of anaphora.. In order to resolve coreference, we need to find anaphor and antecedent at first. There are three types of anaphors: the pronouns, the dBNP (Definite Base Noun Phrases) and the abbreviations. But not all the dBNP are anaphors, there are some exceptions: the answer of the question (e.g. the name in " What were the names of the victims?") , some special words (e.g. earth, sky, etc.). The candidate antecedent may be in the target, in previous questions, or in the answer of the previous questions.</p><p>We find that the syntax relation between anaphor and antecedent is not very close. It is difficult to resolve coreference according to the syntax restriction. Therefore we resolve coreference according to the coherent of type, gender and number between the anaphor and the candidate antecedent. We divide the type of anaphors and antecedents into six categories: person, organization, location, event, time and other.</p><p>During the resolution, we first classify the anaphor. Then the candidate antecedent has the same type is selected as the anaphor. The priority of selecting candidate antecedent is according to their position: target &gt;answer of previous question (nearer question&gt;further question)&gt; the noun phrase in previous question (nearer question&gt;further question).</p><p>If the type, gender and number are all matched, the resolution is finished. If the antecedent of some anaphora has appeared in the current questions, the resolution will not be done, or the result of resolution will be not in the habit. If there are not any coherent candidate antecedents, the resolution will not be done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">List Question</head><p>Our list question answering system adopts the general factoid question answering framework and composes of three modules, answer type classification module, passage retrieval module, and answer generation module.</p><p>The answer type classification module considers the character of list questions. The passage retrieval module focuses on the recall of the passage returned, which is more important in list questions. And the answer generation module gives a strategy of how to extract and select multiple answers from different documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Answer Type Classification</head><p>We first create a two -level ontology to define the answer type. It has six coarse-grained answer types and ninety-seven fine-grained answer types. And each fine-grained answer type is mapped to a coarse-grained answer type. Then, we classify the question into one of the fine-grained answer type based on WordNet. We start from the informer of the question, then use following algorithm to find its answer type:</p><p>Find </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End for</head><p>Output NIL After we get the output of the fine-grained answer type, we can immediately get the coarse-grained answer type from a type mapping.</p><p>Testing on the 93 questions of TREC 2005, the above simple algorithm can get a satisfying precision 87% on the fine-grained answer type classification.</p><p>The informer of a question is a word or a phrase that helps to classify the type of the answer. For example, given a question "What book did he wrote?" the informer of the question is the word "book". It is very useful for the classification of the answer type of a given question.</p><p>[ <ref type="bibr" coords="3,92.51,230.72,85.33,8.74" target="#b6">Krishnan et al, 2005</ref>] also find out the informer is a key element for classifying a question's answer type.</p><p>Compared with <ref type="bibr" coords="3,179.25,260.72,91.00,8.74" target="#b6">[Krishnan et al, 2005]</ref> , we use a more semantic and easier way which also has a high precision in the list questions to extract the informers in a question. Due to the specialty of the List questions which usually start with some definite words such as what, list, name, etc., we use a relation parser Minipar to extract the informers which are related to these specific words of list questions. Minipar can extract such a triple: (K R S), where K represents the informers we need, S is the specific word in the question, and R is the relation between the informers and the specific words. As of the example above, Minipar will output such a triple: (book det what), thus the word "book" is extracted as t he informers of the question, what is used here as the specific words and their relation is det. We use some heuristic rules to choose the specific words from the question and the relations from the minipar. And the precision of extracting informers of the 93 TREC 2005 list questions is 95% (5 are wrong).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Document Retrieval</head><p>We discover that the document searching strategy in list question answering is quite different from that of the factoid. And some effective document searching methods of factoid question answering are not suitable for list question answering. There are two main elements in the searching phase of the list question answering, the first factor is the relevance of the target and the question, and the second one is the number of the documents returned. For factoid question, the answer is unique, so we only want the most relevant documents, and the number of documents returned is usually less than 10. While for list question, the number of answers is unknown, so any document that is related to the target or the question may contain the answer, and we should not risk missing any of them in the document searching phase. The number of documents returned is usually more than 50. In a word, list question answering emphasis more on recall than precision. We use a framework, Lucene, as our document search engine. And the result shows our document searching strategy raised the performance by 8% in TREC 2005 list question set and 15% in TREC 2004 list question set (Top 10 document recall). Our list document searching strategy is presented in the following respects: Indexing, query generation and document scoring.</p><p>Since we only use the TREC Aquaint as the searching corpus, and no other external documents, the index process is necessary and important for our system. The main difference between the index for list question and the index for factoid question is that the index is based on the morphed Aquaint corpus. Every word except the proper nouns is morphed into its original form before it i s indexed. This index strategy can avoid missing some important documents in some degree.</p><p>To match the indexed word, the query words are also morphed. The query is composed of two parts: the target part and the question part. The weight rate of the target and question is 6:4. Also there are two fields in each part: the text field and the headline field. The weight rate of the two fields is 50:1 Lucene is a VSM model based search engine framework, thus the document ranking is based on the basic tf idf score of each dimension of the vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sentence Scoring</head><p>Although we get the ranked documents, they may be too long for processing. We wish to find the most important and relevant part of the document. This helps to locate the answer in a more precise range (a sentence). Also the sentence score can compensate for the shortcoming of the tf idf ranking strategy. The very heuristic rules of the sentence scoring process are that the more words of the sentence appear in the target or the question, the higher its score, and that the more important the words of the sentence, the higher its score.</p><p>According to these two rules, each sentence of each document is given a sentence score ranged from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Document Re-ranking</head><p>In order to get a better ranking result of the documents, we combine the Lucene document ranking score and the best sentence score in a document. Let S be the score of the sentence factor and D represent the score of the document. Our task is to find a function φ (S, D) which is the combination of the two scores.</p><p>First, we should determine the value of S and D. The score D is simply the similarity returned by the search engine Lucene. While to get the S score, we have several options. Suppose there are N sentences in the document, and each sentence score is S i (0&lt;i&lt;N).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Then</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( )</head><p>Ss θ = v is a vector to value function. We've tried the following θ functions: Max, Min, Average and Median. Our experiment results show that the Max function is better than any other functions. So let ( )</p><formula xml:id="formula_0" coords="4,190.50,590.47,125.08,22.43">SMaxs = v in our system.</formula><p>Second, we will try to find the functionφ. Only the linear combinations of the two parameters of φ are considered. So φ is a linear function and has the formφ =α*S+β*D. α, β are the parameters to be determined. Our experiments show that when α =0.7, β =0.3, the ranking is most improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Answer Extracting</head><p>Although the document is re-ranked, we still need a more precise and small range to extract answer, for the extracting process is time consuming. We extract answer from sentences ranked from 1 to N in the previous stage. Since the context information of each sentence may also contain the right answer, we also extract answers from the next K sentences and previous K sentences of the top ranked sentences. Here, the N is supposed to be 200 and K is 2.</p><p>The extraction is based on the Named Entities and some chunked phrases in the sentence. According to the answer type that has been classified, we extract relevant named entities. The answer types used here is the coarse one because our named entities classifier can only classify the coarse types.</p><p>During the extraction, a distance score which is associated with each answer is also calculated. This score is used to indicate how far it is from the answer to the important words of target and question. Assuming that there are d words between the candidate answer c and the important word k, the distance score is calculated by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Answer Ranking</head><p>Since for each candidate answer, we've already calculated several scores, the ranking procedure is to sort the candidates according to a combination of the scores. We use the simplest linear combination of three scores; they are the Lucene similarity score, sentence score, and the maximum distance score. So, the final score of a candidate answer is tan Lucenesentencedisce</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSSS αβγ =⋅+⋅+⋅</head><p>. Our experiment is to determine all the parameters ,, αβγ. The result indicate that when 0.3,0.5,0.2 αβγ ===</p><p>, the final F score is best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Answer Filtering</head><p>Although the answers are ranked, there may be some redundancy and noise in the ranked answer list. The answer filtering module is to reduce the redundancy and eliminate as much noise as possible.</p><p>To reduce redundancy, we keep the longer answer and delete the shorter one.</p><p>Eliminating noise is a difficult job, for we don't know what kind of answer is noise. Thus, we did it only in few cases where the answer type has a finite answer collection (such as country, city, river and etc.). In such cases, a potential answer list for each of these answe r types is used to validate the extracted answer, and the answers that do not appear in the list are eliminated. Thus, we partially eliminate the noise, resulting in a high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Definition Question</head><p>In order to automatically identify definition sentences from a large collection of documents, we first extract related knowledge as much as possible by question target, and then apply the knowledge to pick out the question answers. The knowledge includes online definitions and relative terms. In our system, the extraction of relative terms differ s from traditional methods based on calculating the co-occurred frequency of the target words by using the scores of candidate answer sentences for integrative selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Overview</head><p>We design a general architecture for definition QA. The system consists of four modules: document processing, web knowledge acquisition, relative terms extraction and definition generation. The flow chart for definition question of FDUQA is in figure <ref type="figure" coords="6,412.13,451.45,4.28,9.41">1</ref>. First, the document processing module generates the candidate sentence set according to the target term. This module has three steps: document retrieval, candidate sentence extraction and initial score calculation. Second, the Web knowledge acquisition module acquires the definitions of the target term from the Web knowledge base (KB). We search the definitions about the target from a number of online knowledge bases. These knowledge bases are the WordNet glosses and other online dictionaries such as the biography dictionary at www.encyclopedia.com. The definitions from them often supply knowledge that can be exploited directly and they are quite helpful to answering definition questions. We choose several authoritative KBs that cover different kinds of concept to achieve our goal. If we can find the definitions of question target from these sources, we use them to score the candidate sentences. (More detail please refer to <ref type="bibr" coords="6,99.75,632.95,70.86,9.41" target="#b0">[Wu et al, 2004]</ref>) Third, we automatically extract relative terms based on the candidate sentences, and then we score the candidate sentences using these relative terms. The extraction of relative terms will be described in Section 4.3.</p><p>At last, the definition generation module ranks the candidate sentences, based on target term, relative terms and web knowledge, and determines the question answers. We will describe the detail of the definition generation module in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document Processing</head><p>The document processing module has three steps: document retrieval, candidate sentence extraction and initial score calculation. In this section we will describe the initial score calculation.</p><p>Suppose the candidate answer sentences set is S= { s 1 , s 2 ,…, s n }, the initial score of sentence s i can be calculated by its target score and document score. The calculation of initial score is as follow,</p><formula xml:id="formula_1" coords="7,171.75,503.40,250.08,14.37">) ( csc * ) 1 ( ) ( arg ) ( i i i s ore do s etscore t s initscore θ θ - + * =</formula><p>Where we use initscore(si) for sentence si's initial score, targetscore(si ) for sentence si's target score, docscore(s i ) for sentence s i ' s document score and θ for weight .</p><p>Target score is the corresponding score based on the words, phrases and entities, which occurred in the question target. It can be calculated as follow:</p><formula xml:id="formula_2" coords="7,207.00,596.35,198.85,28.12">e p w i n e c n p c n w c s etscore t ) ( ) ( ) ( ) ( arg γ β α + + =</formula><p>where n w , n p , n e respectively represent the number of words, phrases and Name Entities contained in S i , and c(w), c(p), c(e) denotes the number of the words, phrases and Name Entities that in both S i and the target. In our system, α, β and γ are allotted to 0.3, 0.3 and 0.4 respectively.</p><p>The document score docscore(s i ) of the sentence s i can be calculated as follow:</p><p>Relative Where docn(s i ) is the number of returned document including the sentence s i , and Maxdocw(s i ) is the max score of these documents. The document score increases apparently with Maxdocw(s i ) and docn(s i ).</p><p>Both target score and document score are normalized and set their weights empirically as 0.8 and 0.2 respectively, because the target information is more important than document information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relative Terms Extraction</head><p>Because getting web definition directly by question target is becoming a bottleneck, the system has to try other approaches to extract reliable information associated with target. Key phrase extraction and expansion are widely used in the text summarization. We utilized the technique for question target expansion. This process, called relative terms extraction, tried to obtain the words, phrases and entities that related with the target closely.</p><p>Given that T={ t 1 , t 2 ,…, t n } are the all words, phrases and entities in candidate sentence set S={ s 1 , s 2 ,…, s n }, calculate the relativity r(t i ) between t i and target as follow formula:</p><formula xml:id="formula_3" coords="8,149.25,379.66,315.36,38.20">∑ = * = n j j i i s initscore s t E t r 1 j ) ( ) , ( ) ( , Where      ∉ ∈ = j j j 0 1 ) , ( s t s t s t E i i i</formula><p>Rank T by r(t i ), and select appropriate number of t i as relative terms. In our system, we extracted top 15 relative words, phrases and entities which achieved the better performance.</p><p>Given that candidate sentence s i has n w words, n p phrases and n e entities. And these words, phrases and entities contained r w relative words, r p relative phrases and r e relative related entities. The similarity between target and relative words, phrases and entities are denoted by r(w i ), r(p i ) and r(e i ) respectively. Then the relative terms score rwscore(s i ) of candidate sentence s i can be calculated as below: Because the entities are more important than general words and phrases, we assigned higher weight to entities. In our system, the weights, α, β and γ are set as 0.3, 0.3 and 0.4 respectively by empirical.</p><formula xml:id="formula_4" coords="8,160.50,545.02,312.65,9.85">) / ) ( ( * ) / ) ( ( * ) / ) ( ( * ) (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Answer Ranking and Generation</head><p>Generally, candidate answers are excessive. Therefore these sentences could not be selected as answers directly. Synthetic method is used to rank and select these candidate sentences as question answers. In our system, every candidate answers are evaluated by the linear combination of three scores: initial score, web score <ref type="bibr" coords="8,364.50,695.72,83.32,8.74" target="#b4">[Zhang et al, 2005]</ref> and relative terms score.</p><p>In candidate answer ranking, we set these score weights dynamically with our evaluation system. After ranking these candidate sentences, redundancy removal will be done. And we choose the top 20 candidate sentences as the final question answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We submitted t hree runs for the main task of TREC15 QA Track: FDUQA15A, FDUQA15B and FDUQA15C. In the three runs, the algorithms used to answer factoid questions are same; the only difference is we use different question series anaphora resolution results. The difference between the three runs of list questions described as follow: In FDUQA15C, we tried new method of answer type classification and new answer ranking metrics. In FDUQA15B, besides the new methods for answer type classification and answer ranking, we optimize the document searching strategies for list question answering. In FDUQA15A, we put all these new methods in FDUQA15C and FDUQA15B together and filter the redundant answers in the final answer list. Also we tune the parameters in each step to get its best performance in the test set of TREC 2005 93 list questions. As to definition questions, difference between the t hree runs is FDUQA15C use the last year's system, FDUQA15B is this year's system and FDUQA15A combines the results of FDUQA15C and FDUQA15B. From Table <ref type="table" coords="9,169.50,606.70,5.66,9.41">3</ref> , we can see that we get a lot of improvements of our list question answering system. That owes to the improvement of answer type classification, document searching, answer ranking and answer filtering. The algorithm we use to answer definition questions is quite promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FDUQA15A</head><p>There're still a lot of things to be improved in our list question answering system. The linear combination is too simple; some more sophistic methods can be used to improve the performance. The number of the answers for each question should have differed; however, we only simply pick out the equally top 15 answers for each question. The final score should be more precise to divide the answers into two parts, one for the final answer, and the other for elimination. Some answer clustering methods also should be considered to improve the ranking performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,417.00,354.05,77.07,10.48;5,348.75,361.55,128.98,10.48;5,421.50,370.55,14.85,10.48;5,342.75,361.55,24.67,10.48;5,404.25,370.55,91.17,10.48;5,383.25,357.74,77.00,15.18;5,397.50,350.99,104.97,15.18;5,375.00,357.74,110.48,15.18;5,397.50,360.74,104.97,15.18;5,413.25,366.74,6.39,15.18;5,397.50,368.24,104.97,15.18;5,504.00,362.20,2.83,9.41;5,87.75,398.62,320.95,11.05;5,397.50,395.07,30.61,15.39;5,430.93,399.70,74.75,9.41;5,87.75,437.95,73.95,9.41;5,205.50,437.57,5.63,10.13;5,167.25,433.89,36.18,14.66;5,212.25,437.95,2.83,9.41;5,254.25,429.80,5.82,10.48;5,258.75,446.30,14.85,10.48;5,241.50,446.30,5.82,10.48;5,220.50,433.49,7.35,15.18;5,234.75,426.74,44.97,15.18;5,231.00,433.49,48.72,18.18;5,250.50,442.49,6.39,15.18;5,234.75,443.99,44.97,15.18;5,282.00,437.95,160.09,9.41;5,483.75,429.80,5.82,10.48;5,466.50,437.30,5.82,10.48;5,483.00,446.30,7.77,10.48;5,449.25,433.49,6.39,15.18;5,462.00,426.74,35.22,15.18;5,458.25,433.49,21.53,15.18;5,462.00,436.49,35.22,15.18;5,462.00,443.99,35.22,15.18;5,498.75,437.95,7.42,9.41;5,87.75,475.45,250.02,9.41;5,359.25,475.07,52.94,10.13;5,339.00,471.39,55.68,14.66;5,414.00,475.45,91.26,9.41;5,87.75,506.20,67.11,9.41;5,168.75,511.21,11.32,5.98;5,181.50,504.90,13.58,10.35;5,162.75,504.90,108.28,10.71;5,288.75,505.82,23.94,10.13;5,282.75,505.82,221.11,10.13;5,87.75,535.45,25.37,9.41;5,138.00,534.66,12.94,10.66;5,126.75,541.07,9.80,6.16;5,120.75,534.66,108.67,10.66;5,244.50,534.68,24.11,10.63;5,238.50,534.68,267.75,10.63;5,87.75,559.45,362.20,9.41"><head></head><label></label><figDesc>the weight of the key word k in the question or target. , average (,) fck of all k. Each extracted candidate answer has a maximum distance score and an average distance score together stored with it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,98.25,402.70,397.34,180.41"><head>Table 1</head><label>1</label><figDesc>Performance of FDUQA Runs in TREC 2005</figDesc><table coords="9,366.75,402.70,128.84,9.41"><row><cell>FDUQA15B</cell><cell>FDUQA1 5C</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">60435020</rs> and No. <rs type="grantNumber">60503070</rs>. We are very thankful to <rs type="person">Zhongchao Fei</rs>, <rs type="person">Feng Ji</rs>, <rs type="person">Bo li</rs>, <rs type="person">Yindong Chen</rs> and <rs type="person">Chao Shen</rs> for their contributions in our work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wveZ9Zb">
					<idno type="grant-number">60435020</idno>
				</org>
				<org type="funding" xml:id="_8vCuJhz">
					<idno type="grant-number">60503070</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,87.75,274.45,415.40,9.41;10,107.25,290.20,397.68,9.41;10,107.25,306.70,137.49,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,451.50,274.45,51.65,9.41;10,107.25,290.20,98.96,9.41">FDUQA on TREC2004 QA Track</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhushuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,232.33,290.20,267.47,9.41">Proceedings of the Thirteenth Text REtreival Conference</title>
		<meeting>the Thirteenth Text REtreival Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.75,323.20,418.08,9.41;10,106.50,338.95,321.63,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,404.25,323.20,101.58,9.41;10,106.50,338.95,38.15,9.41">FDUQA on TREC2003 QA Task</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongping</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,152.74,338.95,129.35,9.41">Proceedings of the TREC-12</title>
		<meeting>the TREC-12<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.75,354.70,416.28,9.41;10,106.50,371.20,400.21,9.41;10,106.50,387.70,140.08,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,470.28,354.70,33.76,9.41;10,106.50,371.20,242.35,9.41">FDU at TREC2002: Filtering, QA, Web and Video Tasks</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,360.91,371.20,140.19,9.41">Proceedings of the TREC-11</title>
		<meeting>the TREC-11<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.75,403.45,417.34,9.41;10,106.50,419.20,345.66,9.41" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhushuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Lin</surname></persName>
		</author>
		<title level="m" coord="10,429.75,403.97,75.34,8.74;10,106.50,419.20,197.57,9.41">FDUQA on TREC 2005 QA Track. Proceedings of the TREC-14</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.75,436.45,417.06,9.41;10,107.25,452.20,398.64,9.41;10,107.25,467.95,179.55,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,359.70,436.45,145.11,9.41;10,107.25,452.20,123.07,9.41">Answering Definition Questions using web knowledge base</title>
		<author>
			<persName coords=""><forename type="first">Zhushuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,254.06,452.20,251.84,9.41;10,107.25,467.95,148.61,9.41">Proceedings of the 2nd International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.75,484.45,416.06,9.41;10,107.25,500.95,395.32,9.41;10,107.25,516.70,195.59,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,345.75,484.45,158.06,9.41;10,107.25,500.95,215.01,9.41">A comparative Study o Sentence Retrieval for Definitional Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,347.39,500.95,155.18,9.41;10,107.25,516.70,165.30,9.41">Proceedings of the 27th Annual International ACM SIGIR Conference</title>
		<meeting>the 27th Annual International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.75,533.20,418.64,9.41;10,106.50,549.70,399.27,9.41;10,106.50,565.45,116.28,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,327.00,533.20,179.39,9.41;10,106.50,549.70,161.16,9.41">Enhanced Answer Type Inference from Questions using Sequential Models</title>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sujatha</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,289.84,549.70,215.93,9.41;10,106.50,565.97,91.51,8.74">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
