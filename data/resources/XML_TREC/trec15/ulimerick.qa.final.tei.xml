<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,80.97,111.02,434.64,15.77">Question Answering using the DLT System at TREC 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,196.05,142.94,121.83,12.30"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.iekieran.white@ul.ieigal.gabbay@ul.iemichael.mulcahy@ul.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.37,142.94,73.99,12.30"><forename type="first">Kieran</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.85,158.90,63.96,12.30"><forename type="first">Igal</forename><surname>Gabbay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.77,158.90,97.07,12.30"><forename type="first">Michael</forename><surname>Mulcahy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,80.97,111.02,434.64,15.77">Question Answering using the DLT System at TREC 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A645774281946F2B022CF4A9EABF86A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article summarises our participation in the Question Answering (QA) Track at TREC 2006. Section 2 outlines the architecture of our system. Section 3 describes the changes made for this year. Section 4 summarises the results of our submitted runs while Section 5 presents conclusions and proposes further steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Outline of System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Strategy</head><p>As in previous years, the following stages are at the core of our approach:</p><p>• Question analysis: Process the input query attempting to find its type (e.g. who or colour) and to identify significant phrases.</p><p>• Document retrieval: Formulate a search query based on the results of the previous stage. Use this together with a search engine indexed on the document collection to produce a list of candidate documents which are likely to contain answers to the question.</p><p>• Named entity recognition: Based on the query type identified in the first stage, search for corresponding named entities (NEs) in the candidate documents which co-occur with terms derived from the query.</p><p>• Answer selection: Decide which NE (or NEs) should be chosen as the answer.</p><p>These steps are very typical of first generation QA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DLT System Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Summary of Enhancements</head><p>Relative to last year there were only two changes. Firstly, instead of using the Xelda tagger we used the Connexor (2006) parser and extracted part-of-speech tags from the parse trees. This was because Xelda was not available at Essex. Secondly, instead of including the Topic in addition to the Question for both document retrieval and answer identification we experimented with the use of just the Topic information for the first stage and just the Answer information for the second. This was in Run 1, with Run 2 using both texts in both stages as was previously the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Types and their Identification</head><p>Our system can recognise 82 different query types plus unknown, but few of these are used in practice, especially with the more homogenous style of questions in the new question groups. This year, 34 question types were actually used in answering questions (see table) exactly the same number as last year. In addition a further seven of the question types should have been used (mostly where the system incorrectly assigned the type 'unknown'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Analysis</head><p>Exactly as last year (except use of Connexor), the following steps are carried out on the query:</p><p>• Parse the query using the Connexor parser;</p><p>• Extract part-of-speech information and convert to Xelda tag set;</p><p>• Recognise instances of eleven different constructs;</p><p>• Weight these according to their importance;</p><p>• Order them according to weight;</p><p>• Use the conjunction of these as the initial search expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Search Expression Formulation</head><p>Searches of the document collection use boolean queries. Constructs as identified in the previous stage are ordered by increasing score and then joined with AND operators to make a single boolean query. This is then used as the starting point of a search for documents.</p><p>Classif.  The entire corpus is split into individual sentences each of which is indexed separately using the Lucene system. Each 'document' retrieved by the system is thus a sentence. The complete query is submitted and the first n results found are returned (Lucene orders documents even for boolean queries). n is set to 30. If no document is found, the query is relaxed by removing the least significant term and then re-submited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct Classification Incorrect Classification Total</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><formula xml:id="formula_0" coords="3,100.65,477.13,395.22,169.49">0 0 0 0 1 0 0 0 0 1 colour 1 0 0 0 0 1 0 0 0 0 1 how_did_die 1 0 1 0 0 0 0 0 0 0 1 how_often 1 0 0 0 0 1 0 0 0 0 1 organisation 1 0 0 0 0 1 0 0 0 0 1 profession 1 0 0 0 0 1 0 0 0 0 1 sport 1 0 1 0 0 0 0 0 0 0 1 what_continent 1 0 0 0 0 1 0 0 0 0 1 what_county_us 0 1 0 0 0 0 0 0 0 1 1 what_island 1 0 0 0 0 1 0 0 0 0 1 what_mountain_range 1 0 0 0 0 1 0 0 0 0 1 where_school 1 0 0 0 0 1 0 0 0 0</formula><p>The process continues until results are returned or no further simplification is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Named Entity Recognition</head><p>We have our own module for NE recognition which uses a mixture of simple grammars and lists.</p><p>Because of the changes in question content and form in recent years, few of the lists are in fact used. Following <ref type="bibr" coords="4,117.45,268.89,81.85,9.67" target="#b0">Clarke et al. (2003)</ref>, queries of unknown type are answered by searching for general names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Answer Selection</head><p>During this stage, each candidate NE found within a returned document is scored and the highest scoring NE is returned as the answer to the question. Scoring is done using a measure which incorporates the number of co-occurring key phrases, their assigned weights and their distance from the NE. The distance between a candidate NE and a key phrase is measured in words, e.g. if the phrase is adjacent to the NE its distance is 1, if one word separates them it is 2 and so on. Certain stop words such as prepositions do not contribute to this distance. The reciprocal of the distance is taken and this is multiplied by the weight assigned to the phrase. The sum of all such values is taken to provide an intermediate score for the NE. The final score is this intermediate score multiplied by the Lucene score assigned to the containing document. Following this process, the highest scoring NE is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Runs and Results</head><p>Two runs were submitted. In Run 1, we used the Target information to search for documents and then used the Query to identify terms in retrieved documents (sentences) in the vicinity of candidate NEs. In Run 2, both Target and Query were used in both stages. The results are shown in Table <ref type="table" coords="4,464.25,495.09,4.04,9.67" target="#tab_1">2</ref>. Concerning classification accuracy, 337 out of 403 queries were classified correctly, i.e. 83.62%. The figure for last year was very similar. However in considering such figures we need to bear in mind that this includes the 'correct' classification of 36 queries as 'unknown'. This is effectively rewarding the system for saying that it can not classify a query. The breakdown of query types in Table <ref type="table" coords="4,384.57,545.49,5.37,9.67" target="#tab_1">2</ref> is illuminating. The first two columns show correct and incorrect classification and the rows are by decreasing frequency of query type. Essentially a very small number of types (for example nine) account for a very high proportion of the queries (337 out of 403 factoids). This means that very few query types need really be processed by a QA system because the TREC query collections are much more homogenous than they used to be in the days of state mottos and baseball scores. As 36 of the 403 queries are correctly classified as 'unknown', we can interpret this as meaning that 8.93% of queries are totally outside the scope of the system. However, the true figure is probably much higher than that.</p><p>The performance of Run 2 was better than Run 1 so only the former is summarised in Table <ref type="table" coords="4,482.37,659.01,4.04,9.67" target="#tab_1">2</ref>. Overall QA performance was 44 Right out of 403 i.e. 10.92%. This is much worse than the figure of 17.68% achieved last year. Even the lenient figure this year of 59 out of 403 (44R+3L+12X) is only 14.64% compared to 20.17% last year. This year there were 12 ineXact answers compared to 9 in Run 1 last year. This is an increase but it is unlikely to be significant. By contrast, other groups have reported a large increase in the number of ineXact judgements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,105.57,125.25,385.42,141.08"><head>Table 1 : Breakdown of 75 Question Groups in TREC 2006. This</head><label>1</label><figDesc>is an approximate classification.</figDesc><table coords="2,223.29,125.25,140.03,104.72"><row><cell>Type</cell><cell>Count</cell></row><row><cell>person</cell><cell>20</cell></row><row><cell>event</cell><cell>18</cell></row><row><cell>organisation</cell><cell>13</cell></row><row><cell>miscellaneous</cell><cell>11</cell></row><row><cell>monument</cell><cell>6</cell></row><row><cell>company</cell><cell>5</cell></row><row><cell>tv_show</cell><cell>2</cell></row><row><cell>Total</cell><cell>75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,100.65,637.57,395.22,97.97"><head>Table 2 : Results by Query Type for Run 2.</head><label>2</label><figDesc>1    The columns C and NC show the numbers of queries of a particular type which were classified correctly and not correctly. Those classified correctly are</figDesc><table coords="3,100.65,652.57,395.22,8.12"><row><cell>Totals</cell><cell>337</cell><cell>66</cell><cell>47</cell><cell>12</cell><cell>4</cell><cell>275</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>66</cell><cell>403</cell></row></table><note coords="3,105.57,704.01,385.42,8.72;3,105.57,715.41,385.43,8.72;3,105.57,726.81,39.26,8.72"><p>then broken down into Right+Logically Correct, ineXact, Unsupported and Wrong. Next, those classified incorrectly are also broken down. The final column shows the total number of queries for each type.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Conclusions</head><p>Very little time was available for our experiments this year and the results were poor even by comparison with last year. We plan to devote much more time to next year's system.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When is something a purpose and how do you discern it?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>206.3: How much water fell on Johnstown?</head><p>Hard to predict the answer -inches of rainfall perhaps? We provide a couple of tables giving further information on the queries this year. Table <ref type="table" coords="5,475.05,570.45,5.37,9.67">1</ref> shows an approximate breakdown of the 75 question groups into seven categories. Not surprisingly, most of the groups concern persons, events and organisations. Our processing of the main NEs concerning these is very primitive by today's standards.</p><p>Table <ref type="table" coords="5,98.61,633.57,5.37,9.67">3</ref> shows examples of some of the 'difficult' questions this year. The problem in most cases is that the precise question is hard even for a person to understand and the form of the answer is difficult to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Many thanks to Ken Litkowski for providing answer patterns and supporting documents for the question collection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,70.53,200.01,455.57,9.67;6,70.53,212.61,455.57,9.67;6,70.53,225.21,455.49,9.79;6,70.53,238.05,455.57,9.79" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,107.01,212.61,364.30,9.67">Statistical Selection of Exact Answers (MultiText Experiments for TREC 2002)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kemkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Laszlo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Terra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Tilker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,238.53,225.21,287.49,9.67;6,70.53,238.05,23.14,9.67">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2003. November 19-22, 2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.53,250.89,408.77,9.67;6,70.53,267.33,171.53,9.67;6,70.53,283.89,166.49,9.67" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Gaithersburg</surname></persName>
		</author>
		<ptr target="www.connexor.com.Xelda" />
		<imprint>
			<date type="published" when="2003">2006. 2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology. Connexor</orgName>
		</respStmt>
	</monogr>
	<note>www.temis-group.com</note>
</biblStruct>

<biblStruct coords="6,70.53,300.33,455.51,9.67;6,70.53,313.05,294.77,9.79" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,209.25,300.33,316.79,9.67;6,70.53,313.17,35.88,9.67">Improving the Effectiveness of Information Retrieval with Local Context Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,113.49,313.05,187.06,9.67">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="112" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
