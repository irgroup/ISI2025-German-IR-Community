<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.66,101.19,354.68,15.15;1,213.53,121.11,184.94,15.15">Juru at TREC 2006: TAAT versus DAAT in the Terabyte Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,233.47,181.70,67.49,8.77"><forename type="first">David</forename><surname>Carmel</surname></persName>
							<email>carmel@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.46,181.70,68.07,8.77"><forename type="first">Einat</forename><surname>Amitay</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.66,101.19,354.68,15.15;1,213.53,121.11,184.94,15.15">Juru at TREC 2006: TAAT versus DAAT in the Terabyte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">823B59F04716D0455AD560B1B6F0F2DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our experiments focused this year on the ad-hock task of the Terabyte track. We experimented with WAND, a document-ata-time evaluation algorithm we developed recently. Our results demonstrate the superiority of WAND over traditional term-atime strategy while searching over a large collection such as gov2. We demonstrate how Web expansion can be successfully applied to significantly improve search results. In addition, we describe several schemes for creating manual queries, following this year's goal to enrich the pool of results by manual runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ad-hock task of the Terabyte track evaluates the ability of a search system to retrieve precise search results from a large collection in a reasonable time. Last year, due to scale limitations of our system, we distributed the .gov2 data into 10 sub-collections and applied federated search over them <ref type="bibr" coords="1,380.63,460.94,9.96,8.74" target="#b3">[4]</ref>. Following significant improvements in our search system, Juru <ref type="bibr" coords="1,306.17,471.90,9.96,8.74" target="#b1">[2]</ref>, this is the first time that we were able to index and search the entire .gov2 collection using only one index.</p><p>Juru's original query processing is a standard term-at-a-time (TAAT ) strategy as applied by many IR systems. During query execution, it maintains a heap for holding document partial scores. The algorithm orders the query terms by decreasing weight, according to their document frequency, and then traverses the infrequent terms first until the heap is full. After that, the algorithm moves to a "continue" mode for which it only updates already existing document accumulators in the heap. New non-scored documents in the remaining posting lists are ignored. During the continue mode posting lists are traversed very quickly by accessing only documents already found in the heap.</p><p>On the other hand, document-at-a-time (DAAT ) strategies fully evaluate the document score by considering the contributions of all query terms with respect to the document before moving to the next document. While TAAT strategies are by far most common in traditional IR systems due to the simplicity of their implementation, there are some clear advantages to (DAAT ) strategies, especially when dealing with very large data sets. First, it is usually not feasible to maintain partial results for all candidate documents in main memory. Second, I/O operations required for posting retrieval can be easily parallelized when using DAAT strategies, since all posting lists are traversed in parallel. Finally, advanced search features such as Boolean operators, proximity operators, and numeric range constraints, can be handled more efficiently by DAAT strategies, since all conditions can be evaluated at the same time to decide whether a document "satisfies" the query. It is thus clear that search engines which must handle context-sensitive queries over very large collections should prefer DAAT strategies. Indeed most Web search engines employ such strategies.</p><p>The main goal of our experiments this year, in the ad-hock task of the Terabyte track, was to implement a DAAT algorithm in Juru, and to optimally tune it for Web search. WAND <ref type="bibr" coords="2,218.71,201.68,10.52,8.74" target="#b0">[1]</ref> is a DAAT algorithm specifically designed for high precision search over very large collection, hence, it perfectly fits the Terabyte ad-hock task.</p><p>The rest of the paper is organized as follows. Section 2 shortly describes the WAND algorithm. A full description can be found in <ref type="bibr" coords="2,317.93,240.54,9.96,8.74" target="#b0">[1]</ref>. Section 3 describes our experiments with WAND, comparing its performance to our original TAAT algorithm. Section 4 describe several schemes for creating manual queries, following this year's goal to enrich the pool by submitting manual runs. Section 5 analyses the results of our official runs. Finally, Section 6 summarizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The WAND algorithm</head><p>The WAND algorithm is suited for DAAT strategies that evaluates queries using two levels of granularity. The algorithm iterates in parallel over query term postings and identifies candidate documents using a preliminary evaluation taking into account only partial information on term occurrences and no query independent factors. Once a candidate document is identified, it is fully evaluated and its exact score is computed. Furthermore, as in the standard DAAT approach, our algorithm iterates in parallel over query term postings but the nature of the preliminary evaluation is such that it is possible to skip quickly over large portions of the posting lists. If the result of this "fast and rough" evaluation is above a certain threshold, varied dynamically during the execution, then a full evaluation is performed and the exact score is computed.</p><p>Our approach allows both safe optimization and approximate optimization. For safe optimization, the two-level strategy makes no false-negative errors and thus it is guaranteed to return the top documents in the correct order and with accurate scores. For an approximate optimization, dynamic pruning techniques are used such that fewer documents pass the preliminary evaluation step, that is, we allow some false-negative errors at the risk of missing some candidate documents whose accurate scores would have placed them in the returned document set. The amount of pruning can be controlled by the user as a function of time allocated for query evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document scoring</head><p>The final score of a document involves a textual score which is based on the document textual similarity to the query, as well as other query independent factors such as connectivity for web pages, citation count for scientific papers, inventory for e-commerce items, etc. We assume an additive scoring model, that is, the textual score of each document is determined by summing the contribution of all query terms belonging to the document. Thus, the textual score of a document d for query</p><formula xml:id="formula_0" coords="3,126.00,75.16,360.00,33.80">q is: Score(d, q) = t∈q∩d α t w(t, d)<label>(1)</label></formula><p>For example, for the tf × idf scoring model, α t is a function of the number of occurrences of t in the query, multiplied by the inverse document frequency (idf ) of t in the index and w(t, d) is a function of the term frequency (tf ) of t in d, divided by the document length |d|.</p><p>In addition we assume that each term is associated with an upper bound on its maximal contribution to any document score, UB t such that UB t ≥ α t max(w(t, d 1 ), w(t, d 2 ), . . .).</p><p>Thus, by summing the upper bounds of all query terms appearing in a document, we can determine an upper bound on the document's query-dependent score.</p><formula xml:id="formula_1" coords="3,227.63,243.82,258.38,20.14">UB (d, q) = t∈q∩d UB t ≥ Score(d, q).<label>(2)</label></formula><p>At the heart of our approach, there is a Boolean predicate called WAND standing for Weak AND, or Weighted AND. WAND takes as arguments a list of Boolean variables X 1 , X 2 , . . . , X k , a list of associated positive weights, w 1 , w 2 , . . . , w k , and a threshold θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By definition, WAND</head><formula xml:id="formula_2" coords="3,221.42,328.99,264.58,41.27">(X 1 , w 1 , . . . X k , w k , θ) is true iff 1≤i≤k x i w i ≥ θ,<label>(3)</label></formula><p>where x i is the indicator variable for X i , that is</p><formula xml:id="formula_3" coords="3,257.96,398.21,93.48,19.70">x i = 1, if X i is true 0, otherwise.</formula><p>Given this setup, our preliminary scoring consists of evaluating for each document d, WAND(X 1 , UB 1 , X 2 , UB 2 , . . . , X k , UB k , θ), where X i is an indicator variable for the presence of query term i in document d and the threshold θ is varied during the algorithm as explained below. If WAND evaluates to true, then the document d undergoes a full evaluation.</p><p>The threshold θ is set dynamically by the algorithm based on the minimum score m among the top n results found so far, where n is the number of requested documents. The larger the threshold, the more documents will be skipped and thus we will need to compute full scores for fewer documents. It is easy to see that if the contribution upper bounds are accurate, then the final score of a document is no greater than its preliminary upper bound, and therefore all documents skipped by WAND with θ = m would not be placed in the top scoring document set by any other alternative scheme that uses the same additive scoring model.</p><p>However, (a) we might have only approximate upper bounds for the contribution of each term, (b) the score might involve query independent factors, and (c) we might want to use a higher threshold in order to execute fewer full evaluations. Thus in practice we will set</p><formula xml:id="formula_4" coords="3,285.02,633.02,200.99,8.74">θ = F • m (4)</formula><p>where F is a threshold factor chosen to balance the positive and negative errors for the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Setting the WAND Threshold</head><p>Assume that we wish to retrieve the top n scoring documents for a given query. The algorithm will maintain a heap of size n to keep track of the top n results. When a new candidate is returned by the WAND iterator, this document is fully evaluated using the system's scoring model resulting in the precise score for this document. If the heap is not full the candidate is inserted into the heap. If the heap is full and the new score is larger than the minimum score in the heap, the new document is inserted into the heap, replacing the one with the minimum score.</p><p>The threshold value that is passed to the WAND iterator is set based on the minimum score of all documents currently in the heap. Recall that this threshold determines the lower bound that must be exceeded for a document to be considered as candidate and to be passed to the full evaluation step.</p><p>The initial threshold is set based on the query type. For an OR query, or for a free-text query, the initial threshold is set to zero. The approximate score of any document that contains at least one of the query terms would exceed this threshold and would thus be returned as a candidate. Once the heap is full and a more realistic threshold is set, we only fully evaluate documents that have enough terms to yield a high score. For an AND query, the initial threshold can be set to the sum of all term upper bounds. Only documents containing all query terms would have a high enough approximate score to be considered candidates.</p><p>The initial threshold can also be used to handle mandatory terms (those preceded by a '+'). The upper bound for such terms can be set to some huge value, H, which is much larger than the sum of all the other terms upper bounds. By setting the initial threshold to H, only documents containing the mandatory term will be returned as candidates. If the query contains k mandatory terms, the initial threshold should be set to k • H.</p><p>So far we have only described methods in which we are guaranteed to return accurate results for the query (safe evaluation). However, the threshold can additionally be used to expedite the evaluation process by being more opportunistic in terms of selecting candidate documents for full evaluation. In this case, the threshold would be set to a value larger than the minimum score in the heap. By increasing the threshold, the algorithm can dynamically prune documents during the approximation step and thus fully evaluate less overall candidates but with higher potential. The cost of dynamic pruning is the risk of missing some high scoring documents and thus the results are not guaranteed to be accurate. However, in many cases this can be a very effective technique. For example, systems that govern the maximum time spent on a given query can increase the threshold when the time limit is approaching thus enforcing larger skips and fully evaluating only documents that are very likely to make the final result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our experiments are based on the the .gov2 collection and on the 99 topics of the Terabyte tracks of 2004 and 2005. Short queries are based on topic titles while long queries are based on topic title plus description. We measure average query time by measuring the time required to retrieve the top 20 results (following the track's instructions). Precision is measured by p@5, p@10, and MAP, by retrieving the top 1000 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TAAT versus DAAT</head><p>In the first experiment we compared the original TAAT strategy, originally used by Juru, with the new DAAT strategy as implemented by WAND. For TAAT we used 2 different variants of heap size, 100K and 200K, which are reasonable for the expected number of results (1000). For WAND we used much smaller heaps of 1K and 2K (for query time evaluation we used a heap of size 20). The threshold factor F was fixed to 1.0 (see Equation <ref type="formula" coords="5,273.87,150.32,3.88,8.74">4</ref>). Both implementations used the same scoring mechanism of Juru. Table <ref type="table" coords="5,241.13,161.28,4.98,8.74" target="#tab_0">1</ref> shows the results in terms of query execution time and precision.</p><p>Short Query Long Query time(sec) P@5 P@10 MAP time P@5 P@10 MAP TAAT 100K</p><p>2.9 The results clearly show the advantage of WAND compared to TAAT in terms of precision and search time. While handling a much smaller heap WAND enables to achieve much better results than the TAAT strategy. The main reason is the better pruning mechanism of WAND compared to the pruning process concealed in TAAT. There is also significant improvement in search time. Retrieving top 20 results by WAND only requires a heap of size 20 -a factor that strongly affects search time, while in contrast, TAAT must handle a huge heap even when only 20 results are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phrase Expansion</head><p>During query evaluation, query terms are constructed to include the query's original keywords and lexical affinities (LAs) of the query <ref type="bibr" coords="5,349.46,460.94,9.97,8.74" target="#b2">[3]</ref>. This is achieved by finding all pairs of query words found close to each other in a window of some predefined small size. For each LA, Juru creates a pseudo posting list by merging the posting lists of the LA terms. It then finds all documents in which these terms appear close to each other, and adds them to the posting list of the LA with all the relevant occurrence information. After creating the posting list, the new LA is treated by the retrieval algorithm as any other term in the query. The user can also control the relative weight between keywords and LAs, thus giving more (or less) significance to LAs in relation to simple keywords in computing the relevance score.</p><p>The query can also be expanded by the query phrase. The posting list of the query phrase is created by merging the postings of all terms, considering only documents containing the query terms in adjacent offsets and in the right order. Thus, documents containing the query phrase are biased compared to other documents.</p><p>Similarly to LA weight which specifies the relative weight between an LA term and simple keyword term, a phrase weight specifies the relative weight of a phrase term. Table <ref type="table" coords="5,180.67,637.28,4.98,8.74">2</ref> shows the results for phrase expansion with several different phrase weights. The results are averaged for the 99 short queries of the Terabyte topics, using WAND with 1K heap size.</p><p>Phrase weight 0 0.1 0.2 0.3 P@5 0.618 0.614 0.602 0.594 P@10 0.575 0.577 0.573 0.562 MAP 0.305 0.310 0.307 0.304 Table <ref type="table" coords="6,218.71,125.87,4.13,7.89">2</ref>. Phrase expansion with different phrase weights.</p><p>It seems that phrase expansion slightly contributes to average search precision. However, it does not help to improve the top results. This is quite surprising due to the common assumption that the existence of the query phrase in a document is a good indication for its relevancy. This phenomenon should be further investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Web query expansion</head><p>Our next experiment dealt with query expansion based on external resource. Following the good results obtained by several groups using Web expansion in previous years, we upgraded our system to benefit Web expansion using Answers.com search engine. Answers.com is a free search service, providing instant answers over many topics. As opposed to standard search engines that serve up a list of links to follow, it displays quick, snapshot answers with concise, reliable information. If it fails to answer the query it returns the first result returned by Google for that query. Our expansion procedure works by first submitting the topic title to answer.com, and then using the result page for query expansion. All query terms are expanded by their lexical affinities as extracted from the expanding Web page <ref type="bibr" coords="6,411.24,346.54,9.96,8.74" target="#b2">[3]</ref>.</p><p>Unfortunately, We could not demonstrate significant improvement using Web expansion for the 99 training topics. However, following our belief in the quality of this mechanism, one of our official runs for this year's task is based on Web expansion. Indeed, this is our best run for this year's task (see Table <ref type="table" coords="6,379.72,396.35,3.88,8.74" target="#tab_1">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Some other experiments</head><p>We also experimented with several other parameters to tune our system for the .gov2 collection while using WAND. In agreement with previous results, document static scores, based on number of in-links of the web pages, were found to be very useful. The document's static score is linearly combined with its textual score to provide its final score.</p><p>In contrast to static scores, anchor-text which is considered to be very helpful for TREC Web tasks, damaged the results for the 99 training topics we used. We do not have a good explanation for that. One speculation is that anchor text should be only used when it contains all query terms. This will avoid the cases that one query term, frequently appearing within many anchors, negatively biases the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Manual Run</head><p>A challenging task this year was to manually construct queries that will answer the topic while enriching the pool with unique results. In order to promote unique results we first studied the results returned with the plain runs, assuming most automatic runs will have a similar bias toward documents containing topic terms.</p><p>Our main assumption was that the way to achieve the best uniqueness score is to try and re-rank the top 50 results of each run to include results that will not appear in a normal topic title run so that the manual and the automatic runs will agree on as few documents as possible. We came up with taxonomy of query styles that will yield different results from the "conventional" run: Skew Query, Reduce Query, Alternate Query, and Substitute Query. Figure <ref type="figure" coords="7,331.63,108.03,4.98,8.74" target="#fig_0">1</ref> shows the schemes of the different query types. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skew Query</head><p>This is a query that keeps the most important terms in the query but changes one of the less important terms to create a partial overlap with the expected automatic run. An example for this is topic number 801 Kudzu Pueraria lobata, which we changed to +Kudzu Eradication, control, herbicide, Tordon<ref type="foot" coords="7,386.28,319.74,3.97,6.12" target="#foot_0">1</ref> . We identified Kudzu to be a unique and unambiguous term which appears 1756 times in the collection.</p><p>Similarly, topic number 829, Spanish Civil War support, was manually changed to +"Spanish Civil War" aid. The unambiguous phrase "Spanish Civil War" appears in the collection 354 times. In these topics the terms Kudzu and "Spanish Civil War" were enough to define the scope of the query and by forcing the system to return only answers that contain them we were able to farther filter those relevant documents with terms that slightly change the ranking of the results. In both queries we retrieved all documents in the Qrels set (801: 128/128 and 829: 24/24).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reduce Query</head><p>An example for this can be found in our editing for topic 816 USAID assistance to Galapagos which we changed to +USAID +Galapagos. In the collection, the term Galapagos and the term USAID appear as lexical affinity only six times, which means that most occurrences of the terms in the Qrels (there are 23 of those) do not appear next to each other. However, the term assistance appears 2798691 times from which 10666 times as a lexical affinity of USAID. This means that by reducing the query to just USAID and Galapagos we were able to promote only the essential terms in the query. We contend that the term assistance provide more noise than information in this case and that by reducing the query to two terms we remove most of the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternate Query</head><p>This type of modification is an "OR" query where both sides of the "OR" are conceived as perfectly good queries. Query 806, Doctors Without Borders, is an excellent example. For this query we requested all the documents that contain either "Doctors Without Borders" or "Medecins sans Frontieres". Each of the queries describes the topic and is unambiguous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Substitute Query</head><p>This is a query in which all the original terms are replaced with new terms. For example, we replaced the terms in topic 820, imported fire ants, with their Latin name "Solenopsis invicta" "Solenopsis richteri" since it is likely that only articles that specifically discuss imported fire ants will make the effort to mention their Latin name.</p><p>In many of the reformulated queries we reduced the number of possible results and "risked" ourselves in finding fewer relevant results and thus achieve a worse run overall. However, since most of the unique results are found somewhere in the "fringes" of the data this was a calculated risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Official Submissions</head><p>We submitted 4 official runs for this year ad-hock task:</p><p>1. JuruMan: queries were created manually for the new topics. The main goal is to diversify the results from those returned by automatic runs which are usually biased toward documents containing topic terms.</p><p>2. JuruT: queries are automatically created based on the topic title.</p><p>3. JuruTD: queries are automatically created based on the topic title and description.</p><p>4. JuruTWE: title queries are expanded by LAs extracted from the Web page returned by Answers.com for that query. The LAs are those containing one of the query terms.</p><p>For all runs we used WAND, using static scores, and applying phrase expansion for the automatic runs. Anchor text was disregarded by setting the anchor term weight to zero. Table <ref type="table" coords="8,188.07,411.59,4.98,8.74" target="#tab_1">3</ref> shows the results, compared to the average median and the average best results of all participating systems. Figur 2 shows the difference between the AP of Juru's title-based runs to the median AP of all automatic runs, for each of the new topics. Our best run is JuruTWE. Web expansion significantly improves the results, mostly P@10. However, it is interesting to note that Web expansion works best for "easy" topics -for many difficult topics expansion does not help much. For most topics, Juru's results seems to be consistently better than the median result of all runs. This is also true for the manual run that its results are better than the median results of all 19 manual runs. However, the results of Juru's manual run are lower than the results of the automatic runs. This is agreement with the other manual runs for which the median MAP is lower than the median MAP of the automatic runs. A possible explanation is that manual runs focus on diversity from the automatic runs. In addition, the manual runs return less results per topic than the 1000 returned by the automatic runs (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>The results we obtained from this year's experiments demonstrate the superiority of DAAT strategy over TAAT for searching over large collection. While handling a much smaller heap in memory, WAND enables to achieve better results than the TAAT strategy. The main reason is the better pruning mechanism of WAND compared to the pruning process concealed in TAAT. Furthermore, Web expansion proved to be a powerful tool for improving search results.</p><p>Finally, this is the first time that we were able to index and search over (half) terabyte of data in a reasonable time, using only one index, while achieving decent results -an important milestone for our search system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,216.07,228.00,179.86,7.89;7,198.00,151.26,216.00,62.96"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schemes of the different query types.</figDesc><graphic coords="7,198.00,151.26,216.00,62.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,126.00,273.60,360.01,7.89;9,126.00,283.58,110.41,7.86;9,126.00,72.00,360.00,187.82"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The difference between the AP of Juru's title-based runs to the median AP of all 61 automatic ad-hock runs.</figDesc><graphic coords="9,126.00,72.00,360.00,187.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,126.00,229.66,360.00,66.49"><head>Table 1 .</head><label>1</label><figDesc>The search results of the original Juru's TAAT strategy compared to WAND, averaged over the 99 Terabyte topics.</figDesc><table coords="5,177.36,229.66,254.65,40.94"><row><cell></cell><cell></cell><cell>0.53 0.49 0.25 11.8 0.53 0.523 0.25</cell></row><row><cell>TAAT 200K</cell><cell>3.3</cell><cell>0.53 0.49 0.25 12.3 0.53 0.523 0.25</cell></row><row><cell>WAND 1K</cell><cell>1.87</cell><cell>0.58 0.56 0.29 3.02 0.58 0.55 0.25</cell></row><row><cell>WAND 2K</cell><cell>1.87</cell><cell>0.62 0.58 0.31 3.02 0.59 0.56 0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,126.00,480.50,360.00,122.28"><head>Table 3 .</head><label>3</label><figDesc>Official results of TREC 2006, compared to the average median and the average best results of all runs.</figDesc><table coords="8,237.42,480.50,137.15,94.74"><row><cell></cell><cell>MAP P@10 infAP</cell></row><row><cell>JuruT</cell><cell>0.329 0.572 0.269</cell></row><row><cell>JuruTD</cell><cell>0.343 0.580 0.280</cell></row><row><cell>JuruTWE</cell><cell>0.351 0.638 0.269</cell></row><row><cell cols="2">auto-Median 0.291 0.540 0.241</cell></row><row><cell>auto-Best</cell><cell>0.517 0.870 0.473</cell></row><row><cell>JuruMan</cell><cell>0.275 0.570 0.241</cell></row><row><cell cols="2">man-Median 0.243 0.562 0.198</cell></row><row><cell>man-Best</cell><cell>0.512 0.908 0.479</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,135.96,659.88,342.94,7.86"><p>In the query syntax of our system, a query term marked by '+' is must-appear term.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,129.59,586.02,356.42,7.86;9,138.15,595.98,347.85,7.86;9,138.15,605.94,347.85,7.86;9,138.15,615.91,200.19,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,468.98,586.02,17.03,7.86;9,138.15,595.98,225.23,7.86">Efficient query evaluation using a two-level retrieval process</title>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,382.57,595.98,103.43,7.86;9,138.15,605.94,318.01,7.86">CIKM &apos;03: Proceedings of the twelfth international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.59,629.99,356.42,7.86;9,138.15,639.95,347.86,7.86;9,138.15,649.91,347.85,7.86;9,138.15,659.88,48.64,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,184.75,639.95,205.46,7.86">Juru at TREC 10 -Experiments with Index Pruning</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Einat</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miki</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoelle</forename><forename type="middle">S</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yael</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,408.14,639.95,77.86,7.86;9,138.15,649.91,146.37,7.86">Proceeding of Tenth Text REtrieval Conference (TREC-10</title>
		<meeting>eeding of Tenth Text REtrieval Conference (TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology. NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.59,75.84,356.42,7.86;10,138.15,85.80,347.85,7.86;10,138.15,95.76,347.84,7.86;10,138.15,105.73,193.34,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,389.97,75.84,96.03,7.86;10,138.15,85.80,236.32,7.86">Automatic query refinement using lexical affinities with maximal information gain</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eitan</forename><surname>Farchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yael</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.76,85.80,92.24,7.86;10,138.15,95.76,347.84,7.86;10,138.15,105.73,51.00,7.86">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.59,119.67,356.42,7.86;10,138.15,129.64,347.85,7.86;10,138.15,139.60,347.85,7.86;10,138.15,149.56,170.82,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,163.92,129.64,318.27,7.86">Juru at TREC 2005: Query Prediction in the Terabyte and the Robust tracks</title>
		<author>
			<persName coords=""><forename type="first">Elad</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shai</forename><surname>Errera-Yaakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shai</forename><surname>Fine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,149.70,139.60,202.54,7.86">Proceedings of the 14th Text REtrieval Conference</title>
		<meeting>the 14th Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology. NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
