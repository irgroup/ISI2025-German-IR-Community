<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.75,152.19,279.75,18.08">UMass at TREC 2006: Enterprise Track</title>
				<funder>
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
				<funder ref="#_WTNG8MG">
					<orgName type="full">Acquisition Services Division</orgName>
				</funder>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder>
					<orgName type="full">Department of the Interior, NBC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.96,185.55,90.84,12.55"><forename type="first">Desislava</forename><surname>Petkova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.45,185.55,79.81,12.55"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.75,152.19,279.75,18.08">UMass at TREC 2006: Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4504C3543A81EF1C8790FD13B330CEA6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper gives an overview of the work done at the University of Massachusetts, Amherst for the TREC 2006 Enterprise track. For the discussion search task, we compare two methods for incorporating thread evidence into the language models of email messages. For the expert finding task, we create implicit expert representations as mixtures of language models from associated documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we discuss two retrieval tasks in an enterprise setting: finding email messages which contain pro or con arguments on a given topic, and finding people who are knowledgeable in particular area. Both tasks offer unique challenges that are particular to enterprise search: diversity of structured and unstructured documents, variety of languages and formats, existence of social networks, security requirements. In our work we consider some of these issues, in particular document structure and heterogeneity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data processing</head><p>For our experiments, we used the Indri search engine in the Lemur toolkit <ref type="bibr" coords="1,162.99,604.57,9.96,10.46" target="#b4">[5]</ref>. Its powerful query language allows formulating richly structured queries and incorporating various sources of contextual evidence.</p><p>We preprocessed the www and lists subcollections of the W3c corpus, removing HTML tags from web documents and stripping quoted text and signature lines from email documents. We used Jangada <ref type="bibr" coords="1,528.73,268.15,10.52,10.46" target="#b0">[1]</ref> to extract signature blocks and reply-to lines from emails. Finally, we used the all-in-reply-to list compiled by William Webber to group emails by thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion Search</head><p>Emails form a considerable part of the communication in an organization and are characterized by rich internal and external structure. Previous work has shown that email structure is a useful source of information in known-item finding <ref type="bibr" coords="1,443.44,406.32,9.96,10.46" target="#b3">[4]</ref>.</p><p>We exploit internal structure by weighting header and mainbody text differently and external structure by adding a third component corresponding to thread text. In the header we combine evidence from the subject, date, to, from and cc fields. The mainbody is the original text of a message with reply-to, forward and signature lines removed. And the thread is the concatenated text of messages in the tree-like structure of an email conversation.</p><p>One approach for incorporating thread context is to estimate language model for the thread and interpolate it with the smoothed language models of the other email components. This corresponds to the following retrieval model:</p><formula xml:id="formula_0" coords="1,319.07,593.99,211.72,26.30">P (t|D) = λ 1 P (t|D header ) + λ 2 P (t|D mainbody ) + λ 3 P (t|D thread )</formula><p>where λ 1 + λ 2 + λ 3 = 1 and</p><formula xml:id="formula_1" coords="1,331.63,649.52,186.60,25.40">P (t|D component ) = P M LE (t|D component ) + αP (t|Corpus)</formula><p>with α being a general symbol for smoothing. An alternative way to take advantage of thread information is to use it as a background model for smoothing maximum likelihood estimates from the header and mainbody. The hypothesis is that threads will provide a more reasonable fallback distribution than a word distribution for general English.</p><formula xml:id="formula_2" coords="2,124.51,215.41,123.62,26.31">P (t|D) = λ 1 P (t|D header ) + λ 2 P (t|D body )</formula><p>where</p><formula xml:id="formula_3" coords="2,93.03,264.78,186.60,41.24">P (t|D component ) = P M LE (t|D component ) + αP (t|D thread ) P (t|T hread) = P M LE (t|T hread)</formula><p>+ βP (t|Corpus)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Runs</head><p>We submitted four official runs for evaluation. For all runs we the Term Dependency Model <ref type="bibr" coords="2,268.53,367.03,10.52,10.46" target="#b2">[3]</ref> and the Relevance Model <ref type="bibr" coords="2,170.28,378.99,10.52,10.46" target="#b1">[2]</ref> for pseudo-relevance feedback to expand the query. Term dependency increases precision by adding proximity features and pseudo-relevance feedback increases recall by adding terms related to the original query. We use the following mixing weights for the components models: λ header = 0.3,λ mainbody = 0.7,λ thread = 0.3. Results show that smoothing with a thread-based fallback model is more effective than smoothing with a general collection model. However, constructing a mixture of language models from header, main body and thread text is even more effective. And corroborating previous research, internal and external email structure is an useful source of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Expert Search</head><p>We propose a formal method for constructing hierarchical expert representations, based on statistical relevance modeling. Our main goal is that this process takes advantage of various information sources and prior knowledge about the experts, the collection or the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>Our expert modeling approach includes the following steps:</p><p>1. Define what is a reference to E, so that occurrences of E can be detected.</p><p>2. Rank and retrieve profile documents. We use language modeling with Dirichlet smoothing.</p><p>3. For each document D in the profile set S E , compute the posterior P (D|E), assuming that the prior distribution is uniform.</p><formula xml:id="formula_4" coords="2,380.28,648.40,108.03,24.03">P (D|E) = P (E|D)P (D) P (E)</formula><p>4. Form a term distribution for E by incorporating the document model P (t|D) and marginalizing. P (t|D) is the maximum likelihood estimate.</p><formula xml:id="formula_5" coords="3,129.94,173.47,132.69,22.37">P (t|E) = D∈S E P (t|D)P (D|E)</formula><p>To summarize, we represent an expert as a mixture of documents, where the mixing weights are specified by the posterior distribution P (D|E). Once we have built models for all candidates, we find experts relevant to a particular topic Q by ranking the candidates according to query likelihood. The result of Eq. ( <ref type="formula" coords="3,175.45,279.86,4.24,10.46">4</ref>) is a probability distribution of words describing the context of an expert's name, where P (•|E) is estimated using a particular name definition and from a homogeneous collection of documents. Representations estimated from different collections or alternative name definitions can be interpolated to build hierarchical expert models.</p><formula xml:id="formula_6" coords="3,107.53,374.12,157.59,21.77">P (t|E) = c∈C λ c P (t|E c ), c∈C λ c = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>With our experiments we want to determine whether hierarchical expert models can effectively create rich representations from heterogeneous data and answer complex queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heterogeneous sources</head><p>The W3C corpus is composed of several subcollections comprising documents of particular type. We independently build a language model from one subcollection at a time and then represent an expert as a mixture of those models. This allows us to treat each subcollection differently according to its specific intrinsic properties, e.g. when smoothing to estimate P (E|D), as well as to weight the information sources.</p><p>We use the lists subcollection (average length 450 words after preprocessing) and the www collection (average length 2000). We automatically set the Dirichlet smoothing µ parameter to the average document length, and we experimentally determine an optimal value for the mixing parameter λ www = 0.6.</p><p>Results are reported in Table <ref type="table" coords="3,447.83,126.48,3.87,10.46">2</ref>. Although models built from www outperform models built from lists, by combining the two we achieve an even better performance, indicating that email discussion lists contain information not contained in the web pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Term dependency</head><p>An important feature of our model is that it preserves information inherent in documents, such as structure and term positions. Consequently, it can capture higher-level language features such as relationships between terms. Note that because experts are modeled indirectly as a set of documents, it is possible that query terms appear in the profile set but do not co-occur in any document within that set.</p><p>We implement term dependency as described by Metzler and Croft <ref type="bibr" coords="3,394.59,347.65,9.96,10.46" target="#b2">[3]</ref>, using both sequential dependency and full dependency between query terms to include restrictions on terms appearing in close proximity in the text. Results (Table <ref type="table" coords="3,458.79,383.51,4.43,10.46">2</ref>) show consistent improvement in mean average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining expert definitions</head><p>Finally we compare two rules to define a person entity: LAST matches the last name of a candidate, and FULL matches the first and last name separated by at most two words. LAST is a loose definition since some people have the same family name. FULL is more strict but misses some true associations, since people are not necessarily referred to with their full names, especially in emails.</p><p>This is an example of the tradeoff between recall and precision. The profile set from a loose definition is larger but more ambiguous as some associations are incorrect. On the other hand, the profile set from a strict definition is smaller but more precise as retrieved documents are reliably associated with the person but at the same time valid documents are overlooked. Combining two expert definitions, LAST and FULL gives a slightly better performance than either alternative separately (Table <ref type="table" coords="3,467.38,664.46,3.87,10.46">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conclusion</head><p>We described a language modeling approach for finding people who are experts on a given topic. It is based on collecting evidence for expertise from multiple sources in a heterogeneous collection, using language modeling to find associations between documents and experts and estimate the degree of association, and finally integrating models to construct rich and effective expert representations.</p><p>Our approach provides a broad framework for answering questions about experts. It is based on the following two assumptions: 1. We can come up with useful definition(s) of a named entity in the form of a query. This allows us to use any retrieval technique to find associated documents. 2. We can assume that the co-occurrence of terms and named entities is evidence of topical connection. Note that this means we do not distinguish between positive and negative document support. On the other hand, since the model does not have any specific knowledge about what it means to be an expert, it is very general and can be applied to building representations for other named entities such as places, events, organizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,79.19,126.48,460.05,548.44"><head>Table 1 :</head><label>1</label><figDesc>Similar to UMaTiMixThr but also using the description field of the query topic. Discussion search: submitted runs.</figDesc><table coords="2,79.19,562.73,221.45,112.19"><row><cell>3. UMaTiSmoThr: The header and mainbody com-</cell></row><row><cell>ponents are both smoothed with the thread</cell></row><row><cell>which a particular message is part of. (Messages</cell></row><row><cell>are not part of a thread are smoothed with the</cell></row><row><cell>collection, as in UMaTiMixHdr.)</cell></row><row><cell>4. UMaTiMixThr: Emails have three components -</cell></row><row><cell>header, mainbody and text, smoothed indepen-</cell></row><row><cell>dently with background models of header, main-</cell></row><row><cell>body and thread text.</cell></row></table><note coords="2,79.19,466.87,221.45,10.46;2,91.93,478.83,208.72,10.46;2,91.93,490.78,139.57,10.46;2,79.19,508.83,221.45,10.46;2,91.93,520.78,208.71,10.46;2,91.93,532.73,208.72,10.46;2,91.93,544.69,152.79,10.46"><p>1. Baseline: (Unofficial) Uses the entire email content (header fields and mainbody) without breaking it up into components. 2. UMaTiMixHdr: Emails are divided into header and mainbody. The components are smoothed independently with background models of all header text and all mainbody text.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,96.92,525.40,178.81,10.46"><head>Table 3 :</head><label>3</label><figDesc>Expert finding: submitted runs.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs> and in part by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs>, through the <rs type="funder">Department of the Interior, NBC</rs>, <rs type="funder">Acquisition Services Division</rs>, under contract number <rs type="grantNumber">NBCHD030010</rs>. Any opinions, findings and conclusions expressed in this material are the author's and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WTNG8MG">
					<idno type="grant-number">NBCHD030010</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="4,99.06,223.96,3.87,10.46">2</ref>: mAP is incrementally improved by combining several representations to form a hierarchy. Mixing parameters: λ F U LL = 0.9,λ LAST = 0.1,λ www = 0.6,λ lists = 0.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runs</head><p>We submitted four official runs which use the hierarchical representation described above. We expand the title-only query with the description and narrative fields, and with pseudo-relevance feedback terms. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="4,326.09,473.97,213.15,10.46;4,326.10,485.92,213.15,10.46;4,326.10,497.87,46.47,10.46" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,488.08,473.97,51.17,10.46;4,326.10,485.92,193.48,10.46">Learning to extract signature and reply lines from email</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">R</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In CEAS &apos;04</note>
</biblStruct>

<biblStruct coords="4,326.09,516.46,213.16,10.46;4,326.10,528.41,139.20,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,467.77,516.46,71.48,10.46;4,326.10,528.41,69.79,10.46">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,417.48,528.41,43.82,10.46">SIGIR &apos;01</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.09,547.00,213.15,10.46;4,326.10,558.96,213.15,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,458.83,547.00,80.42,10.46;4,326.10,558.96,144.51,10.46">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,491.59,558.96,43.66,10.46">SIGIR &apos;05</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.09,577.55,213.16,10.46;4,326.10,589.50,213.14,10.46;4,326.10,601.45,106.41,10.46" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,441.06,577.55,98.19,10.46;4,326.10,589.50,213.14,10.46;4,326.10,601.45,37.04,10.46">Experiments with language models for known-item finding of e-mail messages</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno>TREC &apos;05</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.09,620.04,213.16,10.46;4,326.10,632.00,213.15,10.46;4,326.10,643.95,213.15,10.46;4,326.10,655.91,101.75,10.46" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,362.96,632.00,176.30,10.46;4,326.10,643.95,121.25,10.46">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>UMass, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
