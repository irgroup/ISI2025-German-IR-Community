<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,54.00,261.84,238.16,9.00;1,54.00,273.84,238.16,9.00;1,54.00,285.84,238.16,9.00;1,54.00,297.84,238.16,9.00;1,54.00,309.84,238.16,9.00;1,54.00,321.84,238.16,9.00;1,54.00,333.84,238.16,9.00;1,54.00,345.84,238.16,9.00;1,54.00,357.84,238.16,9.00;1,54.00,369.84,238.16,9.00;1,54.00,381.84,238.16,9.00;1,54.00,393.84,238.21,9.00;1,54.00,405.84,188.66,9.00">The QACTIS system has been tested in previous years at the TREC Question Answering Evaluations. This paper describes new enhancements to the system specific to TREC-2006, including basic improvements and thresholding experiments, filtered and Internet-supported pseudo-relevance feedback for information retrieval, and emerging statistics-driven question-answering. For contrast, we also compare our TREC-2006 system performance to that of our top systems from TREC-2004 and TREC-2005 applied to this year&apos;s data. Lastly, we analyze evaluator-declared unsupportedness of factoids and nugget decisions of &quot;other&quot; questions to understand major negative changes in performance for these categories over last year</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dragon Development Corporation</orgName>
								<address>
									<settlement>Columbia</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Henggeler Computer Consultants</orgName>
								<address>
									<settlement>Columbia</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Johns Hopkins Applies Physics Laboratory</orgName>
								<address>
									<settlement>Laurel</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,54.00,261.84,238.16,9.00;1,54.00,273.84,238.16,9.00;1,54.00,285.84,238.16,9.00;1,54.00,297.84,238.16,9.00;1,54.00,309.84,238.16,9.00;1,54.00,321.84,238.16,9.00;1,54.00,333.84,238.16,9.00;1,54.00,345.84,238.16,9.00;1,54.00,357.84,238.16,9.00;1,54.00,369.84,238.16,9.00;1,54.00,381.84,238.16,9.00;1,54.00,393.84,238.21,9.00;1,54.00,405.84,188.66,9.00">The QACTIS system has been tested in previous years at the TREC Question Answering Evaluations. This paper describes new enhancements to the system specific to TREC-2006, including basic improvements and thresholding experiments, filtered and Internet-supported pseudo-relevance feedback for information retrieval, and emerging statistics-driven question-answering. For contrast, we also compare our TREC-2006 system performance to that of our top systems from TREC-2004 and TREC-2005 applied to this year&apos;s data. Lastly, we analyze evaluator-declared unsupportedness of factoids and nugget decisions of &quot;other&quot; questions to understand major negative changes in performance for these categories over last year</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18F0D4B03CC1A2BFED9F351253436D1B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>QACTIS (pronounced "cactus"), which breaks out to "Question-Answering for Cross-Lingual Text, Image, and Speech," is a research protoype system being developed by the U.S. Department of Defense. The goals and descriptions of this system are specifically described in past TREC descriptions (see <ref type="bibr" coords="1,124.74,508.84,79.40,9.00" target="#b0">Schone, et al., 2004</ref><ref type="bibr" coords="1,211.79,508.84,20.00,9.00" target="#b1">Schone, et al., , 2005</ref> in <ref type="bibr" coords="1,245.88,508.84,10.62,9.00" target="#b0">[1]</ref>, <ref type="bibr" coords="1,263.19,508.84,10.49,9.00" target="#b1">[2]</ref>). In this paper, though, we provide a self-contained description of modifications that have been made to the system in 2006. There were three major points of study upon which we conducted research this year: (1) basic improvements to the general processing strategy, (2) information retrieval enhancements as a prefilter, and (3) a move toward integration of more purely-statistical question answering. We describe each of these research avenues in some detail. For the sake of demonstrating these improvements, we evaluate our best systems from TREC-2004 and TREC-2005 on this year's evaluation data as a means of comparison. This comparison does not completely re-create all of the nuances of past-year systems, but we believe it provides an appropriate reflection of system performance over time.</p><p>After discussion of the system enhancements, we conduct a post-evaluation analysis of the results from the TREC-2006 evaluation. Our system improved slightly this year in terms of factoid and list answering. However, we experienced 10% and 20% relative losses in system performance due respectively to unsupportedness and inexactness --numbers which are too large to go without notice. The inexactness losses seem high but hopefully such degradation has been uniformly observed across systems. On the other hand, the unsupportedness is more suspicious. Unlike many other systems, which use the Internet as a pre-mechanism for finding answers and thereby have a chance of recalling the wrong file, our system solely draws its answers from TREC documents and does not mine the Web for answers. We conducted a number of post-evaluation experiments. One of these attempted to make a determination as to whether the unsupported labels were justified. We found through this analysis no systematic biases. Along a similar vein, at TREC-2005, QACTIS's "Other" answerer received the highest score, whereas this year, it suffered a 40% degradation in overall score. We conducted a study to understand this degradation. This evaluation also eliminated concerns about potential assessment problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CY2006 SYSTEM ENHANCEMENTS</head><p>In 2006, there were a number of new avenues of research on QACTIS. As mentioned earlier, these fall into three main directions. Specifically, these involved improvements to the base system, information retrieval enhancements for preselection of appropriate documents, and, lastly, a process to move away from symbolic processing to a more statistical system. We discuss each of these in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General System Improvements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Overcoming Competition-level Holes</head><p>One major modification was designed to overcome problems that only arise with the appearance of fresh data --QACTIS Enhancements in TREC QA-2006 P. Schone, G. Ciany*, R. Cutts a , P. McNamee ¦ , J. Mayfield ¦ , Tom Smith ¦ U.S. Department of Defense Ft. George G. Meade, MD 20755-6000 problems which unfortunately only really occur during competitions. In TREC2005, we had noticed that there were a number of questions which our parser failed to properly handle; there were other questions for which the system did not know what kind of information it was seeking; there were questions that were so long that our NIL-tagger generated inadvertent false positives; and lastly, there were many QACTIS-provided answers (as much as 20% relative) marked inexact.</p><p>The first two of these were readily solved. We ensured that the system always parsed any previouslyunprocessed documents and that it handled these properly at run time. We also attempted to require that any factoid question whose target answer form was unknown would at least return an entity type. Also, we sought to prevent non-responses for other-style questions by requiring the system to revisit the question in a new way if an earlier stage failed to provide a response. These changes were important for yielding a more robust question answerer, and it is possible that as much as 0.5-1% of the factoid score improvements is attributable to these fixes (particularly the parsing fixes).</p><p>Perhaps the most dramatic change to handling the problem of the unforeseen was to change the system's scoring metric to take length of question into consideration. Our system attempts to provide a probability-like estimate of the likelihood that some answer is legitimate given the question. All the non-stopwords of the question are important in the question-answering process, so longer questions will naturally have lower estimated probabilities than shorter questions. Nevertheless, it had previously been our policy to use a threshold as a means of estimating whether an answer should be reported as a NIL. This meant that long questions were more likely to erroneously report NIL as their answers than short questions. Through least squares fit, we determined that the probability scores were approximately proportional to 0.01 length_in_words for questions with at least three words. Therefore, we multiplied the scores of such questions by 100 (length_in_words-3) . We conducted an exhaustive search and determined an optimal new NIL threshold (of 10 -12 ). In our developments, this augmentation to the weight seemed to have only limited effect on score but prevented accidental discarding of legitimate answers.</p><p>The last challenge was to overcome the large number of answers that QACTIS produced that were being identified as inexact in past years. Our system might identify the appropriate last name of a person that should be reported as an answer, or it might identify the city without the state. We had previously built name and anaphora resolution into our system which we had not been using, and we experimented with various settings of these components to see if we might get some additional gains, but we were unsuccessful. We reasoned that use of a more recently-built content extractor with such resolution embedded could be especially beneficial. BBN was able to generate output for us from their SERIF <ref type="bibr" coords="2,480.01,107.85,11.66,9.00" target="#b2">[3]</ref> engine, and we began work to incorporate this information into an exactness filter. Unfortunately, we were not able to make use of this information prior to the evaluation. Ultimately, the only additional resolution that we could incorporate into the system by the time of the competition was to get the base system to augment city names with their corresponding state names when such information was present in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">What is the Actual Information Need?</head><p>Based on evaluations over past years, we noted that QAC-TIS produced erroneous answes for questions about court decisions, court cases, ranks, ball teams, scores, campuses, manufacturers, and, in some cases, titles of works of art. These problems were due largely to issues of either underspecificity or to providing a hyponym for a concept rather than a required instance of that concept.</p><p>With regard to underspecificity, "teams" provide a great example. If a question were of the form "What team won Super Bowl ...," it is clear to a human that the team that is being referenced is an NFL football team. Instead, if "World Cup" replaced "Super Bowl," the team should be a soccer team. Formerly, the system would seek out any kind of team for the appropriate response --a problem of underspecificity. To avoid this problem, we encoded knowledge into the system to help it better be able to reach the correct level of specificity particularly with teams. Likewise, in TREC-2005, the system was prepared to identify titles of works of art, but whether or not it could do it was subject to the way the question was posed. We tried to incorporate more generality into its ability to recognize when such a work of art was being requested. We have not removed all problems with this underspecificity, so this will be continued work as we prepare toward TREC-2007.</p><p>The hyponym/instance-of issue is likewise a prominent problem in the system. If the system were to see a question "What was the court decision in..." or "what was the score..." the system would think that it was looking for some hyponym of "court decision" and "score" rather than a particular verdict (guilty, innocent) or a numeric value, respectively. We implemented a number of specialized functions to tackle rarer-occuring questions such as these and ensure that the appropriate type of information was provided to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Missing Information Needs: Auto-hypernymy</head><p>There are related problems to looking for either hyponyms or instances of classes which are due to lack of world knowledge in some areas. For systems that use the Internet to provide them potential answers, they get around the problem of missing world information. Our base system is, for the most part, self-contained and we do not currently make direct use of the Web. Therefore, we need to ingest resources to support our question-answering. In past years, we have made use of WordNet <ref type="bibr" coords="3,247.71,131.84,14.42,9.00" target="#b3">[4]</ref> and a knowledge source we had previously developed called SemanticForests <ref type="bibr" coords="3,122.38,155.84,10.62,9.00" target="#b4">[5]</ref>, plus we had targeted specific categories of questions and derived large inventories of potential concepts under those categories through the use of <ref type="bibr" coords="3,262.73,179.84,18.34,9.00;3,54.00,191.84,34.85,9.00">Wikipedia [6]</ref>. This year, however, we tried to grow our world knowledge to much more than hundreds of categories and instead try to (a) ingest much or all of Wikipedia's taxonomic structure, and (b) automatically induce taxonomic structure on the fly.</p><p>For the first effort, we downloaded the entire English component of Wikipedia and distilled out all of its lists. Then we developed code that could turn those lists into a structure akin to the taxonomic structure required for ingestion by our system. Time constraints limited our ability to do this in a flawless fashion (and revisiting this issue is certainly in order for the future).</p><p>In addition to the use of this Wiki-generated taxonomic structure, we also experimented with hypernym induction as a means of finding still more information. In 2004, Snow, et al <ref type="bibr" coords="3,126.14,371.84,11.66,9.00" target="#b5">[7]</ref> described an interesting method for hypernym induction that was based on supervised machine learning using features derived from dependency parses. Once trained, the learner can be applied to unannotated corpora to identify new hypenym pairs. Snow provided us with his code, and we began investigating this technology and extending its scalability for application to our question-answering problem.</p><p>We used these two new datasets to augment the data that we previously had (and which we had been growing by hand throughout the year) to see if these approaches would yield system improvements. We created two variant taxonomic dictionaries of different sizes and plugged them into the existing system. In Table <ref type="table" coords="3,212.35,527.84,3.75,9.00" target="#tab_0">1</ref>, we illustrate the results of these variants as compared with our baseline system that makes use of largely hand-assembled data. (Note that the scores listed are the number of number one answers (#1) and the F-score for lists on past TREC sets with question identifiers in the specified ranges. In these evaluations, also, the system judgments are automatic, and the judgments do not count off for unsupportedness nor for temporal incorrectness. Moreover, the evaluation counts as correct any outputs which have exact answers embedded therein, but which may not truly be exact. For example, "Coach Harold Solomon" would be scored as correct even if the exact form should only be "Harold Solomon.") The table illustrates a disappointing result: that despite this interesting effort, the taxonomy-growing approach as it currently stands yielded slightly negative results for factoid answering and Variant1 actually yielded significant losses in list performance. Needless to say, we chose to not select these updated data sources for use in our actual evaluation systems. This will be a subject of study for the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">Longer Term Attempts</head><p>There were two other areas of research on QACTIS which we undertook with intentions of incorporating by the time of the evaluation but which required more effort than expected to make ready on time. These are mentioned only briefly for the sake of completeness. One of these areas dealt with morphological information and the other with multitiered processing.</p><p>With morphology, our system attempts to crudely generate directed conflation sets for all of the words of the question and for the documents which hopefully contain the answers. A number of questions have been answered incorrectly due to incorrect morphological information related often to word sense. We therefore began what turned out to be a significant effort to convert the way the system did morphological processing to one that would also make use of the part of speech in its stemming process. We hope that this information will strengthen the system at a later point even though it is currently not embedded in the processing.</p><p>Another effort which we were not able to finish attempted to convert QACTIS's current process from one that fuses all forms of annotation (named entity, parsing, etc.) from a single stream into one where each stream can be accessed independently. This would allow the system to derive an answer from one stream even if another stream would have yielded some other interpretation. This notion has potentially very positive gains, but we are currently at some distance away from knowing its longterm benefits.</p><p>Although the full-blown morphology revision was not incorporated by the time of evaluation, we were able to incorporate a weaker effort regarding morphology with regard to list-processing. The QACTIS system has been developed primarily with a focus on factoid answering, but morphological structure of questions was not well addressed for tackling lists. We therefore did work on beefing the system up in terms of its list-handling capability and, in the long run, this effort proved to be quite useful in that our list-answering capability on the whole improved substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieving Documents</head><p>Like many other question-answering systems, QACTIS begins its question-answering phase by first attempting to find documents on the same subject We had used the Lemur system <ref type="bibr" coords="4,115.24,184.84,10.62,9.00" target="#b6">[8]</ref>, version 2.2, since TREC13, and have found its results to be satisfactory. In fact, at TREC14, using this system out of the box yielded one of the top IR systems. We experimented with new versions of Lemur, but were not able to get any better results for QA.</p><p>Even still, when we look at the results of our question-answerer, we see that it has a less-than-perfect upper bound due to limitations in information retrieval. If we could enhance our ability to identify appropriate documents, we would likely have a higher performance and a higher upper bound on performance. We set out to improve our ability to preselect documents which would hopefully contain the desired answers. We experimented with two approaches. The first of these was an approach which identified key phrases from the question and tried to ensure that the returned documents actually contained those phrases. We will call this process phrase-based filtering of information retrieval. The second process used the Web as a mechanism for pseudo-relevance feedback. We discuss each of these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Phrase-based IR Filtering</head><p>By the time we had competed our system in the TREC-2005 competition, the base system as applied to one of the older TREC collections (the 1894 question series) was getting mean reciprocal ranks of about 40%. Upon examination of the documents returned by the IR component, it was discovered that a large number of irrelevant documents were being returned. One reason for this was that peoples' names, such as 'Virginia Woolf', were broken into two separate query terms. The resultant documents returned some that contained 'Virginia Woolf' as well as some that related to a 'Bob Woolf' who lived in 'Virginia'. A question pertaining to 'Ozzy Osborne' also returned a document containing a reference to a woman who owned a dog named Ozzy which bit a 'Mrs. Osborne' on the wrist.</p><p>Further analysis of the IR set showed that the top 10 documents for each question contained the correct answer 55% of the time; the top 30 documents contained the correct answer 67% of the time. (The numbers were determined by comparing the document list returned by the IR system to the list of correct documents for each question as provided by the TREC competition committee.) The IR system for the 2005 data set was much better--the top 10 documents contained the correct answer 70% of the time while the top 30 documents contained the correct answer 80% of the time.</p><p>A pseudo IR set was built for the 1894 series using the answer set provided by TREC -we referred to this set as a 'perfect IR' set. The number of correct answers provided by the base system when this data set was used was ~60% --an absolute gain of 20% just by removing irrelevant documents.</p><p>An additional step was added to the overall system that attempted to filter the IR using the named entities present in the question. This list also includes dates, titles, and anything in quotes. This process did provide an increase in scores as long as it was not overly aggressive in filtering out too many documents.</p><p>Further attempts were made to include multi-word terms and low-frequency words (words in the question which had a lower frequency of occurrence in the overall corpus) as filter terms, but there was not enough time to adequately analyze the effect. Additional parameters such as how many of the top 1000 documents should we examine, how many documents should we retain and how many documents from the original IR should we keep by default also had to be factored in to the result.</p><p>By the time of the TREC-2006 evaluation, it was determined that no more than the top 50 documents should be examined. There was no difference in our system in examining 50 or 75 documents. 100 documents degraded the overall system performance. There was also a significant boost in looking at 50 documents as opposed to just 30. Also, because of list questions, it was determined arbitrarily that at least 10 documents should be retained. Since our IR system showed the top 5 docs for each question to be relevant about 60% of the time, we decided to keep the top 5 documents as a matter of course.</p><p>We ran our phrase-based filtering on all of the collections at our disposal on the day before the TREC-2006 evaluation. Table <ref type="table" coords="4,383.95,527.84,5.00,9.00" target="#tab_1">2</ref> illustrates these results (whose scoring follows the paradigm mentioned in Table <ref type="table" coords="4,494.77,539.84,3.61,9.00" target="#tab_0">1</ref>). As can be seen, this approach affords small (2.6% relative) but positive improvements in the overall system performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Google-Enhanced Information Retrieval</head><p>The second approach to prefiltering was a multistep technique that took advantage of the Internet without actually trying to mine answers from it. This is a process which apparently has been used by other TREC-QA participants in past years. To improve our document retrieval phase, we used Google to select augmentation terms for each question. Each question in the test set was converted to a Google query and submitted to Google using the Google API. The top 80 unique snippets returned for each question were used as the augmentation collection. Given a question, we counted the number of times each snippet word co-occurred with a question word in the snippet sentence. These sums were multiplied by the inverse document frequency of the term; document frequencies were calculated from the AQUAINT collection. The resulting scores were used to rank the co-occurring terms. The top eight terms were selected as augmentation terms, and were added to the original query with weight 1/3. The resulting queries were then used for document retrieval.</p><p>In selecting these parameters, we were faced with many parameter choices for definition of co-occurrence, term weighting, etc. With over a thousand combinations to choose from, it was not practical to run full questionanswering tests on each one to select the best. Instead, we used a proxy for question answering performance: the number of words that occur in question answers that were selected as expansion terms by the method. We mined the answer patterns from past TREC QA tracks for words. Each time a method selected one of these words as an expansion term for the training collection, it was given a point. We used the highest scoring method in our TREC-2006 run.</p><p>In our developments with this process, we were quite excited about the gains we were seeing with it. We experimented with five different configurations of this process and one process (S2) yielded particularly successful results. In Table <ref type="table" coords="5,127.82,707.84,3.75,9.00" target="#tab_2">3</ref>, as can be seen from the three most recent years of TREC, as compared to the baseline, the S2 system in preliminary tests yielded a 6.4% relative improvement in factoid performance and a 4.1% relative improvement in list performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">A Word about Coupled Retrieval</head><p>One last experiment we tried was the coupling of these two prefilters. The hope was that if each could give an improvement, then perhaps in combination the improvement would increase. Unfortunately, this was not the case. It turns out that the approaches are somewhat at odds with each other. The phrase-filtering approach attempts to ensure that only documents that contain some or all of the important question phrases should be retained, while the Google-assisted approach attempts to look for documents that might have terminology that was not in the original question. If one applies a phrase-based filtering system to documents that have been obtained by the Google-assisted process, the likelihood is that even fewer of the documents than before will actually have the appropriate terminology. We tried several other variations on this theme after the actual TREC submission, but no combination yielded an improvement in overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Beginning to Incorporate Statistical QA</head><p>In the past few TREC evaluations, there has been an emergence of statistical QA systems which have the property that they learn relations between the posed question, the answer passage, and the actual answer. Then, when a user poses a future question, various answer passages are evaluated using statistical or machine-learning processes to determine how likely they are to contain a needed answer. As a final step, the system must distill out the answer from the existing passage. Statistical learners are particularly appealing in that they hold potential capability of developing language-independent QA. For such capability, one need only provide question-answer pairs for training.</p><p>We began in 2005 to develop a statistical QA system. The infrastructure for this system was in place by the time of the TREC-2006 evaluation and the system had begun to be taught how to automatically answer a limited number of questions, so we thought we would couple it with the existing system and allow it to answer those few question structures that it was equipped to address. Since this system is new and emerging, we provide a bit more information about the process and ways that we attempted to exploit the process during 2006.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">From Document Selection to Passage Selection</head><p>The first step of the process of developing a statistical system was to move from mere document selection to some form of passage selection. The first 45 documents reported from the Lemur IR system were screened to identify sentences (and sometimes surrounding sentences) which reflected the information from the important noun words of the question. The first 80 sentences that satisfied the criteria were retained and the sentence selection process was terminated. The 45-document and 80-sentence limits were determined to be empirically optimal threshold.</p><p>The next goal was to order these sentences by their potential for answering the question. The reordering component was based on support vector machines (SVMs) . The reordering effort was treated as a two-way classification problem --a customary domain for SVMs. The classifier was based on 26-dimensional feature vectors that were drawn from the data. Examples of these features were: (a) reporting 1.0 if the direct object from the question was in the putative answer sentence and 0.25 otherwise; (b) reporting 1.0 if the direct object from the question was missing but a WordNet synonym is in the putative answer sentence, and 0.25 otherwise; (c) reporting the ratio of question words to putative answer sentence words; and so forth. The classifier was then presented with positive examples of feature vectors drawn from actual answer sentences of past-year TRECs and it was also presented with a comparable number of negative examples drawn from bogus sentences.</p><p>From these training examples, the system was taught with quite good accuracy to learn the difference between good question-answering sentences and poor ones. In fact, for questions where the initial IR actually captured relevant documents, the percentage of true answer sentences identified by the SVM on a held out TREC QA collection was: 30% in the top-1 sentence, 43% in the top-2 sentences, 59% in the top-5 sentences, 74% in the top-10 sentences, and 80% in the top-15 sentences. It seemed highly likely that this process could afford a dramatic improvement in overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Pulling out the Answers from the Sentences</head><p>The next issue was to extract the answer from the answer sentences. One strategy was to insert these sentences into the existing question-answerer and hope it could distill out the answer. This process yielded a 20% relative degradation in performance due largely to the fact that the current system requires itself to find all relevant components of questions, whereas the best sentence may have the answer but not all relevant question components (needed for supportedness). Although we will attempt to modify the base system during the remainder of 2006 and 2007 to tackle the problem, there was also a desire to get a fully-statistical QA system.</p><p>We have yet to develop a full statistical learning process for finding answers, but we did begin a simple and potentially language-independent process for answering the questions. Using a copy of Wikipedia, we first used named-entity matching and part of speech tagging to see if we could draw out an answer directly from the Wiki pages. Barring this, we looked for redundant but normally rare information from the SVM sentences and, if it existed, this information was returned as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM EVALUATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description of Results</head><p>In TREC-2006, we submitted three runs from among the various configurations at our disposal. All of our runs used the same "Other" processing as in TREC-2005 (except that this system was slightly more robust to failure than last year). Also, in each situation, the system reported the top 20 answers from our factoid system as the "list" response. In terms of factoids, the first of these runs made use of our base engine but its information retrieval phase was prefiltered using phrase-based filtering as mentioned before. The second system replaced phrasebased filtering with our Google-enhanced information retrieval efforts. The third system was the same as the second but whenever the statistical system was deemed itself able to answer the question, it would supplant the original answer with its own. Since the statistical system is in its infancy, there were very few answers that it actually supplanted.</p><p>The results of these runs are detailed in Table <ref type="table" coords="6,532.50,400.84,3.75,9.00" target="#tab_3">4</ref>. Under "Factoid," the number of correct answers is listed and is followed by the triple (unsupported,inexact,temporally-incorrect) and by the fraction of first place answers. Under the "List" and "Other" scores are the NISTreported F-scores. The "All" category is the average of the three preceding columns, which represents the official NIST score. To our surprise, none of these variations provided significantly different results in the "All" category. However, it seems clear that factoids were negatively impacted by Google-enhanced IR as compared to phrasebased filtering, and the opposite is true for Lists. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison to Past Years</head><p>With these scores seeming to be only marginally better than they were last year, we wanted to determine if there had actually been any true system improvements since last year. We were able to identify our best competition systems from 2004 and 2005 and conduct a small experiment to test for system improvements by applying these past systems to this year's data. Our experiments would solely focus on factoids and we would not identify questions for which an assessor, had he or she seen the output of the older system, may have judged an answer as correct. Likewise, whereas we had some parsing problems in years past, we would allow the system to directly access the new and updated parses of today (since the broken and/or empty parses have long since been removed). It was our expectation that these two oversights would likely balance each other and provide a fairly accurate comparison of past year performance to that of the current year. Additionally, since past-year systems were not concerned with temporally inaccurate answers, and since "unsupported" is difficult to truly judge without the input of an assessor, we scored the three systems by allowing what TREC-2006 assessors had declared to be "Right," "Locally Correct", and "Correct but Unsupported. <ref type="bibr" coords="7,258.56,347.84,4.45,9.00">"</ref> The following table provides the performance comparisons.</p><p>Table <ref type="table" coords="7,96.36,514.84,5.00,9.00" target="#tab_4">5</ref> shows a gratifying result. From the first three rows we see that there have been reasonable improvements to our system over the course of the past two years.</p><p>The last row of Table <ref type="table" coords="7,173.44,550.84,5.00,9.00" target="#tab_4">5</ref> is merely informational. Since we were only able to submit three runs to TREC, we were not able to determine the impact of our basic system improvements as opposed to those that were coupled with IR improvements. With the incorporation of the last row, we are able to see that the basic system improvements contributed to about 21 more right answers and that IR contributed to 6 beyond that. We did not run the experiment of enhanced IR without the basic additions to the system, but while we were originally developing the algorithms, this paradigm was tested and it was typically the case that IR improvements and basic improvements had 1-3 correct answers in common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Considering the "Other"s</head><p>At TREC-2005, the F=0.248 that our system generated was the maximum score for any of the "Other" question answerers. This year, our F-score dropped by an absolute 10% and our position fell to just above the median. If we had made changes to our "Other" answerer, we would have believed that we had simply put forth a poor effort in making changes. On the other hand, since the only change we made was to add a stage which would thrice ensure against empty answers, we had to seek to understand why the performance would have fallen as it had.</p><p>At TREC-2004, our first year of participation, our system received a very high score and only 1/4 of the answers were given zero credit (with half of these due to non-responsiveness of our system). In this year, though, half of our answers were given no credit even though our method for "Other"-answering is sort of a "kitchen sink" approach which reports tons of information. We therefore reviewed the first ten of these zero-scored answers to see what would have changed.</p><p>Reviewing the "Other" questions is a non-trivial task. The core issue with these is not knowing how the determination is made as to whether something is vital or not. It appeared this year that since there were so many questions asked from each series, the "Other" questions had little information to choose from that was both novel and vital. Even so, there are three situations that arise in giving a system a zero score in such an the evaluation: (a) the QA system did not return items that assessors found to be valuable, (b) the QA system did return such items and received no credit, and (c) the QA system produced items that these assessors deemed to be non-vital but other assessors might have been perceived of as nuggets. Since the task is subjective, an evaluation of "c" is not a particularly helpful direction to study. Yet we will touch briefly on the first two of these given our in-depth review of the ten zero-scored answers that we studied. (It should be noted for reference, though, that there is some subjectivity which is inconsistent: in 164.8, credit is given for the fact that Judi Dench was awarded Order of the British Empire, but credit was not given in 153.8 for Alfred Hitchcock's receiving of higher honors as a Knight Commander.)</p><p>By far the biggest problem for us with category "a" above was that what were being deemed as nuggets this year were largely less-important pieces of information which surfaced to the top as vitals because more interesting information was posed as questions. If one were to ask a system: "Tell me everything interesting and unique you can about Warren Moon" (Q141.8), one would expect to receive information about his life profile: birth, death (if appropriate), his profession, and other major successes. Since the Q141 series asks about his position on a football team, the college where he played ball, his birth year, his time as a pro bowler, his coaches, and the teams Further reviewing in the "a" arena, we noted that vital nuggets for 152.7 (Mozart) appeared in our 17th and 60th documents; a vital for 163.8 (Hermitage) was not in our top 100; the sole vital for 164.8 was in 47th place; the two vitals from 175.5 were in 18th place and non-top-100; and so forth. The absence of such information in the higher-IR documents was obviously a leading contributor to our reduced scores. In past years, there were fewer questions asked per series, and many of those questions did not focus so much on key events of people and organizations but were focused more on exercising the QA systems. These facts seem to be the primary reason why our "Other" results would be so drastically degraded.</p><p>However, there are a few instances of "b" occurring as well. That is, our system reported vital information that was overlooked. In 163.8, our system reported without credit that the Hermitage was "comparable to the Louvre in Paris or the Metropolitan Museum in New York," which was a specified vital nugget. Such issues are less frequent, though, and they are not unexpected given that our system reports tons of information as answers. Furthermore, if the answers we perused are indicative, the issue of vitals not receiving credit would probably contribute less that .05 absolute to the current F-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unsupportedness</head><p>As mentioned previously, the large number of answers from our system that assessors were tagging as "unsupported" seemed somewhat suspicious to us given that our system does not draw its answers from the Web. We sought to review the answers being proposed by our system and determine what the unsupported issues were.</p><p>First, based on the cumulative information provided for all 59 competing system runs, we were able to determine that the average run had 12 answers that were declared to be inexact. We looked at our highest-scored factoid run and noted that we had 10 apparently unsupported answers. Although 10 was less than the average, we still wanted to understand the issues. We reviewed each answer and found that all the answers were indeed unsupported (and possible inexact as well). The table below summarizes this information: the question number (QID), the answer our system reported, and the reason why the answer was unsupported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FUTURE DIRECTIONS</head><p>The future of QACTIS still holds a direction of multilingual and multimedia question-answering as a primary goal. Yet we anticipate future participation in TREC next year until we have ironed out the wrinkles in our system. Our focus on textual QA for the next year will be to address the issues that have yet to be completed but what were mentioned in this paper, such as improvements and exactness filtering using more modern content extractors, better incorporation of hypernyms, and making improvements to our statistical QA system. We also plan to make our baseline system cleaner and more robust.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,309.34,141.84,220.15,118.00"><head>Table 1 : Wikipedia Ingestion</head><label>1</label><figDesc></figDesc><table coords="3,309.34,160.84,220.15,99.00"><row><cell>QA Set</cell><cell cols="2">Baseline #1 List</cell><cell cols="2">Variant1 #1 List</cell><cell cols="2">Variant2 #1 List</cell></row><row><cell>201-700</cell><cell>165</cell><cell></cell><cell>164</cell><cell></cell><cell></cell><cell></cell></row><row><cell>894-1393</cell><cell>116</cell><cell></cell><cell>113</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1394-1893</cell><cell>133</cell><cell></cell><cell>133</cell><cell></cell><cell>133</cell><cell></cell></row><row><cell>1894-2393</cell><cell cols="6">154 .165 152 .137 152 .169</cell></row><row><cell>1.1-65.5</cell><cell>72</cell><cell>.197</cell><cell>71</cell><cell>.130</cell><cell>71</cell><cell>.188</cell></row><row><cell>66.1-140.5</cell><cell cols="6">121 .163 124 .109 121 .157</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,309.34,597.84,226.90,118.00"><head>Table 2 : Filtered IR DevSet Improvements</head><label>2</label><figDesc></figDesc><table coords="4,309.34,616.84,226.90,99.00"><row><cell>QA Set</cell><cell>Baseline #1 Mrr List</cell><cell cols="2">w/ Filtered IR Diff in #1s</cell></row><row><cell>201-700</cell><cell>165 .432</cell><cell>172 .446</cell><cell>+7</cell></row><row><cell>894-1393</cell><cell>116 .351</cell><cell>118 .348</cell><cell>+2</cell></row><row><cell cols="2">1394-1893 133 .393</cell><cell>138 .402</cell><cell>+5</cell></row><row><cell cols="3">1894-2393 154 .431 .165 158 .444 .176</cell><cell>+4</cell></row><row><cell>1.1-65.5</cell><cell cols="2">72 .395 .197 71 .402 .197</cell><cell>-1</cell></row><row><cell cols="3">66.1-140.5 121 .445 .163 127 .456 .162</cell><cell>+6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,55.00,489.84,225.55,154.00"><head>Table 3 : Google-Enhanced Improvements</head><label>3</label><figDesc></figDesc><table coords="5,55.00,508.84,225.55,135.00"><row><cell>QA Set</cell><cell></cell><cell>BL</cell><cell cols="4">Google-Enhanced TF LS LT S</cell><cell>S2</cell></row><row><cell></cell><cell cols="7">#1 154 156 130 156 152 157</cell></row><row><cell>1894-2393</cell><cell cols="7">Mrr .431 .455 .377 .455 .442 .462</cell></row><row><cell></cell><cell cols="7">List .165 .191 .148 .192 .179 .191</cell></row><row><cell></cell><cell>#1</cell><cell>72</cell><cell>76</cell><cell>80</cell><cell>81</cell><cell>76</cell><cell>79</cell></row><row><cell>1.1-65.5</cell><cell cols="6">Mrr .395 .434 .405 .443 .434</cell></row><row><cell></cell><cell cols="7">List .191 .176 .179 .182 .189 .195</cell></row><row><cell></cell><cell cols="7">#1 121 126 116 123 122 130</cell></row><row><cell>66.1-140.5</cell><cell cols="7">Mrr .445 .464 .473 .458 .463 .473</cell></row><row><cell></cell><cell cols="7">List .163 .158 .179 .159 .163 .168</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,310.97,554.84,220.95,161.00"><head>Table 4 : TREC 2006 Performance</head><label>4</label><figDesc></figDesc><table coords="6,310.97,575.84,220.95,140.00"><row><cell>Strategy</cell><cell>Factoid</cell><cell>List Other All</cell></row><row><cell>Phrase-filtered IR</cell><cell>107</cell><cell>.147 .148 .185</cell></row><row><cell>+ improved QA</cell><cell>(10/20/5)</cell><cell></cell></row><row><cell>[#1]</cell><cell>.266</cell><cell></cell></row><row><cell>Google-enhanced</cell><cell>95</cell><cell>.156 .151 .181</cell></row><row><cell>IR, improved QA</cell><cell>(14/22/4)</cell><cell></cell></row><row><cell>[#2]</cell><cell>.236</cell><cell></cell></row><row><cell>Google-enhanced</cell><cell>96</cell><cell>.156 .154 .183</cell></row><row><cell>IR, improved QA,</cell><cell>(14/22/4)</cell><cell></cell></row><row><cell>some statistical</cell><cell>.238</cell><cell></cell></row><row><cell>QA [#3]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,63.83,393.84,211.60,104.00"><head>Table 5 : TREC 2006 vs. Past Years</head><label>5</label><figDesc>played, the remaining relevant information must address his successes and possibly his death. Since he has not died, only successes remain. Thus, the fact that he had the third all-time highest career passing yards, and a 15-year football career are note-worthy. Even so, our IR prefilter rejected one of the documents containing one of these items, and the other item appeared in a 17th-place document ... deeper than our Other processing typically goes.</figDesc><table coords="7,63.83,414.84,211.60,83.00"><row><cell>TREC Year</cell><cell>Factoid #R+L+U/"Correct" Score</cell></row><row><cell>TREC-2006 System</cell><cell>122 / .303</cell></row><row><cell>TREC-2005 System</cell><cell>95 / .236</cell></row><row><cell>TREC-2004 System</cell><cell>53/ .132</cell></row><row><cell>(TREC-2006 w/o Filter)</cell><cell>(116 / .288)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,310.97,93.84,226.12,554.00"><head>Table 6 : Unsupported Answers: Why?</head><label>6</label><figDesc>Needed a city and state of company's origin. The answer is inexact, too, but this document only says "based in," not originated in.</figDesc><table coords="8,310.97,114.84,226.12,533.00"><row><cell>QID</cell><cell>Our Answer [Document]</cell><cell>Reason Unsupported</cell></row><row><cell cols="2">145.4 february 23,</cell><cell>Needed a conviction date. Docu-</cell></row><row><cell></cell><cell>1999</cell><cell>ment dated 3/2/1999 refers to "last</cell></row><row><cell></cell><cell>[NYT19990</cell><cell>week" which is ambiguous.</cell></row><row><cell></cell><cell>302.0069]</cell><cell></cell></row><row><cell>154.4</cell><cell>Margot</cell><cell>Needed most-frequent actress in</cell></row><row><cell></cell><cell>Kidder</cell><cell>Superman. Document states that</cell></row><row><cell></cell><cell>[APW19981</cell><cell>Kidder starred with Reeve, but</cell></row><row><cell></cell><cell>213.1025]</cell><cell>nothing about "most"</cell></row><row><cell cols="2">172.1 Burlington</cell><cell></cell></row><row><cell></cell><cell>[NYT20000</cell><cell></cell></row><row><cell></cell><cell>124.0364]</cell><cell></cell></row><row><cell>182.5</cell><cell>Scotland</cell><cell>Needed country of Edinburg</cell></row><row><cell></cell><cell>[APW19990</cell><cell>Fringe. Document discusses poli-</cell></row><row><cell></cell><cell>506.0176]</cell><cell>tics in Scotland and a "fringe</cell></row><row><cell></cell><cell></cell><cell>party" --polysemy problem.</cell></row><row><cell cols="2">188.1 California</cell><cell>Needed US state with highest avo-</cell></row><row><cell></cell><cell>[APW19990</cell><cell>cado production. California is</cell></row><row><cell></cell><cell>117.0079]</cell><cell>only mentioned in passing and</cell></row><row><cell></cell><cell></cell><cell>nothing mentions production rates.</cell></row><row><cell cols="2">189.7 Edinburgh</cell><cell>Needed city of JK Rowling in</cell></row><row><cell></cell><cell>[NYT20000</cell><cell>2000. Document states that she</cell></row><row><cell></cell><cell>112.0203]</cell><cell>lived in Edinburgh in 1993.</cell></row><row><cell></cell><cell></cell><cell>Unclear if she lived there in 2000.</cell></row><row><cell>190.1</cell><cell>PITTS-</cell><cell>Needed city of HJ Heinz. The</cell></row><row><cell></cell><cell>BURGH</cell><cell>byline gives Pittsburgh and dis-</cell></row><row><cell></cell><cell>[APW19990</cell><cell>cusses company profits, but does</cell></row><row><cell></cell><cell>615.0036]</cell><cell>not explicitly say its base as there.</cell></row><row><cell cols="2">191.3 Germany</cell><cell>Needed country that won first four</cell></row><row><cell></cell><cell>[XIE199906</cell><cell>IRFR World Cups. Document</cell></row><row><cell></cell><cell>20.0031]</cell><cell>mentions "Germany" and "four"</cell></row><row><cell></cell><cell></cell><cell>but not that Germany won 4 times.</cell></row><row><cell>194.3</cell><cell>six</cell><cell>Need number of players at 1996</cell></row><row><cell></cell><cell>[XIE199603</cell><cell>World Chess Super Tournament.</cell></row><row><cell></cell><cell>20.0094]</cell><cell>Document mentions 6 players, but</cell></row><row><cell></cell><cell></cell><cell>wrong tournament and game.</cell></row><row><cell>214.6</cell><cell>seven</cell><cell>Need number of Miss America</cell></row><row><cell></cell><cell>[NYT20000</cell><cell>pageant judges. Document is on</cell></row><row><cell></cell><cell>717.0370]</cell><cell>Miss Texas Scholastic Pageant</cell></row><row><cell></cell><cell></cell><cell>which had 7 judges.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.71,205.84,214.95,9.00;9,71.14,217.84,214.52,9.00;9,71.14,229.84,214.52,9.00;9,71.14,241.84,214.52,9.00;9,71.14,253.84,90.83,9.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,125.24,217.84,160.42,9.00;9,71.14,229.84,25.56,9.00">Question Answering with QACTIS at TREC</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ciany</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,132.28,229.84,153.38,9.00;9,71.14,241.84,52.19,9.00">The 13th Text Retrieval Conference (TREC-2004)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,68.74,265.84,216.92,9.00;9,71.14,277.84,214.52,9.00;9,71.14,289.84,214.52,9.00;9,71.14,301.84,212.09,9.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,180.97,277.84,104.69,9.00;9,71.14,289.84,106.81,9.00">QACTIS-based Question Answering at TREC-2005</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ciany</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">.</forename><surname>Cutts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,187.13,289.84,98.53,9.00;9,71.14,301.84,101.80,9.00">The 14th Text Retrieval Conference (TREC-2005)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,68.16,316.50,56.96,9.00;9,125.12,313.87,12.00,7.20;9,139.62,316.50,29.16,9.00" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Bbn's Serif Tm Engine</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,68.02,328.50,217.64,9.00;9,71.14,340.50,214.52,9.00;9,71.14,352.50,214.52,9.00;9,71.14,364.50,22.50,9.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,129.79,340.50,147.74,9.00">WordNet: An online lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,71.14,352.50,153.94,9.00">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,68.12,376.50,217.54,9.00;9,71.14,388.50,214.52,9.00;9,71.14,400.50,214.52,9.00;9,71.14,412.50,58.33,9.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,268.12,376.50,17.54,9.00;9,71.14,388.50,214.52,9.00;9,71.14,400.50,37.21,9.00">Text Retrieval via Semantic Forests</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,123.26,400.50,105.37,9.00">NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="761" to="773" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>TREC-6, Gaithersburg, MD</note>
</biblStruct>

<biblStruct coords="9,68.88,436.50,216.78,9.00;9,71.14,448.50,214.57,9.00;9,71.14,460.50,46.11,9.00" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,229.51,436.50,56.15,9.00;9,71.14,448.50,206.36,9.00">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.63,472.50,104.45,9.00;9,203.02,472.50,82.64,9.00" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Lemur</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>System</surname></persName>
		</author>
		<ptr target="http://www-" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
