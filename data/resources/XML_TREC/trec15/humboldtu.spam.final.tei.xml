<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.04,139.01,319.98,15.55">Highly Scalable Discriminative Spam Filtering</title>
				<funder ref="#_2kWHvmF">
					<orgName type="full">STRATO AG</orgName>
				</funder>
				<funder>
					<orgName type="full">German Science Foundation DFG</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.24,171.54,87.84,10.87"><forename type="first">Michael</forename><surname>Brückner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Computer Science</orgName>
								<orgName type="institution">Humboldt Universität zu Berlin</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country>Germany, Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.56,171.54,63.08,10.87"><forename type="first">Peter</forename><surname>Haider</surname></persName>
							<email>haider@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Computer Science</orgName>
								<orgName type="institution">Humboldt Universität zu Berlin</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country>Germany, Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.73,171.54,76.91,10.87"><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
							<email>scheffer@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Computer Science</orgName>
								<orgName type="institution">Humboldt Universität zu Berlin</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country>Germany, Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.04,139.01,319.98,15.55">Highly Scalable Discriminative Spam Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9CD1F5B970DAEF7740E1563AAFF7CE98</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses several lessons learned from the SpamTREC 2006 challenge. We discuss issues related to decoding, preprocessing, and tokenization of email messages. Using the Winnow algorithm with orthogonal sparse bigram features, we construct an efficient, highly scalable incremental classifier, trained to maximize a discriminative optimization criterion. The algorithm easily scales to millions of training messages and millions of features. We address the composition of training corpora and discuss experiments that guide the construction of our SpamTREC entry. We describe our submission for the filtering tasks with periodical re-training and active learning strategies, and report on the evaluation on the publicly available corpora.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spam filtering remains a technological challenge; the commercial incentive for spam senders results in an arms race between filtering methods and spam obfuscation techniques. Naive Bayes <ref type="bibr" coords="1,129.23,432.65,10.57,9.96" target="#b4">[5,</ref><ref type="bibr" coords="1,144.71,432.65,7.81,9.96" target="#b6">7]</ref> and rule based learners <ref type="bibr" coords="1,268.21,432.65,10.45,9.96" target="#b1">[2]</ref> have been very popular; discriminative approaches like Support Vector Machines, Logistic Regression as well as Maximum Entropy have also been studied for spam filtering. Most discriminatively trained methods are non-incremental and often scale poorly to large training samples. Regular updates of the classifier which are necessary due to topic drift and the adversarial nature of spam, are consequently costly or even infeasible.</p><p>P-norm algorithms such as Winnow <ref type="bibr" coords="1,272.18,492.41,10.45,9.96" target="#b8">[9,</ref><ref type="bibr" coords="1,285.74,492.41,12.73,9.96" target="#b9">10]</ref> are incremental and can be implemented to scale to large amounts of training data. These methods have proven to be very robust <ref type="bibr" coords="1,453.01,504.41,10.45,9.96" target="#b3">[4]</ref> and highly scalable in practice. A number of alternative approaches such as network-based spam detection, collaborative filtering strategies, or email batch detection using graph theoretical methods have been studied but cannot be applied to the SpamTREC challenge because of the nature of the data that is provided.</p><p>In this paper, we address challenging issues for the construction of practical filtering systems. We discuss how a large-scale filter can be trained using a discriminative optimization criterion. We address preprocessing, decoding, and tokenization issues, and questions regarding the assembly of training corpora. We report on experiments that guide the construction of our filtering system. We discuss the evaluation of this system on the public SpamTREC corpora and conclude with a number of lessons learned.</p><p>The rest of this paper is structured as follows. We address challenges for practical spam filters in Section 2 and report on our experiments. Section 3 describes the system that we used in the competition; Section 4 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Challenges in Email Classification</head><p>In this section, we address design issues for practical spam filtering systems and report on experiments that guide the construction of a system that performs well for the SpamTREC challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scalable Discriminative Training</head><p>Text classification requires highly scalable methods as it involves large amounts of high dimensional data. Winnow, a perceptron-like algorithm with multiplicative updates <ref type="bibr" coords="2,440.89,205.73,10.00,9.96" target="#b8">[9]</ref>, can handle a huge amount of data very efficiently, outperforming other well-established filters such as Naive Bayes at the same time <ref type="bibr" coords="2,211.41,229.61,14.60,9.96" target="#b9">[10]</ref>. However, practical difficulties and implementation issues still remain.</p><p>Beyond preprocessing and tokenization, an efficient implementation has to compute the features -hash values of the parsed tokens or N-grams -and spam scores on the fly. Computation of the score involves huge hash structures with several million buckets. Our implementation is an efficient version of Winnow which scales to large corpora and feature vectors. A related version of our filtering system is used by a commercial webspace and email service provider and filters about 35 million email messages per day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing and Tokenization</head><p>Encoding schemes provide spam senders with a rich set of tools to obfuscate tokens. For example, the word "café" would be HTML-encoded as "caf&amp;eacute;", URL-encoded as "caf=E9", or base64-encoded as "Y2Fm6Q==". In order to avoid an inflation of the attribute space, it is highly desirable to map all these representations to the same token. This can be achieved by transforming tokens into a canonical encoding scheme, such as UTF-8.</p><p>Our spam filtering system includes modules for a variety of different preprocessing measures. They incorporate procedures to analyze the structure, and conformity to several standards, of each email and its parts. They include procedures that transform the email contents into the canonical UTF-8 encoding scheme. These transformation steps result in a representation of the message text that is independent of the particular encapsulation method used.</p><p>The conformity checks and the attachment dissection provide additional features that can be used for classification. For example, spammers often forge the date of the email, such that the message appears user as the most recent item the user's inbox for a long time.</p><p>In detail, prior to tokenization each email is subjected to the following actions:</p><p>• parsing structure of MIME-parts;</p><p>• decoding MIME-part contents (e.g., base64 or "quoted/printable" decoding);</p><p>• transforming the character set into the UTF-8 encoding scheme;</p><p>• decoding the subject-string according to RFC2947 and RFC2231;</p><p>• transforming HTML-or URL-encoded characters into UTF-8;</p><p>• plausibility checks of the "received" time according to RFC2822;</p><p>• extraction of language information based on used character sets;</p><p>• extraction of information about attachment types;</p><p>• checking of standard conformity of MIME structure and attached files.</p><p>We conduct a set of experiments in order to study the benefit of these pre-processing and feature extraction steps. First, we study the effectiveness of pre-processing and its contribution to the classification results. Therefore we train two classifiers using identical training sets containing 100,000 randomly selected emails from our English corpus (Table <ref type="table" coords="3,441.48,157.61,3.88,9.96" target="#tab_0">1</ref>). For the first classifier, the itemized preprocessing steps are carried out, whereas they are disabled for the second classifier. We use an evaluation set of again 100,000 emails from the same corpus. We observe an AUC performance of 0.999477 when preprocessing is employed and 0.999380 when it is disabled. That is, the preprocessing step decreases the risk (1 -AU C) by 16% from 0.00062 to 0.000523. In this experiment, clearly the accuracy is high for both classifiers. Nevertheless, a 16% reduction of the risk is a significant finding that emphasizes the importance of the preprocessing step.</p><p>Preprocessing has a noticeable effect on the subsequent feature generation step. Without preprocessing, trained on 100,000 English documents, the filter has 6 million features with nonzero weights. When preprocessing is employed, only 3 million features are used.</p><p>The stream of preprocessed characters has to be tokenized in a way that facilitates the extraction of discriminative attributes. Whereas tokenization can be based on whitespace characters and punctuation symbols for European languages, such whitespace characters are absent for many Asian languages such as Chinese, Japanese and Korean languages. For Asian languages, we treat each character as an individual token, resulting in syllabic tokens. The resulting loss of inter-syllabic context is compensated for by the use of N-gram features in the following feature extraction step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text Feature Extraction</head><p>From the stream of preprocessed tokens, features have to be extracted that provide the classification method with sufficiently discriminative information to allow for a highly accurate decision.</p><p>In many cases, contextual information that spans across multiple tokens hints at the semantics of sentences. This is particularly true for Asian languages for which words span across multiple tokens. In addition, it has become popular among spam senders to blur the bag-of-word-view of the messages by appending random sets of good words, individually drawn for each message.</p><p>Orthogonal sparse bigrams provide a mechanism for obtaining discriminative features <ref type="bibr" coords="3,494.52,667.25,14.60,9.96" target="#b9">[10]</ref>. Empirically, they have shown to be an effective and scalable mechanism to represent information contained in N adjacent tokens. A window of fixed width, in our case N = 5, slides over the sequence of tokens. For each window position, the set of all two-elementary combinations of the N tokens is generated. Each combination is constrained to always include the left-most token and zero or one other token; the remaining tokens in the window are replaced by a special skip symbol "•". The resulting N orthogonal sparse bigram combinations uniquely represent the content of the current window position. For example, the window containing the sequence "All medications at low price", is represented by the orthogonal sparse bigram features All , All medications , All • at , All •• low , and All • • • price . The combination of these five features represents the entire content of the window. Information is lost, however, when the orthogonal sparse bigram features of the entire message are pooled into a single feature vector. Thus, orthogonal sparse bigrams implement an appealing and empirically proven trade-off between scalability and representational richness. Table <ref type="table" coords="4,127.08,405.77,4.98,9.96" target="#tab_1">2</ref> displays the number of features that the filter employs, based on the training corpus. Between 600,000 and 8.6 million orthogonal sparse bigram features have nonzero weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Large Training Corpora</head><p>The high dimensionality of the feature representation and the required high accuracy of the resulting classifier call for an extremely large training corpus. In this section, we want to clarify to which extent the winnow algorithm can benefit from additional training data and which amount of data is tractable. Therefore we trained four classifiers on between 50,000 and 400,000 emails of the English corpus (Table <ref type="table" coords="4,258.38,511.73,3.88,9.96" target="#tab_0">1</ref>). Another 100,000 emails were kept for testing. Table <ref type="table" coords="4,507.84,511.73,4.98,9.96" target="#tab_2">3</ref> and Figure <ref type="figure" coords="4,151.46,523.73,4.98,9.96" target="#fig_0">1</ref> show the results of the experiments. They confirm the linear runtime behavior of Winnow as well as a significant improvement of accuracy when using larger training sets. It can be seen that the number of training examples has a much greater impact on classification performance than preprocessing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Differences in Distributions Underlying Training and Test Data</head><p>Most machine learning methods assume that training data be governed by the exact same distribution that the classifier is exposed to at application time. In the spam filtering application, control over the data generation process is less perfect. A number of public sources of spam messages and a much more limited number of sources of non-spam emails are available. On the other hand, American, Chinese, professional, recreational and many other groups of email users receive emails from a range of diverging distributions. Questions arise about the effect of such divergence between training and testing data on the classifier, and about the optimal way of dealing with this divergence. For a discussion of these generally under-studied questions, see <ref type="bibr" coords="5,99.24,406.97,9.91,9.96" target="#b0">[1]</ref>.</p><p>It is relatively easy to identify the language used in a message. We study the impact that training a classifier on an English, Chinese, or mixed corpus will have on its performance on English, Chinese, or mixed testing data. Our goal is to obtain guidance on the optimal training corpus for a classifier that may be exposed to additional, unforeseen languages. We would also like to know whether it is advisable to use a joint classifier, or separate classifiers that perform well for only one language.</p><p>We perform several experiments using a Chinese and an English email corpus containing 200,000 emails each. After splitting the data in 50% training and 50% test data we train three classifiers using both data sets and a mixed corpus. The experimental results, given in Table <ref type="table" coords="5,99.24,526.49,3.90,9.96" target="#tab_3">4</ref>, indicate that a classifier trained on emails of both languages performs very similar to the monolingually trained classifiers on each individual language.</p><p>Recently, the compensation of sample selection bias is being studied (e.g., <ref type="bibr" coords="5,442.33,550.49,10.31,9.96" target="#b0">[1]</ref>). In order to explicitly account for a divergence between training and testing data, the (unlabeled) testing data has to be available at training time. Unfortunately, this is not the case in the SpamTREC challenge.</p><p>From our experiments, we conclude that in the absence of unlabeled testing data, a classifier trained on a heterogeneously mixed corpus is generally preferable, because it does not lose accuracy on the individual languages while gaining additional robustness. We therefore compile a training corpus from maximally heterogeneous sources. It includes mailing lists, newsletters, several distinct spam traps, personal emails, moderated usenet groups, public email corpora such as the Enron dataset, and many more. For details, see Section 3.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TREC 2006 Spam Track Tasks</head><p>The TREC 2006 Spam Track consists of two different tasks. In the online filtering task, the spam filter classifies each message from the test corpus, and subsequently receives the true label of each message for training. There are two sub-tasks, the first with immediate feedback, and the second with delayed feedback; for the second sub-task, training is carried out after a randomly sized chunk of messages has been processed. In the active filtering task, the spam filter gets to choose a number of emails for which the label is then disclosed.</p><p>All tasks are carried out on four distinct evaluation corpora. There are two private corpora, tagged B2 and X2, one public corpus with English emails and one public corpus with Chinese messages. A summary of the corpus sizes is shown in Table <ref type="table" coords="6,361.77,321.05,3.90,9.96" target="#tab_4">5</ref>. The rules of the Spam Track allow each contestant to submit four different filter configurations for each of the two main tasks. In the following, we describe the configurations of our entries, and the results on the different evaluation corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Training</head><p>To take advantage of the capability of the winnow algorithm to handle large sets of known features and large training corpora, we assemble a corpus of emails to pre-train our filter prior to submission. Table <ref type="table" coords="6,192.00,531.53,4.98,9.96" target="#tab_5">6</ref> gives an overview of the sources of our training emails and their numbers of spam and non-spam emails.</p><p>Extensive pre-training imposes a risk if the chosen training data only poorly reflects the distribution at application time. Therefore we submit one weakly trained filter configuration for each task. For this, only a single iteration of the Winnow algorithm over the data is exercised, instead of re-iteration until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online Classification with Periodical Re-Training</head><p>The winnow algorithm performs best when the model is trained by iterating several times over all training emails. However, all TREC tasks are designed to expose the filter to each training email only once. To overcome this discrepancy, we experiment with several strategies to cache all seen training emails and re-train the classifier on them periodically. Our experiments on different corpora show a notable influence of the re-training strategy on overall accuracy, but no single strategy can outperform the others consistently. Therefore, we choose to vary the strategies over the four allowed submissions. We include two submissions where a re-training is executed after each misclassified email, one configuration with re-training after every 500th email, and one submission with no re-training at all. The results on the four evaluation datasets in Table <ref type="table" coords="7,347.30,346.01,4.98,9.96" target="#tab_6">7</ref> show that the periodical re-training is indeed successful in improving the performance in the online setting. On all but one dataset the configuration without re-training performs worst.</p><p>The private B2 corpus seems to be the most difficult dataset with the most mistakes made. Presumably this is due to the corpus differing strongly from our training data. This assumption is also consistent with the second configuration performing best on this dataset. The weak pre-training allows it to faster adapt to the different evaluation set. show learning curves for all participants of SpamTREC, aggregated over all (public and private) evaluation corpora. The data has kindly been provided Godon Cormak. For all participants, the best of all submitted filters was used. When few or no training examples have been provided, the AUC mesaure of our filter exceeds the AUC of other submitted systems in both cases. For larger training samples, the filter falls back behind other submissions. We believe that this behavior is at  least partly explained by the large-scale pre-training that we used for all of our submissionseven the weakly trained filter was pre-trained with many emails. Pre-training leads to a better performance from the start but slows down the adaptation to the evaluation corpora. A careful comparative evaluation of all submitted filter is provided by <ref type="bibr" coords="8,364.56,520.49,9.91,9.96" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Active Learning Task</head><p>In the Active Learning task, the spam filtering system has to decide which emails from a given set are to be labeled for training. There are various possible methods to select the next email. The idea behind most of them is to select those training emails which are expected to provide the most knowledge about the class labels of the test emails. One standard approach is uncertainty sampling <ref type="bibr" coords="8,142.93,614.57,10.00,9.96" target="#b7">[8]</ref>, where one selects the training item which lies closest to the current decision boundary. This captures the intuition that an update of the decision boundary with this item has the largest effect on the shape of the decision regions, and therefore the greatest knowledge gain.</p><p>Besides uncertainty sampling, we evaluate several ad-hoc strategies, such as selecting the email with the highest number of unknown features, or selecting the email with the best feature coverage over all available training mails. But our experiments show that none of them can outperform uncertainty sampling. Therefore, three of our four submissions for Task 2 use this standard method. The remaining, weakly trained configuration uses random sampling, because uncertainty sampling is unlikely to yield good results if the distance to the decision boundary depends only on very few update steps.</p><p>In contrast to our preliminary experiments, uncertainty sampling does not consistently outperform random sampling on the evaluation data of the competition. In combination with the periodical re-training strategy, the spam classifier tends to degrade to a highly imbalanced state, yielding almost useless spam scores. Apparently the selection of some disadvantageous training emails shifted the score of a large portion of spam emails after re-training far below the decision threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Private corpus B2</head><p>Private corpus X2  This problem does not occur with our two submitted configurations without pre-training, one with uncertainty sampling and the other with random sampling. In Figure <ref type="figure" coords="9,445.22,556.85,4.98,9.96" target="#fig_3">4</ref> one can again see that the B2 corpus has different characteristics than the others. As one would expect on an evaluation corpus which is highly different from the training corpus, the uncertainty sampling strategy performs worse than random sampling due to strong initial fluctuations of the decision boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The winnow classifier in conjunction with orthogonal sparse bigram features proves to be highly scalable and capable of efficiently handling, and benefiting from, hundreds of thousands of training messages and millions of features. Our experiments emphasize the importance of large, diverse training sets; accuracy and robustness still improve when the training data is already very large. The accuracy of the classifier is further improved, and the resulting dimensionality of the classifier reduced, by character set decoding and normalization.</p><p>For the incremental SpamTREC tasks, using the cache of all previously seen messages in each model update step outperforms an update based on just the newly seen mail. In both tasks, the weakly trained classifier managed to adapt faster to the apparently most difficult evaluation corpus, as we expected. We make an interesting observation with respect to the benefit of uncertainty sampling versus random sampling. Uncertainty sampling is beneficial for the public, but detrimental for the private corpora that apparently deviate from the training data more strongly. Negative results for uncertainty sampling are rare in the literature, possibly because usually the case of identically distributed training and testing data is studied.</p><p>Recently, image spam is challenging spam filters. Even state-of-the-art filters are nearly helpless as the number of image spam messages explodes. Visual data requires different feature extraction procedures; the efficiency of the processing steps is crucial.</p><p>It would be interesting for future SpamTREC competitions to include evaluation corpora with emails from many different users with complete, unmodified headers. This would permit non-text based approaches to spam filtering, such as classification based on social networks or information about the sending servers. To explore the benefits of collective classification approaches, an additional task could expose the spam filters to more than one email at once instead of one at a time, and thus allow the incorporation of relationships between each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,99.24,249.17,413.53,9.96;5,99.24,261.17,49.96,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plot of classifier quality (left) and training time (right) depending on the size of the training set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,115.44,265.37,381.13,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results of the evaluation aggregated over all corpora for immediate feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,121.92,452.81,368.18,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of the evaluation aggregated over all corpora for delayed feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,208.56,519.05,194.87,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results on the active learning task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,140.04,391.61,332.13,153.60"><head>Table 1 :</head><label>1</label><figDesc>Data used for experiments Source English docs Chinese docs CCERT Data Set (www.ccert.edu.cn/spam)</figDesc><table coords="3,378.23,415.61,93.90,9.96"><row><cell>10125</cell><cell>64015</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,159.72,108.65,293.01,129.72"><head>Table 2 :</head><label>2</label><figDesc>Dimensionality of the filter, based on training corpus</figDesc><table coords="4,159.72,120.75,293.01,117.62"><row><cell>Corpus</cell><cell>number of documents</cell><cell>features</cell></row><row><cell>English &amp; Chinese</cell><cell cols="2">200,000 3,737,556</cell></row><row><cell>English</cell><cell cols="2">400,000 6,264,128</cell></row><row><cell>English</cell><cell cols="2">200,000 4,477,527</cell></row><row><cell>English</cell><cell cols="2">50,000 2,246,516</cell></row><row><cell>English</cell><cell cols="2">100,000 3,207,268</cell></row><row><cell>English, no preprocessing</cell><cell cols="2">100,000 6,064,259</cell></row><row><cell>Chinese</cell><cell>100,000</cell><cell>656,150</cell></row><row><cell>TREC fully trained classifier</cell><cell cols="2">530,000 8,626,863</cell></row><row><cell>TREC weakly trained classifier</cell><cell cols="2">530,000 4,092,195</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,133.80,590.33,344.28,81.84"><head>Table 3 :</head><label>3</label><figDesc>Impact of training set size on classifier performance and training time</figDesc><table coords="4,138.60,602.43,333.12,69.74"><row><cell>Training set size</cell><cell>AUC</cell><cell>Max</cell><cell>Spam precision at</cell><cell>Training</cell></row><row><cell></cell><cell></cell><cell cols="3">f-measure 0.1% non-spam error time in s</cell></row><row><cell>50,000 emails</cell><cell cols="2">0.981703 0.925522</cell><cell>73.26 %</cell><cell>2,269</cell></row><row><cell>100,000 emails</cell><cell cols="2">0.987880 0.927897</cell><cell>82.17 %</cell><cell>4,658</cell></row><row><cell>200,000 emails</cell><cell cols="2">0.991310 0.932474</cell><cell>78.53 %</cell><cell>8,233</cell></row><row><cell>400,000 emails</cell><cell cols="2">0.994391 0.937107</cell><cell>84.25 %</cell><cell>19,284</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,149.88,108.65,311.56,71.16"><head>Table 4 :</head><label>4</label><figDesc>Performance of classifier depending on language</figDesc><table coords="6,149.88,120.41,311.56,59.40"><row><cell></cell><cell cols="2">Chinese test data</cell><cell cols="2">English test data</cell></row><row><cell>Training data</cell><cell>AUC</cell><cell>Max f-measure</cell><cell>AUC</cell><cell>Max f-measure</cell></row><row><cell>Chinese</cell><cell>0.999880</cell><cell>0.999314</cell><cell>0.852904</cell><cell>0.831978</cell></row><row><cell>English</cell><cell>0.986990</cell><cell>0.957835</cell><cell>0.999477</cell><cell>0.996775</cell></row><row><cell>Both</cell><cell>0.999884</cell><cell>0.998543</cell><cell>0.999464</cell><cell>0.996166</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,211.80,351.65,188.73,69.96"><head>Table 5 :</head><label>5</label><figDesc>Evaluation corpora</figDesc><table coords="6,211.80,363.75,188.73,57.86"><row><cell>Dataset</cell><cell cols="2">#Non-spams #Spams</cell></row><row><cell>B2 (private)</cell><cell>9274</cell><cell>2751</cell></row><row><cell>X2 (private)</cell><cell>9039</cell><cell>40135</cell></row><row><cell>English (public)</cell><cell>12910</cell><cell>24912</cell></row><row><cell>Chinese (public)</cell><cell>21766</cell><cell>42854</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,149.16,108.65,313.98,165.60"><head>Table 6 :</head><label>6</label><figDesc>Training corpus composition</figDesc><table coords="7,149.16,120.75,313.98,153.50"><row><cell>Source</cell><cell cols="2">#Non-spams #Spams</cell></row><row><cell>CCERT Data Set (www.ccert.edu.cn/spam)</cell><cell>9272</cell><cell>30390</cell></row><row><cell>Disclosed Enron mails [6]</cell><cell>14000</cell><cell>0</cell></row><row><cell>Guenter spam trap (untroubled.org/spam)</cell><cell>187</cell><cell>49983</cell></row><row><cell>IMC mailing lists (www.imc.org)</cell><cell>14000</cell><cell>0</cell></row><row><cell>Various newsletters</cell><cell>18000</cell><cell>0</cell></row><row><cell>Nazario corpus (monkey.org/∼jose/phishing)</cell><cell>0</cell><cell>1000</cell></row><row><cell>SpamArchive.org</cell><cell>671</cell><cell>29938</cell></row><row><cell>SpamAssassin collection</cell><cell>1994</cell><cell>6</cell></row><row><cell>TREC 2005 corpus</cell><cell>30000</cell><cell>0</cell></row><row><cell>Various moderated Usenet groups</cell><cell>80000</cell><cell>0</cell></row><row><cell>Wouters archive (www.xtdnet.nl/paul/spam)</cell><cell>23</cell><cell>5000</cell></row><row><cell>Own collection</cell><cell>35682</cell><cell>209854</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,99.24,448.97,413.53,181.32"><head>Table 7 :</head><label>7</label><figDesc>AUC results for the online filter tasks, on the two private corpora (B2, X2), the public English (E), and the public Chinese (C) corpus</figDesc><table coords="7,114.12,473.21,335.19,157.08"><row><cell>Number</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>Pre-Training</cell><cell>Full</cell><cell>Weak</cell><cell>Full</cell><cell>Full</cell></row><row><cell>Re-train after</cell><cell>Mistakes</cell><cell>Mistakes</cell><cell>Never</cell><cell>Every 500th</cell></row><row><cell>B2 immediate</cell><cell cols="3">0.994705 0.995724 0.993775</cell><cell>0.994223</cell></row><row><cell>B2 delayed</cell><cell cols="3">0.994221 0.994694 0.991896</cell><cell>0.993216</cell></row><row><cell cols="4">X2 immediate 0.998820 0.998550 0.997898</cell><cell>0.998615</cell></row><row><cell>X2 delayed</cell><cell cols="3">0.998641 0.998173 0.997706</cell><cell>0.998237</cell></row><row><cell>E immediate</cell><cell cols="3">0.998690 0.998306 0.998436</cell><cell>0.998671</cell></row><row><cell>E delayed</cell><cell cols="3">0.998582 0.997048 0.998042</cell><cell>0.997994</cell></row><row><cell>C immediate</cell><cell>0.999762</cell><cell cols="2">0.999727 0.999646</cell><cell>0.999767</cell></row><row><cell>C delayed</cell><cell cols="3">0.999681 0.999631 0.999505</cell><cell>0.999670</cell></row><row><cell cols="4">Figures 2 (for immediate feedback) and 3 (for delayed feedback)</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was supported by a Grant from <rs type="funder">STRATO AG</rs>. T.S. is supported by Grant <rs type="grantNumber">SCHE 540/10-2</rs> of the <rs type="funder">German Science Foundation DFG</rs>. We wish to thank <rs type="person">Gordon Cormack</rs> for his great support!</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2kWHvmF">
					<idno type="grant-number">SCHE 540/10-2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,119.64,453.89,393.20,9.96;10,119.64,465.89,344.65,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,273.24,453.89,239.61,9.96;10,119.64,465.89,15.56,9.96">Dirichlet-enhanced spam filtering based on biased samples</title>
		<author>
			<persName coords=""><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,156.00,466.23,225.01,9.18">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,119.64,485.81,393.07,9.96;10,119.64,497.69,274.57,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,211.22,485.81,225.20,9.96">Learning to classify English text with ILP methods</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,460.07,486.15,52.64,9.18;10,119.64,498.03,125.76,9.18">Advances in Inductive Logic Programming</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="124" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,119.64,517.61,393.18,9.96;10,119.64,529.61,119.29,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,207.61,517.61,139.35,9.96">Trec 2006 spam treck overview</title>
		<author>
			<persName coords=""><forename type="first">Gorgon</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,374.64,517.95,138.19,9.18;10,119.64,529.95,88.40,9.18">Proceedings of the TREC Text Retrieval Conference</title>
		<meeting>the TREC Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,119.64,549.53,393.42,9.96;10,119.64,561.53,44.05,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,195.74,549.53,177.81,9.96">The robustness of the p-norm algorithms</title>
		<author>
			<persName coords=""><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,382.68,549.87,76.73,9.18">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="299" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,119.64,581.45,393.36,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,184.34,581.45,103.04,9.96">Better bayesian filtering</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,306.60,581.79,175.88,9.18">Proceedings of the MIT Spam Conference</title>
		<meeting>the MIT Spam Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,119.65,601.37,393.37,9.96;10,119.64,613.25,393.22,9.96;10,119.64,625.25,299.77,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,263.79,601.37,249.22,9.96;10,119.64,613.25,34.03,9.96">The enron corpus: A new dataset for email classification research</title>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,177.60,613.59,295.36,9.18">Proccedings of the 15th European Conference on Machine Learning</title>
		<meeting>cedings of the 15th European Conference on Machine Learning<address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3201</biblScope>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,119.64,645.17,393.30,9.96;10,119.64,657.17,39.01,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,175.93,645.17,144.69,9.96">Generalized naive bayes classifiers</title>
		<author>
			<persName coords=""><forename type="first">Kim</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,327.24,645.51,143.13,9.18">SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="81" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,119.64,101.81,393.27,9.96;11,119.64,113.69,393.20,9.96;11,119.64,125.69,267.06,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,285.86,101.81,227.06,9.96;11,119.64,113.69,33.62,9.96">Heterogeneous uncertainty sampling for supervised learning</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Catlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,174.24,114.03,306.76,9.18">Proceedings of the 11th International Conference on Machine Learning</title>
		<meeting>the 11th International Conference on Machine Learning<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,119.64,145.61,393.22,9.96;11,119.64,157.61,262.10,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,204.36,145.61,308.50,9.96;11,119.64,157.61,84.33,9.96">Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Littlestone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,212.76,157.95,76.73,9.18">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="285" to="318" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,119.64,177.53,393.21,9.96;11,119.64,189.41,393.14,9.96;11,119.64,201.75,393.13,9.18;11,119.64,213.41,214.11,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,465.50,177.53,47.35,9.96;11,119.64,189.41,297.20,9.96">Combining winnow and orthogonal sparse bigrams for incremental spam filtering</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Siefkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fidelis</forename><surname>Assis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shalendra</forename><surname>Chhabra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Yerazunis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,435.84,189.75,76.94,9.18;11,119.64,201.75,388.57,9.18">Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="410" to="421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
