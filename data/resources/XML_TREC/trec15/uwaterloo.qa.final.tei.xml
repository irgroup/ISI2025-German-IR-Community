<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.32,75.33,321.35,12.64;1,165.60,91.41,281.05,12.64">Identifying Relationships Between Entities in Text for Complex Interactive Question Answering Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.56,132.17,87.53,10.80"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.84,132.17,105.60,10.80"><forename type="first">Murat</forename><surname>Karamuftuoglu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bilkent University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.32,75.33,321.35,12.64;1,165.60,91.41,281.05,12.64">Identifying Relationships Between Entities in Text for Complex Interactive Question Answering Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9685375682D36877AF75131B6A50C7CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe our participation in the Complex Interactive Question Answering (ciQA) task of the QA track. We investigated the use of lexical cohesive ties (called lexical bonds) between sentences containing different question entities in finding information about relationships between these entities. We also investigated the role of clarification forms in assisting the system in finding answers to complex questions. The rest of the paper is organised as follows: in section 2 we present our approach to calculating lexical bonds between sentences containing different entities, section 3 contains the detailed description of our systems, in section 4 we present the results, and section 5 contains discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relationships Between Entities in Text</head><p>We hypothesise that a lexical bond between two sentences can be used to predict whether they discuss the same topic. Since the task in ciQA is to find a piece of information (nugget) about a "relationship" between two entities, we propose to use lexical bonds in order to determine whether the contents of a sentence containing one entity is related to the contents of a sentence containing the second entity. In other words, by calculating lexical bonds we aim to find sentences that are likely to discuss something related to both entities.</p><p>In order to determine if two sentences have a lexical bond, we first need to identify lexical links that they have. According to <ref type="bibr" coords="1,192.84,405.36,52.04,8.96" target="#b1">Hoey (1991)</ref> a lexical link exists between two words in text that are related by means of one of the following: (1) simple lexical repetition, (2) complex lexical repetition, (3) simple partial paraphrase, (4) simple mutual paraphrase, (5) complex paraphrase, (6) superordinate, hyponymic and co-reference repetition. In the reported experiments we only considered lexical links formed by simple lexical repetition, i.e., a lexical link is considered to exist between two instances of the same lexeme, but with possible morphological variations, such as past tense forms of verbs, plural forms of nouns, etc. This is done by stemming the document representation in advance and calculating lexical links using stemmed terms<ref type="foot" coords="1,112.20,483.70,3.24,5.83" target="#foot_0">1</ref> (excluding common stopwords). In an earlier work <ref type="bibr" coords="1,324.84,485.88,105.32,8.96" target="#b5">(Vechtomova et al., 2006)</ref> we experimented with lexical links formed by simple lexical repetition, synonymy and hyponymy (determined using WordNet), and found that repetition on its own performs as well as the other relationships in a document ranking task.</p><p>The number of lexical links (x) that must exist between two sentences in order for them to form a bond, was determined empirically in an earlier experiment, and x=1 was used in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline runs</head><p>First, for each topic we build a query and retrieve n top documents using the Okapi retrieval system <ref type="bibr" coords="1,90.00,610.20,92.97,8.96" target="#b4">(Robertson et al., 1995)</ref>. The query is built as follows:</p><p>We will refer to the bracketed text segments in the Template section of the topic as facets. All words from the facets are used as the query. If template id=2 <ref type="bibr" coords="1,323.28,633.24,86.84,8.96">("What [relationship]</ref> exist between [entity] and [entity]?"), we do not extract the first facet ("relationship"). The only exception is when relationship type is "financial ties", in which case the first facet is represented by words "financial, money, funds, monetary". All proper nouns from the Narrative section, identified using Brill's Part-of-Speech tagger <ref type="bibr" coords="1,454.44,667.68,48.45,8.96" target="#b0">(Brill, 1995)</ref>, are added to the query as well. The resulting query is used to retrieve 200 documents using Okapi.</p><p>In the next step, we expand the facets as follows: if the facet contains an adjective derived from a proper noun, we identify its pertainym via WordNet and add it to the facet (e.g. adjective "English" and its pertainym "England").</p><p>Our next step is to get a subset of "valid" documents that contain at least one proper noun in each facet (if it has any). If none of the facets contain proper nouns, then the whole document set is considered "valid".</p><p>All "valid" documents are split into sentences. We experimented with two different sentence ranking methods. In the first one (UWATCIQA1) sentences are ranked as follows:</p><p>1. Number of facets the sentence contains. A sentence is considered to have a facet if at least one word from that facet is present. We discard all sentences that have no facets. 2. Resolve ties by the number of query terms the sentence contains. 3. Resolve ties by the number of lexical bonds the sentence has with the following sentences in the document. A sentence is said to have a lexical bond with another sentence if they have at least one lexeme in common.</p><p>In the second run (UWATCIQA2) the following ranking method was used:</p><p>1. Number of facets the sentence contains (as in UWATCIQA1). 2. Resolve ties by the number of query terms the sentence contains (as in UWATCIQA1).</p><p>3. Resolve ties by the average inverse document frequency (idf) of all nonstopwords in the nugget.</p><p>In order to exclude very long and verbose sentences we remove those whose length exceeds 50 nonstopword stems. We also remove (near-)duplicate sentences as follows: if a sentence has more than 50% non-stopword stems in common with any sentence ranked higher, remove it. Thirty top-ranked remaining sentences in each run are then submitted as nuggets to NIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Final runs</head><p>Our clarification form contained top 15 nuggets retrieved in UWATCIQA1 run. The evaluators were asked to select relevant nuggets. We used the feedback to clarification forms in two different ways as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run UWATCIQA3:</head><p>Nuggets which have lexical bonds with the nuggets selected by the users in the clarification form are included in this run. The ranking order of nuggets is the same as in UWATCIQA1 run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run UWATCIQA4:</head><p>Query expansion terms were extracted from the user-selected nuggets, ranked by Offer Weight <ref type="bibr" coords="2,474.96,514.56,47.01,8.96;2,90.00,526.08,21.58,8.96" target="#b3">(Robertson, 1990)</ref>, and top 20 were added to the original query. Nuggets were extracted from the top 200 retrieved documents in the same way as for UWATCIQA1 run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results of the runs submitted to ciQA are presented in Table <ref type="table" coords="2,349.44,577.92,3.77,8.96" target="#tab_0">1</ref>. There is a statistically significant (t-test P&lt;0.002) difference of 4% in MANuR between UWATCIQA1 and UWATCIQA2. This provides some evidence that the use of lexical bonds in nugget ranking helps retrieve relevant nuggets, compared to the use of average idf of terms in the nuggets. Difference in F-Measure between the two runs, however, was not significant. Out of the total of 431 nuggets shown to users in the clarification forms in all topics, 274 (63.6%) were selected as relevant. Comparison of the two expansion runs shows that UWATCIQA4 is better than UWATCIQA3 in F-measure by 8% (not statistically significant). In MANuR, however, UWATCIQA3 is better than UWATCIQA4 by 1.4% (statistically significant).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>All our runs perform better in F-measure than the median (0.209) calculated over 23 runs submitted by participants in the track. Two topics in UWATCIQA1 and three topics in UWATCIQA4 have the best Fmeasure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Having analysed the results of the two baseline runs, we noticed substantial inconsistency in nugget judgements by the evaluators. There were many cases where both runs A and B returned the same nugget, however it is only marked as containing the correct answer in run A, but not in run B, although none of the other nuggets in run B were marked to contain this answer. The judgement inconsistencies are summarised in Table <ref type="table" coords="3,125.64,252.48,3.77,8.96" target="#tab_1">2</ref>. Such a large number of missed answers makes the evaluation rather unreliable, and prevents us from drawing any firm conclusions from the official results regarding the developed methods. One possibility would be to re-calculate the performance measures having restored the missing answers. This, however, is only a partial remedy. We think that a new methodology is needed to obtain more consistent judgements across systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Also, in the current evaluation methodology only the first nugget containing the correct answer is noted. When a nugget contains the correct answer but was ranked below the first nugget containing the same answer, the fact that it contains the correct answer is not recorded, which limits the usefulness of track judgements for further experiments. This could be avoided if evaluators note all nuggets that contain the correct answer, but only the first correct is still used in calculating the performance measures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,178.56,601.68,242.51,80.36"><head>Table 1 : Official results of the runs submitted to ciQA</head><label>1</label><figDesc></figDesc><table coords="2,178.56,601.68,213.83,56.72"><row><cell></cell><cell>F-Measure</cell><cell>MANuR</cell></row><row><cell>UWATCIQA1</cell><cell>0.247</cell><cell>0.179</cell></row><row><cell>UWATCIQA2</cell><cell>0.246</cell><cell>0.172</cell></row><row><cell>UWATCIQA3</cell><cell>0.247</cell><cell>0.189</cell></row><row><cell>UWATCIQA4</cell><cell>0.268</cell><cell>0.186</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,99.48,276.24,396.63,67.88"><head>Table 2 : Judgement inconsistencies observed in the two baseline runs</head><label>2</label><figDesc></figDesc><table coords="3,99.48,276.24,396.63,44.24"><row><cell></cell><cell>Total nuggets retrieved</cell><cell>Answers found in the</cell><cell>Missed answers</cell></row><row><cell></cell><cell></cell><cell>retrieved nuggets</cell><cell></cell></row><row><cell>UWATCIQA1</cell><cell>838</cell><cell>138</cell><cell>28</cell></row><row><cell>UWATCIQA2</cell><cell>842</cell><cell>136</cell><cell>34</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,95.76,710.64,253.52,8.96"><p>Porter's weak stemmer was used for this purpose<ref type="bibr" coords="1,293.04,710.64,56.24,8.96" target="#b2">(Porter, 1980)</ref> </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,90.00,501.72,432.18,8.96;3,101.52,513.24,277.29,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,155.76,501.72,366.42,8.96;3,101.52,513.24,97.19,8.96">Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,205.32,513.24,104.91,8.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="565" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,90.00,530.76,265.89,8.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="3,128.04,530.76,96.24,8.96">Patterns of Lexis in Text</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hoey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,90.00,548.16,338.37,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,173.16,548.16,130.09,8.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,309.60,548.16,33.78,8.96">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,90.00,565.68,425.01,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,184.56,565.68,151.55,8.96">On term selection for query expansion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,342.96,565.68,106.89,8.96">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,90.00,583.20,432.06,8.96;3,101.52,594.72,420.56,8.96;3,101.52,606.24,17.61,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,432.84,583.20,71.22,8.96">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hankock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,171.24,594.72,205.67,8.96">Proceedings of the Third Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Third Text Retrieval Conference<address><addrLine>Gaithersburg, U.S.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,90.00,623.64,432.01,8.96;3,101.52,635.16,392.01,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,364.56,623.64,157.45,8.96;3,101.52,635.16,126.82,8.96">On Document Relevance and Lexical Cohesion between Query Terms</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karamuftuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,235.56,635.16,162.76,8.96">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1230" to="1247" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
