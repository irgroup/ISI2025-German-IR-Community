<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.86,164.85,333.52,15.11">Concordia University at the TREC-15 QA track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,139.75,197.33,68.71,10.48"><forename type="first">Leila</forename><surname>Kosseim</surname></persName>
							<email>kosseim@cse.concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">CLaC Laboratory Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd. West Montreal</addrLine>
									<postCode>H3G 1M8</postCode>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.08,197.33,72.83,10.48"><forename type="first">Alex</forename><surname>Beaudoin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CLaC Laboratory Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd. West Montreal</addrLine>
									<postCode>H3G 1M8</postCode>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.49,197.33,98.20,10.48"><forename type="first">Abolfazl</forename><surname>Keighbadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CLaC Laboratory Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd. West Montreal</addrLine>
									<postCode>H3G 1M8</postCode>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.35,197.33,80.16,10.48"><forename type="first">Majid</forename><surname>Razmara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CLaC Laboratory Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>1455 de Maisonneuve Blvd. West Montreal</addrLine>
									<postCode>H3G 1M8</postCode>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.86,164.85,333.52,15.11">Concordia University at the TREC-15 QA track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">043007B254F94E65E036316D8E3C4BEC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the system we used for the TREC Question Answering Track. For factoid and list questions two different approaches were exploited: A redundancy-based approach using a modified version of aranea and a parse-tree based unifier. The modified version of aranea essentially uses Google snippets for extracting answers and then projects them to aquaint. The parse-tree based unifier is a linguistic-based approach that chunks candidate sentences syntactically and uses a heuristic measure to compute the similarity of each chunk in a candidate to its counterpart in the question. To answer other types of questions, our system extracts from Wikipedia articles a list of interest-marking terms related to the topic and uses them to extract and score sentences from the aquaint document collection using various interest-marking triggers.</p><p>We submitted 3 runs using different variations of the system. In the factoid run, the average of our 3 runs is 0.202, for the list, we achieved an average of 0.084, and for the "Other", we achieved an average F-score of 0.192.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first year in a while that Concordia University participated in TREC-QA. We only participated in the main task, which consisted in three types of questions: factoid, list and other. Although all three question types contributed equally in the final system score, we put most of our efforts on factoid and other questions, and spend very little time on list questions.</p><p>As our old system <ref type="bibr" coords="1,226.43,612.28,10.52,8.74" target="#b0">[1]</ref> received poor results in 2002 and had not been improved since then, we decided to take advantage of the freely available aranea system 1 , successor of AskMSR <ref type="bibr" coords="1,227.16,636.19,10.51,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,240.07,636.19,7.01,8.74" target="#b2">3]</ref>. For factoid questions, we used two main approaches: a redundancy-based QA system working on the Web (a modified version of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lucene / AQUAINT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List-Aranea</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia</head><p>Lucene / AQUAINT Web List Other Figure <ref type="figure" coords="2,245.61,403.87,3.88,8.74">1</ref>: Overall Architecture of QASCU aranea) and a linguistic-based system working on aquaint only. We spend about 3 person-month improving aranea and modifying it for the 2005 task specifications. In parallel, we developed our own QA module that does a more fine-grained analysis of candidate sentences based on the parse tree similarity between the questions and candidate sentences.</p><p>For the Other questions, we used terms extracted from Wikipedia and projected them on the aquaint collection. Sentences containing these Wikipedia terms are then ranked using various interest-marking triggers.</p><p>In the following sections, we describe our approach to answer factoid, list and other questions. Finally, we present our results in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Overall Architecture</head><p>The overall architecture of our system, called QASCU, is shown in Figure <ref type="figure" coords="2,469.73,610.01,3.88,8.74">1</ref>. The three main modules are shown: the linguistic module based on parse tree unification, the modified aranea module, and the other module. The listquestion module is also shown in the figure, but as mentioned previously, very little work was done on this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Answering Factoid Questions</head><p>Our approach for answering factoid questions is a combination of:</p><p>1. a modified version of Jimmy Lin's aranea system, and 2. a linguistic parse-tree unification algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Modified Aranea</head><p>aranea is a web-based rule and statistical QA system. A simplistic overview of the system is that it generates queries and question reformulations, gets snippets from Google and Teoma, generates ngrams as possible answers, collates the ngrams based on n-gram similarity, does filtering based on the expected type of answer and number of supporting documents, reranks candidates based on frequency of words within aquaint, then projects the answer onto the aquaint corpus. Detailed descriptions of aranea can be found in <ref type="bibr" coords="3,385.98,306.06,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,399.82,306.06,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="3,410.89,306.06,7.01,8.74" target="#b5">6]</ref>.</p><p>Several changes have been brought to the original aranea system. These include: reformulating the web query, adding new data sources, predicting answer types and projecting the answers onto the aquaint corpus.</p><p>aranea uses snippets returned from Google and Teoma for its IR. Teoma was dropped since it was bought by Ask.com and the web interface changed. The system was originally written before TREC included the target based questions. Initially, we opted to solve this by simply adding the target to our query. Unfortunately, this caused the query size to balloon. Since Google has a hard limit of 10 words in its query, we chose to put the target at the beginning of the query and only keep the nouns, adjectives, verbs and numbers from the questions into the query. One noticeable source of error was that aranea would not find a correct answer at all, nearly half the time. Therefore, in addition to the Google snippets, we used two new data sources:</p><p>• aquaint -Lucene<ref type="foot" coords="3,244.03,491.78,3.97,6.12" target="#foot_0">2</ref> was used to search for relevant articles from the aquaint document collection using the same query sent to Google.</p><p>• GoogleText -In addition, also we downloaded the actual web pages found by Google (hereto known as GoogleText), as opposed to searching only the Google snippet. Not all Google documents were downloaded since large, unfocused articles are quite common and we do not want to add too much noise, therefore we chose to only include those who were smaller than 30K.</p><p>All these documents were broken into sentences and ranked by their word overlap with the question. Another noticeable source of error was that aranea would often return answers that were not of the correct type. For example, returning a location instead of a person. The QASCU system generates an expected type (Number, Date, Person, Organization, Location, Unknown) through a simple word-match and ordering algorithm which was generated by hand from the 2005 question set, then uses the GATE NE <ref type="bibr" coords="4,268.22,163.83,10.51,8.74" target="#b6">[7]</ref> tagger, with a few extras for Unknown and Numbers, to return candidates that are then scored by their relevance to the type of answer expected. These candidates are then used instead of the snippets returned by Google, Lucene or GoogleText. For Numbers, all numbers and following noun phrases are returned and for Unknown type questions, noun phrases and capitalized entities (strings of capitalized words, including internal prepositions (e.g.: Unites States of America)) are returned.</p><p>Because aranea is fundamentally based on answer redundancy, it would often give the most common piece of information of the correct type even though it may not answer the specified question (e.g: a question on Kurt Cobain's birth will return the date of his death since there are so many more texts that cover his death than his birth.) To diminish this, instead of using the frequency over the whole of the aquaint corpus, we rerank using the frequency over the top 50 documents from prise.</p><p>Since aranea is a web based solution, the answer must be projected onto the corpus. This projection is done quite naively by increasing the number of documents returned by Lucene and searching through them when searching for supporting documents. If no document is found, then a subset of the answer is searched for depending on the answer type.</p><p>With these modifications, we managed to double the performance of aranea on the 2005 TREC data (from an accuracy of 0.170 to 0.321).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">The Parse-Tree Unifier</head><p>In addition to the modified aranea, we also developed a parse-tree matching algorithm to identify and rank candidate sentences from the aquaint collection with the given question based on syntax.</p><p>The unifier performs its own information retrieval on the aquaint collection only. For this, we used Lucene. To build the query for Lucene, we send the original question+target to a part-of-speech tagger and only keep open class words. The keywords are then weighted according to their parts-of-speech: proper nouns are given a higher weight, then nouns, then verbs, then adjectives and finally, adverbs. The query is passed through Lucene to retrieve a list of relevant documents. If no document is found, the least valuable keyword is dropped from the query and IR is tried again until at least one document is returned. The system then tokenizes each document and marks the sentences that include at least LIBERT Y RAT IO percent of the keywords (this value is set to 60% but experiments show insensitivity of the unifier's accuracy to this value). The semantic relatedness of the question's main verb to each verb in the candidate sentences is verified in the next step. To do so, we use Leacock and Chodorow's similarity measure from WordNet::Similarity <ref type="bibr" coords="4,408.11,662.41,9.96,8.74" target="#b7">[8]</ref>. Finally, the arguments are the matched verbs are mapped on each other and the similarity between them is computed. Candidate sentences are then ranked by the extracted similarity values: their verb relatedness to the questions's verb, and the unification score of their arguments.</p><p>Once the sentences are ranked, we try to match their parse-tree to the parsetree of the question. Here, we use the Minipar parser <ref type="bibr" coords="5,376.52,187.74,10.52,8.74" target="#b8">[9]</ref> and employ a fuzzy unification method. We observed that starting and limiting the parse-tree unification method from the most similar verb to the question's main verb does not always lead to the correct position in the candidate parse tree; a strong verb similarity must co-occur with an entity match in the subtree. This suggests that a stronger seed point is the root of the subtree that contains the question's head noun phrase.</p><p>To choose the question head, we rank all noun phrases in the question and pick the one that contains the most valuable question keywords. If this head phrase is found in the candidate sentence, it will become an anchor to find the relevant verb: we move up from this noun phrase in the parse tree to reach the first parent verb. In long candidate sentence, using an anchor reduces the candidate verbs to the ones that include the question head (or a reference to it).</p><p>After a candidate subtree is chosen based on the semantic similarity of its verb, we proceed with the unification by checking whether the selected verb relates the same entities as the question's main verb. A heuristic method evaluates how similar the two subject subtrees are (likewise for object or modifier) subtrees if any). In other words, the object and subject arguments are strong constraints, while the other arguments (such as modifiers) are used to score the accepted sentences. These similarity scores add up to produce the final score of the candidate:</p><formula xml:id="formula_0" coords="5,201.18,459.95,208.90,9.65">Score = Σ i:subtree Score(Question i , Candidate i )</formula><p>To unify two phrases (subject, object or modifier subtrees) marked by the linguistic method as the arguments of verbs, we apply a heuristic process. This step uses two measures: the number of overlapping words based on a bagof-words approach and the number of overlapping links. More formally, we compute α × W ordOverlap + (1 -α) × LinkOverlap as the total unification score of a sentence. The parameter α shows the relative importance of the two features: in our configuration, we use α = 1 3 , which considers the link-overlap feature to be twice as important as the bag-of-words feature.</p><p>The reason we relax our linguistic constraints at this stage is that we are focusing on a sentence that conveys a similar event or state to the question; only a clue about similarity of its verb arguments is sufficient to conclude that its verb is affecting the same entities as the question. Syntactic differences of verb arguments (subtrees) should not critically affect our judgment. We reject a candidate if its arguments have no keyword based overlap with the question's.</p><p>the entities might probably be mentioned in syntactically different phrases.</p><p>Since the unifier identifies sentences (as opposed to exact answers), we use the unifier to semantically validate and to find a supporting document for aranea's answers.</p><p>We used the output of aranea on the TREC data to improve our IR result. We added m documents from the IR result for the extended query AraneaAnswers QuestionKeywords to the regular document list we get for the original QuestionKeywords query. The expected answer type is also taken from ranea's output.</p><p>The top-most aranea candidate is searched in the unifier's answers. If it is found, then this answer is confirmed and the document that this sentence is extracted from is returned as the supporting document. However, if no unifier candidate contains the top answer, the validation process is tried for the top two aranea answer and so on until the unifier confirms that answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answering Other Questions</head><p>To answer "Other" questions, we used a notion of interest marking terms. Fundamentally, we hypothesized that interesting nuggets can be extracted using two types of interest markers:</p><p>1. target-specific interest marking terms (e.g. Titanic ⇒ White Star Line, J.F. Kennedy ⇒ Lee Harvey Osward), and 2. universal interest marking terms (e.g. first man on the moon, 150 people died)</p><p>To identify target-specific interest marking terms, we used the Wikipedia<ref type="foot" coords="6,473.01,447.63,3.97,6.12" target="#foot_1">3</ref> online dictionary. Articles in Wikipedia relate to many target types and the content of each article is a short summary that highlights the most interesting facts -precisely what we are looking for to find target-specific interest markers.</p><p>The first stage to answering Other questions is to find the proper Wikipedia article. We use the Google API to search in the Wikipedia domain using the target as query. The first Wikipedia article that satisfies the query is taken. However, if no Wikipage satisfies the query, then we try to loosen the query and eventually if still no Wikipage is not found, the top N aquaint documents are used for term extraction.</p><p>After the Wikipage or top N aquaint documents are retrieved, we extract named entities as interesting terms for each target, and we search aquaint for the N most relevant documents. These documents are retrieved by Lucene using the same query generated for the target as in the Wikipage search and a secondary query from the title of the Wikipage if it has been found. This secondary query is ORed to the Google query.</p><p>Within the documents chosen as the domain, the frequency of each interest marking term is then computed. For each term, we compute a weight as the logarithm of its frequency.</p><p>W eight(T i ) = Log(F requency(T i ))</p><p>All sentences from the domain documents are then scored according to how many target-specific interesting terms it contains. This is computed as the sum of the weight of the interesting terms it contains.</p><formula xml:id="formula_1" coords="7,190.46,244.74,230.32,30.32">Score(S i ) = n j=1 W eight(T j ) | T j ∈ S i ∀1 ≤ j ≤ n</formula><p>After scoring the sentences and throwing away those with a score of zero (i.e. no interesting term in the sentence), we try to remove paraphrases. In order not to remove false paraphrases, we play it conservatively, and only remove lexically similar sentences. Either the sentences are almost equivalent to each other at the string level or they share similar words but not the same syntax. To compare sentences, we have used the SecondString package <ref type="foot" coords="7,402.60,345.34,3.97,6.12" target="#foot_2">4</ref> , an open-source Java-based package of approximate string-matching techniques <ref type="bibr" coords="7,410.87,358.87,14.62,8.74" target="#b9">[10]</ref>.</p><p>Once the sentences are ranked based on target-specific interesting terms, we boost the score of sentences that contain terms that generally mark interesting information regardless of the topic. Such markers were determined empirically by analyzing the previous TREC data. These markers consists of superlatives, numeral and target-type specific keywords. This last type of marker is essentially a list of terms that do not fit any specific grammatical category, but just happen to be more frequent in interesting nuggets. To identify these terms, we analyzed the data of the 2005 other questions and identified, for each type of target (person, organization, . . . ) a list of terms that are more frequent in interesting nuggets. The score of any sentence that contains a superlative, a numeral or a interesting-term is boosted by 20% per term. Finally, the top N sentences making up 7000 non-white-space characters are returned as our nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answering List Questions</head><p>Although list questions accounted for a third of the overall score, we spent very little time on them (about 3 days/person). List are answered by returning the entire list of possible answers returned by the factoid module with two small differences. The list questions differ by requiring fewer supporting documents and that since "Unknown" answer-types return a lot of noise, only the top 50% of those are returned. Work has been done to increase the accuracy of the answers but this actually lowers the score. Surprisingly, our list runs also performed better than the median; even though we invested so little effort on it. Since the list answers are derived directly from the output of the factoid runs, our performance on the lists is probably a side-effect of our performance on the factoid.</p><p>Finally, our F-score and pyramid score on the other questions are also higher than the median. Our best runs are QASCU1 and QASCU2 which do not use aquaint for term extraction. This seems to confirm our intuition that aquaint should not be used for term extraction, and Wikepedia, or another source is more appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described the 3 runs we submitted to the QA track. The factoid run is based on a modified version of aranea coupled with a syntactic tree unifier. Our other run is based on terms found in Wikepedia entries and ranking of nuggets is done through the use of target-specific and target-independent interest markers. Finally, our list run is based on a very crude processing of the output of the factoid runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,133.77,632.30,343.71,20.69"><head>Table 1 :</head><label>1</label><figDesc>The reason for this is that, based on the TREC 2005 data, 56% of the list score comes from the best 9 answered Official results of the 3 runs.</figDesc><table coords="9,174.28,126.76,262.70,119.92"><row><cell></cell><cell cols="4">QASCU1 QASCU2 QASCU3 median</cell></row><row><cell>Factoid</cell><cell>0.194</cell><cell>0.199</cell><cell>0.213</cell><cell>0.186</cell></row><row><cell>incorrect</cell><cell>272</cell><cell>270</cell><cell>260</cell><cell></cell></row><row><cell>unsupported</cell><cell>20</cell><cell>22</cell><cell>29</cell><cell></cell></row><row><cell>inexact</cell><cell>31</cell><cell>29</cell><cell>25</cell><cell></cell></row><row><cell>locally correct</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell></cell></row><row><cell>globally correct</cell><cell>78</cell><cell>80</cell><cell>86</cell><cell></cell></row><row><cell>List</cell><cell>0.094</cell><cell>0.096</cell><cell>0.063</cell><cell>0.087</cell></row><row><cell>Other (F-score)</cell><cell>0.197</cell><cell>0.180</cell><cell>0.199</cell><cell>0.125</cell></row><row><cell>Other (pyramid)</cell><cell>0.206</cell><cell>0.194</cell><cell>0.203</cell><cell>0.139</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,149.01,660.31,101.62,6.64"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,149.01,659.99,97.39,6.64"><p>http://en.wikipedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,149.01,663.75,148.19,6.64"><p>http://secondstring.sourceforge.net</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>First and foremost, we need to thank <rs type="person">Jimmy Lin</rs> for making aranea available for download. Sharing resources allows smaller teams to contribute to research rather than play catch-up.</p><p>This project was financially supported by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs> and <rs type="institution">Bell University Laboratories (BUL)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>list questions (10%). These tend to be lists with few elements. Unfortunately, when removing results to increase accuracy these few questions tend to do more poorly and thereby lower the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>For TREC-2006 we submitted three runs. The factoid versions of the system are:</p><p>QASCU1-factoid: Modified version of aranea alone. We drop anything without at least two supporting documents taken from Google, Lucene, or GoogleText, and we rerank the remaining using the whole aquaint corpus.</p><p>QASCU2-factoid: Same as QASCU1-factoid + the parse tree matcher.</p><p>QASCU3-factoid: Same as QASCU1-factoid, but we drop anything without at least three supporting documents, and we rerank the remaining using the top 50 prise documents on the target.</p><p>The Other section of our systems have also three different submissions:</p><p>QASCU1-other: Taking the intersection of top 25 documents from prise and Lucene/aquaint as the domain documents.</p><p>QASCU2-other: Taking the top 50 Lucene/aquaint documents as the domain and adding two documents from aquaint to the Wikipage for term extraction.</p><p>QASCU3-other: Using top 50 Lucene/aquaint documents as the domain.</p><p>The list part of the 3 runs we submitted consisted of: QASCU1-list: Less accurate algorithm for "Unknown" type questions.</p><p>QASCU2-list: Require only one supporting document.</p><p>QASCU3-list: Require at least two supporting documents.</p><p>Table <ref type="table" coords="8,177.16,577.25,4.98,8.74">1</ref> shows the official evaluation details of our 3 runs along with the median score of all systems. The results of all our runs are above the median. For the factoid runs, QASCU3 was significantly better than QASCU2 and QASCU1. QASCU2 (the run with the parse tree unifier) is better than QASCU1 (without the parse tree unifier), but since QASCU3 did not include the unifier and performed much better than the other two, we are not convinced of the unifier's usefulness, when run independently. Most of the performance seems attributable to the modified aranea.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,154.24,149.78,323.23,8.74;10,154.25,161.74,323.23,8.74;10,154.25,173.69,315.31,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,374.97,149.78,102.51,8.74;10,154.25,161.74,124.03,8.74">the quantum questionanswering system at trec-11</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kosseim</surname></persName>
		</author>
		<idno>TREC-11</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,307.77,161.74,169.71,8.74;10,154.25,173.69,99.21,8.74">Proceedings of the 11th Text Retrieval Conference (TREC-11)</title>
		<meeting>the 11th Text Retrieval Conference (TREC-11)<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,193.62,323.24,8.74;10,154.25,205.57,153.75,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,207.67,193.62,248.11,8.74">AskMSR: Question answering using the Worldwide Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,154.25,205.57,123.11,8.74">Proceedings of EMNLP 2002</title>
		<meeting>EMNLP 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,225.50,323.23,8.74;10,154.25,237.45,249.41,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,313.24,225.50,164.23,8.74;10,154.25,237.45,74.15,8.74">An analysis of the AskMSR questionanswering system</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.91,237.45,123.11,8.74">Proceedings of EMNLP 2002</title>
		<meeting>EMNLP 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,257.38,323.24,8.74;10,154.25,269.33,323.24,8.74;10,154.25,281.29,323.24,8.74;10,154.25,293.24,311.32,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,154.25,281.29,318.66,8.74">Integrating web-based and corpus-based techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.70,293.24,267.56,8.74">Proceedings of the 12th Text Retrieval Conference (TREC-12)</title>
		<meeting>the 12th Text Retrieval Conference (TREC-12)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,313.17,323.23,8.74;10,154.25,325.12,323.23,8.74;10,154.25,337.08,323.24,8.74;10,154.25,349.03,89.70,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,275.95,313.17,201.52,8.74;10,154.25,325.12,220.60,8.74">Question answering from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,398.47,325.12,79.02,8.74;10,154.25,337.08,323.24,8.74;10,154.25,349.03,58.19,8.74">Proceedings of the twelfth international conference on Information and knowledge management (CIKM)</title>
		<meeting>the twelfth international conference on Information and knowledge management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,368.96,323.23,8.74;10,154.25,380.91,323.24,8.74;10,154.25,392.87,323.23,8.74;10,154.25,404.82,130.52,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,187.94,380.91,289.55,8.74;10,154.25,392.87,127.46,8.74">Extracting answers from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,307.47,392.87,170.01,8.74;10,154.25,404.82,47.99,8.74">Proceedings of the 11th Text Retrieval Conference</title>
		<meeting>the 11th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,424.75,323.24,8.74;10,154.25,436.71,218.58,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,237.29,424.75,235.65,8.74">GATE, a General Architecture for Text Engineering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,154.25,436.71,134.11,8.74">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="223" to="254" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,456.63,323.23,8.74;10,154.25,468.59,323.23,8.74;10,154.25,480.54,77.87,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,450.33,456.63,27.14,8.74;10,154.25,468.59,237.21,8.74">Wordnet::similarity -measuring the relatedness of concepts</title>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,415.48,468.59,62.00,8.74;10,154.25,480.54,46.10,8.74">Proceedings of NAACL-04</title>
		<meeting>NAACL-04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.24,500.47,323.24,8.74;10,154.25,512.42,116.90,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,208.73,500.47,200.60,8.74">Principle-based parsing without overgeneration</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,427.82,500.47,49.66,8.74;10,154.25,512.42,43.47,8.74">Proceedings of ACL-93</title>
		<meeting>ACL-93</meeting>
		<imprint>
			<biblScope unit="page" from="112" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.25,532.35,323.23,8.74;10,154.25,544.30,323.24,8.74;10,154.25,556.26,323.23,8.74;10,154.25,568.21,136.02,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,403.13,532.35,74.35,8.74;10,154.25,544.30,220.36,8.74">A Comparison of String Distance Metrics for Name-Matching Tasks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,398.44,544.30,79.05,8.74;10,154.25,556.26,291.09,8.74">Proceedings of the IJCAI Workshop on Information Integration on the Web (IIWeb)</title>
		<meeting>the IJCAI Workshop on Information Integration on the Web (IIWeb)<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
