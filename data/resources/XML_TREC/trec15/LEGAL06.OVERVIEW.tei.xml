<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.00,75.53,209.63,12.58">TREC-2006 Legal Track Overview</title>
				<funder>
					<orgName type="full">Keith Ivey of Tobacco Documents Online</orgName>
				</funder>
				<funder ref="#_4S4uPSA">
					<orgName type="full">IIT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,90.00,104.72,76.19,10.80"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
							<email>jason.baron@nara.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Office of General Counsel</orgName>
								<orgName type="laboratory">National Archives and Records Administration</orgName>
								<address>
									<addrLine>Suite 3110</addrLine>
									<postCode>20740</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,90.00,146.12,75.81,10.80"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">David D. Lewis Consulting</orgName>
								<address>
									<addrLine>858 W. Armitage Ave. #296</addrLine>
									<postCode>60614</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,90.00,187.52,86.63,10.80"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff2">
								<orgName type="department">College of Information Studies and Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.00,75.53,209.63,12.58">TREC-2006 Legal Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C3BA2E433978EC21EF3A2243A12AE87C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the first year of a new TREC track focused on "e-discovery" of business records and other materials. A large collection of scanned documents produced by multiple real world discovery requests was adopted as the basis for the test collection. Topic statements were developed using a process representative of current practice in e-discovery applications, with both Boolean and natural language queries being supported. Relevance judgments were performed by personnel who had received professional training, and often considerable experience, in review of similar materials for this task. Six research teams and one manual searcher submitted a total of 33 retrieved sets for each topic. These were pooled and a portion assessed to support evaluation of both the retrieved sets themselves and for future use of the collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The use of information retrieval techniques in law has traditionally focused on providing access to legislation, regulations, and judicial decisions. Searching business records for information pertinent to a case (or "discovery") has also been important, but digitally searchable records were until recently the exception rather than the norm. That is rapidly changing, however. The motivating goal of this new legal track at the Text Retrieval Conference (TREC) is to assess the ability of information retrieval technology to meet the needs of the legal community for tools to help with retrieval of business records. This is an issue of increasing importance given the vast amount of information in electronic form to which access is required during litigation. Ideally, the results of our studies will also help to advance the discussion of the capabilities and limitations of automated support for e-discovery in the legal community.</p><p>The importance of doing well at e-discovery is hard to overstate. In the past few years, lawsuits involving giant corporations and single individuals alike have resulted in huge multi-million and even billion dollar adverse verdicts turning on the failure of a party to the litigation to properly preserve and provide access to various forms of electronic records, including most notably e-mail, and data on backup tapes (see, e.g., <ref type="bibr" coords="1,90.00,575.71,63.87,9.02" target="#b7">Coleman, 2005;</ref><ref type="bibr" coords="1,156.36,575.71,63.77,9.02">Zubulake, 2004)</ref>. Beyond the headlines, however, are a growing percentage of lawsuits that involve the production of responsive electronic data stored in vast corporate, governmental, and other repositories. Lawyers are struggling to keep up with the profusion of electronic data and metadata in all its forms, on desktops and networks. So too, troves of "legacy" documents, sometimes going back decades, continue to be maintained and need to be searched in response to discovery requests.</p><p>The results of the legal track are especially timely and important given recent changes in the U.S. Federal Rules of Civil Procedure that went into effect on December 1, 2006. The amended rules introduce a new category of evidence, namely, "electronically stored information" ("ESI") in "any medium," intended to stand on an equal footing with existing rules covering the production of "documents." Rule 26(f) specifically directs that at an initial conference of the parties, "any issues relating to disclosure or discovery of electronically stored information, including the form or forms in which it should be produced" are to be discussed. Such issues will necessarily include the need to consider how appropriate ESI is made accessible to opposing parties. Providing access involves more than just search technology, of courseinitial query formulation, iterative query refinement, and review of search results for relevance and privilege are important components of the entire process. The Advisory Committee notes to Rule 34 say that in talking about "ESI in any medium," the rules amendments were intended to "encompass future developments in computer technology," which speaks specifically to our goals for the TREC Legal Track.</p><p>Against the backdrop of the Federal Rules changes, the status quo in the legal profession, even in large and complex litigation, is continued reliance on free-text Boolean searching for satisfying document (and now ESI) production demands <ref type="bibr" coords="2,194.70,180.12,105.41,9.02">(Sedona Conference 2005)</ref>. Thus, to the extent a trend exists in the case law, it is where courts have intervened at early stages to ensure that parties negotiate "search protocols." To date these have consisted solely of a static list of agreed upon query terms, rather than more complex forms of negotiations over, for example, complex (extended) Boolean queries (e.g., those specifying truncation and/or proximity operators). Moreover, as of the date of this paper, there is no reported case law in the United States where courts have been called upon to adjudicate the reasonableness of alternative forms of search methodologies (e.g., ranked retrieval). It is only a matter of time, however, before parties in litigation will more fully utilize alternative techniques, enter into negotiations regarding search system selection and/or query formulation, and, inevitably, conduct formal adjudication over the reasonableness and efficacy of such alternative approaches.</p><p>An important aspect of e-discovery and thus of the TREC legal track is an emphasis on recall over precision. In light of the fact that a large percentage of requests for production of documents (and now ESI) routinely state that "all" such evidence is to be produced, it becomes incumbent on responding parties to attempt to maximize the number of responsive documents found as the result of a search. All things being equal, lawyers would be expected to move towards alternative search methods that produce greater numbers of responsive documents for the same resources expended; conversely, alternatives that produce fewer responsive documents are likely to be judged as insufficient, even if greater precision (economy) is achieved overall. If recall comparable to the presently used techniques could be assured, then interest would likely exist in increasing precision (thereby diminishing the need to manually review false positive hits generated by automated means).</p><p>There have been to date few research efforts studying effectiveness of retrieval in civil discovery contexts. The seminal study <ref type="bibr" coords="2,165.76,444.60,89.31,9.02" target="#b3">(Blair &amp; Maron, 1985)</ref>, found that while attorneys believed they had found 75% of the relevant documents for litigation involving a train accident, in fact only an estimated 20% of relevant documents were discovered. The authors attributed this to the inherent ambiguity of language. At least one later study has looked at a comparison of Boolean and natural language searches in the context of a structured database of case precedents <ref type="bibr" coords="2,245.31,490.63,51.80,9.02" target="#b14">(Turtle 1994)</ref>, and experiments with Boolean systems on outside the legal context have been reported at TREC (e.g., <ref type="bibr" coords="2,283.01,502.09,58.40,9.02">Lu et al. 1993;</ref><ref type="bibr" coords="2,343.87,502.09,52.48,9.02">Jacobs 1995)</ref> and elsewhere.</p><p>The key goal of the TREC 2006 legal track was to apply objective benchmark criteria for comparing search technologies, using topics and documents approximating those of actual discovery settings. Given the reality of the use of Boolean search in present day litigation, of significant interest was comparing the efficacy of Boolean search using negotiated queries with alternative methods. The chosen collection, about seven million scanned documents from the tobacco Master Settlement Agreement, can also be used for technology-centered experiments comparing retrieval techniques based on metadata and/or optical character recognition.</p><p>The remainder of this paper is organized as follows. Section 2 describes the document collection. Section 3 then explains the topic development process. In Section 4, the process by which relevance judgments were created is presented. Section 5 identifies the participating research teams and presents some preliminary results. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Document Collection</head><p>The Legal Track required a collection reflecting the scope and diversity of documents searched in real discovery settings. Obtaining access to the internal documents of large enterprises for research purposes is difficult, but ironically discovery proceedings in real legal cases provide one source of such material. As the Legal Track test collection we chose the IIT CDIP Test Collection, version 1.0 (which we will refer to as "IIT CDIP 1.0") which is based on documents released under the tobacco "Master Settlement Agreement" (MSA).</p><p>The MSA settled a range of lawsuits by the Attorneys General of several US states against seven US tobacco organizations (five tobacco companies and two research institutes). One part of this agreement required those organizations to make public on the World Wide Web (through at least June 30, 2010) all documents produced in discovery proceedings in the lawsuits by the states, as well as all documents produced in a number of other smoking and health-related lawsuits. Notable among the provisions is that the tobacco organizations were required to provide to the National Association of Attorneys General (NAAG) a copy of metadata and the scanned documents from the websites, and are forbidden from objecting to any subsequent distribution of this material. The text of the MSA and accompanying appendices and other documents can be found at the websites of Attorneys General of several US states, including California (http://ag.ca.gov/tobacco/msa.php).</p><p>The University of California San Francisco (UCSF) Library, with support from the American Legacy Foundation, has created a permanent repository, the Legacy Tobacco Documents Library (LTDL), for tobacco documents <ref type="bibr" coords="3,169.46,304.32,127.48,9.02" target="#b13">(Schmidt, Butter &amp; Rider 2002)</ref> in order to assure continued availability of these materials. The Illinois Institute of Technology (IIT) Complex Document Information Processing (CDIP) 1.0 collection is based on a snapshot, generated between November 2005 and January 2006, of the MSA subcollection of the LTDL. The snapshot consisted of 1.5 TB of scanned document images, as well as metadata records and optical character recognition (OCR) output produced from the images by UCSF. The IIT CDIP project subsequently reformatted the metadata and OCR, combined the metadata with a slightly different version obtained from UCSF in July 2005, and discarded some documents with formatting problems, to produce the IIT CDIP 1.0 collection <ref type="bibr" coords="3,289.47,384.86,76.34,9.02">(Lewis, et. al 2006)</ref>.</p><p>The IIT CDIP 1.0 collection consists of 6,910,192 document records in the form of XML elements. The two subelements which provide the most conventional target for text retrieval are &lt;ti&gt; (the document title) and &lt;ot&gt; (the OCR text). The highly variable quality of the OCR, combined with the great variations in document length (from one page to thousands of pages) makes retrieval even on these fields a challenge. In addition to the text subelements, there are a wide range of other metadata subelements present in some or all of the records, including senders and recipients, important names mentioned in the document, controlled vocabulary categories, geographical and organizational context identifiers, and many others. The degree to which this information is present varies with the originating tobacco organization and other factors. Overall, the structure of the data is extremely rich and still not well understood.</p><p>IIT CDIP 1.0 had strengths and weaknesses as a collection for the Legal Track. The wide range of document genres (including letters, memos, budgets, reports, agendas, minutes, plans, transcripts, scientific articles, email, and many others) and the large number of documents are very typical of legal discovery settings. The fact that documents were scanned and OCR'd is representative of some discovery situations, but perhaps not those of most interest to those concerned with electronic discovery. The rich but variable quality metadata is also perhaps not typical. The fact that the MSA documents were themselves the output of legal discovery proceedings might suggest they are unrepresentative as inputs to TREC's simulation of a legal discovery situation. Our worries about that point are mitigated to some extent, however, by the fact that the MSA documents originated from seven different organizations in response to hundreds of distinct document requests in multiple legal cases. Thus their diversity is more representative of a diverse population of company records than perhaps might initially be imagined. We further addressed this concern by using a range of topics in the evaluation, some with content highly similar to MSA discovery requests, and others very different. The fact that documents originated from seven different organizations but were searched as a unit is decidedly anomalous from the perspective of federated search, and some future users of the collection may wish to treat the seven subcollections in a more separate manner.</p><p>Several minor glitches in the preparation of IIT CDIP 1.0 turned up during indexing of the data by Legal Track participants. In addition, a number of documents turned out to have XML records but no document images, which was both an immediate problem for relevance assessment, and also a problem for the types of document image retrieval and mining studies towards which the CDIP project is targeted <ref type="bibr" coords="4,458.86,108.85,49.66,9.02;4,90.00,120.37,17.21,9.02" target="#b0">(Agam et al. 2006</ref>). These problems are being investigated in ongoing work by the IIT CDIP project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Topic Development</head><p>Topic development was modeled on U.S. civil discovery practice. In the litigation context, a "Complaint" is filed in court, outlining the theory of the case, including factual assertions and causes of action representing the legal theories of the case. In a regulatory context, often formal letters of inquiry serve a similar purpose by outlining the scope of the proposed investigation. In both situations, soon thereafter one or more parties create and transmit formal "requests for the production of documents" to adversary parties, based on the issues raised in the Complaint or Letter of Inquiry. (If in federal court, this type of demand is typically filed pursuant to Fed. R. Civ. P. 34, but may also be sent to third party non-defendants via subpoena under Fed. R. Civ. P. 45.) Requests to produce documents are typically very broadly worded, in an attempt to force the opposing party to provide a maximum number of responsive documents. In some cases, however, requests are purposely more narrowly tailored when the focus is on particular documents known to be in the possession of a party which are deemed useful at trial. A third category of requests are aimed at finding only particular types of documents (e.g. all "internal memoranda" on a designated topic.)</p><p>It is increasingly common for lawyers to consider requesting that specific search terms be used for the purpose of searching large databases for potentially responsive documents. Courts have begun referring to the development of "search protocols," which are to be developed either unilaterally or, to a greater or lesser extent, made subject to negotiations between parties prior to conducting searches. At present, it is typically assumed that an extended Boolean search (i.e., one with truncation and/or proximity operators) will be performed, although some legal technology firms now also support other types of search technology. Less well known is what percentage of cases have utilized a robust or sophisticated process of negotiations over how search terms, wildcards, Boolean logic, and proximity operators are to be combined to form queries. Nevertheless, for the purpose of the TREC 2006 legal track, it was deemed important to develop topics that stood in as proxies for real-life requests to produce documents in which a set of Boolean strings were developed by a negotiation process between two parties.</p><p>For the TREC 2006 legal track, five hypothetical complaints were created by members of the Sedona Conference®, a group of lawyers who are leading the development of professional practices for ediscovery. These complaints described: (1) an investigation into a fictional tobacco company's improper campaign contributions; (2) a consumer protection lawsuit challenging a fictional tobacco company's "product placement" decisions in television, film, and theatre shows watched by children; (3) an "insider trading" securities lawsuit involving fictional tobacco executives; (4) an antitrust lawsuit involving the movement of commerce in California; and (5) a product liability lawsuit involving defective surgical devices as shown in animal testing. In using fictional names and jurisdictions, the track coordinators, on behalf of TREC, attempted to ensure that no third party would mistake the academic nature of the TREC legal track for an actual lawsuit against real-world companies, and any would-be link or association with either past or present real litigation involving such companies was entirely unintentional.</p><p>For each of the five complaints, a set of topics (formally, "requests to produce") were initially created by the creator of the complaint, and revised by the track co-coordinators. Revisions were considered necessary where the initial topic appeared to have too few or too many relevant documents for effective evaluation, or when it was feared assessors would find the topic too ambiguous. (In this respect, the TREC exercise models real-life objections that often are made to "overbroad," "vague," or "ambiguous" discovery requests, sometimes resulting in courts requiring parties to re-submit narrower and more focused requests.) In the end, 43 topics were selected by the track coordinators for use in the evaluation.</p><p>Two aspects of this screening process were less than ideal. First, the evaluation of breadth and ambiguity was done by the track organizers and a professional tobacco searcher, not by the eventual assessor for each topic, as NIST has often been able to do in past TRECs. (Most assessors had not yet been recruited at the time topics were drawn up.) Second, the screeners did not have access to ranked retrieval search of the collection. Screening was done using the Boolean interface available from UCSF,<ref type="foot" coords="5,422.58,95.23,3.24,5.83" target="#foot_0">1</ref> which at that time had only a beta version of OCR search.</p><p>For each of these 43 topics, the initial topic creator and a track coordinator took the roles of requester and respondent (respectively) in a discovery process, and engaged in an iterated negotiation over the form of a Boolean query for the topic. The final XML topic file contained 43 entries, each including the production request, the associated complaint (which for simplicity was repeated in full for each production request associated with that complaint), the extended Boolean query initially proposed by the (simulated) requesting party, the final extended Boolean query that was agreed upon, and any additional extended Boolean queries in the negotiation history. Human-readable versions of the complaints and the production requests were also prepared for use by relevance assessors and interactive searchers, and a cross-reference to each was recorded in the XML topic file. The topic file is available from the track Web page, http://treclegal.umiacs.umd.edu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Relevance Judgments</head><p>This section describes the process by which relevance judgments were created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Creating Judgment Pools</head><p>The complexity of the CDIP documents and topics, and a report of pooling problems with other large collections <ref type="bibr" coords="5,135.86,352.62,84.48,9.02" target="#b4">(Buckley, et al 2006)</ref> generated some concern about the adequacy of conventional pooling approaches for the Legal Track. We adopted several strategies for addressing these problems, though none were a complete solution.</p><p>We invited track participants to submit up to eight runs (in an effort to maximize pool diversity), asked for runs to depth 5,000 (to facilitate computation of recall-oriented evaluation measures), and asked participating teams to designate their runs for inclusion in the assessment pools in priority order. We included in the assessment pools the top 100 documents from the highest priority run from each team and the top 10 documents from each of the other runs from that team. This yielded a maximum of 170 documents per team for any topic, although usually fewer documents than that were added to the pools because duplicates were removed (both within and across teams). A total of six participating teams submitted a total of 31 runs for official scoring. Two additional runs that were commissioned especially for the track were then used to further enrich the pools.</p><p>It is well known that expert searchers can and will often find documents that fully automated termmatching techniques would miss. The IIT CDIP project therefore contracted with an expert tobacco document searcher (Celia White, http://professionalresearchservices.com) to produce a set of approximately 100 documents for each topic to add to the pools. Working with a track coordinator, she attempted to find documents that were both relevant to a topic and unlikely to be highly ranked by ranked retrieval systems.</p><p>A particular interest in the Legal Track was to compare the effectiveness of the final negotiated Boolean query with the effectiveness of ranked retrieval systems. Hummingbird generously agreed to submit for our use as a baseline Boolean run the retrieved sets resulting from directly executing the negotiated Boolean query (with only a few format corrections, as described in the Open Text<ref type="foot" coords="5,417.66,626.53,3.24,5.83" target="#foot_1">2</ref> team's paper). This run was not counted as an official submission of the Hummingbird team, but rather as a track baseline. We then drew a stratified sample <ref type="bibr" coords="5,207.97,651.60,65.10,9.02" target="#b6">(Cochran, 1977;</ref><ref type="bibr" coords="5,275.57,651.60,52.72,9.02" target="#b10">Lewis, 1996)</ref> from the set of documents retrieved by the negotiated Boolean query for each topic in order to support unbiased estimation of certain evaluation measures for these sets.</p><p>Stratification was done by assigning each document from a baseline Boolean set to one of three strata based on whether and how that document occurred in the 31 official submitted runs. The three strata were:</p><p>• Stratum 1 (documents occurring in the top 5,000 for at least one official run submitted by each of two or more of the six participating sites), • Stratum 2 (documents occurring in the top 5,000 for one or more official runs from exactly one of the six participating sites), and • Stratum 3 (documents not occurring in the top 5,000 for any official run submitted by any participating site).</p><p>For each topic, NIST drew a simple random sample of 100 documents from Stratum 1, 50 from Stratum 2, and 50 from Stratum 3 to add to the pool for that topic. When a stratum was exhausted, leftover documents were drawn from the other strata in proportion to their original allocation. Using different stratification strategies for different topics could have improved our estimates, but would have complicated the sampling procedure. An unexpected downside of the above stratification was that Stratum 3 often turned out to be empty. This may have resulted from use of terms from the negotiated Boolean query by ranked retrieval systems, which was allowed (and, indeed, encouraged) by the track guidelines.</p><p>One participating team, Hummingbird, leveraged the track's approach to constructing assessment pools (which was known by the participants) to do their own stratified sampling experiment. Their main run (humL06tvz) actually drew documents from various depths of a standard ranked run, enabling them to compute unbiased estimates of precision (and the number of unjudged relevant documents) to depth 9,000. Details can be found in their paper <ref type="bibr" coords="6,230.80,363.97,73.26,9.02">(Tomlinson, 2006)</ref>. This strategy almost certainly increased the diversity of the assessed pools (at the cost of some richness in relevant documents) by increasing the number of lower-ranked documents assessed. It also invalidated our computation of standard evaluation measures for that run (which are shown in Figure <ref type="figure" coords="6,289.73,398.48,5.01,9.02" target="#fig_0">3</ref> only for completeness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relevance Judgment Process</head><p>A total of 35 volunteers from government, law firms, legal technology firms, and law schools (plus two unaffiliated individual volunteers) assessed a total of 32,738 documents in the judgment pools for 40 of the topics. Due to lack of assessment capacity, no assessments were performed for the three remaining topics, and they were thus removed from the evaluation. The volunteers included eight lawyers, ten law students (with 1 st , 2 nd and 3 rd year students all represented), three paralegals with substantial legal experience, one professional archivist, one historian, and several individuals with degrees with science or finance. The affiliations of volunteers for primary assessments were the National Archives and Records Administration (8 topics), George Washington University Law School (D.C., 8 topics), H5 Technologies Inc. (San Francisco, 7 topics), Lewis &amp; Roca LLP (Phoenix, 4 topics), Preston Gates LLP (Seattle, 3 topics), Bank of America (Charlotte, 2 topics), FTI Consulting, (New York City, 2 topics), one topic each by George Mason University School of Law (Virginia), Reasonable Discovery LLC (Virginia), New Mexico State Attorney General's Office, and one topic each from three private individuals (in Florida, California, and the U.K.).</p><p>The assessors used a beta version of a Web-based platform to view the scanned MSA documents and record their relevance judgments. (The platform was designed by David D. Lewis Consulting, and implemented by Smokescreen Consulting, as part of the IIT CDIP project.) We provided the assessors with a "How To Guide" <ref type="bibr" coords="6,168.38,633.06,113.60,9.02" target="#b2">(Baron, Lewis &amp; Oard, 2006</ref>) that explained that the project was modeled on the ways in which lawyers make and respond to real requests for documents, including in electronic form. Assessors were told to assume that they had been requested by a senior partner, or hired by a law firm or another company, to review a set of documents for "relevance." No special, comprehensive knowledge of the matters discussed in each complaint was expected (e.g., no need to be an expert in federal election law, product liability, etc.). The heart of the exercise was to look for relevant and nonrelevant documents within a topic. Relevance, consistent with all known legal definitions from Wigmore to Wikipedia, was to be defined broadly. Specifically, assessors were instructed that a document should be considered relevant when the reference to the topic was found in the document. Assessors were reminded that a document may be relevant even if it fails to contain any of the words in the topic request, and conversely, that a document may end up being considered not relevant despite containing one or more words from the topic request. Assessors were also informed that for some topics, the document type would circumscribe the scope of the topic (e.g., all internal memoranda of a company on topic x), and that (for a very few topics) the scope might be limited by a specified date span (e.g., all documents created in 1992). Relevance judgments were to be recorded as a binary value (yes or no), although a third "unsure" category was also available in the assessment platform.</p><p>The first phase of assessment (the only phase initially planned) began on August 7, 2006, and was completed on September 15, 2006. This was the first time that distributed assessment of document images had been used in TREC, and a few complications unsurprisingly arose. It became apparent during assessment that the collection contained some extremely long documents (e.g. a 3,500 page card catalog) and that the participating systems had retrieved a disproportionate number of these long documents. The assessment guidelines were changed in mid-August to allow assessors to mark documents longer than 300 pages as "unsure" if their relevance could not be determined by examining the available metadata and a few pages of the document. Documents marked as unsure were treated as not relevant. When surveyed after completion of their work, some assessors suggested that graded relevance judgments be supported in future years, so as to distinguish between mere "passing references" to a topic (which were recorded as relevant for this year's track) and documents that materially or substantively discuss a topic (which were also recorded as relevant this year).</p><p>Some of the assessors went beyond the text of the topic (the complaint, the production request, and the Boolean queries) to perform additional legal research which they viewed as helpful to the exercise. For example, the assessor for Topic 30 researched at greater length what the numbered statutory code provisions were corresponding to the California Cartwright Act, to ensure that all documents containing such references, with or without reference to the Cartwright Act itself, would be marked as responsive.</p><p>The assessor on Topic 10 performed independent research into the ban on tobacco advertising, as an aid to understanding what documents might be expected to be found in response to a topic involving tobacco product placement in television or film. One assessor asked for assistance on the definition of one of the keywords in the topic, leading to additional research conducted on the Internet. Some differences were observed in how liberally or narrowly assessors viewed the scope of their discretion to find responsiveness. In some exceptional cases, assessors were willing to find responsiveness even where a key term might be missing, if the document was otherwise sufficiently generic and might yet be viewed as responsive with the aid of further research. For example, the assessor for Topic 9 ("All documents discussing, referencing or relating to payment of compensation to 20 th Century Fox Corporation for placement of products and/or brands in a film production") marked certain documents as relevant even if the film company was not expressly mentioned, where the context indicated that the company might be involved. In most cases, however, assessors appeared to adopt relatively restrictive interpretations on what met the mark for relevance.</p><p>Assessors reported some confusion as to whether they should exclude documents that might be within the literal scope of a production request when read in isolation, but which weren't relevant to the main thrust of the associated complaint (i.e., the document had nothing to do with the causes of action in the lawsuit or investigation). The question of scope arose in particular for production requests associated with the one complaint that on its face did not involve allegations against the tobacco industry (but which was instead about medical devices). Topic 49, which coupled that complaint with a production request for "[a]ll documents created between 1962 and 1999 referencing or including warnings or draft warnings used in the United States," proved to be particularly problematic because it was read by the assessor as being aimed at warnings for faulty medical devices. Not surprisingly, no relevant documents were found for topic 49. It was therefore removed from the evaluation because topics with no known relevant documents can not be used to compare the effectiveness of alternative system designs. Results are therefore reported for the remaining 39 topics.</p><p>As is often the case, assessors found some unintended ambiguity in the topics, either due to grammatical construction of the topic (e.g., what did the word "their" refer to), or due to inherent ambiguity embedded within words or concepts (e.g., what constitutes "lobbying efforts," "advertising," "marketing," and "promotion"). For one assessor, the word "event" (in a topic asking for all documents relating to the placement of product logos at events held in California), prompted them to consult the Random House Dictionary, where the word is defined as "something that occurs in a certain place during a particular interval of time." Therefore, in this assessor's view, documents that mentioned such activities as the America's Cup Race, speed skiing, auto racing, Hispanic Cultural events, Swing jam weekend, an antiviolence campaign, a country music festival, and an anti-smoking campaign called "Tobacco is Whacko," were all properly within the scope of the topic.</p><p>Another miscellaneous concern of one or more assessors involved how to deal with documents containing foreign language text. The track coordinators instructed assessors to make judgments based on English portions of documents, or otherwise mark the document as unsure.</p><p>In general, assessors took their jobs very seriously. A number of assessors made a second pass through their document set to resolve anomalies or to revisit judgments based on knowledge gained on the first pass. Many requests were directed to the track coordinators for help in resolving technical concerns.</p><p>It turned out that a nontrivial portion of the documents in the judgment pools could not be assessed at all using the assessment platform. While the same set of UCSF XML records provided the starting point for both the IIT CDIP version 1.0 collection and for the assessment platform's database, a few records with formatting problems were inadvertently treated differently by the two groups. In addition, a substantial number of XML records with variant formatting could not be loaded until assessment was already underway. More importantly, an even larger number of documents could not have their page images displayed during much of the assessment period. The total number of documents affected was less than 5% of the total collection, although somewhat more than 5% of the assessment pools were affected because longer documents were more likely to be affected. We addressed these problems by asking assessors to view documents at the LTDL Web site (http://legacy.library.ucsf.edu/) if their images could not be viewed on the CDIP platform, and record their assessments using the CDIP platform. In a very few cases, no record at all was loaded on the CDIP platform and assessments were sent by email. Also in a very few cases document images were found to be partial or missing on the LTDL Web site as well. In those few cases, assessors were asked to make a judgment based on the metadata record if possible, or to mark the document as "unsure."</p><p>The track coordinators asked assessors to record how much time they spent in performing assessment review. Based on post-assessment survey responses and related emails, assessment time data is available for 16 participants representing 39% of the overall assessment effort (12,743 of the 32,738 assessments).</p><p>The reported review rate of documents reviewed per hour ranged from a low of 12.33 (Topic 31) to a high of 67.5 (Topic 25). The average review rate constituted 24.7 documents per hour. Note that each of the assigned topics included within it a highly varied set of documents, in terms of both differences in subject matter complexity as well as in total length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inter-Assessor Agreement</head><p>In order to assess the effects of differing assessor interpretations, we performed a limited amount of dual assessment after completion of the first phase of assessments. A sample of 50 documents (25 that had been judged as relevant, and 25 that had been judged as not relevant) was drawn from the pool for each of the 40 assessed topics. (Topic 49 was included for dual assessments, even though it could not be used for evaluating systems.) When fewer than 25 relevant documents had been identified, the number of nonrelevant documents was increased to keep the total at 50. These sets were then assessed by a different assessor, without knowledge of the previous judgments. A total of 12 volunteers assessed documents in this second round, seven first-round veterans who received new topics to review, plus five new recruits.</p><p>Figure <ref type="figure" coords="9,118.63,74.34,5.01,9.02">1</ref> shows the values of Cohen's kappa <ref type="bibr" coords="9,270.32,74.34,88.23,9.02">(Shoukri, 2004, Sec. 3</ref>.3), a chance-corrected measure of agreement, for each topic, as computed from the sample of 50 documents. Let:</p><p>n 00 = number documents judged nonrelevant by main and secondary assessor, n 01 = number documents judged nonrelevant by main, but relevant by secondary, n 10 = number documents judged relevant by main, but nonrelevant by secondary, and n 11 = number of documents judged relevant by both main and secondary assessor.</p><p>where n = n 00 + n 01 + n 10 + n 11 is for us equal to 50. To compute kappa, one first computes the observed proportion of agreement between the assessors:</p><formula xml:id="formula_0" coords="9,126.00,200.82,73.35,9.75">p o = (n 00 + n 11 ) / n</formula><p>and the proportion agreement expected by chance under the assumption the assessors make their judgments independently with their particular observed frequencies of relevant and nonrelevant:</p><p>p e = (n 00 + n 01 )(n 00 + n 10 ) /n 2 + (n 10 + n 11 )(n 01 + n 11 ) /n 2 .</p><p>Cohen's kappa is then:</p><formula xml:id="formula_1" coords="9,128.52,304.32,83.38,9.75">K = (p o -p e )/(1 -p e ) .</formula><p>The mean value of kappa over the 40 topics was +0.49, indicating moderate overall agreement between assessors (kappa ranges between -1 for complete disagreement to +1 for complete agreement), although considerable variation was evident across topics. The kappa values shown in Figure <ref type="figure" coords="9,430.03,350.34,5.01,9.02">1</ref> are based on a sample of documents with (usually) 25 documents that the main assessor judged positive, and 25 they judged negative. The kappa value would have been different if a random sample from the pool had been judged by both assessors. We can compute an approximation of what kappa on the pool would have been by treating the 50 documents as a stratified sample and computing the expected values of the four contingency table cells that go into kappa. This is not quite an unbiased estimate of what kappa would have been on the pool, since kappa is a nonlinear function of the contingency table cells, but it is a reasonable approximation. Table <ref type="table" coords="9,228.96,430.82,5.01,9.02" target="#tab_4">1</ref> (which can be found at the end of this paper) shows the raw values of the contingency table entries along with kappa and other associated statistics. Table <ref type="table" coords="9,426.89,442.35,5.01,9.02" target="#tab_5">2</ref> (also at the end of the paper) shows the stratified estimates of what the contingency table cells would be for the full pool, along with approximations to the agreement measures computed by plugging the expected values of the contingency table cells into the formula for each measure.</p><p>As Voorhees has shown, moderate inter-annotator agreement can yield comparisons that are stable when one set of assessments are substituted for the other <ref type="bibr" coords="9,294.62,511.32,65.74,9.02" target="#b16">(Voorhees 2000)</ref>. Evaluation measures should, therefore, be interpreted on a comparative rather than an absolute basis.</p><p>- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Six participating sites submitted 31 ranked runs with no more than 5,000 documents per topic. Three of those runs applied a Boolean restriction when producing the document sets-those three runs consisted of substantially fewer than 5,000 documents for some topics. The baseline Boolean run, on the other hand, was not required to be ranked (although in practice it was first subjected to the Boolean constraint and then resulting Boolean set was ranked), so no upper bound on the size of the retrieved set was imposed in that case. The actual sizes of the submitted sets for the baseline Boolean run varied from 1 to 128,195 documents across topics. In addition to these 32 runs, the sets of approximately 100 documents found by the human expert for each topic (described in Section 4.1) were scored as if they were a 33 rd run, (although as described below this comparison is not a fair one). Runs were given names beginning with an abbreviation that identified the submitting site. In this section, we briefly review the techniques used by each site; additional details can be found in the papers posted on the TREC Web site (http://trec.nist.gov).</p><p>o Hummingbird (hum). Hummingbird (now Open Text Corporation) submitted eight runs that explored the effects of alternative ways of formulating queries, different choices of index terms, and blind relevance feedback, plus the reference Boolean run (humL06B). The documents were indexed using the Livelink ECM-eDocs SearchServer system. The OCR field was indexed in every case, and all metadata was indexed together with OCR for seven runs, including the reference Boolean run (the exceptions being humL06dvo and humL06tvo). Queries were constructed automatically in six cases (the exceptions being humLo6B-the reference Boolean run, humL06t-the same run with a cutoff at 5,000, and humL06t0-a contrastive Boolean run using the first query in the negotiation history rather than the last query). For five of those six runs, the queries were automatically constructed from words in the Boolean queries (but without the use of Boolean or proximity operators); for the sixth run (humL06dvo) the queries were automatically constructed from the production request field.</p><p>o National University of Singapore (NUS). The National University of Singapore submitted two runs to explore the effects of evidence combination from multiple topic fields. The contents of the OCR field were indexed using the Lucene text retrieval system, and queries were formed from words found in the production request and the Boolean queries (but without the use of Boolean or proximity operators).</p><p>o Sabir Research (Sab). Sabir Research submitted seven runs to explore the effects of vocabulary filtering on OCR indexing and blind relevance feedback. The contents of the OCR and all metadata fields were indexed together using a vector space text retrieval system with pivoted document length normalization. Queries were formed from words in the production request and words in the Boolean Query for five of those runs; one run used only words from the production request (SabLeg06ar1) and one run used words from the production request, words from the Boolean query (without Boolean or proximity operators) and words from the Complaint (SabLeg06aa1).</p><p>o University of Maryland (Umd). The University of Maryland submitted four runs that explored the effects of different sources of query terms. The contents of the OCR and all metadata fields were indexed together using the Indri text retrieval system. Queries were formulated automatically for three runs: UmdBase (from words in the production request field), UmdBoolAuto (from words found in the final Boolean query, but without Boolean or proximity operators), and UmdComb (from both). For the fourth run (UmdBool), Indri queries were manually constructed to approximate the Boolean operators as closely as possible using Indri's query language (which does not directly support some required operators).</p><p>o University of Missouri-Kansas City (UMKC). The University of Missouri-Kansas City submitted eight runs that explored the effects of blind relevance feedback. The contents of the OCR field were indexed using the Lucene text retrieval system. Queries were formed automatically from words in the Boolean query (with Boolean operators, and sometimes also with proximity operators).</p><p>o York University (york). York University submitted two runs that explored the effects of blind relevance feedback. The contents of the OCR and all metadata fields were indexed together using Okapi BM 25 term weights. Queries were formulated automatically from words found in the Boolean query negotiation history (but without Boolean or proximity operators).</p><p>o Expert manual searcher "run" (EXPMANUAL). As described in section 4.1, the expert manual searcher used an interactive search system to identify up to 100 documents per topic that she felt would be unlikely to be retrieved by fully automated systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Uniques Analysis</head><p>One way of characterizing the results of different approaches to searching is to examine the contribution of each approach to the total set of known relevant documents. Figure <ref type="figure" coords="11,363.62,469.92,5.01,9.02">2</ref> shows one way of looking at those statistics. As the grey bars show, on average across the 39 topics, 57% of the known relevant documents<ref type="foot" coords="11,510.06,479.29,3.24,5.83" target="#foot_2">3</ref> were found by the reference Boolean query (i.e., either uniquely by the reference Boolean system, or by the reference Boolean system and also one or more other systems). As the analysis in Section 5.3 shows, our pooling strategy results in an underestimate of the actual number of relevant documents found by the reference Boolean system for topics with large numbers of relevant documents. Nevertheless, we this serves as a useful reference point from which to start an analysis of documents uniquely retrieved by other techniques.</p><p>The black bars stacked above the grey bars show the additional relevant documents found by the expert manual searcher but not by the reference Boolean system. On average across the 39 topics, the expert searcher found an additional 11% of the known relevant documents. In this case, the counts are accurate, since every document added to the pools by the expert searcher was judged. From this, we can conclude that by reformulating their query the expert searcher was able to find a substantial number of relevant documents that were not found by the reference Boolean system.</p><p>The white bars stacked above the black and grey bars show the additional relevant documents that were found by some system other then the reference Boolean system or the expert manual searcher. On average across the 39 topics, these other systems found an additional 32% of the known relevant documents. Our pooling strategy, which focuses on documents near the top of at least one ranked list and which includes no more than 100 documents from any one system, likely underestimates the number of relevant documents that ranked retrieval systems can find. Indeed, results for the "depth probe" run reported in the Hummingbird (Open Text) paper suggest that this underestimate may be substantial for at least some topics. Nonetheless, we can state with confidence that there were a large number of known relevant documents (1,417 across 39 topics) that were not found by the reference Boolean system or by the expert searcher. There was, therefore, scope for ranked retrieval systems to substantially outperform both the reference Boolean system and the expert manual searcher because there were a substantial number of known relevant documents that neither of those systems found. As we will see below, that did not happen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Known Relevant Documents</head><p>Boolean Expert Searcher Ranked Only Figure <ref type="figure" coords="12,123.38,503.89,4.18,9.27">2</ref>. Known relevant documents found by the Reference Boolean system (grey), found by the expert searcher but not the reference Boolean system (black), and found uniquely by at least one other system (white).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">R-Precision</head><p>Although our principal focus is on recall rather than precision, it is convenient to begin with a precisionoriented measures because precision-oriented measures are well understood, widely reported, and easily computed. Figure <ref type="figure" coords="12,165.54,609.90,5.01,9.02" target="#fig_0">3</ref> compares the ranked retrieval runs using mean R-precision, a precision-oriented measure computed as the average across topics of the density of relevant documents at rank R (where R is the number of known relevant documents for that topic). The seven dark bars show the best scoring run from each participating team (and from the manual searcher). For comparison, all other runs (in order: the expert manual search, the reference Boolean run, and three Boolean runs from participating teams) are shown to the left of the ranked runs. Because R-precision is focused early in the ranked list, this measure would be expected to favor ranked retrieval systems. All four Boolean runs were, however, ranked in some way after being subjected to the Boolean constraint. The result is, therefore, in some sense fair in those cases. The expert human searcher "run" is disadvantaged in this comparison, however. It consisted of only about 100 documents, those documents were not intentionally ranked by probability of relevance, and the searcher focused on finding diverse relevant documents to enrich the pool rather than the easiest relevant documents to boost measured effectiveness.</p><p>Three results are clearly evident in this data. First, the best runs from three of the participating sites were nearly indistinguishable by the R-precision measure, and one of those three runs (humL06t) was subjected to a Boolean constraint. Indeed, the reference Boolean run did about as well on this precision-oriented measure as the best unconstrained ranked retrieval runs. This is notable because Boolean runs can retrieve only documents that satisfy the Boolean query, while the ranked runs had no such constraint. From this we can conclude that (when averaged over 39 topics), little adverse effect resulted from respecting the Boolean constraint. Of course, with only six participating systems we are nowhere near exhausting the design space for search techniques, so ways may yet be found to achieve improvements that are not available to a Boolean system. All we can say at this point is that such improvements have not yet been demonstrated in the TREC legal track.</p><p>The second obvious result is that Boolean systems are not all created equal-two of the four Boolean runs did about twice as well (by this precision-oriented measure) as the other two! In one case (Hummingbird) this appears to result from using the initial rather than the final Boolean queries. In the other case (Maryland) the differences appear to result from incomplete support for extended Boolean operators.</p><p>When we first proposed this track, one of our shorthand goals was to see if someone could "beat Boolean." This year's results indicate that might be easily achieved in the wrong way (by inadvertently creating an underperforming "Boolean" baseline), and that careful attention to the process by which the Boolean queries are created and used will be important if we are to produce meaningful comparisons.</p><p>Third, the expert manual searcher's submitted sets had, despite the factors discussed above that would tend to decrease R-precision scores, noticeably higher R-precision than any of actual submitted runs (all of which were essentially fully automatic, although in a few cases some query reformatting was done manually). This suggests that focusing attention on interactive search might yield interesting results.  Runs EXPMANUAL and humL06tvz were not conventionally ranked and thus are disadvantaged by this measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">P@B</head><p>A set-oriented comparison of ranked retrieval with the reference Boolean run was possible for 22 topics for which 5,000 or fewer documents were included in the Boolean set. <ref type="foot" coords="14,357.60,145.87,3.24,5.83" target="#foot_3">4</ref> Let B be the size of the submitted set for the baseline Boolean run for a particular topic. The idea is to treat the top B documents of a ranked run for that topic as if it were a submitted set of size B and then compute P@B, the density of relevant documents in that set (treating unassessed documents as not relevant). Although the true number of relevant documents is not known, the precision at any fixed cutoff is proportional to the recall at that same cutoff, so we can interpret P@B for any individual topic as a measure of recall. Averaging across topics yields somewhat different results than a direct computation of recall would, however, since the constant of proportionality varies by topic.</p><p>In Figure <ref type="figure" coords="14,129.47,251.40,5.01,9.02" target="#fig_2">4</ref> we compare P@B values for SabLeg06ao2 (one of the top-scoring runs by P@R) with those of the baseline Boolean run. For 12 of 22 topics, P@B favors the reference Boolean run, while for 7 of 22 the ranked run is favored. Three topics had tied values of P@B that were near 0.</p><p>The above analysis understates the true value of P@B since the assessed pools are incomplete and biased in favor of documents ranked highly by submitted runs. This problem is worse for a set-based measure like P@B than for measures like R-precision that focus on the documents closest to the top of a ranked list. We had no alternative to pooling for evaluating the ranked run, but for the baseline Boolean run an unbiased estimate of P@B could be computed using stratified sampling.  It turned out that the identity of the original stratified samples (Section 4.1) from the baseline Boolean run could not be recovered at the time that evaluation measures were computed because of a hardware failure. Further, the original stratification could not be reconstructed from the pools themselves because documents meeting the strata definitions could have come from ranked runs, the expert manual run, or the stratified sampling process. However, we were able to define new strata in a way that still allowed the computation of unbiased estimates of set-based effectiveness measures for the baseline Boolean run.</p><p>We separated the documents in the baseline Boolean set for each topic into four strata based on which other runs they occurred in:</p><p>• Stratum 0': Documents occurring in the top 100 of any site's main run, top 10 of any run from any site, or in the expert manual set.</p><p>• Stratum 1': Documents in former Stratum 1, but not in Stratum 0'.</p><p>• Stratum 2': Documents in former Stratum 2, but not in Stratum 0'.</p><p>• Stratum 3': Documents in former Stratum 3, but not in Stratum 0'.</p><p>By putting all documents added to the pool by a run other than the baseline Boolean run into Stratum 0', we can treat any remaining documents as if they had been drawn randomly from the newly defined strata. Stratum 0' is treated as having all its documents sampled, while the number of documents treated as sampled from Strata 1', 2', and 3' varies by topic. We used the resulting stratified samples to produce unbiased estimates of P@B for the baseline Boolean run, as well as computing a 95% confidence interval for these estimates using the Gaussian approximation to the binomial <ref type="bibr" coords="15,368.80,353.17,54.39,9.02" target="#b10">(Lewis, 1996)</ref>. Because these new strata generally contain fewer documents than under the original stratification, our estimates of P@B usually have a higher sampling variance than they would have with the original stratification.</p><p>As Figure <ref type="figure" coords="15,132.25,399.18,5.01,9.02" target="#fig_3">5</ref> shows, pooling and stratified sampling produce the same estimate of P@B when B is at or below 267. The situation is quite different as B grows, however. In 10 of the 16 cases for which B is 528 or higher, and for which the pooled estimate of P@B is nonzero, the pooled estimate falls below the lower limit of the confidence interval on the stratified estimate. This result reinforces our earlier that our poolbased effectiveness measures do not provide a measure of the absolute effectiveness of any of the participating systems. Further, the large gap between the pool-based P@B and the true value (or at least an unbiased estimate of it) means more danger that biases in pool construction will affect even comparisons of relative effectiveness.</p><p>Analysis reported in the Hummingbird (Open Text) paper indicates that similar effects are present in at least the one ranked "depth probe" run for which a kind of stratified sampling was done (humL06tvz). Our future work on comparison of ranked and Boolean runs will require a more nuanced strategy than we have yet applied.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This first year of the TREC Legal Track has produced a new test collection that models present practice in e-discovery, and that will also be of interest to researchers working on retrieval from scanned document images and to researchers working on the integrated use of structured metadata with document text as a basis for retrieval. Six research teams participated in the evaluation, contributing to the creation of relevance assessment pools that were judged in a manner representative of the human review process that precedes release in an e-discovery process. These judgments provide a basis for both this year's evaluation and for development of new approaches that are tuned to the unique characteristics of this task.</p><p>Analysis of the results yielded a number of useful insights. Perhaps the most striking result is the strong performance of the Boolean queries. The reference Boolean run did about as well (by R-precision) as the best ranked runs, and the top seven ranked runs (again, by R-precision) all used terms from the Boolean queries as part of their automatic query formulation process. This suggests that the negotiated Boolean queries are information-rich, which has implications both for practice (propounding Boolean queries is a productive activity) and for system design (leveraging manually constructed Boolean queries when they are available can yield improved retrieval effectiveness). A second important result is objectively quantifying the fact that there are many relevant documents to be found beyond those identified by strict application of negotiated Boolean queries. This should not be surprising, of course, since it is well known that formulating queries that are both sufficiently inclusive and sufficiently precise is difficult. Perhaps the most important implication of this observation is that exploring system designs based on relaxation of the Boolean query and based on augmenting queries using terms from other sources (e.g., the production request) may ultimately yield better retrieval effectiveness than strict application of Boolean logic. While that potential was not realized in the TREC 2006 legal track (at least not by the P@R measure), this year's relevance judgments are exactly what is needed to explore the space of possible system designs to determine whether such gains can indeed be achieved.</p><p>From the perspective of evaluation design, the clearest conclusion is that additional work on statistical estimation for recall-oriented measures is needed. The analyses in this paper and in the Open Text paper indicate that statistical estimates of retrieval effectiveness for both the reference Boolean run and for one ranked run yield markedly different results from the more commonly used metrics in which unassessed documents are treated as not relevant. Additional analysis will be needed before we can directly compare those two runs, and the potential for statistical estimation for other ranked retrieval runs from 2006 is limited by the sampling strategies that were employed when forming the assessment pools. It will therefore be important to revisit both our choice of measures and our sampling strategies for the 2007 Legal Track.</p><p>Our focus in this first year of the Legal Track was on the design of automated systems, but of course automated systems are ultimately used by people. Our expert searcher run yielded some interesting insights, however, finding an average of 13 documents per topic that the reference Boolean query had missed and achieving better retrieval effectiveness (by the P@R measure) than any other run. This suggests that a focused effort to explore interactive search techniques in the TREC 2007 legal track might yield additional insights.</p><p>Perhaps the greatest accomplishment of the TREC 2006 Legal Track is that it happened at all. More than 50 volunteers contributed to assembling and distributing the collection, creating topics, developing systems, managing submissions, creating pools, judging relevance, developing metrics, creating scoring software, analyzing results, and coordinating those activities. This has yielded the results that we would hope for from any TREC track in its first year: (1) a reusable test collection to support future research, (2) a set of baseline results to which future research can be compared, and (3) a community of researchers who bring a variety of perspectives to these important challenges. The coordinators trust that a second year of research will continue to yield important results. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="13,90.00,686.52,406.41,9.02;13,90.00,698.04,417.75,9.02"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Mean precision at R (the actual number of known relevant documents for each topic). Ranked runs on left side, Reference runs on right side. Best run for each team shown as solid bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,90.00,635.35,420.55,9.27;14,90.00,646.87,215.94,9.27"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Recall-oriented effectiveness measure, by topic, in increasing order of Boolean set size. Topic 33 (for which B=1) not shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,103.14,344.23,405.55,9.27"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of stratified estimate of P@B with pool-based estimate of P@B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="19,90.00,89.01,430.96,617.39"><head>Table 1 :</head><label>1</label><figDesc>Raw contingency table entries from interassessor comparison study.</figDesc><table coords="19,92.58,89.01,398.53,547.53"><row><cell>Topic</cell><cell>n</cell><cell cols="3">n11 n01 n10 n00</cell><cell>Agree</cell><cell>Agree R</cell><cell>Agree N</cell><cell>Kappa</cell></row><row><cell>6</cell><cell>50</cell><cell>0</cell><cell cols="2">0 25 25</cell><cell>0.5</cell><cell>0</cell><cell>0.667</cell><cell>0</cell></row><row><cell>7</cell><cell>50</cell><cell>21</cell><cell>4</cell><cell>4 21</cell><cell>0.84</cell><cell>0.84</cell><cell>0.84</cell><cell>0.68</cell></row><row><cell>8</cell><cell>50</cell><cell>19</cell><cell>2</cell><cell>6 23</cell><cell>0.84</cell><cell>0.826</cell><cell>0.852</cell><cell>0.68</cell></row><row><cell>9</cell><cell>49</cell><cell>9</cell><cell cols="2">0 16 24</cell><cell>0.673</cell><cell>0.529</cell><cell>0.75</cell><cell>0.355</cell></row><row><cell>10</cell><cell>50</cell><cell>2</cell><cell>0</cell><cell>3 45</cell><cell>0.94</cell><cell>0.571</cell><cell>0.968</cell><cell>0.545</cell></row><row><cell>13</cell><cell>50</cell><cell>11</cell><cell cols="2">0 14 25</cell><cell>0.72</cell><cell>0.611</cell><cell>0.781</cell><cell>0.44</cell></row><row><cell>14</cell><cell>50</cell><cell>11</cell><cell cols="2">4 14 21</cell><cell>0.64</cell><cell>0.55</cell><cell>0.7</cell><cell>0.28</cell></row><row><cell>17</cell><cell>50</cell><cell>4</cell><cell>0</cell><cell>0 46</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>18</cell><cell>50</cell><cell>20</cell><cell>7</cell><cell>5 18</cell><cell>0.76</cell><cell>0.769</cell><cell>0.75</cell><cell>0.52</cell></row><row><cell>19</cell><cell>50</cell><cell>8</cell><cell cols="2">0 17 25</cell><cell>0.66</cell><cell>0.485</cell><cell>0.746</cell><cell>0.32</cell></row><row><cell>20</cell><cell>50</cell><cell>7</cell><cell cols="2">1 18 24</cell><cell>0.62</cell><cell>0.424</cell><cell>0.716</cell><cell>0.24</cell></row><row><cell>21</cell><cell>50</cell><cell>5</cell><cell cols="2">4 20 21</cell><cell>0.52</cell><cell>0.294</cell><cell>0.636</cell><cell>0.04</cell></row><row><cell>22</cell><cell>50</cell><cell>5</cell><cell cols="2">0 20 25</cell><cell>0.6</cell><cell>0.333</cell><cell>0.714</cell><cell>0.2</cell></row><row><cell>23</cell><cell>50</cell><cell>9</cell><cell cols="2">4 16 21</cell><cell>0.6</cell><cell>0.474</cell><cell>0.677</cell><cell>0.2</cell></row><row><cell>24</cell><cell>50</cell><cell>0</cell><cell>1</cell><cell>9 40</cell><cell>0.8</cell><cell>0</cell><cell>0.889</cell><cell>-0.037</cell></row><row><cell>25</cell><cell>50</cell><cell>7</cell><cell>6</cell><cell>5 32</cell><cell>0.78</cell><cell>0.56</cell><cell>0.853</cell><cell>0.414</cell></row><row><cell>26</cell><cell>50</cell><cell>21</cell><cell>5</cell><cell>4 20</cell><cell>0.82</cell><cell>0.824</cell><cell>0.816</cell><cell>0.64</cell></row><row><cell>27</cell><cell>50</cell><cell>22</cell><cell>2</cell><cell>3 23</cell><cell>0.9</cell><cell>0.898</cell><cell>0.902</cell><cell>0.8</cell></row><row><cell>28</cell><cell>50</cell><cell>21</cell><cell>1</cell><cell>4 24</cell><cell>0.9</cell><cell>0.894</cell><cell>0.906</cell><cell>0.8</cell></row><row><cell>29</cell><cell>50</cell><cell>16</cell><cell>1</cell><cell>1 32</cell><cell>0.96</cell><cell>0.941</cell><cell>0.97</cell><cell>0.911</cell></row><row><cell>30</cell><cell>50</cell><cell>22</cell><cell>2</cell><cell>3 23</cell><cell>0.9</cell><cell>0.898</cell><cell>0.902</cell><cell>0.8</cell></row><row><cell>31</cell><cell>50</cell><cell>23</cell><cell>6</cell><cell>2 19</cell><cell>0.84</cell><cell>0.852</cell><cell>0.826</cell><cell>0.68</cell></row><row><cell>32</cell><cell>50</cell><cell>20</cell><cell>7</cell><cell>5 18</cell><cell>0.76</cell><cell>0.769</cell><cell>0.75</cell><cell>0.52</cell></row><row><cell>33</cell><cell>50</cell><cell>0</cell><cell cols="2">0 25 25</cell><cell>0.5</cell><cell>0</cell><cell>0.667</cell><cell>0</cell></row><row><cell>34</cell><cell>50</cell><cell>20</cell><cell>4</cell><cell>5 21</cell><cell>0.82</cell><cell>0.816</cell><cell>0.824</cell><cell>0.64</cell></row><row><cell>35</cell><cell>50</cell><cell>14</cell><cell cols="2">1 11 24</cell><cell>0.76</cell><cell>0.7</cell><cell>0.8</cell><cell>0.52</cell></row><row><cell>36</cell><cell>50</cell><cell>9</cell><cell>3</cell><cell>4 34</cell><cell>0.86</cell><cell>0.72</cell><cell>0.907</cell><cell>0.627</cell></row><row><cell>37</cell><cell>50</cell><cell>14</cell><cell cols="2">2 11 23</cell><cell>0.74</cell><cell>0.683</cell><cell>0.78</cell><cell>0.48</cell></row><row><cell>38</cell><cell>50</cell><cell cols="2">17 12</cell><cell>8 13</cell><cell>0.6</cell><cell>0.63</cell><cell>0.565</cell><cell>0.2</cell></row><row><cell>39</cell><cell>50</cell><cell>15</cell><cell>4</cell><cell>3 28</cell><cell>0.86</cell><cell>0.811</cell><cell>0.889</cell><cell>0.7</cell></row><row><cell>40</cell><cell>50</cell><cell>1</cell><cell>2</cell><cell>0 47</cell><cell>0.96</cell><cell>0.5</cell><cell>0.979</cell><cell>0.485</cell></row><row><cell>41</cell><cell>50</cell><cell>1</cell><cell>0</cell><cell>0 49</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>43</cell><cell>50</cell><cell>10</cell><cell cols="2">4 15 21</cell><cell>0.62</cell><cell>0.513</cell><cell>0.689</cell><cell>0.24</cell></row><row><cell>44</cell><cell>50</cell><cell>12</cell><cell cols="2">0 13 25</cell><cell>0.74</cell><cell>0.649</cell><cell>0.794</cell><cell>0.48</cell></row><row><cell>45</cell><cell>50</cell><cell>19</cell><cell>0</cell><cell>6 25</cell><cell>0.88</cell><cell>0.864</cell><cell>0.893</cell><cell>0.76</cell></row><row><cell>46</cell><cell>50</cell><cell>8</cell><cell cols="2">0 17 25</cell><cell>0.66</cell><cell>0.485</cell><cell>0.746</cell><cell>0.32</cell></row><row><cell>47</cell><cell>50</cell><cell>4</cell><cell>3</cell><cell>2 41</cell><cell>0.9</cell><cell>0.615</cell><cell>0.943</cell><cell>0.558</cell></row><row><cell>49</cell><cell>50</cell><cell cols="2">0 32</cell><cell>0 18</cell><cell>0.36</cell><cell>0</cell><cell>0.529</cell><cell>0</cell></row><row><cell>50</cell><cell>50</cell><cell>19</cell><cell>3</cell><cell>6 22</cell><cell>0.82</cell><cell>0.809</cell><cell>0.83</cell><cell>0.64</cell></row><row><cell>51</cell><cell>50</cell><cell>24</cell><cell>1</cell><cell>1 24</cell><cell>0.96</cell><cell>0.96</cell><cell>0.96</cell><cell>0.92</cell></row><row><cell>MEAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.765</cell><cell>0.627</cell><cell>0.810</cell><cell>0.490</cell></row></table><note coords="19,90.00,667.64,398.53,11.15;19,90.00,681.44,430.96,11.15;19,90.00,695.24,106.32,11.15"><p>We show agreement, i.e. (n00 + n11)/n, agreement on relevant, i.e. 2*n11 / (2*n11 + n01 + n10), agreement on nonrelevant, i.e. 2*n00 / (2*n00 + n01 + n10), and Cohen's kappa.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="20,90.00,667.64,422.85,38.75"><head>Table 2 :</head><label>2</label><figDesc>Stratified estimates of what the interassessor agreement contingency table values would be on the full pools, along with approximate expected values of agreement, agreement on relevant, agreement on nonrelevant, and kappa.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,96.36,687.55,127.29,9.31"><p>http://legacy.library.ucsf.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,96.36,699.07,416.30,9.31;5,90.00,710.53,230.11,9.31"><p>Hummingbird was acquired by Open Text Corporation in October 2006. Hence the Open Text Corporation paper describes the Hummingbird runs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="11,96.36,676.03,399.57,9.31;11,90.00,687.55,422.32,9.31;11,90.00,699.07,409.06,9.31;11,90.00,710.53,260.20,9.31"><p>In this section, and through the paper, the "known relevant documents" that we refer to are those judged as relevant by the primary assessor. Documents identified as relevant only by the second assessor in the inter-annotator agreement studies were not treated as relevant in the uniques analysis or when computing effectiveness metrics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="14,96.36,687.55,403.24,9.31;14,89.99,699.07,427.63,9.31;14,89.99,710.53,252.34,9.31"><p>There were actually 23 topics with B≤5,000, but using topic 33, for which B=1, would not be informative because when B=1 precision can only be 0 or 1. Precision at B for topic 33 was 0 for the reference Boolean run, and 1 for the best ranked run.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This track would not have been possible without the generous support of the <rs type="funder">IIT</rs> <rs type="projectName">CDIP</rs> project (including <rs type="person">Gady Agam</rs>, <rs type="person">Shlomo Argamon</rs>, <rs type="person">Ophir Frieder</rs>, and <rs type="person">Dave Grossman</rs>, plus special thanks to <rs type="person">David Roberts</rs>), the <rs type="institution">University of California at San Francisco Library</rs> (particularly <rs type="person">Karen Butter</rs>, <rs type="person">Albert Jew</rs>, <rs type="person">Kirsten Neilsen</rs>, and <rs type="person">Heidi Schmidt</rs>), members of the <rs type="institution">Sedona Conference®</rs>, and the volunteer relevance assessors and their participating firms and institutions. In particular, the coordinators wish to thank <rs type="person">Ryan Bilbrey</rs>, <rs type="person">Conor Crowley</rs>, <rs type="person">Joe Looby</rs>, and <rs type="person">Stephanie Mendelsohn</rs>, for their greatly appreciated assistance in writing draft complaints, in topic development, and for participating in "Boolean negotiations," and <rs type="person">Anna Marshall</rs> at <rs type="affiliation">George Washington University School of Law</rs> for her extraordinary assessor recruitment efforts. Special acknowledgement is due <rs type="person">Richard Braman</rs>, Executive Director of The Sedona Conference ®, for all of his assistance in facilitating a successful outcome of year 1 of the Legal Track. Thanks also to <rs type="person">Michael Tacelosky</rs> and <rs type="funder">Keith Ivey of Tobacco Documents Online</rs> and smokefree.net for access to and help with their collection of tobacco documents and for their work on our assessment platform. Finally, special thanks also go to our colleagues at <rs type="institution">NIST</rs> for handling much of the logistics.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_4S4uPSA">
					<orgName type="project" subtype="full">CDIP</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="17,90.00,555.00,403.08,9.02;17,90.00,566.52,421.32,9.02;17,90.00,577.98,235.47,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,362.78,555.00,130.31,9.02;17,90.00,566.52,394.58,9.02">Complex Document Information Processing: Prototype, Test Collection, and Evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,490.79,566.52,20.53,9.02;17,90.00,577.98,48.84,9.02">SPIE Proceedings</title>
		<imprint>
			<biblScope unit="volume">6067</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">60670</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Document Recognition and Retrieval XIII</note>
</biblStruct>

<biblStruct coords="17,90.00,601.02,425.31,9.02;17,90.00,612.48,406.36,9.02;17,90.00,624.00,31.72,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,135.94,601.02,379.36,9.02;17,90.00,612.48,58.96,9.02">Toward a Federal Benchmarking Standard for Evaluating Information Retrieval Products Used in E-Discovery</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,160.32,612.48,109.92,9.02">Sedona Conference Journal</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
	<note>available from Westlaw and LEXIS</note>
</biblStruct>

<biblStruct coords="17,90.00,646.98,385.54,9.02;17,90.00,658.50,425.90,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,230.94,646.98,216.05,9.02">Guide for Assessors -TREC Legal Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/TRECLegal_HowToGuide_Version4Final.doc" />
	</analytic>
	<monogr>
		<title level="j" coord="17,90.00,658.50,31.72,9.02">Version</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2006-08-20">2006. Aug. 20, 2006</date>
		</imprint>
	</monogr>
	<note>&apos; &quot;How To</note>
</biblStruct>

<biblStruct coords="17,90.00,681.48,416.50,9.02;17,90.00,693.00,242.82,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,197.77,681.48,308.73,9.02;17,90.00,693.00,27.30,9.02">An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,128.88,693.00,113.02,9.02">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="299" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,74.34,426.95,9.02;18,89.99,85.86,414.64,9.02;18,90.00,97.32,147.29,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,324.64,74.34,121.17,9.02">Bias and the Limits of Pooling</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,466.97,74.34,49.98,9.02;18,89.99,85.86,414.64,9.02;18,90.00,97.32,35.25,9.02">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="619" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,120.36,415.49,9.02;18,90.00,131.82,372.60,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,218.08,120.36,113.29,9.02">Retrieval System Evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,354.96,120.36,150.54,9.02;18,90.00,131.82,174.80,9.02">TREC: Experiment and Evaluation in Information Retrieval, E. M. Voorhees and</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="53" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,154.86,350.33,9.02" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Sampling</forename><surname>Cochran</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Techniques</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>rd edition</note>
</biblStruct>

<biblStruct coords="18,90.00,177.84,298.57,9.02" xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Coleman</forename><forename type="middle">V Morgan</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,229.78,177.84,15.60,9.02">WL</title>
		<imprint>
			<biblScope unit="volume">679071</biblScope>
			<date type="published" when="2005-03-01">2005. Mar. 1, 2005</date>
			<pubPlace>Fla. Cir. Ct</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,200.82,389.87,9.02;18,90.00,212.34,419.39,9.02;18,90.00,223.80,116.43,9.02;18,90.00,235.32,429.16,9.02;18,90.00,246.84,10.05,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,257.53,200.82,222.34,9.02;18,90.00,212.34,379.49,9.02">Advancing Information Sharing, Access, Discovery and Assimilation of Diverse Digital Collections Governed by Heterogeneous Sensitivities, held Nov</title>
		<ptr target="http://colab.cim3.net/cgi-bin/wiki.pl?AdvancingInformationSharing_DiverseDigitalCollections_HeterogeneousSensitivities_11_08_05" />
	</analytic>
	<monogr>
		<title level="m" coord="18,90.00,200.82,160.75,9.02">Collaborative Expedition Workshop #45</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,269.82,416.98,9.02;18,90.00,281.34,388.82,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,140.63,269.82,348.14,9.02">GE in TREC-2: Results of a Boolean Approximation Method for Routing and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,90.00,281.34,193.58,9.02">The Second Text Retrieval Conference (TREC-2)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-08">August, 1993</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,304.32,428.33,9.02;18,90.01,315.84,134.23,9.02" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,139.06,304.32,112.04,9.02">The TREC-5 Filtering Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,272.34,304.32,184.11,9.02">The Fifth Text Retrieval Conference (TREC-5)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-11">November, 1996</date>
			<biblScope unit="page" from="75" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,338.82,421.14,9.02;18,89.99,350.34,418.85,9.02;18,89.99,361.80,411.40,9.02" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="18,405.21,338.82,105.93,9.02;18,89.99,350.34,189.75,9.02">Building a Test Collection for Complex Document Information Processing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,303.35,350.34,205.49,9.02;18,89.99,361.80,296.85,9.02">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="665" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,384.84,424.88,9.02;18,90.00,396.30,333.52,9.02" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,206.21,384.84,232.51,9.02">Boolean Systems Revisited: Its Performance and Behavior</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,449.88,384.84,65.00,9.02;18,90.00,396.30,125.00,9.02">The Fourth Text Retrieval Conference (TREC-4)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-11">November, 1995</date>
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,419.34,426.15,9.02;18,90.00,430.80,405.74,9.02;18,90.00,453.78,428.41,9.02;18,90.00,465.30,265.27,9.02;18,90.00,476.82,296.03,9.02" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="18,251.06,419.34,265.09,9.02;18,90.00,430.80,39.15,9.02">Building Digital Tobacco Document Libraries at the University of California</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Butter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rider</surname></persName>
		</author>
		<ptr target="http://www.thesedonaconference.org/content/miscFiles/publications_html" />
	</analytic>
	<monogr>
		<title level="m" coord="18,135.57,430.80,232.88,9.02;18,90.00,453.78,428.41,9.02;18,90.00,465.30,132.77,9.02">Sedona Conference, The Sedona Principles: Best Practices Recommendations &amp; Principles for Addressing Electronic Document Production</title>
		<imprint>
			<date type="published" when="2002">2002. 2005</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>San Francisco Library/Center for Knowledge Management. version</note>
</biblStruct>

<biblStruct coords="18,90.00,499.80,417.96,9.02;18,90.00,511.32,416.80,9.02;18,90.01,522.78,200.66,9.02" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="18,138.49,499.80,360.54,9.02">Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,90.00,511.32,416.80,9.02;18,90.01,522.78,85.54,9.02">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,90.00,545.82,327.21,9.02" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="18,110.34,545.82,129.78,9.02">Federal Rules of Civil Procedure</title>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006-12-01">Dec. 1, 2006</date>
		</imprint>
	</monogr>
	<note>Rules 26 &amp; 34, as amended</note>
</biblStruct>

<biblStruct coords="18,90.00,568.80,404.61,9.02;18,90.00,580.32,254.46,9.02;18,90.00,603.30,242.00,9.02" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="18,151.15,568.80,335.35,9.02">Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,90.00,580.32,164.51,9.02">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">217</biblScope>
			<date type="published" when="2000">2000</date>
			<pubPlace>Warburg</pubPlace>
		</imprint>
	</monogr>
	<note>R.D. 309 (S.D.N.Y. 2004</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
