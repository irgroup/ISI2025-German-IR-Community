<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,208.44,154.51,194.31,15.53">UMass at TREC ciQA 2006</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_grn5KSS">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.00,186.94,95.57,10.99"><forename type="first">Giridhar</forename><surname>Kumaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.20,186.94,62.95,10.99"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,208.44,154.51,194.31,15.53">UMass at TREC ciQA 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C2FE4803654D7786222686CBFD0F0B40</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The characteristics of the ciQA Track namely the short templated queries and the scope for user interaction were the motivating factors for our interest in participating in the track. Templated queries represent a new paradigm of information-seeking more suited for specialized tasks. While work has been done in document retrieval for templated queries as part of the Global Autonomous Language Exploitation 1 (GALE) program, the retrieval of snippets of information in lieu of documents was an interesting challenge. We also utilized the opportunity to try a suite of minimally interactive techniques, some of which helped and some did not. We believe we have a reasonable understanding of why some approaches worked while other failed, and contend that more experimentation and analysis is necessary to tease out various interaction effects between the suite of approaches we tried.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline Runs</head><p>We used version 2.3.2 of the Indri search engine, developed as part of the Lemur 2 project as the foundation for creating our baseline QA system. While the inference network-based retrieval framework of Indri permits the use of structured queries, the use of language modeling techniques provides better estimates of probabilities for query evaluation. We used the query-likelihood variant of statistical language mod-1 http://www.darpa.mil/ipto/programs/gale/ 2 http://www.lemurproject.org eling. Given a query Q = q 1 q 2 q 3 . . . q n , and a document</p><formula xml:id="formula_0" coords="1,385.44,322.53,79.08,10.33">D = d 1 d 2 d 3 . . . d n ,</formula><p>the probability P (Q|D) that the query would be generated by the document is</p><formula xml:id="formula_1" coords="1,310.56,377.08,175.67,77.23">P (Q|D) = n j=1 P (q j |D) with P M L (q j |D) = c(q j ; D) n i=1 c(w i ; D)</formula><p>where c(q i ; D) represents the number of times that term q i occurs in document D and M L refers to maximum likelihood.</p><p>The pseudo-relevance feedback mechanism we used is based on relevance models <ref type="bibr" coords="1,441.85,508.77,9.98,9.96" target="#b2">[3]</ref>.</p><p>Our QA system derived from Indri was composed of two stages. While the first stage converted the templated queries into Indri language queries and extracted relevant passages, the second stage worked at a lower level to identify relevant snippets. The snippets we found were primarily relevant sentences.</p><p>We now present the series of steps we followed to create our baseline QA system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stage I</head><p>1. Phrase identification: The first step was to identify phrases of length two in the query, both in the fields of the template as well as in the narrative. To do this, we used the expressionCnt operator available in the Indri query language. By checking if every adjacent pair of terms in the query occurred in the corpus at least once within a term window of size three, we identified potentially useful bigrams. Phrases are known to be precision enhancing, and our intention was to use the identified phrases to retrieve vital nuggets. The identified bigrams were included in the query using operators that specified that the constituent terms must occur adjacent to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Narrative processing:</head><p>The narrative accompanying each templated query was an elaboration of the information need. However, the narrative often contained a number of terms that hampered retrieval instead of helping with it. Our simple attempted solution to this was to consider only terms in the narrative that occurred within a window of five terms, of terms that appeared in the template fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Passage Retrieval and Processing:</head><p>With the exception of Template 2, for which the goods field was weighted twice as much as the terms in other fields, we weighted all the terms from the template and narrative equally to create a query.</p><p>The query was further modified to retrieve passages of length 200 from the collection. Using the passage-specific query, we retrieved the top 10 passages. We observed from training data that often the passages retrieved were lead lines of documents, and contained a lot of extraneous information that contributed noise to the final output. To oversome this problem, we utilized the fact that lead lines are mostly capitalized, and created a simple filter that rejected a passage from consideration for further processing if more than 50% of the terms in it were capitalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stage II</head><p>1. Sentence segmentation: Once we obtained the top passages from the previous stage, we split them into sentences using a sentence segmenter <ref type="bibr" coords="2,330.48,139.29,9.98,9.96" target="#b3">[4]</ref>. We discarded sentences of length less than 5 terms, and more than 50 terms. In the former case, the sentences conveyed too little information to be useful, while in the latter they contained too much extraneous information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sentence selection and Processing:</head><p>We re-ran the original query created in Stage I, bereft of the passage modification, against our set of candidate sentences. Once we obtained the ranked list, we used each sentence in the ranked list as a query against all sentences lower in the list. By eliminating all the sentences that have a similarity more than a particular threshold, we removed potential duplicates and redundant information. Finally, we output the remaining sentences in the order they appeared in the ranked list until the character limit was reached.</p><p>UMass fielded two baseline systems, UMASSauto1 and UMASSauto2. While UMASSauto1 was the system described above, UMASSauto2 differed in the way the query was created in Step 3 of Stage I. Using the original query, a USENET archive<ref type="foot" coords="2,479.76,411.46,3.95,4.84" target="#foot_0">3</ref> was queried to obtain the top matching USENET newsgroups. Once this was done, the query was re-issued targeting the most frequent newsgroup that appeared in response to the previous query. The 100 documents that were retrieved were used as an external relevance model to expand the original query <ref type="bibr" coords="2,499.88,484.05,9.98,9.96" target="#b1">[2]</ref>. This expanded query was used further along the baseline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interaction Runs</head><p>There has recently been great interest in utilizing annotations in data provided by the Information Extraction community for Information Retrieval. While we were unable to utilize in time annotations from the Automatic Content Extraction<ref type="foot" coords="2,463.32,613.30,3.95,4.84" target="#foot_1">4</ref> program, we attempted a step towards that direction by making use of named entities identified using BBN Identifinder <ref type="bibr" coords="3,72.00,127.29,9.98,9.96" target="#b0">[1]</ref>. To also move away from the usual attempt to identify named entities and fold them into the query in a different way, we decided to instead use them for post-retrieval snippet processing. This was motivated by the fact that we noticed that in the training data often unrelated people, organizations and locations were returned in the results. Since it is not possible to predict which named entities were unrelated, we decided to involve the user in making that decision, and use the information to clean-up the final results. To this end, as one of our interaction mechanisms, we provided the user with separate lists of people, locations, and organizations identified in the final output from our baseline run. Along with each named entity, we provided a sentence in which it occurred to provide some context to help the user make a decision whether the named entity was not relevant to the information need. We hoped that this kind of negative feedback will help improve the precision of our results.</p><p>The second interface mechanism we deployed was a simple spelling-correction system that was intended to take care of spelling variations in the corpus due to typos and cultural differences. For example, estrogen and oestrogen are accepted spellings in different countries, but no stemming algorithm places them to the same equivalence class. For each term in the query that had at most two terms in the corpus with a edit distance of one, we displayed the term along with the identified terms and asked the user to select what they considered true typos and alternate spellings. We believed that this information from the user could help improve recall in a few queries.</p><p>In our second baseline system we used USENET newsgroups as a model of external relevance information. However, the model was selected automatically. This provided another opportunity for interaction in which the user was provided the titles of USENET newsgroups from an initial retrieval along with a brief description. Given the description, the user was asked to select the newsgroup(s) they thought were most likely to discuss the query's topic. The information gleaned from this interaction had potential to improve both precision as well as recall.</p><p>While the first interactive run we submitted UMASSi1 had all the above features, the second in-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We now present the results of all our runs. Figure <ref type="figure" coords="3,328.55,448.41,5.03,9.96" target="#fig_1">1</ref> compares the two automatic runs we submitted, UMASSauto1 and UMASSauto2. We can observe that the second automatic run that used an external corpus for pseudo-relevance feedback outperformed the one that did not do so. The improvement in recall is more pronounced lower down the ranked list of snippets returned by the system. Figure <ref type="figure" coords="3,351.45,532.89,5.03,9.96">2</ref> provides a comparison between the better automatic run and the two interactive runs. The interesting thing to note is that both interactive runs performed worse than the automatic runs. This shows that users were unable to select the best news group for pseudo-relevance feedback. Even the interactive run that used a mixture of manual and automatic selection of news groups was not able to match the performance of the automatic run.</p><p>Table <ref type="table" coords="3,350.02,641.37,5.03,9.96" target="#tab_0">1</ref> contains the F-scores that each system achieved. As expected, UMASSauto2 achieved a higher F-score. The fact that recall is weighted thrice as more as precision in the formula to calculate the F-score further amplified the difference between the various systems. Table <ref type="table" coords="4,112.06,477.69,5.03,9.96" target="#tab_1">2</ref> compares the Mean Average Precision (MAP) scores of the four runs. In contrast to the drop in recall, the precision of the interactive runs is higher. We hypothesize that this is the effect of using the negative feedback provided by the users in the form of non-relevant named-entity identification. However, the problems caused by the apparent failure of newsgroup selection seemed to have dampened the effect of named-entity processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The failure of one of the interaction mechanisms and the success of another led to mixed results for the interactive runs. We designed our interaction mechanisms keeping in mind that the user should be spared from expending too much effort in understanding and answering the questions. Analysis of the time taken by the users to complete the interaction forms showed that for almost 80% of the time the users took three minutes or more to complete their responses. This shows that we still have a long way to go before we achieve our goal of minimal interaction, and believe that the actual design of the interface too plays an important role.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,317.16,296.37,212.10,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of the two automatic runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,132.03,228.54,287.47"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the F-scores for each run.</figDesc><table coords="4,72.00,132.03,228.54,265.51"><row><cell></cell><cell cols="7">Comparison of UMASSi1 and UMASSi2 with UMASSauto2</cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">UMASSi1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">UMASSi2</cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">UMASSauto2</cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell><cell>2500</cell><cell>3000</cell><cell>3500</cell><cell>4000</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Character Length</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Figure 2: Comparison of the better automatic run</cell></row><row><cell cols="5">with the two interactive runs.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">F-score</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">UMASSauto1</cell><cell>0.132</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">UMASSauto2</cell><cell>0.170</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">UMASSi1</cell><cell></cell><cell>0.150</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">UMASSi2</cell><cell></cell><cell>0.160</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,310.56,126.09,228.62,103.92"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the MAP scores for each run. The values in brackets are p-values from a two-tailed paired t-test comparing the run with UMASSauto1</figDesc><table coords="4,362.64,126.09,127.93,58.20"><row><cell></cell><cell>MAP</cell></row><row><cell>UMASSauto1</cell><cell>0.061</cell></row><row><cell>UMASSauto2</cell><cell>0.074</cell></row><row><cell>UMASSi1</cell><cell>0.074</cell></row><row><cell>UMASSi2</cell><cell>0.081 (0.067)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,325.80,657.71,100.73,7.35"><p>http://groups.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,325.80,667.19,151.40,7.35"><p>http://www.nist.gov/speech/tests/ace</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs> and in part by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> under contract number <rs type="grantNumber">HR0011-06-C-0023</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_grn5KSS">
					<idno type="grant-number">HR0011-06-C-0023</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,325.32,516.00,213.71,8.97;4,325.32,526.92,213.71,8.97;4,325.32,537.84,195.90,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,380.74,526.92,158.29,8.97;4,325.32,537.84,19.19,8.97">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,353.51,538.14,70.54,8.26">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,325.31,552.60,213.79,8.97;4,325.32,563.52,213.80,8.97;4,325.32,574.44,163.06,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,472.56,552.60,66.55,8.97;4,325.32,563.52,175.56,8.97">Simple questions to improve pseudo-relevance feedback results</title>
		<author>
			<persName coords=""><forename type="first">Giridhar</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,517.65,563.82,21.47,8.26;4,325.32,574.74,71.78,8.26">ACM SIGIR Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="661" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,325.31,589.20,213.81,8.97;4,325.32,600.12,213.69,8.97;4,325.32,611.04,58.62,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,474.61,589.20,64.52,8.97;4,325.32,600.12,64.74,8.97">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,411.78,600.42,97.21,8.26">ACM SIGIR Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,325.31,625.80,213.80,8.97;4,325.32,636.72,213.91,8.97;4,325.32,647.76,213.61,8.97;4,325.32,658.68,120.24,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,506.38,625.80,32.73,8.97;4,325.32,636.72,213.91,8.97;4,325.32,647.76,17.39,8.97">A maximum entropy approach to identifying sentence boundaries</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,361.65,648.06,177.28,8.26;4,325.32,658.98,38.46,8.26">Fifth conference on applied natural language processing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
