<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.77,83.76,416.17,15.48;1,225.38,105.68,158.95,15.48">Experiments with Document and Query Representations for a Terabyte of Text</title>
				<funder ref="#_Tng4xQq">
					<orgName type="full">E.U</orgName>
				</funder>
				<funder ref="#_UXMZ2kj">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_qzJcRuC #_GBW9eM2 #_dgp4FD7">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,266.92,138.20,63.92,10.75"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.77,83.76,416.17,15.48;1,225.38,105.68,158.95,15.48">Experiments with Document and Query Representations for a Terabyte of Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF0B4992F289FFFC812E0F7C2E98063E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As part of the TREC 2006 Terabyte track, we conducted a range of experiments investigating the effects of larger test collections for both Adhoc and known-item topics. First, we looked at the amount of smoothing required for large-scale collections, and found that the largescale collections require little smoothing. Second, we investigated the relative effectiveness of various web-centric document representations based on document-text, incoming anchor-texts, and page titles. We found that these are of little value for the Adhoc task, but can provide crucial additional retrieval cues for the Named page finding task. Third, we studied the relative effectiveness of various query representations, both short and verbose statements of the topic of request, plus an intermediate query based on the most characteristic terms in the whole topic statement. We we found that using a more verbose query leads to an improvement of retrieval effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As part of the TREC 2005 Terabyte track, we conducted a range of experiments investigating the effects of larger collections. We submitted runs for two of the Terabyte track's tasks: the Adhoc task, and the Named page finding task. In addition to the submitted runs, we also discuss postsubmission results for the efficiency task. Furthermore, we discuss a range of more extensive experiments that investigate: i) the amount of smoothing required for terabyte-scale collections; ii) the relative effectiveness of various webcentric document representations based on document-text, incoming anchor-texts, and page titles; and iii) the relative effectiveness of various query representations, both short and verbose statements of the topic of request, plus an intermediate query based on the most characteristic terms in the whole topic statement.</p><p>The rest of this paper is organized as follows. In Section 2, we detail the experimental set-up for the two tasks in the Terabyte track. In Section 3, we discuss our official submis-sions and results, broken down over the Adhoc task ( §3.2) and the Named page finding task ( §3.3). In Section 4, we zoom in on a set of experiments on the amount of smoothing for terabyte-sized collections. Followed by Section 5 where we experiment with the different document representations, and Section 6 where we experiment with different query representations. Section 7 gives an initial analysis of the relative difference of the document and query representations and the set of pooled and judged documents. Finally, we summarize our findings in Section 8.</p><p>2 Experimental Set-up 2.1 Retrieval set-up Our retrieval system is based on the Lucene engine with a number of home-grown extensions <ref type="bibr" coords="1,458.08,414.39,10.79,8.65" target="#b2">[3,</ref><ref type="bibr" coords="1,471.36,414.39,7.19,8.65" target="#b7">8]</ref>.</p><p>Indexes The Terabyte track uses the GOV2 test collection, containing 25,205,178 documents (426 Gb uncompressed). The indexing approach is similar to our earlier experiments in the TREC Web and Terabyte tracks <ref type="bibr" coords="1,472.59,476.23,10.79,8.65" target="#b3">[4,</ref><ref type="bibr" coords="1,486.25,476.23,7.47,8.65" target="#b5">6,</ref><ref type="bibr" coords="1,496.58,476.23,7.19,8.65" target="#b6">7]</ref>. We created four separate indexes for Full-text the full textual content of the documents (covering the whole collection);</p><p>Titles the text in the title tags of each document, if present (covering 86% of the collection);</p><p>Anchors the anchor-texts pointing toward the document ignoring relative links and extracting only full explicit URLs (covering 6.5% of the collection);</p><p>All anchors another anchor-texts index in which we unfold all relative links (covering 49% of the collection).</p><p>The difference between the two anchor text indexes is that the second index includes far more within-site links. In both cases, we normalized the URLs, and did not index repeated occurrences of the same anchor-text. As to tokenization, we removed HTML-tags, punctuation marks, applied casefolding, and mapped marked characters into the unmarked tokens. We used the Snowball stemming algorithm <ref type="bibr" coords="1,522.15,710.82,10.58,8.65" target="#b8">[9]</ref>. The main full document text index was created as a single, non-distributed index. The size of our full-text index is 61 Gb. Building the full-text index (including all further processing) took a massive 15 days, 6 hours, and 21 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query representations</head><p>We experimented with a variety of query representations. The main goal of the richer query representations was to target relevant pages that may not be retrieved by the standard short topic statement.</p><p>T Our first query representation is based on the short topic statement in the title field. This is the realistic approximation of end user request on current Internet search engines.</p><p>TDN By including all the fields of the topic-title, description, and narrative-we obtain a much more verbose statement of the information need.</p><p>TDN10 The verbose statement also contains generic stopwords (like function words), or specific phrases related to the search procedure (like "find documents that"). Hence, we decide to include only those terms that are most characteristic for a single topic, with reference to the whole topic set. That is, the terms that best distinguish the topic at hand from the other topics in the topic set. For this we use a variant of the parsimonious language modeling techniques <ref type="bibr" coords="2,187.37,577.55,10.58,8.65" target="#b1">[2]</ref>, and create a query by selecting the 10 terms that are most characteristic for the topic.</p><p>TDN10r The repeated occurrence of the same term in the topic may be an important indicator of its relevance.</p><p>In order to boost these terms we create an alternative query, with the same 10 terms, but now each term is repeated as often as it occurs in the entire topic statement.</p><p>Table <ref type="table" coords="2,79.27,698.87,4.98,8.65" target="#tab_0">1</ref> shows examples of the four different queries. All queries were further processed analogous to the documents.</p><p>Retrieval model For ranking, we work within the language modeling framework. Our language model is an extension to Lucene <ref type="bibr" coords="2,392.86,81.19,10.58,8.65" target="#b2">[3]</ref>, i.e., for a collection D, document d and query q:</p><formula xml:id="formula_0" coords="2,338.09,110.89,196.55,17.82">P(d|q) = P(d) • ∏ t∈q ((1 -λ) • P(t|D) + λ • P(t|d)) ,</formula><p>where</p><formula xml:id="formula_1" coords="2,368.56,153.15,67.58,22.29">P(t|d) = tf t,d<label>|d|</label></formula><formula xml:id="formula_2" coords="2,366.86,181.80,137.82,52.28">P(t|D) = doc freq(t, D) ∑ t ∈D doc freq(t , D) P(d) = |d| ∑ d ∈D |d |</formula><p>The standard value for the smoothing parameter λ is 0.15. In last year's TREC Terabyte track, we found out that the GOV2 collection requires substantially less smoothing <ref type="bibr" coords="2,508.48,265.95,10.58,8.65" target="#b3">[4]</ref>. That is, a value of λ close to 1.0. We use a standard length prior.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Official runs</head><p>We submitted nine runs in total. For the Adhoc task, we submitted five runs. We submitted a full-text index run:</p><p>UAmsT06aTeLM Language model (λ = 0.90) on the fulltext index, using only the short topic statement in the title.</p><p>Next, we submitted a plain anchor-text index run:</p><p>UAmsT06aAnLM Language model (λ = 0.90) on the anchor-text index containing only explicitly spelled-out URLs, using only the short topic statement in the title.</p><p>Since the anchor-texts provide a document representation completely disjoint from the document's text, it is of interest to investigate how different both sets of retrieved documents are. Hence, we also submitted a run that combines different sources of evidence:</p><p>UAmsT06a3SUM Weighted CombSUM of language model (λ = 0.90) runs on the full-text index (relative weight 0.8), anchor-text index (relative weight 0.1), and titles index (relative weight 0.1), all using only the short topic statement in the title.</p><p>Since the short title statement is a relatively poor representation of the underlying (pseudo) information need, we also experimented with different representations of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UAmsT06aTDN</head><p>Language model (λ = 0.70) on the full-text index, using a query based on all three fields of the topic statement. The query consists of the 10 most significant terms in the topic statement, where each of these 10 terms is repeated as often as it occurs. That is, with the query labeled TDN10r above.</p><p>UAmsT06aTTDN Unweighted CombSUM combination of UAmsT06aTeLM and UAmsT06aTDN.</p><p>For the Named page finding task, we submitted four runs all using only the short topic statement in the title. We submitted a plain language model run on the full-text index: UAmsT06nTeLM Language model (λ = 0.90) on the fulltext index.</p><p>Next, we submitted a plain anchor-text index run:</p><p>UAmsT06nAnLM Language model (λ = 0.90) on the larger anchor-text index containing both relative and explicitly spelled-out URLs.</p><p>And, similar to the Adhoc Task, we also submitted a run that combines different sources of evidence:</p><p>UAmsT06n3SUM Weighted CombSUM of language model (λ = 0.90) runs on the full-text index (relative weight 0.8), anchor-text index (relative weight 0.1), and titles index (relative weight 0.1).</p><p>We also experimented with a web-centric prior that assumes that pages with shorter URLs are more likely to be relevant <ref type="bibr" coords="3,73.20,328.59,10.79,8.65" target="#b4">[5]</ref>:</p><p>UAmsT06nTurl Language model (λ = 0.90) on the fulltext index, with a URL prior instead of the standard length prior.</p><p>We calculated the number of components in the domain and file path of the URL, e.g, trec.nist.gov/act part/act part.html has 3 (domain) plus 2 (file path) components. Since our implementation of the language model calculates the logs of the probabilities, we took the exponent of the retrieval score, and multiplied it with the reciprocal of the length of the URL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adhoc task</head><p>The topic set contains the combined of 2004 (topic numbers 701-750); 2005 (topic numbers 751-850); and 2006 (topic numbers 801-850). We look here only at the 50 "fresh" topics of 2006. The number of relevant documents per topic varies from 5 to 571, with a mean of 118 and a median 87. Table <ref type="table" coords="3,89.13,567.36,4.98,8.65" target="#tab_1">2</ref> shows the results for the Adhoc task. Let us first focus on the short topic statement in the title-fields of the topics. Here, the run using the massive full-text index (UAmsT06aTeLM) clearly outperforms the run on the anchortext index (UAmsT06aAnLM). The anchor text index seems to be of some use in the first 10 ranks. For the runs using the verbose topic statement, we see that the UAmsT06aTDN run outperforms the T-only run (UAmsT06aTeLM) on the bpref and infAP measures, but loses out on the map and P@10 measures. The combination of these two runs (UAmsT06aTTDN) is improving over the T-only run on all measures, but is no equivocal improvement over the verbose run alone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Named page finding task</head><p>In total there are 181 Named page finding topics numbered 901-1081. The minimal number of relevant documents per topic is 1 and the maximum is 257. For 138 topics there is a unique relevant page, there are 7 topics with 10 or more relevant pages (caused by page-duplicates in the collection). This leads to a skewed distribution with a mean of 4.5 and a median of 1 relevant page. Table <ref type="table" coords="3,463.48,387.25,4.98,8.65" target="#tab_2">3</ref> shows the results for the Named page finding task. We make a number of observations. First, although runs using the full-text index outperform runs using the anchor-text index on all measures, the anchor-text runs turn out to be fairly competitive, with 4 less topics solved at rank 1, and 6 less topics solved at rank 5. Second, the combination run, based on the full-text index, the anchor-text index, and a titles index, comfortably outperforms runs based on only the full-text index. The success of the combination run shows the value of different document representations. Third, the URL prior leads to mixed results: a loss of mean reciprocal rank, but a gain in the number of topics with the relevant page in the top 5 and the top 10. Finally, the overall performance is, with the targeted page in the top 3 on average, quite impressive. More worrying though is that the performance is not equally good for all topics: at rank 10, no targeted page is found for 45% of the topics, and at rank 1,000, there are still more than 20% of the topics unsatisfied. There appears to be room for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Smoothing Experiments</head><p>In the language modeling framework, smoothing plays an important role: it helps to overcome data-sparseness, it introduces an inverted document frequency effect, and it expresses the relative importance of query terms <ref type="bibr" coords="3,502.50,710.82,15.27,8.65" target="#b9">[10]</ref>. In prac- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Named page finding task</head><p>First, we focus on the Named page finding task. Since finding a 'unique' page requires precision rather than recall, we may expect a relatively high value for the smoothing parameter. Table <ref type="table" coords="4,99.21,406.20,4.98,8.65" target="#tab_3">4</ref> shows the results while varying the smoothing parameter over the interval between 0 and 1. We make a few observations. As expected, we see that the Named page finding topics do not require much smoothing. In fact, as long as we put some weight on the collection model, the less smoothing the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adhoc task</head><p>Next, we focus on the Adhoc task. Since Adhoc topics require a delicate balance between precision and recall, the standard is to use a relatively low value for the smoothing parameter (i.e., λ = 0.15). Table <ref type="table" coords="4,189.84,551.18,4.98,8.65" target="#tab_4">5</ref> shows the results while varying the smoothing parameter over the interval between 0 and 1. On the large scale GOV2 collection, we see that also for Adhoc retrieval the performance increases if we apply less smoothing. Hence our experiments confirm our findings of last year: the Adhoc task evaluated by average precision seems to behave very much like an early precision task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Document Representation Experiments</head><p>We experiment with the four different document representations introduced in Section 2: All runs are based on the short query statement in the title field of the topics, and use little smoothing (λ = 0.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Adhoc task</head><p>We run the Adhoc topics on all four indexes. The runs using the Full-text (UAmsT06aTeLM) and Anchors (UAmsT06aAnLM) indexes were also official submissions. We also include the three-way combination of Full-text, Titles, and Anchors (official submission UAmsT06a3SUM), and a variant using the other All anchors index. Table <ref type="table" coords="4,350.38,627.14,4.98,8.65" target="#tab_5">6</ref> shows the results for the Adhoc task. We see that runs on the full-text index outperform all other runs on the other indexes, and all combinations with runs on other indexes. Only in terms of early precision, the alternative representation perform to a certain degree. The performance at early ranks is still much inferior to the full-text index, butconsidering that they are substantially smaller-the anchor and title indexes offer reasonable "value-for-money." </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Named page finding task</head><p>We run the known-item topics on all four indexes. The runs using the Full-text (UAmsT06nTeLM) and Anchors (UAmsT06nAnLM) indexes were also official submissions. We also include the three-way combination of Full-text, Titles, and Anchors, and a variant using the other All anchors index (official submission UAmsT06n3SUM).</p><p>Table <ref type="table" coords="5,86.88,275.66,4.98,8.65" target="#tab_6">7</ref> shows the results for the Named page finding task. We make a number of observations. Here the situation is quite different from the Adhoc task: the full-text index is still the best performing of all the individual indexes, but the titles index is a close second, followed again closely by the allanchors index. The relative effectiveness of the titles-index, usually indexing but a few words per document, seems to reveal a clear bias for the topic creators to base their query on (their recollection of) the page's title. The document representations of the full-text and anchor-text indexes are based on text from disjoint sources, and-as a result-the combination of these different sources of evidence leads to a substantial improvement over the performance of the individual indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Query Representation Experiments</head><p>The experiments with different query representations are restricted to the Adhoc task; there is only a short topic statement available for Named page finding task.</p><p>We experiment with the four query representations introduced in Section 2:</p><p>T short topic statement from the title field of the topic statement;</p><p>TDN verbose topic statement combining all the fields of the topic statement;</p><p>TDN10 10 most characteristic terms in any of the fields of the topic statement;</p><p>TDN10r 10 most characteristic terms in any of the fields of the topic statement, repeated by their term frequency in the topic;</p><p>All runs are based on the Full-text index, and use little smoothing (λ = 0.9). The run using the T query is identical to the official run UAmsT06aTeLM; the run using the TDN10r query is similar to the official submission UAmsT06aTDN which used λ = 0.7. We also include combinations of the T query run with each of the verbose queries, using an unweighted CombSUM combination method. The combination T-TDN10r is a variant of the official run UAmsT06aTTDN which used λ = 0.7.</p><p>The results for each of these runs are shown in Table <ref type="table" coords="5,548.44,302.31,3.74,8.65" target="#tab_7">8</ref>. The results are interesting. First, runs using the verbose topic statement indeed improve over those using the short topic statement. Second, the retrieval model seems to deal well with straightforward combination of all topic fields, which also contain many terms without relation to the topical content of the search request. In fact, the TDN runs outperform the runs using only selected terms from the verbose topic. Of course, the straightforward TDN query contains many terms causing an efficiency penalty. Third, the topic frequency of terms seems not to help performance, although more sophisticated query term weighting could be applied. Finally, in combination with a run based on the short title statement, the runs using 10 selected terms are more effective than the combination with straightforward TDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Pool versus Document and Query Representations</head><p>In this section, we analyse the presence of absence of documents in the assessment pool for a number of document and query representations. We use the top-50 pool consisting of 31,984 documents, of which 26,091 documents have been judged non-relevant, 5,467 documents have been judged relevant, and 426 documents have been judged highly relevant.</p><p>The results for the topic representations based on the Fulltext index are shown in Table <ref type="table" coords="5,441.31,627.14,3.74,8.65" target="#tab_8">9</ref>. There are up to 13% unjudged documents in the top 10 (for the TDN query), and up to 32% unjudged documents in the top 100 (for the TDN10 query). These fractions of unjudged documents are up to three times higher than the 11% of unjudged documents in the top 100 for the standard full-text index, which was part of the pool of documents to be judged. When looking at the percentage of relevant vs. nonrelevant documents in the  top 100, we see that 63% of the judged documents is relevant for the T query, 93% for the TDN query, 96% for the TDN10 query, and 83% for the TDN10r query. The precision over the set of retrieved and judged documents is very high for the richer query representations. Of course, these percentages are no realistic estimation of the presence of relevant unjudged documents. But, at the same time, these percentages strongly suggest that relevant unjudged documents may have been retrieved by these richer query representations.</p><p>The results for the document representations based on the short query statement are shown in Table <ref type="table" coords="6,217.69,567.36,8.30,8.65" target="#tab_9">10</ref>. There are up to 42% unjudged documents in the top 10 (for the All anchors index), and up to 73% unjudged documents in the top 100 (for the Titles index). These fractions of unjudged documents are substantially higher than the 11% of unjudged documents in the top 100 for the standard full-text index. When looking at the percentage of relevant vs. nonrelevant documents in the top 100, we see that 45% of the judged documents is relevant for the Titles index, 10% for the Anchors index, 29% for the All Anchors index, and 63% for the Full-text index. Again, these percentages are no realistic estimation of the presence of relevant unjudged documents, but do strongly suggest that relevant unjudged documents may have been retrieved.</p><p>The presence of a substantial number of unjudged documents is of concern when evaluating with traditional MAP or Precision at rank cut-offs since these treat unjudged documents as non-relevant. To a lesser extent the same hold for the MRR and Success at rank cut-offs. As a case in point, the run on the title index results in 38% unjudged in the top 10 and 74% unjudged in the top 100. This run was an official submission to the TREC 2006 Terabyte Track, clearly indicating that the incompleteness of the recall base is also affecting the official submissions that were not part of the top 50 pool. New measures like bpref are less sensitive since they consider only the judged documents, although they are still affected by a loss of data-points, and by a potential bias in the pool for a particular category of relevance <ref type="bibr" coords="6,511.20,225.53,10.58,8.65" target="#b0">[1]</ref>.</p><p>When comparing the different document and query representations, we see that the document representation generate more "unjudged" documents than the query representations. The most interesting question is the presence of relevant documents among the retrieved unjudged documents. For the assessments available at the time of writing, we cannot answer this question. The much larger pool of documents, used for the inferred average precision measure, contains only a smaller subset of judged documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>During the TREC 2006 Terabyte track, we conducted a range of experiments with smoothing, document representations, and query representations. We now summarize our main findings.</p><p>For the smoothing experiments, we found that the largescale collections require little smoothing. This confirms earlier results on the TREC 2005 Terabyte track <ref type="bibr" coords="6,500.30,469.07,10.58,8.65" target="#b3">[4]</ref>. This may even suggest that modern, advanced retrieval models are not necessarily more effective than simpler ranking formula's (such as straightforward term-frequency).</p><p>For the different document representation, we found that these are of little value for the Adhoc task, but can provide crucial additional retrieval cues for the Named page finding task. The full-text and anchor-texts indexes are derived from disjoint sources, and the combination of these different sources of evidence leads to a substantial improvement of retrieval effectiveness.</p><p>For the different query representations, we found that using a more verbose query leads to an improvement of retrieval effectiveness. Modern retrieval models seem to have no problem with long verbose queries also containing many off-topic terms. Selecting the terms that are most characteristic for the topic at hand, leads to an improvement of efficiency without a loss of retrieval effectiveness.</p><p>We compared the set of pooled documents for the Adhoc task with the sets of documents retrieved by the different query and document representations. We saw that the richer query representations retrieve relatively more unjudged documents, combined with a very good precision on the judged documents. The different document representations result in massive numbers of unjudged documents. When comparing the different document and query representations, we see that the document representation generate more "unjudged" documents than the query representations. The most interesting question is the presence of relevant documents among the retrieved unjudged documents. For the assessments available at the time of writing, we cannot answer this question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,59.78,64.07,227.15,151.16"><head>Table 1 :</head><label>1</label><figDesc>Query representations for Adhoc topic 701.</figDesc><table coords="2,59.78,86.55,227.15,128.68"><row><cell>T</cell><cell>U.S. oil industry history</cell></row><row><cell>TDN</cell><cell>U.S. oil industry history the history of the U.S. oil in-</cell></row><row><cell></cell><cell>dustry Relevant documents will include those on his-</cell></row><row><cell></cell><cell>torical exploration and drilling as well as history of</cell></row><row><cell></cell><cell>regulatory bodies. Relevant are history of the oil in-</cell></row><row><cell></cell><cell>dustry in various states, even if drilling began in 1950</cell></row><row><cell></cell><cell>or later.</cell></row><row><cell cols="2">TDN10 history oil industry drilling u later bodies exploration</cell></row><row><cell></cell><cell>began 1950</cell></row><row><cell cols="2">TDN10r history history history history oil oil oil industry in-</cell></row><row><cell></cell><cell>dustry industry drilling drilling u u later bodies ex-</cell></row><row><cell></cell><cell>ploration began 1950</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,316.81,64.07,239.10,108.07"><head>Table 2 :</head><label>2</label><figDesc>Results for the Adhoc task over the 50 new topics: (top half) title-only runs, (bottom half) verbose topic statement runs.</figDesc><table coords="3,322.79,108.77,227.15,63.37"><row><cell>UAmsT06</cell><cell>Topic</cell><cell>map</cell><cell>bpref</cell><cell>infAP</cell><cell>P@10</cell></row><row><cell>. . . aTeLM</cell><cell>T</cell><cell cols="4">0.2958 0.3528 0.2363 0.5260</cell></row><row><cell>. . . aAnLM</cell><cell>T</cell><cell cols="4">0.0143 0.0336 0.0081 0.1340</cell></row><row><cell>. . . a3SUM</cell><cell>T</cell><cell cols="4">0.2759 0.3273 0.1982 0.5060</cell></row><row><cell>. . . aTDN</cell><cell>TDN</cell><cell cols="4">0.2848 0.3879 0.2446 0.5020</cell></row><row><cell>. . . aTTDN</cell><cell>TDN</cell><cell cols="4">0.3284 0.3837 0.2379 0.5740</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,319.05,196.38,235.74,74.85"><head>Table 3 :</head><label>3</label><figDesc>Results for the Named page finding task.</figDesc><table coords="3,319.05,219.21,235.74,52.02"><row><cell>UAmsT06 MRR</cell><cell>S@1</cell><cell>S@5</cell><cell>S@10 not found</cell></row><row><cell cols="4">. . . nTeLM 0.262 33/18.2% 58/32.0% 72/39.8% 43/23.8%</cell></row><row><cell cols="4">. . . nAnLM 0.218 29/16.0% 52/28.7% 58/32.0% 95/52.5%</cell></row><row><cell cols="4">. . . n3SUM 0.363 49/27.1% 85/47.0% 100/55.2% 43/23.8%</cell></row><row><cell cols="4">. . . nTurl 0.241 26/14.4% 64/35.4% 75/41.4% 44/24.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,53.80,64.07,239.10,265.58"><head>Table 4 :</head><label>4</label><figDesc>Smoothing for the Named page finding task using the full-text index.</figDesc><table coords="4,53.80,96.81,239.10,232.84"><row><cell>λ</cell><cell>MRR</cell><cell>S@1</cell><cell>S@5</cell><cell>S@10</cell><cell>not found</cell></row><row><cell>0.0</cell><cell cols="5">0.0002 0/ 0.0% 0/ 0.0% 0/ 0.0% 178/98.3%</cell></row><row><cell>0.1</cell><cell cols="5">0.0877 10/ 5.5% 22/12.2% 27/14.9% 115/63.5%</cell></row><row><cell>0.2</cell><cell cols="5">0.1434 19/10.5% 32/17.7% 38/21.0% 89/49.2%</cell></row><row><cell>0.3</cell><cell cols="5">0.1681 23/12.7% 36/19.9% 42/23.2% 71/39.2%</cell></row><row><cell>0.4</cell><cell cols="5">0.1902 26/14.4% 40/22.1% 49/27.1% 62/34.3%</cell></row><row><cell>0.5</cell><cell cols="5">0.2061 28/15.5% 44/24.3% 53/29.3% 56/30.9%</cell></row><row><cell>0.6</cell><cell cols="5">0.2242 29/16.0% 49/27.1% 60/33.1% 52/28.7%</cell></row><row><cell>0.7</cell><cell cols="5">0.2368 32/17.7% 50/27.6% 62/34.3% 45/24.9%</cell></row><row><cell>0.8</cell><cell cols="5">0.2463 33/18.2% 52/28.7% 68/37.6% 45/24.9%</cell></row><row><cell>0.9</cell><cell cols="5">0.2616 33/18.2% 58/32.0% 72/39.8% 43/23.8%</cell></row><row><cell>1.0</cell><cell cols="5">0.2534 32/17.7% 60/33.1% 68/37.6% 48/26.5%</cell></row><row><cell cols="6">tice, smoothing is also a handle to tune a run toward re-</cell></row><row><cell cols="6">call (much smoothing) or precision (little smoothing). At</cell></row><row><cell cols="6">last year's edition of the TREC Terabyte track, we observed</cell></row><row><cell cols="6">that our runs required very little smoothing. We redo the</cell></row><row><cell cols="6">smoothing experiments on the Terabyte 2006 data, focus-</cell></row><row><cell cols="6">ing on varying the smoothing parameter in linear or Jelinek-</cell></row><row><cell cols="2">Mercer smoothing.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,316.81,64.07,239.10,163.13"><head>Table 5 :</head><label>5</label><figDesc>Smoothing for the Adhoc task using the full-text index (over 1,000 retrieved results).</figDesc><table coords="4,322.79,98.47,227.15,128.73"><row><cell>λ</cell><cell>MAP</cell><cell>B-Pref</cell><cell>P@1</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell>0.0</cell><cell cols="5">0.0001 0.0036 0.0007 0.0000 0.0000</cell></row><row><cell>0.1</cell><cell cols="5">0.0950 0.1760 0.5607 0.3320 0.2920</cell></row><row><cell>0.2</cell><cell cols="5">0.1502 0.2414 0.6043 0.3760 0.3520</cell></row><row><cell>0.3</cell><cell cols="5">0.1824 0.2665 0.6330 0.4240 0.3860</cell></row><row><cell>0.4</cell><cell cols="5">0.2034 0.2811 0.6762 0.4520 0.4200</cell></row><row><cell>0.5</cell><cell cols="5">0.2221 0.2954 0.7066 0.4920 0.4600</cell></row><row><cell>0.6</cell><cell cols="5">0.2404 0.3067 0.7227 0.5280 0.4820</cell></row><row><cell>0.7</cell><cell cols="5">0.2571 0.3179 0.7012 0.5320 0.4980</cell></row><row><cell>0.8</cell><cell cols="5">0.2737 0.3290 0.7206 0.5480 0.5140</cell></row><row><cell>0.9</cell><cell cols="5">0.2878 0.3402 0.7225 0.5440 0.5260</cell></row><row><cell>1.0</cell><cell cols="5">0.2903 0.3474 0.7192 0.5440 0.5260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,316.81,250.94,239.11,229.63"><head>Table 6 :</head><label>6</label><figDesc>Results for the Adhoc task over the 50 new topics (over 1,000 retrieved results).</figDesc><table coords="4,316.81,285.34,239.11,74.33"><row><cell></cell><cell>map</cell><cell>bpref</cell><cell>P@1</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell>1.Full-text</cell><cell cols="5">0.2878 0.3402 0.7225 0.5440 0.5260</cell></row><row><cell>2.Anchors</cell><cell cols="5">0.0142 0.0289 0.4348 0.1720 0.1340</cell></row><row><cell>3.All anchors</cell><cell cols="5">0.0306 0.0727 0.5164 0.2520 0.2160</cell></row><row><cell>4.Titles</cell><cell cols="5">0.0354 0.0942 0.4698 0.2400 0.1980</cell></row><row><cell>1+2+4</cell><cell cols="5">0.2759 0.3273 0.7609 0.5080 0.5060</cell></row><row><cell>1+3+4</cell><cell cols="5">0.2761 0.3297 0.7623 0.4960 0.4920</cell></row></table><note coords="4,316.81,383.20,189.39,9.04;4,316.81,404.68,239.10,9.04;4,336.74,417.02,93.26,8.65;4,316.81,438.10,239.10,9.04;4,336.74,450.45,137.68,8.65;4,316.81,471.53,235.86,9.04"><p><p><p>Full-text All textual content of the documents; Anchors Incoming anchor-texts based on only fully explicit URLs in the collection;</p>All anchors Incoming anchor-texts based on both absolute and relative links in the collection;</p>Title Content of the title field of the documents, if present.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,53.80,64.07,239.10,97.16"><head>Table 7 :</head><label>7</label><figDesc>Results for the Named page finding task.</figDesc><table coords="5,53.80,86.90,239.10,74.33"><row><cell></cell><cell>MRR</cell><cell>S@1</cell><cell>S@5</cell><cell>S@10 not found</cell></row><row><cell cols="5">1.Full-text 0.262 33/18.2% 58/32.0% 72/39.8% 43/23.8%</cell></row><row><cell cols="5">2.Anchors 0.136 17/ 9.4% 34/18.8% 39/21.6% 129/71.3%</cell></row><row><cell cols="5">3.All anchors0.218 29/16.0% 52/28.7% 58/32.0% 94/51.9%</cell></row><row><cell>4.Titles</cell><cell cols="4">0.256 38/21.0% 59/32.6% 65/35.9% 86/47.5%</cell></row><row><cell>1+2+4</cell><cell cols="4">0.353 47/26.0% 86/47.5% 97/53.6% 43/23.8%</cell></row><row><cell>1+3+4</cell><cell cols="4">0.363 49/27.1% 85/47.0% 100/55.3% 43/23.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,316.81,64.07,239.10,131.64"><head>Table 8 :</head><label>8</label><figDesc>Results for the different query representations for the Adhoc task over the 50 new topics (over 1,000 retrieved results).</figDesc><table coords="5,322.79,110.42,227.15,85.29"><row><cell></cell><cell>map</cell><cell>bpref</cell><cell>P@1</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell>T</cell><cell cols="5">0.2878 0.3402 0.7225 0.5440 0.5260</cell></row><row><cell>TDN</cell><cell cols="5">0.3063 0.4254 0.7806 0.5348 0.5130</cell></row><row><cell>TDN10</cell><cell cols="5">0.2887 0.4106 0.7968 0.5600 0.5320</cell></row><row><cell>TDN10r</cell><cell cols="5">0.3042 0.4044 0.8188 0.5560 0.5360</cell></row><row><cell>T-TDN</cell><cell cols="5">0.3383 0.4012 0.8476 0.6040 0.5720</cell></row><row><cell>T-TDN10</cell><cell cols="5">0.3601 0.4246 0.8729 0.6560 0.6220</cell></row><row><cell>T-TDN10r</cell><cell cols="5">0.3405 0.3997 0.8441 0.6200 0.5860</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,53.80,64.07,239.10,164.91"><head>Table 9 :</head><label>9</label><figDesc>Relevant, nonrelevant, and unjudged documents for the different query representations for the Adhoc task over the 50 new topics.</figDesc><table coords="6,59.78,110.81,227.15,118.17"><row><cell></cell><cell>Rank</cell><cell cols="2">Relevant</cell><cell cols="2">Nonrelevant</cell><cell>Unjudged</cell></row><row><cell></cell><cell></cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>%</cell></row><row><cell>T</cell><cell>10</cell><cell cols="2">263 52.60</cell><cell cols="2">237 47.40</cell><cell>0 0.00</cell></row><row><cell></cell><cell cols="5">100 1,712 34.24 2,737 54.74</cell><cell>551 11.02</cell></row><row><cell>TDN</cell><cell>10</cell><cell cols="2">236 47.20</cell><cell cols="2">160 32.00</cell><cell>64 12.80</cell></row><row><cell></cell><cell cols="6">100 1,566 31.32 1,691 33.82 1,343 26.86</cell></row><row><cell>TDN10</cell><cell>10</cell><cell cols="2">266 53.20</cell><cell cols="2">188 37.60</cell><cell>46 9.20</cell></row><row><cell></cell><cell cols="6">100 1,676 33.52 1,744 34.88 1,580 31.60</cell></row><row><cell>TDN10r</cell><cell>10</cell><cell cols="2">268 53.60</cell><cell cols="2">199 39.80</cell><cell>33 6.60</cell></row><row><cell></cell><cell cols="6">100 1,741 34.82 2,102 42.04 1,157 23.14</cell></row><row><cell cols="4">Run was in the top-50 pool.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="6,53.80,254.99,239.10,164.91"><head>Table 10 :</head><label>10</label><figDesc>Relevant, nonrelevant, and unjudged documents for the different document representations for the Adhoc task over the 50 new topics.</figDesc><table coords="6,59.78,301.73,227.15,118.17"><row><cell></cell><cell>Rank</cell><cell cols="2">Relevant</cell><cell cols="2">Nonrelevant</cell><cell>Unjudged</cell></row><row><cell></cell><cell></cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>%</cell><cell>#</cell><cell>%</cell></row><row><cell>Titles</cell><cell>10</cell><cell cols="2">100 20.00</cell><cell cols="2">212 42.40</cell><cell>188 37.60</cell></row><row><cell></cell><cell>100</cell><cell cols="2">407 8.14</cell><cell cols="3">910 18.20 3,626 72.52</cell></row><row><cell>Anchors</cell><cell>10</cell><cell cols="2">68 13.60</cell><cell cols="2">427 85.40</cell><cell>0 0.00</cell></row><row><cell></cell><cell>100</cell><cell cols="5">216 4.32 2,247 44.94 2,289 45.78</cell></row><row><cell cols="2">All anchors 10</cell><cell cols="2">106 21.20</cell><cell cols="2">184 36.80</cell><cell>210 42.00</cell></row><row><cell></cell><cell>100</cell><cell cols="5">354 7.08 1,223 24.46 3,363 67.26</cell></row><row><cell>Full-text</cell><cell>10</cell><cell cols="2">263 52.60</cell><cell cols="2">237 47.40</cell><cell>0 0.00</cell></row><row><cell></cell><cell cols="5">100 1,712 34.24 2,737 54.74</cell><cell>551 11.02</cell></row><row><cell cols="4">Runs was in the top-50 pool.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grants # <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.066.513</rs>, <rs type="grantNumber">639.072.601</rs>, and <rs type="grantNumber">640.-001.501</rs>), and by the <rs type="funder">E.U</rs>.'s 6th <rs type="programName">FP for RTD</rs> (project <rs type="projectName">Multi-MATCH</rs> contract <rs type="grantNumber">IST-033104</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qzJcRuC">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_GBW9eM2">
					<idno type="grant-number">612.066.513</idno>
				</org>
				<org type="funding" xml:id="_dgp4FD7">
					<idno type="grant-number">639.072.601</idno>
				</org>
				<org type="funded-project" xml:id="_Tng4xQq">
					<idno type="grant-number">640.-001.501</idno>
					<orgName type="project" subtype="full">Multi-MATCH</orgName>
					<orgName type="program" subtype="full">FP for RTD</orgName>
				</org>
				<org type="funding" xml:id="_UXMZ2kj">
					<idno type="grant-number">IST-033104</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,75.38,306.71,217.52,8.65;7,75.38,318.49,217.52,8.82;7,75.38,330.45,217.52,8.58;7,75.38,342.40,217.52,8.58;7,75.38,354.53,217.52,8.65;7,75.38,366.49,22.42,8.65" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,75.38,318.66,120.22,8.65">Bias and the limits of pooling</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,217.17,318.49,75.73,8.58;7,75.38,330.45,217.52,8.58;7,75.38,342.40,213.64,8.58">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="619" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,386.02,217.52,8.65;7,75.38,397.97,217.52,8.65;7,75.38,409.76,217.52,8.58;7,75.38,421.71,217.52,8.58;7,75.38,433.67,217.53,8.82;7,75.38,445.79,62.37,8.65" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,269.80,386.02,23.09,8.65;7,75.38,397.97,202.02,8.65">Parsimonious language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,75.38,409.76,217.52,8.58;7,75.38,421.71,217.52,8.58;7,75.38,433.67,76.77,8.58">Proceedings of the 27th Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval<address><addrLine>New York NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,465.32,217.52,8.65;7,75.38,477.28,216.49,8.65" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,100.81,465.32,187.94,8.65">The ILPS extension of the Lucene search engine</title>
		<author>
			<persName coords=""><surname>Ilps</surname></persName>
		</author>
		<ptr target="http://ilps.science.uva.nl/Resources/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,496.81,217.52,8.65;7,75.38,508.60,217.53,8.82;7,75.38,520.55,217.52,8.58;7,75.38,532.68,217.52,8.65;7,75.38,544.63,104.61,8.65" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,121.73,496.81,167.71,8.65">Effective smoothing for a terabyte of text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,277.96,508.60,14.94,8.58;7,75.38,520.55,208.95,8.58">The Fourteenth Text REtrieval Conference (TREC 2005</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,563.99,217.53,8.82;7,75.38,575.95,217.52,8.58;7,75.38,587.90,217.53,8.82;7,75.38,600.03,134.34,8.65" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,119.85,564.16,116.82,8.65">Web-centric language models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,256.84,563.99,36.07,8.58;7,75.38,575.95,217.52,8.58;7,75.38,587.90,188.56,8.58">Proceedings of the Fourteenth ACM Conference on Information and Knowledge Management (CIKM 2005)</title>
		<meeting>the Fourteenth ACM Conference on Information and Knowledge Management (CIKM 2005)<address><addrLine>New York NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,619.56,217.52,8.65;7,75.38,631.52,217.52,8.65;7,75.38,643.30,217.53,8.82;7,75.38,655.26,217.52,8.82;7,75.38,667.38,217.52,8.65;7,75.38,679.34,184.21,8.65" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,130.74,631.52,158.64,8.65">Approaches to robust and web retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,277.96,643.30,14.94,8.58;7,75.38,655.26,188.38,8.58">The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,698.87,217.52,8.65;7,75.38,710.82,217.52,8.65;7,338.39,57.11,217.53,8.82;7,338.39,69.06,217.52,8.82;7,338.39,81.19,217.52,8.65;7,338.39,93.14,60.60,8.65" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,230.56,698.87,62.34,8.65;7,75.38,710.82,132.06,8.65">Language models for searching in Web corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,457.12,57.11,98.79,8.58;7,338.39,69.06,74.32,8.58">The Thirteenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,113.07,217.53,8.65;7,338.39,125.96,93.95,7.04" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,377.18,113.07,105.81,8.65">The Lucene search engine</title>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,144.95,217.52,8.65;7,338.39,156.90,217.52,8.65;7,338.39,169.79,22.81,7.04" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,381.35,144.95,174.56,8.65;7,338.39,156.90,31.67,8.65">Stemming algorithms for use in information retrieval</title>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,188.78,217.52,8.65;7,338.39,200.74,217.52,8.65;7,338.39,212.52,217.52,8.82;7,338.39,224.48,217.53,8.82;7,338.39,236.60,178.12,8.65" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,440.26,188.78,115.65,8.65;7,338.39,200.74,217.52,8.65;7,338.39,212.69,31.67,8.65">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,389.26,212.52,166.65,8.58;7,338.39,224.48,187.25,8.58">ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
