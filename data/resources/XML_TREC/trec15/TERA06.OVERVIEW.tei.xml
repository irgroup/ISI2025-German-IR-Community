<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,193.44,114.69,225.23,15.53">The TREC 2006 Terabyte Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,100.08,170.38,81.12,10.97"><forename type="first">Stefan</forename><surname>BÃ¼ttcher</surname></persName>
							<email>stefan@buettcher.org</email>
						</author>
						<author>
							<persName coords="1,249.96,170.38,105.77,10.97"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<email>claclark@plg.uwaterloo.ca</email>
						</author>
						<author>
							<persName coords="1,436.92,170.38,62.89,10.97"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,193.44,114.69,225.23,15.53">The TREC 2006 Terabyte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3901BE1139E329F64946AAF4E132398C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>All tasks in the track use a collection of Web data crawled from Web sites in the gov domain during early 2004. We believe this collection ("GOV2") contains a large proportion of the crawlable pages present in gov at that time, including HTML and text, along with the extracted contents of PDF, Word and postscript files. The collection is 426GB in size and contains 25 million documents. For TREC 2004, the collection was distributed by CSIRO, Australia, who assisted in its creation. For 2005 and 2006, the collection was distributed by the University of Glasgow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The primary goal of the Terabyte Track is to develop an evaluation methodology for terabyte-scale document collections. In addition, we are interested in efficiency and scalability issues, which can be studied more easily in the context of a larger collection.</p><p>TREC 2006 is the third year for the track. The track was introduced as part of TREC 2004, with a single adhoc retrieval task. For TREC 2005, the track was expanded with two optional tasks: a named page finding task and an efficiency task. These three tasks were continued in 2006, with 20 groups submitting runs to the adhoc retrieval task, 11 groups submitting runs to the named page finding task, and 8 groups submitting runs to the efficiency task. This report provides an overview of each task, summarizes the results, and outlines directions for the future. Further background information on the development of the track can be found in the 2004 and 2005 track reports <ref type="bibr" coords="1,513.85,434.86,11.44,10.91" target="#b3">[4,</ref><ref type="bibr" coords="1,528.50,434.86,7.62,10.91" target="#b4">5]</ref>.</p><p>For TREC 2006, we made the following major changes to the tasks:</p><p>1. We strongly encouraged the submission of adhoc manual runs, as well as runs using pseudorelevance feedback and other query expansion techniques. Our goal was to increase the diversity of the judging pools in order to a create a more re-usable test collection. Special recognition (and a prize) was offered to the group submitting the run contributing the most unique relevant documents to the judging pool.</p><p>2. The named page finding topics were created by task participants, with each group asked to create at least 12 topics.</p><p>3. The experimental procedure for the efficiency track was re-defined to permit more realistic intra-and inter-system comparisons, and to generate separate measurements of latency and throughput. In order to compare systems across various hardware configurations, comparative runs using publicly available search engines were encouraged.</p><p>3 Adhoc Retrieval Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description</head><p>An adhoc retrieval task investigates the performance of systems that search a static set of documents using previously-unseen topics. For each topic, participants create a query and generate a ranked list of documents. For the 2006 task, NIST created and assessed 50 new topics. An example is provided in Figure <ref type="figure" coords="2,166.23,287.26,4.22,10.91" target="#fig_1">1</ref>. In addition, the 99 topics created in 2004 and 2005 (topics 701-800) were re-used for automatic runs. As is the case for most TREC adhoc tasks, a topic describes the underlying information need in several forms. The title field essentially contains a keyword query, similar to a query that might be entered into a Web search engine. The description field provides a longer statement of the topic requirements, in the form of a complete sentence or question. The narrative, which may be a full paragraph in length, supplements the other two fields and provides additional information required to specify the nature of a relevant document.</p><p>For the adhoc task, an experimental run consisted of the top 10,000 documents for each topic. To generate a run, participants could create queries automatically or manually from the topics. For a run to be considered automatic it must be created from the topics without any human intervention. All other runs are manual. Manual runs used only the 50 new topics; automatic runs used all 149 topics from 2004-2006.</p><p>For most experimental runs, participants could use any or all of the topic fields when creating queries from the topic statements. However, a group submitting any automatic run was required to submit at least one automatic run that used only the title field of the topic statement. Manual runs were encouraged, since these runs often add relevant documents to the evaluation pool that are not found by automatic systems using current technology. We offered a prize to the group with the run that returned the most unique relevant documents. Groups could submit up to five runs.</p><p>Runs were pooled by NIST for judging. The details of the pooling process differ substantially from previous years, and from previous TREC adhoc tasks, and are detailed in a separate section.</p><p>Assessors used a three-way scale of "not relevant", "relevant", and "highly relevant". A document is considered relevant if any part of the document contains information which the assessor would include in a report on the topic. It is not sufficient for a document to contain a link that appears to point to a relevant Web page, the document itself must contain the relevant information. It was left to the individual assessors to determine their own criteria for distinguishing between relevant and highly relevant documents. For the purpose of computing effectiveness measures that require binary relevance judgments, the relevant and highly relevant documents are combined into a single "relevant" set. Why is Boston's Central Artery project, also known as "The Big Dig", characterized as "pork"? &lt;narr&gt; Narrative: Relevant documents discuss the Big Dig project, Boston's Central Artery Highway project, as being a big rip-off to American taxpayers or refer to the project as "pork". Not relevant are documents which report fraudulent acts by individual contractors. Also not relevant are reports of cost-overruns on their own. In addition to the top 10,000 documents for each run, we collected details about the hardware and software configuration used to generate them, including performance measurements such as total query processing time. For total query processing time, groups were asked to report the time required to return the top 20 documents, not the time to return the top 10,000. It was acceptable to execute a system twice for each run, once to generate the top 10,000 documents and once to measure the execution time for the top 20 documents, provided that the top 20 documents were the same in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/top&gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adhoc Pooling</head><p>Last year, we reported that the pools were increasingly dominated by documents containing the title words of topics, to the possible exclusion of other relevant documents <ref type="bibr" coords="3,424.92,513.34,11.56,10.91" target="#b4">[5,</ref><ref type="bibr" coords="3,439.69,513.34,8.43,10.91" target="#b5">6,</ref><ref type="bibr" coords="3,451.33,513.34,7.62,10.91" target="#b1">2]</ref>. This could lead to a test collection that is biased towards simple title-query-based retrieval approaches and that may not measure other systems fairly. We were able to observe this phenomenon directly in the AQUAINT collection due to the presence of a feedback run that retrieved many unique relevant documents. It seemed clear that it must also occur in GOV2, although we had no direct evidence in the form of missed relevant documents. For 2006, we took steps to gather data to answer the bias question for GOV2, including the active solicitation of manual runs in an effort to diversify the pools.</p><p>In addition, we required the adhoc runs to include the older topics to determine if the newer runs retrieved unusual amounts of unjudged documents for these topics. If so, it would provide more evidence of bias in the collection as well as data for analyzing the impact of that bias. However, since this activity was limited to automatic runs, we did not expect to see these runs fall far from the original qrels. Lastly, we constructed three separate pools for the 2006 topics, two of which were used in the evaluation. The runs contributing to the pools are the same for all three: one automatic, one manual, and one efficiency task run per group. If a group only did manual or automatic runs, we took two of that type from that group.</p><p>The first pool is a traditional TREC pool to depth 50. This pool allows us to report traditional measures such as MAP and precision at fixed document cutoffs to some degree of accuracy, and thus provide some degree of continuity to track participants while experimental methods were tried.</p><p>The second pool is also a traditional TREC pool, but drawn starting at depth 400. This pool was motivated by the values of the titlestat measure, described by Buckley et al. <ref type="bibr" coords="4,461.64,583.78,10.92,10.91" target="#b1">[2]</ref>. The authors of that paper primarily computed the titlestat of judged relevant documents, but titlestat can be computed over any group of documents. For this application, we computed the titlestat of 100document strata of the "complete" pool (that is, all documents pooled to depth 10,000). These titlestats are plotted to depth 5000 in figure <ref type="figure" coords="4,284.43,637.90,4.22,10.91" target="#fig_2">2</ref>. Whereas the pool from depths 1-100 has a titlestat of 0.74, at depths 400-500 the titlestat of the pool is just over 0.6. This pool starts at depth 400, but goes down to a different depth for each topic, such that the total size per topic for this plus the initial depth-50 pool is 1000 documents. While this pool was not used for the evaluation, the relevance judgments allow us to see if relevant documents still occur frequently in lower-titlestat regions.</p><p>The third pool is a random sample, drawn in such a way as to attempt to gather more relevant documents deeply from topics where we expect them to occur. Using the relevance judgments from the depth-50 pool, we calculate the probability of relevance given the pool rank of a document. Using a simple linear fit based on experiments with last year's pools, we then estimate the depth at which we will find a given ratio of relevant documents to pool size. We then draw a random sample of about 200 documents up to that target depth. This third pool varies in maximum pool depth from 57 to 1252 depending on how many relevant documents were found in the depth-50 pool for each topic.</p><p>The qrels from the third pool are not usable with MAP and other traditional measures. Instead, they are intended to be used with a new measure called inferred average precision, or infAP <ref type="bibr" coords="5,520.68,223.78,10.92,10.91" target="#b6">[7]</ref>. <ref type="foot" coords="5,535.20,222.70,4.23,7.29" target="#foot_0">1</ref>infAP estimates average precision based on a known pool where only a sample of documents in the pool are judged. The expected precision at rank k is</p><formula xml:id="formula_0" coords="5,224.40,270.46,155.87,25.67">1 k â¢ 1 + k -1 k P k -1 â¢ R + Ç« R + N + Ç«</formula><p>where P is the number of documents in the pool, R is the number of known relevant documents above rank k, and N is the number of known nonrelevant documents above rank k. infAP is the average of these precisions at each relevant document. infAP is similar to bpref in that it is intended for incomplete judgments but differs in that it is an direct estimate of average precision based on a sample.</p><p>The official evaluation results report MAP (and other standard trec eval measures) on the depth-50 pool, infAP on the random-sample pool, and for automatic runs MAP on all 149 terabyte track topics (where the AP scores for the 2006 topics are from the depth-50 pool.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adhoc Results</head><p>Table <ref type="table" coords="5,102.72,448.66,5.45,10.91" target="#tab_0">1</ref> provides an summary of the results obtained on the title-only automatic runs sorted by bpref. Only the best run from each group is shown. Figure <ref type="figure" coords="5,366.54,462.22,5.45,10.91" target="#fig_2">2</ref> provides the same information for the manual runs. The first two columns of each table identify the group and run. The next three columns provide the values of three standard effective measures for each run: bpref <ref type="bibr" coords="5,479.43,489.34,10.83,10.91" target="#b2">[3]</ref>, precision at 20 documents (p@20), and mean average precision (MAP). The sixth colunm provides values for the new infAP measure decribed above. The last two columns list the number of CPUs used to generate the run and the total query processing time.</p><p>The top-scoring automatics runs were generated using various retrieval methods, including Okapi BM25 and language modeling approaches. Many of these runs took features such as phrases, term proximity and matches in the title field into account during ranking. Of particular note is the prevalence of pseudo-relevance feedback, which substantially improved performance for most groups. On the other hand, none of the top-eight runs used anchor text, and only one used link analysis techniques.</p><p>A prize was offered to the group submitting the run containing the most unique relevant documents, excluding other runs from the same group. The prize (a NIST clock) was awarded to Chris Buckley of Sabir Research for the run sabtb06man1, which contributed 222 unique documents. 4 Named Page Finding Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Description</head><p>Users sometimes search for a page by name. In such cases, an effective search system will return that page at or near rank one. In many cases there is only one correct answer. In other cases, any document from a small set of "near duplicates" is correct. The objective of the task, therefore, is to find a particular page in the GOV2 collection, given a topic that describes it. For example, the query "Apollo 11 Mission" would be satisfied by NASA's history page on the first moon landing. Named page topics were created by track participants through a purpose-built Web interface to the Wumpus search engine<ref type="foot" coords="6,213.48,555.10,4.23,7.29" target="#foot_1">2</ref> and hosted at the University of Waterloo. Participants were asked to imagine they were using a search engine to locate an interesting page that they found once but couldn't quite remember where it was. Their goal was to identify interesting, "bookmark-worthy" pages, that they thought they might want to go back and find again. Once such a page was found, they were to give it a name such as they might assign to a bookmark. The name was to approximate what they might type into a Web search engine to locate that page again.</p><p>Participants could identify interesting pages in one of two ways. One was to request random pages from the search engine, and to keep looking at random pages until one struck their fancy. Another was to search for subjects of interest to the participant, and to look through the search results until something worth keeping was found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Participants were instructed to make each page's name specific to that page. To check this, they were requested to perform a search with the name as a query, and to check to see if other pages came up which could take the same name. The named page itself did not need to appear in these search results, although it was acceptable if it did. The purpose of this check search was to weed out similar (but not near-duplicate) pages that might need to be distinguished in order to obtain a good named page topic. Near-duplicates of the page, which differ only in formatting or by trivial content changes, were permitted.</p><p>When evaluating the submitted runs, we identified these near-duplicates using NIST's implementation of Bernstein and Zobel's DECO algorithm <ref type="bibr" coords="7,330.16,443.98,10.92,10.91" target="#b0">[1]</ref>. We ran DECO on the top 100 retrieved documents from all submitted named page runs, identified near-duplicates of the known targets, and manually checked those for relevance. Near-duplicates are treated as equivalent to the original page and are included in the qrels file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named Page Finding Results</head><p>Figure <ref type="figure" coords="7,106.93,534.34,5.45,10.91" target="#fig_3">3</ref> summarizes the results of the named page finding task. The performance of the runs is evaluated using three metrics:</p><p>â¢ MRR: The mean reciprocal rank of the first correct answer.</p><p>â¢ % Top 10: The proportion of queries for which a correct answer was found in the first 10 search results.</p><p>â¢ % Not Found: The proportion of queries for which no correct answer was found in the results list.</p><p>The figure lists the best run from the each group by MRR. In addition, the figure indicates the runs that exploit link analysis techniques (such as pagerank), anchor text, and document structure (such as giving greater weight to terms appearing in titles). 5 Efficiency Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Description</head><p>The efficiency task extends both the adhoc task and the named page finding task, providing a vehicle for discussing and comparing efficiency and scalability issues in IR systems by defining better methodology to determine query processing times. Two weeks before the new topics for the adhoc task were made available, NIST released a set of 100,000 efficiency topics. These topics were extracted from the logs of a commercial Web search engine. Because an analysis of last year's 50,000 efficiency topics, which also had been extracted from a Web search engine log, had shown that the topics did not match GOV2 very well and consequently could be processed much faster than the adhoc topics, this year we made sure that each query in the efficiency topic set:</p><p>â¢ had produced at least one clickthrough to .gov in the Web search engine, and</p><p>â¢ matched at least 20 documents in GOV2 (Boolean OR).</p><p>After creating a set of representative topics in this way, the title fields of the adhoc topics (751-850) and the named page finding topics (NP601-NP872, NP901-NP1081) from this year's and last year's Terabyte track were seeded into the topic set, but were not distinguished in any way. Figure <ref type="figure" coords="8,534.65,609.70,5.45,10.91" target="#fig_3">3</ref> provides some examples from the resulting topic set. Participating groups were required to process these topics automatically; manual runs were not permitted.</p><p>The efficiency topic set was distributed in 4 separate files, representing 4 independent query streams. Groups were required to process queries within the same stream sequentially and in the order in which they appear in the topic file. Processing of each query in a stream was to be 68964:easy to do science projects for 5th grade 68965:who to contact when civil rights are violated 68966:ergonomic courses illinois 68967:big dig pork 68968:signs of a partial seizure 68969:food at jfk 68970:natural gas power point 68971:va home care portland oregon 68972:lexapro package insert completed before processing of the next query was started. Queries from different query streams could be processed concurrently or interleaved in any arbitrary order. The existence of independent query streams allowed systems to take better advantage of parallelism and I/O scheduling.</p><p>Each participating group ran their system on the entire topic set (all four streams), reporting the top 20 documents for each topic, the average per-topic reponse time (referred to as query processing latency), and the total time between reading the first topic and writing the last result set (used to calculate query throughput). The total time was recorded without taking into account system startup times. By processing all queries strictly sequentially, latency was minimized. A group was able to choose, however, to process queries from different streams in parallel in order to make better use of parallelism and to increase their system's query throughput.</p><p>In general, document retrieval systems can employ two different kinds of parallelism: intra-query and inter-query. With intra-query parallelism, the same query is processed on multiple CPUs in parallel, for example by splitting the document collection into equal parts and distributing these parts among different machines. Intra-query parallelism improves both latency and throughput, although not necessarily in a linear fashion. Inter-query parallelism, on the other hand, refers to the situation in which multiple queries are being processed at the same time. It can be used to increase query throughput, usually in a linear or near-linear way, but does not improve query latency. Distributing the 100,000 efficiency topics in four independent streams was meant to explicitly encourage inter-query parallelism and to allow groups to study latency/throughput trade-offs.</p><p>One of the goals of the Terabyte track is to be able to compare different approaches to highperformance information retrieval and to evaluate them quantitatively. The validity of direct comparisons between groups, however, is limited by the range of hardware used, which varies from desktop PCs to supercomputers. Last year, we tried to overcome this issue by applying some informal normalizations, based on the number of CPUs and the total cost of a system. Those attempts were only partially successful. In order to obtain more reliable inter-group performance comparisons, participants this year were were encouraged to submit at least one run that was conducted in a single-CPU configuration, with all queries being processed sequentially. They were also encouraged to download special TREC versions of the three open-source information retrieval systems</p><p>â¢ Indri (http://www.lemurproject.org/indri/),</p><p>â¢ Wumpus (http://www.wumpus-search.org/), and  <ref type="table" coords="10,102.60,415.42,4.22,10.91">4</ref>: Efficiency Results. Best run (according to P@20) and fastest run (according to average query latency) from each participating group. Effectiveness is reported for the the adhoc (P@20) and the named page finding (MRR) topics from 2006, not for the 2005 topics also present in the efficiency topics.</p><p>to compile them, build an index for GOV2, and run 10,000 queries (a subset of the 100,000 efficiency topics) through these systems. For each such comparative efficiency run, participants were required to report the total time to process all queries and the total CPU consumed, i.e., the time during which the CPU was busy processing queries and not waiting for the hard drive, for instance.</p><p>Our incentive was that this data could be used to find out whether an inter-group performance comparison, across different hardware configurations, is feasible at all and also how efficiency numbers need to be normalized in order to obtain a fair comparison of two systems running on different hardware.</p><p>We would like to point out that the versions of the three systems we used for comparative reasons were modified and re-packaged for this specific purpose, without special support from their developers. It is therefore unlikely that the efficiency numbers that were obtained reflect the true performance of the systems. They were used exclusively to determine the performance of the hardware they were run on.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Efficiency Results and Discussion</head><p>The efficiency results for the 100,000 efficiency topics are shown in Table <ref type="table" coords="11,413.53,264.58,4.22,10.91">4</ref>. For each group, the best run (according to mean precision at 20 documents, for adhoc topics 801-850) and the fastest run (according to average query latency) are summarized. Both performance and precision vary greatly among the systems. The two fastest runs exhibit an average latency of 13 ms per query, while the slowest run consumed almost 5 seconds. P@20, on the other hand, varies quita substantially as well, between 0.150 and 0.505.</p><p>Although groups were allowed to have their system process queries from the four separate query streams in parallel, most groups chose not to do so. Only 5 out of the 25 efficiency runs processed queries in parallel. For some of these runs, however, the gains achieved from processing queries in parallel are tremendous, leading to a query throughput of up to 185 queries per second in the case of CWI06DIST8.</p><p>The efficiency measures reported in Table <ref type="table" coords="11,301.67,413.62,5.45,10.91">4</ref> include throughput and latency, both in their raw form (as measured by the respective group) as well as normalized, applying the same ad-hoc performance normalizations (CPU-normalized and cost-normalized) as last year:</p><p>â¢ the CPU-normalized query latency is the real query latency, multiplied by the total number of CPUs in the system;</p><p>â¢ the cost-normalized query latency compares each run with a run conducted on a hypothetical computer system costing 1,000 USD; thus, the latency of a run conducted on a $5,000 computer was multiplied by 5.</p><p>Efficiency vs. effectiveness trade-off plots, both normalized and non-normalized, for all 25 efficiency runs are shown in Figure <ref type="figure" coords="11,194.79,557.98,4.22,10.91" target="#fig_5">4</ref>.</p><p>The validity of calculating CPU-normalized query latency is actually somewhat questionable. Using multiple CPUs in parallel only decreases latency in the case of intra-query parallelism, not in the case of inter-query parallelism. Nonetheless, because we do not know exactly which type of parallelism a group employed in their runs, we decided to apply CPU normalization to both efficiency measures, throughput and latency.</p><p>Unfortunately, however, neither CPU normalization nor cost normalization are completely satisfying. The number of CPUs in a system does not say anything about the performance of these CPUs. The total cost of a computer, on the other hand, might include components like graphics cards and additional hard drives that were not used in a run at all. In order to obtain more reliable comparative performance results, we analyzed the efficiency numbers reported as part of the comparative efficiency runs conducted with Indri, Wumpus, and Zettair. Comparative runs were submitted by 4 out of 8 participating groups. In addition, Trevor Strohman from the University of Massachusetts (umass.allan) submitted three comparative runs in order to help us with our data sparseness problem. The results are shown in Table <ref type="table" coords="12,483.61,281.74,4.22,10.91" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>When examining the reported efficiency numbers, we noticed that, while performance figures were largely consistent between two different 32-bit systems (for both umass.allan and rmit.scholer, for instance, Wumpus is about 3.8 times faster than Zettair, while Indri is about 4 times slower than Zettair), the situation is different when comparing 32-bit hardware with 64-bit hardware. On uwaterloo-clarke's hardware, Wumpus is about 6.8 times faster than Zettair, while Indri is only 2.8 times slower than Zettair.</p><p>The large discrepancy between Wumpus and Zettair is due to Wumpus being designed for 64bit and using 64-bit integer arithmetic throughout. When compiled for 32-bit, all 64-bit integer instructions have to simulated by sequences of 32-bit integer instructions, a transformation that is very costly. Zettair, on the other hand, uses 32-bit integer arithmetic and is compiled for a 32-bit architecture. Therefore, it does not experience the same slowdown when executed on a 32bit computer. When uwaterloo-clarke recompiled Wumpus for 32-bit and ran it on their 64-bit hardware, this discrepancy almost vanished, and Wumpus was only 5 times faster than Zettair.</p><p>However, the non-proportional performance difference between the three retrieval systems when moving between different hardware configurations is not only because of CPU issues, but also because of different hard drive performance in the individual computer systems: uwaterloo-clarke stored index structures on a single disk, while umass.allan stored the index on a 3-disk RAID, lowlands-team.deVries on a 10-disk RAID, and rmit.scholer even on a 12-disk RAID. Because Indri, Wumpus, and Zettair produce inverted files of different size, the hard drive configuration has different effects on the three systems. For example, while Wumpus exhibits about the same performance on the hardware configurations used by lowlands-team.deVries and uwaterloo-clarke (220 vs. 240 ms per query), Zettair is 75% faster on lowlands-team.deVries's hardware than on uwaterloo-clarke's (930 vs. 1620 ms).</p><p>Unfortunately, the effect of different hard disk configurations cannot be eliminated by comparing CPU times, either. A system that uses software RAID, for instance, usually exhibits higher CPU utilization than a system using hardware RAID.</p><p>Despite these irregularities, we tried to come up with a tentative performance normalization procedure that allows us to compare the performance of different retrieval systems across hardware configuration boundaries. Because all four groups participating in the comparative efficiency task GHz Pentium IV with 2 GB RAM and a 3-way software RAID-0).</p><p>submitted a comparative run using Wumpus, we chose to use Wumpus to establish the true performance of the underlying hardware and to normalized the latency numbers reported by the groups based on the performance estimate obtained through Wumpus. For each group, two performance estimates were obtained, one based on CPU time, the other based on average query latency. This led to a normalized efficiency interval instead of a single efficiency number. The results are shown in Table <ref type="table" coords="13,113.53,281.74,4.22,10.91" target="#tab_5">6</ref>. Because of all the difficulties explained above, the outcome of the normalization process should be taken with a grain of salt.</p><p>If we have learned anything from our attempt to conduct a fair performance comparisons of different retrieval systems, then it is the insight that such a comparison is incredibly difficult, if not impossible, to achieve. The assumption that all document retrieval systems are relatively similar to each other and thus have similar performance characteristics (and by that we do not mean raw latency or throughput values) does not hold. It is entirely possible that moving to a different hardware configuration improves the performance of retrieval system A while depleting the performance of system B, as documented by Table <ref type="table" coords="13,336.00,390.10,5.45,10.91" target="#tab_4">5</ref> 6 The Future of the Terabyte Track 2006 is the final year of the Terabyte Track in its current form. After three years, we have a reasonable number of topics and judgments, and we cannot see significant value in another year of similar experiments on the same collection.</p><p>In the future, we hope to resurrect the track with a substantially larger collection and an renewed focused on Web retrieval. Along with the standard adhoc and named page finding tasks, we plan to examine problems such as Web spam filtering and snippet extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,229.32,322.18,153.30,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Adhoc Task Topic 835</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,150.36,442.30,306.00,10.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Values for titlestat in 100-document strata of the pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,189.96,214.30,231.90,10.91"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Efficiency Task Topics 68964 to 68972</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,91.12,87.49,8.22,5.47;11,91.12,99.83,8.22,5.47;11,91.12,112.17,8.22,5.47;11,91.12,124.51,8.22,5.47;11,91.12,136.85,8.22,5.47;11,91.12,149.19,8.22,5.47;11,210.28,167.45,11.51,5.47;11,192.19,167.45,8.22,5.47;11,174.23,167.45,8.22,5.47;11,152.85,167.45,11.51,5.47;11,134.84,167.45,11.51,5.47;11,115.15,167.45,11.51,5.47;11,97.14,167.45,11.51,5.47;11,76.36,137.31,5.47,24.33;11,76.36,130.73,5.47,4.93;11,76.36,122.51,5.47,6.58;11,76.36,91.92,5.47,28.94;11,116.21,176.33,86.49,5.47;11,131.67,78.61,55.57,5.47;11,247.84,87.49,8.22,5.47;11,247.84,99.83,8.22,5.47;11,247.84,112.17,8.22,5.47;11,247.84,124.51,8.22,5.47;11,247.84,136.85,8.22,5.47;11,247.84,149.19,8.22,5.47;11,367.00,167.45,11.51,5.47;11,348.91,167.45,8.22,5.47;11,330.95,167.45,8.22,5.47;11,309.57,167.45,11.51,5.47;11,291.56,167.45,11.51,5.47;11,271.86,167.45,11.51,5.47;11,253.86,167.45,11.51,5.47;11,233.08,137.31,5.47,24.33;11,233.08,130.73,5.47,4.93;11,233.08,122.51,5.47,6.58;11,233.08,91.92,5.47,28.94;11,272.93,176.33,86.49,5.47;11,263.07,78.61,106.20,5.47;11,404.56,87.49,8.22,5.47;11,404.56,99.83,8.22,5.47;11,404.56,112.17,8.22,5.47;11,404.56,124.51,8.22,5.47;11,404.56,136.85,8.22,5.47;11,404.56,149.19,8.22,5.47;11,523.72,167.45,11.51,5.47;11,505.63,167.45,8.22,5.47;11,487.67,167.45,8.22,5.47;11,466.29,167.45,11.51,5.47;11,448.28,167.45,11.51,5.47;11,428.58,167.45,11.51,5.47;11,410.58,167.45,11.51,5.47;11,389.80,137.31,5.47,24.33;11,389.80,130.73,5.47,4.93;11,389.80,122.51,5.47,6.58;11,389.80,91.92,5.47,28.94;11,429.65,176.33,86.49,5.47;11,426.86,78.61,92.06,5.47"><head></head><label></label><figDesc>Cost-normalized (x Cost/1000$)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,72.00,196.90,467.92,10.91;11,72.00,210.34,332.63,10.91"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Efficiency (average latency for the 100,000 efficiency topics) and effectiveness (P@20 for adhoc topics 801-850) of all 25 runs submitted for the efficiency task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,78.00,74.74,476.50,332.75"><head>Table 1 :</head><label>1</label><figDesc>Adhoc Results. Best automatic title-only run from each group, according to bpref.</figDesc><table coords="6,78.00,74.74,476.50,297.83"><row><cell></cell><cell>Run</cell><cell cols="2">bpref p@20 MAP infAP CPUs Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(sec)</cell></row><row><cell>uwaterloo-clarke</cell><cell>uwmtFadTPFB</cell><cell>0.4251 0.5570 0.3392 0.2999</cell><cell>964</cell></row><row><cell>umass.allan</cell><cell>indri06AlceB</cell><cell>0.4229 0.5410 0.3687 0.3157</cell><cell>38737</cell></row><row><cell>pekingu.yan</cell><cell>TWTB06AD01</cell><cell>0.4193 0.5150 0.3737 0.3224</cell><cell>56160</cell></row><row><cell>hummingbird.tomlinson</cell><cell>humT06xle</cell><cell>0.4172 0.5820 0.3452 0.2947</cell><cell>36000</cell></row><row><cell>ibm.carmel</cell><cell>JuruTWE</cell><cell>0.4002 0.5670 0.3506 0.2687</cell><cell>3375</cell></row><row><cell>uglasgow.ounis</cell><cell>uogTB06QET2</cell><cell>0.3995 0.5400 0.3456 0.2861</cell><cell>N/A</cell></row><row><cell cols="2">ecole-des-mines.beigbeder AMRIMtp20006</cell><cell>0.3942 0.5170 0.3120 0.2994</cell><cell>68344</cell></row><row><cell>coveo.soucy</cell><cell>CoveoRun1</cell><cell>0.3886 0.5440 0.3296 0.2564</cell><cell>135</cell></row><row><cell>umilano.vigna</cell><cell>mg4jAutoV</cell><cell>0.3774 0.4510 0.2882 0.2765</cell><cell>3000</cell></row><row><cell>rmit.scholer</cell><cell>zetadir</cell><cell>0.3726 0.4800 0.3056 0.2599</cell><cell>466.5</cell></row><row><cell>umelbourne.ngoc-anh</cell><cell>MU06TBa2</cell><cell>0.3682 0.5130 0.3039 0.2549</cell><cell>25.25</cell></row><row><cell>uamsterdam.ilps</cell><cell cols="2">UAmsT06aTeLM 0.3528 0.4850 0.2958 0.2363</cell><cell>2394</cell></row><row><cell>dublincityu.gurrin</cell><cell>DCU05BASE</cell><cell>0.3509 0.5090 0.2695 0.2067</cell><cell>495</cell></row><row><cell>tsinghuau.zhang</cell><cell>THUADALL</cell><cell>0.3432 0.4600 0.2858 0.2444</cell><cell>560</cell></row><row><cell>lowlands-team.deVries</cell><cell>CWI06DISK1ah</cell><cell>0.3361 0.4780 0.2770 0.2299</cell><cell>60.3</cell></row><row><cell>polytechnicu.suel</cell><cell>p6tbadt</cell><cell>0.3073 0.3920 0.2274 0.1972</cell><cell>60</cell></row><row><cell>max-planck.theobald</cell><cell>mpiirtitle</cell><cell>0.2849 0.4270 0.1805 0.1678</cell><cell>38</cell></row><row><cell>northeasternu.aslam</cell><cell>hedge0</cell><cell>0.2568 0.3460 0.1771 0.1388</cell><cell>110000</cell></row><row><cell>sabir.buckley</cell><cell>sabtb06at1</cell><cell>0.2434 0.3250 0.1361 0.1045</cell><cell>77</cell></row><row><cell>ualaska.fairbanks.newby</cell><cell>arscDomAlog</cell><cell>0.1463 0.0550 0.0541 0.0675</cell><cell>108 120000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,78.00,74.74,475.18,211.43"><head>Table 2 :</head><label>2</label><figDesc>Adhoc Results (manual runs), sorted by bpref.</figDesc><table coords="7,78.00,74.74,475.18,175.91"><row><cell></cell><cell>Run</cell><cell cols="3">bpref p@20 MAP infAP CPUs Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(sec)</cell></row><row><cell>uwaterloo-clarke</cell><cell>uwmtFmanual</cell><cell>0.4785 0.7030 0.4246 0.3503</cell><cell>1</cell><cell>20000</cell></row><row><cell>sabir.buckley</cell><cell>sabtb06man1</cell><cell>0.4104 0.6070 0.2666 0.2161</cell><cell>1</cell><cell>21600</cell></row><row><cell>pekingu.yan</cell><cell>TWTB06AD02</cell><cell>0.4089 0.5070 0.3152 0.2749</cell><cell>4</cell><cell>9625</cell></row><row><cell>rmit.scholer</cell><cell>zetaman</cell><cell>0.3976 0.5290 0.2873 0.2369</cell><cell>2</cell><cell>307</cell></row><row><cell>umilano.vigna</cell><cell>mg4jAdhocBV</cell><cell>0.3944 0.4930 0.2822 0.2465</cell><cell>4</cell><cell>610</cell></row><row><cell>umelbourne.ngoc-anh</cell><cell>MU06TBa1</cell><cell>0.3900 0.5420 0.2927 0.2431</cell><cell>8</cell><cell>15.50</cell></row><row><cell cols="3">ecole-des-mines.beigbeder AMRIMtpm5006 0.3793 0.4390 0.2705 0.2702</cell><cell>2</cell><cell>39032</cell></row><row><cell>ibm.carmel</cell><cell>JuruMan</cell><cell>0.3570 0.5190 0.2754 0.2410</cell><cell>1</cell><cell>60</cell></row><row><cell>northeasternu.aslam</cell><cell>hedge30</cell><cell>0.3180 0.5110 0.2561 0.1942</cell><cell>4</cell><cell>18000</cell></row><row><cell>max-planck.theobald</cell><cell>mpiirmanual</cell><cell>0.3041 0.4810 0.1981 0.1692</cell><cell>2</cell><cell>25</cell></row><row><cell>ualaska.fairbanks.newby</cell><cell>arscDomManL</cell><cell>0.1202 0.0400 0.0351 0.0511</cell><cell cols="2">108 150000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,87.72,80.75,438.92,262.90"><head>Table 3 :</head><label>3</label><figDesc>Named Page Finding Results. Best run from each group, according to MRR.</figDesc><table coords="8,87.72,80.75,438.92,227.98"><row><cell></cell><cell>Run</cell><cell>MRR</cell><cell>% Top 10</cell><cell>% Not Found</cell><cell>CPUs</cell><cell>Time (sec)</cell><cell>Links?</cell><cell>Anchors?</cell><cell>Structure?</cell></row><row><cell>umass.allan</cell><cell>indri06Nsdp</cell><cell cols="8">0.512 69.6 13.8 6 10860 Y Y Y</cell></row><row><cell>uglasgow.ounis</cell><cell>uogTB06MP</cell><cell cols="4">0.466 65.2 12.7 1</cell><cell></cell><cell cols="3">N Y Y</cell></row><row><cell>coveo.soucy</cell><cell>CoveoNPRun2</cell><cell cols="4">0.431 59.1 19.9 5</cell><cell cols="4">235 N N Y</cell></row><row><cell>tsinghuau.zhang</cell><cell>THUNPNOSTOP</cell><cell cols="4">0.430 64.1 16.0 2</cell><cell cols="4">3240 N Y N</cell></row><row><cell cols="2">hummingbird.tomlinson humTN06dpl</cell><cell cols="4">0.408 56.9 13.3 1</cell><cell cols="4">4600 N N Y</cell></row><row><cell>umelbourne.ngoc-anh</cell><cell>MU06TBn5</cell><cell cols="4">0.397 62.4 13.8 1</cell><cell cols="4">50 N Y N</cell></row><row><cell>rmit.scholer</cell><cell>zetnpft</cell><cell cols="4">0.389 54.7 19.3 2</cell><cell cols="4">2001 N N Y</cell></row><row><cell>uwaterloo-clarke</cell><cell>uwmtFnpsRR1</cell><cell cols="4">0.386 54.7 18.8 1</cell><cell cols="4">1149 Y Y Y</cell></row><row><cell>uamsterdam.ilps</cell><cell>UAmsT06n3SUM</cell><cell cols="4">0.363 55.2 23.8 2</cell><cell cols="4">2545 N Y Y</cell></row><row><cell>cas-ict.wang</cell><cell>icttb0603</cell><cell cols="4">0.337 44.2 28.7 1</cell><cell cols="4">427 N N Y</cell></row><row><cell>pekingu.yan</cell><cell>TWTB06NP02</cell><cell cols="4">0.238 34.3 44.2 4</cell><cell cols="4">3240 N N Y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.32,681.94,259.95,10.91"><head>â¢</head><label></label><figDesc>Zettair (http://www.seg.rmit.edu.au/zettair/);</figDesc><table coords="10,72.00,73.66,461.25,352.67"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Latency</cell><cell></cell><cell cols="3">Throughput</cell><cell cols="2">Effectiveness</cell></row><row><cell>Run</cell><cell>Number of CPUs</cell><cell>System cost (USD)</cell><cell>Query streams</cell><cell>measured (ms/query)</cell><cell>CPU-normalized</cell><cell>cost-normalized</cell><cell>measured (queries/s)</cell><cell>CPU-normalized</cell><cell>cost-normalized</cell><cell>P@20 (801-850)</cell><cell>MRR (901-1081)</cell></row><row><cell>CWI06DIST8</cell><cell>16</cell><cell cols="2">6,400 4</cell><cell>13</cell><cell>211</cell><cell cols="6">85 185.5 11.6 29.0 0.4680 0.181</cell></row><row><cell>CWI06MEM4</cell><cell cols="2">4 10,000</cell><cell>4</cell><cell>80</cell><cell>322</cell><cell>805</cell><cell cols="2">48.7 12.2</cell><cell cols="3">4.9 0.4720 0.190</cell></row><row><cell>humTE06i3</cell><cell>1</cell><cell cols="4">5,000 1 1,680 1,680</cell><cell>8,400</cell><cell>0.6</cell><cell>0.6</cell><cell cols="3">0.1 0.3690 0.123</cell></row><row><cell>humTE06v2</cell><cell>1</cell><cell cols="5">5,000 1 4,630 4,630 23,150</cell><cell>0.2</cell><cell>0.2</cell><cell cols="3">0.0 0.4290 0.373</cell></row><row><cell>mpiiotopk2p</cell><cell>2</cell><cell cols="2">5,000 4</cell><cell>29</cell><cell>57</cell><cell>143</cell><cell cols="2">35.0 17.5</cell><cell cols="3">7.0 0.4330 0.280</cell></row><row><cell>mpiiotopkpar</cell><cell>2</cell><cell cols="2">5,000 4</cell><cell>74</cell><cell>148</cell><cell>369</cell><cell>13.6</cell><cell>6.8</cell><cell cols="3">2.7 0.4280 0.291</cell></row><row><cell>MU06TBy6</cell><cell>1</cell><cell cols="2">500 4</cell><cell>55</cell><cell>55</cell><cell>28</cell><cell cols="5">18.2 18.2 36.4 0.4890 0.271</cell></row><row><cell>MU06TBy2</cell><cell>1</cell><cell cols="2">500 1</cell><cell>229</cell><cell>229</cell><cell>114</cell><cell>4.4</cell><cell>4.4</cell><cell cols="3">8.8 0.5050 0.256</cell></row><row><cell>p6tbep8</cell><cell>1</cell><cell cols="2">1,400 1</cell><cell>109</cell><cell>109</cell><cell>153</cell><cell>9.1</cell><cell>9.1</cell><cell cols="3">6.5 0.3890 0.254</cell></row><row><cell>p6tbeb</cell><cell>1</cell><cell cols="2">1,400 1</cell><cell>167</cell><cell>167</cell><cell>234</cell><cell>6.0</cell><cell>6.0</cell><cell cols="3">4.3 0.4540 0.244</cell></row><row><cell>rmit06effic</cell><cell>2</cell><cell cols="4">4,000 1 2,202 4,404</cell><cell>8,808</cell><cell>0.5</cell><cell>0.2</cell><cell cols="3">0.1 0.4650 0.258</cell></row><row><cell>THUTeraEff02</cell><cell>4</cell><cell cols="2">2,000 1</cell><cell cols="2">534 2,136</cell><cell>1,068</cell><cell>1.9</cell><cell>0.5</cell><cell cols="3">0.9 0.1500 0.222</cell></row><row><cell>THUTeraEff01</cell><cell>4</cell><cell cols="2">2,000 1</cell><cell cols="2">808 3.232</cell><cell>1,616</cell><cell>1.2</cell><cell>0.3</cell><cell cols="3">0.6 0.3920 0.246</cell></row><row><cell>uwmtFdcp03</cell><cell>1</cell><cell cols="2">1,800 1</cell><cell>13</cell><cell>13</cell><cell>23</cell><cell cols="5">80.0 80.0 44.4 0.4110 0.164</cell></row><row><cell>uwmtFdcp12</cell><cell>1</cell><cell cols="2">1,800 1</cell><cell>32</cell><cell>32</cell><cell>58</cell><cell cols="5">31.4 31.4 17.4 0.4790 0.219</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,72.00,73.66,467.97,131.15"><head>Table 5 :</head><label>5</label><figDesc>Comparative efficiency runs using Indri, Wumpus, and Zettair. Each field contains the average query latency in seconds (left), the average CPU time per query in seconds (middle), and the CPU type (right), either 32-bit or 64-bit.</figDesc><table coords="12,79.68,73.66,452.11,81.11"><row><cell></cell><cell>Comp. Run</cell><cell>Indri</cell><cell>Wumpus</cell><cell>Zettair</cell></row><row><cell>umass.allan</cell><cell>n/a</cell><cell cols="3">7.38/6.04/32-bit 0.48/0.32/32-bit 1.83/0.92/32-bit</cell></row><row><cell>max-planck.theobald</cell><cell>mpiiotopk2</cell><cell></cell><cell>0.23/0.18/64-bit</cell></row><row><cell>rmit.scholer</cell><cell>rmit06effic</cell><cell cols="3">5.89/5.24/32-bit 0.39/0.30/32-bit 1.50/1.10/32-bit</cell></row><row><cell cols="5">lowlands-team.deVries CWI06DISK1 4.64/4.17/64-bit 0.22/0.14/64-bit 0.93/0.81/32-bit</cell></row><row><cell>uwaterloo-clarke</cell><cell>uwmtFdcp12</cell><cell cols="3">4.49/3.02/64-bit 0.24/0.11/64-bit 1.62/0.62/32-bit</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,72.00,73.66,467.96,104.03"><head>Table 6 :</head><label>6</label><figDesc>Normalized efficiency based on the true performance of the underlying hardware configuration (estimated through Wumpus). Hardware performance is given relative to umass.allan(2.6   </figDesc><table coords="13,82.80,73.66,446.32,67.55"><row><cell>Run</cell><cell cols="4">HW performance Latency Normalized latency P@20 MRR</cell></row><row><cell cols="2">CWI06DISK1 CPU: 2.23, Total: 2.23</cell><cell>197 ms</cell><cell>439 ms . . . 440 ms 0.4720</cell><cell>0.196</cell></row><row><cell>mpiiotopk2</cell><cell>CPU: 1.71, Total: 2.15</cell><cell>75 ms</cell><cell>128 ms . . . 161 ms 0.4330</cell><cell>0.280</cell></row><row><cell>rmit06effic</cell><cell cols="3">CPU: 1.05, Total: 1.26 2,202 ms 2,304 ms . . . 2,768 ms 0.4650</cell><cell>0.258</cell></row><row><cell>uwmtFdcp12</cell><cell>CPU: 2.89, Total: 2.01</cell><cell>32 ms</cell><cell>64 ms . . . 92 ms 0.4790</cell><cell>0.219</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,88.56,672.78,450.87,8.47;5,72.00,683.90,65.55,8.27"><p>A technical note describing infAP can be found at http://trec.nist.gov/act part/tracks/terabyte/ inferredAP.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,88.56,672.74,136.35,8.27"><p>http://www.wumpus-search.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank everyone who helped to establish and operate the track over the past three years, particularly <rs type="person">Yaniv Bernstein</rs>, <rs type="person">Nick Craswell</rs>, <rs type="person">David Hawking</rs>, <rs type="person">Doug Oard</rs>, <rs type="person">Falk Scholer</rs> and <rs type="person">Ellen Voorhees</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="14,88.92,99.10,451.13,10.91;14,88.92,112.66,450.75,10.91;14,88.92,126.22,93.90,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,260.20,99.10,274.45,10.91">A scalable system for identifying co-derivative documents</title>
		<author>
			<persName coords=""><forename type="first">Yaniv</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,102.36,112.66,369.66,10.91">Proceedings of the Symposium on String Processing and Information Retrieval</title>
		<meeting>the Symposium on String Processing and Information Retrieval<address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="55" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,88.92,148.78,451.05,10.91;14,88.92,162.34,216.28,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,430.20,148.78,109.78,10.91;14,88.92,162.34,33.43,10.91">Bias and the limits of pooling</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darrin</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,145.68,162.34,125.93,10.91">Proceedings of SIGIR 2006</title>
		<meeting>SIGIR 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,88.92,184.78,451.17,10.91;14,88.92,198.34,451.14,10.91;14,88.92,211.90,320.56,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,282.27,184.78,235.76,10.91">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,88.92,198.34,451.14,10.91;14,88.92,211.90,153.91,10.91">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,88.92,234.46,451.14,10.91;14,88.92,248.02,450.90,10.91;14,88.92,261.58,265.71,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,309.04,234.46,211.21,10.91">Overview of the TREC 2004 Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,88.92,248.02,269.67,10.91">Proceedings of the Thirteenth Text REtrieval Conference</title>
		<meeting>the Thirteenth Text REtrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
	<note>See trec.nist.gov</note>
</biblStruct>

<biblStruct coords="14,88.92,284.02,451.06,10.91;14,88.92,297.58,451.15,10.91;14,88.92,311.14,198.75,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,298.25,284.02,152.88,10.91">The TREC 2005 Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,473.28,284.02,66.71,10.91;14,88.92,297.58,198.99,10.91">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
			<biblScope unit="page" from="500" to="266" />
		</imprint>
	</monogr>
	<note>See trec.nist.gov</note>
</biblStruct>

<biblStruct coords="14,88.92,333.70,451.06,10.91;14,88.92,347.26,451.15,10.91;14,88.92,360.70,198.75,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,192.14,333.70,249.89,10.91">Overview of the TREC 2005 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,471.36,333.70,68.63,10.91;14,88.92,347.26,198.99,10.91">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
			<biblScope unit="page" from="500" to="266" />
		</imprint>
	</monogr>
	<note>See trec.nist.gov</note>
</biblStruct>

<biblStruct coords="14,88.93,383.26,451.14,10.91;14,88.92,396.82,280.60,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,251.90,383.26,288.17,10.91;14,88.92,396.82,47.02,10.91">Estimating average precision with incomplete and imperfect judgments</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,159.60,396.82,125.60,10.91">Proceedings of CIKM 2006</title>
		<meeting>CIKM 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
