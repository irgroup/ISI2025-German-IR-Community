<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.46,78.43,349.12,11.47;1,246.84,107.77,118.37,11.47">Combining Language Model with Sentiment Analysis for Opinion Retrieval of Blog-Post</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.00,130.74,59.65,8.61"><forename type="first">Xiangwen</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Software Department</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.27,130.74,48.94,8.61"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Software Department</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.35,130.74,45.52,8.61"><forename type="first">Songbo</forename><surname>Tan</surname></persName>
							<email>tansongbo@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Software Department</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.33,130.74,30.13,8.61"><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<email>liuyue@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Software Department</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.05,130.74,59.16,8.61"><forename type="first">Guodong</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Software Department</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,424.53,130.74,52.62,8.61"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Software Department</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.46,78.43,349.12,11.47;1,246.84,107.77,118.37,11.47">Combining Language Model with Sentiment Analysis for Opinion Retrieval of Blog-Post</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">608FC20EBAD813636C10828C47EC3D7F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in Blog Opinion Retrieval task this year. We conduct experiments on "FirteX" platform that is developed by our lab. Language Model is used to retrieve related blog unit. Interactive Knowledge is adopted to expand query for retrieve blog unit include opinion. Then we introduce a novel extracting technology to extract text from retrieved blog-post. Finally, Lexicon based method is used to rerank the document by opinion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since opinion retrieval is a new retrieve task emerging this year, our aim to Blog Opinion Retrieval task is to construct an opinion retrieval framework using Blog data as an important test bed. Compared with traditional retrieval task, opinion retrieval is more challenged since it is very difficult to judge opinion about some topic.</p><p>After filtering spam permalink data, we obtain about 80G permalink data. Statistics Language Model [1] is used to retrieve related blog unit. Interactive Knowledge is adopted to expand query for retrieve blog unit including opinion. Then we introduce a novel extracting technology to extract text from the retrieved result. In order to rank the documents with stronger opinion in topper position, Lexicon <ref type="bibr" coords="1,210.48,438.96,11.50,8.61" target="#b1">[2]</ref> based method is used to rerank the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieve Model</head><p>In our system, we used the basic language modeling approach. The language modeling approach to IR is attractive and promising because it connects the problem of retrieval with that of language model estimation. The basic idea behind it can be described as follows.</p><p>Suppose we have a collection with total N documents, whose vocabulary is</p><formula xml:id="formula_0" coords="1,420.48,527.04,83.47,9.35">V = {w 1 , w 2 , …, w L }.</formula><p>Given a query Q = q 1 q 2 …q m (q k ∈V, 1≤k≤m), the task is to rank the documents according to the relevance of each document with the query. Based on the assumption of words independence, the language model approach views each document D as an observed sequence of words generated by a multinomial language model parameterized by</p><formula xml:id="formula_1" coords="1,309.00,583.31,125.47,11.88">θ D = (θ D1 , θ D2 , …, θ DL )∈[0,1] L</formula><p>, where θ D1 + θ D2 + … + θ DL =1 and θ Di = p(w i |θ D ), the probability of observing word w i under the multinomial generation model (i.e. unigram model) parameterized by θ D . And the relevance of document D with the query Q is measured by the likelihood of Q according to the multinomial model θ D :</p><formula xml:id="formula_2" coords="1,217.32,644.37,284.09,22.02">∏ ∏ ∈ = = = Q w Q w c D m i D i D w p q p Q P ) , ( 1 ) | ( ) | ( ) | ( θ θ θ (1)</formula><p>where c(w,Q) is the frequency of word w occurring in query Q.</p><p>As shown in equation (1), the retrieval problem is now essentially reduced to a multinomial language model estimation problem. The simplest method to estimate θ D is to utilize the maximum likelihood estimator (MLE):</p><formula xml:id="formula_3" coords="2,260.40,117.14,241.01,20.12">|D| D w c w p w p D ML D ) , ( ) | ( ) | ( = = θ θ (2)</formula><p>where c(w,D) is the frequency of word w occurring in document D and |D| is the length of D, i.e.</p><formula xml:id="formula_4" coords="2,111.12,163.30,54.17,14.13">∑ = w D w c |D| ) , (</formula><p>. However, using MLE can cause serious zero probability problem due to data sparseness. A direct solution to this problem is smoothing, which adjusts the MLE so as to assign a nonzero probability to the unseen words and improve the accuracy of language model estimation.</p><p>According to <ref type="bibr" coords="2,167.46,233.52,10.49,8.61" target="#b4">[5]</ref>, there are three popular smoothing methods applied to the language modeling approaches to ad hoc IR: Jelinek-Mercer, Dirichlet and Absolute Discounting, summarized in Table <ref type="table" coords="2,136.32,262.86,3.72,8.61">1</ref>. Notice that |D| u is the number of unique words in document D. The collection language model θ C is typically estimated by MLE based on the whole collection, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑ ∑ ∑</head><formula xml:id="formula_5" coords="2,156.12,292.28,345.29,124.59">= = w D D C ML C D w c D w c w p w p ) , ( ) , ( ) | ( ) | ( θ θ (3) Table 1. Summary of the three popular smoothing methods Jelinek-Mercer ) | ( ) 1 ( ) | ( ) | ( C D ML D w p w p w p θ θ θ λ λ - + = Dirichlet µ µ + + = |D| w p D w c w p C D ) | ( ) , ( ) | ( θ θ Absolute Discounting ) | ( ) 0 , ) , ( max( ) | ( C u D w p |D| |D| |D| D w c w p θ θ δ δ + - =</formula><p>In our experiments, we utilized the Dirichlet smoothing method for the document language model estimation, where the smoothing parameter u is set to 2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Expansion Using Interactive Knowledge</head><p>Query expansion is an effect method to bridge the semantic gap between the vocabulary of documents and that of users. There are three kinds of traditional query expansion approaches: Lexicon based Query Expansion, Global-Analysis Based Query Expansion <ref type="bibr" coords="2,422.94,528.90,10.46,8.61" target="#b5">[6]</ref>, Local-Analysis based Query Expansion <ref type="bibr" coords="2,210.18,543.60,10.46,8.61" target="#b3">[4]</ref>. Lexicon based Query Expansion uses lexicon, such as WordNet, to select terms semantically related to Query terms. Global-Analysis Based Query Expansion selects terms co-occurring in the Corpus with Query terms, while Local-Analysis based Query Expansion selects terms co-occurring in the top-N documents with Query terms.</p><p>Query expansion can be performed either manually or automatically. During opinion retrieval task, we are concerned with semi-automatic query expansion. For each query, traditional query expansion often selects expansion term by co-occurrence statistics. We first classify each query into different categories. For example, the query "March of the Penguins" belongs to film category. And then select terms that could describe the firm from Inquirerbasic lexicon <ref type="bibr" coords="2,423.44,661.02,10.45,8.61" target="#b1">[2]</ref>. After obtaining the first initiated result, we get the adj terms around the query from top-50 documents and then manually select some terms much more suitable to describe the film. Finally, we use these terms to expand the query and retrieve the top 1000 documents.</p><p>Blog Extraction is a kind of difficult job because each blog site has a lot of different blog templates which can be chosen by blogers. Each template has different styles which greatly enhance the difficulty of extraction. Under this condition, we cannot easily build one extracting method for any blog template. In order to address this issue, we need to build up a common infrastructure which grasps the essential factors of blog pages. With the aim to achieve this goal, we combine the similarity of html tag tree with the similarity of content to find the most suitable extracting range.</p><p>Based on above idea, we propose a novel extraction algorithm that can effectively extract the main texts and comments from blog pages. It is described as Figure <ref type="figure" coords="3,380.44,204.18,3.69,8.61">1</ref>.</p><p>Figure1: The outline of extracting algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Lexicon based Opinion Ranking</head><p>The sentiment Lexicon we used for identifying positive and negative terms is taken from the General Inquirer <ref type="bibr" coords="3,182.07,556.38,11.53,8.61" target="#b1">[2]</ref> (GI for simplicity), which is a dictionary that contains information about English word senses. GI is a system which lists terms as well as different senses for the terms. For each sense it provides a short definition as well as other information about the term. This includes tags that label the term as being positive, negative, a negation term, an overstatement, or an understatement. For example, there are two senses of the word "fun". One sense is a noun or adjective for enjoyment or enjoyable. The second sense is a verb that means to ridicule or tease, to make fun of. The first sense of the word is positive, marked as "Positiv", while the second is negative, marked as "Negativ". There are other labels for each sense.</p><p>One difficulty in using this lexicon is that English word always contains many inflections. For example, the word "take" may has four inflections: "takes", "taking", "took", "taken". In order to address this problem we employed the Porter stemming algorithm2 <ref type="bibr" coords="4,387.11,72.06,10.44,8.61" target="#b2">[3]</ref>. This algorithm does not produce a lemma for a term, but rather maps similar words to a string. After removing word suffix, we obtain 1743 negative words and 1313 positive words. The method we used to classify a blog-post is very simple and straightforward, that is, only to count positive and negative terms in it. If the blog-post contains more positive than negative terms we deem it as positive, and if the number of negative terms is greater than the number of positive terms we assign it as negative. If a blog-post contains an equal number of positive or negative terms, we say it is neutral.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,168.66,234.76,282.04,241.50"><head></head><label></label><figDesc>Choose a tag which has more words and its position is wider than other sibling tags in html tag tree. 1.4 Use the range of chosen html tag as the main text range. 2. Locate the title of main text.2.1 For each line in range, find the lines which are highlighted or have bigger font size. 2.2 Use the words in "&lt;title&gt;" as a clue to decide which selected line is title. 3. Locate the start of comments.3.1 Find those lines which have some key words of start of comment, like "comment", "reply". 3.2 Choose the line which has more few words and has bigger font size. 4. Calculate the number of comments.4.1 From the start of comments, find each line which has time expression. 4.2 Compute the difference of those selected lines and choose the more similar lines as a symbol of comment. 4.3 Compute the start and end of each comment through the html tag tree.</figDesc><table coords="3,168.66,234.76,281.99,56.53"><row><cell>Extracting algorithm:</cell></row><row><cell>1. Locate the range of main text.</cell></row><row><cell>1.1 Build a html tag tree.</cell></row><row><cell>1.2 For each html tag, calculate the number of word in it and its</cell></row><row><cell>position in vision.</cell></row><row><cell>1.3</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,166.05,218.82,337.88,8.61;4,110.64,233.52,158.68,8.61" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,272.56,218.82,227.50,8.61">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,121.40,233.52,105.08,8.61">Proceedings of SIGIR1998</title>
		<meeting>SIGIR1998</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,125.11,248.16,376.41,8.61;4,110.64,262.86,331.96,8.61" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,486.22,248.16,15.30,8.61;4,110.64,262.86,238.99,8.61">The General Inquirer: A Computer Approach to Content Analysis</title>
		<author>
			<persName coords=""><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dexter</forename><forename type="middle">C</forename><surname>Dunphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marshall</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Associates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,124.64,277.56,328.55,8.61" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,191.79,277.56,129.14,8.61">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,327.22,277.56,31.85,8.61">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,126.36,292.20,375.09,8.61;4,110.64,306.90,390.97,8.61;4,110.64,321.54,259.57,8.61" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,363.21,292.20,138.24,8.61;4,110.64,306.90,29.35,8.61">Automatic query expansion using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,159.12,306.90,236.27,8.61">Proceedings of the 3rd Text Retrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the 3rd Text Retrieval Conference (TREC-3)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,126.21,336.24,375.28,8.61;4,110.64,350.94,162.40,8.61" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,300.46,336.24,162.87,8.61">A hierarchical Dirichlet language model</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linda</forename><forename type="middle">C</forename><surname>Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,471.93,336.24,29.56,8.61;4,110.64,350.94,88.46,8.61">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.66,365.58,372.85,8.61;4,110.64,380.28,90.07,8.61" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,238.54,365.58,259.10,8.61">Automatic Keyword Classification for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<pubPlace>Butterworths, London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
