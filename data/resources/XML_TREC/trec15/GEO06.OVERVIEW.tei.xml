<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.86,76.37,254.18,13.97">TREC 2006 Genomics Track Overview</title>
				<funder ref="#_4MMTtmV">
					<orgName type="full">U.S. National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Lori Buckland of NIST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.42,105.96,65.08,9.57"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<email>hersh@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.58,105.96,74.16,9.57"><forename type="first">Aaron</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
							<email>cohenaa@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.80,105.96,69.28,9.57"><forename type="first">Phoebe</forename><surname>Roberts</surname></persName>
							<email>phoebe.roberts@biogenidec.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Biogen Idec Corp</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.03,105.96,102.09,9.57"><forename type="first">Hari</forename><forename type="middle">Krishna</forename><surname>Rekapalli</surname></persName>
							<email>rekapall]@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.86,76.37,254.18,13.97">TREC 2006 Genomics Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92403A0565621B7EFBD8CA39308C5D7A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Genomics Track implemented a new task in 2006 that focused on passage retrieval for question answering using full-text documents from the biomedical literature. A test collection of 162,259 full-text documents and 28 topics expressed as questions was assembled. Systems were required to return passages that contained answers to the questions. Expert judges determined the relevance of passages and grouped them into aspects identified by one or more Medical Subject Headings (MeSH) terms. Document relevance was defined by the presence of one or more relevant aspects. The performance of submitted runs was scored using mean average precision (MAP) at the passage, aspect, and document level. In general, passage MAP was low, while aspect and document MAP were somewhat higher.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of most information retrieval (IR) systems is to retrieve documents that a user might find relevant to his or her information need. In contrast, the goal of most information extraction (IE) or text mining (TM) systems is to process document text to provide the user with one or more "answers" to a question or information need <ref type="bibr" coords="1,150.22,536.03,110.67,9.57" target="#b2">(Cohen and Hersh, 2005;</ref><ref type="bibr" coords="1,72.00,548.69,63.39,9.57" target="#b8">Roberts, 2006)</ref>. We propose that what many information seekers, especially users of the biomedical literature, really desire is something in the middle, i.e., a system that attempts to provide short, specific answers to questions and put them in context by providing supporting information and linking to original sources <ref type="bibr" coords="1,72.00,637.26,59.13,9.57" target="#b4">(Hersh, 2005)</ref>. This motivated us to go beyond the ad hoc retrieval task from previous years of the TREC Genomics Track <ref type="bibr" coords="1,194.86,662.52,91.03,9.57;1,72.00,675.18,25.07,9.57" target="#b6">(Hersh, Cohen et al., 2005;</ref><ref type="bibr" coords="1,99.78,675.18,137.18,9.57" target="#b5">Hersh, Bhupatiraju et al., 2006)</ref>.</p><p>For the TREC 2006 Genomics Track, we developed a new task that focused on retrieval of short passages (from phrase to sentence to paragraph in length) that specifically addressed an information need, along with linkage to the location in the original source document. Topics were expressed as questions and systems were measured on how well they retrieved relevant information at the passage, aspect, and document levels. Systems were required to return passages linked to source documents, while relevance judges not only rated the passages, but also grouped them by aspect. For this task, aspect was defined similar to its definition in the TREC Interactive Track aspectual recall task <ref type="bibr" coords="1,415.38,371.63,59.14,9.57" target="#b3">(Hersh, 2001)</ref>, representing answers that covered a similar portion of a full answer to the topic question. We also drew upon experience in passage retrieval from the previous TREC High Accuracy Retrieval from Documents (HARD) Track <ref type="bibr" coords="1,446.48,434.87,58.62,9.57" target="#b0">(Allan, 2003;</ref><ref type="bibr" coords="1,507.88,434.87,27.19,9.57;1,324.00,447.47,23.73,9.57" target="#b1">Allan, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Document collection</head><p>The documents for this year's task came from a new full-text biomedical corpus. We obtained permission from a number of publishers who use Highwire Press (www.highwire.org) for electronic distribution of their journals. They agreed to allow us to include their full text in HTML format, which preserved formatting, structure, table and figure legends, etc. The document collection was derived from 49 journals and were obtained by a Web crawl of the Highwire site, with post-processing to eliminate as much non-article material as we could. The full collection contained 162,259 documents. The collection was about 12.3 GB when uncompressed. Appendix 1 lists the journals and number of documents from each.</p><p>Several notable issues were uncovered when the collection was compiled:</p><p>• The collection was not complete from the standpoint of each entire journal. That is, there were some articles that appeared in the journal but did not make it into our collection. This was acceptable to us, since we viewed the collection as a closed and fixed collection.</p><p>• Some of the PMIDs were incorrect, emanating from errors in the URLs linking to Pubmed in the source data from Highwire Press.</p><p>• Some of the HTML files were empty or nearly empty (i.e., only contained a small amount of meaningless text). Some of this was due to errors in our processing, but most was related to the incorrect or ambiguous links on the Highwire site and in the HTML documents themselves. We kept these files in the collection since they were small and unlikely to have any relevant passages.</p><p>We also created a text file, metadata.txt (Windows ASCII format, 11.9 MB), which listed the original URL of the article, the file name in our collection, and the file size in kilobytes. The name of each document file was its PMID plus the extension ".html", which facilitated accessing the associated MEDLINE record.</p><p>In addition to the full-text data, the National Library of Medicine (NLM) provided us with both ASCII and XML formatted collections of all the MEDLINE records for the full-text documents in our Highwire collection. We identified 1,767 instances (about 1% of the 162K documents) where the Highwire file PMID was invalid. We investigated the problem and found that for all of instances we checked, the problem was in the original Highwire HTML file having an incorrect PMID in the link to the PubMed record. In other words, the error was inherent in the Highwire data, and not introduced as a result of our processing.</p><p>Another file made available to participants was legalspans.txt. This file contained all "legal spans" for all documents in the collection. Legal spans were defined as any contiguous text &gt;0 characters in length not including any HTML paragraph tags, defined as any tag that started with &lt;P or &lt;/P (case insensitive). There were a total of 12,641,127 legal spans in the collection. We used these spans to define allowed passages in the pooling and evaluation process, and to limit the size of the passages that needed reviewing by the expert judges Retrieved passages could contain any span of text that did not include any part of an HTML paragraph tag (i.e., one starting with &lt;P or &lt;/P). Because there was some confusion about the different types of passages, we defined the following terms:</p><p>• Nominated passage -This was the passage that systems nominated in their runs and were scored in the passage retrieval evaluation. To be legal, these passages had to be a subset of a maximum-length legal span.</p><p>• Maximum-length legal span -These were all the passages obtained by delimited the text of each document by the HTML paragraph tags. As noted below, nominated passages could not cross an HTML paragraph boundary. So these spans represented the longest possible passage that could be designated as relevant. As also noted below, we built pools of these spans for the relevance judges. The judges were given the plain text from the entire maximum-length legal span, even if no system nominated the entire span. However, the judges did not need to designate the entire span as relevant, and were able to select just a part of the span as the relevant passage. Each maximum length span was identifier by a triple value of (PMID, offset, length).</p><p>• Relevant passage -These were the spans that the judges designated as definitely or possibly relevant. These were portions of the original HTML files, represented by the value triple: PMID, offset, and length. These spans may or may not include HTML markup tags, depending upon whether these tags were inside the relevant answer passages designated by the experts.</p><p>The following should also be noted about the maximum-length legal spans:</p><p>• The first and last spans were delimited at the beginning and end of the file respectively.</p><p>• Other HTML tags (e.g., &lt;B&gt;) could occur within the spans.</p><p>• "Empty" (zero character) spans were not included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Topics</head><p>The topics for the 2006 track were expressed as questions. They were derived from the set of biologically relevant questions based on the Generic Topic Types (GTTs) developed last year for the 2005 track. These questions each had one or more aspects that were contained in the literature corpus (i.e., one or more answers to each question). A few things should be noted about the topics for 2006:</p><p>• Even though the questions were derived from the 2005 track topics, many of them changed, some substantially.</p><p>• Groups were instructed that if their systems made use of knowledge about the 2005 topics, then they needed to classify their 2006 runs as interactive, even if they only used automated methods in 2006.</p><p>• The official topics were the text of the questions in the text file that was provided. We also provided an Excel spreadsheet, and corresponding PDF, which showed the 2005 topics from which the 2006 topics were derived. However, the information from the 2005 questions was for reference only, and was not to be considered part of the 2006 data.</p><p>The questions (and GTTs) all had the general format of containing one or more biological objects and processes and some explicit relationship between them:</p><p>Biological object (1..many) ← relationship → Biological process (1..many)</p><p>The biological objects might be genes, proteins, gene mutations, etc. The biological process could be physiological processes or diseases. The relationships could be anything, but were typically verbs such as causes, contributes to, affects, associated with, or regulates. We determined that four out of the five GTTs from 2005 could be reformulated into the above structure, with the exception of the first GTT that asked about procedures or methods. The patterns for doing this from the GTTs were based on the examples in Table <ref type="table" coords="3,465.12,125.51,4.12,9.57" target="#tab_0">1</ref>. The topics for the 2006 track are listed in Table <ref type="table" coords="3,471.61,138.17,4.12,9.57">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions</head><p>Submitted runs could contain up to 1000 passages per topic that were predicted to be relevant to answering the topic question. Passages had to be identified by the PMID, the start offset into the text file in characters, and the length of the passage in characters. The first character of each file was defined to be at offset zero.</p><p>Passages were required to be contiguous and not longer than one paragraph. As described above, this was operationalized by prohibiting any passage from containing HTML markup tags, i.e., those starting with &lt;P or &lt;/P. Any passages containing these tags were ignored in the judgment pooling process but not omitted from the scoring process. (In other words, not counted as potentially relevant for pooling but counted as retrieved for scoring.) Each participating group was allowed to submit up to three official runs, all of which were used for building pools. Each passage was required to be assigned a corresponding rank number and value. The rank number, starting at one and ascending, was used to order nominated passages for rank-based performance computations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTT</head><p>Question Pattern Example Find articles describing the role of a gene involved in a given disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is the role of gene in disease?</head><p>What is the role of DRD4 in alcoholism? Find articles describing the role of a gene in a specific biological process.</p><p>What effect does gene have on biological process? What effect does the insulin receptor gene have on tumorigenesis? Find articles describing interactions <ref type="bibr" coords="4,72.00,188.76,164.08,9.57">(e.g., promote, suppress, inhibit, etc.)</ref> between two or more genes in the function of an organ or in a disease.</p><p>How do genes interact in organ function?</p><p>How do HMG and HMGB1 interact in hepatitis?</p><p>Find articles describing one or more mutations of a given gene and its biological impact.</p><p>How does a mutation in gene influence biological process?</p><p>How does a mutation in Ret influence thyroid function?</p><p>Table <ref type="table" coords="4,99.81,277.32,5.49,9.57">2</ref> -Topics for TREC 2006 Genomics Track. &lt;160&gt;What is the role of PrnP in mad cow disease? &lt;161&gt;What is the role of IDE in Alzheimer's disease &lt;162&gt;What is the role of MMS2 in cancer? &lt;163&gt;What is the role of APC (adenomatous polyposis coli) in colon cancer? &lt;164&gt;What is the role of Nurr-77 in Parkinson's disease? &lt;165&gt;How do Cathepsin D (CTSD) and apolipoprotein E (ApoE) interactions contribute to Alzheimer's disease? &lt;166&gt;What is the role of Transforming growth factor-beta1 (TGF-beta1) in cerebral amyloid angiopathy (CAA)? &lt;167&gt;How does nucleoside diphosphate kinase (NM23) contribute to tumor progression? &lt;168&gt;How does BARD1 regulate BRCA1 activity? &lt;169&gt;How does APC (adenomatous polyposis coli) protein affect actin assembly &lt;170&gt;How does COP2 contribute to CFTR export from the endoplasmic reticulum? &lt;171&gt;How does Nurr-77 delete T cells before they migrate to the spleen or lymph nodes and how does this impact autoimmunity? &lt;172&gt;How does p53 affect apoptosis? &lt;173&gt;How do alpha7 nicotinic receptor subunits affect ethanol metabolism? &lt;174&gt;How does BRCA1 ubiquitinating activity contribute to cancer? &lt;175&gt;How does L2 interact with L1 to form HPV11 viral capsids? &lt;176&gt;How does Sec61-mediated CFTR degradation contribute to cystic fibrosis? &lt;177&gt;How do Bop-Pes interactions affect cell growth? &lt;178&gt;How do interactions between insulin-like GFs and the insulin receptor affect skin biology? &lt;179&gt;How do interactions between HNF4 and COUP-TF1 suppress liver function? &lt;180&gt;How do Ret-GDNF interactions affect liver development? &lt;181&gt;How do mutations in the Huntingtin gene affect Huntington's disease? &lt;182&gt;How do mutations in Sonic Hedgehog genes affect developmental disorders? &lt;183&gt;How do mutations in the NM23 gene affect tracheal development? &lt;184&gt;How do mutations in the Pes gene affect cell growth? &lt;185&gt;How do mutations in the hypocretin receptor 2 gene affect narcolepsy? &lt;186&gt;How do mutations in the Presenilin-1 gene affect Alzheimer's disease? &lt;187&gt;How do mutations in familial hemiplegic migraine type 1 (FHM1) gene affect calcium ion influx in hippocampal neurons? Each submitted run was submitted in a separate file, with each line defining one nominated passage using the following format based loosely on trec_eval. Each line in the file had to contain the following data elements, separated by white space:</p><p>• Topic ID -from 160 to 187.</p><p>• Doc ID -name of the HTML file minus the .html extension. This was the PMID that was designated by Highwire, even though this may not have been the true PMID assigned by the NLM (i.e., used in MEDLINE). But this was the official identifier for the document within the corpus.</p><p>• Rank number -rank of the passage for the topic, starting with 1 for the top-ranked passage and preceding down to as high as 1000.</p><p>• Rank value -system-assigned score for the rank of the passage, an internal number that should descend in value from passages ranked higher.</p><p>• Passage start -the character offset in the Doc ID file where the passage begins, where the first character of the file is offset 0.</p><p>• Passage length -the length of the passage in 8-bit ASCII characters.</p><p>• Run tag -a tag assigned by the submitting group that should be distinct from all the group's other runs.</p><p>Because of the complex nature of this year's task, and most groups' not having a system in place before release of the topics, the classification of runs was complicated. "Usual" TREC rules (detailed at http://trec.nist.gov/act_part/guidelines/trec8_gui des.html) would ordinarily categorize runs as follows:</p><p>• Automatic -no human modification of topics into queries for a system whatsoever.</p><p>• Manual -human modification of queries entered into a system but no modification based on results obtained (i.e., you cannot look at the output from your runs to modify the queries). • Interactive -human interaction with the system, including modification of the queries after viewing the output (i.e., you look at the output from the topics and corpus and adjust your system to produce different output).</p><p>However, because we reused topics (with modification, sometimes substantial) from 2005, and because people were building systems up to the last minute, we adopted the following rules to be applied to classification of runs:</p><p>• If a group made any tuning or optimization of their retrieval system based on last year's topics, then their run should have been categorized as interactive this year, even if they did everything else in an automated fashion.</p><p>• If a group made any human generated modifications to the topic statements or their system for queries entered into their system, then the run should have been classified as manual.</p><p>• If groups made any modifications to the topic statements or their system for the queries entered into it based on looking at the output of passages and/or documents, then their run should have been classified as interactive.</p><p>As with many TREC tasks, groups were allowed to manually modify topics to create their queries to their systems. In addition, they were allowed to consult outside resources on the Web (e.g., gene databases), but only in a fully automated fashion. In other words, the original queries could be manually modified, but interaction with external resources could only be done in an automated fashion. For example, if a system pulled information from SOURCE, GenBank, or any other resource, the query to those sources and the information obtained from them had to be done in an automated way, i.e., without manual intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Relevance assessments a. Pooling</head><p>There were 92 submitted runs, with each nominating up to 1000 passages over 28 topics. Given our resources, this was far too much data to perform an exhaustive expert evaluation. Instead, we used a pooling method, similar to that used by other document retrieval tasks in TREC.</p><p>For each topic a separate pool of passages was created for expert judging. Each ranked and submitted passage consisted of a (PMID, offset, length) triple, which was mapped to its corresponding maximum-length span, also identified as a (PMID, offset, length) triple. These spans were distributed in the legalspans.txt made available before submissions were due. Then, for each topic, pooling was done by taking the top ranked maximal legal span from each submitted run in a round-robin fashion, until the topic pool contained 1000 unique spans. In other words, the top ranked passage was taken from each submitted run, and then the second ranked passage if not yet included, and so on, until the submissions were exhausted or a pool contained 1000 spans.</p><p>To consistently subdivide source documents into shorter passages, the HTML &lt;P&gt; tag was used to approximate splitting up the document into paragraphs; as noted above we called these maximum-length legal spans. Likewise, legal submitted passages were limited to not include any HTML &lt;P&gt; tags. By definition, maximal legal spans did not overlap. Therefore, any legally submitted passage would have to be either a maximal legal span or a subset of exactly one maximal legal span.</p><p>In addition to HTML &lt;P&gt; tags, additional markup characters were embedded in the text, hampering the readability (thought they generally rendered well in a browser). Maximal legal spans generated in the previous step were converted to plain text by removing the HTML markup. This allowed the judges to concentrate on the content of the passages instead of having to deal with erratic formatting issues. Despite the attempt to remove HTML formatting, plain text was not fully restored to publication quality. Common modifications included loss of inline images that represented characters such as Greek symbols, and lack of conversion of HTML entity codes to more easily readable plain text punctuation characters such as ampersands At times, for some judges, these changes proved to be a distraction.</p><p>The plain text content from the pooled spans was then imported into an Excel spreadsheet. Columns were added to allow easy relevance judging. A drop down menu was provided to set the relevance of each passage, and cells were provided for the judges to cut-and-paste relevant plain text from the maximal legal span text field into an "answer text" field. Another column was provided for judges to cut-and-paste MeSH terms corresponding to relevant passage aspects. To make the Excel forms more user friendly for the judges, hyperlinks were added to the PubMed record for the PMID for the journal article for each passage, and also to enable quick access to the PubMed MeSH browser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Judging</head><p>Relevance judges were provided with guidelines and a one-hour training course to improve the judging process. As this year's track was developed by the steering committee, the question and answer nature of the task raised discussion about what constituted a complete answer, prompting development of guidelines for dealing with anaphora and abbreviations to benefit participants and judges alike. In addition, the guidelines offered a brief introduction to MeSH, and tips for taking advantage of Excel features to monitor consistency and completion. Nine judges participated, and they were provided with an email list to discuss issues as they came up.</p><p>To assess relevance, judges were instructed to break down the question into required elements (e.g., the biological entities and processes that make up the GTT) and isolate the minimum contiguous substring that answered the question. In general, a passage was definitely relevant if it contained all required elements of the question and it answered the question. A passage was possibly relevant if it contained the majority of required elements, missing elements were within the realm of possibility (i.e. more general terms are mentioned that probably include the missing elements), and it possibly answered the question.</p><p>It was possible for a judge to designate any number of relevant passages from an individual article. It was also possible for a judge to designate multiple non-overlapping relevant passages from an individual pooled span. The judges evaluated the text of the maximum-length legal span for relevance, and identified the portion of this text that contained an answer, hereafter called the gold standard passage. This could be all of the text of the maximum legal span, or any contiguous substring. It was possible that one maximum legal span could contain two or more separate gold standard passages. Judges were instructed to duplicate rows with more than one gold standard passage, and process each row independently. Judges were not shown how many systems had retrieved each maximum-length span. Appendix 2 shows the number of maximum-length legal spans where part or all of the span was judged as definitely or possibly relevant; the remainder were counted as not relevant.</p><p>Relevance judges next determined the "best" answer passages and grouped them into related concepts. The judges then assigned one or more Medical Subject Headings (MeSH) terms (possibly with subheadings) to capture similarities and differences among retrieved passage aspects. We originally considered using Gene Ontology (GO) terms for this purpose, but an early analysis by our genomics domain expert determined that GO lacked sufficient coverage in many areas needed for this task and MeSH terms alone would provide sufficient coverage.</p><p>Judges assigned MeSH term-based aspects to each gold standard passage. They were instructed to use the most specific MeSH term, with the option of adding subheadings, similar to the NLM literature indexing process. If one term was insufficient to denote all aspects of the gold standard passage, judges assigned additional MeSH terms. All passages judged as definitely or possibly relevant were required to have a gold standard passage and at least one MeSH term.</p><p>A total of six topics were selected randomly for judgment in duplicate: <ref type="bibr" coords="7,173.84,682.07,107.16,9.57;7,72.00,694.73,33.08,9.57">160, 165, 176, 179, 181, and 185</ref>. (We hoped to have more topics judged in duplicate but were unable to recruit judges for additional work.) Table <ref type="table" coords="7,430.34,74.93,5.49,9.57" target="#tab_1">3</ref> shows the agreement of the original and duplicate judges, where agreement indicates that any part of a maximumlength legal span was judged as relevant or not. The kappa statistic was calculated to assess chance-corrected inter-rater agreement. For five of the topics, the kappa statistic indicated "good" inter-rater agreement, with a value of 0.60. For topic 181, however, the kappa statistic was poor, with a value of 0.028. This outlier brought the overall kappa value for the six topics down to 0.032. What happened for topic 181 was that one judge interpreted relevance to the question very broadly and the other very narrowly. Table <ref type="table" coords="7,397.02,251.99,5.49,9.57" target="#tab_2">4</ref> shows the agreement of original and duplicate judges for MeSH terms assigned for aspects, which shows an even poorer rate of agreement. (Kappa could not be calculated due to the inability to calculate the number of MeSH terms not assigned.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. Processing</head><p>The final result of the judging process was a set of filled-out forms in Excel spreadsheet format. Each spreadsheet corresponded to the judged passages for one topic, one row per passage. If a passage was marked "Not" relevant, no further processing needed to be done, as this passage was not included in the gold standard. Passages marked "Definitely" or "Possibly" relevant were treated as relevant for purposes of the gold standard. The "Definitely" and "Possibly" relevant passages also had two additional associated data items: the relevant answer text cut and pasted from the maximal legal span, and a list of pipe character-separated MeSH terms.</p><p>The text and MeSH data associated with the relevant passages was processed to create a set of gold standard passages for each topic. Each gold standard passage consisted of the PMID of the document that the passage was from, the starting character offset, the length of the gold standard passage, and a list of pipe characterseparated MeSH terms corresponding to the aspects for that passage. The starting character offset and length of the gold standard passage in the HTML journal article file was determined by an automated process. Using a dynamic programming algorithm similar to the third stage alignment step in <ref type="bibr" coords="8,103.82,416.45,171.89,9.57">BLAST (McGinnis and Madden, 2004)</ref>, the relevant answer text selected by the expert judge was aligned with the text of the corresponding maximum length span in the HTML file in order to determine the best overlap. This step had the effect of finding the plain answer text in the HTML file, accounting and skipping over any intervening HTML markup. The starting offset into the HTML file, along with the length in characters in the HTML file matching up with the answer text was taken to be the gold standard passage for that judgment.</p><p>As noted above, judges assigned MeSH terms to designate the aspects of a complete topic answer that were addressed by each relevant gold standard passage. This allowed grouping of answer passages and estimatation of the performance of systems in providing a complete answer. Ideally, the MeSH terms provided by the expert judges would have been copied from the MeSH browser without error. However, an additional processing step was necessary because of several types of variation. First, sometimes judges typed in MeSH terms instead of cut and pasting them. Spelling errors were introduced, and these needed to be corrected. A second type of error was created by judges using a MeSH entry term instead of the official MeSH descriptor. These entry terms needed to be mapped to the official term. A few errors were introduced by the judges when non-MeSH terms were used, these needed to be mapped to the closest official MeSH term.</p><p>Except for the spelling variations, judges were consistent within a topic, and so the MeSH term variations did not have any effect on the final results. However the MeSH assignments were normalized by mapping to the official MeSH descriptor in order to improve the overall quality and reusability of the test collection. A table driven program was created to fix these errors and map all aspects to official MeSH terms. The table was reviewed by our lead biological expert (P.R.) before finalizing the gold standard aspects. All MeSH terms were also normalized to upper case. Subheadings were preserved if used by the relevance judges.</p><p>After mapping the answer text to the HTML source documents and correcting variation in the MeSH terms, the gold standard passages for each topic were combined into a single file. This file contained 3451 gold standard passages, giving the topic identifier, the source document PMID, the starting offset and length of the relevant passage, and a list of pipe character separated normalized MeSH terms.</p><p>Appendix 3 lists the number, average length, and standard deviation of passages per topic as well as the number of aspects per topic. Table <ref type="table" coords="9,254.95,201.42,5.49,9.57" target="#tab_3">5</ref> shows the minimum, mean, median, and maximum for the topics of these values. It is clear that there is significant variation among the topics as far as number of relevant passages in the literature corpus, the length of those passages, and the number of aspects per topic that were found by the judges. Note that two topics, 173 and 180, had no relevant passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Performance Measures</head><p>For this year's track, there were three levels of retrieval performance that we measured: passage retrieval, aspect retrieval, and document retrieval. Each of these provided insight into the overall performance for a user trying to answer the given topic questions. Each was measured by some variant of mean average precision (MAP).</p><p>Because this was a new task, and uncharted research territory, we decided to measure the three types of performance separately. We did not propose any summary metric to grade overall performance, but instead wished to examine each aspect of performance in a way that was both as meaningful and straightforward as we could at our current level of experience with this task.</p><p>a. Passage-level MAP This measure used a variation of MAP, computing individual precision scores for passages based on character-level precision, using a variant of a similar approach used for the TREC 2004 HARD Track <ref type="bibr" coords="9,189.98,644.15,57.24,9.57" target="#b1">(Allan, 2004)</ref>. For each nominated passage, the number of characters that overlapped with those deemed relevant by the judges in the gold standard was determined. For each relevant retrieved passage, precision was computed as the fraction of characters overlapping with the gold standard passages divided by the total number of characters included in all nominated passages from this system for the topic up until that point. Similar to regular MAP, remaining relevant passages that were not retrieved (no overlap with any nominated passages) were added into the calculation as well, with precision set to 0 for these relevant non-retrieved gold standard passages. Then the mean of these average precisions over all topics was calculated to compute the MAP for passages. Note that this measure is essentially the fraction of retrieved characters that are part of an answer to the topic question.</p><p>b. Aspect-level MAP Aspect retrieval was measured using the average precision for the aspects of a topic, averaged across all topics. To compute this, for each submitted run, the ranked passages were transformed to two types of values, either the aspect(s) of the gold standard passage that the submitted passage overlapped with or the value "not relevant". This resulted was a ranked list, for each run and each topic, of lists of aspects per passage, Non-relevant passages had empty lists of aspects. Because we were uncertain of the utility for a user of a repeated aspect (e.g., same aspect occurring again further down the list), we discarded these from the output to be analyzed. For the remaining aspects of a topic, we calculated MAP similar to how it is calculated for documents, with the additional wrinkle that a single passage may have associated with it multiple aspects. Therefore the precision for the retrieval of each aspect was computed as the fraction of relevant passages for the retrieved passages up to the current passage under consideration. These fractions at each point of first aspect retrieval were then averaged together to compute the average aspect precision. Relevant passages that did not contribute any new aspects to the aspects retrieved by higher ranked passages were removed from the ranking. Taking the mean over all topics produced the final aspect-based MAP. For the purposes of this measure, any PMID that had a passage associated with a topic ID in the set of gold standard passages was considered a relevant document for that topic. All other documents were considered not relevant for that topic. System run outputs were collapsed by PMID document identifier, with the documents appearing in the same order as the first time the corresponding PMID appeared in the nominated passages for that topic. For a given system run, average precision was measured at each point of correct (relevant) recall for a topic. The MAP was the mean of the average precisions across topics.</p><p>Two topics, 173 and 180, had no relevant passages. These were not included in the scoring for any of the three measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>Information about each run is listed in Appendix 4, including a brief system description provided by the group. The results from all submissions are provided in Appendix 5. A summary of the medium and maximum run results by run type is shown in Figure <ref type="figure" coords="10,398.26,277.31,4.12,9.57">1</ref>. The best results per topic are seen in Figure <ref type="figure" coords="10,389.12,289.97,4.12,9.57">2</ref>. In general, document MAP scores are highest, followed by aspect, and then passage, although these scores are not directly comparable since they measure precision at recall of different things. There was a general, though far from perfect, correlation between passage, aspect, and document MAP, as shown in Figure <ref type="figure" coords="10,366.74,378.53,4.12,9.57">3</ref>. As seen in many TREC-style evaluations and demonstrated in Figure <ref type="figure" coords="10,500.34,391.13,4.12,9.57">4</ref>, statistical significance, based on pair-wise comparison with the top-ranking score in an ANOVA model, was not achieved for any measure until well down the ranked list of runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Analysis</head><p>Overall there was a wide variation in system performance across submissions for each of the three measures. In general, scores grouped into three sets. A few groups dominated the high scores of each measure, followed by a large group with scores around the mean, and then another large group of relatively low scores. Submissions that scored well on document retrieval tended to score well on both passage and aspect. While a correlation between passage and document retrieval might have been expected, the correlation between document and aspect retrieval is more surprising since aspect retrieval places an emphasis on novelty and document retrieval does not.</p><p>Certainly the task and the three measures provided a significant challenge to the participants. The best scores for document retrieval were moderate, and the highest scores on the passage and aspect measures were moderate and fairly low, respectively. No MAP for any system or measure was much greater than 0.50.</p><p>For all three measures, the best automatic approaches were as good or better than manual or interactive systems. Manual and interactive approaches did not appear to provide an advantage over automatic methods. However, because the definitions of automatic, manual, and interactive were not as solid as in previous years because systems had the topic questions available during development, inference should be limited from these observations.</p><p>Although a comprehensive analysis was not performed, it was clear from the results and techniques of the top-performing groups in passage retrieval that certain approaches were quite effective. In particular, "trimming" passages to shorten them was done in all the runs with the highest passage MAP. Indeed, some groups noted that non-content manipulations of passages had substantial effects on passage MAP, with one group claiming that breaking passages in half with no other changes doubled their (otherwise low) score. To this end, we defined an alternative passage MAP (PASSAGE2) that calculated MAP as if each character in each passage were a ranked document. In essence, the output of passages was concatenated, with each character being from a relevant passage or not. The complete results are shown in Appendix 6, and summarized in Figure <ref type="figure" coords="13,423.38,87.59,4.12,9.57" target="#fig_1">5</ref>, where it can be seen that some re-ranking of runs occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions and Future Directions</head><p>This  We plan to continue the TREC 2007 Genomics Track in the same direction, using the existing document collection and task structure but adding completely new topics and potentially new topic types. The 2007 track will be the last running of the Genomics Track within TREC, although future options to continue biomedical IR challenge evaluations are being explored.</p><p>Appendix 3 -Number, average length, and standard deviation of relevant passages and number of aspects per topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,111.90,693.72,388.26,9.57"><head>Figure 1 -Figure 3 -Figure 4 -</head><label>134</label><figDesc>Figure1-MAP for all runs and those categorized as automatic, manual, and interactive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="13,95.76,659.70,420.46,9.57"><head>Figure 5 -</head><label>5</label><figDesc>Figure 5 -Comparison of runs using original passage MAP and revised measure (PASSAGE2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,74.94,312.46,9.57"><head>Table 1 -</head><label>1</label><figDesc>Generic topic types used in the TREC 2006 Genomics Track.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,74.94,452.75,123.39"><head>Table 3 -</head><label>3</label><figDesc>Agreement of original and duplicate judges for relevant passages, where agreement indicates that any part of a maximum-length legal span was judged as relevant or not.</figDesc><table coords="8,72.00,112.62,452.75,85.71"><row><cell></cell><cell cols="2">Five topics (not including 181)</cell><cell cols="2">Six topics (including 181)</cell></row><row><cell></cell><cell>Duplicate judge</cell><cell>Duplicate judge</cell><cell>Duplicate judge</cell><cell>Duplicate judge</cell></row><row><cell></cell><cell>relevant</cell><cell>not relevant</cell><cell>relevant</cell><cell>not relevant</cell></row><row><cell>Original judge</cell><cell>234</cell><cell>228</cell><cell>253</cell><cell>789</cell></row><row><cell>relevant</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original judge not</cell><cell>53</cell><cell>4485</cell><cell>53</cell><cell>4905</cell></row><row><cell>relevant</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.00,214.08,456.04,123.39"><head>Table 4 -</head><label>4</label><figDesc>Agreement of original and duplicate judges for MeSH terms assigned. (The cell where neither assigned in undefined.)</figDesc><table coords="8,72.00,251.76,452.75,85.71"><row><cell></cell><cell cols="2">Five topics (not including 181)</cell><cell cols="2">Six topics (including 181)</cell></row><row><cell></cell><cell>Duplicate judge</cell><cell>Duplicate judge</cell><cell>Duplicate judge</cell><cell>Duplicate judge</cell></row><row><cell></cell><cell>assigned</cell><cell>did not assign</cell><cell>assigned</cell><cell>did not assign</cell></row><row><cell>Original judge</cell><cell>83</cell><cell>730</cell><cell>90</cell><cell>2407</cell></row><row><cell>assigned</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original judge did</cell><cell>632</cell><cell>N/A</cell><cell>652</cell><cell>N/A</cell></row><row><cell>not assign</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,72.00,74.94,450.66,123.39"><head>Table 5 -</head><label>5</label><figDesc>Range and central tendency of relevant passages, their length, and distinct aspects per topic.</figDesc><table coords="10,72.00,100.02,450.66,98.31"><row><cell>Measure</cell><cell>Relevant passages per</cell><cell>Mean relevant passage</cell><cell>Distinct aspects per</cell></row><row><cell></cell><cell>topic</cell><cell>length</cell><cell>topic</cell></row><row><cell>Minimum</cell><cell>3</cell><cell>27</cell><cell>7</cell></row><row><cell>Mean</cell><cell>35</cell><cell>400</cell><cell>22</cell></row><row><cell>Median</cell><cell>133</cell><cell>229</cell><cell>30</cell></row><row><cell>Maximum</cell><cell>593</cell><cell>6928</cell><cell>96</cell></row><row><cell>c. Document-level MAP</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="18,72.00,112.62,434.95,389.31"><head></head><label></label><figDesc>Passage selection is based on identification of concepts from the UMLS metathesaurus and a gene thesaurus in both the query and the documents. The concepts identified in the query were manually checked and corrected. consists of manually constructed queries generally consisting of a conjunction of topic terms each of which is a disjunction of synonyms. The synonyms were obtained both by introspection and by consulting databases such as Entrez Gene, GeneCards and MeSH. Query development sometimes also involved examination of PubMed and Essie results of preliminary query formulations. The queries were processed by Essie, and the results were automatically trimmed of text unrelated to the topics. nlm.aronson NLMmanual manual This is similar to the automatic Essie method which is part of our automatic fusion run but with some manually modified queries and with results automatically trimmed of text unrelated to the topics. was performed with queries at the full article level only. Slider position 200. In this run, we used the MG4J Vigna scorer as baseline. The Vigna scorer favors matches where search terms appear in short text intervals. All runs are performed with the Twease slider at position 200. At this position, the slider expands the query with all the morphological word variants, abbreviations, and MeSH synonyms that match the query words. Morphological word variants are discovered at runtime, with a statistical model trained on Medline 2006 (Campagne, F. unpublished, 2006). Passages are assigned as the minimal intervals where the query match the documents. weill-med-cornellu icb2 interactive Run 2 was performed with parts of the queries at the sentence-level, when appropriate, other terms matching the rest of the article, and ranking by context. Slider at position 200. Context ranking is a new ranking strategy implemented in our textractor framework for the 2006 TREC genomics track. Context queries are expressed as (query)/(context). Briefly, context ranking allows to rank documents matching query by a context, specified as a query expression (e.g., "colon cancer" as a phrase or keywords with boolean clauses). The words in the context do not necessarily occur in the document being ranked. The documents matching the context part of the query are used to infer words that are associated with the context in the corpus. These words are then used to rank the specific set of documents. All runs are performed with the Twease slider at position 200. At this position, the slider expands the query with all the morphological word variants, abbreviations, and MeSH synonyms that match the query words. Morphological word variants are discovered at runtime, with a statistical model trained on Medline 2006 (Campagne, F. unpublished, 2006). Passages are assigned as the minimal intervals where the query match the documents. weill-med-cornellu icb3 interactive Run 3 was performed with queries at the full article level, ranked by context as in Run 2. The context of queries in Run 2 were added to queries from Run 1 to form queries for this run. Slider at position 200. For each topic, queries have the form (query run 1) / (context run 2). All runs are performed with the Twease slider at position 200. At this position, the slider expands the query with all the morphological word variants, abbreviations, and MeSH synonyms that match the query words. Morphological word variants are discovered at runtime, with a statistical model trained on Medline 2006 (Campagne, F. unpublished, 2006). Passages are assigned as the minimal intervals where the query match the documents. yorku.huang york06ga1 automatic 1. Use Okapi BM25 for concept-based structured query 2. Use the blind feedback with term selection technique 3. Use a dual index model for passage retrieval 4. No aspect-level retrieval yorku.huang york06ga3 automatic Split the top 500 retrieved passages into 5 groups with 100 passages in each group and then use the EM clustering algorithm to re-rank the 100 passages in each group for aspect-level retrieval yorku.huang york06ga4 automaticThis run is for document-level retrieval. That is documents will appear in the front of list for only once and those retrieved by different passage previously will be put at the end of list. No aspect-level retrieval.Appendix 6 -Comparison of results and ranks of original (PASSAGE) and modified (PASSAGE2) passage MAP.</figDesc><table coords="18,72.00,112.62,434.95,389.31"><row><cell>uamsterdam.meij</cell><cell cols="2">UAmsBaseLine automatic</cell><cell>Baseline. Just some naive index-specific acronym axpansion on identified (and based indexing IR system 2 Divergence from randomness, 5-gram indexing</cell></row><row><cell>utokyo.ishii</cell><cell>Tlab6rGT1</cell><cell>automatic</cell><cell>extracted) NP's Automatically calculating abstract level of biomedical concepts and</cell></row><row><cell>uamsterdam.meij</cell><cell>UAmsExp</cell><cell>automatic</cell><cell>Massive query expansion, using online resources and iteratively gathered disambiguation of them.</cell></row><row><cell cols="4">Number of Relevant Passages 527 68 18 262 7 17 34 208 243 103 36 50 593 0 36 33 14 9 7 13 0 589 144 19 5 25 388 3 3451 Group 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 Total Run arizona-stateu.gonzalez asubaral arizona-stateu.gonzalez asubaral2 arizona-stateu.gonzalez asubaral3 concordiau.bergler BioKI1 concordiau.bergler BioKI2 concordiau.bergler BioKI3 dalianu.yang DUTgen1 dalianu.yang DUTgen2 dalianu.yang DUTgen3 erasmus.schuemie EMCUT1 erasmus.schuemie EMCUT2 fudanu.niu fdugen1 fudanu.niu fdugen2 fudanu.niu fdugen3 iit.urbain iitx1 iit.urbain iitx2 iit.urbain iitx3 inst-infocomm-res.yu i2rg061 inst-infocomm-res.yu i2rg062 inst-infocomm-res.yu i2rg063 kyotou.wan kyoto1 kyotou.wan kyoto2 kyotou.wan kyoto20 nlm.aronson NLMfusion nlm.aronson NLMinter NTUadh1 ntu.chen NTUadh2 ntu.chen NTUadh3 ohsu.hersh OHSUBigclu Mean Passage Length 307 390 350 289 405 251 485 605 251 1012 234 306 171 0 461 416 412 366 410 360 0 775 293 188 318 209 286 1107 Type automatic automatic automatic interactive Weighted keyphrases interactively optimized over 2005 data for each query. Standard Deviation of Passage Length Number of Distinct MeSH Aspects 234 32 449 94 334 20 171 35 210 14 125 11 553 19 612 35 186 35 1077 32 168 23 134 13 232 78 0 0 285 12 554 27 281 9 240 12 155 21 283 7 0 0 691 96 239 35 116 11 103 10 183 55 291 32 954 13 Brief Description First complete run after question variants in. Using subject-verb-object as part of ranking together with keyword frequency, distance between keywords. Similar to first run, but less restrictive in filtering. Only require the subject to be in the passage. Output limited to sentence boundaries. interactive Weighted keyphrases interactively optimized over 2005 data for each query. Output limited to paragraph boundaries. interactive Weighted keyphrases (weight fixed at 25) interactively optimized over 2005 data for all queries. Output limited to paragraph boundaries. interactive Rocchio feedback based on 2005's gold standard, Two levels of indexes, BM25, Paragraph-first reranking interactive Rocchio feedback based on 2005's gold standard, Two levels of indexes, BM25, Combining reranking interactive Rocchio feedback and SVM based on 2005's gold standard, Two levels of indexes, BM25, Paragraph-first reranking automatic Document retrieval is performed using a language-modelling approach. Passage selection is based on identification of concepts from the UMLS metathesaurus and a gene thesaurus in both the query and the documents. manual Document retrieval is performed using a language-modelling approach. manual passage retrieval, svm classification. manual passage retireval , svm classification, less positive files manual sentence retrieval, pattern matching. automatic sentMatchRatioNormSC + passMatchRatioNormSC automatic sentmatchrationormsc+sentnormsc+passmatchrationormsc+passnormsc)/4 automatic (1*sentmatchrationormsc+0.1*passmatchrationormsc+0.01*sentnormsc+0.001 *passnormsc) automatic document retrieval automatic document reranking automatic Passage Retrieval automatic Paragraph-level IR with impact-based retrieval and a probabilistic model for term co-occurrence with their scores merged. Queries expanded automatically with synonyms. automatic a combination of IR impact-based retrieval at document level with a probabilistic model of term coocurance at paragraph level; for the first phase, queries are automatically expanded using synonyms. automatic a combination of IR impact-based retrieval at document level with a probabilistic model of term coocurance at paragraph level; for the first phase, original queries are employed. automatic This run is the equally-weighted fusion of the results of four automatic methods (1) Essie, a search engine developed specifically for biomedical text supporting flexible query expansion; (2) NCBI, a method that performs selective query expansion based on theme analysis; (3) UniGe, a method based on the EasyIR search engine using term and document weightings as well as pivoted normalization; and (4) Smart, a method based on the Smart search engine. Automatic query expansion based on MetaMap and Theme was available to each of the basic methods. Each method produced paragraphs which were then merged into a final list. automatic The underlying retrieval model is KL-divergence. Synonyms for query expansion are selected by checking that the synonyms co-occur with the original query terms in Pubmed's Medline abstracts. automatic A baseline run using KL-divergence retrieval model. manual Same as NTUadh1, except that Nur-77 is manually added to queries containing Nurr-77. automatic Same as cluster run. Reranking by clustering of similar returns. Parameters for clustering were modified so that cluster were looser. ohsu.hersh OHSUCluster automatic Same as noclu. The returned passages were further processed by clustering with CLUTO. Features for clustering are text words from the passage with stopwords filtered out and stemming. ohsu.hersh OHSUNoclu automatic Automatically generated queries with concept expansion. Documents indexed at legal span granularity with Lucene. Retrieved passages scored by tfidf. purdueu.si PCPsgAspect automatic Combine multiple types of resources for constructing queries; Hierarchical language model smoothing; Post result filter; Aspect retrieval based on vector representation of MMR purdueu.si PCPsgClean automatic Combine multiple types of resources for constructing queries; Hierarchical language model smoothing; Post result filter purdueu.si PCPsgRescore automatic Combine multiple types of resources for constructing queries; Hierarchical language model smoothing; Post result filter; Combine multiple types of evidence queenslandu.geva Baseline1M automatic Baseline run, Identify paragraphs queenslandu.geva Z1KL5KX automatic Legal span queenslandu.geva Z1KL5KY automatic Max 5K span queenslandu.geva zoom0p5K1M automatic Identify complete paragraphs queenslandu.geva zoom1K1M automatic zoom on passage ( 500 chars either size ) suny-buffalo.ruiz UBexp1 automatic This run uses a pre-retrieval query expansion method that adds gene names and synonyms. Retrieval is performed using SMART Lnu.ltu and returning full paragraphs. suny-buffalo.ruiz UBexp1M automatic The run has been generated with SMART using pivoted normalization. suny-buffalo.ruiz UBexp2 automatic This run uses automatic pre-retrieval query that adds gene names and synonyms. Retrieval is performed using SMART with atn ann weighting scheme. Retrieval step returns full paragraphs. suny-buffalo.ruiz UBexp2M automatic The run has been generated with SMART using pivoted normalization (2nd run from Miguel Ruiz). technion.gabrilovich LARAg06pe0 automatic In the preprocessing phase, documents are indexed with BOW and with an additional set of knowledge-rich features based on Wikipedia concepts. First, a simple BOW query is generated from the topic (no expansion or other enhancements). Then, the top 10 returned documents are mapped into most relevant Wikipedia concepts. The resulting concepts are used to query the second index of documents. No explicit domain-specific knowledge is used. Due to lack of time, retrieval is of entire paragraphs, not passages. technion.gabrilovich LARAg06pe5 automatic Note this run is identical to LARAg06pe0 except the use of query expansion. In the preprocessing phase, documents are indexed with BOW and with an additional set of knowledge-rich features based on Wikipedia concepts. First, a simple BOW query is generated from the topic, with blind feedback query expansion. Then, the top 10 returned documents are mapped into most relevant Wikipedia concepts. The resulting concepts are used to query the second index of documents. No explicit domain-specific knowledge is used. Due to lack of time, retrieval is of entire paragraphs, not passages. technion.gabrilovich LARAg06t automatic Document and query are represented using features generated by an auxiliary acronyms uamsterdam.meij UAmsExpSel automatic Automatically identified obligatory terms (and expansions) ucal-berkeley.larso biotext1 automatic Basic run. Returns complete legal spans. Ranking based on Lucene score. ucal-berkeley.larso biotext3 automatic Ranked ucal-berkeley.larso biotextweb automatic Reranking of the first submission run, using n-grams from the Web. ucolorado.cohen uchsc1 interactive Expanded queries are sent to the search engine Lemur. Results undergo zone filtering, and top remaining Lemur results are sent to a singular value decomposition algorithm to expand the results pool by selecting similar paragraphs based on a latent semantic Dirichlet similarity score. Results of the SVD are filtered using Naive Bayes with lexical and conceptual features with training data dervied from manual evaluation of Lemer output. ucolorado.cohen uchsc2 interactive Expanded queries are sent to the search engine Lemur. Results undergo zone filtering. A second, less strict, set of queries is sent to the Lemur search engine and results are filtered using zone filtering and Naive Bayes with lexical and conceptual features with training data dervied from manual evaluation of Lemer output. ucolorado.cohen uchsc3 manual Expanded queries are sent to the search engine Lemur. Results undergo zone filtering. uguelph.song UofG0 automatic Retrieval based on the language modeling approach. uguelph.song UofG1 automatic Retrieval based on the language modeling approach. The results are further filtered based on document coverage. uguelph.song UofG2 automatic Retrieval based on language modeling approach. The results are further filtered based on document and aspect coverage. uhosp-geneva.ruch UniGe automatic Use the easyIR engine a vector-space with tf.idf weightings and a modified version of pivoted normalization. Basic run. uhosp-geneva.ruch UnigeGO automatic Use the easyIR engine a vector-space with tf.idf weightings and a modified version of pivoted normalization. GO specific reranking. uhosp-geneva.ruch UnigeMesh automatic Use the easyIR engine a vector-space with tf.idf weightings and a modified version of pivoted normalization. Template-specific semantic filtering and expansion. uillinois.chicago.zhou UICGenRun1 automatic two-dimensional ranking uillinois.chicago.zhou UICGenRun2 automatic two-dimensional ranking query expansion uillinois.chicago.zhou UICGenRun3 automatic 2-dimensional ranking; query expansion; passage retrieval uiowa.eichmann UIowa06Geno1 automatic NLP processing of question, entire paragraph returned as result uiowa.eichmann UIowa06Geno2 automatic NLP processing of question, paragraphs contracted to only those sentences mentioning query terms. uiowa.eichmann UIowa06Geno3 automatic NLP processing of question, entire paragraphs returned, but only those at least 300 characters long (as an ad hoc citation exclusion mechanism). uiuc.zhai UIUCauto automatic Automatic run. uiuc.zhai UIUCinter interactive Interactive run. uiuc.zhai UIUCinter2 interactive Interactive run 2. umass.allan UMassCIIR1 interactive Query-biased pseudo relevance feedback. 250 word passages with overlap removed. umass.allan UMassCIIR1L interactive Query-biased pseudo relevance feedback. The UMassCIIR1 run was "legalized" to only be spans from the legalspans file. Legal spans less than 750 chars were excluded. umass.allan UMassCIIR2 interactive Query-biased pseudo relevance feedback. 500 word passages with overlap removed. uneuchatel.savoy UniNE1 automatic Data fusion of two IR systems (based on normalized RSV values using Z-score) IR system 1 Divergence from randomness, word-based indexing, spelling correction &amp; word variant generation IR system 2 Divergence from randomness, 5-gram indexing uneuchatel.savoy UniNE2 automatic Data fusion of two IR systems (based on normalized RSV values (max)) IR utokyo.ishii Tlab6rGT2 automatic Automatically calculating abstract level of biomedical concepts and disambiguation of them. Another condition. utokyo.ishii Tlab6rGT3 automatic Automatically calculating abstract level of biomedical concepts and disambiguation of them. Yet another condition. uwisconsin.madison WiscRun1 automatic Performs POS chunking on topic questions to identify significant noun phrases -Automatically generates expansion term lists for each NP using the MeSH database -Uses Lemur/Indri toolkit to execute queries that require one item in each term list to be found in a paragraph -Ranks results using likelihood of paragraphs given all the expansion term lists concatenated together -Adjusts passage boundaries to include only sentences between the first and last occurrence of key terms uwisconsin. madison WiscRun2 automatic Begins with the same baseline results as our WiscRun1 run -Re-Ranks these results by performing hierarchical clustering on passage bag-of-words vectors -Interleaves results from clusters to promote aspect diversity (Note that clusters are repeatedly considered in order of their average initial rank) uwisconsin. madison WiscRun3 automatic same baseline results as our WiscRun1 run -Re-Ranks using GRASSHOPPER, a graph theoretical algorithm that Performs random walk with absorbing states on the results, to Automatically balance the representativeness and diversity of the final rank weill-med-cornellu icb1 PASSAGE MAP PASSAGE2 MAP PASSAGE MAP PASSAGE2 MAP interactive Run 1 Run Rank Rank THU2 0.148593 0.085316 1 2 UICGenRun3 0.147916 0.084342 2 3 THU1 0.144239 0.082738 3 5 THU3 0.141929 0.083562 4 4 UICGenRun2 0.124390 0.074536 5 7 PCPsgRescore 0.108766 0.063310 6 12 PCPsgAspect 0.106500 0.064048 7 11 PCPsgClean 0.099922 0.061270 8 13 0.082714 0.101262 9 1 UICGenRun1 0.075050 0.043047 10 24 DUTgen2 0.073024 0.064767 11 8 DUTgen1 0.070666 0.061039 12 14 UIUCinter2 0.060380 0.053200 13 16 UIUCinter 0.059062 0.053124 14 17 uchsc2 0.055976 0.064229 15 10 iitx1 0.054941 0.044172 16 23 uchsc1 0.054570 0.064268 17 9 uchsc3 0.054223 0.082599 18 6 icb1 0.051705 0.027911 19 52 iitx3 0.051309 0.042971 20 25 UofG0 0.049608 0.037067 21 35 UIUCauto 0.048644 0.049393 22 20 UAmsExpSel 0.048445 0.060108 23 15 i2rg061 0.047251 0.018594 24 70 NLMmanual 0.047048 0.037467 25 30 NLMfusion 0.046584 0.040631 26 26 NTUadh1 0.046493 0.049792 27 19 NTUadh3 0.046379 0.049894 28 18 DUTgen3 0.044680 0.045511 29 22 i2rg063 0.044458 0.018773 30 68 i2rg062 0.044096 0.017759 31 73 NTUadh2 0.042941 0.046341 32 21 BioKI1 0.041915 0.036084 33 37 OHSUNoclu 0.041866 0.029858 34 49 UniNE3 0.040747 0.034017 35 40 UBexp2 0.040306 0.037583 36 28 UBexp2M 0.040306 0.037583 37 29 UniNE1 0.038983 0.033616 38 41 UniNE2 0.038359 0.032431 39 44 OHSUBigclu 0.037946 0.030458 40 48 iitx2 0.036266 0.039009 41 27 interactive This run ntu.chen classifier that was built using world knowledge extracted from Wikipedia. No other domain-specific or general information is used. Due to lack of time, retrieval is of entire paragraphs, not passages. tsinghuau.zhang THU1 automatic Our best result. system 1 Divergence from randomness, word-based indexing, spelling icb2 0.034804 0.016423 42 78 correction &amp; word variant generation the document title is included to all biotext1 0.034778 0.024210 43 61 passages generated from the article IR system 2 Divergence from randomness, 5-gram indexing UBexp1 0.034650 0.037421 44 31</cell></row><row><cell>tsinghuau.zhang uneuchatel.savoy UBexp1M</cell><cell>THU2 UniNE3 0.034650</cell><cell cols="2">automatic automatic 0.037421 Shorter Passages to return. Data fusion of two IR systems (based on normalized RSV values (Z-score), 45 32</cell></row><row><cell>tsinghuau.zhang BioKI2</cell><cell>THU3 0.034603</cell><cell cols="2">automatic 0.032756 Longer Passages to return. baserun for comparisons) IR system 1 Divergence from randomness, word-46</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The <rs type="institution">TREC Genomics Track</rs> is funded by grant <rs type="grantNumber">ITR-0325160</rs> from the <rs type="funder">U.S. National Science Foundation</rs>. The track also thanks <rs type="person">Ellen Voorhees</rs>, <rs type="person">Ian Soboroff</rs>, and <rs type="funder">Lori Buckland of NIST</rs> for their help in various ways. We also thank <rs type="person">John Sack</rs> and <rs type="person">Highwire Press</rs> (www.highwire.org) for facilitating the use of documents from the respective publishers.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4MMTtmV">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,72.00,353.21,189.14,9.57;14,108.00,365.87,162.85,9.57;14,108.00,378.24,151.26,9.88;14,108.00,390.84,159.23,9.88;14,108.00,403.79,174.80,9.57;14,108.00,416.45,151.25,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,146.57,353.21,114.57,9.57;14,108.00,365.87,162.85,9.57;14,108.00,378.53,69.52,9.57">HARD Track overview in TREC 2003 -high accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,185.34,378.24,73.92,9.88;14,108.00,390.84,154.28,9.88">The Twelfth Text REtrieval Conference -TREC 2003</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="24" to="37" />
		</imprint>
		<respStmt>
			<orgName>Naitonal Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,429.11,189.14,9.57;14,108.00,441.77,162.85,9.57;14,108.00,454.08,164.04,9.88;14,108.00,466.74,158.26,9.88;14,108.00,479.69,174.80,9.57;14,108.00,492.35,120.06,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,146.57,429.11,114.57,9.57;14,108.00,441.77,162.85,9.57;14,108.00,454.37,69.52,9.57">HARD Track overview in TREC 2004 -high accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,185.34,454.08,86.70,9.88;14,108.00,466.74,153.54,9.88">The Thirteenth Text Retrieval Conference (TREC 2004)</title>
		<meeting><address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>MD. National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,505.01,200.75,9.57;14,108.00,517.67,175.41,9.57;14,108.00,529.98,165.04,9.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,220.76,505.01,51.99,9.57;14,108.00,517.67,170.57,9.57">A survey of current work in biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,108.00,529.98,117.97,9.88">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,542.93,186.04,9.57;14,108.00,555.59,134.65,9.57;14,108.00,568.02,123.52,9.88;14,108.00,580.62,119.20,9.88" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<title level="m" coord="14,154.52,542.93,103.52,9.57;14,108.00,555.59,134.65,9.57;14,108.00,568.02,123.52,9.88;14,108.00,580.62,54.70,9.88">Interactivity at the Text Retrieval Conference (TREC). Information Processing and Management</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="365" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,593.51,213.01,9.57;14,108.00,606.17,168.45,9.57;14,108.00,618.54,148.50,9.88;14,108.00,631.19,121.61,9.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,154.59,593.51,130.42,9.57;14,108.00,606.17,168.45,9.57;14,108.00,618.83,90.10,9.57">Evaluation of biomedical text mining systems: lessons learned from information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,204.78,618.54,51.72,9.88;14,108.00,631.19,63.54,9.88">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="344" to="356" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,644.15,179.69,9.57;14,108.00,656.81,169.20,9.57;14,108.00,669.11,179.62,9.88;14,108.00,681.83,116.70,9.88;14,108.00,694.43,87.39,9.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,108.00,656.81,169.20,9.57;14,108.00,669.41,126.62,9.57">Enhancing access to the bibliome: the TREC 2004 Genomics Track</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bhupatiraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,242.10,669.11,45.52,9.88;14,108.00,681.83,116.70,9.88;14,108.00,694.43,60.45,9.88">Journal of Biomedical Discovery and Collaboration</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,324.00,74.93,212.69,9.57;14,360.00,87.29,138.64,9.88;14,360.00,100.01,173.46,9.88;14,360.00,112.55,140.52,9.88;14,360.00,125.51,149.63,9.57;14,360.00,138.17,55.32,9.57" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m" coord="14,483.89,74.93,52.81,9.57;14,360.00,87.29,138.64,9.88;14,360.00,100.01,173.46,9.88;14,360.00,112.55,49.51,9.88">TREC 2005 Genomics Track overview. The Fourteenth Text Retrieval Conference -TREC 2005</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards &amp; Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,324.00,150.83,206.25,9.57;14,360.00,163.49,175.92,9.57;14,360.00,175.85,153.60,9.88;14,360.00,188.45,139.59,9.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,492.42,150.83,37.83,9.57;14,360.00,163.49,175.92,9.57;14,360.00,176.15,113.36,9.57">BLAST: at the core of a powerful and diverse set of sequence analysis tools</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mcginnis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,480.06,175.85,33.54,9.88;14,360.00,188.45,66.00,9.88">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="20" to="W25" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,324.00,201.41,214.08,9.57;14,360.00,213.77,172.90,9.88;14,360.00,226.73,39.46,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,410.15,201.41,127.94,9.57;14,360.00,214.07,31.86,9.57">Mining literature for systems biology</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,399.18,213.77,117.88,9.88">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="399" to="406" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
