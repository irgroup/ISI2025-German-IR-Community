<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.60,63.46,314.20,27.01;1,77.88,83.38,453.51,27.01;1,217.32,117.55,174.70,17.20">Index Pruning and Result Reranking: Effects on Ad-Hoc Retrieval and Named Page Finding (Wumpus at TREC 2006)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.76,157.42,81.93,17.71"><forename type="first">Stefan</forename><forename type="middle">B</forename><surname>Üttcher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.49,157.42,106.84,17.71"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.01,157.42,93.45,17.71"><forename type="first">Peter</forename><forename type="middle">C K</forename><surname>Yeung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.60,63.46,314.20,27.01;1,77.88,83.38,453.51,27.01;1,217.32,117.55,174.70,17.20">Index Pruning and Result Reranking: Effects on Ad-Hoc Retrieval and Named Page Finding (Wumpus at TREC 2006)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E15B7AC77AAF8968A64BD21336556528</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe experiments conducted for the TREC 2006 Terabyte track. Our experiments are centered around two concepts: Static index pruning (for increased retrieval efficiency) and result reranking (for improved precision).</p><p>We investigate their effect on retrieval efficiency and effectiveness, paying special attention to the difference between ad-hoc retrieval and named page finding. We show that index pruning and reranking based on relevance models can be beneficial in an ad-hoc retrieval setting, but have a disastrous repercussion on the effectiveness of named page finding. Result reranking based on anchor text, on the other hand, is very useful for named page finding, but should not be used for ad-hoc retrieval.</p><p>This dichotomy poses a problem for search engines, as there is no easy way for a search engine to decide whether a given query represents an ad-hoc retrieval task, with the purpose to satisfy an abstract information need, or a named page finding task, targeting a specific document.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the general context of text-based document retrieval, we can distinguish between at least three different types of search tasks:</p><p>• In an ad-hoc retrieval task, the user is interested in all documents that satisfy an abstract information need, e.g., that contain the answer to a question the user might have. All documents satisfying this information need are considered relevant, while documents that do not satisfy it are considered irrelevant.</p><p>• In a topic distillation task, the user is looking for a page that is representative of a certain topic, possibly a page that contains links to other pages important in the chosen context, and that gives her a good overview of what information is available on this topic. A topic distillation task can be thought of as a second-order ad-hoc retrieval task, because the user is looking for documents that give her easy access (for instance by providing hyperlinks) to documents that would be considered relevant in an ad-hoc retrieval setting.</p><p>TREC 2006, November 14-17, 2006, Gaithersburg, Maryland, USA. This paper is available on-line at: http://stefan.buettcher.org/papers/buettcher trec2006.pdf</p><p>• In a named page finding task, the user is interested in one specific document, or page. Only this particular page (plus possible duplicates) is relevant to the user's need, while all other documents are irrelevant.</p><p>Often the simulated scenario is the situation where a user visited a web page some time ago and is trying to find back to it several days or weeks later, only remembering a few keywords. The home page finding task, a web search task in which the user is interested in a particular web home page, can be considered a special case of named page finding.</p><p>These three task types are different in nature and thus require different techniques if the search engine is to return high-quality search results to the user. This was first noticed when researchers -unsuccessfully -tried to apply link-based retrieval methods, like PageRank <ref type="bibr" coords="1,457.94,397.44,13.33,8.97" target="#b12">[12]</ref>, to ad-hoc retrieval tasks <ref type="bibr" coords="1,340.18,407.88,14.15,8.97" target="#b10">[10]</ref>  <ref type="bibr" coords="1,357.57,407.88,9.59,8.97">[9]</ref> [2] <ref type="bibr" coords="1,383.11,407.88,9.09,8.97" target="#b8">[8]</ref>.</p><p>In order to apply the appropriate ranking functions to the different tasks, a search engine would need to be able to decide whether a given search query, submitted by the user, represents an ad-hoc retrieval, a topic distillation, or a named page finding task. Unfortunately, inferring the type of the search task, given a keyword query, is not always easy. For example, consider the following three queries:</p><p>• A. blue grass music festival history • B. arizona retirement system history • C. kalamazoo public library history Query A (TREC 796) represents an ad-hoc topic ("Describe the history of bluegrass music and give location of bluegrass festivals."), while queries B (NP849) and C (NP1060) represent named page finding tasks, targeting specific documents. By just looking at each query, it is rather difficult to determine its type -even for a human, and much more so for a computer.</p><p>In this paper, we do not try to solve the problem of inferring the type of a search task from the given query. Instead, we look at different techniques that can be applied to the retrieval method used for any document retrieval task and analyze their effects on the different types of tasks. In particular, we study a static index pruning method and two different result reranking techniques and look at how they affect the quality of the search results in ad-hoc retrieval and in named page finding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EVALUATION METHODOLOGY</head><p>To be able to study the effect of different retrieval methods on ad-hoc and named page finding tasks at the same time, it is convenient to have a single evaluation measure that can be applied to the search results for both task types. Unfortunately, the quality measures that are usually employed do not meet this criterion:</p><p>• In named page finding tasks, retrieval effectiveness is usually measured by the mean reciprocal rank (MRR) of the relevant documents in the rankings produced by the search engine. This evaluation measure gives high weight to documents ranking near the top of the search results.</p><p>• In ad-hoc retrieval tasks, on the other hand, the most prominent measure is mean average precision (MAP), which puts more emphasis on recall than does MRR.</p><p>While MRR and MAP are compatible in the sense that using MAP to evaluate the retrieval effectiveness in a named page finding task, with only a single relevant document, leads to the same result as MRR, doing so would be misleading. Especially when the effectiveness of result reranking techniques that mainly affect the order of the top search results is to be evaluated, changes in MAP will be dramatically different between ad-hoc retrieval and named page finding, simply because MAP does not give enough weight to top-ranking documents if the number of relevant documents for the given topic is large. MRR, on the other hand, gives too much weight to topranking documents, reducing the score of a ranking from 1 to 0.5 if the named page is moved from position 1 to position 2 in the ranking produced by the search engine. Of course, this penalty is not reflected by the usefulness of the ranking to the user. In most cases, the quality difference between two such rankings will be negligible.</p><p>In our choice of the quality measure to be applied to both ad-hoc retrieval and named page finding, we are guided by view that what really matters to a user is the content of the first page of search results. For an ad-hoc retrieval task, the quality of the first page depends on the number of relevant documents it contains; thus, we choose precision at 10 documents (P@10) as our primary measure. For a named page finding task, the quality of the first page depends on whether it contains the named page or not, and we choose success at 10 documents (S@10) as our primary measure. We combine these two measures into one single measure Goodness@10 (G@10), our primary effectiveness measure: G@10 = ( S@10 for named page finding P@10 for ad-hoc retrieval</p><p>When evaluating the two task types independently, we also look at generalizations of this measure (G@k), as well as more traditional measures, such as MAP, bpref, and MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RETRIEVAL BASELINE</head><p>Our retrieval baseline is based on a standard documentordered frequency index, without any positional information. Documents are ranked by Okapi BM25 <ref type="bibr" coords="2,249.94,689.63,13.33,8.97" target="#b15">[15]</ref>, using Porter's algorithm <ref type="bibr" coords="2,133.01,700.07,14.15,8.97" target="#b13">[13]</ref> for term stemming. The retrieval effectiveness of this method, for both ad-hoc retrieval and What this means is that on average a document that is relevant to one of the ad-hoc topics tends to be substantially larger than a typical named page. In fact, when analyzing the qrels file for the topics from 2005, it turns out that for the ad-hoc retrieval topics a relevant document contains 6537 tokens on average; for named page finding a relevant document only contains 2350 tokens on average. It is not clear whether this finding has any deeper meaning or whether it is just an artifact of the topic creation process.</p><p>Since the optimal values of the BM25 parameter b are so different between the two task types, and since it is not clear which task the retrieval function should be optimized for, we use the default value b = 0.75 throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Structure</head><p>Robertson et al. <ref type="bibr" coords="2,392.83,496.44,14.15,8.97" target="#b14">[14]</ref> proposed a method to integrate document structure by computing within-document term frequency values according to predefined weights of different fields within each document (e.g., title, anchor text). This is different from earlier methods, where information from different fields was usually fused by computing a linear combination of the individual scores.</p><p>We adjust term frequency values according to the following rules:</p><p>• &lt;title&gt;: +3</p><p>• &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, &lt;b&gt;, &lt;strong&gt;: +2</p><p>• &lt;i&gt;, &lt;em&gt;, &lt;u&gt;, &lt;dochdr&gt;: +1 That is, every time a query term appears in the document title, it is counted as 4 occurrences (1+3). When it appears italicized (&lt;i&gt;) and underlined (&lt;u&gt;), it is counted as 3 occurrences (1+1+1).</p><p>In contrast to Robertson et al. <ref type="bibr" coords="2,452.29,689.64,13.33,8.97" target="#b14">[14]</ref>, we do not adjust the value of BM25's k1 parameter to take the now increased average TF values into account. This is because our retrieval framework did not give us easy access to the necessary information.</p><p>The retrieval effectiveness of this structure-aware version of BM25 on the given topic sets (ad-hoc and named page finding) is shown in Table <ref type="table" coords="3,161.93,239.76,3.54,8.97">2</ref>. By comparing the column for G@10 in Table <ref type="table" coords="3,115.88,250.20,4.55,8.97">2</ref> with the numbers shown in Table <ref type="table" coords="3,259.91,250.20,3.54,8.97">1</ref>, it can be seen that that retrieval effectiveness for the ad-hoc retrieval tasks is almost unaffected by giving additional weight to terms appearing in special fields of an HTML document; the slight decrease is probably due to our not adjusting the value of k1 when increasing the effective TF values. Effectiveness for named page finding, however, as measured by G@10, improves significantly, from 0.5159 to 0.5794 for topics NP601-872. We therefore decided to use this variant of BM25 as the baseline for all experiments discussed in the remainder of this paper.</p><p>It should be noted that increasing TF values based on document structure gives unfair advantage to HTML documents over unstructured text, like plain text and PDF. This might be one reason why it performs so well in the named page finding tasks, because named pages tend to be HTML documents (again, this might be an artifact of the topic creation process).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details and Efficiency Baseline</head><p>We implemented the method described above in the Wumpus 1 information retrieval system. Document scores are computed from posting lists stored in a frequency index, containing for each term a list of postings of the form:</p><p>(document ID, term frequency)</p><p>The index does not contain any positional information, as our baseline retrieval method does not make any use of proximity information anyway. Postings are encoded as integers, with the 5 least-significant bits representing the term frequency and the remaining bits representing the document ID. Term frequency values are encoded in the following way:</p><formula xml:id="formula_1" coords="3,67.08,594.80,225.82,37.57">enc(tf) = 8 &gt; &lt; &gt; : tf : tf ≤ 15 16 + ¨log 1.15 ( tf 16 ) ˇ: 15 &lt; tf &lt; 1218 31 : tf ≥ 1218 (2)</formula><p>For the majority of all postings, this method leads to an exact encoding of their term frequency values, while for most other postings (those with tf values greater than 15), a small error -less than 8% -will be introduced. While this small inaccuracy for terms with high withdocument frequency results in a minor decrease of precision 1 http://www.wumpus-search.org/ (MAP, for instance, decreased by about 0.005 in our experiments), it has the advantage that queries can be processed more efficiently. By using a similar bucketing technique for document length values, the restriction to 31 possible tf values allows us to construct a table with precomputed score impacts and to obtain the score impact of a given (doclen, tf) pair through a simple table lookup -instead of having to perform the full BM25 score computation for every posting.</p><p>The encoded postings are compressed using a byte-aligned index compression technique <ref type="bibr" coords="3,438.64,151.20,13.33,8.97" target="#b16">[16]</ref>. This procedure results in an inverted file with a total size of 13.6 GB for GOV2. Queries are processed sequentially at a speed of 290 ms per query on average (returning the document IDs of the top 20 documents for each of the 100,000 efficiency queries).</p><p>This number, as well as all other performance figures reported in this paper, was obtained under Linux 2.6.13.3, running a 64-bit version of Wumpus on a single-core Athlon64 3500+ (2.2 GHz) with 2 GB of RAM and a 7,200-rpm SATA hard drive, accessing the index through an ext3 file system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">STATIC INDEX PRUNING</head><p>The notion of static index pruning was officially introduced by Carmel et al. <ref type="bibr" coords="3,414.31,293.04,9.09,8.97" target="#b6">[6]</ref>. In their paper, they propose a method to limit the postings in each inverted list to those that have the greatest impact on a document's score when encountered during query processing. By applying their technique to an inverted file, it is possible to dramatically decrease its size, which will then lead to faster query processing at the cost of decrease retrieval effectiveness. By pruning more or less aggressively, the right point in this efficiency vs. effectiveness trade-off can be chosen.</p><p>Carmel's pruning technique is static because it is applied during index construction, without any knowledge about the queries that are to be processed. It is term-centric because each posting list is pruned independently of all the other posting lists in the inverted file.</p><p>For last year's TREC Terabyte efficiency task, we conducted experiments with a variation of Carmel's method, where a pruned index for the n most frequent terms is held in memory, while an unpruned index for all remaining terms is kept on disk <ref type="bibr" coords="3,368.00,481.31,9.09,8.97" target="#b3">[3]</ref>. By increasing the value of n (we performed experiments for 500 ≤ n ≤ 20000), the number of queries for which the unpruned on-disk needs to be accessed can be decreased, which then results in lower query processing latency -again, at the cost of lower-quality search results.</p><p>More recently, we have conducted experiments with document-centric static index pruning <ref type="bibr" coords="3,479.03,544.07,9.09,8.97" target="#b4">[4]</ref>. In documentcentric pruning, the index construction process does not select the top postings from each term (as in <ref type="bibr" coords="3,507.59,565.07,9.59,8.97" target="#b6">[6]</ref> and <ref type="bibr" coords="3,539.85,565.07,9.43,8.97" target="#b3">[3]</ref>), but the top postings from each document.</p><p>This type of pruning was first motivated by experiments with pseudo-relevance feedback, trying to reproduce the results obtained by Carpineto et al. <ref type="bibr" coords="3,451.94,606.83,9.09,8.97" target="#b7">[7]</ref>. Carpineto's method is based on the Kullback-Leibler divergence between the unigram language model defined by an individual document and the language model of the entire text collection. It uses each term's contribution to the document's KL divergence to assign feedback scores to potential expansion terms.</p><p>Given two discrete probability distributions (here: unigram language models) P and Q, their KL divergence is:  where T is the set of all terms in the vocabulary, and P (T ) and Q(T ) denote T 's probability of occurrence under the distribution P and Q, respectively. Note that KLD is not a metric and in particular is not symmetric, i.e., in general we have KLD(P, Q) = KLD(Q, P ). However, it is non-negative, and it is zero if and only if P = Q. Thus, it can be understood as a measure for how far apart two term distributions are from each other.</p><formula xml:id="formula_2" coords="3,357.96,696.20,197.98,24.78">KLD(P, Q) = X T ∈T P (T ) • log " P (T ) Q(T ) « ,<label>(3)</label></formula><p>In their feedback mechanism, Carpineto et al. select a set R of pseudo-relevant documents, build a language model MR for each document R ∈ R, and compute the feedback score of each term T appearing in R according to the rule</p><formula xml:id="formula_3" coords="4,84.24,385.64,208.67,24.78">ScoreFB(T ) = X R∈R MR(T ) • log " MR(T ) M * (T ) « ,<label>(4)</label></formula><p>where M * is the global language model of the entire text collection. That is, each term's score is the sum of its contributions to the documents' KL divergence from the global language model. The idea behind this definition of the feedback score is that a good expansion term is a term that has a high contribution to the difference between relevant documents and the rest of the collection. When conducting some initial experiments with this feedback method, we noticed that for almost every documents considered for feedback at least one of the query terms what among the top expansion terms from the document (a more detailed analysis is given in <ref type="bibr" coords="4,190.11,531.96,9.43,8.97" target="#b4">[4]</ref>). Thus, by performing pseudo-relevance feedback on individual documents, without taking any query into account, it is possible to predict the terms for which each document would be assigned a top rank if these terms appear in a search query.</p><p>This led us to a first definition of our document-centric index pruning method. For each document D in the text collection, a unigram language model MD is; each term T in the document D is assigned a score:</p><formula xml:id="formula_4" coords="4,89.88,630.56,203.03,21.25">ScoreDCP(T ) = MD(T ) • log " MD(T ) M * (T ) « ,<label>(5)</label></formula><p>where M * is the background language model of the text collection. All terms within a document are ranked according to their score, and the top p% are kept for each document; all other terms are discarded. Limiting the number of postings per document in this way results in a pruned index that is much smaller than the original, unpruned index. In fact, the index can be made so small that it can be completely loaded into main memory, leading to even greater gains.</p><p>We performed experiments with this pruning function and found that the pruned lists for very common terms, such as "the", tend to be rather long, longer than p% of their original size, which would be suggested by the pruning criterion. We just made a applied a minor modification to the pruning function, giving less weight to the raw term frequency of a term and more weight to the difference between the term's frequency in the given document and its relative frequency in the whole collection:</p><formula xml:id="formula_5" coords="4,346.20,545.60,209.74,21.37">ScoreDCP(T ) = MD(T ) 1-δ • log " MD(T ) M * (T ) « ,<label>(6)</label></formula><p>For the experiments reported here, we chose δ = 0.15, but our experimental results indicate that anything between 0 and 0.2 results in both decreased query latency and improved retrieval effectiveness, compared to the initial definition with δ = 0. Using the pruning criterion defined by equation 6, we built pruned indices for various pruning levels p. The size of the pruned index depends on the actual value of p. For p = 2, for instance (top 2% from each document), we obtain a pruned index of size 439 MB. For p = 8 (top 8% from each document), the size of the pruned index is 1509 MB. Note that these numbers are different from those in <ref type="bibr" coords="4,517.18,689.64,9.09,8.97" target="#b4">[4]</ref>. This is partly due to a slightly different index structure, taking document structure into account, but mainly due to a bug  <ref type="table" coords="5,83.85,118.92,4.13,8.97">4</ref>: Reranking based on relevance models and its effect on ad-hoc retrieval effectiveness. Precision values for ρ = 0 (left), ρ = 1 (middle), and ρ = |Q| (right).</p><p>found in the original implementation of our pruning method.</p><p>For query processing, we follow the same strategy already employed in <ref type="bibr" coords="5,107.49,180.48,9.09,8.97" target="#b4">[4]</ref>. The pruned index, loaded into memory, and the original, unpruned index, stored on disk, are used in parallel. Whenever a query term cannot not be found in the pruned in-memory index, the unpruned index has to be consulted. Of course, this leads to an increased query latency. On the other hand, it seems like the only way to guarantee that no terms are lost.</p><p>The impact of static index pruning on search efficiency (average query latency) and search result quality (G@10) is shown in Figure <ref type="figure" coords="5,122.96,274.68,3.54,8.97">4</ref>. For the ad-hoc topics, G@10 is almost unaffected by pruning the primary index. Only for p &lt; 6, the decrease becomes noticeable. This is not true for named page finding. The loss of precision is huge; for p = 5 and the 2005 NP topics, for instance, Success@10 decreases by 30% (from 0.5794 to 0.4048), compared to our baseline. Table <ref type="table" coords="5,80.63,337.44,4.55,8.97">4</ref> proves that this difference is not just caused by our using different measures (S@10 vs. P@10) for named page finding and ad-hoc retrieval. Even when using the same measure (MRR), the differences are enormous. They are caused by the two task types being so fundamentally different. In ad-hoc retrieval, if, by pruning the index, a relevant document disappears from the top documents, it can simply be replaced by another relevant document. For named page finding, the document is lost, and -assuming there are no duplicate documents -cannot be replaced by another relevant document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RERANKING FOR AD-HOC RETRIEVAL: RELEVANCE MODELS</head><p>Lavrenko and Croft <ref type="bibr" coords="5,151.37,491.16,14.15,8.97" target="#b11">[11]</ref> presented a retrieval method based on relevance models -language models derived from the top documents in an initial retrieval stage. Their method is similar to traditional query expansion by means of pseudo-relevance feedback <ref type="bibr" coords="5,177.99,532.92,9.09,8.97" target="#b7">[7]</ref>, but does not require a predefined limit for the number of expansion terms, because it replaces the original query by an entire language model, without any bounds on the number of terms in it.</p><p>We present a method similar to theirs, but for performance reasons do not use the language model constructed from the top documents retrieved to perform a new ranking, but only to rerank the documents retrieved by our baseline retrieval function. The reranking is performed by computing the Kullback-Leibler divergence between the language model of each document in the initial ranking and a language model built from the top 10 documents of the initial retrieval stage.</p><p>Given an initial ranking R = (Dj, sD j ) 1≤j≤K  nating them and treating them as a continuous stream of terms. The language model is a mapping from each term to its probability of occurrence:</p><formula xml:id="formula_6" coords="5,356.16,343.47,157.46,11.38">M * k : T → [0, 1] ; T → M * k (T ) = p(T )</formula><p>, where T is the set of all terms in the text collection. We use Porter's algorithm to group all stem-equivalent terms into equivalence classes and employ the maximum likelihood estimate (MLE) to generate M * k . After the language model M * k has been built from the text found in the top k documents, each document D in the initial ranking R is scored according to its similarity to M * k . This is done by analyzing the text found inside D, building a language model MD representing the document (using MLE), and computing the Kullback-Leibler divergence between the language model MD and the background model M * k . The definition of KL divergence has not changed since the previous section; the KLD between these two language models still is:</p><formula xml:id="formula_7" coords="5,335.28,510.68,202.20,25.78">KLD(MD, M * k ) = X T ∈T D MD(T ) • log " MD(T ) M * k (T ) « ,</formula><p>where TD is the set of all terms within the document D; MD(T ) and M * k (T ) are the probabilites of occurrence for the term T according to the respective language model. A smaller KL divergence means that the language model defined by document D is closer to the language model defined by the top k documents. A larger KL divergence means that it is farther away. In most cases, the KLD between the two language models is fairly small (between 0.5 and 3.0). When all KL divergence scores have been computed, the final score for each document D in R is calculated according to the rule:</p><formula xml:id="formula_8" coords="5,367.08,660.80,135.54,12.90">s (new) D := sD -ρ • KLD(MD, M * k )</formula><p>. ρ is a tuning parameter that can be used to increase or decrease the impact of the KLD score on the final ranking.</p><p>We tested two parameter configurations: ρ = 1 and ρ = |Q|, where Q is the set of query terms that was used to obtain the initial ranking (after stopword removal). The rationale behind ρ = |Q| is that the initial document scores (according to BM25) are usually higher when there are more query terms. We tried to compensate for this by increasing the weight of the KLD component in an equal fashion.</p><p>The effect of this reranking method on search quality is documented by Tables <ref type="table" coords="6,151.38,119.76,4.55,8.97">4</ref> and<ref type="table" coords="6,179.68,119.76,3.54,8.97" target="#tab_4">5</ref>. Table <ref type="table" coords="6,221.79,119.76,4.55,8.97">4</ref> shows that the method improves precision in ad-hoc retrieval tasks substantially according to all four measures, consistently across all three ad-hoc topic sets examined, and regardless of the exact value of the reranking parameter ρ. Most of these improvements are statistically significant according to a paired t-test (p &lt; 0.05). For named page finding tasks, on the other hand, reranking based on language models decreases search quality. For ρ = |Q|, for instance, S@10 drops from 0.5794 to 0.3929 on the NP '05 topic set. For ρ = 1, the quality also deteriorates, but the decrease is not caught by our primary measure G@10, as the reranking weight is not big enough yes to push the named page out of the top 10 documents.</p><p>Apart from its not working for named page finding tasks, the drawback of this method is that, in our current implementation, it requires access to the full text of the K documents that are to be reranked. As all documents are stored on disk, this is very slow; it requires at least K disk seeks. Faster implementations, using document vectors in a forward index, are possible, but share the same general shortcoming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RERANKING FOR NAMED PAGE FIND-ING: ANCHOR TEXT</head><p>For the named page finding task, the relatively simple Okapi BM25 baseline, with weighted fields to take document structure into account, already works very well for named page finding, as shown in section 3. Not surprisingly, though, it is still underperforming compared to linkor anchor-text-based retrieval functions. The best named page finding run in the TREC 2005 Terabyte track, for instance, which made use of the anchor text found in incoming links of a document, achieved an MRR of 0.463 (MRR of our baseline: 0.4236).</p><p>We addressed this issue by integrating a result reranking technique, similar to the one discussed in the previous section, that, however, is based on the anchor text of incoming hyperlinks instead of the text found in the document itself. Every link from a document Dj to document D k whose anchor text includes some of the query terms is considered additional evidence that D k is relevant. The strength of this evidence depends on Dj 's original score and the number of query terms found in the anchor text.</p><p>More precisely, given an initial ranking</p><formula xml:id="formula_9" coords="6,130.56,600.36,85.56,10.03">R = (Dj , sD j ) 1≤j≤K ,</formula><p>for each document Dj among the top K search results (K = 1000 in our experiments) we compute its anchor score as</p><formula xml:id="formula_10" coords="6,106.20,645.72,186.70,24.26">aD = 1 P Q∈Q wQ • X Q∈Q aD,Q • wQ,<label>(7)</label></formula><p>where wQ is the IDF weight of the query term Q, and aD,Q is the anchor score of the term Q for the document D. The set Q, however, is not simply the set of query terms, but is augmented by a pseudo-term Q * . This pseudo-term, which by Topics ρ = 0 ρ = .1 ρ = .2 ρ = . definition appears in the anchor text assigned with any hyperlink between two documents, is assigned the IDF weight</p><formula xml:id="formula_11" coords="6,393.24,357.48,162.70,25.39">wQ * := 1 4 X Q∈Q orig wQ,<label>(8)</label></formula><p>where Qorig is the original set of query terms, without the pseudo-term. The effect of adding the pseudo-term Q * to the query is that a link between two documents carries some weight (20% of its maximum weight), even if it does not contain any query terms. The anchor score aD,Q for a document D and a query term Q is then computed as follows:</p><formula xml:id="formula_12" coords="6,359.28,465.46,196.66,42.64">aD,Q = vD,Q k1 + vD,Q ,<label>(9) vD</label></formula><formula xml:id="formula_13" coords="6,369.11,490.61,186.71,27.09">,Q = K X j=1 [Dj → Q D] • sD j sD 1 • d j-1 ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_14" coords="6,325.32,539.84,65.64,32.17">[Dj → Q D] = 8 &gt; &lt; &gt; :</formula><p>1 : Dj uses Q in the anchor text of a link to D 0 : otherwise The formulation of equation 9 was motivated by BM25; for the free parameter k1, we chose its BM25 default value: k1 = 1.2. The parameter d in equation 10 is a standard damping factor that limits the impact of the "long tail" of documents on the ranking of highly ranked documents. In our experiments, we set d := 0.99.</p><p>The technique can be thought of as letting the documents in the initial ranking vote for each other (hence the notation vD,Q). If a document Dj links to the document D and uses the query term Q in the anchor text that goes with this link, then we count this as a vote for D's relevance. The weight of the vote is a combination of Dj 's rank and score in the initial ranking. Votes from the top-ranking document carry The votes for each pair (D, Q) are scored in an Okapilike fashion (equation 9), and these scores are combined as defined by equation 7. Once the anchor score aD for each document D has been computed, it is combined with the original score sD according to the following formula:</p><formula xml:id="formula_15" coords="7,116.88,370.16,175.90,12.78">s (new) D = s (old) D * (1 + ρ • aD).<label>(11)</label></formula><p>This gives us a new ranking Rnew. The parameter ρ in equation 11 is a tuning parameter and was set to ρ = 0.15 in our official runs. Table <ref type="table" coords="7,88.55,421.08,4.55,8.97" target="#tab_5">6</ref> shows the results we obtained for various values of the reranking parameter ρ, starting from the anchor-textunaware retrieval function defined in section 3 (BM25 plus document structure). For named page finding, the improvements caused by the anchor-based reranking, compared to the baseline, technique are statistically significant for all measures we looked at. With ρ = 0.3, our method improves Success@3 by 10% on the 2005 NP topics and by 25% on the 2006 NP topics.</p><p>For the ad-hoc retrieval topics, however, the effect is completely different. Precision decreases, and it does so quite a bit. Reranking with ρ = 0.3 decreases Precision@3 by 15%, 11%, and 3%, respectively, for the three different ad-hoc topic sets. Thus, we have the same situation that we had for the relevance-model-based reranking technique discussed in the previous section, only that this time an improvement is achieved for named page finding, while the search result quality for ad-hoc retrieval suffers.</p><p>Like for the reranking method based on relevance models, discussed in section 5, our current implementation of the reranking technique described above requires access to the full text of all documents involved in the reranking step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">OFFICIAL RUNS</head><p>We submitted runs for all three tasks of this year's Terabyte track. The runs we submitted are slightly different from the experiments described in this paper, because the system used a slightly different set of tokenization rules and Topics Latency P@5 P@10 P@20 uwmtFnoprune 246.0 ms 0.5520 0.5180 0.4770 uwmtFdcp12 32.0 ms 0.5720 0.5300 0.4790 uwmtFdcp06 17.5 ms 0.5280 0.5220 0.4610 uwmtFdcp03 12.5 ms 0.5160 0.4820 0.4110 because we optimized the parameters of the retrieval function for the respective task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ad-hoc Retrieval</head><p>For the ad-hoc retrieval task, we submitted four runsthree automatic runs and one manual run. All three automatic runs were title-only.</p><p>uwmtFadTPFB -This is a pseudo-relevance feedback run, using the technique described by Billerbeck and Zobel <ref type="bibr" coords="7,354.23,294.00,9.09,8.97" target="#b1">[1]</ref>, with 15 pseudo-relevant documents and 15 expansion terms. The initial ranking is produced by BM25TP <ref type="bibr" coords="7,379.41,315.00,9.59,8.97" target="#b5">[5]</ref> (k1 = 1.2, b = 0.5).</p><p>uwmtFadTPRR -For this run we used the reranking technique based on relevance models described in section 5. Like for uwmtFadTPFB, the initial ranking was produced by BM25TP (k1 = 1.2, b = 0.5).</p><p>uwmtFadDS -This run is very similar to the baseline retrieval method described in section 3, except that we set b := 0.5 instead of 0.75 in BM25.</p><p>uwmtFmanual -A manual run involving the assessment of 1,800 documents by a bored PhD student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Page Finding</head><p>For the named page finding task, we submitted three runs. All three runs used a frequency index in which the term frequency values were updated according to the procedure discussed in section 3. The BM25 document length normalization parameter was b = 0.75 for all runs. uwmtFnpstr1 -Simply the baseline method from section 3.</p><p>uwmtFnpstr2 -This run is like uwmtFnpstr1, except that, as a postprocessing step, all documents for which there is a duplicate document with higher rank are removed from the ranking. Since in named page finding, as opposed to ad-hoc retrieval, duplicate documents only count once, this procedure can be expected to result in a slight improvement over uwmtFnpstr1.</p><p>uwmtFnpsRR1 -The anchor-text-based reranking method described in section 6, with the reranking parameter set to ρ = 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency</head><p>For the efficiency task, we submitted four runs. All four runs were conducted on a single PC with an AMD Athlon64 3500+ CPU. Three of the four runs make use of the static index pruning method discussed in section 4. For none of the four runs, document structure was taken into account.</p><p>For all of them, BM25's document length normalization parameter was set to b := 0.5. In contrast to the experiments reported on in section 4, our official runs exclusively used the pruned in-memory index to produce search results and did not access the unpruned on-disk index at all. This resulted in a slightly decreased query latency (between 1 and 2 ms per query for the efficiency query streams), at the risk of missing query terms not present in the pruned index.</p><p>uwmtFnoprune -This run is very similar to the baseline method from section 3, with the exception that document structure was not taken into account and that the BM25 document length normalization parameter was set to b := 0.5. uwmtFdcp03 -Document-centric pruning, as described in section 4. The top 3% of all terms in a document were taken into the pruned index.</p><p>uwmtFdcp06 -The same as uwmtFdcp03, with 6% of the terms within a document taken into the pruned index.</p><p>uwmtFdcp12 -Similar to uwmtFdcp06, with 12% of the terms within a document taken into the pruned index. The only difference is that, because the original pruned index, compressed using a byte-aligned encoding method, was too large to fit into main memory, we recompressed it using a Huffman code that treated the gaps between two consecutive document IDs and the term frequency values independently, building two separate Huffman trees. The resulting pruned index was almost 30% smaller than the byte-aligned index, but required some extra computational effort at query time, due to the more complicated compression method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>We have studied the effects of static index pruning and two different result reranking techniques, one based on language models, the other one based on anchor text in links between the top-ranking documents, on ad-hoc retrieval and named page finding effectiveness.</p><p>Index pruning can decrease the average query latency greatly and -if not applied too aggressively -almost does not affect search quality in ad-hoc retrieval tasks. For named page finding, however, index pruning has disastrous effects on search quality. Similarly, search result reranking based on language models created from the top documents and reranking based on the anchor text found in inter-document hyperlinks only work for one task type, while they decrease the quality of the search results when used for the other type. This makes it difficult to apply these techniques in a general-purpose search engine that is regularly confronted with both types of search tasks.</p><p>We leave the problem of designing a unified ranking function, optimizing the quality of search results in both search contexts, for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,79.50,87.46,7.44,6.87;4,79.50,103.71,7.44,6.87;4,79.50,119.97,7.44,6.87;4,79.50,136.22,7.44,6.87;4,83.22,152.47,3.72,6.87;4,285.53,175.41,9.67,6.87;4,252.31,175.41,9.67,6.87;4,219.04,175.41,9.67,6.87;4,185.82,175.41,9.67,6.87;4,152.60,175.41,9.67,6.87;4,119.34,175.41,9.67,6.87;4,86.12,175.41,9.67,6.87;4,64.81,140.16,6.87,24.92;4,64.81,125.66,6.87,12.64;4,64.81,114.13,6.87,9.67;4,64.81,95.54,6.87,16.74;4,64.81,80.30,6.87,13.38;4,120.00,185.45,141.31,6.87;4,139.71,61.17,101.89,6.87;4,104.35,84.21,98.92,6.87;4,104.35,90.90,98.92,6.87;4,321.89,85.12,13.02,6.87;4,321.89,99.08,13.02,6.87;4,321.89,112.99,13.02,6.87;4,321.89,126.94,13.02,6.87;4,321.89,140.85,13.02,6.87;4,321.89,154.81,13.02,6.87;4,529.49,175.41,9.67,6.87;4,496.94,175.41,9.67,6.87;4,464.34,175.41,9.67,6.87;4,431.79,175.41,9.67,6.87;4,399.24,175.41,9.67,6.87;4,366.64,175.41,9.67,6.87;4,334.09,175.41,9.67,6.87;4,308.77,133.11,6.87,30.50;4,308.77,125.67,6.87,5.58;4,308.77,116.36,6.87,7.44;4,308.77,81.78,6.87,32.73;4,365.97,185.45,141.31,6.87;4,397.95,61.17,77.35,6.87;4,405.70,135.31,98.56,6.87;4,405.70,142.00,98.56,6.87;4,388.59,148.69,115.67,6.87"><head></head><label></label><figDesc>query (ms) Relative number of postings in in-memory index (a) Query processing performance TREC TB 2005 efficiency queries TREC TB 2006 efficiency queries 0Relative number of postings in in-memory index (b) Retrieval effectiveness Ad-hoc topics 701-750 (TB 2004) Ad-hoc topics 751-800 (TB 2005) Named page topics 601-872 (TB 2005)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,53.76,205.20,501.72,8.97;4,53.76,215.64,501.71,8.97;4,53.76,226.08,215.99,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Impact of index pruning on query latency and retrieval effectiveness. Latency is measured for the 2005 and 2006 efficiency queries. Effectiveness is measured by mean Goodness@10 for the ad-hoc and named page finding topics from 2004, 2005, and 2006.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,53.76,700.08,224.25,8.97;5,278.16,698.79,3.84,5.51;5,278.16,704.45,3.96,5.49;5,286.68,700.08,6.12,8.97;5,53.76,710.64,238.98,8.97;5,334.32,159.60,25.78,8.97;5,430.19,159.60,21.58,8.97;5,468.81,159.60,21.58,8.97;5,505.51,159.60,29.63,8.97;5,334.32,173.16,200.56,8.97;5,334.32,186.72,198.42,8.97;5,334.32,200.28,198.42,8.97;5,334.32,213.84,68.81,8.97;5,428.09,213.84,106.82,8.97;5,334.32,227.40,73.48,8.97;5,428.09,227.40,106.82,8.97"><head>(</head><label></label><figDesc>K = 1000 in our experiments), a language model M * k is built from the top k documents (here: k = 10) by concate-Topics ρ = 0 ρ = 1 ρ = |Q| 701-750 (ad-hoc '04) 0.4980 0.5367 0.5531 751-800 (ad-hoc '05) 0.5900 0.6320 0.6320 801-850 (ad-hoc '06) 0.4800 0.5240 0.5500 601-872 (NP '05) 0.5794 0.5754 0.3929 901-1081 (NP '06) 0.5193 0.5193 0.3315</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,316.80,255.72,239.06,154.65"><head>Table 3 :</head><label>3</label><figDesc>Impact of document-centric static index pruning on average query latency and search result quality, for pruning level p = 5 (top 5% terms from each document). Note that latency is given for the actual ad-hoc and named page topics, not for the efficiency queries.</figDesc><table coords="4,322.56,255.72,232.69,76.89"><row><cell>Topics</cell><cell>Latency</cell><cell>G@3</cell><cell>G@10 MRR</cell></row><row><cell cols="4">701-750 (ad-hoc '04) 27.2 ms 0.5510 0.4857 0.720</cell></row><row><cell cols="4">751-800 (ad-hoc '05) 22.2 ms 0.6400 0.5640 0.770</cell></row><row><cell cols="4">801-850 (ad-hoc '06) 24.1 ms 0.4933 0.5000 0.608</cell></row><row><cell>601-872 (NP '05)</cell><cell cols="3">33.9 ms 0.2976 0.4048 0.282</cell></row><row><cell>901-1081 (NP '06)</cell><cell cols="3">28.0 ms 0.3260 0.4088 0.290</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,316.80,248.64,239.03,40.29"><head>Table 5 :</head><label>5</label><figDesc>The effect of reranking based on relevance models. Precision is measured by G@10. Bold numbers indicate statistical significance (paired t-test, p &lt; 0.05) compared to ρ = 0.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,316.80,57.34,239.06,190.07"><head>Table 6 :</head><label>6</label><figDesc>The effect of reranking based on query terms found in the anchor text of incoming links.</figDesc><table coords="6,541.90,57.34,3.54,8.32"><row><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,316.80,259.68,238.96,50.73"><head>Table 7 :</head><label>7</label><figDesc>Reranking based on anchor text and its effect on named page finding effectiveness. Precision values for ρ = 0 (left) and ρ = 0.2 (right). The improvement is statistically significant for all three measures on both topic sets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,316.80,132.48,239.06,40.41"><head>Table 10 :</head><label>10</label><figDesc>Official submissions for the efficiency task. Retrieval effectiveness for ad-hoc topics 801-850 (TREC Terabyte 2006). Query latency for the efficiency query stream composed of 100,000 queries.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,58.25,631.16,96.59,17.92" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.60,646.79,212.50,8.97;8,72.60,657.23,187.52,8.97;8,72.60,667.79,216.02,8.97;8,72.60,678.23,165.17,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,208.97,646.79,76.13,8.97;8,72.60,657.23,187.52,8.97;8,72.60,667.79,43.71,8.97">Questioning Query Expansion: An Examination of Behaviour and Parameters</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Billerbeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,135.55,668.10,153.08,8.26;8,72.60,678.54,88.80,8.26">Proceedings of the 15th Conference on Australasian Database</title>
		<meeting>the 15th Conference on Australasian Database</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.60,689.63,196.54,8.97;8,72.60,700.07,200.91,8.97;8,72.60,710.63,158.99,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,194.20,689.63,74.94,8.97;8,72.60,700.07,31.24,8.97">SabIR Research at TREC-9</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Walz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,123.80,700.38,149.72,8.26;8,72.60,710.94,44.12,8.26">Proceedings of the 9th Text REtrieval Conference</title>
		<meeting>the 9th Text REtrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,56.99,216.23,8.97;8,335.64,67.55,217.27,8.97;8,335.64,77.99,213.50,8.97;8,335.64,88.43,154.44,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,498.98,56.99,52.88,8.97;8,335.64,67.55,213.40,8.97">Efficiency vs. Effectiveness in Terabyte-Scale Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,347.15,78.30,201.99,8.26">Proceedings of the 14th Text REtrieval Conference</title>
		<meeting>the 14th Text REtrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,99.83,170.30,8.97;8,335.64,110.39,214.64,8.97;8,335.64,120.83,214.24,8.97;8,335.64,131.58,197.34,8.26;8,335.64,141.71,195.45,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,498.98,99.83,6.96,8.97;8,335.64,110.39,214.64,8.97;8,335.64,120.83,103.05,8.97">A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,457.72,121.14,92.16,8.26;8,335.64,131.58,197.34,8.26;8,335.64,142.02,51.07,8.26">Proceedings of the 15th ACM Conference on Information and Knowledge Management</title>
		<meeting>the 15th ACM Conference on Information and Knowledge Management<address><addrLine>Arlington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,153.54,208.43,8.15;8,335.64,163.67,210.92,8.97;8,335.64,174.11,203.47,8.97;8,335.64,184.86,188.10,8.26;8,335.64,194.99,215.46,8.97;8,335.64,205.43,56.35,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,348.48,163.67,198.09,8.97;8,335.64,174.11,112.95,8.97">Term Proximity Scoring for Ad-Hoc Retrieval on Very Large Text Collections</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,467.32,174.42,71.80,8.26;8,335.64,184.86,188.10,8.26;8,335.64,195.30,152.85,8.26">Proceedings of the 29th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08">August 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,217.26,203.74,8.15;8,335.64,227.70,199.21,8.15;8,335.64,237.83,188.59,8.97;8,335.64,248.27,196.25,8.97;8,335.64,259.14,179.59,8.26;8,335.64,269.27,163.40,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,335.64,237.83,188.59,8.97;8,335.64,248.27,30.74,8.97">Static Index Pruning for Information Retrieval Systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,385.40,248.58,146.49,8.26;8,335.64,259.14,179.59,8.26;8,335.64,269.58,86.91,8.26">Proceedings of the 24th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,280.98,204.36,8.15;8,335.65,291.11,194.49,8.97;8,335.65,301.67,210.41,8.97;8,335.64,312.11,161.24,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,372.82,291.11,157.31,8.97;8,335.65,301.67,113.55,8.97">An Information-Theoretic Approach to Automatic Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>De Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,456.87,301.98,89.19,8.26;8,335.64,312.42,83.08,8.26">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,323.51,181.38,8.97;8,335.65,333.95,211.13,8.97;8,335.65,344.51,214.37,8.97;8,335.65,354.95,24.33,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,471.70,323.51,45.32,8.97;8,335.65,333.95,120.83,8.97">Link-Based Approaches for Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gevrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,474.98,334.26,71.80,8.26;8,335.65,344.82,126.72,8.26">Proceedings of the 10th Text REtrieval Conference</title>
		<meeting>the 10th Text REtrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,366.35,210.57,8.97;8,335.65,376.79,208.71,8.97;8,335.65,387.35,156.48,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,394.40,366.35,147.48,8.97">Overview of the TREC-9 Web Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,347.15,377.10,197.20,8.26">Proceedings of the 9th Text REtrieval Conference</title>
		<meeting>the 9th Text REtrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,399.06,210.36,8.15;8,335.65,409.19,212.61,8.97;8,335.65,419.94,197.20,8.26;8,335.65,430.19,150.24,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,384.69,409.19,147.60,8.97">Overview of the TREC-8 Web Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,335.65,419.94,197.20,8.26">Proceedings of the 8th Text REtrieval Conference</title>
		<meeting>the 8th Text REtrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-02">February 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,441.59,216.11,8.97;8,335.65,452.03,204.53,8.97;8,335.65,462.78,208.97,8.26;8,335.65,473.03,205.40,8.97;8,335.65,483.47,77.93,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,484.96,441.59,66.79,8.97;8,335.65,452.03,68.23,8.97">Relevance-Based Language Models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,423.06,452.34,117.11,8.26;8,335.65,462.78,208.97,8.26;8,335.65,473.34,86.91,8.26">Proceedings of the 24th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,495.18,216.45,8.15;8,335.65,505.31,213.55,8.97;8,335.65,515.87,195.23,8.97;8,335.65,526.31,109.72,8.97" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,348.60,505.31,200.60,8.97;8,335.65,515.87,43.03,8.97">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Stanford Digital Library Technologies Project</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct coords="8,335.64,537.71,205.67,8.97;8,335.65,548.15,190.97,8.97;8,335.65,558.59,34.40,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,402.09,537.71,135.22,8.97">An Algorithm for Suffix Stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,335.65,548.46,136.90,8.26">Readings in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,570.42,205.56,8.15;8,335.65,580.55,216.46,8.97;8,335.65,590.99,214.12,8.97;8,335.65,601.43,216.64,8.97;8,335.65,611.99,93.29,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,335.65,580.55,212.69,8.97">Simple BM25 Extension to Multiple Weighted Fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,347.16,591.30,202.61,8.26;8,335.65,601.74,166.39,8.26">Proceedings of the Thirteenth ACM Conference on Information and Knowledge Management</title>
		<meeting>the Thirteenth ACM Conference on Information and Knowledge Management<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,623.70,180.74,8.15;8,335.65,633.83,212.37,8.97;8,335.65,644.27,219.98,8.97;8,335.65,654.83,202.53,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,524.17,633.83,23.85,8.97;8,335.65,644.27,42.27,8.97">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,397.28,644.58,158.36,8.26;8,335.65,655.14,44.12,8.26">Proceedings of the Third Text REtrieval Conference</title>
		<meeting>the Third Text REtrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.64,666.54,205.42,8.15;8,335.65,676.67,210.08,8.97;8,335.65,687.11,205.47,8.97;8,335.65,697.98,208.97,8.26;8,335.65,708.11,196.64,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,379.43,676.67,166.30,8.97;8,335.65,687.11,69.57,8.97">Compression of Inverted Indexes for Fast Query Evaluation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yiannis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,424.01,687.42,117.11,8.26;8,335.65,697.98,208.97,8.26;8,335.65,708.42,86.91,8.26">Proceedings of the 25th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
