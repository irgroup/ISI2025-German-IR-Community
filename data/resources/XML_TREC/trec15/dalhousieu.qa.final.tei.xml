<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.18,164.85,316.89,15.11;1,158.75,186.77,293.76,15.11;1,236.28,208.69,138.70,15.11">DalTREC 2006 QA System Jellyfish: Regular Expressions Mark-and-Match Approach to Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-10-24">24 October 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.52,241.17,65.08,10.48"><forename type="first">Vlado</forename><surname>Kešelj</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.45,241.17,100.28,10.48"><forename type="first">Tony</forename><surname>Abou-Assaleh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.65,255.12,67.95,10.48"><forename type="first">Nick</forename><surname>Cercone</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.18,164.85,316.89,15.11;1,158.75,186.77,293.76,15.11;1,236.28,208.69,138.70,15.11">DalTREC 2006 QA System Jellyfish: Regular Expressions Mark-and-Match Approach to Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-10-24">24 October 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">78B736A129FECB565B8E3D4DDD7AD6CD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a question-answering system Jellyfish. Our approach is based on marking and matching steps that are implemented using the methodology of cascaded regular-expression rewriting. We present the system architecture and evaluate the system using the TREC 2004TREC  , 2005, and 2006 datasets. TREC 2004 was used as a training dataset, while TREC 2005 and TREC 2006 were used as testing dataset. The robustness of our approach is demonstrated in the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous work has explored the application of several approaches to question answering in the overlapping area of unification-based and stochastic NLP (Natural Language Processing) techniques <ref type="bibr" coords="1,280.09,517.68,10.51,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,293.74,517.68,7.01,8.74" target="#b0">1]</ref>. Two novel methods that were explored relied on the notions of modularity and just-in-time sub-grammar extraction.</p><p>One of the learned lessons of the previous experiments is that the regular expression (RegExp) substitutions are a very succinct, efficient, maintainable, and scalable method to model many NL subtasks of the QA task. This is also observed in the context of lexical source-code transformations of arbitrary programming languages <ref type="bibr" coords="1,241.82,589.41,9.97,8.74" target="#b1">[2]</ref>, where RegExp substitutions are an alternative to manipulating the abstract syntax tree, and proved to be more robust in the face of missing header files, errors, usage of macros, templates, and other embedded programming language constructs.</p><p>We employ RegExp rewriting as a primary technique in our question-answering (QA) system Jellyfish. We use RegExpmatching and rewriting at various stages of the system, whose architecture is described in section 3. We evaluate our system using the datasets from TREC 2004, TREC 2005, and TREC 2006. Our system is developed with respect to the TREC 2004 dataset, which acts as a training dataset. The TREC 2005 and TREC 2006 datasets are used for testing.</p><p>Our goal is to apply a unification-based approach as a high-level answerextraction step on top of the low-level RegExp processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Regular Expression Rewriting</head><p>The basic method used at various components of the QA system is RegExp rewriting. The open angle bracket (&lt;) is used as a special escape character, hence we make sure that it does not appear in the source text, which is either a question or a passage. The basic text substrings, such as the target or named entities, are recognized using regular expressions and replaced with an anglebracket-delimited expression. For example, the target is marked as &lt;TARGET&gt;. More commonly, a named entity e of type t is replaced with &lt;t_e s &gt;, where e s is the named entity e encoded as a string of printable characters that do not include &lt;. The RegExp rewriting can be seen as a bottom-up deterministic parsing technique. For example, the rewriting in which "&lt;NP_x&gt; &lt;VP_y&gt;" is replaced with "&lt;S_z&gt;" corresponds to the context-free rule S → NP VP. The value z is obtained by decoding x and y, concatenating them, and encoding the result again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Architecture</head><p>The current system consists of the following phases: 1) Question Processing, 2) Passage Retrieval, 3) Target Marking, 4) Question Category Marking, 5) Matching, 6) Answer selection, and 7) Post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Processing</head><p>The Question Processing phase takes the original questions as input, parses them, and generates complete questions as output. Questions are parsed using RegExp matching and substitution to identify the question category and extract some related metadata. Some metadata extracted during parsing is analyzed using WordNet <ref type="bibr" coords="2,204.61,558.74,10.52,8.74" target="#b2">[3]</ref> to identify additional metadata and relationships between the target and what the question is asking about.</p><p>In TREC datasets, question are grouped by targets. Replacing the anaphoric references in questions with the target results in self-contained questions. These complete questions are used in passage retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Retrieval</head><p>The passage retrieval from the AQUAINT dataset is performed by an external search engine using the full questions generated in the question processing phase. We use two different sets of passages. The first set is the passages provided by NIST using the PRISE search engine. The second set is generate using our MySQL-based search engine that employed MySQL's full-text indexing functions. In both cases, the PRISE and MySQL, the results are treated in the same way-as passages relevant to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Target Marking</head><p>The string in the question that constitutes that target is identified during the question processing step (section 3.1). Using simple RegExp rewriting rules, the target is identified in the passages and replaced with the &lt;TARGET&gt; tag. In effect, this phase annotates sentences in the passages that may contain an answer. Currently, our system does not handle intra-sentence references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Question Category Marking</head><p>During this step, the system scans all the relevant passages and uses RegExp rewriting to mark entities that belong to the question category. The type of Reg-Exp used depends on the question category and may be a simple keyword-based RegExp or a sophisticated multipart RegExp. Question category marking acts as a just-in-time annotation phase where only the passages that may be relevant to the current question are annotated, and the annotation data is customized based on the question category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Matching</head><p>During answer matching, question metadata and annotated passages are combined using special RegExp rules to generate candidate matches. The rules are applied sequentially. Every time a match is found, it is added to the list of matches. The realtive order of the rules imposes an implicit ranking of matches. Consequently, more specific rules are placed before the more general ones. Since some questions may have no answers in the dataset, one must avoid using rules that are too general. Appropriate rule generality and ordering depends on the question category. Typically, more specific question categories permit the usage of more general rules, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Answer Selection</head><p>This phase is a filtering step that takes the list of answer for each question from the matching phase and selects the answers that are to be included in the output. For TREC, we set the number of answers for factoid questions to 1 and for list questions to 7. Presently, we simply select the first answers in the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Post-processing</head><p>This phase formats the output either for evaluation, inspection, or integration in other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The TREC 2004 QA dataset was used for deriving the rules and fine tuning the system. Testing of the system used TREC 2005 and TREC 2006 QA datasets. On the training data, our system was able to correctly answer upto 46 out of 230 facoitd questions yeilding an accuracy of 20%. In the TREC 2005 dataset, the number of factoid questions was increased to 364, and included new types of questions dealing with events. Our system answered correctly 41 questions on the testing dataset yielding an accuracy of 11.3%. In the TREC 2006 dataset, the number of factoid question was 403. The formulation of the questions in TREC 2006 relied more heavily on expanding the questions to incorporate the context information provided in the target of each series. Our system answered 36 questions globally correctly, and 2 locally correctly, yielding global correctness accuracy of 8.9%. The results of evaluation on all 3 datasets are presented in table <ref type="table" coords="4,171.75,293.24,3.88,8.74" target="#tab_0">1</ref>, where the number in the Correct column reflects the globally correct answers and in paranthesis the locally correct answer; the Pyramid column shows a new alternative method for evaluating the "Other" questions. The suffix in the run name is interpreted as follows </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Lessons Learned</head><p>RegExp rewriting is a simple and powerful parsing technique. We have effectively used coarse-grained modularization of RegExp, and combined it with dynamic loading of RegExp. Fine grained modularization of RegExp is possible, and would simplify the task of creating RegExp rules. Using a macro system, such as Starfish, can greatly reduce the complexity of RegExp rules. Just-in-time RegExp-based annotation can lower the computation requirements of deeper analysis. Our system is fairly robust. There performance of Jellyfish on TREC 2005 and TREC 2006 questions is comparably. There is a slight decline in the accuracy on factoid question, a significant improvement in the list questions score, and a noticeable decline in the "other" questions score. We note that the core rules of the system are almost identical between for our 2005 and 2006 submissions, subjected only to minor bug fixes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,133.77,327.53,343.72,199.89"><head>Table 1 :</head><label>1</label><figDesc>1 :m means the MySQL-based search engine is used, p means the top 50 results from the PRISE search engine are used, and e means the top 100 results from PRISE are used. Jellyfish results from TREC QA Track</figDesc><table coords="4,156.05,375.68,299.16,129.88"><row><cell>Run</cell><cell cols="6">Correct Factual List Other Pyramid Session</cell></row><row><cell>Dal06e</cell><cell>36(2)</cell><cell>0.0 89</cell><cell cols="2">0.021 0.028</cell><cell>0.032</cell><cell>0.046</cell></row><row><cell>Dal06p</cell><cell>33(2)</cell><cell>0.082</cell><cell cols="2">0.019 0.033</cell><cell>0.038</cell><cell>0.045</cell></row><row><cell>Dal06m</cell><cell>20(1)</cell><cell>0.050</cell><cell cols="2">0.010 0.037</cell><cell>0.033</cell><cell>0.033</cell></row><row><cell>Dal05p</cell><cell>41</cell><cell>0.113</cell><cell cols="2">0.033 0.088</cell><cell>-</cell><cell>0.087</cell></row><row><cell>Dal05m</cell><cell>27</cell><cell>0.075</cell><cell cols="2">0.017 0.056</cell><cell>-</cell><cell>0.056</cell></row><row><cell>Dal04p</cell><cell>46</cell><cell>0.200</cell><cell>0.092</cell><cell>-</cell><cell>-</cell><cell>0.164</cell></row><row><cell>Dal04m</cell><cell>41</cell><cell>0.178</cell><cell>0.083</cell><cell>-</cell><cell>-</cell><cell>0.146</cell></row><row><cell>Best 06</cell><cell>-</cell><cell>57.8</cell><cell cols="2">0.433 0.250</cell><cell>0.251</cell><cell>0.394</cell></row><row><cell>Median 06</cell><cell>-</cell><cell>18.6</cell><cell cols="2">0.087 0.125</cell><cell>0.139</cell><cell>0.134</cell></row><row><cell>Worst 06</cell><cell>-</cell><cell>4.0</cell><cell cols="2">0.000 0.000</cell><cell>0.000</cell><cell>0.013</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,149.01,664.11,299.31,6.99"><p>The labels of runs for previous years may differ from the actual submitted labels</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,149.27,242.50,328.21,8.74;5,149.27,254.46,328.21,8.74;5,149.27,266.41,85.82,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,454.62,242.50,22.87,8.74;5,149.27,254.46,197.49,8.74">From computational intelligence to web intelligence</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Naruedomkul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,355.30,254.46,68.33,8.74">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="72" to="76" />
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,286.34,328.21,8.74;5,149.27,298.29,328.22,8.74;5,149.27,310.25,151.98,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,364.35,286.34,113.13,8.74;5,149.27,298.29,40.63,8.74">Lexical source-code transformation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Abou-Assaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,216.88,298.29,254.14,8.74">Proceedings of the STS&apos;04 Workshop at GPCE/OOPSLA</title>
		<meeting>the STS&apos;04 Workshop at GPCE/OOPSLA<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,330.17,328.21,8.74;5,149.27,342.13,22.70,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,240.55,330.17,179.49,8.74">WordNet: An Electronic Lexical Database</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,149.27,362.05,328.21,8.74;5,149.27,374.01,328.21,8.74;5,149.27,385.96,328.22,8.74;5,149.27,397.92,22.70,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,193.95,362.05,224.69,8.74">Question answering using unification-based grammar</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,267.63,374.01,145.61,8.74;5,171.89,385.96,146.37,8.74">Advances in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Stroulia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</editor>
		<meeting><address><addrLine>AI; Ottawa</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
	<note>LNAI 2056 of L.N. in Comp.Sci.</note>
</biblStruct>

<biblStruct coords="5,149.27,417.85,328.21,8.74;5,149.27,429.80,328.22,8.74;5,149.27,441.76,171.22,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,200.95,417.85,223.15,8.74">Modular stochastic HPSGs for question answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
		<idno>CS-2002-28</idno>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
			<pubPlace>Waterloo, Ontario, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
