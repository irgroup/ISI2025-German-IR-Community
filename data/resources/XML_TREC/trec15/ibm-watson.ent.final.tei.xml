<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.34,87.88,294.10,16.65">IBM in TREC2006 Enterprise Track</title>
				<funder ref="#_5dT6Y2P">
					<orgName type="full">Disruptive Technology Office</orgName>
					<orgName type="abbreviated">DTO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,110.10,122.70,83.10,9.02"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
						</author>
						<author>
							<persName coords="1,225.71,122.70,83.10,9.02"><forename type="first">Guillermo</forename><surname>Averboch</surname></persName>
						</author>
						<author>
							<persName coords="1,351.28,122.70,57.05,9.02"><forename type="first">Pablo</forename><surname>Duboue</surname></persName>
							<email>duboue@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,443.30,122.70,58.67,9.02"><forename type="first">David</forename><surname>Gondek</surname></persName>
							<email>dgondek@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,233.14,134.16,71.99,9.02"><forename type="first">William</forename><surname>Murdock</surname></persName>
							<email>murdockj@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,340.14,134.16,47.53,9.02"><forename type="first">John</forename><surname>Prager</surname></persName>
							<email>jprager@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,249.12,180.18,61.39,9.02"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
							<email>hoffmanp@cs.pitt.edu</email>
						</author>
						<author>
							<persName coords="1,343.10,180.18,55.84,9.02"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
							<email>wiebe@cs.pitt.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.34,87.88,294.10,16.65">IBM in TREC2006 Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF303914E305FC0A5E9E28242D6A91A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In 2006, IBM participated for the first time in the Enterprise Track, submitting runs for both the discussion and expert tasks. The Enterprise Track is intended to address information seeking tasks common in corporate settings using information that is readily available on corporate intranets. Because of confidentiality issues, the corpus used for this task is a snapshot of w3c.org as of June 2004, considering the W3C as a "corporation" that conducts its dayto-day business on the web. This corpus consists of a heterogeneous set of data sources, including web pages, mailing lists, code, wiki, etc. <ref type="bibr" coords="1,192.10,328.56,82.05,9.02" target="#b2">[Craswell et al. 2006</ref>]. The discussion task seeks e-mail messages that discuss the pro or con of a given subject matter, while the expert task requires that experts for given topics be identified.</p><p>The main foci of our Enterprise Track experiments this year were on 1) problem-solving through adoption of multiple discussion and expert finding strategies, 2) combination of structured, semi-structured, and unstructured information for discussion and expert finding, 3) investigation of impact of select NLP techniques, such as multidocument summarization for expert pseudo-document generation and pro/con sentiment identification and retrieval, and 4) use of external resources for discussion and expert finding. The rest of this paper discusses the systems we developed for Enterprise Track participation, focusing on the four aspects outlined above. We will also discuss specific strategies we took to configure the systems for each of the four runs in both tasks, as well as their impact on system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discussion Task</head><p>The discussion task utilizes only the lists section of w3c.org, which contains the archives of W3C mailing lists. Given a topic, a ranked list of documents (e-mail messages) is returned in which an e-mail message contains at least one pro or con statement regarding the topic in the "new content" portion of the message (i.e., the relevant pro/con statements do not occur only in quoted text). In the discussion task, we focused our investigation on using multiple problem-solving strategies, adopting NLP techniques for pro/con-based re-ranking of search results, and using external resources for query analysis and expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Corpus Processing</head><p>The lists portion of the w3c corpus was processed to identify the author, subject, and new vs. quoted texts for each e-mail message. A corresponding new document is constructed that contains the author, subject, and new text portion of each e-mail message. These documents form the corpus from which three keyword indices are built for the three search engines we employ in our system. In addition, each document is annotated with our pro/con sentiment analyzer and the annotated documents are stored for runtime use by the IBM pro/con assessor component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Runtime System Architecture and Components</head><p>Figure <ref type="figure" coords="1,100.68,677.16,5.01,9.02">1</ref> shows the architecture of our pro/con discussion retrieval system. Given a topic, which consists of a title, a description, and a narrative, our query analysis component transforms the topic into one or more generic abstract search query representations. The document retrieval module is implemented as a pluggable framework that accepts query generator and search engine pairs to return document hit lists given the abstract query representations. The hit lists returned by the retrieval engines are combined using a linear combination algorithm and additional potentially relevant documents are included by the e-mail-thread-based augmenter. The resulting hit list contains topic-relevant documents. The documents in the hit list are then evaluated for containment of pro/con statements again in a pluggable pro/con re-ranking framework which accepts assessors that rescore the original document list. The rescored lists are again combined to form the final ranked document list. The rest of this section describes select key system components in further detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Query Analysis</head><p>A discussion search topic contains a title, a description, and a narrative. Our system makes use of the title and description fields of the topic to generate one or more abstract query representations from which specific search engine queries can be generated. Aside from standard stopword removal, we perform query expansion as well as identify important keywords and their respective salience in an iterative fashion through the use of an external domain-specific dictionary.</p><p>Our domain-specific dictionary was developed based primarily on FOLDOC,<ref type="foot" coords="2,392.64,515.23,3.24,5.83" target="#foot_0">1</ref> an online dictionary of computing terms containing more than 14,000 entries. Entries in FOLDOC contain a natural language description of the terms being defined and may also include hyperlinks to other entries in the dictionary. We automatically processed these definitions in FOLDOC and extracted, for each term, its acronym (or expansion if the term is an acronym), if any, and the system's confidence that the acronym and expansion are co-referents of one another. In addition, we extract phrases highly associated with each entry term. We further augment the dictionary with terms of interest that are not present in FOLDOC, in particular, topics addressed by W3C standards.</p><p>Our query analysis process employs an iterative procedure that produces one or more abstract query representations based on dictionary lookup and a set of heuristics. An initial abstract query representation contains terms in the title and description fields in the topic, after stopword removal. Based on an n-gram (in our current implementation n=3) dictionary lookup, the abstract query representation may be augmented/duplicated in the following ways. First, all phrases present in the dictionary are marked as "phrase". Second, all dictionary terms/phrases in the query are marked as "required". Third, all high-confidence acronyms/expansions for terms/phrases in the query are returned and a duplicate abstract query representation is constructed in which the acronym/expansion substitutes the original term in the query. The resulting set of abstract query representations forms query representations of high salience.</p><p>In the second iteration, a set of query representations of medium salience is generated by removing the "required" and "phrase" markers on all query terms from the high-salience set. The third and fourth iterations are identical to the first and second, except that low-confidence acronyms/expansions are returned from the dictionary. This ranked set of abstract query representations is the output of our query analysis component, and is used by downstream document retrieval components to generate search engine specific queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Document Retrieval</head><p>As discussed previously, our document retrieval component is a pluggable framework that accepts one or more query generator and search engine pairs. In our TREC system, we employed three publicly available, off-the-shelf search engines: IBM's own Juru search engine<ref type="foot" coords="3,270.54,205.69,3.24,5.83" target="#foot_1">2</ref>  <ref type="bibr" coords="3,276.72,207.78,87.09,9.02" target="#b1">[Carmel et al., 2002]</ref>, the Lucene search engine<ref type="foot" coords="3,479.94,205.69,3.24,5.83" target="#foot_2">3</ref>  <ref type="bibr" coords="3,486.12,207.78,53.82,9.02;3,72.00,219.30,80.00,9.02" target="#b5">[Hatcher and Gospodnetic, 2004]</ref>, and UMass/CMU's Indri search engine<ref type="foot" coords="3,332.16,217.21,3.24,5.83" target="#foot_3">4</ref>  <ref type="bibr" coords="3,339.36,219.30,96.99,9.02" target="#b10">[Strohman et al, 2005]</ref>. We developed query generation components for each of these search engines in an attempt to generate search queries that best leverage the expressiveness of the query languages of the underlying search engine.</p><p>We hypothesized that using multiple search engines and merging their hit lists will result in better performance because the search engines support different query capabilities and employ different ranking algorithms. For instance, Indri supports a window operator (either ordered or unordered) for grouping select query terms as well as weights associated with terms, while Juru and its extension JuruXML support search over semantic types as well as keywords.<ref type="foot" coords="3,113.64,309.19,3.24,5.83" target="#foot_4">5</ref> Furthermore, Indri supports pseudo-relevance feedback, while the other two search engines do not, <ref type="foot" coords="3,536.76,309.19,3.24,5.83" target="#foot_5">6</ref>leading to additional diversity in the documents in the hit lists. Typically, each search engine will use abstract query representations starting from those in the highest salience set. If insufficient documents are retrieved after executing queries at one salience level, then the query representations in the next salience level are used. Once each search engine assembles its hit list, they are combined and augmented as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Hit List Combination and Augmentation</head><p>Our hit list combination component adopts a linear combination algorithm trained to optimally combine hit lists from multiple search engines. The weights were trained on the TREC 2005 discussion task dataset using the topic relevance judgments. In order to avoid overfitting, grid search was used to optimize for MAP score and the solution was checked for stability.</p><p>Since the indexed portion of each e-mail message contains only the author, subject, and new text section of the message, we developed an e-mail-thread-based augmentation component to ensure that all documents in the same thread as a document in the hit list are also included for pro/con assessment. We made use of the e-mail-threading information made available by William Webber <ref type="bibr" coords="3,279.96,490.68,65.14,9.02" target="#b11">[Webber, 2005]</ref> and appended all additional documents found through this mechanism at the end of the hit list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Pro/Con Assessment</head><p>Our pro/con assessment component is again designed as a pluggable framework for experimenting with and combining multiple assessment algorithms. In our TREC system, two assessment components were used, a rulebased pro/con sentiment recognizer developed at IBM, and a statistical topic independent pro/con recognizer developed at the University of Pittsburgh.</p><p>The IBM pro/con assessor makes use of a rule-based sentiment recognizer built on top of ESG, a slot-grammar parser <ref type="bibr" coords="3,99.01,612.66,63.14,9.02" target="#b6">[McCord, 1990]</ref>. These rules are developed through manual compilation of 700+ sentiment keywords such as "deficient" and "compelling", and development of a rule set that makes sentiment annotations based on these keywords in the context of a parse tree. A sample sentiment annotation is shown in the following: &lt;REL name="Sentiment"&gt; &lt;RNG type="Obj"&gt;HTTP with simple XML&lt;/RNG&gt; may be most &lt;DMN type="SentimentKeyword"&gt;appropriate&lt;/DMN&gt; &lt;/REL&gt; Given a document, our assessment component identifies text snippets relevant to the topic and determines the pro/con relevance of the document based on the proximity of the sentiment annotations and the relevant text snippets. Other features such as depth of e-mail message in a thread chain, presence of relevant pro/con statements in the quoted texts portion of a document, as well as the frequency that passages in the current document are quoted by others, are also taken into account in the determining the final ranking of the documents.</p><p>The UPitt topic independent pro/con classifier takes as input a document and returns a confidence score on whether or not it contains a pro/con argument. The classifier uses the BoosTexter AdaBoost.MH machine learning algorithm <ref type="bibr" coords="4,72.00,234.66,114.13,9.02" target="#b9">[Schapire and Singer, 2000]</ref> with two features. The first feature is a bag-of-words representation of words with the following parts of speech: adverb, verb, noun, pronoun, adjective, modal, existential there, and number. The other feature is a bag-of-words representation of pro/con and non-pro/con extraction patterns generated using AutoSlog-TS from the Sundance/AutoSlog package <ref type="bibr" coords="4,241.93,269.16,104.50,9.02">[Riloff and Phillips, 2004]</ref>. AutoSlog-TS takes a set of positive examples (documents judged as pro/con in the TREC 2005 dataset) and negative examples (topic-relevant but not pro/con documents), and generates extraction patterns, ranked in the order of their association with the positive examples (most extraction pattern types used are listed in Figure <ref type="figure" coords="4,294.96,303.66,5.01,9.02">1</ref> of <ref type="bibr" coords="4,313.80,303.66,75.98,9.02" target="#b8">[Riloff et al., 2006]</ref>). During development, the classifier was cross-validated with the TREC 2005 discussion task dataset. For our TREC 2006 submission, all topic-relevant documents in the 2005 dataset were used to train the classifier.</p><p>The ranked hit lists from the assessment components are again combined using a linear combination algorithm to generate the system's final ranked list of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation Results</head><p>We submitted four runs to the discussion task. For all runs, FOLDOC was used in the query analysis process for query expansion. When the description field is used, only terms found in FOLDOC are included in the query. The key characteristics of the four runs are as follows:</p><p>• IBM06JAQ: This run used only the title field of the query. For document retrieval, only the Juru search engine is used, and for pro/con assessment, only the IBM pro/con assessor was used. • IBM06JAQD: This run is identical to IBM06JAQ except that both the title and description of the query were used. • IBM06JILAQD: This run used both titles and descriptions. For document retrieval, all three search engines were used, but only the IBM pro/con assessor was used for sentiment analysis. • IBM06JILAPQD: This run is identical to IBM06JILAQD, except that both pro/con assessors were used to re-rank the documents returned by the search engines.</p><p>In order to better determine the contributions of individual components, we ran three additional configurations of the system which were not included in our submission, as follows:</p><p>• JQ: This run used only the title field of the query. For document retrieval, only the Juru search engine was used and no pro/con sentiment analysis was performed. • JILQ: This run is identical to JQ except that for document retrieval, all three search engines were used.</p><p>• JILQD: This run is identical to JILQ except that both the title and description were used.</p><p>Error! Not a valid bookmark self-reference. shows the results of our runs. The top four rows are the results of our official submitted runs while the last three are additional runs performed post TREC. The columns labeled "topic" measure topic relevance of the hit list, while those labeled "p/c" additionally take into account the presence of pro/con statements in the document. As expected, the run IBM06JILAPQD in which both titles and descriptions were used in query analysis, and where all available search engines and pro/con assessors were employed and their results merged, outperformed other configurations of the system in almost all cases. Our results show that 1) leveraging information provided in the description field of the topic for query analysis results in more effective search queries than using the title alone (JILQD vs. JILQ; rows 7 and 6), 2) effective combining hit lists from multiple search engines can substantially outperform using a one search engine alone (JQ vs. JILQ; rows 5 and 6; as well as IBM06JAQD vs. IBM06JILAQD; rows 2 and 3), 3) leveraging NLP technologies to perform pro/con analysis on the resulting hit list results in further substantial improvement in performance (JILQD vs. IBM06JILAQD; rows 7 and 3), and 4) adopting multiple pro/con assessment components can further improve system performance compared with single strategy pro/con analysis (IBM06JILAPQD vs. IBM06JILAQD; rows 4 and 3). Interestingly, when including the pro/con assessment components in the pipeline, the scores for both topic relevance and pro/con relevance are improved. We hypothesize that this is partially due to the fact that the IBM pro/con assessor performs finer-grained topic relevance analysis at the text snippet level, thus affecting the relative ranked order of topic relevant documents as a side effect of pro/con re-ranking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Expert Task</head><p>For the expert task, a system is expected to return a ranked list of up to 100 people from a list of 1092 candidates who it considers experts for a given topic. The system may make use of the entire W3C corpus as well as any external resources in determining the ranked list of experts. The expertise of each expert, however, should be justified by up to 20 documents from the W3C corpus. In the expert task, we focused our investigation on using multiple problem-solving strategies, adopting NLP techniques for expertise-driven information extraction and pseudo-document generation, exploiting use of structured, semi-structured, and unstructured information on expert finding, and augmenting strategies that make use of the W3C corpus with those that consult external resources.</p><p>Figure <ref type="figure" coords="5,101.53,446.22,5.01,9.02">2</ref> shows the architecture of our multi-agent expert search system. At the heart of the system there are six agents each adopting different problem-solving strategies to identify zero or more experts for a given topic. Three of the agents adopt the pseudo-document approach, which is inspired by the top-performing system from TREC 2005 in which a pseudo-document is generated for each candidate expert to represent their expertise <ref type="bibr" coords="5,475.88,480.72,62.80,9.02" target="#b4">[Fu et al., 2006]</ref>; although the strategies we adopted for pseudo-document generation differ from theirs substantially. Our three pseudo-document based agents employ different algorithms/components for ranking expertise. The Google Scholar agent makes use of the eponymous external resource to identify experts based on publications. The Expert MetaData agent identifies experts based on expertise information extracted from semi-structured W3C standards documents. Finally, the EKDB agent leverages structured data extracted from texts using task-specific relations for expert identification. Our expert search system leverages the same query analysis component as described in Section 2.2.1, as well as the machine learning based hit list combination component described in Section 2.2.3 to merge the results produced by the six agents. This hit list optionally goes through two heuristic-based postprocessors. The first post-processor affects the expert ranked list in which experts in the original hit list may be reordered based on certain external criteria. The second post-processor affects the support document ranked list for each expert in that a support document may be removed because it was deemed non-supportive, or that the support documents may be re-ordered based on select heuristics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Corpus Processing and Pseudo-Document Generation</head><p>Our pseudo-document generation process begins with identifying instances of the candidate experts in documents.</p><p>To increase the recall of candidate identification, we performed accent normalization on the corpus and developed a set of heuristics for name variation generation, including dropping of middle names and replacing first names with first initials. We adopted four approaches to generate six sets of pseudo-documents in which each set contains one document for each candidate identified in the corpus as follows:</p><p>• Windowing approach, in which the n sentences before and after each mention of a candidate is selected and appended to the candidate's pseudo-document. We used n=5 and n=50 in our system. • Top sentence approach, in which the first n sentences of a document in which a candidate is mentioned is selected and appended to the candidate's pseudo-document. We used n=20 and n=100 in our system. • Whole document approach, in which all documents in which a candidate is mentioned are concatenated to form the pseudo-document. • Summarization approach, in which MEAD 7 [Radev et al., 2003], a query-based multi-document summarizer was used to generate the pseudo-document for each candidate.</p><p>To experiment with different search strategies, we built two search indices for each pseudo-document set, one for Lucene and one for Indri. In addition, we also built a collection of Lucene document vectors for each pseudodocument set.</p><p>We also performed additional corpus processing for use by the other agents. The lists and www portions of the corpus were analyzed using a set of named entity and relation recognizers. The named entity recognizers identify generic entity types such as Person, Date, Organization, and Country, as well as domain specific types such as EnterpriseMember and ComputingTopic. The relation recognizers cover two relations of interest for this task, ExpertIn and AuthorOf. The former identifies text fragments that are indicative of expertise, such as "James Larson, W3C Voice Browser Working Group co-Chair", and associates the person and topic with an ExpertIn relation, while the latter pairs all e-mail authors and subjects. The entities and relations in this annotated corpus are stored in our Extracted Knowledge DataBase (EKDB) <ref type="bibr" coords="6,242.19,625.50,89.62,9.02" target="#b3">[Ferrucci et al., 2006]</ref> for use by the EKDB agent, and all the keywords and annotations are used to generate a JuruXML semantic search index which was used in our manual submission.</p><p>Finally, we automatically identified a set of W3C-specific standards documents which, for the most part, have a semi-structured common format. We processed these documents to extract document title, document ID, editorship, and authorship information. We stored this information in as standards document metadata, to be used by our Expert MetaData agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Runtime Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Pseudo-Document Agents</head><p>The key idea behind the pseudo-document agents is to construct, for each candidate expert, a document that represents his/her expertise. Given a topic, the agent compares the topic to the pseudo-documents and returns experts whose pseudo-document best matches the topic. We experimented with a number of heuristics for pseudodocument generation as described in Section 3.1. The windowing approach hypothesizes that one's expertise commonly occurs in close proximity to mentions of his/her name. The top sentence approach hypothesizes that one's expertise is indicated by the topic of the document, which is typically established at the beginning of the document. The whole document approach targets a recall-oriented strategy. Finally, the summarization approach experiments with the utility of a multi-document summarization system for capturing the expertise of a given person.</p><p>Figure <ref type="figure" coords="7,101.25,258.90,5.01,9.02">2</ref> shows the configuration of our three pseudo-document agents, each utilizing a different retrieval strategy, namely Lucene, Indri, and vector similarity, on each of the pseudo-document sets generated.<ref type="foot" coords="7,459.72,268.27,3.24,5.83" target="#foot_7">8</ref> Given a query, a pseudo-document agent sends the query to each pseudo-document index and combines their result to generate its expert hit list. The supporting document list is generated by issuing the same topic to a regular document index and selecting from the most relevant documents those in which the candidate expert occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">EKDB Agent</head><p>As discussed earlier, we store in our Extracted Knowledge DataBase (EKDB) entities and relations extracted from our automatically annotated corpus. In addition, the EKDB records provenance information which includes documents from which entities and relations were extracted. Of the two relations used for this task, ExpertIn, which indicates direct assertions of expertise, is used as high-confidence evidence for expertise, while AuthorOf, which indicates authorship of e-mail subjects, is used as low-confidence evidence for expertise.</p><p>The EKDB agent generates database queries based on the abstract query representations produced by the query analysis component, and experts are scored based on the system's confidence level as indicated by the ExpertIn or AuthorOf relation, and the salience level of the abstract query representation used to generate the database queries. Furthermore, documents from which the relation was extracted are used as supporting evidence. If the same expert is found from multiple relations and/or multiple queries, the confidence score is boosted and the confidence values assigned to the supporting documents are also scaled accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Expert MetaData Agent</head><p>The Expert MetaData agent targets high-precision performance by basing expert identification on highly reliable cues for expertise indication. In our TREC system, it made use of the metadata extracted from W3C standards documents in which editorship and authorship of a standards document are considered definite and highly reliable indications of expertise, respectively.</p><p>Given a topic, all standards documents potentially relevant to the topic are retrieved. Linguistic heuristics are then applied to determine topic subsumption and thus the degree of match between the query topic and the document topic. The editors and authors of relevant documents are extracted and scored based on the degree of topic match, while the standards document is used as support for the candidate's expertise. Multiple pieces of evidence for the same expert are combined to arrive at a final ranked expert list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Google Scholar Agent</head><p>An external resource we exploited in our expert search system is Google Scholar, 9 which supports search over scholarly publications. For a given query topic (issued in quotes to Google Scholar), we extract the author list and the number of citations for each of the top 20 publications returned. An author's expertise in the topic is computed based on factors such as the number of publications retrieved, the number of citations as well as the author's order in the author list for each publication. The ranked list is then filtered using the candidate expert list provided to return only eligible experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">Expert Post-Processor</head><p>We developed an affinity-based expert re-ranker to perform post-hoc re-ordering of less confident candidates in the expert list, using an affinity metric based on the hypothesis that experts of the same topic tend to co-occur in documents. The algorithm initially populates a new hit list with the top 5 experts from the original list, and iteratively adds experts to the hit list by selecting those experts whose scores representing their affinity with those already on the hit list exceed a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6.">Supporting Document Post-Processors</head><p>We developed two types of supporting document post-processors, one that removes supporting documents from the list, and two that re-ranks the documents. The acknowledgments document filter removes those supporting documents in which the candidate's name appears only in the acknowledgments section of the document. The duplicate document remover increases the diversity of supporting documents by eliminating all duplicates and nearduplicates in the document list using a vector similarity based measure. Finally, the EKDB re-ranker considers documents that support the ExpertIn and AuthorOf relations in the EKDB to be more reliable supporting documents than others, and uses this information to re-rank supporting documents returned by other agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Results</head><p>We submitted four runs to the expert task. Three of our runs are automatic, labeled IBM06QO, IBM06PR, and IBM06EXP. All three runs used all of the four agents described above, with different query processing and support document selection strategies. The run labeled IBM06MA was partially manual. The key characteristics of our automatic runs are described below:</p><p>• IBM06QO: This run used only the title field of the topic. FOLDOC was used for query expansion.</p><p>• IBM06PR: This run used both the title and description fields of the topic in query analysis Select agent parameters were tuned to target higher precision. It also included two additional components: the EKDB document re-ranker and the acknowledgments filter. • IBM06EXP: This run also used both the title and description fields of the topic. It included two additional components on top of the IBM06QO configuration: the affinity-based expert re-ranker and the duplicate document remover. Table <ref type="table" coords="8,98.20,595.98,5.01,9.02" target="#tab_3">2</ref> shows the results of our submitted runs. For each performance measure reported, the score under column heading "exp" represents score when considering only expert correctness, while that under "sup" takes into account whether or not a correct supporting document was found. For all measures, the manual run (IBM06MA) received the highest score. The highest scoring of the three automatic runs for each measure is boldfaced. It is worth noting that our two best scoring runs, IBM06MA and IBM06QO, were our priorities 3 and 4 runs, and therefore were not included in the results pooled for judging.</p><p>Our results show that IBM06QO achieved the highest automatic score in almost all cases except for precision@10. The IBM06PR run, which targeted higher precision, did not outperform either of the two systems, even using the P@5 and P@10 measures. We are currently running finer grained experiments to determine the contribution (or lack thereof) of the individual components included in this run. The contributions of components in the IBM06EXP run are easier to determine. The affinity-based expert re-ranker increased precision@10 but fared worse in all other measures. The result for the duplicate document remover is somewhat counter-intuitive. In theory, with the binary supporting document metric adopted, the greater diversity in supporting documents produced by the duplicate document remover should result in an increase performance. However, this does not appear to be the case when comparing the scores for IBM06EXP and IBM06QO. In particular, while there is an increase in expert only score in P@10, the scores with supporting documents are identical for P@10 for the two runs. This is probably due to incomplete judgment for supporting documents.</p><p>The goals of our manual run were twofold. First, we wanted to evaluate the potential of incorporating semantic search capabilities into an automatic expert finding system. Since automatic semantic search query construction from descriptions and narratives is a difficult task, our manual run consists of having an expert manually construct queries that leverage our semantic annotations to retrieve more relevant documents. Second, we were interested in evaluating the effectiveness of SAW II, an interactive system that supports semantic query construction, refinement, and results exploration, for this problem-solving task. Figure <ref type="figure" coords="9,327.52,235.32,5.01,9.02">3</ref> shows a screenshot of SAW II, illustrating query construction and refinement support (top left panel), result browsing support (top right panel), and detailed document drill down support (bottom panel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3 SAW II Screenshot</head><p>The query shown in Figure <ref type="figure" coords="9,187.82,663.54,5.01,9.02">3</ref> demonstrates how we leveraged our semantic annotations to answer topic EX53 on W3C translation policy. By including the Nation entity type, which matches both the nominal and adjectival forms of country names, in the query, the returned documents, which must have included a country or language, were much more on topic. In our submitted manual run, we targeted around 5 manually identified experts for each topic. The rest of the hit list was augmented with results from the IBM06PR run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>This paper described our efforts in this year's Enterprise Track discussion and expert tasks. Our discussion task results show that our conservative query expansion strategy focusing on FOLDOC-identified domain-specific terms improves performance slightly, while combining the results from multiple search engines substantially outperforms single search engine results. Finally, our NLP based pro/con assessment components again results in substantial performance improvement. On the expert task, our current run results are somewhat less conclusive. We are currently in the process of running further experiments and ablation studies to determine the contributions of individual agents and components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,72.00,280.14,471.12,346.08"><head></head><label></label><figDesc></figDesc><graphic coords="9,72.00,280.14,471.12,346.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,170.16,190.09,271.72,225.90"><head>Figure 1 Architecture for Pro/Con Discussion Retrieval System</head><label></label><figDesc></figDesc><table coords="2,196.32,190.09,205.64,188.75"><row><cell></cell><cell>Query</cell><cell></cell></row><row><cell></cell><cell>Analysis</cell><cell></cell></row><row><cell>Document Retrieval</cell><cell></cell><cell></cell></row><row><cell>Juru Query</cell><cell>Lucene Query</cell><cell>Indri Query</cell></row><row><cell>Generator</cell><cell>Generator</cell><cell>Generator</cell></row><row><cell>Juru</cell><cell>Lucene</cell><cell>Indri</cell></row><row><cell>Search</cell><cell>Search</cell><cell>Search</cell></row><row><cell cols="2">Hit List Combiner</cell><cell></cell></row><row><cell cols="2">Thread-Based Hit List Augmenter</cell><cell></cell></row><row><cell>Pro/Con Re-ranking</cell><cell></cell><cell></cell></row><row><cell>IBM</cell><cell>UPitt</cell><cell></cell></row><row><cell>Pro/Con</cell><cell>Pro/Con</cell><cell></cell></row><row><cell>Assessor</cell><cell>Assessor</cell><cell></cell></row><row><cell></cell><cell>Combiner</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,70.92,189.48,468.77,116.84"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="5,70.92,189.48,468.77,116.84"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Discussion Task Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run Tag</cell><cell>MAP</cell><cell></cell><cell>R-prec</cell><cell></cell><cell>bpref</cell><cell></cell><cell>P@10</cell><cell></cell><cell cols="2">P@100</cell></row><row><cell></cell><cell>topic</cell><cell>p/c</cell><cell>topic</cell><cell>p/c</cell><cell>topic</cell><cell>p/c</cell><cell>topic</cell><cell>p/c</cell><cell>topic</cell><cell>p/c</cell></row><row><cell>IBM06JAQ</cell><cell cols="10">0.3146 0.2030 0.3527 0.2481 0.3572 0.2337 0.5440 0.3391 0.3998 0.2487</cell></row><row><cell>IBM06JAQD</cell><cell cols="10">0.3069 0.1967 0.3493 0.2485 0.3499 0.2336 0.5400 0.3283 0.4054 0.2467</cell></row><row><cell>IBM06JILAQD</cell><cell cols="10">0.3305 0.2004 0.3709 0.2509 0.3700 0.2294 0.5600 0.3326 0.4184 0.2493</cell></row><row><cell cols="11">IBM06JILAPQD 0.3310 0.2021 0.3717 0.2558 0.3709 0.2323 0.5640 0.3391 0.4186 0.2515</cell></row><row><cell>JQ</cell><cell cols="10">0.2745 0.1654 0.3257 0.2209 0.3218 0.2082 0.4950 0.2800 0.3590 0.2038</cell></row><row><cell>JILQ</cell><cell cols="10">0.3017 0.1762 0.3524 0.2288 0.3472 0.2083 0.5360 0.2978 0.3844 0.2250</cell></row><row><cell>JILQD</cell><cell cols="10">0.3095 0.1835 0.3621 0.2422 0.3559 0.2174 0.5360 0.3065 0.3998 0.2359</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,74.46,484.08,460.29,95.24"><head>Table 2 Expert Task Results</head><label>2</label><figDesc></figDesc><table coords="8,74.46,501.96,460.29,77.36"><row><cell>Run Tag</cell><cell>MAP</cell><cell></cell><cell cols="2">R-prec</cell><cell>bpref</cell><cell></cell><cell>P@5</cell><cell></cell><cell>P@10</cell></row><row><cell></cell><cell>exp</cell><cell>sup</cell><cell>exp</cell><cell>sup</cell><cell>exp</cell><cell>sup</cell><cell>exp</cell><cell>sup</cell><cell>exp</cell><cell>sup</cell></row><row><cell>IBM06QO</cell><cell cols="10">0.4536 0.2863 0.4519 0.3303 0.4402 0.3711 0.6653 0.4857 0.5408 0.4041</cell></row><row><cell>IBM06PR</cell><cell cols="10">0.4455 0.2789 0.4442 0.3211 0.4306 0.3522 0.6531 0.4776 0.5265 0.3939</cell></row><row><cell>IBM06EXP</cell><cell cols="10">0.4357 0.2730 0.4434 0.3201 0.4338 0.3465 0.6531 0.4816 0.5469 0.4041</cell></row><row><cell>IBM06MA</cell><cell cols="10">0.5235 0.3346 0.5192 0.3829 0.5180 0.4135 0.7673 0.5878 0.6449 0.4878</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,77.76,710.82,326.51,9.02"><p>http://www.foldoc.org. Content downloadable from http://foldoc.org/source.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,77.76,641.82,414.51,9.02"><p>Available as part of the open source UIMA framework, http://sourceforge.net/projects/uima-framework</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,77.76,653.34,96.01,9.02"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,77.76,664.86,139.40,9.02"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,77.70,676.32,432.87,9.02;3,72.00,687.84,191.41,9.02"><p>We did not make use of this semantic search feature of Juru in our TREC system, but we plan to exploit this functionality for discussion search in the future.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,77.76,699.36,438.87,9.02;3,72.00,710.82,312.99,9.02"><p>It is, of course, possible to implement some pseudo-relevance feedback mechanism for use with the other two engines. However, because of time constraints, we did not explore this option.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,79.02,710.40,261.63,9.02"><p>Available for download at http://www.summarization.com/mead/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="7,77.70,699.36,451.84,9.02"><p>We were unable to generate some of the indices for larger pseudo-document sets because of memory constraints.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="7,77.76,710.82,102.28,9.02"><p>http://scholar.google.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgments</head><p>We would like to thank <rs type="person">Brad Andrews</rs>, <rs type="person">Eric Brown</rs>, <rs type="person">David Ferrucci</rs>, <rs type="person">Sarah Luger</rs>, <rs type="person">Rajen Subba</rs>, and <rs type="person">Mikalai Yatskavich</rs> for helpful discussions and/or for their help in the development of a heldout test set for the expert task. This work was supposed in part by the <rs type="funder">Disruptive Technology Office (DTO)</rs>'s <rs type="programName">Advanced Question Answering for Intelligence (AQUAINT) program</rs> under contract number <rs type="grantNumber">H98230-04-C-1577</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5dT6Y2P">
					<idno type="grant-number">H98230-04-C-1577</idno>
					<orgName type="program" subtype="full">Advanced Question Answering for Intelligence (AQUAINT) program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,78.67,268.70,101.14,14.82" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,289.02,467.16,9.02;10,72.00,300.54,221.63,9.02;10,293.64,298.45,5.04,5.83;10,301.14,300.54,134.12,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,463.27,289.02,75.89,9.02;10,72.00,300.54,127.43,9.02">Juru at TREC-10 -Experiments with index pruning</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Einat</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miki</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoelle</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yael</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,206.16,300.54,87.47,9.02;10,293.64,298.45,5.04,5.83;10,301.14,300.54,104.71,9.02">Proceedings of the 10 th Text REtrieval Conference</title>
		<meeting>the 10 th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,323.52,467.44,9.02;10,72.00,335.04,10.05,9.02;10,82.08,332.95,5.04,5.83;10,89.58,335.04,134.11,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,134.55,323.52,320.61,9.02">Arjen de Vries, and Ian Soboroff. Overview of the TREC-2005 Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,464.40,323.52,75.04,9.02;10,72.00,335.04,10.05,9.02;10,82.08,332.95,5.04,5.83;10,89.58,335.04,104.64,9.02">Proceedings of the 14 th Text REtrieval Conference</title>
		<meeting>the 14 th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,358.02,448.27,9.02;10,72.00,369.54,244.53,9.02" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,321.63,358.02,198.64,9.02;10,72.00,369.54,80.09,9.02">Overview of Component Services for Knowledge Integration in UIMA</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">William</forename><surname>David Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welty</surname></persName>
		</author>
		<idno>RC24074</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">IBM Research Report</note>
</biblStruct>

<biblStruct coords="10,72.00,392.52,458.65,9.02;10,72.00,404.04,87.52,9.02;10,159.54,401.95,5.04,5.83;10,167.04,404.04,134.17,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,365.07,392.52,161.28,9.02">THUIR at TREC 2005: Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">Yupeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yize</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,72.00,404.04,87.52,9.02;10,159.54,401.95,5.04,5.83;10,167.04,404.04,104.76,9.02">Proceedings of the 14 th Text REtrieval Conference</title>
		<meeting>the 14 th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,427.02,334.03,9.02" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,217.56,427.02,66.21,9.02">Lucene in Action</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Manning Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,450.00,458.99,9.02;10,72.00,461.52,397.81,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,157.30,450.00,368.72,9.02">Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mccord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,82.85,461.52,259.52,9.02">Natural Language and Logic: International Scientific Symposium</title>
		<imprint>
			<publisher>Springer Verlag LNCS</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,484.50,465.56,9.02;10,72.00,496.02,419.02,9.02;10,72.00,507.48,393.88,9.02;10,72.00,530.52,435.91,9.02;10,72.00,541.98,257.02,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,136.42,496.02,350.83,9.02;10,72.00,530.52,359.86,9.02">Ellen Riloff and William Phillips. An Introduction to the Sundance and AutoSlog Systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joracio</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wai</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jong</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arda</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danyu</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elliott</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Drabek</surname></persName>
		</author>
		<idno>UUCS-04-015</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,72.00,507.48,364.18,9.02">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>School of Computing, University of Utah</publisher>
			<date type="published" when="2003">2003. 2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Evaluation challenges in large-scale multi-document summarization: the MEAD project</note>
</biblStruct>

<biblStruct coords="10,72.00,565.02,457.18,9.02;10,72.00,576.48,355.41,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,298.47,565.02,164.08,9.02">Feature subsumption for opinion analysis</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,468.90,565.02,60.28,9.02;10,72.00,576.48,325.14,9.02">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,599.52,439.91,9.02;10,72.00,610.98,98.67,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,228.66,599.52,241.89,9.02">BoosTexter: A boosting-based system for text categorization</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,476.94,599.52,34.97,9.02;10,72.00,610.98,34.87,9.02">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,634.02,444.15,9.02;10,72.00,645.48,251.76,9.02" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,283.73,634.02,232.42,9.02;10,72.00,645.48,27.49,9.02">Indri: A language-model based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno>IR-407</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">UMass Amherst CIIR Technical Report</note>
</biblStruct>

<biblStruct coords="10,72.00,668.52,460.39,9.02" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<ptr target="http://www.cs.mu.oz.au/~wew/w" />
		<title level="m" coord="10,143.97,668.52,113.81,9.02">Thread structure of w3c lists</title>
		<imprint>
			<date type="published" when="2005">3c-lists-threads-w3w.tar.gz. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
