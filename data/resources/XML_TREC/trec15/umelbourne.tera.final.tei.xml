<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.48,85.76,340.50,23.74">Melbourne University at the 2006 Terabyte Track</title>
				<funder>
					<orgName type="full">ARC Centre for Perceptive and Intelligent Machines in Complex Environments</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.00,121.77,41.94,16.49"><forename type="first">Vo</forename><surname>Ngoc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.94,121.77,20.61,16.49;1,269.43,121.77,78.41,16.49"><forename type="first">Anh</forename><forename type="middle">William</forename><surname>Webber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.84,121.77,71.58,16.49"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.48,85.76,340.50,23.74">Melbourne University at the 2006 Terabyte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92351F2D1BD02B430C3652A679F14E37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done at The University of Melbourne for the TREC-2006 Terabyte Track.  For this track, we participated in all three main tasks. We continued our work with impact-based ranking and sought to reduce indexing as well as query time. However, to support the named-page task, more conventional retrieval mechanisms were also employed. The results show that, in general, the efficiency performance is slightly better than the previous year. The effectiveness level remains the same.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In TREC 2006, The University of Melbourne participated in three tasks of the Terabyte Track: ad-hoc, efficiency, and named-page. For the first two tasks impact-based ranking was employed. For the named-page task, BM25 was also used. All the experiments were performed using our locally-developed software. The system was developed last year and enhanced further with new index compression schemes and index structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ranking Schemes</head><p>This year, we continue to employ an impact-based ranking technique <ref type="bibr" coords="1,405.94,456.04,105.15,15.04" target="#b1">[Anh and Moffat, 2005]</ref> as the main retrieval mechanism. In the framework of the track, the technique turned out to provide an excellent combination of retrieval efficiency and retrieval effectiveness. However, for the namedpage task, we also applied the BM25 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Impact-Based Ranking</head><p>This sub-section expands the description of impact-based ranking described in our 2005 TREC paper. Impact ranking specifies a method to map each pair (t,x), where t is a term in text x, to an integer impact value that represents the importance of t in x. When x is a document d from a document collection, the resultant impact ω t,d is referred to as the document term impact of t in d. When x is a query q, the value ω t,q is the query term impact of t in q. Document and query term impacts normally correlate with the term frequencies in documents or queries, but might or might not also depend on some collection-wide statistics such as collection frequency f t .</p><p>The mapping from each term-document pair (t,d) to the corresponding impact ω d,t is done locally within each document during the indexing phase in the following manner. First, frequencies of terms appearing in d are counted. Then, the list of within-document terms is sorted in decreasing order of frequencies. Next, the list is then partitioned in k intervals, with interval lengths forming a geometric sequence. Finally, all terms in the i-th interval (1 ≤ i ≤ k) are assigned the same integral impact value of ki + 1. Since document term impacts do not rely on collection-wide statistics (in particular, they are independent of IDF), the indexing can be done with only one pass through the document collection.</p><p>In contrast to conventional information retrieval systems, where raw term statistics are stored in indexes, and most of the similarity calculation is carried out during query processing, impactbased ranking allows computation of all of the document term impacts while the index is being constructed. The document term impacts are then stored in the index, reducing the computational burden during query processing. Normally, the indexes are impact-sorted.</p><p>The mapping from each term-query pair (t,q) to the corresponding impact ω q,t is done at query time, using within-query frequencies and collection frequencies. First, for each term t ∈ q, the collection frequency f t and within-query frequency f q,t are determined. Then, a vector-space formulation of within-query term weight is applied to generate the term weight w q,t (in our experiences, any vector-space formulation that involves IDF factor can be used for this purpose). Finally, the value of w q,t is transformed in a uniform manner, and truncated to an integral impact ω q,t , so that the maximal value of ω q,t for all terms in the query is exactly k.</p><p>Once the document and query term impacts have been determined, the ranking of documents d against a query q is done based on the similarity score:</p><formula xml:id="formula_0" coords="2,249.60,279.72,104.51,23.05">S d,q = t∈d∩q ω d,t • ω q,t .</formula><p>(1)</p><p>Note that although the indexing does not depend on any collection-wide statistics, the IDF factor is still used in ranking as an integrated part of the query impact. Also, the above scoring computation is performed based only on the integral document and query term impacts. No floating-point operations are required, and no document weights are used or even computed. This makes the ranking process fast. <ref type="bibr" coords="2,199.23,363.40,105.40,15.04" target="#b1">Anh and Moffat [2005]</ref> also describe a ranking technique which is also employed in our runs to further enhance the querying speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BM25</head><p>For the named-page task, in addition to each original document, we also employ the incoming anchor text to it. This year, we simply add all incoming texts found in the collection to the respective document, creating a new document collection. This collection is then used as the input to the indexing process. We noticed that after adding anchor text to the destinated documents, there might be a great change in within-document term frequency distribution. For example, a small number of terms might be boosted to a very high frequency level. That, in turn, might affect the above described process of assigning document impacts, and perhaps in a negative manner. To test this hypothesis, we also employ BM25 <ref type="bibr" coords="2,200.70,535.84,100.95,15.04" target="#b3">[Robertson et al., 1994]</ref>. The exact formulation used is:</p><formula xml:id="formula_1" coords="2,159.72,570.72,364.52,30.61">S(d, q) = t∈d∩q ln N -f t + 0.5 f t + 0.5 • f d,t 0.5 + 1.5 • W d /W avg d + f d,t ,<label>(2)</label></formula><p>where W d is the size of d, measured in bytes, and</p><formula xml:id="formula_2" coords="2,333.37,607.23,23.49,19.30">W avg d</formula><p>is the average value of W d over the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Index Structures</head><p>For a document collection, the principal component of its index is the inverted file, where each distinct term of the collection is associated with an inverted list. For the 2006 Terabyte Track, a number of different inverted list structures were employed: impact-sorted: The impact-sorted inverted list for a term t is a list of equal-impact blocks. Each block represents one distinct impact value k, and contains the sequence of document numbers in which t appears and has an impact score of k. Inside a block, document numbers are arranged in increasing order, to facilitate compression. The blocks are arranged in decreasing order of associated impacts, so as to support effective pruning.</p><p>block-interleaved: This is a common name for a family of document-sorted indexes described by <ref type="bibr" coords="3,140.62,139.12,106.41,15.04">Anh and Moffat [2006b]</ref>. In terms of index content there are three variants of indexes, depending on the components used to represent each pair (d, t). Compression is applied to inverted files. In all of our Terabyte Track experiments the wordsynchronized compression scheme slide <ref type="bibr" coords="3,276.83,286.60,109.10,15.04">[Anh and Moffat, 2006a]</ref> was used for inverted list compression. This method provides a good balance between index space and decoding speed. In comparison with the compression scheme employed last year, slide in general has better compression effectiveness and roughly the same decoding speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hardware Configurations and Efficiency Metrics</head><p>As in 2005, two hardware configurations were used in our experiments this year:</p><p>Single: A machine with a single 2.8 GHz Intel Pentium-4 processor, 1 GB of memory and 250 GB of local SATA disk, running Debian GNU/Linux.</p><p>Cluster: A Beowulf-style cluster, consisting of a server and eight additional nodes, where each node is a Single. The server is a dual 2.8 GHz Intel Xeon with 2 GB of memory, again running Debian GNU/Linux. In this configuration, the document data are uniformly distributed across the nodes in a cyclic manner. Indexing and querying are done in parallel, without communication between the nodes, in an autonomous document-distributed manner. The server only plays the role of a broker: it receives queries, broadcasts queries to the nodes, receives the answer lists from the nodes, and selects the final document output based on the local similarity scores computed by the nodes.</p><p>The efficiency metrics reported in our experiments include Index Time (elapsed time for indexing, not including any time needed to distribute documents to the nodes, and, for the case of using incoming anchor text, not including the time to locate and link the anchor text to the destinated document); Index Size (total size of the index, including vocabulary and inverted files); and Query Time (average elapsed time to process a query). Over a sequence of queries, the total of Query Time was measured from the moment when the first query arrived, until the last query had been processed, not including the initialization costs associated with loading a range of memory-resident files. Note that in all but one run, queries are processed sequentially, with a single query being active in the system at any given time. There is one exceptional case in the efficiency task, where multi-threading (with 4 threads) was used so that CPU idle periods arising from disk operations in one process could be exploited by another process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ad-hoc Performance</head><p>Four runs were submitted for the ad-hoc task, including one manual and three automatic run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Runs</head><p>In doing the automatic runs this year, we targetted the tie-breaking problem in impact ranking.</p><p>Because the maximal value of impact, k = 8, is relatively small, the number of documents that have equal similarity score with respect to a query is potentially high. This year we experimented with using proximity for breaking ties. That is, the original impact score is further adjusted by a score calculated from the proximity of query terms in the document. The three submitted runs are:</p><p>AdhocBase (MU06TBa2). This is the baseline, performed with a standard impact-sorted index, using the Local-By-Rank-(TF) computation as described earlier. For this run, all the normal content of documents are indexed. No special treatment was applied to any fields (meta, title, heading, and so on). Incoming anchor text is not considered.</p><p>AdhocBase+Prox (MU06TBa5). This run is similar to the baseline, except that positions of terms in documents are taken into account. The index is document-sorted rather than impactsorted (that is, it is an impact-positional index). Impact scoring is also applied, but the total score for a document is enhanced by another integer score which is calculated based on the inverse of the least distance in words between the positions of any of the query terms in the document.</p><p>AdhocBase,Prox (MU06TBa6). This run is the same as Base+Prox, except that the proximity score is employed just to break the tie of the main impact score. That is, the final ranking is performed by sorting the documents using impact score as the primary sort key, and proximity score as a secondary sort key.</p><p>Table <ref type="table" coords="4,140.70,618.76,5.39,15.04" target="#tab_1">1</ref> shows the effectiveness performance of these ad-hoc runs. In terms of effectiveness, all of the runs had similar performance. It suggests that the treatment by proximity distance fails to give the desired effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Manual Runs</head><p>AdhocBase+Prox+Manual (MU05TBa1). This run is almost identical to the AdhocBase+Prox, except that manual queries are used instead of automatic. Manual queries are developed by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunID</head><p>Topics 801-850 MAP R-prec R.Rank P@10 P@20 AdhocBase+Prox+Manual 0.2927 0.3186 0.8301 0.6160 0.5420 AdhocBase+Prox 0.2888 0.3324 0.7150 0.5300 0.4860 us based on our perception of the topics. In many cases, a simple process of adding some related terms to the query is applied. Consequently, manual queries are in general longer than respective automatic queries.</p><p>Table <ref type="table" coords="5,142.15,377.20,5.39,15.04" target="#tab_2">2</ref> compares the effectiveness of using manual queries with that of using automatic queries. It is interesting to notice that although manual run does not improve MAP, it significantly increases R.Rank, P@10, and P@20.</p><p>The main problem of manual queries is their inconsistent quality. There are some cases when manual query gives worse effectiveness relative to the respective automatic queries. Naturally there are some cases when manual queries do very well. The best two manual queries of our group, that gave best performance amongst all Terabyte runs this year, are for topics 816 and 849 that have MAP scores of 0.9032 and 0.5459 respectively. The original topics are "816. USAID assistance to Galapagos" and "849. Scalable Vector Graphics", while the manual versions are "816. USAID galapagos islands biodiversity activities" and "849. scalable vector graphics svg portability zoom".</p><p>Efficiency performance of all ad-hoc run is presented in Table <ref type="table" coords="5,382.35,526.24,4.07,15.04" target="#tab_3">3</ref>. As can be seen from the table, the last three runs, that employed I DIFP indexes, are much slower than the first one, which used impact-sorted indexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Efficiency Performance</head><p>In addition to the ad-hoc runs, the following four runs were submitted for the efficiency task: SpeedBase (MU06TBy1). This is the baseline run for the efficiency task. It employed basically the standard impact-based ranking process, with a slight modification intended to improve R.Rank. Unfortunately, it is not as good as AdhocBase in terms of P@10 and P@20.</p><p>SpeedBase+Smoothing (MU06TBy2). This run had the same initial setting as the SpeedBase, but with a smoothing process designed to reduce the relative gaps between lower contribution to the similarity score. SpeedBase+Prune (MU06TBy5). This run had SpeedBase as a starting point, but used a dynamic index pruning technique, under which the low-impact blocks were excluded from the index during indexing time.</p><p>SpeedBase+Prune+Multithread (MU06TBy6). This run has the same setting as SpeedBase+Prune, but queries are processed with four threads as required by the task.</p><p>Efficiency performance is described in Table <ref type="table" coords="6,314.94,532.36,4.07,15.04" target="#tab_4">4</ref>. Using a single commodity machine, we can index the GOV2 collection in 8.2 hours, at a rate of approximately 70 GB per hour, and can process queries at the rate of approximately five per second. Moreover, the index is small -just 1.4% of the size of the original document collection.</p><p>Table <ref type="table" coords="6,140.35,586.60,5.39,15.04" target="#tab_4">4</ref> shows the significant improve in query speed of the pruning strategy (with no loss on effectiveness, as shown in Table <ref type="table" coords="6,242.33,600.16,3.91,15.04" target="#tab_5">5</ref>). However, it also shows that the multi-threading run is not up to the desired level. We expect that the situation can be improved with more programming effort.</p><p>Effectiveness performance is shown in Table <ref type="table" coords="6,313.99,627.28,4.07,15.04" target="#tab_5">5</ref>. As mentioned earlier, the baseline run is not in line with that of the adhoc runs, and the pruned run is even slightly better than the full run. In addition, the smoothing process does help to improve P@20, but only slightly. 7 Performance in the Named-Page Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunID</head><p>As described earlier, the document collection used for the named-page (NP) task is not the original one. In the new collection, all incoming anchor text is added to the destinated documents. Given that, the time for indexing can be greatly depends on the time to modify the collection. We were not able to measure this additional time, and the indexing time officially reported excludes the time for modifying the collection. This indexing time is not reported here. Different index structures are employed, as described in the list of runs belows. All runs were performed in the Single hardware.</p><p>NP,Impact (MU06TBn2). This is the baseline run, using an I DI index. The scoring mechanism is the standard impact-based one.</p><p>NP,BM25 (MU06TBn5). This run employs an I DF index and BM25 scoring mechanism. The motivation behind this run is that the original impact-based ranking might be badly affected by the modification of term frequencies using incoming impact. Hence, NP,BM25 represents a second baseline.</p><p>NP,IVSM (MU06TBn6). This run employ an I DI index, and a modified vector-space scoring mechanism over impact. That is, the impact score is now modified by document length, which in turn is calculated based on document impacts (ie. not based on within-document frequencies as usually).</p><p>NP,Impact+BM25 (MU06TBn9). This run employs I DIF , and is operationally equivalent to merging the results of NP,Impact and NP,BM25. Let s 1 and s 2 be the scores of a document in two different ranking systems, the merged score s is defined as:</p><formula xml:id="formula_3" coords="7,247.68,534.12,155.50,28.17">s = 1 -(1 - s 1 s max 1 ) × (1 - s 2 s max 2 ) ,</formula><p>where s max i is the maximal score for the considered query, using retrieval scheme i.</p><p>Table <ref type="table" coords="7,140.47,582.88,5.39,15.04" target="#tab_6">6</ref> describes the performance of the named-page runs. The table shows that the original impacts are in-competitive for this approach of modifying document collection. The NP,IVSM is better than its baseline, but is still not as good as the NP,BM25 baseloine. Finally, the merged approach seems to have a positive affect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Directions</head><p>This year, we applied a number of changes to our system, mainly to allow different kinds of indexes and retrieval approaches. In general, the efficiency performance is slightly better than the previous year. The effectiveness level remains the same.</p><p>Note that we failed to perform the additional task, where the requirement is to employ a publicly available system to compare the efficiency. The reason is that the indexing using the public software was unable to operate on the hardware configuration that was used for the efficiency task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,126.48,152.68,397.43,109.96"><head></head><label></label><figDesc>Namely, the pair is represented by(d, ω d,t  ) in I DI indexes, by (d, f d,t ) in I DF indexes, by (d, ω d,t , f d,t ) in I DIF indexes, and by (d, ω d,t , f d,t , p * ) in I DIFP indexes. Note that the last variant is used for fully positional indexes, with p * representing the list of positions of t within d. In terms of structure, each inverted list is arranged in the block-interleaved manner. For example, an inverted list of the I DIF is a sequence of index blocks, where each index block is a sequence of three fixed-length blocks, one for each of d, ω d,t , and f d,t . In each block, the elements are sorted in increasing order of document numbers, and no equal-impact blocking is performed.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,99.24,47.80,423.30,175.00"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness performance in the automatic ad-hoc task. All results relate to the GOV2 collection.</figDesc><table coords="4,159.36,47.80,304.25,152.20"><row><cell>RunID AdhocBase AdhocBase+Prox AdhocBase,Prox</cell><cell>Topics 801-850 MAP R-prec R.Rank P@10 P@20 0.3039 0.3479 0.6969 0.5460 0.5130 0.2888 0.3324 0.7150 0.5300 0.4860 0.2926 0.3414 0.7067 0.5300 0.4880</cell></row><row><cell>RunID AdhocBase AdhocBase+Prox AdhocBase,Prox</cell><cell>Topics 701-850 MAP R-prec R.Rank P@10 P@20 0.3073 0.3491 0.7694 0.5765 0.5426 0.3025 0.3422 0.7693 0.5711 0.5191 0.3007 0.3448 0.7492 0.5651 0.5211</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,99.24,111.76,424.65,154.84"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness performance of the manual run in comparison with the similar automatic run. All results relate to the GOV2 collection.</figDesc><table coords="5,153.48,156.40,316.40,110.20"><row><cell>RunID AdhocBase AdhocBase+Prox AdhocBase,Prox AdhocBase+Prox+Manual Cluster Hardware Single Cluster Cluster</cell><cell>Efficiency Index time (GB) (minutes) (secs) Index Query size time 6.04 334 0.16 38.60 57 0.21 38.60 57 0.20 38.60 57 0.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,99.24,274.48,424.62,27.39"><head>Table 3 :</head><label>3</label><figDesc>Efficiency performance of all ad-hoc run. The fist run employed impact-sorted index, while all other runs used an I DIFP index.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,99.24,47.80,424.63,343.24"><head>Table 4 :</head><label>4</label><figDesc>Efficiency performance of efficiency runs. All runs employed impact-sorted indexes. All results relate to the GOV2 collection.</figDesc><table coords="6,122.16,47.80,378.65,343.24"><row><cell cols="2">RunID SpeedBase SpeedBase+Smooth SpeedBase+Prune SpeedBase+Prune+Multithread Single Hardware Single Single Single</cell><cell>Efficiency Index time (GB) (minutes) (secs) Index Query size time 6.04 334 0.19 6.04 334 0.23 6.04 334 0.08 6.04 334 0.05</cell></row><row><cell>RunID SpeedBase SpeedBase+Smooth SpeedBase+Prune SpeedBase+Prune+Multithread</cell><cell cols="2">Topics 801-850 MAP R-prec R.Rank P@10 P@20 0.0793 0.1124 0.7102 0.5440 0.4820 0.0838 0.1190 0.6998 0.5540 0.5050 0.0821 0.1166 0.7059 0.5440 0.4890 0.0821 0.1166 0.7059 0.5440 0.4890</cell></row><row><cell>RunID SpeedBase SpeedBase+Smooth SpeedBase+Prune SpeedBase+Prune+Multithread</cell><cell cols="2">Topics 751-800 MAP R-prec R.Rank P@10 P@20 0.0633 0.0758 0.8169 0.6240 0.5520 0.0668 0.0805 0.8162 0.6180 0.5770 0.0641 0.0772 0.8312 0.6220 0.5590 0.0641 0.0772 0.8312 0.6220 0.5590</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,121.20,398.80,377.76,15.04"><head>Table 5 :</head><label>5</label><figDesc>Effectiveness performance of efficiency runs. All results relate to the GOV2 collection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,99.24,47.80,424.84,130.66"><head>Table 6 :</head><label>6</label><figDesc>Performance of named-page runs. All results relate to the modified GOV2 collection, where incoming text is added to destonated documents.</figDesc><table coords="7,143.40,47.80,336.33,96.76"><row><cell>Query time (secs) 0.20 0.27 0.26 NP,Impact+BM25 0.49 NP,Impact NP,BM25 NP,IVSM</cell><cell>R.Rank 0.317 0.397 0.329 0.396</cell><cell>Effectiveness Found Found Found at 1 at 5 at 10 Found Not 23.2% 39.8% 47.0% 14.4% 28.7% 52.5% 62.4% 13.8% 23.2% 43.6% 50.8% 14.9% 29.8% 54.1% 58.6% 13.3%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work was supported by the <rs type="funder">Australian Research Council</rs> and the <rs type="funder">ARC Centre for Perceptive and Intelligent Machines in Complex Environments</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,99.24,178.96,391.40,15.04;8,500.00,183.49,23.72,9.82;8,110.16,197.05,223.70,9.82;8,336.98,192.52,99.26,15.04" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,212.57,178.96,273.58,15.04">Improved word-aligned binary compression for text indexing</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,500.00,183.49,23.72,9.82;8,110.16,197.05,219.02,9.82">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="857" to="861" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.24,215.08,423.97,15.04;8,110.16,228.64,265.24,15.04;8,379.96,233.17,142.97,9.82;8,110.16,246.73,359.29,9.82;8,473.41,242.20,49.94,15.04;8,110.16,255.76,260.96,15.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,223.59,215.08,203.88,15.04">Simplified similarity scoring using term ranks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,379.96,233.17,142.97,9.82;8,110.16,246.73,355.07,9.82">Proc. 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ziviani</surname></persName>
		</editor>
		<meeting>28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Salvador, Brazil; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.24,278.20,424.18,15.04;8,110.16,291.76,232.90,15.04;8,347.14,296.29,176.03,9.82;8,110.16,309.85,118.06,9.82;8,232.42,305.32,291.03,15.04;8,110.16,318.88,39.38,15.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,220.95,278.20,282.08,15.04">Structured index organizations for high-throughput ext querying</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,347.14,296.29,176.03,9.82;8,110.16,309.85,113.83,9.82">Proc. 13th Int. Symp. String Processing and Information Retrieval</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<meeting>13th Int. Symp. String essing and Information Retrieval<address><addrLine>Glasgow, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-10">October 2006</date>
			<biblScope unit="volume">4209</biblScope>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.24,341.44,424.20,15.04;8,110.16,355.00,80.83,15.04;8,193.75,359.53,216.14,9.82;8,412.41,355.00,110.88,15.04;8,110.16,368.56,413.00,15.04;8,110.16,382.00,68.39,15.04;8,181.19,386.41,323.62,10.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,428.09,341.44,75.99,15.04">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://potomac.ncsl.nist.gov:80/TREC/t3_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="8,193.75,359.53,210.86,9.82">Proc. Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>Third Text REtrieval Conference (TREC-3)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 500-225</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
