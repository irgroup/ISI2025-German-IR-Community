<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.58,102.09,364.85,15.12">The Hedge Algorithm for Metasearch at TREC 2006</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-02-07">February 7, 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.01,134.57,81.19,10.48;1,275.19,132.95,1.41,6.99"><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.85,134.57,61.94,10.48"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,364.44,134.57,53.56,10.48"><forename type="first">Carlos</forename><surname>Rei</surname></persName>
							<email>crei@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.58,102.09,364.85,15.12">The Hedge Algorithm for Metasearch at TREC 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-02-07">February 7, 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">91A396B3A4738EEF67E50E77EF3A457B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> Aslam, Pavlu, and Savell [3]  <p>introduced the Hedge algorithm for metasearch which effectively combines the ranked lists of documents returned by multiple retrieval systems in response to a given query and learns which documents are likely to be relevant from a sequence of on-line relevance judgments. It has been demonstrated that the Hedge algorithm is an effective technique for metasearch, often significantly exceeding the performance of standard metasearch and IR techniques over small TREC collections. In this work, we explore the effectiveness of Hedge over the much larger Terabyte 2006 collection. * We gratefully acknowledge the support provided by NSF grants CCF-0418390 and IIS-0534482.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aslam, Pavlu, and Savell introduced a unified framework for simultaneously solving the problems of metasearch, pooling, and system evaluation based on the Hedge algorithm for on-line learning <ref type="bibr" coords="1,255.47,486.60,9.97,8.74" target="#b2">[3]</ref>. Given the ranked lists of documents returned by a collection of IR systems in response to a given query, Hedge is capable of matching and often exceeding the performance of the best underlying retrieval system; given relevance feedback, Hedge is capable of "learning" how to optimally combine the input systems, yielding a level of performance which often significantly exceeds that of the best underlying system.</p><p>In previous experiments with smaller TREC collections <ref type="bibr" coords="1,96.24,606.62,9.96,8.74" target="#b2">[3]</ref>, it has been shown that after only a handful of judged feedback documents, Hedge is able to significantly outperform the CombMNZ and Condorcet metasearch techniques. It has also been shown that Hedge is able to efficiently construct pools contain-ing significant numbers of relevant documents and that these pools are highly effective at evaluating the underlying systems <ref type="bibr" coords="1,416.42,262.32,9.97,8.74" target="#b2">[3]</ref>. Although the Hedge algorithm has been shown to be a strong technique for metasearch, pooling, and system evaluation using the relatively small or moderate TREC collections ( <ref type="bibr" coords="1,317.03,310.14,41.39,8.74">TRECs 3,</ref><ref type="bibr" coords="1,361.82,310.14,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="1,372.98,310.14,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="1,384.13,310.14,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="1,395.27,310.14,7.75,8.74" target="#b7">8)</ref>, it has yet to be demonstrated that the technique is scalable to corpora whose data size is at the terabyte level. In this work, we assess the performance of Hedge on a terabyte scale, summarizing training results using the Terabyte 2005 queries and data and presenting testing results using the Terabyte 2006 queries and data.</p><p>Finally, we note that in the context of TREC, the Hedge algorithm is both an automatic and a manual technique: In the absence of feedback, Hedge is a fully automatic metasearch algorithm; in the presence of feedback, Hedge is a manual technique, capable of "learning" how to optimally combine the underlying systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Metasearch</head><p>The problem of metasearch <ref type="bibr" coords="1,439.30,515.94,10.51,8.74" target="#b1">[2,</ref><ref type="bibr" coords="1,454.60,515.94,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="1,467.15,515.94,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="1,484.67,515.94,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="1,497.20,515.94,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="1,514.73,515.94,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="1,532.25,515.94,7.75,8.74" target="#b3">4]</ref> is to combine the ranked lists of documents output by multiple retrieval systems in response to a given query so as to optimize the quality of the combination and hopefully exceed the performance of the best underlying system. Aslam, Pavlu, and Savell <ref type="bibr" coords="1,508.21,575.71,10.51,8.74" target="#b2">[3]</ref> considered two benchmark metasearch techniques for assessing how well their Hedge algorithm performed: (1) CombMNZ, a technique which sums the (appropriately normalized) relevance scores assigned to each document by the underlying retrieval systems and then multiplies that summation by the number of systems that retrieved the document and (2) Condorcet, a technique based on a well known method for conducting a multicandidate election, where the doc-uments act as candidates and the retrieval systems act as voters providing preferential rankings among these candidates. In experiments using the TREC 3, 5, 6, 7, and 8 collections, Aslam et al. demonstrated that, in the absence of feedback, Hedge consistently outperforms Condorcet and at least equals the performance of CombMNZ; in the presence of even modest amounts of user feedback, Hedge significantly outperforms both CombMNZ and Condorcet, as well as the best underlying system.</p><p>In this work, we discuss our experiments with the Hedge algorithm in the Terabyte track at TREC 2006, and we also compare to those results obtained by using the Hedge algorithm run over the data from the Terabyte track at TREC 2005. In the sections that follow, we begin by briefly describing our methodology and experimental setup, and we then describe our results and conclude with future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We implemented and tested the Hedge algorithm for metasearch as described in Aslam et al. <ref type="bibr" coords="2,254.70,348.77,9.96,8.74" target="#b2">[3]</ref>. While the details of the Hedge algorithm can be found in the aforementioned paper, the relevant intuition for this technique, as quoted from this paper, is given below.</p><p>Consider a user who submits a given query to multiple search engines and receives a collection of ranked lists in response. How would the user select documents to read in order to satisfy his or her information need? In the absence of any knowledge about the quality of the underlying systems, the user would probably begin by selecting some document which is "highly ranked" by "many" systems; such a document has, in effect, the collective weight of the underlying systems behind it. If the selected document were relevant, the user would begin to "trust" systems which retrieved this document highly (i.e., they would be "rewarded"), while the user would begin to "lose faith" in systems which did not retrieve this document highly (i.e., they would be "punished"). Conversely, if the document were non-relevant, the user would punish systems which retrieved the document highly and reward systems which did not. In subsequent rounds, the user would likely select documents ac-cording to his or her faith in the various systems in conjunction with how these systems rank the various documents; in other words, the user would likely pick documents which are ranked highly by trusted systems.</p><p>Our Hedge algorithm for on-line metasearch precisely encodes the above intution using the well studied Hedge algorithm for on-line learning, first proposed by Freund and Schapire <ref type="bibr" coords="2,445.80,182.72,9.97,8.74" target="#b7">[8]</ref>. In our generalization of the Hedge algorithm, Hedge assigns a weight to each system corresponding to Hedge's computed "trust" in that system, and each system assigns a weight to each document corresponding to its "trust" in that document; the overall score assigned to a document is the sum, over all systems, of the product of the Hedge weight assigned to the system (a quantity which varies given user feedback) and the system's weight assigned to that document (a fixed quantity which is a function of the rank of that document according to the system). The weights Hedge assigns to systems are initially uniform, and they are updated given user feedback (in line with the intuition given above), and the document set is dynamically ranked according to the overall document scores which change as the Hedge-assigned system weights change.</p><p>Initially, Hedge assigns a uniform weight to all systems and computes overall scores for the documents as described above; the ranked list of documents ordered by these scores is created, and we refer to this system and corresponding list as "hedge0." A user would naturally begin by examining the top document in this list, and Hedge would seek feedback on the relevance of that document. Given this feedback, Hedge will assign new system weights (rewarding those systems that performed "well" with respect to this document and punishing those that did not), and it will assign new overall scores to the documents based on these new system weights. The remaining unjudged documents would then be re-ranked according to these updated scores, and this new list would be presented to the user in the next round.</p><p>After k documents have been judged, the performance of "hedge k" can be assessed from at least two perspectives, which we refer to as the "user experience" and the "research librarian" perspectives, respectively.</p><p>• User experience: Concatenate the list of k judged documents (in the order that they were presented to the user) with ranking of the unjudged documents produced at the end of round k. This concatenated list corresponds to the "user experience," i.e., the ordered documents that have been examined so far along with those that will be examined if no further feedback is provided.</p><p>• Research librarian: Concatenate the relevant subset of the k judged documents with the ranking of the unjudged documents produced at the end of round k. This concatenated list corresponds to what a research librarian using the Hedge system might present to a client: the relevant documents found thus far followed by the ordered list of unjudged documents in the collection.</p><p>Note that the performance of the "research librarian" is likely to exceed that of the "user experience" by any reasonable measure of retrieval performance since judged non-relevant documents are eliminated from the former concatenated list. In what follows, "hedge k" refers to the system, concatenated list, and performance as defined with respect to the "research librarian" perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup and Results</head><p>We tested the performance of the Hedge algorithm by using the queries from TREC 2005 Terabyte Track.</p><p>Then we run Hedge for Terabyte06 track, using real user feedback (we judged 50 documents per query). Both Terabyte05 and Terabyte06 use the GOV2 collection of about 25 million documents. We indexed the collection using the Lemur Toolkit; that process took about 3 days using a 2-processor dual-core Opteron machine (2.4 GHz/core).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Underlying IR systems</head><p>The underlying systems include: (1) two tf-idf retrieval systems; (2) three KL-divergence retrieval models, one with Dirichlet prior smoothing, one with Jelinek-Mercer smoothing, and the last with absolute discounting;</p><p>(3) a cosine similarity model; (4) the OKAPI retrieval model; (5) and the INQUERY retrieval method. All of the above retrieval models are provided as standard IR systems by the Lemur Toolkit <ref type="bibr" coords="3,106.59,671.35,9.97,8.74" target="#b0">[1]</ref>.</p><p>These models were run against a collection (GOV2) of web data crawled from web sites in the .gov domain during early 2004 by NIST <ref type="bibr" coords="3,499.95,99.07,9.97,8.74" target="#b5">[6]</ref>. The collection is 426GB in size and contains 25 million documents <ref type="bibr" coords="3,363.19,122.98,9.97,8.74" target="#b5">[6]</ref>. Although this collection is not a full terabyte in size, it is still much larger than the collections used at previous TREC conferences.</p><p>For each query and retrieval system, we considered the top 10,000 scored documents for that retrieval system. Once all retrieval systems were run against all queries, we ran the Hedge algorithm described above to perform metasearch on the ranked lists we obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results using Terabyte 2005 queries and qrel</head><p>We used the TREC 2005 qrel files to provide Hedge with relevance feedback. If one of our underlying systems retrieved a document that was not included in the qrel file, we assumed the document to be non-relevant.</p><p>Hedge was run as follows. In the first round each of the underlying systems all have an equal weight and the underlying lists are fused by ranking documents according to highest weighted average mixture loss <ref type="bibr" coords="3,368.95,419.32,9.97,8.74" target="#b2">[3]</ref>. The initial run of Hedge (hedge0) will not acquire any relevance judgments and hence can be compared directly to standard metasearch techniques <ref type="bibr" coords="3,359.74,455.18,10.52,8.74" target="#b2">[3]</ref> (e.g. CombMNZ).</p><p>In the following round, the top document from hedge0 is judged. In our case, we obtain the judgment from TREC qrel file (0 if document not in the qrel). If the document is relevant, it is put at the top our metasearch list, and if it is not, it is discarded. The judgment is then used to re-weight the underlying systems. As described above, systems are re-weigthed based on the rank of the document just judged. Then a new metasearch list is produced, corresponding to hedge1. The next round proceeds in the same manner: the top unjudged document from the last metasearch list is judged and then used to: (1) identify where the document should be placed in the list; (2) update the system weight vector to reward the correct systems and punish the incorrect systems; (3) re-rank the remaining unjudged documents.</p><p>In our experiments we had 50 rounds (relevance judgments) and we note the results of hedge for 0, 5, 10, 15, 20, 30, and 50 judgments.</p><p>For comparison, we also ran Condorcet and CombMNZ over the ranked lists generated by our underlying systems. We then calculated mean average precision scores for each of the three metasearch systems and compared the performance of the Hedge system with the performance of the lists generated by Condorcet and CombMNZ (see Figure <ref type="figure" coords="4,256.56,195.36,7.20,8.74" target="#fig_0">1.</ref>). We compare Hedge to CombMNZ, Condorcet, and the underlying retrieval systems that were used for our metasearch technique. Table <ref type="table" coords="4,215.16,490.73,4.98,8.74" target="#tab_0">2</ref> shows that Hedge, in the absence of any relevance feedback (hedge0), consistently outperforms Condorcet. The performance of hedge0 is comparable with the performance of CombMNZ.</p><p>Table <ref type="table" coords="4,116.51,551.15,4.98,8.74" target="#tab_0">2</ref> illustrates that both hedge0 and CombMNZ are able to exceed the performance of the best underlying system. This demonstrates that Hedge alone, even without any relevance feedback, is a successful metasearch technique.</p><p>After providing the Hedge algorithm with only ten relevance judgments (hedge10), Hedge significantly outperforms CombMNZ, Condorcet, and the best underlying system in terms of MAP (Table <ref type="table" coords="4,261.75,659.40,3.88,8.74">1</ref>). Also hedge50 more than doubles precision at cutoff 20 of the top underlying system. This is in part because documents that have been ranked relevant are placed at the top of the list, whereas the documents that have been judged non-relevant are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results for Terabyte 2006 queries</head><p>For our Terabyte submission to TREC 2006, given the lack of judgments, we manually judged several documents for each query. We choose to run Hedge for 50 rounds (for each query) on top of our underlying IR systems (provided by Lemur, as described above). Therefore, in total, 50 rounds x 50 queries = 2500 documents were judged for relevance.</p><p>As a function of the amount of relevance feedback utilised, four different runs were submitted to Terabyte 2006: hedge0 (no judgments), which is essentially an automatic metasearch system; hedge10 (10 judgments per query); hedge30 (30 judgments per query) and hedge50 (50 judgments per query). The performance of all four runs are presented in Table <ref type="table" coords="4,532.25,683.31,3.88,8.74" target="#tab_2">3</ref>.</p><p>The table reports the mean average precision (MAP), R-precison, and precision-at-cutoff 10, 30, 100 and 500. Against expectations, hedge30 looks slightly better than hedge50 but this is most likely due to the fact that hedge30 was included as a contributor to the TREC pool of judged documents while hedge50 was not. We examined a subset of the mismatched relevance judgments and we believe that there were judgment errors on both sides. Nevertheless all judgment disagreements on judges affect measured hedge performance negatively. For comparison we re-run hedge30 (30 judgments) using the TREC qrel file for relevance feedback. In doing so, we obtained a mean average precision of 0.33, consistent with performance on Terabyte 2005. This would place the new hedge30 run second among all manual runs, as ordered by MAP (Figure <ref type="figure" coords="5,346.29,111.02,3.88,8.74" target="#fig_3">2</ref>).</p><p>We also looked at this new run (hedge30 with TREC qrel file instead of user feedback) on a queryby-query basis. Figure <ref type="figure" coords="5,418.72,146.89,4.98,8.74" target="#fig_2">3</ref> shows a scatterplot comparison, per query, of the original hedge30 performance and the performance using TREC judgments for feedback. Note the significant and nearly uniform improvements obtained using TREC judgments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>It has been shown that the Hedge algorithm for online learning is highly efficient and effective as a metasearch technique. Our experiments show that even without relevance feedback Hedge is still able to produce metasearch lists which are directly comparable to the standard metasearch techniques Condorcet and CombMNZ, and which exceed the performance of the best underlying list. With relevance feedback System MAP R-prec p@10 p@30 p@100 p@500 hedge0 0.  Hedge is able to considerably outperform Condorcet and CombMNZ.</p><p>The performance shown when using TREC qrels file was consistently very good; when using our judgments the relatively poor performance was due to using a set of judgments for feedback and a different set of judgments for evaluation. Ultimately we believe that Hedge is somehow immune to judge disagreement, as long us the feedback comes from the same source (or judge or user) as the performance measurement. Certainly, in practice, it is possible that two users ask the same query but they are looking for different information; in this case user feedback would be different which would lead to different metasearch lists produced and eventually to a satisfactory performance for each user.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.00,430.22,229.02,8.74;4,72.00,442.17,166.48,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Terabyte05: Hedge-m: metasearch performance as more documents are judged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,310.98,486.33,229.01,8.74;5,310.98,498.29,229.02,8.74;5,310.98,510.24,229.02,8.74;5,310.98,522.20,229.01,8.74;5,310.98,534.15,229.02,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Terabyte06: hedge30. Each dot corresponds to a query; x-axis corresponds to hedge30 AP values obtained with our judgments as user feedback; y-axis corresponds to hedge30 AP values using TREC qrel file for feedback. MAP vaues are denoted by "×".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,72.00,370.19,468.00,8.74;6,72.00,382.15,449.65,8.74;6,76.98,174.70,458.04,170.42"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Terabyte06: hedge30 with TREC qrel judgments. The shell shows trec eval measurements on top of the published TREC Terabyte06 ranking of manual runs [5]; It would rank second in terms of MAP.</figDesc><graphic coords="6,76.98,174.70,458.04,170.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,310.98,73.96,229.02,347.20"><head>Table 2 :</head><label>2</label><figDesc>Results for input and metasearch systems on Terabyte05. CombMNZ, Cordorcet, and Hedge N were run over all input systems.</figDesc><table coords="4,310.98,73.96,229.02,291.46"><row><cell>System</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell cols="5">CombMNZ 0.2332 0.2693 0.2715 0.2399</cell></row><row><cell>Condorcet</cell><cell cols="4">0.1997 0.2264 0.2302 0.2119</cell></row><row><cell>Hedge 0</cell><cell cols="4">0.2314 0.2641 0.2687 0.2297</cell></row><row><cell>Hedge 10</cell><cell cols="4">0.2579 0.2944 0.2991 0.2650</cell></row><row><cell>Hedge 50</cell><cell cols="4">0.3199 0.3669 0.3652 0.3493</cell></row><row><cell cols="5">Table 1: Terabyte05: Hedge vs. Metasearch Tech-</cell></row><row><cell cols="5">niques CombMNZ and Condorcet, combining 2 , 4,</cell></row><row><cell cols="2">6, 8 underlying systems.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>System</cell><cell></cell><cell>MAP</cell><cell>p@20</cell><cell></cell></row><row><cell cols="2">Jelinek-Mercer</cell><cell cols="2">0.2257 0.3780</cell><cell></cell></row><row><cell>Dirichlet</cell><cell></cell><cell cols="2">0.2100 0.4200</cell><cell></cell></row><row><cell>TFIDF</cell><cell></cell><cell cols="2">0.1993 0.4250</cell><cell></cell></row><row><cell>Okapi</cell><cell></cell><cell cols="2">0.1906 0.4270</cell><cell></cell></row><row><cell cols="2">log-TFIDF</cell><cell cols="2">0.1661 0.4140</cell><cell></cell></row><row><cell cols="4">Absolute Discounting 0.1575 0.3660</cell><cell></cell></row><row><cell cols="2">Cosine Similarity</cell><cell cols="2">0.0875 0.1960</cell><cell></cell></row><row><cell cols="2">CombMNZ</cell><cell cols="2">0.2399 0.4550</cell><cell></cell></row><row><cell>Condorcet</cell><cell></cell><cell cols="2">0.2119 0.4200</cell><cell></cell></row><row><cell>hedge0</cell><cell></cell><cell cols="2">0.2297 0.4260</cell><cell></cell></row><row><cell>hedge10</cell><cell></cell><cell cols="2">0.2650 0.5270</cell><cell></cell></row><row><cell>hedge50</cell><cell></cell><cell cols="2">0.3493 0.8090</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,171.61,86.32,268.79,76.43"><head>Table 3 :</head><label>3</label><figDesc>Results for Hedge runs on Terabyte06 queries.</figDesc><table coords="6,171.61,86.32,268.79,44.60"><row><cell></cell><cell>177</cell><cell>0.228 0.378 0.320</cell><cell>0.232</cell><cell>0.104</cell></row><row><cell>hedge10</cell><cell>0.239</cell><cell>0.282 0.522 0.394</cell><cell>0.278</cell><cell>0.118</cell></row><row><cell>hedge30</cell><cell>0.256</cell><cell>0.286 0.646 0.451</cell><cell>0.290</cell><cell>0.119</cell></row><row><cell>hedge50</cell><cell>0.250</cell><cell>0.280 0.682 0.470</cell><cell>0.279</cell><cell>0.115</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,92.48,659.40,17.15,8.74;6,131.60,659.40,24.66,8.74;6,178.21,659.40,28.78,8.74;6,228.94,659.40,11.94,8.74;6,262.82,659.40,38.19,8.74;6,92.48,671.35,39.57,8.74;6,153.29,671.35,16.06,8.74;6,190.58,671.35,50.68,8.74;6,262.50,671.35,38.52,8.74;6,92.48,680.68,135.41,11.37" xml:id="b0">
	<monogr>
		<ptr target="http://www.cs.cmu.edu/lemur" />
		<title level="m" coord="6,92.48,659.40,17.15,8.74;6,131.60,659.40,24.66,8.74;6,178.21,659.40,28.78,8.74;6,228.94,659.40,11.94,8.74;6,262.82,659.40,38.19,8.74;6,92.48,671.35,39.57,8.74;6,153.29,671.35,16.06,8.74;6,190.58,671.35,50.68,8.74;6,262.50,671.35,34.66,8.74">The lemur toolkit for language modeling and information retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.46,414.53,208.54,8.74;6,331.46,426.48,208.54,8.74;6,331.46,438.44,208.54,8.74;6,331.46,450.39,208.55,8.74;6,331.46,462.35,208.54,8.74;6,331.46,474.30,208.54,8.74;6,331.46,486.26,127.88,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,494.55,414.53,45.45,8.74;6,331.46,426.48,46.62,8.74">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,520.89,438.44,19.11,8.74;6,331.46,450.39,208.55,8.74;6,331.46,462.35,208.54,8.74;6,331.46,474.30,133.63,8.74">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donald</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.46,508.11,208.54,8.74;6,331.46,520.06,208.54,8.74;6,331.46,532.02,208.54,8.74;6,331.46,543.97,208.54,8.74;6,331.46,555.93,208.54,8.74;6,331.46,567.88,208.54,8.74;6,331.46,579.84,208.54,8.74;6,331.46,591.79,69.77,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,367.85,520.06,172.14,8.74;6,331.46,532.02,118.90,8.74">A unified model for metasearch, pooling, and system evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgiliu</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Savell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,390.34,555.93,149.66,8.74;6,331.46,567.88,208.54,8.74;6,331.46,579.84,75.66,8.74">Proceedings of the Twelfth International Conference on Information and Knowledge Management</title>
		<editor>
			<persName><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joachim</forename><surname>Hammer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sajda</forename><surname>Quershi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Len</forename><surname>Seligman</surname></persName>
		</editor>
		<meeting>the Twelfth International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003-11">November 2003</date>
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.46,613.64,208.54,8.74;6,331.46,625.59,208.55,8.74;6,331.46,637.55,208.54,8.74;6,331.46,649.51,64.82,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,422.89,625.59,117.12,8.74;6,331.46,637.55,142.51,8.74">Automatic combination of multiple ranked retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Bartell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">K</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Belew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,495.27,637.55,40.42,8.74">SIGIR 94</title>
		<imprint>
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.46,671.35,208.54,8.74;6,331.46,683.31,208.55,8.74;7,92.48,75.16,208.54,8.74;7,92.48,87.11,122.88,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,378.23,683.31,140.47,8.74">The TREC 2006 terabyte track</title>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,92.48,75.16,208.54,8.74;7,92.48,87.11,92.33,8.74">Proceedings of the Fifteen Text REtrieval Conference (TREC 2006)</title>
		<meeting>the Fifteen Text REtrieval Conference (TREC 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,107.04,208.54,8.74;7,92.48,118.99,208.55,8.74;7,92.48,130.95,208.55,8.74;7,92.48,142.90,140.70,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,139.25,118.99,140.47,8.74">The TREC 2005 terabyte track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,92.48,130.95,208.55,8.74;7,92.48,142.90,110.15,8.74">Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text REtrieval Conference (TREC 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,162.83,208.53,8.74;7,92.48,174.78,208.53,8.74;7,92.48,186.74,37.64,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,258.68,162.83,42.33,8.74;7,92.48,174.78,108.21,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,226.48,174.78,41.57,8.74">TREC 94</title>
		<imprint>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,206.67,184.58,8.74;7,293.55,206.67,7.47,8.74;7,92.48,218.62,208.54,8.74;7,92.48,230.58,208.54,8.74;7,92.48,242.53,208.54,8.74;7,92.48,254.49,57.34,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,293.55,206.67,7.47,8.74;7,92.48,218.62,208.54,8.74;7,92.48,230.58,152.27,8.74">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName coords=""><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,255.58,230.58,45.44,8.74;7,92.48,242.53,136.78,8.74">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,274.41,208.53,8.74;7,92.48,286.37,168.52,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,151.47,274.41,149.54,8.74;7,92.48,286.37,34.44,8.74">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">Joon</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,148.09,286.37,40.46,8.74">SIGIR 97</title>
		<imprint>
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,306.29,208.53,8.74;7,92.48,318.25,208.54,8.74;7,92.48,330.20,103.47,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,152.69,306.29,148.32,8.74;7,92.48,318.25,174.45,8.74">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">Joon</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,288.01,318.25,13.01,8.74;7,92.48,330.20,31.01,8.74">SI-GIR 95</title>
		<imprint>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,350.13,208.54,8.74;7,92.48,362.08,208.55,8.74;7,92.48,374.04,204.58,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,260.61,350.13,40.41,8.74;7,92.48,362.08,208.55,8.74;7,92.48,374.04,60.32,8.74">Modeling score distributions for combining the outputs of search engines</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,173.97,374.04,28.25,8.74">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,393.96,208.53,8.74;7,92.48,405.92,208.54,8.74;7,92.48,417.87,208.54,8.74;7,92.48,429.83,64.82,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,194.45,393.96,106.56,8.74;7,92.48,405.92,208.54,8.74;7,92.48,417.87,128.23,8.74">How much more is better? Characterizing the effects of adding more IR systems to a combination</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vogt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,247.27,417.87,49.13,8.74">RIAO 2000</title>
		<imprint>
			<biblScope unit="page" from="457" to="475" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
