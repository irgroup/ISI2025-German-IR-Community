<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.04,70.86,326.04,12.93;1,113.28,88.74,403.27,12.93">Spam Filtering using Inexact String Matching in Explicit Feature Space with On-Line Linear Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.80,123.36,53.28,10.90"><forename type="first">D</forename><surname>Sculley</surname></persName>
							<email>dsculley@cs.tufts.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tufts University Medford</orgName>
								<address>
									<postCode>02155</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.54,123.36,114.53,10.90"><forename type="first">Gabriel</forename><forename type="middle">M</forename><surname>Wachman</surname></persName>
							<email>gwachm01@cs.tufts.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tufts University Medford</orgName>
								<address>
									<postCode>02155</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.77,123.36,93.51,10.90"><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
							<email>brodley@cs.tufts.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tufts University Medford</orgName>
								<address>
									<postCode>02155</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.04,70.86,326.04,12.93;1,113.28,88.74,403.27,12.93">Spam Filtering using Inexact String Matching in Explicit Feature Space with On-Line Linear Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B7C35B47732C1AE0FEB9C3554CCCE92</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contemporary spammers commonly seek to defeat statistical spam filters through the use of word obfuscation. Such methods include character level substitutions, repetitions, and insertions to reduce the effectiveness of word-based features. We present an efficient method for combating obfuscation through the use of inexact string matching kernels, which were first developed to measure similarity among mutating genes in computational biology. Our system avoids the high classification costs associated with these kernel methods by working in an explicit feature space, and employs the Perceptron Algorithm using Margins for fast on-line training. No prior domain knowledge was incorporated into this system. We report strong experimental results on the TREC 2006 spam data sets and on other publicly available spam data, including near-perfect performance on the TREC 2006 Chinese spam data set. These results invite further exploration of the use of inexact string matching for spam filtering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Effective spam filtering -automatically blocking unwanted email from the user -is now widely recognized as one of the most important problems in electronic communication. In recent years, statistical spam filters, based on techniques from the field of machine learning, have achieved significant progress. However, as seen in the 2005 TREC Spam Filtering Competition <ref type="bibr" coords="1,502.34,417.36,46.55,10.91;1,81.00,430.80,91.06,10.91">(Cormack and Lynam, 2005b)</ref>, even state of the art methods yield imperfect results.</p><p>Traditional statistical spam filters are built on a foundation of Naive Bayes classification on word-level features <ref type="bibr" coords="1,172.56,459.60,73.84,10.91" target="#b9">(Graham, 2002;</ref><ref type="bibr" coords="1,249.61,459.60,87.38,10.91" target="#b18">Metsis et al., 2006)</ref>, a combination that has proven efficient and reasonably effective. Recently, character-level features, such as those used by the PPM classifier <ref type="bibr" coords="1,81.00,486.72,121.62,10.91" target="#b1">(Bratko and Filipic, 2005)</ref>, have improved performance. Character level features offer resistance to standard spam attacks such as obfuscation, the practice of intentionally misspelling words to defeat word-based spam filters <ref type="bibr" coords="1,195.48,513.84,104.81,10.91" target="#b25">(Wittel and Wu, 2004)</ref>. In this paper, we extend character-level features to include methods of inexact string matching with the goal of further combating obfuscation attacks. These methods have been applied in computational biology, in the form of inexact string matching kernels with support vector machines on genomic data <ref type="bibr" coords="1,350.18,554.52,106.70,10.91">(Leslie et al., 2002a,b;</ref><ref type="bibr" coords="1,461.41,554.52,87.64,10.91;1,81.00,567.96,24.03,10.91" target="#b15">Leslie and Kuang, 2004)</ref>, and have also been used for text classification <ref type="bibr" coords="1,335.78,567.96,90.61,10.91" target="#b17">(Lodhi et al., 2002)</ref>.</p><p>Naive application of these methods carries a high computational cost, which is problematic for industrial-scale spam filtering. Our approach reduces this cost in three ways. First, we use fixed variants of the wildcard and gappy inexact string matching kernels, and conflate the ideas of wildcards and gaps. Second, we map inexact string matching features into an explicit feature space, enabling fast classification. Finally, we use an on-line linear classifier, the Perceptron Algorithm with Margins (PAM) <ref type="bibr" coords="1,184.47,651.00,129.91,10.91" target="#b12">(Krauth and MÃ©zard, 1987;</ref><ref type="bibr" coords="1,318.06,651.00,67.90,10.91" target="#b16">Li et al., 2002)</ref>, which has fast training time and good resistance to noise <ref type="bibr" coords="1,198.23,664.56,148.63,10.91" target="#b11">(Khardon and Wachman, 2005)</ref>.</p><p>The remainder of this paper provides details on the methods of efficient inexact string matching (Section 2) and the PAM classifier (Section 3). Experimental results in Section 4 show that this approach has given top level results on the public Chinese email data set from the 2006 TREC Spam Filtering competition.  There are hundreds of other variants for this word alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Inexact String Matching</head><p>Inexact string matching offers one possible solution to the problem of filtering obscured text. Spam filters that rely on traditional word-based features may be defeated by the spam attack of obfuscation, in which words are intentionally obscured through the use of misspellings, character substitutions, insertions, and deletions <ref type="bibr" coords="2,269.67,292.92,107.88,10.91" target="#b25">(Wittel and Wu, 2004)</ref>. While a case has been made that statistical spam filters will learn to recognize common obfuscations over time, given enough training data <ref type="bibr" coords="2,106.33,319.92,74.32,10.91" target="#b9">(Graham, 2002)</ref>, there are a very large number of possible obfuscations for any given word, each of which may appear infrequently. Furthermore, consider the effect of joining two words by removing their intervening space: likethis. The brute force approach to learning variations is now responsible for a feature space that is suddenly quadratic in the number of dictionary entries, and the combinatorics of this problem quickly grow as more substitutions, insertions, and deletions are employed. Indeed, obfuscations altering of several characters are already common. Evidence for this can be found in the TREC 2005 spam data set (see Figure <ref type="figure" coords="2,402.96,401.28,4.22,10.91" target="#fig_0">1</ref>.)</p><p>Intuitively, it seems reasonable to expect that successful methods for genomic data would also have success in the domain of spam, as these domains have similar characteristics. Genomic data are represented as strings in which character level substitution, insertion, and deletion are common for evolutionary reasons. The use of inexact string matching kernels with Support Vector Machines has proven effective for learning from genomic data <ref type="bibr" coords="2,325.22,471.96,103.11,10.91">(Leslie et al., 2002a,b;</ref><ref type="bibr" coords="2,431.66,471.96,112.36,10.91" target="#b15">Leslie and Kuang, 2004)</ref>. However, industrial spam filtering <ref type="bibr" coords="2,244.95,485.40,208.34,10.91">(and the 2006 TREC Spam Filtering Track)</ref> has severe limits on computational cost that precludes the direct application of inexact string matching kernels.</p><p>To address this cost, we first propose efficient fixed variants of two inexact string matching methods, the wildcard kernel and the gappy kernel, both of which are built off a foundation of k-mers. We map these kernels to explicit feature space to allow fast classification with the PAM classifier described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">k-mers</head><p>The use of character level k-mers is one basic method of inexact string matching <ref type="bibr" coords="2,484.80,609.12,64.11,10.91;2,81.00,622.68,29.34,10.91">(Leslie et al., 2002a)</ref>. A k-mer is a sequence of k contiguous characters drawn from the alphabet Î£. The set S of all possible k-mers in Î£ defines a space X with |Î£| k dimensions, with each unique k-mer a â S indexing a unique dimension in X. A string s may be represented as a vector x â X by mapping âa â S, x a = c(s, a), where c(s, a) returns the number of times that the k-mer a appears in string s, with overlaps allowed.</p><p>In spam filtering, there have been several results showing that binary features may be more effective than count-based features <ref type="bibr" coords="2,255.48,706.80,105.28,10.91" target="#b5">(Drucker et al., 1999;</ref><ref type="bibr" coords="2,366.01,706.80,93.49,10.91" target="#b18">Metsis et al., 2006)</ref>, perhaps because binary features offer greater resistance to the good word attack <ref type="bibr" coords="2,388.80,720.36,108.65,10.91" target="#b25">(Wittel and Wu, 2004)</ref>. A string may be represented as a binary k-mer vector x â X, by mapping âa â S, x a = b(s, a), where b(s, a) = 1 if a appears anywhere in s, and b(s, a) = 0 otherwise.</p><p>Even this simple feature space allows powerful inexact string matching. By removing position information, the k-mer feature space allows the insertion or deletion of any number of characters between k-mers. Furthermore, because the k-mers are overlapping, they implicitly capture some level of sequence information that is lost with the standard bag of words model.</p><p>The choice of k is an important parameter. Too small a value of k creates ambiguous features, while too high a value of k makes the chance of finding exact matches between strings improbable. Indeed, in a spam classification setting, the optimal value of k may be language dependent, or may vary with the amount of expected obfuscation within the text. Thus, in our model, we choose to focus on a range of k values. An (i, j) k-mer mapping creates a space in which there is a dimension for all possible k-mers, where i â¤ k â¤ j. Indeed, the best spam filter from the 2005 TREC Spam Filtering Track was a P P M -filter <ref type="bibr" coords="3,244.32,225.36,122.22,10.91" target="#b1">(Bratko and Filipic, 2005)</ref>, which implicitly uses a feature space of (i, j) k-mers <ref type="bibr" coords="3,155.03,238.92,130.97,10.91" target="#b23">(Sculley and Brodley, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wildcards and Gaps</head><p>In computational biology, it has been found that k-mers alone are not expressive enough to give optimal classification performance on genomic data. For example, a k-mer feature space will have reduced effectiveness on substrings in which at least two character substitutions, insertions, or deletions occur no more than k positions apart. Two forms of inexact string matching kernels address this issue: wildcard kernels and gappy kernels <ref type="bibr" coords="3,334.08,341.52,116.32,10.91" target="#b15">(Leslie and Kuang, 2004)</ref>, which allow k-mers to match even if there have been a small (specified) number of insertions, deletions, or substitutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wildcards.</head><p>The (k, w) wildcard kernel maps each k-mer to a set of k-mers in which up to w "wildcard" characters replace the characters in the original k-mer <ref type="bibr" coords="3,422.15,390.96,121.84,10.91" target="#b15">(Leslie and Kuang, 2004)</ref>. For example, the (3, 1) wildcard mapping of the k-mer abc is the set {abc, *bc, a*b, ab*}. The wildcard character is a special symbol that is allowed to match with any other character. Naturally, allowing wildcards increases computational cost. In our testing with spam data, however, we have found that allowing a fixed wildcard variant gives equivalently strong results to the standard wildcard kernel. A fixed (k, p) wildcard mapping maps a given k-mer to a set of two k-mers: the original, and a k-mer with a wildcard character at position p. Note that we designate the first position in a string as position 0. Thus, the fixed (3, 1) wildcard mapping of abc is {abc, a*c}. This fixed mapping gives more flexibility to the k-mer feature space, but only increases the size of the feature space by a constant factor of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaps.</head><p>The (g, k) gappy kernel (where g â¤ k), allows g-mers to match with k-mers by inserting kg gaps into the g-mer <ref type="bibr" coords="3,202.79,548.88,117.04,10.91" target="#b15">(Leslie and Kuang, 2004)</ref>. Note that this is equivalent to allowing k-mers to match with g-mers by deleting up to kg characters from the k-mer. Thus, the (2, 3) gappy mapping of the string acbd includes positive values for features indexed by {acb, cbd, ac, ab, cb, cd, bd} . As with the wildcard mappings, we reduce computational cost with a fixed (k, p) gappy variant, in which a k-mer is is mapped to a set of k-mers: the original k-mer, and a k-mer in which the character at position p has been removed. For clarity, we define a function remove(a, p) which removes a single character from string a at position p. Thus, if a is a k-mer, remove returns a (k -1)-mer.</p><p>Merging Gaps and Wildcards. The function of gaps and wildcards are similar, with the difference that implementing the gappy mappings with the remove function changes the length of the k-mers. However, our (i, j) k-mer space contains k-mers of different lengths. This allows us to reduce computational cost by defining the wildcard character as a character of length zero. Furthermore, we used the fixed variants, and only allow wildcard replacement and gaps to occur at Given (i, j, p) map string s to vector x:</p><p>For k from i to j do:</p><p>Find set K of all k-mers in s For each k-mer a â K do: position p in all k-mers. We call this a fixed (i, j, p) inexact string feature mapping, pseudo-code for which is given in Figure <ref type="figure" coords="4,215.55,276.24,4.22,10.91" target="#fig_1">2</ref>. The resulting feature space has the following properties:</p><p>â¢ k-mers may match other k-mers exactly â¢ (k + 1)-mers may match other (k + 1)-mers with a mismatch at position p.</p><p>â¢ k-mers may match (k + 1)-mers with a gap at position p.</p><p>Thus, this feature space has a measure of robustness to a number of obfuscation attacks, including character substitution, insertion, and deletion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalization.</head><p>Because email messages may be of varying lengths, we normalize all data vectors using the Euclidean norm. Additionally, normalizing provides some resistance to the sparse data attack <ref type="bibr" coords="4,135.24,443.16,103.61,10.91" target="#b25">(Wittel and Wu, 2004)</ref>, which attempts to defeat a spam filter using very short messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Explicit Feature Mapping</head><p>For efficient classification, we represent the features in the inexact feature space explicitly, using a sparse feature/value representation. This approach is similar to that recommended by <ref type="bibr" coords="4,492.98,504.96,56.36,10.91;4,81.00,518.40,55.44,10.91" target="#b24">Sonnenburg et al. (2005)</ref>, who show that explicit feature mapping is preferable to implicit feature mapping (using, for example, suffix trees) for support vector machine training and classification of strings, when using small k-mers. This allows the PAM classifier to classify a new data example by computing a single (sparse) inner product. On-Line training is equally fast, requiring only a single (sparse) vector addition. Thus, although the feature space is of high dimensionality, the sparsity of the explicit feature vectors allows fast training and classification with manageable space cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">On-Line Linear Classifiers</head><p>In the previous section, we showed an efficient method for mapping email messages to explicit vectors in an (i, j, p) inexact string feature space. Because the vast majority of machine learning methods are designed to accept explicit feature vectors as input, we have a wide range of training and classification methods at our disposal, including Naive Bayes classifiers and support vector machines. In this competition, we choose to classify the data with the PAM classifier <ref type="bibr" coords="4,489.13,693.24,59.97,10.91;4,81.00,706.80,66.28,10.91" target="#b12">(Krauth and MÃ©zard, 1987;</ref><ref type="bibr" coords="4,150.49,706.80,66.87,10.91" target="#b16">Li et al., 2002)</ref>, which learns a linear classifier with tolerance for noise <ref type="bibr" coords="4,482.29,706.80,66.81,10.91;4,81.00,720.36,77.67,10.91" target="#b11">(Khardon and Wachman, 2005)</ref>.</p><p>Given: set of examples and their labels Z = ((x 1 , y 1 ), . . . , (x m , y m )) â (R n Ã {-1, 1}) m , Ï Initialize w := 0 n for every (x j , y j ) â Z do: if y j ( w, x j ) &lt; Ï w := w + Î·y j x j done We chose to use PAM for several reasons. First, it offers fast on-line training and classification. Second, strong performance has been achieved by other linear classifiers on spam such as a variant of logistic regression <ref type="bibr" coords="5,179.64,254.52,124.39,10.91" target="#b8">(Goodman and Yin, 2006)</ref> and linear support vector machines <ref type="bibr" coords="5,476.89,254.52,72.04,10.91;5,81.00,267.96,24.63,10.91" target="#b5">(Drucker et al., 1999;</ref><ref type="bibr" coords="5,109.68,267.96,94.96,10.91" target="#b21">Rios and Zha, 2004)</ref>; since <ref type="bibr" coords="5,240.37,267.96,147.80,10.91" target="#b11">Khardon and Wachman (2005)</ref> demonstrate experimentally that PAM is competitive with SVM on many data sets, it is reasonable to expect PAM to perform well on spam. Third, it has been shown that while generative models such as Naive Bayes may have steeper (faster) learning curves, discriminative models such as linear classifiers have lower asymptotic error <ref type="bibr" coords="5,163.68,322.20,106.02,10.91" target="#b19">(Ng and Jordan, 2002)</ref>. In the remainder of this section, we will review the basic Perceptron Algorithm, and explore relevant features of the PAM variant that makes it an attractive choice for on-line spam filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Perceptron Algorithm</head><p>The perceptron algorithm <ref type="bibr" coords="5,208.22,396.96,88.25,10.91" target="#b22">(Rosenblatt, 1958)</ref> takes as input a set of training examples in R n with labels in {-1, 1}. Using a weight vector, w â R n , initialized to 0 n , it predicts the label of each training example x to be y = sign( w, x ). The algorithm adjusts w on each misclassified example by an additive factor.<ref type="foot" coords="5,184.08,436.56,4.23,7.29" target="#foot_0">1</ref> An upper-bound on the number of mistakes committed by the perceptron algorithm can be shown both when the data are linearly separable <ref type="bibr" coords="5,399.36,451.20,70.72,10.91" target="#b20">(Novikoff, 1962</ref>) and when they are not linearly separable <ref type="bibr" coords="5,205.70,464.76,133.75,10.91" target="#b6">(Freund and Schapire, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Perceptron Algorithm with Margins (PAM)</head><p>The classical perceptron attempts to separate the data but has no guarantees on the separation margin obtained. The Perceptron Algorithm with Margins (PAM) <ref type="bibr" coords="5,403.92,525.96,131.22,10.91" target="#b12">(Krauth and MÃ©zard, 1987;</ref><ref type="bibr" coords="5,539.20,525.96,9.87,10.91;5,81.00,539.40,57.65,10.91" target="#b16">Li et al., 2002)</ref> attempts to establish such a margin, Ï , during the training process. Following work on support vector machines <ref type="bibr" coords="5,214.33,552.96,89.68,10.91" target="#b0">(Boser et al., 1992)</ref> one may expect that providing the perceptron with higher margin will add to the stability and accuracy of the hypothesis produced <ref type="bibr" coords="5,472.69,566.52,76.41,10.91;5,81.00,580.08,94.72,10.91" target="#b4">(Cristianini and Shawe-Taylor, 2000)</ref>.</p><p>To establish the margin, instead of only updating on examples for which the classifier makes a mistake, PAM also updates on x j if y j ( x j , w ) &lt; Ï . When the data are linearly separable, the margin of the classifier produced by PAM can be lower-bounded <ref type="bibr" coords="5,401.16,621.00,133.27,10.91" target="#b12">(Krauth and MÃ©zard, 1987;</ref><ref type="bibr" coords="5,539.19,621.00,9.87,10.91;5,81.00,634.44,54.99,10.91" target="#b16">Li et al., 2002)</ref>. The algorithm is summarized in Figure <ref type="figure" coords="5,336.86,634.44,4.22,10.91" target="#fig_2">3</ref>.</p><p>It is important to select a reasonable value for Ï . If Ï is too large, the algorithm will not be able to find a stable hypothesis until the norm of w grows large enough at which point individual updates will have little effect; if Ï is too small, the margin of the hypothesis will be small and the performance may suffer. The learning rate, Î·, controls the extent to which w changes on a single update; too large of a value causes the algorithm to make large fluctuations, and too small of a value results in slow convergence to a stable hypothesis and hence many mistakes. Note that we can eliminate Î· in our case by scaling Ï by 1 Î· , but we leave it in for discussion as it is part of the classical algorithm. PAM enables fast classification and on-line training. The classification time of PAM is dominated by the computation of the inner product w, x . A naive inner product takes O(m) time, where m is the number of features. When x is sparse, containing only s âª m non-zero features, we can compute this inner product in O(s) time. Similarly, the time for an on-line update is dominated by updating the hypothesis vector w, which can be done in O(s) time as well. Moreover, PAM does not require training updates for well-classified examples. Thus, the total number of updates is likely to be significantly less than the total number of training examples.</p><p>In comparison with Naive Bayes and linear support vector machines, PAM has the same classification cost O(m), but will have lower overall training time than either method. Naive Bayes requires O(m)-cost updates on every example in the training set, while PAM does not train on well classified examples. A linear support vector machine that trains in O(sn) time has recently been proposed <ref type="bibr" coords="6,154.69,386.52,79.11,10.91" target="#b10">(Joachims, 2006)</ref>, but in practice the constant is quite high and PAM training is significantly faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we report experimental results on spam classification for the 2006 TREC Spam Filtering track. Our general approach was to map email messages to feature vectors, using the fixed (i, j, p) inexact string feature space described in Section 2. On-Line training and classification were performed using the PAM algorithm, as described in Section 3. We set Î· to 0.1, and Ï to 100.</p><p>Experimental Setup. We tested this filter at four settings, as shown in in Table <ref type="table" coords="6,486.51,519.72,4.22,10.91" target="#tab_1">1</ref>. Each filter was given a unique setting of the maximum k-mer size, specified by j in the fixed (i, j, p) inexact string feature space. The value of Ï = 100 was chosen by parameter search, using the SpamAssassin public corpus<ref type="foot" coords="6,145.32,559.20,4.23,7.29" target="#foot_1">2</ref> as a tuning set. No preprocessing of the email messages was performed. We simply used the first n characters from the raw text of the email (including any header information and attachments) as an input string, and created a sparse feature vector from that string. Our initial filters used a maximum of 200K characters, and performed successfully on initial tests for the trec05p-1 data set <ref type="bibr" coords="6,191.76,614.52,142.52,10.91">(Cormack and Lynam, 2005a)</ref>, and on all private data sets from the 2006 competition. However, two filters with larger maximum k-mer sizes failed to complete testing on the pe{i,d} data set, due to lack of memory. When we reduced the maximum input string length from 200K to the first 3000 characters, this problem was eliminated -and performance for all filters improved. For example, on the pei tests, tufS1F improved from 0.062 to 0.040 on (1-ROCA)% using the first 3000 characters. However, note that the official results for the 2006 competition were with tufS filters using the first 200K characters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>For our initial tests, run before the 2006 competition, we tested our filters on the trec05p-1 data set, and found that our filters were competitive with the best filters from the TREC 2005 Spam Filtering track (see Table <ref type="table" coords="7,296.17,320.16,4.82,10.91" target="#tab_2">2</ref>) <ref type="bibr" coords="7,309.49,320.16,141.31,10.91">(Cormack and Lynam, 2005b)</ref>.</p><p>Our results from the TREC 2006 competition were strong (see Table <ref type="table" coords="7,442.80,336.00,4.21,10.91" target="#tab_2">2</ref>). In particular, our method achieved extremely strong performance on the public corpus of Chinese email, with a steep learning curve and a 1 -ROCA score of .0023 for tufS1F , and .0031 for tufS2F on the incremental task, pci, which the initial report suggests are at or near the top level of performance for the 2006 competition, and are an order of magnitude better than the reported median. The results for the delayed learning task on Chinese email, pcd, were also very strong. Although absolute performance on the delayed task was reduced, due to the more difficult nature of training on delayed feedback, our methods still gave top-level results. However, these results must be taken with a grain of salt. At the TREC 2006 conference meeting, it was noted that results on this corpus of Chinese email were very strong with many filters. This raises the possibility that this corpus may be a flawed benchmark data set.</p><p>In general, our results on other data sets were encouraging, giving second place aggregate results in the 2006 TREC spam competition. On the public English corpus, our methods gave results well above the median for both the incremental learning task pei and the delayed learning task ped. Our results on the two private corpora, x2 and b2, were less strong. At present, we do not know the nature of these data sets, and cannot yet speculate on the cause of the relative decrease in performance on these data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>These results raise a number of interesting questions. First, we would like to investigate the use of our filter on other data sets of Chinese email. We believe our methods should be robust to the several encoding schemes for Chinese email, each of which employs multiple characters per word. Because Chinese does not employ a phonetic alphabet, short character-level sequences may carry more information in this language than they do in English. We conjecture that it is this fact that enables the strong results achieved by our methods. Indeed, one of the strengths of character-level pattern recognition for spam filtering is that no assumptions are made about language or encoding scheme. Future work should include additional experiments comparing performance on phonetic and ideographic emails in a range of other languages, to ensure that this conjecture holds true. Secondly, we draw attention to the learning curve results for the pei and ped experiments, which hit very low (i.e., good) levels of the 1 -ROCA measure between 10,000 and 25,000 messages, and then rises sharply to a plateau of lower performance for both the incremental and delayed learning tasks. This is not the convergent behavior we would expect to see after many thousands of examples, and requires investigation. For example, the problem could be caused by concept drift, an overcorrection for an anomalous email, or perhaps an email with a noisy label. These issues might be addressed by varying the learning rate Î· or the margin Ï as the number of seen training examples increases as in <ref type="bibr" coords="9,152.75,156.60,66.24,10.91" target="#b7">Gentile (2001)</ref>.</p><p>Third, we note that our initial choice of filtering based on the first 200K characters in the email message was not optimal. Later tests using only the first 3,000 characters from each email not only reduced both memory usage and computational cost, but also improved performance on all versions of our filter.</p><p>Overall, the fixed (i, j, p) inexact string features, represented as sparse explicit feature vectors, used in conjunction with the on-line linear classifier PAM has given strong performance on a number of tests. These results were obtained using inexact string matching in a fairly naive way, without taking domain knowledge into account. In the future, we plan on investigating the use of inexact string matching on email specific features, such as the subject heading and sender information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,81.00,169.68,468.09,10.91;2,129.36,183.24,419.34,10.91;2,129.36,196.80,271.89,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Obscured Text. These are the 25 most common variants of the word 'Viagra' found in the 2005 TREC spam data set, illustrating the problem of word obfuscation and tokenization.There are hundreds of other variants for this word alone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,145.44,230.64,339.18,10.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pseudo code for fixed (i, j, p) inexact string feature mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,172.80,204.72,284.31,10.91"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The On-Line Perceptron Algorithm with Margins</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,81.00,291.96,468.08,10.91;8,129.36,305.52,182.97,10.91;8,313.44,331.08,3.03,10.91"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Learning curve for public Chinese corpus: incremental learning pci and delayed learning pcd, with filters tufS1F and tufS2F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,211.80,68.28,206.46,82.79"><head>Table 1 :</head><label>1</label><figDesc>Parameter settings for tufS filters.</figDesc><table coords="6,253.68,92.87,115.55,58.20"><row><cell>filter</cell><cell>(i, j, p)</cell><cell>Ï</cell></row><row><cell cols="3">tufS1F (2, 4, 1) 100</cell></row><row><cell cols="3">tufS2F (2, 5, 1) 100</cell></row><row><cell cols="3">tufS3F (2, 6, 1) 100</cell></row><row><cell cols="3">tufS4F (2, 7, 1) 100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,81.00,65.03,468.19,177.24"><head>Table 2 :</head><label>2</label><figDesc>Summary of Results on (1-ROCA)% measure. Results are reported for the tests on TREC 2006 public Chinese corpus pci and pcd, public English corpus pei and ped, Mr. X private corpus x2 and x2d, b2 private corpus b2 and b2d, and the 2005 TREC public corpus trec05p-1. Results on sets ending in d are for delayed feedback experiments; others are for incremental learning experiments. Results marked with * were produced using variant that only considered first 3000 characters, rather than the first 200K.</figDesc><table coords="7,112.20,65.03,398.52,82.08"><row><cell>filter</cell><cell>pci</cell><cell>pcd</cell><cell>pei</cell><cell>ped</cell><cell>x2</cell><cell>x2d</cell><cell>b2</cell><cell>b2d</cell><cell>trec05p-1</cell></row><row><cell>best</cell><cell cols="2">0.003 0.01</cell><cell>0.03</cell><cell>0.1</cell><cell>0.03</cell><cell>0.03</cell><cell>0.1</cell><cell>0.3</cell><cell>0.019</cell></row><row><cell cols="10">tufS1F 0.002 0.008 0.060 0.211 0.095 0.199 0.390 0.836 0.020</cell></row><row><cell cols="10">tufS2F 0.003 0.010 0.060 0.203 0.069 0.145 0.338 0.692 0.018</cell></row><row><cell cols="10">tufS3F 0.004 0.012 0.042  *  0.132  *  0.063 0.126 0.335 0.614 0.018</cell></row><row><cell cols="10">tufS4F 0.005 0.011 0.041  *  0.136  *  0.075 0.131 0.320 0.570 0.017</cell></row><row><cell cols="2">median 0.03</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>1</cell><cell>0.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,96.00,710.90,452.96,8.97;5,96.00,721.82,212.88,8.97"><p>The classical perceptron includes a bias term Î¸. In our preliminary experiments, we found our simplified version to perform as well as the classical one on spam data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,96.00,721.82,181.57,8.97"><p>http://spamassassin.apache.org/publiccorpus</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,81.00,332.52,467.70,10.91;9,91.92,346.08,444.27,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,251.56,332.52,237.00,10.91">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,509.64,332.52,39.06,10.91;9,91.92,346.08,336.03,10.91">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.00,371.28,467.80,10.91;9,91.92,384.84,405.68,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,202.59,371.28,189.60,10.91">Spam filtering using compression models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Filipic</surname></persName>
		</author>
		<idno>IJS-DP-9227</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,371.16,384.84,47.84,10.91">L jubljana</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Jozef Stefan Institute</publisher>
			<pubPlace>Slovenia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Intelligent Systems</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,81.00,410.16,467.93,10.91;9,91.92,423.72,254.68,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,249.86,410.16,151.46,10.91">Spam corpus creation for TREC</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,427.08,410.16,121.85,10.91;9,91.92,423.72,215.03,10.91">Proceedings of the Second Conference on Email and Anti-Spam (CEAS)</title>
		<meeting>the Second Conference on Email and Anti-Spam (CEAS)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct coords="9,81.00,448.92,468.03,10.91;9,91.92,462.48,265.12,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,259.34,448.92,160.73,10.91">TREC 2006 spam track overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,448.32,448.92,100.71,10.91;9,91.92,462.48,225.67,10.91">The Fourteenth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>TREC 2005) Proceedings</note>
</biblStruct>

<biblStruct coords="9,81.00,487.68,468.05,10.91;9,91.92,501.24,91.71,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,256.32,487.68,205.42,10.91">An introduction to support vector machines</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.00,526.44,468.07,10.91;9,91.92,540.00,274.36,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,269.45,526.44,238.96,10.91">Support vector machines for spam categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,522.60,526.44,26.47,10.91;9,91.92,540.00,157.10,10.91">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1048" to="1054" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.00,565.20,467.98,10.91;9,91.92,578.76,131.68,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,218.92,565.20,279.03,10.91">Large margin classification using the perceptron algorithm</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,508.56,565.20,40.42,10.91;9,91.92,578.76,39.50,10.91">Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="277" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.00,604.08,467.99,10.91;9,91.92,617.64,171.64,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,143.40,604.08,295.38,10.91">A new approximate maximal margin classification algorithm</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,452.88,604.08,96.11,10.91;9,91.92,617.64,84.87,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="213" to="242" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.00,642.84,467.82,10.91;9,91.92,656.40,249.27,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,210.39,642.84,197.70,10.91">Online discriminative spam filter training</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,432.48,642.84,116.34,10.91;9,91.92,656.40,215.03,10.91">Proceedings of the Third Conference on Email and Anti-Spam (CEAS)</title>
		<meeting>the Third Conference on Email and Anti-Spam (CEAS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.00,681.60,434.44,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,141.01,681.60,75.09,10.91">A plan for spam</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://www.paulgraham.com/spam.html" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,81.00,706.80,468.07,10.91;9,91.92,720.36,456.87,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,151.91,706.80,170.07,10.91">Training linear svms in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,350.40,706.80,198.67,10.91;9,91.92,720.36,351.66,10.91">KDD &apos;06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,61.80,467.81,10.91;10,91.92,75.24,457.02,10.91;10,91.92,88.80,232.24,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,241.24,61.80,249.39,10.91">Noise tolerant variants of the perceptron algorithm</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Khardon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wachman</surname></persName>
		</author>
		<ptr target="http://www.cs.tufts.edu/tr/techreps/TR-2005-8" />
	</analytic>
	<monogr>
		<title level="j" coord="10,304.96,75.24,180.96,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Tufts University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>To appear in</note>
</biblStruct>

<biblStruct coords="10,81.00,111.36,467.92,10.91;10,91.92,124.92,169.84,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,214.96,111.36,288.89,10.91">Learning algorithms with optimal stability in neural networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Krauth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>MÃ©zard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,512.76,111.36,36.16,10.91;10,91.92,124.92,57.92,10.91">Journal of Physics A</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="745" to="752" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,147.36,467.97,10.91;10,91.92,160.92,366.87,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,280.12,147.36,268.85,10.91;10,91.92,160.92,58.72,10.91">The spectrum kernel: A string kernel for svm protein classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,173.52,160.92,171.61,10.91">Pacific Symposium on Biocomputing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="566" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,183.48,467.93,10.91;10,91.92,197.04,360.88,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,319.01,183.48,229.92,10.91;10,91.92,197.04,27.39,10.91">Mismatch string kernels for svm protein classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,142.56,197.04,185.13,10.91">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,219.60,467.91,10.91;10,91.92,233.04,239.68,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,200.66,219.60,302.58,10.91">Fast string kernels using inexact matching for protein sequences</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,512.76,219.60,36.16,10.91;10,91.92,233.04,141.99,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1435" to="1455" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,255.60,467.96,10.91;10,91.92,269.16,426.27,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,399.01,255.60,149.96,10.91;10,91.92,269.16,72.59,10.91">The perceptrom algorithm with uneven margins</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,188.04,269.16,222.14,10.91">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,291.72,468.07,10.91;10,91.92,305.28,341.80,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,435.01,291.72,114.06,10.91;10,91.92,305.28,62.28,10.91">Text classification using string kernels</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,163.56,305.28,183.32,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,327.72,467.95,10.91;10,91.92,341.28,316.48,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Metsis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<title level="m" coord="10,325.74,327.72,223.22,10.91;10,91.92,341.28,282.35,10.91">Spam filtering with naive bayes -which naive bayes? Third Conference on Email and Anti-Spam (CEAS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,363.84,467.91,10.91;10,91.92,377.40,129.29,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="10,190.60,363.84,358.32,10.91;10,91.92,377.40,96.06,10.91">On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,399.84,468.01,10.91;10,91.92,413.40,136.72,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,156.61,399.84,179.36,10.91">On convergence proofs on perceptrons</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Novikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,345.12,399.84,203.89,10.91;10,91.92,413.40,43.98,10.91">Symposium on the Mathematical Theory of Automata</title>
		<imprint>
			<date type="published" when="1962">1962</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="615" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,435.96,467.79,10.91;10,91.92,449.52,379.11,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,186.38,435.96,357.77,10.91">Exploring support vector machines and random forests for spam detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,105.60,449.52,331.31,10.91">Proceedings of the First Conference on Email and Anti-Spam (CEAS)</title>
		<meeting>the First Conference on Email and Anti-Spam (CEAS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,471.96,467.98,10.91;10,91.92,485.52,240.88,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,153.37,471.96,395.61,10.91;10,91.92,485.52,42.06,10.91">The perceptron: A probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,143.40,485.52,97.13,10.91">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="386" to="407" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,508.08,467.94,10.91;10,91.92,521.64,413.21,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,231.16,508.08,317.78,10.91;10,91.92,521.64,60.88,10.91">Compression and machine learning: A new perspective on feature space vectors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.36,521.64,173.31,10.91">DCC: Data Compression Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="332" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,544.08,468.01,10.91;10,91.92,557.64,456.87,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,293.95,544.08,206.10,10.91">Large scale genomic sequence svm classifiers</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>RÃ¤tsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,520.32,544.08,28.69,10.91;10,91.92,557.64,350.44,10.91">ICML &apos;05: Proceedings of the 22nd international conference on Machine learning</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="848" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,81.00,580.20,467.92,10.91;10,91.92,593.76,102.99,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,211.48,580.20,168.23,10.91">On attacking statistical spam filters</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Wittel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,387.24,580.20,161.69,10.91;10,91.92,593.76,69.21,10.91">CEAS: First Conference on Email and Anti-Spam</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
