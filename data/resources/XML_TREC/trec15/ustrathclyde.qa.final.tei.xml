<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.02,87.44,417.95,14.82;1,196.14,105.86,219.68,14.82">Contextual information and assessor characteristics in complex question answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.98,137.64,66.74,9.02"><forename type="first">Cindy</forename><surname>Azzopardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<postCode>G1 1XH</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.64,137.64,58.89,9.02"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<postCode>G1 1XH</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.45,137.64,49.60,9.02"><forename type="first">Mark</forename><surname>Baillie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<postCode>G1 1XH</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.20,137.64,43.33,9.02"><forename type="first">Ralf</forename><surname>Bierig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<postCode>G1 1XH</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.97,137.64,49.19,9.02"><forename type="first">Emma</forename><surname>Nicol</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<postCode>G1 1XH</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.80,137.64,49.22,9.02"><forename type="first">Ian</forename><surname>Ruthven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<postCode>G1 1XH</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.11,149.16,64.70,9.02"><forename type="first">Simon</forename><surname>Sweeney</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<postCode>G1 1XH</postCode>
									<settlement>Glasgow</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.02,87.44,417.95,14.82;1,196.14,105.86,219.68,14.82">Contextual information and assessor characteristics in complex question answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">61BBCD76A767A2CB494C6E130EB3ED67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ciqa track investigates the role of interaction in answering complex questions: questions that relate two or more entities by some specified relationship. In our submission to the first ciqa track we were interested in the interplay between groups of variables: variables describing the question creators, the questions asked and the presentation of answers to the questions.</p><p>We used two interaction forms -html questionnaires completed before answer assessment -to gain contextual information from the answer assessors to better understand what factors influence assessors when judging retrieved answers to complex questions.</p><p>Our results indicate the importance of understanding the assessor's personal relationship to the questiontheir existing topical knowledge for example -and also the presentation of the answers -contextual information about the answer to aid in the assessment of the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variables under study</head><p>In our participation in this year's track we studied three groups of variables and selected relationships between these variables. The three groups of variables focus on variables relating to the assessors themselves (section 2.1.1), the questions set (section 2.1.2) and the answers presented (section 2. <ref type="bibr" coords="1,491.78,438.90,14.75,9.02">1.3)</ref>. In this section we discuss the variables we selected and how we measured them. The majority of variables were investigated through the use of interaction forms -html forms presented to and completed by the answer assessors. Each form was required to be answered within three minutes.</p><p>We designed two interaction forms: Interaction Form 1 gathered information on the assessor and on the question, Interaction Form 2 gathered information on sample answers to the questions set by the assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Assessor variables</head><p>The first group of variables relate to the assessors themselves. In ciqa the same person who assesses the answers to the question also completes the interaction forms. Knowing more about this person could provide useful information regarding preferences for answer formats or how personal characteristics affect the answer assessment process.</p><p>To gain information on the assessor we asked for four responses gathered in Interaction Form 1.</p><p>• topical knowledge. Topical knowledge has been shown to be one of the major factors in assessing relevance <ref type="bibr" coords="1,145.22,630.54,7.47,9.02" target="#b0">[1]</ref><ref type="bibr" coords="1,152.69,630.54,3.74,9.02" target="#b1">[2]</ref><ref type="bibr" coords="1,152.69,630.54,3.74,9.02" target="#b2">[3]</ref><ref type="bibr" coords="1,156.43,630.54,7.47,9.02" target="#b3">[4]</ref>. Consequently, we asked the assessor to rate their topical knowledge ("How much do you think you know about the topics in this question:") of the major topics in the question on a threevalued scale "not much/same as most people/know quite a lot").</p><p>• confidence in assessing answers. Although the questions are created by the assessors themselves this does not guarantee that they will find the task of assessing answers easy. In a previous study <ref type="bibr" coords="1,494.92,688.80,11.70,9.02" target="#b4">[5]</ref> we found that asking assessors to rate their confidence in assessing retrieved material was a useful question in identifying searcher behaviour regarding the assessment process. Consequently, we asked the assessors to rate their confidence in assessing correct answers to their own questions ("For this question, how confident are you that you could recognise correct answers to this question?") on a threevalued category ("very confident/depends on the answers returned/not very confident").</p><p>• prior expectations. Next we asked the assessor if they already had an expectation of an answer to the question set, i.e. a page of answers. We asked "Do you have an answer in mind for this question (could you provide an answer without searching)?" and solicited responses on a four-category scale ("yes/no/I could provide a partial answer/no answer but have an idea of what an answer might look like"). The questions set by the assessors are original questions so we expect they have some exposure to the topics in their questions even if they may know little about the topics. The prior expectation question was designed to elicit some information about their knowledge of possible answers, as opposed to the topical knowledge question which elicited information about their knowledge of the topic of the question. The latter two possible responses ("I could provide a partial answer/no answer but have an idea of what an answer might look like") were intended to differentiate between situations where the assessor is confident enough to provide a partial answer (but may require more information to provide a full answer) and situations where the assessor does not know the answer but has an expectation of the likely answers or at least the direction of the answer. For example, for topic 32 "The analyst is especially interested in opinions of scientists as to whether there is a family link between dinosaurs and birds, and what evidence they cite concerning their opinions", the assessor may suspect that there is a family link between dinosaurs and birds and is looking for confirmatory evidence.</p><p>• variety of opinions. The final question we asked the assessor was about what they would prefer in terms of a set of answers. We asked the assessor about what type of answer set they required -"For this question, would good set of answers contain?" -and asked them to respond using one of two options "a variety of similar opinions (or evidence)/as many different opinions as possible". Although few topics gave clues to which of these two options would be applicable to the topic, we felt that assessors may well have reasons for asking for the information and may prefer answers of particular direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Question variables</head><p>The second group of variables relate to the questions and question descriptions before presentation of answers. In studying the questions we are interested in five variables:</p><p>• time. The questions used in ciqa often relate to news events and the time of these events can be important in detecting good answers. For each question the assessor was asked about preferences on the date of good answers "For this question, would good answers come from?" The response was modelled as a categorical variable ("recent articles/older articles/any articles"). This variable was asked in Interaction Form 1.</p><p>• number of entities.  • predicted difficulty. A final variable was the predicted difficulty of the search. Although the assessors were being asked to judge only retrieved answers to their own questions, rather than perform a search themselves, we felt it would be useful to ask for their opinion on how difficult the information problem (i.e. the description field) would be to answer using a general purpose search engine such as Google or MSN. We ask this in case the assessors have any pre-conceived view on the information problem that might affect their judgement of the quality of the answers presented in the second interaction form. That is, if the assessor believes the question is easy to answer they might rate answers more strictly than if they believe the question to be difficult to answer. The predicted difficulty was measured using a 5 point scale ("very difficult/fairly difficult/cannot predict how difficult/fairly easy/very easy") to the question "If you were searching for answers to this question using a web search engine such as Google, how easy do you think it would be to find good answers?"). This variable was asked in Interaction Form 1.</p><p>• complexity-the questions set are designed to be complex in the sense that they relate several entities or concepts (e.g. drug companies and universities in Figure <ref type="figure" coords="3,341.09,432.24,3.63,9.02" target="#fig_0">2</ref>). The description field describes why the information is required and other details of the information need promoting the question (e.g. "the analyst is concerned about universities which do research on medical subjects slanting their findings, especially concerning drugs, towards drug companies which have provided money to the universities") from question 32 in Figure <ref type="figure" coords="3,212.92,478.26,3.75,9.02" target="#fig_0">2</ref>. The description field, therefore, provides additional information that will be used to assess the answers returned by participating groups. For some questions this additional information simplifies the original question, e.g. "Specifically, the analyst seeks evidence that smugglers use the island of San Andres for such a purpose"; for other questions the description field extends the original question to include additional questions. For example for question 32 the question asks "What familial ties exist between dinosaurs and birds?" whereas the narrative makes it clear that an answer should contain both opinions on whether dinosaurs and birds are related and the evidence for such positions ("The analyst is especially interested in opinions of scientists as to whether there is a family link between dinosaurs and birds, and what evidence they cite concerning their opinions"). A question with several sub-questions that require answer we deemed as being more complex to answer. Conflating complexity with number of facets we performed an internal classification of the number of facets in each topic description. Three internal assessors were asked, independently, to count the number of facets in each topic description. For all except 6 topics the assessors agreed on the number of facets contained in the description. A short discussion resolved the disagreement on these 6 topics to give a number of facets for each topic<ref type="foot" coords="3,153.96,637.15,3.24,5.83" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Answer variables</head><p>Interaction Form 2 presents a series of 8 answers to the assessor. In our study we did not attempt any novel question answering research. Our major interest was in the presentation of answers and the effect of contextual information in the assessment process, in this case date, source and quality information. All presented answers were selected from manual searching of the web. Although the answers to be returned to ciqa for assessment were to be answers from the AQUAINT news collection we felt that we could obtain better answers from the general web. All answers contain a textual answer with ellipses to denote missing text if the answer is a fragment of a sentence e.g. "..an appearance on Oprah or Today can shoot book sales through the roof…". We deliberately selected short answers, rather than whole sentences or paragraphs, to simulate the main question answering task in which short answers are preferred. Thus, what we were trying to do was investigate interaction with a good questions answering system.</p><p>Each answer had a common layout consisting of three offwhite <ref type="foot" coords="4,369.24,216.67,3.24,5.83" target="#foot_3">3</ref> fields comprising the answer and contextual information and three pale yellow fields containing our questions to the assessor regarding the answer. The first answer line contained the answer to the question presented in red font, the second line contained a source for the answer presented as a URL and the date of the article presented in a dark blue, and the third line contained sources that supported (agreed) with the answer. When presenting the answers we set out to investigate three variables</p><p>• time of answer. Answers were either presented with information on the date of the source containing the answer. This was to test whether the date of the information was useful in assessing the answer. Sources were randomly assigned dates from one of two lists of dates: recent dates, in this case only from 2006, or older dates, in this case prior to 2004.</p><p>• quality of source. Each answer was associated with a source which was a website URL. All URLs shown were presented as hypertext links but were not linked to any other page, i.e. clicking on the text did not transfer the assessor to a new page. Although all answers presented were genuine, the sources were manually assigned and did not correspond to the actual sources of the answer. Rather we sought to distinguish between high and low quality sources of information. High quality sources of information were ones that we felt that most assessors would recognise as established sources of reputable information, even if they did not agree with any particular political stance or editorial policy of these sources. We developed a list of these sources which were primarily chosen from a list of top US newspapers and several well known television stations. Low quality sources of information were ones that we felt assessors would be unfamiliar with, primarily sources that had unusual names. The answers provided bear no relation to the actual content of these sources; the sources were only used to test whether the source of the information was important in assessing the quality of an answer presented on the interaction form.</p><p>• supporting evidence. According to Barry and Schamber <ref type="bibr" coords="4,372.13,633.84,11.67,9.02" target="#b5">[6]</ref> one of the important criteria in assessing relevance is the presence of supporting or confirmatory evidence. That is, evidence that information (in our case an answer) is supported by multiple sources can lead to the information being more likely to be assessed relevant. Accordingly we presented some answers as having multiple sources of information agreeing on the answer. For example in Figure <ref type="figure" coords="5,458.71,85.86,5.01,9.02" target="#fig_1">3</ref> the answer is given by www.seattletimes.com and supported by www.newslink.org and www.houstonchronicle.com. If an answer had supporting sites these correspond to the perceived quality of the original source, i.e. high-quality sources were supported by high-quality sources and weak-quality sources were supported by weak-quality sources. It would have been useful to mix these two conditions (quality of original source vs quality of supporting sources) but the number of combinations required would have been too many to assess within the three minute condition. These supporting sources were also manually assigned and bear no relationship to the actual content of the sources. This allowed us to test whether supported answers were preferred.</p><p>The cross combination of three variables (recent vs older information, high-quality source vs low-quality source, supporting vs no supporting sources) gives 8 combinations of answer presentation.</p><p>For each answer we asked the assessor to assess:</p><p>• quality of answer. The first question, asked on all answers, was on the general quality of the answer "Is this a good answer to the topic description" and assessors were asked to respond using the categories "yes/no/partially good/need more information to decide". "Partially good" was intended to reflect answers that supply some useful information but not necessarily all the required information, and "need more information to decide" to reflect the situation where the assessor would need more context from the document to decide on the value of the answer.</p><p>• expectation of answer. Next we asked about the fit of the answer to the assessors prior expectation of the answers -"was this one of the answers you expected" which was to be answered using the categories "yes/no/had no expected answer"</p><p>• next action. Finally we asked what the assessor would do given this answer from a search "Given this answer from a search would you?". In this case the answers were limited to "accept this answer/read the document/look for a better answer".</p><p>A final set of questions, displayed after all answers, asked the assessor about the set of answers as a whole, Figure <ref type="figure" coords="5,119.27,432.96,3.76,9.02" target="#fig_2">4</ref>. We first asked whether the set of answers provided useful information. The answers themselves might answer the assessors need without reading the full text of the documents ("yes"), or might be inappropriate answers ("no") or the assessor may require to read the documents to judge how useful the answers were ("depends on the actual documents"). Next we asked what, if anything, would have made the answers more useful. Here we had three choices and the assessor could select any combination. Answers could have been more useful if they were longer, more varied (i.e. contained more different types of information or different answers) or more complete (i.e. contained more facets of the initial question and description). Finally we asked what the assessor would do given this form (set of answers) from a search, either browse the documents themselves or start a new search. As we mentioned previously we were interested in the use of answers and surrogates in web search. A poor set of answers might lead to a new query whereas a good set of answers should encourage the searcher to explore the documents retrieved. This final question was intended to reflect an overall assessment of the answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In this section we present some initial findings starting with general trends. Firstly, examining the quality of the answers provided in the forms as classified by the ciqa assessors in Table <ref type="table" coords="5,432.09,698.64,5.01,9.02" target="#tab_2">1</ref> we can see that the majority of answers (57%) were deemed to be good answers to the questions and a far smaller percentage were rated as being poor answers (16%). For a small percentage of answers (slightly more than one answer per form on average) the assessor could not decide on the quality of the answer without reference to the entire document. That is, the answer on its own did not allow the assessor to make a decision with seeing the answer in the context of the entire article. A small number of questions were rated as only partially good. Secondly, as can be seen from Table <ref type="table" coords="6,247.06,245.58,5.01,9.02" target="#tab_3">2</ref> the most common responses from the assessors was to assert an average level of topical knowledge with a high confidence in their ability to assess the accuracy of answers to the questions even though the questions were perceived to be difficult to answer. Even though the assessors felt that they only had average knowledge of the topics in the questions for most questions (22 out of 30) they had sufficient information to guess at least a partial answer to the question suggesting a certain degree of existing topical knowledge. The date of answers in most cases was perceived not to be important for these questions but for most questions the preferred output was a set of different answers rather than very similar answers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response to question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questions Responses knowledge of major topics in question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Answer variables</head><p>Regarding the presentation of answers we had three main variables: date of answer, quality of answer source and presence/absence of supporting sources.</p><p>In Table <ref type="table" coords="7,128.33,85.86,5.01,9.02" target="#tab_5">3</ref> we look at what percentage of answers in each assessment category were labelled as coming from good or poor sources, e.g. 50.6% of good answers were presented as coming from a source we felt would be recognised as a good source of information whereas 49.4% of good answers came from poor sources of information. Good answers were equally as likely to come from good sources as weak sources.</p><p>Poor answers, however, were more likely to be rated as coming from weak sources and partially good answers from good sources. Answers therefore may benefit from having good sources but be weakened by coming from weak source.</p><p>When examining the next action based on good/weak sources, Table <ref type="table" coords="7,376.40,177.84,3.74,9.02" target="#tab_6">4</ref>, we see that there is a very slight tendency to read documents from good sources whereas the far more common response for answers from weak sources is to look for a better answer. In Table <ref type="table" coords="7,129.00,378.84,5.01,9.02" target="#tab_8">5</ref> we repeat the analysis for the presence/absence of supporting information. Here there is s a slight trend in that good answers more often supported and poor answers are far more likely not to be supported. Unsupported answers also are more likely to be seen as only partially good. The presence of supporting evidence causes more answers to be judged as cannot decide. Perhaps, although we cannot check from this data, the presence of supporting evidence for a poor answer leads to some uncertainty in whether the answer actually is poor. When examining the next action based on good/weak sources, Table <ref type="table" coords="7,514.53,436.32,3.76,9.02" target="#tab_8">5</ref>, we see that there is a slight tendency to read documents from supported sources, a more marked tendency simply to accept the answers without reference to the document for supported answers whereas the more common response for unsupported answers is to look for a better answer. Finally in Table <ref type="table" coords="7,156.73,658.32,5.01,9.02" target="#tab_10">7</ref> we repeat the analysis for the recency of information. Here there is again a slight trend in that good answers more often recent, but the stronger pattern is related to the uncertain categories: older answers lead to more cannot decide category and partially good answers. When examining the next action based on good/weak sources, Table <ref type="table" coords="7,240.37,692.82,3.76,9.02" target="#tab_11">8</ref>, we see that there is a tendency to read older answers (perhaps to verify the information which may be out of date), and to accept recent answers without reading. </p><formula xml:id="formula_0" coords="7,188.46,225.36,20.59,9.02">good</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Assessor variables</head><p>We established a number of assessor variables. So far, we only have had time to evaluate the effect of a few of these but we present these to give indications of the importance of recording this class of information. First we compare how topical knowledge affects the judgements on the answers given and the predicted next actions based on the answers. In Table <ref type="table" coords="8,295.32,323.94,3.76,9.02" target="#tab_13">9</ref>, we compare the percentage of answers rated as good/poor/etc under the variables for topical knowledge. For the topics the assessor feels they know little (not much) the tendency is to be conservative: relatively low use of the definite categories (good/poor answer) and higher use of the partially good and cannot decide categories. Indeed the majority of answers for low topical knowledge reflect some uncertainty regarding the quality of the answer which requires resolution from the whole document. This is indicated in Table <ref type="table" coords="8,347.99,381.42,10.01,9.02" target="#tab_14">10</ref> where the most common next action for assessors with low topical knowledge is to decide they would read the whole document.</p><p>Assessors with higher levels of topical knowledge (same as most, know a lot) can be more decisive about the quality of answers presented with at least 85% of answers being rated as good or poor and few cases where the assessor cannot decide on the quality of the answer or rates the answer as being partially good.</p><p>Assessors with the highest level of topical knowledge are far more likely to act on the answer itself without recourse to the full text as, for 95% of answers, the predicted next action is to either accept the answer as presented or move to find a better answer. For the middle range of topical knowledge (same as most) the most likely action is one based solely on the answer (accept or move) but for almost 40% of answers the assessor would seek further information from the document (read Examining the relationship between prior expectation and answers, Table <ref type="table" coords="9,391.64,74.34,8.34,9.02" target="#tab_15">11</ref>, we see that if assessors who have no prior expectation of what answers might look like have a very distinct pattern reflecting a conservative approach to assessment: no answers are rated bad, an almost even split between good and partially good and a high rate of cannot decide decisions. This group of assessors also felt they would read the majority of documents, Table <ref type="table" coords="9,230.27,120.36,8.35,9.02" target="#tab_16">12</ref>. For the other groups the percentage of answers rated good fell and percentage of answers rated as bad increased as prior expectation of answer reduced.</p><p>Looking at predicted next actions, Table <ref type="table" coords="9,256.70,154.86,8.35,9.02" target="#tab_16">12</ref>, we see that for assessors with high prior expectation the most common next action was predicted to be simply accepting the answer. For assessors with a partial answer in mind the most common predicted next action was to accept the answer (but close second was to look for a better one). For assessors who had a low expectation of answers the more common action was to read the document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question variables</head><p>We have not finally analysed all of the variables involved in our analysis but here present the same analysis as previously for the relationship type in Tables <ref type="table" coords="9,286.21,463.92,10.04,9.02" target="#tab_17">13</ref> and<ref type="table" coords="9,316.20,463.92,8.34,9.02" target="#tab_18">14</ref>. Ciqa investigated five types of relation in 2006 (transport, relationship, effect, position and evidence). One striking observation is that for some question types there were seen to be more good answers than others. For example, for the effect and evidence question types, the assessors rated the answers as being 80% good whereas for the relationship type only 24% of answers were seen as good and more answers were seen as bad. Whether this is because good answers are easier to find for some questions (i.e. better answers were presented) or whether the answers were easier to evaluate by the assessors is not something we can answer within the current ciqa protocol but are working on.</p><p>We can summarise the answers and next actions for each question type as follows:</p><p>• Transport type. Here there was a fair proportion of good answers, compared to the average of 58% Table <ref type="table" coords="9,173.55,591.12,3.76,9.02" target="#tab_2">1</ref>, but a relatively high proportion of answers where the assessor could not decide on the quality of the answers. Even though the assessor felt that they would accept 24% of the answers, there still seems to be some uncertainty over the quality of answers in this group as, for most answers, the assessor would read the document containing the answer. • Relationship type. Here answers were not seen as being very good: low proportion of good answers, higher proportion of poor answers and most answers being only partially good or needing more information to decide on their quality. Very few answers were good enough to accept without further information and a high number would be rejected (30.83%) in favour of a search for better answers.</p><p>• Effect type. This class of question contained one of the highest proportions of good answers and one of the lowest proportion of poor and cannot decide decisions. However, for almost half the answers the assessor would read the document containing the answer. • Position type. This type of question contained a fair number of good answers and poor answers.</p><p>However the good answers appear to be very good as half of the answers presented would be accepted without further recourse to the document. • Evidence type. This class of answers had the highest proportion of good answers and the lowest proportion of cases where the assessor could not decide on the quality of answer. It also had a high proportion of accept decisions: cases where the assessor would accept the answer as presented. </p><formula xml:id="formula_1" coords="10,248.34,204.72,40.65,9.02">transport</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>We have many more analyses to run on the ciqa data from this year's track but what we have uncovered already demonstrates that answer assessment is not a neutral process. As well as question type, section 3.3, having an effect on the answer assessment process, so does the assessor's personal context, section 3.2, and how the answers are presented, section 3.1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,242.82,258.30,126.33,9.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ciqa topic number 32</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,207.24,378.72,197.51,9.02;4,90.30,285.42,431.40,79.44"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Answer template in Interaction Form 2</figDesc><graphic coords="4,90.30,285.42,431.40,79.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,216.12,637.80,179.86,9.02"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Questions on quality of answer set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,207.84,144.54,257.78,63.56"><head>"Is this a good answer to the question?" percentage</head><label></label><figDesc></figDesc><table coords="6,207.84,158.10,249.83,50.00"><row><cell>yes</cell><cell>57.48%</cell></row><row><cell>no</cell><cell>16.36%</cell></row><row><cell>need more information to decide</cell><cell>15.42%</cell></row><row><cell>partially good</cell><cell>10.75%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,184.92,222.60,242.26,9.02"><head>Table 1 :</head><label>1</label><figDesc>Percentage of answers in each assessment category</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,120.00,637.56,371.94,9.02"><head>Table 2 :</head><label>2</label><figDesc>Assessors' responses to Interaction Form 2. Most common response shown in bold</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,121.26,225.36,369.49,38.48"><head>answers poor answers cannot decide partially good answers good</head><label></label><figDesc></figDesc><table coords="7,121.26,239.70,329.87,24.14"><row><cell>sources</cell><cell>50.6</cell><cell>43.8</cell><cell>49.0</cell><cell>57.8</cell></row><row><cell>weak sources</cell><cell>49.4</cell><cell>56.2</cell><cell>51.0</cell><cell>42.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,175.32,278.34,261.39,62.12"><head>Table 3 :</head><label>3</label><figDesc>Percentages of answers rated under different categories.</figDesc><table coords="7,222.06,302.88,167.88,37.58"><row><cell></cell><cell cols="3">read accept move</cell></row><row><cell>good sources</cell><cell>52.7</cell><cell>50.8</cell><cell>42.6</cell></row><row><cell>weak sources</cell><cell>47.3</cell><cell>49.2</cell><cell>57.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,177.24,355.80,257.57,9.02"><head>Table 4 :</head><label>4</label><figDesc>Percentages of actions rated under different categories.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,130.20,494.40,351.59,48.92"><head>good answers bad answers cannot decide partially good answers presence of supporting information</head><label></label><figDesc></figDesc><table coords="7,130.20,520.08,330.47,23.24"><row><cell></cell><cell>52.8</cell><cell>39.8</cell><cell>60.2</cell><cell>36.4</cell></row><row><cell>absence of supporting information</cell><cell>47.2</cell><cell>60.2</cell><cell>39.8</cell><cell>63.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,175.32,557.82,261.39,62.12"><head>Table 5 :</head><label>5</label><figDesc>Percentages of answers rated under different categories.</figDesc><table coords="7,341.28,582.36,91.08,9.02"><row><cell>read accept move</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,177.24,635.28,257.57,9.02"><head>Table 6 :</head><label>6</label><figDesc>Percentages of actions rated under different categories.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,175.32,109.44,261.39,125.54"><head>Table 7 :</head><label>7</label><figDesc>Percentages of answers rated under different categories.</figDesc><table coords="8,192.30,109.44,227.37,125.54"><row><cell></cell><cell>good</cell><cell>bad</cell><cell>cannot</cell><cell>partially good</cell></row><row><cell></cell><cell>answers</cell><cell>answers</cell><cell>decide</cell><cell>answers</cell></row><row><cell>recent</cell><cell>52.6</cell><cell>50.2</cell><cell>43.3</cell><cell>45.3</cell></row><row><cell>older</cell><cell>47.4</cell><cell>49.8</cell><cell>56.7</cell><cell>54.7</cell></row><row><cell></cell><cell></cell><cell cols="3">read accept move</cell></row><row><cell></cell><cell>recent</cell><cell>44.7</cell><cell>54.7</cell><cell>51.2</cell></row><row><cell></cell><cell>older</cell><cell>55.3</cell><cell>45.3</cell><cell>48.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="8,177.24,250.32,257.57,9.02"><head>Table 8 :</head><label>8</label><figDesc>Percentages of actions rated under different categories.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="8,131.46,496.44,349.06,80.60"><head>). not much (n=8) same as most (n=17) know a lot (n=5) average</head><label></label><figDesc></figDesc><table coords="8,131.46,531.90,330.48,45.14"><row><cell>good</cell><cell>32.14%</cell><cell>71.04%</cell><cell>54.86%</cell></row><row><cell>average poor</cell><cell>7.14%</cell><cell>14.78%</cell><cell>34.64%</cell></row><row><cell>average cannot decide</cell><cell>37.50%</cell><cell>7.84%</cell><cell>4.00%</cell></row><row><cell>average partially</cell><cell>23.21%</cell><cell>6.34%</cell><cell>6.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="8,99.18,591.42,413.61,68.60"><head>Table 9 :</head><label>9</label><figDesc>Knowledge and answer quality. Highest value within each knowledge category shown in bold</figDesc><table coords="8,165.06,615.00,281.86,45.02"><row><cell></cell><cell cols="3">not much (n=8) same as most (n=17) know a lot (n=5)</cell></row><row><cell>read</cell><cell>69.64%</cell><cell>39.68%</cell><cell>5.00%</cell></row><row><cell>accept</cell><cell>21.43%</cell><cell>41.10%</cell><cell>54.86%</cell></row><row><cell>move</cell><cell>8.93%</cell><cell>19.22%</cell><cell>40.14%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="8,94.26,674.40,423.44,20.54"><head>Table 10 :</head><label>10</label><figDesc>Knowledge and predicted next action. Highest value within each knowledge category shown in bold</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="9,94.50,224.40,422.96,151.52"><head>Table 11 :</head><label>11</label><figDesc>Prior expectation and answer quality. Highest value within each expectation category shown in bold</figDesc><table coords="9,129.60,224.40,352.69,151.52"><row><cell></cell><cell cols="5">yes (n=4) no but idea (n=7) partially (n=15) no (n=4)</cell></row><row><cell>average good</cell><cell>68.75%</cell><cell>65.31%</cell><cell></cell><cell>52.78%</cell><cell>33.93%</cell></row><row><cell>average bad</cell><cell>6.25%</cell><cell>16.90%</cell><cell></cell><cell>32.64%</cell><cell>0.00%</cell></row><row><cell cols="2">average cannot decide 12.50%</cell><cell>11.33%</cell><cell></cell><cell>9.72%</cell><cell>34.38%</cell></row><row><cell>average partially</cell><cell>12.50%</cell><cell>6.45%</cell><cell></cell><cell>4.86%</cell><cell>31.70%</cell></row><row><cell cols="6">yes (n=4) no but idea (n=7) partially (n=15) no (n=4)</cell></row><row><cell>read</cell><cell>36.88%</cell><cell>39.17%</cell><cell>24.17%</cell><cell>76.79%</cell></row><row><cell cols="2">accept 53.75%</cell><cell>38.01%</cell><cell>40.83%</cell><cell>23.21%</cell></row><row><cell>move</cell><cell>9.38%</cell><cell>22.83%</cell><cell>35.00%</cell><cell>0.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="9,101.34,390.30,409.37,20.54"><head>Table 12 :</head><label>12</label><figDesc>Prior expectation and predicted next action. Highest value within each expectation category shown in bold</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="10,90.72,204.72,430.49,144.38"><head>Table 13 :</head><label>13</label><figDesc>Question type and answer quality. Highest value within each expectation category shown in bold</figDesc><table coords="10,134.16,204.72,337.14,144.38"><row><cell></cell><cell></cell><cell cols="5">relationship effect position evidence</cell></row><row><cell>average good</cell><cell></cell><cell>51.79%</cell><cell>23.61%</cell><cell cols="3">80.24% 56.55% 80.95%</cell></row><row><cell>average bad</cell><cell></cell><cell>10.71%</cell><cell>28.47%</cell><cell></cell><cell cols="2">6.55% 22.62% 12.50%</cell></row><row><cell cols="2">average cannot decide</cell><cell>27.50%</cell><cell>28.47%</cell><cell></cell><cell cols="2">5.42% 10.42%</cell><cell>2.08%</cell></row><row><cell>average partially</cell><cell></cell><cell>10.00%</cell><cell>19.44%</cell><cell></cell><cell cols="2">7.80% 10.42%</cell><cell>4.46%</cell></row><row><cell></cell><cell cols="3">transport relationship effect</cell><cell></cell><cell>position</cell><cell>evidence</cell></row><row><cell>read</cell><cell>57.50%</cell><cell>57.50%</cell><cell cols="3">44.52% 25.00%</cell><cell>22.92%</cell></row><row><cell>accept</cell><cell>24.17%</cell><cell>11.67%</cell><cell cols="3">40.48% 50.30%</cell><cell>64.58%</cell></row><row><cell>move</cell><cell>18.33%</cell><cell>30.83%</cell><cell cols="2">15.00%</cell><cell>24.70%</cell><cell>12.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="10,93.36,363.48,425.35,20.54"><head>Table 14 :</head><label>14</label><figDesc>Question type and predicted next action. Highest value within each expectation category shown in bold</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,98.10,664.86,427.16,9.02;3,90.00,676.32,83.77,9.02"><p>where [relationship] is an element of {"financial relationships", "organizational ties", "familial ties", "common interests"}</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,96.12,687.84,23.40,9.02"><p>Topic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_2" coords="3,135.25,687.84,386.74,9.02;3,90.00,699.36,432.03,9.02;3,90.00,710.82,284.77,9.02"><p>was agreed to have 3 facets, topics26, 27, 29, 30, 33, 34, 35, 38, 41, 42, 44, 45, 46, 48, 49, 50, and 51 had 2 facets and the remaining topics had only 1 facet. We found no correlation between the number of entities in the question and number of facets in the topic description.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="4,96.36,687.84,425.66,9.02;4,90.00,699.36,432.22,9.02;4,90.00,710.82,194.47,9.02"><p>The colour of these fields may not appear very strong in this paper version. After initial pilot testing we reached a balance between contrast of information and visual separation of answers to ciqa questions (offwhite) and questions to the assessor (yellow)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,104.32,491.10,417.91,9.02;10,90.00,502.62,432.15,9.02;10,90.00,514.14,20.01,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,252.86,491.10,269.36,9.02;10,90.00,502.62,78.94,9.02">The effects on topic familiarity on online search behaviour and use of relevance criteria</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Borlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,178.44,502.62,286.22,9.02">Proceedings of the 28th European Conference in Information Retrieval</title>
		<meeting>the 28th European Conference in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">2006</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.05,525.60,413.96,9.02;10,90.00,537.12,208.66,9.02" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,165.56,525.60,356.46,9.02;10,90.00,537.12,86.64,9.02">Effects of search experience and subject knowledge on the search tactics of novice and experienced searchers</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Hsieh-Yee</surname></persName>
		</author>
		<idno>JASIS. 44. 3. 161-174. 1993</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.44,548.58,417.51,9.02;10,90.00,560.10,273.31,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,150.51,548.58,371.44,9.02;10,90.00,560.10,152.91,9.02">What is Used During Cognitive Processing in Information Retrieval and Library Searching? Eleven Sources of Search Information</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,247.26,560.10,21.74,9.02">JASIS</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="498" to="514" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.84,571.62,417.19,9.02;10,90.00,583.08,297.69,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,211.37,571.62,310.67,9.02;10,90.00,583.08,168.58,9.02">The anticipated and assessed contribution of information types in references retrieved for preparing a research proposal</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Serola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vakkari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,265.18,583.08,29.27,9.02">JASIST</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="373" to="381" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.00,594.60,417.05,9.02;10,90.00,606.12,225.73,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,273.02,594.60,249.02,9.02;10,90.00,606.12,76.40,9.02">The relative effects of knowledge, interest and confidence in assessing relevance</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Baillie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Elsweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,172.92,606.12,102.40,9.02">Journal of Documentation</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="10,105.08,617.58,416.99,9.02;10,90.00,629.10,278.96,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,231.62,617.58,285.99,9.02">Users&apos; criteria for relevance evaluation: a cross-situational comparison</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schamber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,90.00,629.10,165.35,9.02">Information, Processing and Management</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="219" to="237" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
