<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,107.48,112.05,397.06,15.12;1,189.39,133.97,233.22,15.12">Partitioning the Gov2 Corpus by Internet Domain Name: A Result-set Merging Experiment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-11-15">November 15, 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,189.52,166.45,110.78,10.48"><forename type="first">Christopher</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
							<email>fallen@arsc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center Fairbanks</orgName>
								<address>
									<region>AK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.97,166.45,95.51,10.48"><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center Fairbanks</orgName>
								<address>
									<region>AK</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,107.48,112.05,397.06,15.12;1,189.39,133.97,233.22,15.12">Partitioning the Gov2 Corpus by Internet Domain Name: A Result-set Merging Experiment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-11-15">November 15, 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">7617D0FFDFE3C827904B8F066D3EADAA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To study the MultiSearch problem and complete the Ad Hoc Task of the 2006 TREC Terabyte Track, the Gov2 collection was divided according to web domain and for each topic, the results from each domain were merged into single ranked list. The mean average precision scores of the results from two different merge algorithms applied to the domain-divided Gov2 collection and a randomized domain-divided collection are compared with a 2-way analysis of variance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result-set Merging</head><p>Modern IR tools need to operate on document collections of enormous size and scope. The size of the collection and the distributed nature of the information itself often requires a divide and conquer approach where the collection is divided into smaller sub-collections and IR operations like keyword search are applied to the smaller sub-collections individually before merging into a single ranked list.</p><p>The number of possible ways to divide a collection of any reasonable size is always astronomical. Sometimes the divisions are arbitrary with respect to the content of the documents for purposes of load balancing. Or the divisions reflect the network topology of the machines that store the data. One way to divide the set of web pages found in nature is by web domain. If a distributed IR application were used to search each web domain individually, how should the results from each domain be weighted so that the results can be merged and presented to the end user or trec eval?</p><p>The purpose of this study is to build an experiment using the TREC Terabyte Track topics and the Gov2 corpus to determine the relative IR performance of two result set merging techniques in a simulated Internet-scale Grid Information Retrieval (GIR) environment. The model used to design the experiment is of a hypothetical GIR application that searches each of the approximately 17,000 unique web domains in the Gov2 corpus individually as if using a local search application provided by each domain and then merges the result sets into a single ranked list to be presented to the user or to be evaluated with trec eval. The web domains in the Gov2 corpus range in size from one to nearly one million pages so any merge method in this experimental framework needs to merge thousands of result sets with a similarly wide range of lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid Information Retrieval (GIR)</head><p>One of the main inspirations for this work is our continued interest in distributed information retrieval systems. The author and colleagues are involved with the Open Grid Forum's <ref type="bibr" coords="1,413.57,665.04,15.49,8.74" target="#b12">[12]</ref> standards working group on Grid Information Retrieval <ref type="bibr" coords="1,206.51,676.99,9.97,8.74" target="#b6">[6]</ref>, seeking to produce standard methods for IR systems to interpolate. GIR spans several major themes: distributed indexing, transport methods for queries and result sets, human interface, and methods for query persistence. For TREC purposes, though, the emphasis is on resultsets merging. The issue is that different IR systems, with different collections or subcollections, each produce their own results for a given query in a distributed IR system. How can these different sets of results (each ordered by relevance, as produced by the independent IR systems), be effectively unified into one set?</p><p>Rather than ranking arbitrarily, the goal is to produce a single relevance-ranked set, with ordering that indicates the relative relevance of each document, regardless of which IR system it came from. In this paper, we present our exploration into doing this relevance re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment Overview: Apparatus Machines</head><p>A standalone server and a Cray XD1 system were employed this year for indexing and searching. An additional pair of servers was used for testing and development of the MultiSearch capability described in the previous section.</p><p>For development and initial testing, a dual Xeon 64-bit system was used. This system, from ASL Inc., features 8GB of memory and 3.6TB of disk space, running Linux. The large disk space allowed for several different iterations of the indexes to be built, during early deployment of the experimental methodology described below. This system built many of the indexes, which were later copied to the larger system.</p><p>The larger system is a Cray XD1, "nelchina." Nelchina features 108 2.6Ghz Opteron processor cores with 4GB of memory each, the PBS Pro scheduler, and Cray's variation on the SuSE Linux operating system. A disk subsystem, provided by Direct Data Networks, provides 18TB of high performance disk space for temporary storage. By experimentation, we found that about six simultaneous indexing or searching threads could operate simultaneously, before the aggregate performance would decrease (keep in mind that each searching thread would need to open about 17000 separate Amberfish indexes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amberfish</head><p>Amberfish is a free command-line based general-purpose text retrieval tool developed by Nassib Nassar and is described at the Etymon Systems website <ref type="bibr" coords="2,272.91,435.69,14.62,8.74" target="#b11">[11]</ref>. The performance of Amberfish in the TREC Terabyte is described by Nassar in the 2004 TREC proceedings <ref type="bibr" coords="2,318.01,447.64,15.50,8.74" target="#b10">[10]</ref> and by Fallen and Newby in the 2005 TREC proceedings <ref type="bibr" coords="2,126.54,459.60,9.97,8.74" target="#b4">[4]</ref>. Version 1.5.10 of Amberfish was used without modifications in this 2006 TREC Terabyte Track experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Partitions</head><p>A partition of the set of documents in the Gov2 corpus is a collection of disjoint subsets that contains the original set <ref type="bibr" coords="2,137.15,529.79,10.93,8.74" target="#b9">[9]</ref>. Two different partitions were constructed at index time, the domain partition and the randomized domain partition, described below. Results from the domain partition were submitted as official TREC runs and results from the randomized domain partition are used for statistical comparison in order to quantify the effects of partitioning a collection in different ways on the performance of the result-set merging algorithm.</p><p>Let G be the set of documents in the Gov2 corpus and let G be a partition of G. Then the elements S i of G are subsets of G such that S i ∩ S j = ∅ for each i and j so the subsets are pairwise disjoint and G ⊆ i S i so the subsets cover G. The cardinality or size of a set S is defined as the number |S| of elements or documents in the set. Note that this formulation of the distributed IR problem excludes the common situation when multiple IR systems crawl a single large dataset and likely index many of the same documents.</p><p>The subsets or subcollections of the domain partition G d are sets of pages with a common domain name. That is, if d i and d j are two documents in the subset S that is itself an element of G d , then the d i and d j are documents with identical domain names in their URLs. One goal of the experiment is to try and isolate the IR measurable effects of grouping related pages together at index-time -where d i is "related" to d j in this context means d i is "in the same domain as" d j -by comparing results retrieved from collection indexes defined by G d and indexes defined by a partition G r with the same subset size distribution as G d but with pages assigned to each subset at random. The partition G r is defined as follows.</p><p>From the first web domain subset S 1 in G d , form a subset R 1 of |S 1 | unique pages drawn at random from G. Form the second subset R 2 with |S 2 | pages drawn from G that are not already in R 1 . In this manner construct the partition G r inductively by filling R i with |S i | pages such that</p><formula xml:id="formula_0" coords="3,269.45,169.98,270.56,19.91">R i ⊆ G - j&lt;i R j .<label>(1)</label></formula><p>There are very good, practical reasons why this procedure of dividing the Gov2 collection by domains before indexing is not often used; one is that there are very many domains on the Internet and a few of them are very big. Consequently, search time on each of the large domains and over the many small domains are affected significantly for the worse. The second problem with this partitioning scheme is closely related to one of the fundamental problems of result set merging across IR systems: weighting results from one collection against results from another collection without downloading the documents themselves or using a priori characterizations of each collection searched is not straightforward. Dividing an Internet-scale collection by domains amplifies this second problem significantly so in a sense, partitioning by domain is an instructive model to explore problems in distributed information retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domains in the Gov2 Corpus</head><p>The Gov2 corpus contains approximately 17, 000 unique domains, tens of domains contain multiples of 100, 000 TREC IDs each; thousands of domains contain less than 10 pages each; and in Figure <ref type="figure" coords="3,485.18,354.88,4.98,8.74" target="#fig_0">1</ref> a rank-size plot illustrates that the size distribution between these two extremes looks to be linear on a log-log scale implying a power-law fit to the rank-size curve. Both of the partitions G d and G r used to construct indexes have size distributions illustrated in the figure. Power-law distributions found in collections of documents from the web are not new or surprising. The distribution of the number of links pointing to or from each document in a collection is known to be a power-law <ref type="bibr" coords="3,295.64,414.66,10.52,8.74" target="#b1">[1]</ref>[5] as is the distribution of domain sizes on the Internet <ref type="bibr" coords="3,72.00,426.62,9.97,8.74" target="#b8">[8]</ref>. In this paper "size distribution" is used colloquially to refer to the rank-size ordered distribution whose graph looks similar to but represents different information than a power-law probability distribution <ref type="bibr" coords="3,506.98,438.57,12.05,8.74" target="#b7">[7]</ref>.</p><p>A practical description of the domain size distribution is that there are a few domains in Gov2 with an enormous number of pages and there are an enormous number of domains with an effectively infinitesimal number of pages. Another important characteristic of the domain-size distribution is that the variance of the elements of a power-law distribution is unbounded so that the descriptive statistic "mean domain size" may not be very descriptive at all in the sense that as domains are added to a sample of a collection, the mean domain size will exhibit a random walk: each large domain is large enough to significantly increase the mean domain size no matter the size of the sample. Similarly, there are enough small domains added to the sample between the large domains to significantly decrease the mean.</p><p>The names of the largest and smallest domains, along with a selection of mid-sized domains are listed in Table <ref type="table" coords="3,111.11,558.12,3.88,8.74">1</ref>. Note that many of the large domains are library sites or repositories of specialized information, the smallest domains are little more than placeholder or temporary pages, and the medium-sized sites are more varied. Therefore it may be useful to craft unique GIR search, filter, and merge algorithms to specific ranges of domain sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Method and Result-set Merge Algorithms</head><p>The Gov2 corpus was split into individual document files and stripped of the TREC headers. To index the domain partition, a separate file constructed from the TREC headers was used to associate TREC IDs to domain names. For each domain, a list was created containing the TREC IDs contained in the domain. Using </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large</head><p>Medium Small 717,321 ghr.nlm.nih.gov the command-line interface to Amberfish, a single inverted index for the documents in each list (domain) was created from an "index worker" script that indexed each domain in serial by selecting domain lists from a queue of unindexed domains.</p><p>The randomized domain partition was indexed by the same scripts used on the domain partition after each randomized domain list was constructed by counting the number of documents in each domain list and drawing that many pages without replacement and at random from the master list of TREC IDs. The filenames of each domain list and associated Amberfish index file contained a number that allowed the actual domain name to be referenced in later analysis; the filenames of each randomized domain list and index were identified with the same number as the corresponding domain list. As will be described below in the discussion of results, the choice of index file filenames potentially biased the results by weighting domains whose filenames appear at the top of a GNU ls listing more than those domains whose filenames appear later.</p><p>At search time, the list of topics was divided and the parts were assigned to "search worker" scripts that, for each topic, called the Amberfish search program on every index corresponding to a domain in a partition. The results from each topic and index (domain) pair were saved to a text file in TREC format and named according to the topic and index name. When the search worker script had searched every index in the partition, the results were for a topic were merged into a single ranked list using merge algorithms described below. By saving the results for each topic and domain pair in separate files, the performance of different new merge algorithms can be compared without searching the collection anew.</p><p>The sort-merge algorithm used as a comparison baseline is the GNU sort command applied to the document score column of all the TREC result files in a partition for a given topic. The log-merge algorithm is the same algorithm used by the authors in TREC 2005 Terabyte track <ref type="bibr" coords="5,396.23,326.22,10.52,8.74" target="#b4">[4]</ref> and is inspired by the logistic regression algorithm described by Savoy, Calvé, and Vrajitoru <ref type="bibr" coords="5,335.74,338.17,16.23,8.74" target="#b13">[13]</ref> <ref type="bibr" coords="5,351.97,338.17,12.17,8.74" target="#b3">[3]</ref>. In their papers, the score vs. log(rank) data for each result set to be merged is first fit to a logistic curve using the method of maximum likelihood to estimate the parameters of the best-fit logistic function and then each document in a result-set is merged into a single ranked list according to its height on the logistic curve evaluated at the log of its document rank in the original result-set. In the log-merge algorithm used here, linear least squares is applied to a modified logistic function to estimate the parameters of the logistic function.</p><p>Let {(x i , y i , d i )} be a result set for a topic from a single domain. The i th ranked document is d i , x i = log i, and y i is a monotonically decreasing sequence of relevance scores. The task is to assign parameters l and m such that the logistic function</p><formula xml:id="formula_1" coords="5,261.48,443.63,278.53,23.89">f l,m (x) = e l+mx 1 + e l+mx<label>(2)</label></formula><p>best fits the result set in the least squares sense of minimizing the sum of the squares of the residuals. A simpler problem is solved instead by taking the logarithm of the equation above</p><formula xml:id="formula_2" coords="5,217.22,505.30,322.78,11.72">(log •f ) l,m (x) = l + mx -log(1 + e l+mx )<label>(3)</label></formula><p>to get an expression that very nearly describes a line, at least when applied to the IR data sets of interest. Then a straightforward application of a linear least squares routine yields estimates for the intercept and slope of the best-fit line. The value of the parameter m can be set to the estimated slope directly and the parameter l can be found by setting x = 0 and solving the equation l -log(1 + e l ) set to the estimated intercept. A new set of log-normalized scores for each result-set is calculated from f l,m and the result-sets are merged in descending order with respect to the new log-normalized scores. data structures used for the TREC Terabyte Track experiment resided on the Lustre Cluster File System (CFS) so the I/O bandwidth available at the time of the experiment depends on the number and type of jobs submitted by multiple users and as such it is difficult to accurately measure TREC Terabyte Track system performance on the XD1 without dedicating the entire machine. Both index and search operations were submitted in stages through batch jobs and the number of jobs running in parallel may also affect the the performance of each job individually. In addition, the index process was interrupted by system downtime. Consequently, the index and search times reported for the Cray XD1 are only rough estimates and no attempt was made to either optimize or carefully assess system performance. Regardless, the times required to index 426 GB reported here are rather large and a significant cause of this is the one-to-one correspondence of domains to Amberfish index files: some domains are very large and many domains are very small. Each node of the Cray XD1 used in this study contains two 64-bit AMD Opteron 250 processors that share 8 GB of RAM <ref type="bibr" coords="6,160.68,294.62,9.96,8.74" target="#b2">[2]</ref>. Two nodes with three to four concurrent Amberfish processes per node were used to index the domain partition of the Gov2 collection used in the Ad Hoc task. Each "index worker" processes selected domains to index from a queue. For informal performance comparison, a generic dual Xeon from ASL with two processors at 3.8 Ghz and hyperthreading enabled, and 8 GB of RAM was used to index the randomized domain partition using three to four concurrent Amberfish processes as on the Cray XD1. The ASL system has a RAID5 array with approximately 3.6TB of space in 13 drives, using SATA-II drives and controllers. Downtime and unknown system loads from other users will also affect the accuracy of the index times reported for the ASL machine.</p><p>For the automatic search run, a total of four concurrent Amberfish "search workers" were started on two dual processor nodes on the XD1. The list of topics was divided among the search workers and each worker processed the topics sequentially. For each topic, the search worker ran the Amberfish command on each domain individually and the top 1000 results from each domain were saved to a file in official TREC result format. After a search worker recorded results from every domain for a given topic, the result files were merged into a single ranked list using the GNU sort command applied to the relevance score and then merged again to another ranked list using the logistic normalization procedure described above. By saving the results for a topic from each domain individually, merge algorithms based on the information contained in the official TREC format result-sets and domain-specific meta information can be tested and compared using relatively modest hardware without searching the collection again.</p><p>This flexibility with respect to result-set merging experiments comes at the price of certain end-user features. For instance, restricting search results for a topic to the top 20 hits will not significantly improve performance with this system because the process of opening each of the 17,000 index files corresponding to the web domains contributes significantly to the total search time. Some performance improvement may be noticed if only the top 20 results from each domain are saved because some of the domains are very large, but this will also affect the IR performance of the log-merge algorithm because the relevance scores from every result in a result set are used in the renormalization procedure. Consequently, the official search times reported are the times to return up to 10,000 documents per topic.</p><p>For the manual search run, rather complex queries were constructed from the topic descriptions with Boolean AND, OR, and NOT. Hence, the manual queries contained more terms on average than the automatically constructed queries and this impacted search performance over the larger domains. In order to complete the manual run before the submission deadline, four dual processor nodes were used instead of the two used in the automatic run but otherwise search and merge procedure was the same as described above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IR Performance</head><p>Similar to the published results in the TREC 2005 Terabyte track <ref type="bibr" coords="7,370.88,230.42,9.96,8.74" target="#b4">[4]</ref>, the trec eval IR measures of the results returned by Amberfish are well below the TREC participant median which is to be expected as Amberfish does not use keyword expansion or other sophisticated techniques beyond Boolean set operations to retrieve results. Searching across a very large number of indexes split according to web domain is not likely to help the results either as information like relative frequencies of search terms in two documents are not going to available if the two documents are from different domains. However, the goal of this study is to investigate the effectiveness of result-set merge algorithms as applied to a large distributed information retrieval system. In both the automatic query extraction run on topics 701-850 and the manual query construction run on topics 801-850, the mean values of the mean average precision (MAP) calculated by version 8.1 of trec eval, recorded in Table <ref type="table" coords="7,151.91,349.97,3.88,8.74" target="#tab_2">4</ref>, for the log-merged results are larger than the corresponding MAP for the sort-merged results. Note that the standard deviation of the MAP in the automatic runs is greater than the mean value of the MAP itself. This slight performance improvement of the log-merged results over the sort-merged results on a web-domain partitioned collection is consistent with the results observed in the TREC 2005 when the collection was divided arbitrarily into a small number of subcollections with uniform size.</p><p>To quantify the effects on IR performance due to the merge methods used as well as the effects due to eliminating the natural corpus structure defined by web domains by dividing the corpus arbitrarily with respect to the document content at index-time, the mean values of the MAP taken over the merged resultsets from 149 automatically extracted queries applied to the domain partition and the randomized domain partition are recorded in Table <ref type="table" coords="7,205.90,457.57,3.88,8.74" target="#tab_3">5</ref>. As observed in the official TREC results from 2005 and 2006, the log-merge method outperforms the sort-merge method regardless of whether the underlying collection is partitioned by web domain or partitioned by randomized web domains. The MAP scores from the log-merge and sort-merge methods applied to the randomized domain partition are slightly better than the respective MAP scores for the domain partition but this difference is not likely to be significant as determined by the results of the 2-way analysis of variance reported in Table <ref type="table" coords="7,271.91,517.34,3.88,8.74" target="#tab_4">6</ref>. Note that the mean and median MAP values reported in Table <ref type="table" coords="7,99.40,529.30,4.98,8.74" target="#tab_3">5</ref> imply that the normality assumption in the ANOVA test is not satisfied by this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>From the analysis of variance referenced above, it is clear that randomly distributing pages among a fixed domain size distribution does not significantly affect search performance. Nonetheless, when the number of domains that returned results for each topic is plotted as in Figure <ref type="figure" coords="7,383.06,635.49,3.88,8.74" target="#fig_2">2</ref>, the number of "domain hits" per topic is consistently larger for the randomized partition than for the domain partition. This suggests that the retrieved results, at least those found by Amberfish, are concentrated in a smaller number of Internet domains than would be expected if the results were distributed randomly over the domains. If relevant retrieved results also exhibit a similar distribution, then identifying those domains that contain a larger   Seemingly little information is available from the relevance score vs. rank profile used by the log-merge method. Nonetheless, the log-merge method does significantly improve result-set merging performance relative to a straightforward sort operation on relevance scores. However, observations made during the experiment suggest that the log-merge method is not particularly well suited to a domain-partitioned collection or the relevance scoring algorithm used by Amberfish. For instance, many domain result-sets contained very few results; many domains returned a single result per query. And since Amberfish does not return cosine-like relevance scores but instead scores the top ranked document in a result-set at 10, 000 regardless of the actual similarity of the top-ranked document to the query, there is little variability among log-normalized scores across domains returning few hits because very few points are used to fit the logistic curve. So for the worst case of merging across many result-sets containing a single result, "relevance" is assigned according to the order in which the result-sets are merged because every result has the same relevance score. In this experiment each result-set was saved to its own file before merging so the merge order was effectively determined by ls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Partitioning a large Terabyte-scale collection of web pages by Internet domain name is not a good design for a monolithic IR application for both machine performance and IR performance considerations. However, the web itself is naturally partitioned by Internet domain name so a distributed GIR application may need to deal with the complications described above anyway. In particular, methods for assigning relative relevance scores to results from different collections that are robust with respect to the number of results in a result-set are needed. If the IR application used returns cosine-like relevance scores then the log-merge method may in fact be robust in that sense. Given the wide range of domain sizes, perhaps different merge methods designed for particular ranges of domain sizes may be more effective than the single merge method approach used here. An in-depth study of the concentrations of the relevant results in each domain for a set of typical queries may also yield additional meta-information useful for result-set merging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,124.76,421.32,361.53,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sizes of web domains in the gov2 corpus as measured by TREC ID count</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,183.74,364.03,4.45,7.40;9,264.75,364.03,8.90,7.40;9,346.50,364.03,13.35,7.40;9,430.51,364.03,13.35,7.40;9,170.99,357.28,8.90,7.40;9,179.24,353.46,3.34,5.55;9,170.99,304.02,8.90,7.40;9,179.24,300.21,3.34,5.55;9,170.99,251.52,8.90,7.40;9,179.24,247.70,3.34,5.55;9,170.99,198.26,8.90,7.40;9,179.24,194.45,3.34,5.55;9,170.99,145.76,8.90,7.40;9,179.24,141.94,3.34,5.55;9,170.99,93.25,8.90,7.40;9,179.24,89.44,3.34,5.55;9,292.50,374.53,37.35,7.40;9,158.50,219.58,7.40,40.68;9,158.50,197.79,7.40,19.57;9,234.00,85.00,156.08,7.40;9,228.75,315.27,136.05,7.40;9,228.75,325.77,181.86,7.40;9,228.75,336.28,131.16,7.40;9,228.75,346.78,176.97,7.40"><head></head><label></label><figDesc>count vs. Topic domain-hit rank Domain partition, terms joined by AND Randomized domain partition, terms joined by AND Domain partition, terms joined by OR Randomized domain partition, terms joined by OR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,72.00,411.14,468.01,8.74;9,72.00,423.09,169.45,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Size vs. rank plot of the number of domains returning hits for queries constructed automatically from TREC Terabyte Track topic titles</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,618.98,468.00,65.93"><head>Table 2 :</head><label>2</label><figDesc>Amberfish index performance</figDesc><table coords="5,72.00,618.98,301.46,35.97"><row><cell>3 Experiment Results: The Ad Hoc Task</cell></row><row><cell>System Performance</cell></row></table><note coords="5,72.00,664.22,467.99,8.74;5,72.00,676.17,468.00,8.74"><p>The Cray XD1 at the Arctic Region Supercomputing Center is primarily used as a shared resource where individual processing nodes are allocated through the PBS Pro batch scheduler. Corpus data and index</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,94.51,73.53,422.97,115.51"><head>Table 3 :</head><label>3</label><figDesc>Amberfish Ad Hoc Task search performance on the Cray XD1 and the domain partition.</figDesc><table coords="7,131.61,73.53,348.78,94.05"><row><cell></cell><cell cols="2">Total search time</cell><cell></cell></row><row><cell>Run Tag</cell><cell>Wall</cell><cell>CPU</cell><cell cols="2">Throughput Merge method</cell></row><row><cell cols="5">149 queries extracted automatically by joining topic title terms with AND</cell></row><row><cell>arscDomAlog</cell><cell cols="3">2,000 [min] 8,000 [min] 13 [min/topic]</cell><cell>Log</cell></row><row><cell>arscDomAsrt</cell><cell cols="3">2,000 [min] 8,000 [min] 13 [min/topic]</cell><cell>Sort</cell></row><row><cell></cell><cell cols="3">50 queries constructed manually from topics 801-850</cell></row><row><cell cols="4">arscDomManL 2,500 [min] 20,000 [min] 50 [min/topic]</cell><cell>Log</cell></row><row><cell cols="4">arscDomManS 2,500 [min] 20,000 [min] 50 [min/topic]</cell><cell>Sort</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.00,115.93,468.00,300.73"><head>Table 4 :</head><label>4</label><figDesc>Official 2006 TREC Terabyte Track Results: Average Precision scores averaged over the topics indicated</figDesc><table coords="8,112.04,115.93,387.92,300.73"><row><cell></cell><cell></cell><cell></cell><cell cols="3">Mean Standard Deviation</cell></row><row><cell></cell><cell cols="4">149 queries extracted automatically from topics 701-850</cell></row><row><cell></cell><cell>arscDomAlog</cell><cell></cell><cell>0.0571</cell><cell>0.0801</cell></row><row><cell></cell><cell>arscDomAsrt</cell><cell></cell><cell>0.0468</cell><cell>0.0676</cell></row><row><cell></cell><cell cols="3">TREC Participant median 0.2922</cell><cell>0.1725</cell></row><row><cell></cell><cell cols="4">50 queries constructed manually from topics 801-850</cell></row><row><cell></cell><cell>arscDomManL</cell><cell></cell><cell>0.0351</cell><cell></cell></row><row><cell></cell><cell>arscDomManS</cell><cell></cell><cell>0.0278</cell><cell></cell></row><row><cell></cell><cell cols="3">TREC Participant median 0.2436</cell><cell>0.1464</cell></row><row><cell></cell><cell></cell><cell cols="2">Average Precision</cell><cell cols="2">Total Relevant</cell></row><row><cell>Partition</cell><cell cols="5">Merge method Mean Median Stdv. Documents Returned</cell></row><row><cell>Domain</cell><cell>Log</cell><cell>0.0571</cell><cell cols="2">0.0240 0.0801</cell><cell>15,484</cell></row><row><cell>Domain</cell><cell>Sort</cell><cell>0.0468</cell><cell cols="2">0.0201 0.0676</cell><cell>14,987</cell></row><row><cell>Randomized</cell><cell>Log</cell><cell>0.0624</cell><cell cols="2">0.0304 0.0790</cell><cell>15,112</cell></row><row><cell>Randomized</cell><cell>Sort</cell><cell>0.0485</cell><cell cols="2">0.0209 0.0639</cell><cell>14,853</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,72.00,429.39,468.00,184.34"><head>Table 5 :</head><label>5</label><figDesc>Mean Average Precision and number of relevant documents retrieved from the 26,917 document TREC relevant document pool. Statistics calculated over 149 topics with automatically extracted queries.</figDesc><table coords="8,221.75,544.40,168.51,69.34"><row><cell></cell><cell>SS</cell><cell>MS</cell><cell>p</cell></row><row><cell>Columns</cell><cell cols="3">0.0216 0.0216 0.0445</cell></row><row><cell>Rows</cell><cell cols="3">0.0019 0.0019 0.5508</cell></row><row><cell cols="4">Interaction 0.0005 0.0005 0.7609</cell></row><row><cell>Error</cell><cell cols="2">3.1561 0.0053</cell></row><row><cell>Total</cell><cell>3.1801</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,72.00,627.02,468.00,20.69"><head>Table 6 :</head><label>6</label><figDesc>Results from a 2-way ANOVA test with 149 repetitions (topics) on MAP scores with merge method represented by columns and partition scheme represented by rows.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,178.16,671.16,282.25,8.74;10,72.00,234.48,75.53,12.62" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,190.34,671.16,270.08,8.74;10,72.00,234.48,75.53,12.62">A selection of domain names and associated TREC ID counts References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,259.34,447.52,8.74;10,92.48,271.30,61.44,8.74" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hawoong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,357.49,259.34,139.51,8.74">Diameter of the world-wide web</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page">130</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,291.22,447.52,9.02;10,92.48,303.18,49.70,9.02" xml:id="b2">
	<monogr>
		<ptr target="http://www.arsc.edu/support/howtos/usingxd1.html" />
		<title level="m" coord="10,92.48,291.22,204.08,8.74">Introduction to using the Cray XD1 at ARSC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,323.10,447.51,8.74;10,92.48,335.06,197.84,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,245.61,323.10,234.48,8.74">Database merging strategy based on logistic regression</title>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Calvé</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,487.91,323.10,52.08,8.74;10,92.48,335.06,112.84,8.74">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,354.98,447.51,8.74;10,92.48,366.94,315.68,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,307.93,354.98,232.06,8.74;10,92.48,366.94,80.46,8.74">Logistic regression merging of amberfish and lucene multisearch results</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,193.70,366.94,183.83,8.74">The Fourteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,386.86,447.52,8.74;10,92.48,398.82,447.52,8.74;10,92.48,410.77,255.97,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,296.46,386.86,224.73,8.74">On power-law relationships of the internet topology</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,92.48,398.82,447.52,8.74;10,92.48,410.77,155.84,8.74">Proceedings of the conference on Applications, technologies, architectures, and protocols for computer communication ACM SIGCOMM&apos;99</title>
		<meeting>the conference on Applications, technologies, architectures, and protocols for computer communication ACM SIGCOMM&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="251" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,430.70,262.00,9.02" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Working</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Group</surname></persName>
		</author>
		<ptr target="http://www.gir-wg.org/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,450.63,447.52,8.74;10,92.48,462.58,337.24,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,323.83,450.63,216.17,8.74;10,92.48,462.58,53.58,8.74">Zipf&apos;s law and the effect of ranking on probability distributions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Levitin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schapiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,154.60,462.58,191.01,8.74">International Journal of Theoretical Physics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="395" to="417" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,482.51,447.52,8.74;10,92.48,494.46,61.44,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,307.42,482.51,187.18,8.74">Growth dynamics of the World-Wide Web</title>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lada</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,507.80,482.51,27.60,8.74">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,514.39,219.22,8.74" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Munkres</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Topology</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,534.31,447.52,8.74;10,92.48,546.27,78.44,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,159.53,534.31,177.71,8.74">Amberfish at the trec 2004 terabyte track</title>
		<author>
			<persName coords=""><forename type="first">Nassib</forename><surname>Nassar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,357.00,534.31,183.00,8.74;10,92.48,546.27,48.06,8.74">The Thirteenth Text REtrieval Conference Proceedings</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,566.19,345.87,9.02" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Nassib</forename><surname>Nassar</surname></persName>
		</author>
		<ptr target="http://www.etymon.com/tr.html" />
		<title level="m" coord="10,160.80,566.19,88.60,8.74">Etymon Systems Inc</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,586.12,229.17,9.02" xml:id="b12">
	<monogr>
		<ptr target="http://www.ogf.org" />
		<title level="m" coord="10,92.48,586.12,96.63,8.74">The Open Grid Forum</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,606.04,447.52,8.74;10,92.48,618.00,298.20,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,327.03,606.04,212.97,8.74;10,92.48,618.00,88.13,8.74">Report on the TREC-5 experiment: Data fusion and collection fusion</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><forename type="middle">Le</forename><surname>Calvé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dana</forename><surname>Vrajitoru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,201.56,618.00,158.50,8.74">The Fifth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
