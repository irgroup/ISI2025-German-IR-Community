<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.87,96.45,453.39,15.11">TREC-2006 at Maryland: Blog, Enterprise, Legal and QA Tracks</title>
				<funder ref="#_VJtTkgU">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.27,128.93,68.18,10.48"><forename type="first">Douglas</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.37,128.93,73.64,10.48"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
							<email>telsayed@umd.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.09,128.93,79.17,10.48"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName coords="1,421.57,128.93,47.36,10.48"><forename type="first">Yejun</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName coords="1,142.66,142.99,67.79,10.48"><forename type="first">Pengyi</forename><surname>Zhang</surname></persName>
							<email>pengyi@umd.edu</email>
						</author>
						<author>
							<persName coords="1,228.37,142.99,61.89,10.48"><forename type="first">Eileen</forename><surname>Abels</surname></persName>
							<email>eileen.abels@ischool.drexel.edu</email>
						</author>
						<author>
							<persName coords="1,303.38,142.99,53.77,10.48"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.79,142.99,82.43,10.48;1,484.22,141.38,1.88,6.99"><forename type="first">Dagbert</forename><surname>Soergel</surname></persName>
							<email>dsoergel@umd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Library and Information Science</orgName>
								<orgName type="department" key="dep2">College of Information Studies</orgName>
								<orgName type="institution" key="instit1">State University of New York at Buffalo</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Information Science and Technology</orgName>
								<orgName type="institution">Drexel University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.87,96.45,453.39,15.11">TREC-2006 at Maryland: Blog, Enterprise, Legal and QA Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">666559641CB330911B996FD546E75A6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2006, teams from the University of Maryland participated in the Blog track, the Expert Search task of the Enterprise track, the Complex Interactive Question Answering task of the Question Answering track, and the Legal track. This paper reports our results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Blog Track</head><p>Blogs are being hailed as fundamentally different from other Internet communication protocols (e.g., email, WWW), and as possessing a socially-transformative, democratizing potential <ref type="bibr" coords="1,419.34,322.78,9.96,8.74" target="#b8">[9]</ref>. Journalists see blogs as alternative sources of news and public opinion <ref type="bibr" coords="1,273.21,334.74,14.62,8.74" target="#b11">[12]</ref>. "Blogs tend to be impressionistic, telegraphic, raw, honest, individualistic, highly opinionated and passionate, often striking an emotional chord" <ref type="bibr" coords="1,452.26,346.69,14.62,8.74" target="#b12">[13]</ref>. Private individuals create blogs as a vehicle for self-expression and self-empowerment <ref type="bibr" coords="1,368.27,358.65,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,382.82,358.65,7.01,8.74" target="#b8">9]</ref>. Therefore the blogosphere is a huge information space of unstructured informal text in which opinions and attitudes are embedded.</p><p>People may have different information needs concerning opinions and attitudes. Political candidates may wish to know both the aggregate attitudes (or sentiments) toward them and which groups of people like/dislike them. Policy makers and journalists may want to know the whole spectrum of attitudes of stakeholders (including foreign countries) on an issue. Advertisers may want to know the change of aggregate attitudes after the advertisement is delivered to the targeted population. Individuals may want to know a certain celebrity's attitudes about an issue, or to find people who share their attitudes to have a discussion, or to find people who disagree with their attitudes to persuade them. Therefore opinion retrieval can be useful to many people.</p><p>Previously there have been opinion retrieval studies on small collections of news articles, blogs, and Web forums. However, the TREC-2006 Blog pilot track offered the first opportunity to create a large public test collection for evaluation of blog opinion search using a large collection of blogs. The pilot track has two tasks, a main task (opinion retrieval) which focuses on the opinionated nature of many blogs, and an open task which aims to determine a suitable second task for 2007 on other aspects of blogs. The University of Maryland participated in the opinion retrieval task, which involves locating blog posts that express an opinion about a given target. "The target can be a 'traditional' named entity -a name of a person, location, or organization -but also a concept (such as a type of technology), a product name, or an event." 1 In this first year of the track, NIST judges created 50 topics and performed relevance judgment. No training topics were available. Results from this year of the opinion retrieval track should therefore be considered preliminary-the main goal is to explore the design space for tasks and metrics and to create an initial test collection for use in formative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Methods</head><p>In our TREC-2006 opinion retrieval experiments, we tested the following ideas: <ref type="bibr" coords="1,427.65,632.07,14.13,8.74" target="#b0">(1)</ref>segmenting permalink documents to natural paragraphs and fixed sized passages to examine their difference in retrieval effectiveness;</p><p>(2)demoting the non-opinionated paragraphs along the retrieved ranked list; (3)query formulation using the title fields only versus using both the title and the description fields.</p><p>The blog collection was crawled over a period of 11 weeks (December 2005 -February 2006). The total size of the collection amounts to 23 GB (compressed) with three main components: feeds (8 GB), permalinks <ref type="bibr" coords="2,544.73,95.43,13.84,8.74;2,72.57,107.38,16.14,8.74">(11 GB)</ref>, and homepages (4 GB). The collection contains spam as well as possibly non-blogs, e.g. RSS feeds from news broadcasters. The distributed collection takes the form of a collection of organized, and uniquely identified XML feeds and their corresponding HTML blog posting pages. <ref type="foot" coords="2,347.40,129.72,3.97,6.12" target="#foot_0">2</ref>Since the retrieval units are the documents from the permalinks, we use the permalinks only. Each permalink file is basically a blog posting plus its comments (if any) embedded in a Web page with HTML markup. We used a perl program to remove javascript codes and then used the Lynx utility to strip off the HTML tags. Each html-stripped document contains a blog posting and possible comments surrounded by noisy information such as Web page navigation, advertising, and copyright. We sent out an email to the participants to call for a joint effort to clean the collection but received little passion and few responses. Since each blog hosting site has its own pattern of web page design, we by ourselves can clean only a very limited number of such Web pages. By taking a look at the permalinks.txt file under each of the 71 folders (from 20051206 to 20060221) which records the permalinks of blog postings and comments, we roughly estimated the frequency of each blog hosting site and selected the top 5 sites (i.e., livejournal.com, spaces.msn.com, xanga.com, blogs.msdn.com, 6-allthe.info) for further cleaning, which accounts for approximately 10% of the total number of permalinks. We then manually examined 100 documents from each of the 5 sites trying to find patterns for removing noisy information on the permalink Web pages. Here are some example patterns for cleaning livejournal.com: -remove anything beneath "Log in now." -remove anything between "Page Summary" and a date pattern such as "November 24th, 2005." -remove anything between the following hint strings and a string starting with a *: "Previous Posts[ :]", "Recent blog posts[ :]", "Recent Posts[ :]", "Categories[ :]", "Recent News[ :]", "Advertising[ :]", "Blogs I read[ :]"; "Documents I liked <ref type="bibr" coords="2,172.39,346.48,13.84,8.74">[ :]</ref>." Here [ :] means either a space or a colon.</p><p>-remove consecutive short lines (less than 41 characters) starting with *, +, #, o, or @ followed by a space. The latter two patterns were also applied to the permalink Web pages of other than the top 5 hosting sites. These are rough cleaning steps in an attempt to remove the titles of other posts, so there is still noisy information remaining after these steps. The size of the original permalinks and processed permalink files is listed in Table We think a whole permalink document may include attitudes toward multiple targets, so the ideal document granularity for attitude analysis is a paragraph or a passage. Paragraph detection is a text classification task which requires training data with boundaries marked. Moreover, blog postings and comments are all personal writings which have not gone through any professional editing, so automatic paragraph detection can be a challenging research problem here. To simplify the complicated problem, we manually examined a random sample of 200 permalink documents to find heuristic patterns for segmenting a document into natural nonoverlapped paragraphs. Our algorithm consists of two steps. The first step identifies candidate paragraphs by merging lines. A current line is merged into a previous line if: <ref type="bibr" coords="2,72.57,595.10,12.73,8.74" target="#b0">(1)</ref> the current line is an empty line or a line ending with any of the following punctuation marks: !, ., ?, ", and ) if the previous line is not as such, or (2) the previous line starts with any of the following symbols: -, &gt;, *, +, o, ., or numbers. These symbols usually represent the title of a section.</p><p>The second step merges a current candidate paragraph into a previous candidate paragraph if: (1) the merged paragraph has &lt; 50 tokens, or (2) the current paragraph has &gt;= 50 tokens but the previous one has &lt; 30 tokens, or (3) the current paragraph has &lt; 50 tokens and the merged paragraph has between 50 and 60 tokens, or (4) the previous paragraph has &gt;= 50 tokens, and the current paragraph has &lt; 10 tokens, and the current paragraph does not start with the pattern of somebody "wrote:", "write:", "writes:", "said:", "say:", "says:", or <ref type="bibr" coords="3,72.57,71.52,12.73,8.74" target="#b4">(5)</ref> the previous paragraph has &gt;= 50 tokens, the current paragraph has &gt;= 10 tokens, the current paragraph does not start with the pattern of somebody "wrote:", "write:", "writes:", "said:", "say:", "says:", and each line of the current paragraph has same number of starting empty spaces as the previous paragraph.</p><p>The algorithm is still flawed since some of the generated paragraphs are very long, especially for those postings and comments which are ill-formatted, but we did not have enough time to improve it.</p><p>An alternative to segmenting a document into paragraph is to segment a document into overlapped fixed sized passages. Text tiling segments text into multi-paragraph subtopic passages <ref type="bibr" coords="3,425.11,143.25,9.97,8.74" target="#b7">[8]</ref>, but is not appropriate here since it sets passage boundary when the topic changes, that is, it may fail to detect an appropriate passage for an attitude target when the topic and the target do not match. We generated overlapped fixed sized passages with window size of 50 words and overlap of 10 words between two neighbor windows.</p><p>We indexed the paragraphs and passages generated from the cleaned permalink collection using Indri<ref type="foot" coords="3,529.75,189.49,3.97,6.12" target="#foot_1">3</ref> . The Porter stemmer was applied and stop words removed during indexing. Topics were processed into Indri queries with stop words removed. Indri automatically stems the query words using the stemmer specified during the indexing process. Two types of queries were formulated -using the title fields only, and using both the title and description fields. The relevant paragraphs or passages were retrieved for opinion analysis.</p><p>We took two steps to analyze the attitudes toward the attitude targets. First we computed the semantic orientations of the words in a relevant paragraph or passage, then aggregated the semantic orientations of the words of the relevant paragraph or passage to get a final sentiment score of the paragraph or passage. At first we collected the prior-polarity subjectivity lexicon from Wilson <ref type="bibr" coords="3,356.93,286.71,15.50,8.74" target="#b19">[20]</ref> which annotates the semantic orientation (i.e., positive, negative, neutral) of 8221 words.</p><p>Turney and Littman <ref type="bibr" coords="3,182.73,310.62,15.50,8.74" target="#b17">[18]</ref> presented an unsupervised learning method for inferring semantic orientation of words (including adjectives, adverbs, nouns, and verbs) from semantic association. Seven positive words (good, nice, excellent, positive, fortunate, correct, and superior) and 7 negative words (bad, nasty, poor, negative, unfortunate, correct, and superior) were intuitively selected as paradigms of positive and negative semantic orientation due to their lack of sensitivity to context. The semantic orientation of a given word was calculated from the strength of its association with the 7 positive words, minus the strength of its association with the 7 negative words. The magnitude of the difference can be considered as the strength of the semantic orientation. The strength of the semantic association between words were computed using two methods-pointwise mutual information (PMI) and latent semantic analysis. PMI was computed using word co-occurrence collected from the web search engine Altavista (i.e., the hits of a query "word1 NEAR word2 ") as follows:</p><formula xml:id="formula_0" coords="3,189.36,438.20,364.96,28.12">P M I(word 1 , word 2 ) = log 2 ( 1 N hits(word 1 N EARword 2 ) 1 N hits(word 1 ) 1 N hits(word 2 ) ) (<label>1</label></formula><formula xml:id="formula_1" coords="3,554.33,447.40,4.24,8.74">)</formula><p>where N is the total number of documents indexed by the search engine. The PMI method, given an unlabeled Web training corpus of approximately one hundred billion words, attained an accuracy of 82.8% which is comparable with Hatzivassiloglou and McKeown's complex method <ref type="bibr" coords="3,369.43,499.95,9.97,8.74" target="#b6">[7]</ref>. Due to the time constraint, we were not allowed to compute the semantic orientations of all the words in the retrieved paragraphs or passages of all the 5 runs. However, Wiebe et al. found that subjectivity clues include low-frequency words, collocations, and adjectives and verbs identified using distributional similarity <ref type="bibr" coords="3,540.30,535.81,14.62,8.74" target="#b18">[19]</ref>, so we selected the low frequency words from the top 1000 retrieved paragraphs using the title fields only. Since the method of counting positive and negative terms requires us to transform the terms into their base forms (lemmas) in order to be able to check if a term is in our sentiment lexicon <ref type="bibr" coords="3,405.25,571.68,14.61,8.74" target="#b9">[10]</ref>, before selecting low frequency words, we lemmatized all the words of the retrieved paragraphs using Morpha<ref type="foot" coords="3,416.98,582.06,3.97,6.12" target="#foot_2">4</ref> , then removed stop words, the words in Wilson's subjectivity lexicon, and those not in a dictionary detected by the unix "spell" utility, then we selected the lemma with document frequency no more than 40 times (DF &lt;= 40). We noticed that when DF &gt; 40 more non-subjective lemmas were seen. Finally we selected 16773 words.</p><p>Since Altavista is not available any more and it is very time consuming to consult any Web search engine for word co-occurrence frequency, we computed the PMIs of the 16773 words by consulting the blog permalinks collections directly. We used Indri's RunQuery to collect hits of words and word co-occurrence. Whenever RunQuery reported a core dump for any word (due to too many retrieved documents), that word was removed. Nine words were removed due to this reason.</p><p>We aggregated a sentiment score of a paragraph or passage from the semantic orientations of the lemmas by checking two sources. If a lemma was in Wilson's subjectivity lemma lexicon, it got 1 point. Otherwise, if it was in the low frequency word list with a score between -3 and -0.05, or between 0.05 and 5, it got 0.2 point. Lemmas with a score of (-0.05, 0.05) were considered neutral, and those with a score (-infinite, -3) or (5, infinite) were considered abnormal, and therefore were removed. The aggregated sentiment score of a paragraph or passage is computed as a score accumulated from all the lemmas in the paragraph/passage normalized by the paragraph/passage length (i.e., the number of lemmas in the paragraph/passage).</p><p>We performed a proportional demotion of paragraphs/passages along the retrieved ranked list if a paragraph/passage has a normalized sentiment score less than 0.15. We tried two demotions -demotion by 2 times and 3 times. By demoting n times we mean demoting a paragraph/passage from its original position of x to the position of nx along the ranked list. Before submitting our runs, we merged the paragraphs/passages from a same permalink document back into one document bottom up along the ranked list. If fewer than 1000 documents were generated, we re-ran our queries to retrieve more paragraphs/passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Relevance Assessment</head><p>TREC organizes assessments for the opinion retrieval task. For the assessment, the content of a blog post is defined as the content of the post itself and the contents of all comments to the post; if the relevant content is in a comment, then the permalink is declared to be relevant. The assessors assigned 6 levels of relevance scores<ref type="foot" coords="4,551.33,271.64,3.97,6.12" target="#foot_3">5</ref> : not judged (-1), irrelevant (0), relevant but non-opinionated (1), relevant with negative opinion (2), relevant with mixed (or ambiguous) opinion (3), relevant with positive opinion (4). Note that for opinion relevance, the topic of the post does not necessarily have to be the target, but an opinion about the target must be present in the post or one of the comments to the post. We report our results at two levels: topic relevant level (&gt;= 1 vs &lt; 1) and opinion relevant level (&gt;= 2 vs &lt; 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Results and Discussion</head><p>We submitted 5 automatic runs consisting of the top 1,000 documents for each topic for the opinion retrieval task.</p><p>ParTitDef. Baseline 1. This is our required automatic run. Permalink documents are segmented into paragraphs. Queries are automatically formulated using the title fields only. No opinion detection is performed.</p><p>ParTitDesDef. Baseline 2. Permalink documents are segmented into paragraphs. Queries are automatically formulated using both the title and description fields. No opinion detection is performed.</p><p>PasTitDesDef. Baseline 3. Permalink documents are segmented into passages. Queries are automatically formulated using both the title and description fields. No opinion detection is performed.</p><p>ParTiDesDmt2. Permalink documents are segmented into paragraphs. Queries are automatically formulated using both the title and description fields. Non-opinionated paragraphs are demoted 2 times proportionally along the retrieved ranked list.</p><p>ParTiDesDmt3. Permalink documents are segmented into paragraphs. Queries are automatically formulated using both the title and description fields. Non-opinionated paragraphs are demoted 3 times proportionally along the retrieved ranked list.</p><p>Our analysis to date has focused on retrieval effectiveness for topic relevance and opinion relevance. Table <ref type="table" coords="4,553.59,598.44,4.98,8.74" target="#tab_2">2</ref> shows retrieval effectiveness measures at topic relevance. Comparing ParTitDesDef with other four runs, we see a minor reduction of Mean uninterpolated Average Precision (MAP) for demotion of opinionated paragraphs (ParTiDesDmt2 and ParTiDesDmt3) and substantial reduction of MAP for ParTiDef and PasTiDesDef.</p><p>A Wilcoxon signed-rank test for paired samples indicated that the reduction in MAP between ParTitDesDef and ParTiDesDmt2 is not significant at p &lt; 0.05, between ParTitDesDef and ParTiDesDmt3 is marginally significant at p = 0.05, between ParTitDesDef and ParTiDef or PasTiDesDef is significant at p &lt; 0.05.  From these comparisons we conclude that demotion of opinionated paragraphs does not help better retrieve either topically relevant or opinion relevant documents; query formulation using both the title and description fields is significantly better than using the title fields only; segmenting permalink documents into paragraphs is significantly better than segmenting into fixed sized passages. For topic relevance, ParTitDesDef yielded our best results when evaluated using MAP and P10 whereas ParTiDesDmt2 yielded best results when evaluating with Bpref and P10. Compared with the median across all 57 runs submitted by any team, ParTitDesDef outperformed the median AP for 37 out of 50 topics. For opinion relevance, ParTiDesDmt2 yielded our best results when evaluating using MAP, Bpref and P10. Compared with the median across all 57 runs, ParTiDesDmt2 outperformed the median AP for 39 out of 50 topics (see Figure <ref type="figure" coords="5,546.95,581.66,3.88,8.74" target="#fig_0">1</ref>), but hit the best AP only once. Median results are likely to be biased somewhat low in the first year of a new track, but this analysis suggests that conventional term weighting and document scoring techniques such as those implemented in Indri perform reasonably well for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Demotion of opinionated documents does not help or hurt much the retrieval effectiveness at both topic relevance and opinion relevance levels. Comparing AP of ParTiDesDmt2 with ParTitDesDef topic by topic (see Figure <ref type="figure" coords="5,103.80,653.39,4.43,8.74" target="#fig_1">2</ref>) reveals that ParTiDesDmt2 does better than ParTitDesDef on some topics but worse than the others. We suspect this might be due to our flawed way of sentiment detection. Additional analysis will be needed to characterize the reasons for the ineffectiveness of our opinion detection approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Enterprise Track</head><p>The primary goal of the expert search task <ref type="bibr" coords="6,263.03,234.47,10.52,8.74" target="#b3">[4]</ref> is to look for a person or multiple people in an organization who are experts on a given topic. The task briefly connects content (emails, web pages, etc.) to people and both represent the major two sides of searching (or making sense of) any informal communication media, matching the main goal of JIKD project in University of Maryland<ref type="foot" coords="6,325.74,268.76,3.97,6.12" target="#foot_4">6</ref> that focuses more on emails as an example of such media. Hence, our first-time participation in the task has two goals. The first is to build a baseline and the second is to apply what we have learned in JIKD about modeling identity <ref type="bibr" coords="6,404.18,294.24,10.52,8.74" target="#b4">[5]</ref> to a public mailing lists such as W3C as opposed to personal archives such as Enron collection <ref type="bibr" coords="6,356.83,306.20,14.61,8.74" target="#b10">[11]</ref>. With both in mind, we have adopted a simple unsupervised approach that focuses solely on mailing lists as the source of evidence and ranks candidates based on references (in the headers and the body text) to their names and email addresses in a set of emails that the system believes as relevant to the topic of interest. The credit granted at each detected reference is computed based on (1) the relevance score of the email where the reference appears, and (2) the specific email field (headers, new text, quoted, etc.) in which it is observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>Duplicate Removal We used W3C mailing lists version that was cleaned by Daqing He<ref type="foot" coords="6,467.14,410.68,3.97,6.12" target="#foot_5">7</ref> . Before any further processing we removed the duplicate emails, yielding 169,053 unique emails out of what was originally 173,146 emails in that cleaned version.</p><p>Detection of signature blocks Since our approach is based on the references in the body of the email, it was natural to isolate the signature blocks of each email so that the names that appear there do not mislead the subsequent scoring. To detect these blocks we used the same technique described in <ref type="bibr" coords="6,441.69,485.97,9.96,8.74" target="#b4">[5]</ref>.</p><p>Indexing We indexed the mailing lists using Lucene, an open source search engine <ref type="foot" coords="6,441.92,510.29,3.97,6.12" target="#foot_6">8</ref> . Body text was stemmed and the stop words were removed from each email and the new text, signature block, quoted headers, and quoted text were indexed in separate fields.</p><p>Building simple models of identity To be able to recognize as many references to the same person as we can, we have built simple models of identity <ref type="bibr" coords="6,271.05,573.62,10.52,8.74" target="#b4">[5]</ref> that associate multiple forms of names and email addresses of the same person from the main and quoted headers. For each person, only strong co-occurrences (whose relative frequency exceeds a specific threshold) are kept in the model in order to get rid of unreliable associations that were incorrectly extracted due to parsing errors. For the expert search task, a list of 1092 candidates (one name and one email address for each) is provided to the search system. The built models are aimed to enrich that list by alternative addresses and forms of names of the same person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Formulation</head><p>We have formulated our queries using one or more (concatenated) fields of the traditional TREC topic format. The specific combinations used in the submitted runs will be given later in section 2.3. Each Name Recognition We used Aho-Corasick algorithm <ref type="bibr" coords="7,320.94,162.42,10.52,8.74" target="#b0">[1]</ref> to recognize references to names or email address in linear time. Note that we restrict the list of recognized names to full names only trying to reduce the uncertainty in resolving them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval Approach</head><p>We first retrieve a set of emails that are relevant to the formulated query using Lucene's vector space model. We experimented with two different retrieval approaches:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Email-based Approach</head><p>This approach considers the top 500 emails as the relevant set of emails R T to the given topic T and computes a score for each candidate as follows:  <ref type="table" coords="7,138.21,434.97,3.88,8.74" target="#tab_5">4</ref>.</p><formula xml:id="formula_2" coords="7,225.90,324.44,72.21,8.74">score(cand|T ) =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Thread-based Approach</head><p>In this alternative approach, we add to R T the first 15 emails in a breadth-first traversal of the threads H rooted by the emails retrieved by the query. Since those just added emails are not directly relevant to the topic T , we have used the following scoring function:</p><formula xml:id="formula_3" coords="7,170.90,525.61,289.33,53.34">score(cand|T ) = h∈H d∈h support(d|cand, T ) support(d|cand, T ) = sim(root(d), T ) • 1 level(d, h d ) • assoc(cand, d)</formula><p>where level(d, h d ) is 1 + the distance from thread root to d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation and Results</head><p>Each participating site was allowed to submit up to 5 runs. For each topic, the system provides a ranked list of candidates (up to 100) along with a ranked list of supporting documents (up to 20) for each candidate. The goal of the later list is to guide both the relevance judgment and evaluation processes. Two evaluation approaches are applied to the task this year. The first (called "expert-retrieval") evaluates each run based solely on the expert ranking while the other (called "expert-support") considers a retrieved candidate relevant only if the system provides a (judged) positive support document for that candidate.</p><p>UMD submitted 5 runs (listed in Table <ref type="table" coords="7,260.61,703.64,4.43,8.74" target="#tab_6">5</ref>) which differ in how the query is formulated given the TREC topic and whether the system retrieves emails or threads. The table gives a brief summary of the evaluation results of  The table indicates that the runs which use both title and narrative parts of the topics were the best with the email-based one in particular the best in most measures. Notice the significant difference between scores in both evaluation approaches for the same run (even in the average-of-medians case), which means that systems were usually not able to justify their choices.</p><p>The results per topic for our best run (UMDemailTLNR) are shown in Figure <ref type="figure" coords="9,436.96,268.61,4.43,8.74" target="#fig_3">3</ref>(a) where the difference in average precision from the median score is plotted for each topic sorted by topic ID and that difference. The figure shows that this particular run may represent an average system among all submitted runs.</p><p>Since our approach only utilizes mailing lists part of the W3C collection, it is interesting to see how that limitation affects the performance of our approach. Here we propose one way to achieve that goal by first computing Email Support Ratio "ESR"; an estimation of the dependency of a topic T on mailing lists as follows:</p><formula xml:id="formula_4" coords="9,214.06,348.81,201.83,22.31">ESR(T) = |Positive Support Emails of T | |Positive Support Documents of T |</formula><p>ESR gives an indication of how much each topic is actually represented (or discussed) in mailing lists. Figure <ref type="figure" coords="9,72.57,391.27,4.57,8.74" target="#fig_3">3</ref>(b) illustrates both ESR and the actual number of positive support emails per topic. The figure shows that ESR spans its range almost uniformly over topics but it is not directly proportional to the number of supporting emails. Two topics (EX63 and EX74) had no email support at all.</p><p>To investigate whether there is a potential relationship between our scores and ESR, we plot in Figure <ref type="figure" coords="9,541.41,427.14,4.29,8.74" target="#fig_3">3</ref>(c) the difference in average precision from both median and best scores over topics sorted by ESR. Fitted cubic curves are also plotted in each case. The curves show that our system (in average) performs better as ESR increases, i.e, scores on topics that are heavily discussed in mailing lists are generally better</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Legal Track</head><p>In the legal community, people have been familiar with using simple term-based techniques to search against large data sets. However, the effectiveness of such technologies in finding responsive documents in legal settings has not been generally studied. The TREC legal track provides a forum to evaluate the results of searches on "topics" approximating real legal requests in litigation. For the first year of the track, the University of Maryland team mainly tried two techniques: Boolean search and combining query terms from multiple sources. Since a boolean query for each search request was provided in the test collection, we wanted to see how a retrieval system that is designed for ranked retrieval would perform if we made it act like a Boolean search system. In this case, we used Indri retrieval engine, and by combining some of its query operators we were able to produce a Boolean run. For comparative purpose, we also produce three other runs, which are explained below.</p><p>We indexed all the fields in the document collection to create one document index. All the four submitted runs used this index. The first is a baseline run, whose queries used all words from the &lt;RequestText&gt; field in the topics to form automatic queries. This is called baseline run since we expected other runs to achieve better retrieval effectiveness through either performing Boolean retrieval or adding query terms from the Boolean text. The second run is a run using manually formulated queries based on the &lt;FinalQuery&gt; field in the topics. Since we did not have a true Boolean retrieval system available, we combined several query operators in Indri retrieval engine to "mock" this Boolean run. In addition, we also submitted a run with queries based on all word from the original Boolean queries to form queries (i.e., ignored all the Boolean syntax and treated each query as a bad of words). A comparison of this run with the above Boolean run will reveal whether the Boolean syntax can improve the retrieval effectiveness. Finally, we combined all word from the original Boolean queries and all words from the &lt;RequestText&gt; to form combined queries. We want to see whether using such a combination of query words can lead to improvement over using either the Boolean query words or using the &lt;RequestText&gt; words alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Techniques</head><p>Although research and practice in information retrieval has run for a long time, we haven't seen much comparison of Boolean retrieval and ranked retrieval. A main reason is that usually a retrieval system is designed for either ranked retrieval or Boolean retrieval, both rarely for both. Boolean retrieval has been widely used in the legal community, while the majority of the IR systems used in TREC evaluation are ranked retrieval. It would be very interesting to conduct such a comparison since the result can inform us whether experimental ranked retrieval systems can achieve performance comparable to or even better than the practical Boolean system. However, it is not easy for research teams like us to get a practical Boolean system. Fortunately, we found that Indri retrieval system provides several query operators that can be used to mock Boolean retrieval. So we decided to use Indri to conduct both the Boolean retrieval and the ranked retrieval. In this section, we describe how we constructed Boolean queries in Indri and other three sets of queries used in our official submission.</p><p>The retrieval model in Indri is based on a combination of the language modeling and inference network retrieval frameworks <ref type="bibr" coords="10,165.32,285.17,14.62,8.74" target="#b15">[16]</ref>. Indri provides a lot of operators that can be sued to form complicated and effective structured queries. Two types of query operators that are particular important for the purpose of this study are "matching" operator and "belief" operator. Matching operators can be used to decide whether a document match the query in question. This is basically done through some sort of term matching. Belief operators are used to decide the degree of matching. For ranked retrieval, they are directly related to the rank of a document. In our study, we used different combinations of those query operators to mimic Boolean retrieval. Specifically,</p><p>• OR For Boolean expression "a OR b", we constructed "#combine( a b )" as the Indri Boolean query. This query will return every document that contains either a or b or both. As the name indicates, #combine operator combines the belief based on term a and term b, hence ranking is possible, although for the purpose of Boolean retrieval, ranks can be ignored.</p><p>• AND For Boolean expression: "a AND b", we constructed "#filreq( #band( a b ) #combine( a b ) )" as the Indri Boolean query. This query will return all documents containing both a and b and then rank documents according to the belief computed with "#combine( a b )". Again, the score can be ignored for Boolean retrieval.</p><p>• NOT For Boolean expression "a NOT b", we constructed "#filrej( b a )". This will score all documents that contains term a but never return any containing term b.</p><p>The above three basic operations can be combined to handle more complicated Boolean queries. For example, for a Boolean expression:</p><p>( However, this query won't work because the #filreq (and also #filrej) operator requires the first argument to an expression that returns matches, not one that returns scores. Being a belief operator, the #combine operator returns scores, so it can't be used as the first argument. In cases like this one, the #syn operator can be used instead. #syn is a matching operator -#syn returns documents that matches at least one of the term specified in it. Consequently, the above query can be revised as:</p><p>#filreq( #band( #syn( a b ) #syn( c d ) ) #combine( #combine( a b ) #combine( c d ) ) ) Following these rules, we were able to work around to convert the Boolean queries provided in the search topics into Indri queries. Although it is possible to automatically derive Indri Boolean queries based on the original Boolean queries of the search topics, we used a semi-automatic approach due to some syntax errors and typos in the original queries. That is, we used a Perl script to form the initial set of Indri Boolean queries based on the above rules, and then manually corrected the errors. In the original Boolean queries, most words were truncated. In formulating the Indri Boolean queries, however, we returned to the full form of each word since we applied the built-in stemming function of Indri to both the queries and the documents.</p><p>The other three set of queries used in out official submission are for comparison purpose. For a baseline ranked retrieval run, we formulated the query by simply using all words contained in the &lt;RequestText&gt; field. Again, we did not perform extra operations such as removing punctuation or stemming since they were taken care of by the Indri functions. The third set of queries used all words in the original Boolean queries of the search topics, but we ignored all Boolean syntax operators. We're interested in knowing whether a ranked retrieval system that does not support Boolean retrieval can achieve reasonable retrieval effectiveness if we simply feed it with Boolean queries constructed by people who are more comfortable with creating Boolean queries. Finally, we formulated another set of queries that used a combination of all words from the &lt;RequestText&gt; and &lt;FinalQuery&gt; (i.e., the original Boolean query field). Here, we would like to see whether such a combination of query words from two sources could lead to better retrieval effectiveness than using each of them alone. Of course, these two sources share a lot of words.</p><p>As mentioned above, we used Indri retrieval engine for our experiment. Indri is retrieval system jointly developed by the University of Massachusetts and Carnegie Mellon University. It has been made freely available for the IR community <ref type="foot" coords="11,170.08,273.18,3.97,6.12" target="#foot_7">9</ref> . Although Indri accepts documents in a variety of formats including XML and the legal documents are in XML format, it requires each document as one file. Since the collection contains 6,910,192 documents, we decided to convert the documents into trecweb format instead of splitting them into smaller files (Indri also accepts trecweb format and does not require one document per file). All the fields in the original XML format were retained as trecweb &lt;BODY&gt; content. Finally we create a single document index with this reformatted document collection. Porter stemmer was used in both query and document processing.</p><p>Due to time constraint, we were not able to try other variants of indexing techniques such as character n-gram. We understood that the documents contain many errors from from an optical character recognition (OCR) process, and character n-gram based retrieval may compensate some of these errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Table <ref type="table" coords="11,99.19,416.67,4.98,8.74" target="#tab_7">6</ref> shows the the average precision of each topic for the four submitted runs. Overall, mean average precision (MAP) is very low for all the four runs, indicating either this is a challenging task or the techniques we explored were not very effective. We suspect that the OCR errors might be a major contributing factor, especially when word-based indexing was used.</p><p>Among the four runs, the one using all words from the &lt;RequestText&gt; field and the field &lt;FinalQuery&gt; (i.e., Comb in the table) has the highest MAP. Wilcoxon signed rank tests indicate that this run significantly outperformed the baseline run and the run with words from the Boolean queries (i.e., Base and BooleanAuto in the table, respectively), while it is indistinguishable from the Boolean run (Boolean in the table). Two things are important here. First, combing query words from different sources seemed to help improve retrieval effectiveness. Secondly, Boolean retrieval can be just as effective as the best ranked retrieval in our study. The Boolean run is statistically indistinguishable from any of the other three runs.</p><p>We were informed by the track organizers that, after the official evaluation was completed, our Boolean run was indeed evaluated as if it were a ranked retrieval.</p><p>We also looked at the number of documents returned for each topic in our Boolean run (see Column "Boolean ret#" in Table <ref type="table" coords="11,142.27,584.05,3.88,8.74" target="#tab_7">6</ref>). In fact, the system did not return any document for eight topics (Topic numbers <ref type="bibr" coords="11,518.38,584.05,40.20,8.74;11,72.57,596.00,97.94,8.74">: 10, 17, 31, 33, 36, 38, 46, 47)</ref>. However, in Table <ref type="table" coords="11,266.26,596.00,3.88,8.74" target="#tab_7">6</ref>, each of those eight topics has one retrieved document. This is because the sanity check script used by NIST requires at least one retrieved document for each topic, so we randomly added one document for each of those eight topic in our Boolean run. Overall, the number of retrieved documents varied greatly among the topics. We're not sure what caused it, and future investigation is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Future Work</head><p>Not surprisingly, for the first year of TREC legal track we could not achieve good retrieval effectiveness. We knew very little about the test collection before we started to work on it. Base: queries were formulated by using all words from the &lt;RequestText&gt; field; Boolean: queries were Indri Boolean queries manually formulated based on the &lt;FinalQuery&gt; field; BooleanAuto: queries were formulated by using all words in the &lt;FinalQuery&gt; field (ignoring the Boolean syntax; Comb: queries were formulated by using all words from the &lt;RequestText&gt; field and the &lt;FinalQuery&gt; filed; Boolean ret#: the number of documents retrieved in the Boolean run; rel#: the number of relevant documents (official).</p><p>us. Our team had to serve as a member of the track organizing team while doing our own research. However, we gained valuable experience. Our immediate plan is to try out character n-gram based indexing and compare the results with the word-based indexing results. This may give us a better understanding of the effect of OCR errors on the retrieval effectiveness. We also would like to index selectively different fields in the document collection (e.g., metadata fields vs. OCR'd fields). In the future, we plan to team up with law librarians/students to explore a broader range of issues, such as manual query formulation based on the full complaints and interactive retrieval. Therefore, we look forward to next year's legal track!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QA Track</head><p>Information needs are often complex, evolving, and difficult to express or capture <ref type="bibr" coords="13,432.35,186.06,14.61,8.74" target="#b16">[17]</ref>, an issue that is not well addressed in batch-oriented information retrieval system evaluations. This issue has received attention in earlier TREC's, most notably in the interactive track and in the HARD track <ref type="bibr" coords="13,383.55,209.97,9.96,8.74" target="#b1">[2]</ref>. In our previous work, we found that well-constructed clarification questions help to better characterize the user's information need and thus yield better retrieval effectiveness <ref type="bibr" coords="13,197.57,233.88,14.61,8.74" target="#b13">[14]</ref>.</p><p>Interactive question answering has recently become a focus of research in the context of complex QA. The question templates in the ciQA task are substantially different from factoid questions in that the information needs cannot be answered by named entities such as people, organizations, locations, dates, etc. To investigate the role of interaction in complex QA, we relied on an approach similar to <ref type="bibr" coords="13,375.11,281.70,14.62,8.74" target="#b13">[14]</ref>: a trained intermediary manually read through relevant documents, generated answers from them, constructed clarification questions, and produced improved answers based on the clarification responses. We also submitted an automatic run to test quasirelevance feedback for question answering. Overall, we had three major goals:</p><p>• To explore the effectiveness of single-iteration written clarification dialogs • To explore different strategies for clarifying user needs in question answering;</p><p>• To better understand the nature of complex, template-based questions.</p><p>The description of our ciQA activities is as follows: Section 4.1 describes our manual runs. Section 4.2 describes our automatic runs. Section 4.3 presents official results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,84.64,152.59,461.85,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: [Blog] Difference between ParTiDesDmt2 and Median, Average Precision at Opinion Relevance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,72.57,180.27,486.00,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: [Blog] Difference between ParTiDesDmt2 and ParTitDesDef, Average Precision at Opinion Relevance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,300.87,338.47,19.66,6.73;7,323.56,324.44,81.68,8.74;7,72.57,351.85,25.49,8.74;7,212.86,372.99,205.42,8.74;7,244.77,391.02,72.37,8.74;7,320.01,405.05,14.18,6.12;7,335.96,391.02,50.40,9.65;7,72.57,423.01,485.99,8.74;7,72.57,434.97,62.32,8.74"><head></head><label></label><figDesc>, T ) = sim(d, T ) • assoc(cand, d) assoc(cand, d) = f ∈d w f (cand, d) and sim(d, T ) is the similarity score assigned by Lucene for document d. The weights of the different fields are listed in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,72.57,646.44,486.00,8.74;8,72.57,658.40,470.51,8.74;8,138.40,111.11,354.33,510.26"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Expert search results per topic: (a) Difference in average precision from median, (b) Judged email support documents, and (c) Difference in average precision from median and best, sorted by ESR per topic.</figDesc><graphic coords="8,138.40,111.11,354.33,510.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,94.71,536.22,122.05,8.74;10,87.51,548.18,211.73,8.74;10,87.51,560.13,177.26,8.74;10,87.51,572.09,145.71,8.74;10,87.51,584.05,466.12,8.74;10,87.51,596.00,471.05,8.74;10,72.57,607.96,339.95,8.74;10,87.51,619.91,445.03,8.74"><head></head><label></label><figDesc>( a OR b ) AND c ) NOT d We can first use Boolean logic to convert it into: ( ( a AND c ) OR ( b AND c ) ) NOT d The resulting Indri query will be: #filrej( d #combine( #filreq( #band( a c ) #combine( a c ) ) #filreq( #band( b c ) #combine( b c ) ) ) ) For more complicated Boolean queries, such as "(a OR b) AND (c OR d)", #syn query operator should be used. This is because if we follow the above rules, we'll have a query like this: #filreq( #band( #combine( a b ) #combine( c d ) ) #combine( #combine( a b ) #combine( c d ) ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,178.93,394.31,379.64,91.91"><head>Table 1 :</head><label>1</label><figDesc>Size of the Permalink Collection.</figDesc><table coords="2,550.82,394.31,7.75,8.74"><row><cell>1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.57,682.13,486.01,20.69"><head>Table 3</head><label>3</label><figDesc>shows retrieval effectiveness measure at opinion relevance. Comparing ParTitDesDef with other four runs, we see similar reduction patterns as Table 2 except a minor improvement of ParTiDesDmt2 over</figDesc><table coords="5,215.09,62.04,200.56,60.41"><row><cell></cell><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Difference in AP</cell><cell>0.00 0.20 0.40</cell><cell>892</cell><cell>859</cell><cell>865</cell><cell>883</cell><cell>852</cell><cell>874</cell><cell>863</cell><cell>870</cell><cell>884</cell><cell>851</cell><cell>873</cell><cell>871</cell><cell>854</cell><cell>867</cell><cell>855</cell><cell>889</cell><cell>875</cell><cell>890</cell><cell>869</cell><cell>ParTiDesDmt2 Better 872 864 861 881 900 895 885 886 856 857 868 897</cell><cell>877</cell><cell>882</cell><cell>878</cell><cell>853</cell><cell>888</cell><cell>891</cell><cell>880</cell><cell>899</cell><cell>898</cell><cell>896</cell><cell>860</cell><cell>862</cell><cell>876</cell><cell>894</cell><cell>887</cell><cell>866</cell><cell>879</cell><cell>858</cell><cell>893</cell></row><row><cell></cell><cell>-0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Median Better</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,195.10,281.35,242.92,203.50"><head>Table 2 :</head><label>2</label><figDesc>Comparison at Topic Relevance.</figDesc><table coords="5,195.10,281.35,242.92,203.50"><row><cell></cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell><cell>R-Prec</cell></row><row><cell>ParTitDesDef</cell><cell cols="2">0.2849 0.3998</cell><cell cols="2">0.6200 0.3490</cell></row><row><cell cols="2">ParTiDesDmt2 0.2845</cell><cell cols="3">0.4040 0.6200 0.3501</cell></row><row><cell cols="2">ParTiDesDmt3 0.2812</cell><cell>0.4034</cell><cell cols="2">0.6200 0.3542</cell></row><row><cell>ParTiDef</cell><cell>0.2362</cell><cell>0.3580</cell><cell>0.5280</cell><cell>0.3162</cell></row><row><cell>PasTiDesDef</cell><cell>0.2733</cell><cell>0.3866</cell><cell>0.5800</cell><cell>0.3516</cell></row><row><cell>Runs</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell><cell>R-Prec</cell></row><row><cell>ParTitDesDef</cell><cell>0.1882</cell><cell>0.2521</cell><cell cols="2">0.3780 0.2441</cell></row><row><cell cols="5">ParTiDesDmt2 0.1887 0.2573 0.3780 0.2421</cell></row><row><cell cols="2">ParTiDesDmt3 0.1873</cell><cell>0.2568</cell><cell cols="2">0.3780 0.2417</cell></row><row><cell>ParTiDef</cell><cell>0.1547</cell><cell>0.2256</cell><cell>0.3360</cell><cell>0.2106</cell></row><row><cell>PasTiDesDef</cell><cell>0.1631</cell><cell>0.2274</cell><cell>0.3460</cell><cell>0.2264</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,219.98,509.93,191.17,8.74"><head>Table 3 :</head><label>3</label><figDesc>Comparison at Opinion Relevance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,72.57,66.48,486.01,78.91"><head>Table 4 :</head><label>4</label><figDesc>Fields f and Weights w f Sender 2.0 Receiver 1.0 Subject 1.0 New text t f Qtd. sender 1.0 Qtd. receiver 0.5 Qtd. text t f query was stemmed and the stop words were removed before being passed to the search engine. The searched email fields are both subject and new text.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,72.57,66.48,486.01,151.09"><head>Table 5 :</head><label>5</label><figDesc>Summary of the Results two different measures for both methods of evaluation. The average score of the median system in each topic is also listed in the table for comparison.</figDesc><table coords="9,366.06,78.15,119.25,8.77"><row><cell>Retrieval</cell><cell>Support</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,376.25,690.10,182.32,8.74"><head>Table 6 :</head><label>6</label><figDesc>Time constraint was another challenge to Average precision (AP) for 4 official runs.</figDesc><table coords="12,154.93,100.56,321.27,486.94"><row><cell>Topic</cell><cell>Base</cell><cell cols="5">Boolean BooleanAuto Comb Boolean ret# rel#</cell></row><row><cell>6</cell><cell>0.0002</cell><cell>0.0027</cell><cell>0.0001</cell><cell>0.0003</cell><cell>15</cell><cell>122</cell></row><row><cell>7</cell><cell>0.0086</cell><cell>0</cell><cell>0.0141</cell><cell>0.0284</cell><cell>13</cell><cell>165</cell></row><row><cell>8</cell><cell>0.0155</cell><cell>0</cell><cell>0.0044</cell><cell>0.0063</cell><cell>1</cell><cell>185</cell></row><row><cell>9</cell><cell>0.0213</cell><cell>0</cell><cell>0.071</cell><cell>0.0565</cell><cell>201</cell><cell>129</cell></row><row><cell>10</cell><cell>0.0001</cell><cell>0</cell><cell>0.0003</cell><cell>0</cell><cell>1</cell><cell>5</cell></row><row><cell>13</cell><cell>0.0253</cell><cell>0.061</cell><cell>0.0206</cell><cell>0.0491</cell><cell>208</cell><cell>160</cell></row><row><cell>14</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>36</cell></row><row><cell>17</cell><cell>0.0137</cell><cell>0</cell><cell>0.004</cell><cell>0.0185</cell><cell>1</cell><cell>4</cell></row><row><cell>18</cell><cell>0.0114</cell><cell>0.0026</cell><cell>0.0331</cell><cell>0.023</cell><cell>21</cell><cell>80</cell></row><row><cell>19</cell><cell>0.0123</cell><cell>0.0402</cell><cell>0.0289</cell><cell>0.0183</cell><cell>456</cell><cell>502</cell></row><row><cell>20</cell><cell>0.0003</cell><cell>0.0211</cell><cell>0.0059</cell><cell>0.0077</cell><cell>2604</cell><cell>35</cell></row><row><cell>21</cell><cell>0.0005</cell><cell>0.1393</cell><cell>0.0908</cell><cell>0.0462</cell><cell>218</cell><cell>289</cell></row><row><cell>22</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>68</cell></row><row><cell>23</cell><cell>0.0039</cell><cell>0.0051</cell><cell>0.0009</cell><cell>0.0035</cell><cell>9</cell><cell>392</cell></row><row><cell>24</cell><cell>0.0023</cell><cell>0</cell><cell>0.014</cell><cell>0.0353</cell><cell>755</cell><cell>9</cell></row><row><cell>25</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2039</cell><cell>12</cell></row><row><cell>26</cell><cell>0.0001</cell><cell>0.0032</cell><cell>0.0001</cell><cell>0.0001</cell><cell>976</cell><cell>352</cell></row><row><cell>27</cell><cell>0.0011</cell><cell>0.008</cell><cell>0.0008</cell><cell>0.0005</cell><cell>1530</cell><cell>184</cell></row><row><cell>28</cell><cell>0.0978</cell><cell>0.1185</cell><cell>0.0084</cell><cell>0.1401</cell><cell>2647</cell><cell>46</cell></row><row><cell>29</cell><cell>0</cell><cell>0</cell><cell>0.0031</cell><cell>0.0005</cell><cell>12</cell><cell>17</cell></row><row><cell>30</cell><cell>0.2212</cell><cell>0.2345</cell><cell>0.2791</cell><cell>0.4789</cell><cell>63</cell><cell>93</cell></row><row><cell>31</cell><cell>0.1316</cell><cell>0.0031</cell><cell>0.1395</cell><cell>0.1404</cell><cell>1</cell><cell>320</cell></row><row><cell>32</cell><cell>0.0268</cell><cell>0.0476</cell><cell>0.0034</cell><cell>0.0362</cell><cell>7</cell><cell>63</cell></row><row><cell>33</cell><cell>0.023</cell><cell>0</cell><cell>0.0734</cell><cell>0.137</cell><cell>1</cell><cell>37</cell></row><row><cell>34</cell><cell>0.055</cell><cell>0.179</cell><cell>0.0447</cell><cell>0.0813</cell><cell>80</cell><cell>245</cell></row><row><cell>35</cell><cell>0.1119</cell><cell>0</cell><cell>0.0629</cell><cell>0.138</cell><cell>1</cell><cell>34</cell></row><row><cell>36</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>13</cell></row><row><cell>37</cell><cell>0.0046</cell><cell>0.0767</cell><cell>0.0243</cell><cell>0.0307</cell><cell>733</cell><cell>78</cell></row><row><cell>38</cell><cell>0.0288</cell><cell>0</cell><cell>0.0047</cell><cell>0.0116</cell><cell>1</cell><cell>136</cell></row><row><cell>39</cell><cell>0.0178</cell><cell>0.0159</cell><cell>0.0048</cell><cell>0.023</cell><cell>856</cell><cell>18</cell></row><row><cell>40</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>1</cell></row><row><cell>41</cell><cell>1</cell><cell>1</cell><cell>0.2</cell><cell>0.5</cell><cell>27</cell><cell>1</cell></row><row><cell>43</cell><cell>0.007</cell><cell>0</cell><cell>0.0032</cell><cell>0.003</cell><cell>3</cell><cell>161</cell></row><row><cell>44</cell><cell>0.0021</cell><cell>0</cell><cell>0</cell><cell>0.0009</cell><cell>4</cell><cell>28</cell></row><row><cell>45</cell><cell>0.0844</cell><cell>0.0064</cell><cell>0.0259</cell><cell>0.1143</cell><cell>2</cell><cell>157</cell></row><row><cell>46</cell><cell>0.0005</cell><cell>0.02</cell><cell>0.0207</cell><cell>0.0333</cell><cell>1</cell><cell>50</cell></row><row><cell>47</cell><cell>0.0013</cell><cell>0</cell><cell>0.0006</cell><cell>0.0018</cell><cell>1</cell><cell>6</cell></row><row><cell>50</cell><cell>0.0004</cell><cell>0</cell><cell>0.0448</cell><cell>0.0176</cell><cell>3</cell><cell>61</cell></row><row><cell>51</cell><cell>0</cell><cell>0</cell><cell>0.0001</cell><cell>0</cell><cell>299</cell><cell>29</cell></row><row><cell cols="2">MAP 0.0495</cell><cell>0.0509</cell><cell>0.0316</cell><cell>0.056</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,87.81,710.22,364.28,6.99"><p>http : //www.science.uva.nl/research/iiwiki/wiki/index.php/Guidelines f or P articipants 2006</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,87.81,696.27,134.34,6.99"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,87.81,705.77,228.64,6.99"><p>http://www.cogs.susx.ac.uk/research/nlp/carroll/morph.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="4,87.81,713.01,364.28,6.99"><p>http : //www.science.uva.nl/research/iiwiki/wiki/index.php/Guidelines f or P articipants 2006</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="6,87.81,690.18,96.96,6.99"><p>www.jikd.umiacs.umd.edu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="6,87.81,699.69,191.34,6.99"><p>http://www.sis.pitt.edu/∼daqing/w3c-cleaned.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="6,87.81,709.19,91.27,6.99"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="11,87.81,709.03,134.34,6.99"><p>http://www.lemurproject.org/indri/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to <rs type="person">Trevor Strohman</rs> and <rs type="person">Don Metzler</rs> for their help with Indri. This work was supported in part by the the <rs type="programName">DARPA GALE program</rs> and by the <rs type="institution">Joint Institute for Knowledge Discovery</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VJtTkgU">
					<orgName type="program" subtype="full">DARPA GALE program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Manual Run</head><p>This section describes our methodology for creating manual pre-and post-clarification runs (UMDM1pre, UMDM1post). We employed a trained intermediary in all phases of the process. Document Retrieval. We employed the building blocks strategy <ref type="bibr" coords="13,361.78,495.35,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="13,375.35,495.35,12.73,8.74" target="#b14">15]</ref> with Lucene to obtain a set of relevant documents. We constructed a Boolean query based on the question template and narrative for each topic, augmented with various query term expansions. Concepts were ANDed, and the set of synonyms for each concept were ORed together.</p><p>We used a number of external sources for expanding query terms:</p><p>• The CIA World Factbook: used for geographic expansion. The World Factbook provides different forms of country names: conventional long form, conventional short form, local long form, local short form, and abbreviation. In addition, we also expanded country or region names to their states/provinces, ports and terminals, and major cities (all fields in the Factbook).</p><p>• Google: used for lists of instances. For example, we expanded "performance-enhancing substances" into the names of banned substances by NCAA and other organizations.</p><p>• WordNet: used to expand general concepts to their hyponyms, hypernyms, and synonyms. For example, we expanded "weapon" to its hyponyms (for example, missile) and its synonyms (for example, arm).</p><p>• Roget's Thesaurus: used for common concepts. For example, we expanded transportation to its various forms, such as barge, rail, air, water, road, cargo, freight, and so on.</p><p>• Wikipedia: used for variation of organization names and related organizations. For example, we expanded IRA to "Republican Movement", "Irish republican organizations", "Irish Republican Army", "Fianna Eireann", and so on.</p><p>The query expansion allowed us to retrieve documents at the conceptual level-beyond simple matching of terms in the topic description.</p><p>Answer Generation. We examined the top 20 documents for each topic to select statements, facts, or evidence that would answer the question. We resolved anaphora, combined multiple sentences, and eliminated redundant information when necessary.</p><p>Answer Ranking. Having gathered all the pieces, we then ranked the answers based on importance and/or logical order to generate a coherent response. The intention was to present the answers in a order that makes sense to the assessor. Depending on the topic, this could be chronological order, order of importance, or a summary statement followed by supporting evidence.</p><p>Clarification Forms Generation. We used three types of clarification questions, depending on how well we understood the question and the answers after the initial run.</p><p>• For 10 questions where the topic narrative seemed insufficient to generate a coherent response, we asked specifically for guidance. See Figure <ref type="figure" coords="14,257.68,294.68,4.98,8.74">4</ref> for an example.</p><p>• For 12 questions where the topic narrative seemed clear and the answers seemed to roughly cluster into categories, we asked the assessor about the importance of the various categories so that we can re-rank the answers. See Figure <ref type="figure" coords="14,187.44,338.51,4.98,8.74">5</ref> for an example.</p><p>In this form, we also presented a few sample answers. Assessors were asked to judge the relevance of each sample answer; the choices were relevant, partially relevant, and not relevant.</p><p>• For 8 questions where answers did not seem to cluster into coherent categories, we picked a number of sample answers for soliciting relevance feedback. The choices were relevant, partially relevant, and not relevant.</p><p>Refinement of Final Answers. After we received feedback from the users, we reselected and reranked the initial answers. Additional searches were performed for a few topics. The number of manual answers for each topic ranged from 5 to 18. We then filled the response with automatically-selected sentences retrieved from the manual queries until the 7,000 character limit was reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Run</head><p>Our process for generating the automatic runs (UMDA1pre, UMDA1post) was relatively simple. For each topic, the question and narrative was used verbatim as a query to Lucene. From the top 20 resulting documents, 10 terms were selected based on tf.idf values. These terms were added to the original query to retrieve a new set of documents. Sentences in this set that did not contain at least one term from the question were discarded. The remaining sentences comprised the initial run. The automatically-generated clarification forms asked the assessor for relevance judgments on each of the sentences (relevant, partially relevant, not relevant, and don't know). The final run was prepared by moving sentences judged as relevant up to the top of the list, followed by the sentences judged partially relevant. Sentences judged as not relevant were discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Official NIST results are shown in Table <ref type="table" coords="14,253.36,656.23,3.88,8.74">7</ref>. For our automatic run, interaction actually decreased performance substantially. We have noticed that there exists a non-straightforward connection between sentence-level relevance judgments and the "nuggetization" process involved in the evaluation methodology-these and related issues are currently under investigation.     </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,93.05,147.35,465.52,8.74;16,93.05,159.31,80.26,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,227.23,147.35,238.92,8.74">Efficient string matching: an aid to bibliographic search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Corasick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,485.75,147.35,72.82,8.74;16,93.05,159.31,47.75,8.74">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,177.49,465.51,8.74;16,93.05,189.45,286.28,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,152.17,177.49,336.60,8.74">HARD track overview in TREC 2005: High accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,508.91,177.49,49.66,8.74;16,93.05,189.45,255.73,8.74">Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text REtrieval Conference (TREC 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,207.63,465.51,8.74;16,93.05,219.59,125.04,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="16,191.67,207.63,327.21,8.74">The weblog handbook: practical advice on creating and maintaining your blog</title>
		<author>
			<persName coords=""><forename type="first">Rebecca</forename><surname>Blood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Perseus Publishing</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,237.77,465.52,8.74;16,93.05,249.73,230.88,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,282.81,237.77,191.52,8.74">Overview of the TREC-2005 enterprise track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,493.96,237.77,64.61,8.74;16,93.05,249.73,200.61,8.74">Working Notes of 2005 Text Retrieval Conference (TREC 05)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,267.91,465.52,8.74;16,93.05,279.87,396.71,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,226.82,267.91,311.87,8.74">Modeling identity in archival collections of email: A preliminary study</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,93.05,279.87,158.31,8.74">Conference on Email and Anti-Spam</title>
		<meeting><address><addrLine>Mountain View, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,298.05,465.52,8.74;16,93.05,310.01,102.56,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="16,166.15,298.05,293.76,8.74">Online Information Retrieval: Concepts, Principles, and Techniques</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Harter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Academic Press</publisher>
			<pubPlace>San Diego, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,328.19,465.51,8.74;16,93.05,340.15,126.75,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,361.82,328.19,196.74,8.74;16,93.05,340.15,18.95,8.74">Predicting the semantic orientation of adjectives</title>
		<author>
			<persName coords=""><forename type="first">Vasileois</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katheleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,120.21,340.15,31.75,8.74">ACL-97</title>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,358.33,465.53,8.74;16,93.05,370.29,182.06,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,186.94,358.33,298.18,8.74">TextTiling: Segmenting text into multi-paragraph subtopic passages</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,494.66,358.33,63.91,8.74;16,93.05,370.29,45.61,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="64" />
			<date type="published" when="1997-03">1997. March 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.05,388.47,465.51,8.74;16,93.05,400.43,454.63,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,440.43,388.47,118.13,8.74;16,93.05,400.43,85.26,8.74">Bridging the Gap: A Genre Analysis of Weblogs</title>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">C</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lois</forename><surname>Scheidt</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elijah</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sabrina</forename><surname>Bonus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,187.57,400.43,332.53,8.74">Proceedings of the 37th Hawaii International Conference on System Sciences</title>
		<meeting>the 37th Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,418.61,465.53,8.74;16,93.05,430.56,191.75,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,293.46,418.61,265.12,8.74;16,93.05,430.56,113.68,8.74">Sentiment classification of movie and product reviews using contextual valence shifters</title>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,214.94,430.56,43.16,8.74">FINEXIN</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,448.75,465.53,8.74;16,93.05,460.70,144.32,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,198.39,448.75,128.46,8.74">Introducing the Enron corpus</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,348.97,448.75,159.07,8.74">Conference on Email and Anti-Spam</title>
		<meeting><address><addrLine>Mountain view, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">July 30-31 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,478.89,428.15,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,181.23,478.89,140.91,8.74">Blogging as a form of journalism</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Lasica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,330.97,478.89,185.51,8.74">USC Annenberg Online Journalism Review</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,497.07,423.05,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,181.23,497.07,135.46,8.74">Weblogs: A new source of news</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Lasica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,325.87,497.07,185.51,8.74">USC Annenberg Online Journalism Review</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,515.26,465.53,8.74;16,93.05,527.21,252.96,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,390.04,515.26,168.53,8.74;16,93.05,527.21,82.89,8.74">Exploring the limits of single-iteration clarification dialogs</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eileen</forename><surname>Abels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,196.90,527.21,50.33,8.74">SIGIR 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,545.40,465.53,8.74;16,93.05,557.35,98.13,8.74" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="16,180.96,545.40,211.55,8.74">Information Seeking in Electronic Environments</title>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Marchionini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,575.54,465.53,8.74;16,93.05,587.49,465.52,8.74;16,93.05,599.45,141.19,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,218.66,575.54,336.07,8.74">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,105.02,587.49,449.40,8.74">Information Processing and Management Special Issue on Bayesian Networks and Information Retrieval</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,617.63,431.68,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,173.48,617.63,136.04,8.74">The process of asking questions</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,318.24,617.63,109.16,8.74">American Documentation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="396" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,635.82,465.53,8.74;16,93.05,647.77,362.71,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,279.87,635.82,278.71,8.74;16,93.05,647.77,69.57,8.74">Measuring praise and criticism: inference of semantic orientation from association</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,171.22,647.77,224.18,8.74">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="315" to="346" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,93.04,665.96,465.53,8.74;16,93.05,677.91,243.21,8.74" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="16,365.37,665.96,129.63,8.74">Learning Subjective Language</title>
		<author>
			<persName coords=""><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rebecca</forename><surname>Bruce</surname></persName>
		</author>
		<idno>TR-02-100</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Comp. Science, Univ. of Pittsburg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct coords="16,93.04,696.10,465.53,8.74;16,93.05,708.05,173.16,8.74" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="16,356.41,696.10,202.16,8.74;16,93.05,708.05,78.26,8.74">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
