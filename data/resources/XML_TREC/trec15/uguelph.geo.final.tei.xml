<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.36,71.94,294.68,11.78">Passage Retrieval by Shrinkage of Language Models</title>
				<funder>
					<orgName type="full">NSERC (National Sciences and Engineering Council of Canada)</orgName>
				</funder>
				<funder>
					<orgName type="full">OCE (Ontario Centres of Excellence)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.88,99.23,34.78,9.34"><forename type="first">Fei</forename><surname>Song</surname></persName>
							<email>fsong@uoguelph.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing and Information Science</orgName>
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<postCode>N1G 2W1</postCode>
									<settlement>Guelph</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.07,99.23,39.93,9.34"><forename type="first">Joe</forename><surname>Vasak</surname></persName>
							<email>jvasak@uoguelph.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing and Information Science</orgName>
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<postCode>N1G 2W1</postCode>
									<settlement>Guelph</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.92,99.23,44.46,9.34"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing and Information Science</orgName>
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<postCode>N1G 2W1</postCode>
									<settlement>Guelph</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.36,71.94,294.68,11.78">Passage Retrieval by Shrinkage of Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4EF1B2391FCC832547DAA5D0AB645A23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Information retrieval is the process of searching for relevant documents that satisfy a user's information need (usually in the form of queries). Some of its successful applications include library catalogue search, medical record retrieval, and Internet search engines (e.g., Google). As the exponential growth of web pages and online documents continues, there is an increasing need for retrieval systems that are capable of dealing with a large collection of documents and at the same time narrowing the scope of the search results (not only relevant documents but also relevant passages or even direct answers).</p><p>A number of conceptual models have been proposed for information retrieval, including the Boolean model <ref type="bibr" coords="1,160.08,315.23,157.98,9.34" target="#b0">[Baeza-Yates and Ribeiro-Neto, 1999]</ref>, the vector-space model <ref type="bibr" coords="1,423.56,315.23,57.42,9.34" target="#b10">[Salton, 1989]</ref>, probabilistic models <ref type="bibr" coords="1,181.39,327.23,147.73,9.34" target="#b9">[Robertson and Sparck Jones, 1976]</ref>, the inference network model [Croft and <ref type="bibr" coords="1,94.80,338.99,52.61,9.34" target="#b2">Turtle, 1992]</ref>, and the language models <ref type="bibr" coords="1,259.87,338.99,97.15,9.34" target="#b7">[Ponte and Croft, 1998;</ref><ref type="bibr" coords="1,359.70,338.99,66.49,9.34" target="#b3">Hiemstra, 1998;</ref><ref type="bibr" coords="1,428.87,338.99,76.10,9.34" target="#b6">Miller et al., 1999]</ref>. Among these models, language models have recently received a lot of attention in the field of information retrieval, since they are based on the solid foundation of statistical natural language processing and are both intuitive and flexible for extensions with more features to handle the retrieval tasks.</p><p>In language modeling, we view each document as a language sample and estimate the probabilities of producing individual terms in a document. A query is treated as a generation process. Given a sequence of terms in a query, we compute the probabilities of generating these terms according to each document model. The multiplication of these probabilities is then used to rank the retrieved documents: the higher the generation probabilities, the more relevant the corresponding documents to the given query.</p><p>One big obstacle in applying language modeling to information retrieval is the sparse data problem. Unlike a collection of documents where we can control the number of documents in it, a document itself is often small in size and its content is always fixed. Even for a relatively long document, some of the words can still be rare or missing according to the Zipf's law of language usage <ref type="bibr" coords="1,120.82,541.07,120.64,9.34" target="#b4">[Manning and Sch√ºtze, 1999]</ref>. As a result, the combination of individual probabilities through multiplications will be meaningless if one of the probabilities is zero (for a missing term in a document). Thus, overcoming the sparse data problem is the key for the success of any language modeling system for information retrieval.</p><p>For TREC 2006 Genomics Track (see http://ir.ohsu.edu/genomics/ for more information), the data set presents several new challenges for language modeling in specific and information retrieval in general. First of all, the search is targeted to the relevant passages within documents (more or less corresponding to paragraphs), since users of the biomedical domain are likely interested in finding answers along with the context that provides supporting information and links to the original sources. Secondly, there is a need to balance the results across different documents and aspects. An aspect is defined as a group of passages of similar content, which will be judged by human evaluators and identified by a set of MeSH terms for the Genomics data set. By ensuring an adequate coverage of the results across documents and aspects, we can reduce the repeats (or duplicate passages) and maintain a reasonable number of novel/unique passages, which may be particularly useful for biomedical researchers. Finally, the retrieved passages may need to be trimmed further to highlight the answers, since passages are typically organized as paragraphs and may contain irrelevant wording before and after the relevant answers.</p><p>In the rest of the working note, we describe our retrieval method based on the language models and their combinations in section 2. In section 3, we explain the enhancements for balancing the results for documents and aspects and narrowing the spans for the answers in the retrieved passages. In section 4, we discuss our experimental results on the Genomics data set. Finally, we conclude and point future directions of our work in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Language Models and Their Combinations</head><p>In <ref type="bibr" coords="2,122.89,215.87,91.44,9.34" target="#b11">Song and Croft [1999]</ref>, we proposed a general language model for information retrieval. The model is based on a range of data smoothing techniques, including Good-Turing estimates, curvefitting functions, and model combinations.</p><p>For TREC 2006 Genomics Track, we apply our model for information retrieval, but instead of searching for relevant documents, we move deeper to look for relevant passages. Since passages are much smaller than documents, the sparse data problem is even more serious with a large number of terms missing for individual passages. As a result, more attention needs to be directed to smooth the passage models so that the missing terms are allocated with meaningful probability mass and the generation probabilities for each query are non-zero and thus can be used to rank the retrieved passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Smoothing a Passage Model with the Good-Turing Estimate</head><p>We can compute the frequencies for each term in a passage. To smooth the probabilities for all the terms, including the missing terms in a vocabulary, the Good-Turing estimate adjusts the raw term frequency (tf) values as follows:</p><formula xml:id="formula_0" coords="2,163.92,430.77,96.40,30.28">) ( ) ( ) 1 ( * 1 + + = tf tf N E N E tf tf</formula><p>Here, N tf is the number of terms with frequency tf in a passage, and E(N tf ) is the expected value of N tf . The probability of a term with frequency tf is then defined as tf*/N p , where Np is the total number of terms in passage p. Note that when tf = 0, tf* is reduced to E(N 1 )/E(N 0 ) and the probability for a missing term becomes E(N 1 )/E(N 0 )N p .</p><p>However, obtaining E(N tf ) is almost impossible since a passage is fixed in size and content. In practice, we hope to substitute the observed N tf for E(N tf ) directly, but this creates two problems. For the terms with the highest frequency tf, their adjusted tf* will be zero, since N tf+1 is always zero, which is counter-intuitive. Furthermore, due to the small size of a passage, the number of terms at some middle frequency levels may also be too small or even zero, resulting in an unstable or anomaly distribution. One way to get around the above problems is to use a curve-fitting function to smooth the observed N tf 's for the expected values. Table <ref type="table" coords="3,285.68,82.91,5.19,9.34" target="#tab_0">1</ref> shows a typical term distribution for a document collection 1 , taken from <ref type="bibr" coords="3,191.69,94.67,118.18,9.34" target="#b4">Manning and Sch√ºtze [1999]</ref>. As can be seen, N tf can be approximated by a decreasing curve as tf gets bigger. Such a decreasing curve ensures that N tf = N tf+1 and allows us to project a non-zero value for N tf+1 with the highest tf.</p><p>With a smoothing function S(N tf ) for N tf , the probability for term t with frequency tf in passage p can be computed as follows:</p><formula xml:id="formula_1" coords="3,152.64,175.08,124.72,33.47">p tf tf GT N N S N S tf p t P ) ( ) ( ) 1 ( ) | ( 1 + + =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Curve-Fitting for Good-Turing Estimates</head><p>Given a set of data points (x i , y i ) for i = 1, 2, ‚Ä¶, n, linear regression helps us identify a line f(x) = mx + b that fits the data points as tightly as possible. This is done by minimizing the sum of squares of differences:</p><formula xml:id="formula_2" coords="3,149.76,284.68,212.83,28.78">‚àë ‚àë = = - - = - = n i n i i i i i b mx y x f y b m SS 1 1 2 2 ) ( )] ( [ ) , (</formula><p>Using calculus (see <ref type="bibr" coords="3,194.50,315.95,119.99,9.34">Manning and Sch√º tze [1999]</ref> for details), we can find the optimal solutions for m and b:</p><formula xml:id="formula_3" coords="3,153.60,340.14,204.24,59.71">‚àë ‚àë = = - - - = n i i n i i i x x y y x x m 1 2 1 ) ( ) )( ( and x m y b - =</formula><p>where x and y are the averages of x i and y i for i = 1, 2, ‚Ä¶, n, respectively.</p><p>Clearly, a linear line does not fit the distribution in Table <ref type="table" coords="3,349.87,428.03,5.19,9.34" target="#tab_0">1</ref> properly, since the values of N tf go down very quickly as tf goes up. This leads us to use a geometric distribution to model a decreasing exponential curve:</p><formula xml:id="formula_4" coords="3,144.00,474.29,246.87,14.80">q p x f x = ) ( =&gt; q p x x f log log ) ( log + =</formula><p>By taking the logarithm on both sides, we turn a geometric distribution into a log-linear combination, which can then be solved in a way similar to linear regression.</p><p>Unfortunately, a simple geometric curve does not fit the typical term distribution either: although N tf decreases very quickly for smaller tf's, the pace slows down dramatically as tf gets much bigger. To fit the typical term distribution as closely as possible, we replace the variable x by a nested logarithmic function: Zeroth order:</p><formula xml:id="formula_5" coords="3,94.80,598.13,256.45,103.52">x x = 0 log First order: ) 1 log( log 1 + = x x Second order: ) 1 ) 1 log(log( log 2 + + = x x Third Order: ) 1 ) 1 ) 1 g( log(log(lo log 3 + + + = x x ‚Ä¶ 1</formula><p>The table is actually for bigram (word pair) distributions, but a similar pattern is also applied to individual terms due to the Zipf's law for language usage <ref type="bibr" coords="3,271.84,708.97,110.28,8.53" target="#b4">[Manning and Sch√ºtze, 1999]</ref>.</p><p>Based on the nested logarithmic function, we develop a greedy algorithm that tries to find an "optimal" geometric distribution:</p><formula xml:id="formula_6" coords="4,163.92,105.17,140.56,14.80">q x p x f m log log log ) ( log + ‚ãÖ =</formula><p>Here, the level of nesting m is selected by testing the above formula until no further improvement can be made in terms of sum of squares of the differences for all given data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Combining Language Models by Shrinkage</head><p>Good-Turing estimate provides us the first smoothing step towards building a suitable language model for passage retrieval. However, a passage model is not stable and accurate in the sense that there is often a large number of missing terms and there can also be anomaly distributions for certain known terms. In particular, we cannot differentiate the contributions of different missing terms at a passage level. In a biomedical document, for example, a passage may not contain terms "genetic" and "crocodile", but in terms of probability distribution, we would prefer that the probability for "genetic" be higher than that for "crocodile", since the document is about biomedicine. Obviously, we need to borrow information from outside the passage in order to make a proper differentiation for different missing terms.</p><p>For TREC 2006 Genomics data set, there are multiple levels of structures: the collection consists of 59 journals<ref type="foot" coords="4,152.16,323.61,3.27,5.89" target="#foot_0">2</ref> ; each journal, multiple documents; and each document, multiple passages. Using the Good-Turing estimate, we can also build language models for documents, journals, and even the entire collection. Clearly, the collection model contains the most information. The journal models are more stable and accurate than the document models, and the document models are more stable and accurate than the passage models. For this reason, we want to extend a passage model by adding information from the corresponding document, journal, and collection models. This can be done by the interpolation or shrinkage method: </p><formula xml:id="formula_7" coords="4,164.16,422.42,322.22,11.72">) ( ) | ( ) | ( ) | ( ) | ( 4 3</formula><formula xml:id="formula_8" coords="4,192.72,418.75,244.24,14.62">Œª Œª Œª Œª + + + =</formula><p>where Œª 1 , Œª 2 , Œª 3 , and Œª 4 are weighting parameters and Œª 1 + Œª 2 + Œª 3 + Œª 4 = 1. Such a linear combination has the advantage that the resulting probabilities are normalized in the range of [0, 1].</p><p>The shrinkage combination has two useful effects. One is that we borrow information from outside the passages so that we can further differentiate the contributions of different terms. The other is that we align passage models against the document, journal, and collection models so that the probability distribution for the known terms can be more stable. Intuitively, the document, journal, and collection models provide the average distributions and the passage models add the variations to them. Together, we get more stable and accurate distributions for passage retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Enhancing the Results for Coverage and Specificity</head><p>For TREC 2006 Genomics Track, the retrieved results are evaluated at three different levels: passage retrieval, aspect retrieval, and document retrieval so that we can get insight into the overall performance for a user trying to answer a given topic. Since each submitted run can contain only up to 1000 passages per topic, the retrieved passages from the initial search should be further processed to ensure a reasonable coverage for relevant documents and aspects. Such efforts help reduce repeats or duplicate passages and at the same time increase the number of novel/unique passages in the search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Improving Coverage for Relevant Documents</head><p>TREC 2006 Genomics data set provides legal spans for all passages (more or less corresponding to paragraphs). Each passage is identified by three components: PMID (PubMed ID) ---uniquely assigned to each document; passage start ---the bye-offset in the document file where the passage begins; and passage length ---the length of the passage in bytes (8-bit ASCII code). As a result, the corresponding document for a passage can be easily found by analyzing the passage identifier. This leads us to a simple solution to ensure a wider coverage for relevant documents. Given the top ranked passages for a particular topic, we first map them to the corresponding documents. If a document has got a reasonable number of retrieved passages, the remaining passages for this document are thrown away. This frees up more rooms in the top 1000 passages for other documents so that the coverage for relevant documents can be expanded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Improving Coverage for Relevant Aspects</head><p>Unlike documents that have clear starts and ends, aspects are hidden units corresponding to groups of passages with similar contents, which will be judged by human evaluators and identified by a set of MeSH terms for the Genomics Track evaluation. As stated in the TREC 2006 Genomics Track Protocol, one aspect typically corresponds to multiple passages, but one passage can also be linked to multiple aspects and some passages may overlap and/or belong to multiple aspects.</p><p>Because aspects are subjectively determined and currently not available for training purposes, we may have to apply some kind of clustering techniques to discover the natural grouping among the available passages. We limit ourselves to the partition of non-overlapping clusters. In other words, no clusters may contain other clusters and there are no overlaps between clusters. If we model aspects by clusters of passages, this assumption implies that each passage can only belong to one aspect and no two aspects share any passages in common. Clearly, this assumption is too strong and needs to be relaxed further for future experiments.</p><p>We use the heuristic clustering method introduced in <ref type="bibr" coords="5,332.17,415.55,56.42,9.34" target="#b10">Salton [1989]</ref> for computing aspects, since it allows us to create clusters rapidly with relatively little expense. Each passage is represented as a weighted vector (TF x IDF) and the distance between two vectors is measured by the cosine similarity. As described in <ref type="bibr" coords="5,161.89,451.55,52.74,9.34" target="#b10">[Salton, 1989</ref>], heuristic clustering is a one-pass process, which takes the elements to be clustered one at a time in an arbitrary order. The first element is placed into a cluster of its own. Each subsequent element is then compared against all existing clusters and is placed into the cluster that is the most similar to the new element. If the new element is not sufficiently similar to any of the existing clusters, it forms a new cluster of its own. This process is continued until all elements are processed. Each cluster is represented by the centroid vector, which is updated every time a new element is added into the cluster.</p><p>Once we obtain the aspects (or clusters) for a set of passages, we can prune the results in a way similar to what we did to documents in section 3.1. In other words, we map the top-ranked passages for a topic to their corresponding aspects. If an aspect gets a reasonable number of retrieved passages, the remaining passages for this aspect are discarded. As a result, we can cover more aspects in the top 1000 retrieved passages in the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Towards More Specific Answers for Retrieved Passages</head><p>The goal of TREC 2006 Genomics Track is to find information that is close to "answers" for a question or information need. Performing search at the passage level is helpful for achieving this goal, since documents are typically too long to be used as "answers". However, since passages more or less correspond to paragraphs, which are marked for the documents rather than for the answers to users, they may contain irrelevant wording before and after the relevant answers. Thus, to highlight the answers in the retrieved passages, we may need to trim the irrelevant wording around the answers in the retrieved passages.</p><p>We trim irrelevant wording through a two-step process. First, we use a sentence splitter to break a passage into a sequence of sentences. Then, we narrow down the scope of the result by identifying the first and last sentences that match some of the terms in a given topic. Since sentences are natural units for semantic meanings, by keeping the complete sentences in the narrowed result, we can preserve meaning and ensure the readability of the final result for the retrieved passage.</p><p>We follow <ref type="bibr" coords="6,157.91,177.95,130.38,9.34" target="#b8">Reynar and Ratnaparkhi [1997]</ref> and implement a sentence splitter based on a maximum entropy approach. We use features of the words around an end-of-sentence punctuation mark (usually period, question mark, or exclamation mark) and train the maximum entropy model with labeled documents for end-of-sentence marks. A maximum entropy model allows us to combine features of different kinds, which in our case contain such attributes as person titles, initial capitalization, abbreviations for months and days, etc.</p><p>Table <ref type="table" coords="6,184.03,260.99,3.87,9.34" target="#tab_0">1</ref>. University of Guelph Results for TREC 2006 Genomics Track tries to improve coverage for aspects (section 3.2). For all three runs, the results are enhanced by narrowing the scopes of the retrieved passages for more specific answers (section 3.3).</p><p>For " UofG0" run, we use the greedy curve-fitting algorithm to optimize the Good-Turing estimates for each language model (mostly at the passage, document, and journal levels), and combine all the related models together through the shrinkage method. Since no training data are available from the previous years for TREC 2006 Genomics data set, we set the weighting parameters as follows by intuition: Œª 1 = 0.7, Œª 2 = 0.21, Œª 3 = 0.063, and Œª 4 = 0.027. As can be seen in Table <ref type="table" coords="7,483.15,155.15,3.87,9.34" target="#tab_0">1</ref>, the average MAP (Mean Average Precision) values of " UofG0" run are 0.351672 at the document level, 0.049608 at the passage level, and 0.185564 at the aspect level. These numbers are higher than the corresponding median scores over the 68 automatic runs from Table <ref type="table" coords="7,379.61,190.43,3.73,9.34" target="#tab_2">2</ref>, which are 0.27905 for documents, 0.024008 for passages, and 0.116862 for aspects. Note that a couple of refined conditions are added to further improve the retrieval performance. First, each retrieved passage should have at least one term in common with the given topic in order to avoid over-smoothing with the shrinkage method. Secondly, the first two passages that appear at the beginning or at the end of a document are removed from the search results, since they tend to be titles, authors, and references for the documents in TREC 2006 Genomics data set. In future, we could further improve the retrieval performance when training data become available to fine-tune the weighting parameters.  <ref type="bibr" coords="7,116.88,337.93,394.57,8.53;7,116.88,349.21,394.57,8.53;7,116.88,360.49,183.13,8.53">160 .925200 .470600 .000000 .212300 .032200 .000000 .366700 .154900 .000000 161 .933900 .332900 .000000 .185200 .044800 .000000 .886900 .285800 .000000 162 .360700 .160000 .000000 .241700</ref> .003300 .000000 .664300 .020900 .000000 163 .699900 .551300 .040800 .249000 .039700 .000300 .463700 .248500 .002900 164 .619300 .003600 .000000 .399100 .000600 .000000 .744300 .001700 .000000 165 .756500 .212900 .000000 .279300 .036500 .000000 .731500 .409700 .000000 166 .271800 .126100 .000000 .164300 .006200 .000000 .565500 .091500 .000000 167 .750500 .498400 .024400 .218200 .072100 .000000 .348700 .164700 .000000 168 .921600 .751300 .000000 .179300 .089600 .000000 .659800 .212900 .000000 169 .732700 .248500 .000000 .459900 .021300 .000000 .773800 .100400 .000000 170 .916700 .081600 .000000 .334900 .000400 .000000 .989100 .023200 .000000 171 .725800 .002500 .000000 .244700 .000000 .000000 .508500 .000000 .000000 <ref type="bibr" coords="7,116.88,473.05,394.57,8.53;7,116.88,484.33,394.57,8.53;7,116.88,495.61,394.57,8.53;7,116.88,506.89,56.17,8.53">172 .495300 .234700 .019000 .013800 .003400 .000000 .207300 .063200 .001100 174 .674500 .329900 .000000 .327000 .009900 .000000 .941700 .194400 .000000 175 .704200 .379300 .000000 .317300 .030700 .000000 .607400 .186900 .000000 176 .492700</ref> .044300 .000000 .108900 .000400 .000000 .629600 .007100 .000000 177 .750000 .000000 .000000 .143800 .000000 .000000 .764600 .000000 .000000 178 .366700 .011600 .000000 .080500 .000100 .000000 .068100 .001000 .000000 179 .307600 .051800 .000000 .050800 .004200 .000000 .717700 .046600 .000000 181 .830300 .578300 .000000 .333600 .117900 .000000 .431200 .151000 .000000 182 .457400 .235700 .000000 .062500 .009700 .000000 .337500 .059800 .000000 183 .521000 .255800 .000000 .060800 .006600 .000000 .740500 .074700 .000000 184 1.00000 .001000 .000000 .067000 .000000 .000000 .572800 .000000 .000000 185 .836800 .487400 .000000 .252500 .034500 .000000 .713500 .209300 .000000 186 .849800 .614900 .000000 .319500 .056100 .000000 .654300 .302900 .000000 187 1.00000 .590900 .000000 .358400 .004000 .000000 .425600 .027300 .000000 Avg .688496 .279050 .003238 .217858 .024008 .000012 .596715 .116862 .000154</p><p>For " UofG1" run, we set the maximum number of passages per document to be 5 and any more passages from the same document are discarded in the final results. As shown in Table <ref type="table" coords="7,458.40,666.59,3.82,9.34" target="#tab_0">1</ref>, this indeed improves the retrieval performance slightly at the document level with the average MAP of 0.36554, but at the cost of decreasing the performance at the passage and aspect levels: 0.028192 and 0.160773, respectively. Due to the restriction of submitting only three runs, we are unable to test the system with different threshold values. Clearly, there are conflicting factors in improving the performance across the passage, aspect, and document levels, and some kind of compromise has to be made in order to get a proper balance between the three different levels.</p><p>For " UofG2" run, we set the minimum similarity level to be 0.5 in order to merge a passage into one of the existing aspects (clusters). As illustrated in Table <ref type="table" coords="8,347.10,130.43,3.86,9.34" target="#tab_0">1</ref>, the performance does not improve; in fact, all the performance numbers are slightly lower than those for " UofG1" run. This is perhaps not surprising, since the heuristic clustering method is not one of the best clustering methods available and the number of terms within a passage is usually too small for computing the distance between passages. The reason we choose the heuristic clustering method is because it allows us to create clusters rapidly with relatively low cost, which is desirable when we are under the constraints of time and machine resources. Clearly, more work needs to be done in order to model and compute aspects efficiently and accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>We show that language models combined by shrinkage are a promising method for passage retrieval. In particular, extending a passage model with information from the corresponding document, journal, and even collection is desirable since a passage is usually too short to capture enough information for matching and comparison purposes. Our approach for language modeling is intuitive and easy to understand, since models are obtained and optimized individually before they are combined with the shrinkage method.</p><p>Although we tried to expand the coverage for documents and aspects, the results did not show much improvement. More study is needed to identify a proper balance for the retrieval performance at the three levels of passages, aspects, and documents. Furthermore, we need to explore different clustering methods to compute the aspects efficiently and accurately. In particular, we need to relax the condition for non-overlapping clusters, since an aspect can correspond to multiple passages, and a passage can also belong to multiple aspects. It will be a big challenge for computing such clustering structures efficiently, since the TREC 2006 Genomics data set contains a total of 12, 641,039 passages, which is quite large to store and compute on a typical personal computer.</p><p>Finally, we could continue improve the performance of our system for passage retrieval with training data. For this year, the data and query sets are both new, so no training data are available. For the next year, we could use the evaluated results from this year as training data and investigate whether the weighting parameters for the model combination can be optimized and set automatically so that we can further improve the retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,180.96,619.07,268.71,89.26"><head>Table 1 .</head><label>1</label><figDesc>A Typical Term Distribution for a Document Collection</figDesc><table coords="2,189.12,637.07,228.48,71.26"><row><cell>tf</cell><cell>N tf</cell><cell>tf</cell><cell>N tf</cell></row><row><cell>0</cell><cell>74,671,100,100</cell><cell>5</cell><cell>68,379</cell></row><row><cell>1</cell><cell>2,018,046</cell><cell>6</cell><cell>48,190</cell></row><row><cell>2</cell><cell>449,721</cell><cell>7</cell><cell>35,709</cell></row><row><cell>3</cell><cell>188,933</cell><cell>8</cell><cell>27,710</cell></row><row><cell>4</cell><cell>105,668</cell><cell>9</cell><cell>22,280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,102.24,297.71,401.87,37.46"><head>Table 2 .</head><label>2</label><figDesc>Statistics Over 68 Automatic Runs</figDesc><table coords="7,102.24,315.37,401.87,19.81"><row><cell></cell><cell cols="2">Document AP</cell><cell cols="2">Passage AP</cell><cell>Aspect AP</cell><cell></cell></row><row><cell>Topic</cell><cell>Best</cell><cell>Median Worst</cell><cell>Best</cell><cell>Median Worst</cell><cell>Best</cell><cell>Median Worst</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,100.16,708.97,293.50,8.53"><p>Actually, there are 49 journals, but one of them is further split into 11 subsets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="funder">NSERC (National Sciences and Engineering Council of Canada)</rs> and <rs type="funder">OCE (Ontario Centres of Excellence)</rs> for their funding support, which helped us tremendously in completing this research work.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,94.80,664.09,397.24,8.53" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Baeza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m" coord="8,282.46,664.09,112.34,8.53">Modern Information Retrieval</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.80,685.69,419.48,8.53;8,94.80,696.49,81.89,8.53;8,176.64,694.48,4.53,5.48;8,183.76,696.49,104.08,8.53" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,245.56,685.69,251.77,8.53">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName coords=""><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,94.80,696.49,81.89,8.53;8,176.64,694.48,4.53,5.48;8,183.76,696.49,15.81,8.53">Proceedings of the 34 th ACL</title>
		<meeting>the 34 th ACL</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,70.57,417.04,8.53;9,94.80,81.37,304.84,8.53" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,244.92,70.57,102.34,8.53">Text retrieval and inference</title>
		<author>
			<persName coords=""><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><forename type="middle">R</forename><surname>Turtle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,366.03,70.57,116.15,8.53">Text-Based Intelligent Systems</title>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Jacob</surname></persName>
		</editor>
		<imprint>
			<publisher>Lawrence Erlbaum Associates Publisher</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="127" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,103.21,389.73,8.53;9,94.80,114.01,205.25,8.53" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,147.96,103.21,262.35,8.53">A linguistically motivated probabilistic model of information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,418.56,103.21,65.96,8.53;9,94.80,114.01,118.13,8.53">Second European Conference on Digital Libraries</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="569" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,135.37,408.51,8.53;9,94.80,146.17,65.11,8.53" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,273.16,135.37,206.84,8.53">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,167.77,403.91,8.53;9,94.80,178.81,228.31,8.53;9,323.04,176.80,4.53,5.48;9,330.28,178.81,178.78,8.53;9,94.80,189.61,81.14,8.53" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,379.50,167.77,119.21,8.53;9,94.80,178.81,127.82,8.53">Improving text classification by shrinkage in a hierarchy of classes</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronald</forename><surname>Resonfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,241.40,178.81,81.71,8.53;9,323.04,176.80,4.53,5.48;9,330.28,178.81,174.76,8.53">Proceedings of the 15 th International Conference on Machine Learning</title>
		<meeting>the 15 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,211.21,386.12,8.53;9,94.80,222.01,180.98,8.53" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,265.84,211.21,198.27,8.53">A hidden markov model information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,94.80,222.01,93.25,8.53">Proceedings of SIGIR&apos;99</title>
		<meeting>SIGIR&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,243.61,413.25,8.53;9,94.80,254.41,122.88,8.53" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,227.89,243.61,205.83,8.53">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,452.23,243.61,55.82,8.53;9,94.80,254.41,34.83,8.53">Proceedings of SIGIR&apos;98</title>
		<meeting>SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,276.01,416.79,8.53;9,94.80,286.81,195.83,8.53" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,262.71,276.01,244.92,8.53">A maximum entropy approach to identifying sentence boundaries</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,105.04,286.81,106.36,8.53">Proceedings of the ANLP&apos;97</title>
		<meeting>the ANLP&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,308.41,423.33,8.53;9,94.80,319.21,164.69,8.53" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,237.64,308.41,136.97,8.53">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,383.37,308.41,134.76,8.53;9,94.80,319.21,78.44,8.53">Journal of the American Society for Information Sciences</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,340.81,399.87,8.53;9,94.80,351.85,133.70,8.53" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,154.45,340.81,340.22,8.53;9,94.80,351.85,35.09,8.53">Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,94.80,373.45,396.47,8.53;9,94.80,384.01,426.48,8.53;9,94.80,394.81,20.97,8.53;9,94.80,416.41,224.45,8.53" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,212.17,373.45,190.92,8.53">A general language model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<ptr target="http://ir.ohsu.edu/genomics/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,421.54,373.45,69.73,8.53;9,94.80,384.01,361.52,8.53">Proceedings of the Eighth ACM International Conference on Information and Knowledge Management (CIKM&apos;99)</title>
		<meeting>the Eighth ACM International Conference on Information and Knowledge Management (CIKM&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999. 2006</date>
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="9,94.80,438.25,402.44,8.53;9,94.80,449.05,304.56,8.53" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,235.56,438.25,261.67,8.53;9,94.80,449.05,76.75,8.53">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,179.80,449.05,131.12,8.53">Proceedings of the ACM-SIGIR&apos;01</title>
		<meeting>the ACM-SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
