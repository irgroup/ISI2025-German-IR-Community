<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,58.12,107.13,478.98,12.91">TREC2006 Question Answering Experiments at Tokyo Institute of Technology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.22,146.63,87.74,10.76"><forename type="first">Edward</forename><surname>Whittaker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Ookayama, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.27,146.63,58.95,10.76"><forename type="first">Josef</forename><surname>Novak</surname></persName>
							<email>novakj@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Ookayama, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.54,146.63,68.73,10.76"><forename type="first">Pierre</forename><surname>Chatain</surname></persName>
							<email>pierre@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Ookayama, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.58,146.63,67.41,10.76"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
							<email>furui@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Ookayama, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,58.12,107.13,478.98,12.91">TREC2006 Question Answering Experiments at Tokyo Institute of Technology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EAB6103F0972790324EF77EC5161312C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe Tokyo Institute of Technology's speech group's second attempt at the TREC2006 question answering (QA) track. Keeping the same theoretical QA model as for the TREC2005 task this year we investigated combinations of variations of models focusing once again on the factoid QA task. An experimental run combining translated answers from separate English, French and Spanish systems proved inconclusive. However, our best combination of all component models gave us a factoid performance of 25.1% (placing us 9th and well above the median of the 30 participating systems of 18.6%) and an overall performance including the results from the list and other question tasks of 11.6% (which was somewhat below the median of 13.4%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe the application of our datadriven and non-linguistic framework for the factoid QA task of TREC2006 that was applied succesfully in the TREC2005 Question Answering (QA) track <ref type="bibr" coords="1,226.89,546.99,10.57,8.97" target="#b6">[7]</ref>. For convenience we copy verbatim the exposition of our mathematical model for question answering in Section 2.</p><p>Three runs were submitted for evaluation (asked06a,b,c) that comprised various combinations of QA systems, primarily based on systems employing our novel, statistical approach. For the list task, an extension to the system used in the factoid QA task was used. For the other question task a variation on a system used for speech summarization <ref type="bibr" coords="1,111.63,655.48,11.62,8.97" target="#b2">[3]</ref> was used, identical to one of the systems used last year.</p><p>Description of system settings and results for this year's task are given in Section 7. A discussion and conclusion are given in Sections 8 and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Factoid question task</head><p>This section is re-produced verbatim from the paper "TREC2005 Question Answering Experiments at Tokyo Institute of Technology" <ref type="bibr" coords="1,399.31,311.39,11.42,8.97" target="#b6">[7]</ref>.</p><p>It is clear that the answer to a question depends primarily on the question itself but also on many other factors such as the person asking the question, the location of the person, what questions the person has asked before, and so on. Although such factors are clearly relevant in a real-world scenario they are difficult to model and also to test in an offline mode, for example, in the context of the TREC evaluations. We therefore choose to consider only the dependence of an answer A on the question Q, where each is considered to be a string of l A words A = a 1 , . . . , a l A and l Q words Q = q 1 , . . . , q l Q , respectively. In particular, we hypothesize that the answer A depends on two sets of features W = W(Q) and X = X (Q) as follows:</p><formula xml:id="formula_0" coords="1,372.42,493.73,172.69,8.97">P (A | Q) = P (A | W, X),<label>(1)</label></formula><p>where W = w 1 , . . . , w l W can be thought of as a set of l W features describing the "question-type" part of Q such as when, why, how, etc. and X = x 1 , . . . , x l X is a set of l X features comprising the "information-bearing" part of Q i.e. what the question is actually about and what it refers to. For example, in the questions, Where was Tom Cruise married? and When was Tom Cruise married? the informationbearing component is identical in both cases whereas the question-type component is different.</p><p>Finding the best answer Â involves a search over all A for the one which maximizes the probability of the above model:</p><formula xml:id="formula_1" coords="1,372.97,681.94,172.14,17.10">Â = arg max A P (A | W, X).<label>(2)</label></formula><p>This is guaranteed to give us the optimal answer in a maximum likelihood sense if the probability distribution is the correct one. We don't know this and it's still difficult to model so we make various modeling assumptions to simplify things. Using Bayes' rule this can be rearranged as</p><formula xml:id="formula_2" coords="2,105.68,144.05,180.68,22.31">arg max A P (W, X | A) • P (A) P (W, X) .<label>(3)</label></formula><p>The denominator can be ignored since it is common to all possible answer sequences and does not change. Further, to facilitate modeling we make the assumption that X is conditionally independent of W given A to obtain:</p><formula xml:id="formula_3" coords="2,89.01,241.74,197.35,14.58">arg max A P (X | A) • P (W | A) • P (A).<label>(4)</label></formula><p>Using Bayes rule, making further conditional independence assumptions and assuming uniform prior probabilities, which therefore do not affect the optimisation criterion, we obtain the final optimisation criterion:</p><formula xml:id="formula_4" coords="2,104.11,332.69,182.26,33.18">arg max A P (A | X) retrieval model • P (W | A) f ilter model .<label>(5)</label></formula><p>The P (A | X) model is essentially a language model which models the probability of an answer sequence A given a set of information-bearing features X, similar to the work of <ref type="bibr" coords="2,98.79,414.15,10.57,8.97" target="#b5">[6]</ref>. It models the proximity of A to features in X. We call this model the retrieval model and examine it further in Section 2.1.</p><p>The P (W | A) model matches an answer A with features in the question-type set W . Roughly speaking this model relates ways of asking a question with classes of valid answers. For example, it associates dates, or days of the week with when-type questions. In general, there are many valid and equiprobable A for a given W so this component can only re-rank candidate answers retrieved by the retrieval model. If the filter model were perfect and the retrieval model were to assign the correct answer a higher probability than any other answers of the same type the correct answer should always be ranked first. Conversely, if an incorrect answer, in the same class of answers as the correct answer, is assigned a higher probability by the retrieval model we cannot recover from this error. Consequently, we call it the filter model and examine it further in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval model</head><p>The retrieval model essentially models the proximity of A to features in X. Since A = a 1 , . . . , a l A we are actually modeling the distribution of multi-word sequences. This should be borne in mind in the following discussion whenever A is used. As mentioned above, we currently use a deterministic information-feature mapping function X = X (Q). This mapping only generates word m-tuples (m = 1, 2, . . .) from single words in Q that are not present in a stop-list of around 50 high-frequency words. In principle the function could of course extract deeper linguistic features but we leave this for future work.</p><p>We first assume that a corpus of text data S is available for searching for answers comprising |S| sentences S 1 , . . . , S |S| and |U | documents and a vocabulary V of |V | unique words. We use the notation X i to define an active set of the features x 1 , . . . , x l X such that</p><formula xml:id="formula_5" coords="2,308.86,208.36,236.25,22.22">X i = x 1 • δ(d 1 ), x 2 • δ(d 2 ), . . . , x l X • δ(d l X )</formula><p>where δ(•) is a discrete indicator function which equals 1 if its argument evaluates true (i.e. its argument(s) are equal, is not an empty set, or is a positive number) and 0 if false (i.e. its argument(s) are not equal, is an empty set, is 0 or is a negative number) and</p><formula xml:id="formula_6" coords="2,308.86,278.17,219.97,14.11">d = [d 1 , . . . , d l X ] is the solution 1 to i = l X j=1 2 j-1 d j .</formula><p>The probability P (A | X) is modeled as a linear interpolation of the 2 l X distributions<ref type="foot" coords="2,436.00,305.16,3.49,6.28" target="#foot_1">2</ref> :</p><formula xml:id="formula_7" coords="2,353.89,334.31,191.22,31.97">P (A | X) = 2 l X -1 i=0 λ Xi • P (A | X i ),<label>(6)</label></formula><p>where λ Xi = 1/2 l X for all i, P (A | X 0 ) is a zerogram distribution, and P (A | X i ) is the conditional probability of A given the feature set X i and is computed as the maximum likelihood estimate from the corpus S:</p><formula xml:id="formula_8" coords="2,376.17,447.85,168.94,23.23">P (A | X i ) = N (A, X i ) N (X i ) ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_9" coords="2,326.44,527.49,218.67,31.18">N (A, X i ) = |S| j=1 δ(X i ∈ X (S j )) • δ(A ∈ S j ),<label>(8)</label></formula><formula xml:id="formula_10" coords="2,338.34,567.31,206.77,20.06">N (X i ) = v∈V N (v, X i ).<label>(9)</label></formula><p>We modify Equation ( <ref type="formula" coords="2,413.65,608.73,3.87,8.97" target="#formula_9">8</ref>) to include contributions from adjacent sentences weighted by λ adj which typically has a value ≤ 1:</p><formula xml:id="formula_11" coords="3,51.77,99.73,234.53,58.65">N (A, X i ) = |S| j=1 δ(X i ∈ X (S j ))• max{δ(A ∈ S j ), λ adj •δ(A ∈ S j-1 ), λ adj •δ(A ∈ S j+1 )}. (<label>10</label></formula><formula xml:id="formula_12" coords="3,286.30,149.42,4.15,8.97">)</formula><p>It turns out that smoothing the maximum likelihood estimates from each component distribution has little effect on performance so none is performed. This is partly because of the inherent smoothing effect achieved by interpolating all the distributions together and partly since there is no need to smooth for non-occurring events since such zerotons are never likely to be selected as answers.</p><p>One clear deficiency, however, is the use of equal-valued interpolation weights for all distributions. One might expect a dependence on the number of active features or on N (X i ), however, no such reliable relationship has so far been determined although investigations continue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filter model</head><p>The question-type mapping function W(Q) extracts ntuples (n = 1, 2, . . .) of question-type features from the question Q, such as How, How many and When were. A set of |V W | = 2522 single-word features is extracted based on frequency of occurrence in questions in previous TREC question sets. Some examples include: when, where, who, whose, how, many, high, deep, long etc.</p><p>Modeling the complex relationship between W and A directly is non-trivial. We therefore introduce an intermediate variable representing classes of example questions-andanswers (q-and-a) c e for e = 1 . . . |C E | drawn from the set C E , and to facilitate modeling we say that W is conditionally independent of c e given A as follows:</p><formula xml:id="formula_13" coords="3,77.88,548.39,208.48,31.06">P (W | A) = |C E | e=1 P (W, c e | A)<label>(11)</label></formula><p>=</p><formula xml:id="formula_14" coords="3,147.65,584.55,134.56,31.06">|C E | e=1 P (W | c e ) • P (c e | A). (<label>12</label></formula><formula xml:id="formula_15" coords="3,282.21,595.82,4.15,8.97">)</formula><p>Given a set E of example q-and-a t j for j = 1 . . . |E| where t j = (q j 1 , . . . , q j l Q j , a j 1 , . . . , a j l A j ) we define a mapping function f : E → C E by f (t j ) = e. Each class c e = (w e 1 , . . . , w e l W e , a e 1 , . . . , a e l A e ) is then obtained by</p><formula xml:id="formula_16" coords="3,50.11,687.87,97.78,26.50">c e = j:f (tj )=e W(t j ) l A j i=1</formula><p>a j i , so that:</p><formula xml:id="formula_17" coords="3,318.83,100.66,68.20,46.90">P (W | A) = |C E | e=1 P (W | w e 1 , .</formula><p>. . , w e l W e ) • P (a e 1 , . . . , a e l A e | A). (13)</p><p>Assuming conditional independence of the answer words in class c e given A, and making the modeling assumption that the jth answer word a e j in the example class c e is dependent only on the jth answer word in A we obtain:</p><formula xml:id="formula_18" coords="3,326.33,232.15,218.78,31.18">P (W | A) = |C E | e=1 P (W | c e ) • l A e j=1 P (a e j | a j ).<label>(14)</label></formula><p>Since our set of example q-and-a cannot be expected to cover all the possible answers to questions that may be asked we perform a similar operation to that above to give us the following:</p><formula xml:id="formula_19" coords="3,308.86,349.27,239.04,42.13">P (W | A) = |C E | e=1 P (W | c e ) l A e j=1 |C A | a=1 P (a e j | c a )P (c a | a j ),<label>(15)</label></formula><p>where c a is a concrete class in the set of |C A | answer classes C A . The independence assumption leads to underestimating the probabilities of multi-word answers so we take the geometric mean of the length of the answer (not shown in Equation ( <ref type="formula" coords="3,351.31,454.47,7.97,8.97" target="#formula_19">15</ref>)) and normalize P (W | A) accordingly.</p><p>The system using the above formulation of filter model given by Equation ( <ref type="formula" coords="3,390.34,478.68,8.30,8.97" target="#formula_19">15</ref>) is referred to as model ONE. Systems using the model given by Equation (13) are referred to as model TWO. The training of Model ONE has been described in detail in <ref type="bibr" coords="3,394.64,514.54,10.58,8.97" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reconciling P (A | X) and P (W | A)</head><p>The approach to QA that has been presented is similar in essence to that of approaches to automatic speech recognition (ASR) where there are separate acoustic and language models. In ASR, it is necessary to include a language model weight, α, which raises the probabilities given by the language model to the power α, otherwise performance is very poor:</p><formula xml:id="formula_20" coords="3,342.19,667.58,172.23,32.22">Â = arg max A P (A | X) α • P (W | A) A P (A | X) α • P (W | A ) .</formula><p>Several, possibly related, explanations have been given for this requirement including compensation for the independence assumption. In any case, the dynamic range of the models is typically very different and needs compensating somehow. α can be optimised easily once the individual models have been optimised separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">List question task</head><p>For the list task we used an identical system to last year's evaluation system which itself is very similar to those systems used in the factoid task. Our factoid QA systems always output a list of all the possible answers they encounter in the data, ranked by their probabilities. The issue for the list task is therefore to determine how many of the top answers to output so as to maximise the F-score. Having investigated different methods during the development phase last year for selecting output thresholds we instead chose simply to output the top 10 answers after system combination had been performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Other question task</head><p>As in last year's evaluation we treat the answering of other questions as a summarization task and employ a variation on a method used for speech summarization <ref type="bibr" coords="4,244.20,384.17,11.62,8.97" target="#b2">[3]</ref> for this purpose. This year we chose to only extract nuggets from the AQUAINT corpus (rather than also extracting them from web data) since this data source demonstrated the best results in last year's evaluation. The data for each question, from which the nuggets are to be extracted, is first cleaned to remove words that are unlikely to be required in a nugget but which occur frequently in the data. Duplicate sentences are also removed along with sentences shorter than 40 bytes and longer than 220 bytes. We then select up to 500 sentences which contain as many of the topic words associated with the question as possible, assigning a score to each topic word based on an idf value obtained from the AQUAINT corpus. This results in a single document which is then summarized by selecting up to 175 important sentences according to a combination of a linguistic score (using a 3-gram language model) and a significance score (measured by a tf/idf score), according to the following:</p><formula xml:id="formula_21" coords="4,91.79,608.16,190.42,30.32">S(W ) = 1 N N i=1 {L(w i ) + α • I(w i )}, (<label>16</label></formula><formula xml:id="formula_22" coords="4,282.21,618.57,4.15,8.97">)</formula><p>where N is the number of words in the sentence W , and L(w i ) and I(w i ) are the linguistic score and the significance score of word w i , respectively. Sentences over 140 bytes are compacted so that all nuggets have a length between 40 and 140 bytes, using a similar summarization process. Finally, upto N U max nuggets are selected accord-ing to their final summarization score, making sure that the byte-wise Levenstein distance between two nuggets is less than R% of the bytes in any previously selected sentence.</p><p>Once the set of nuggets had been determined no attempt was made to suppress nuggets that contained answers already given for factoid or list questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">System combination</head><p>For all runs, this year, the answers for the factoid and list tasks were all generated through some form of system combination since this was found to give the best results both in last year's TREC2005 evaluation and also during development.</p><p>The answers for run asked06a were produced through the weighted combination of the output of an English, French and Spanish model ONE system where the weights had been optimised during development. Answers from the French and Spanish systems were translated in to English prior to combination and up to 100 answers from each component run are considered during the combination where the score for an answer is determined as follows:</p><formula xml:id="formula_23" coords="4,375.76,355.08,169.35,26.35">score(a) = s 1 x s r s (a) ,<label>(17)</label></formula><p>where x s refers to the weight for system s, and r s (a) is the rank of answer a in system s, given the current question. If a is not output by system s we define r s (a) = ∞. The answers, sorted by their new score, then form the ranked output of the combined system. Answer combination for runs asked06b,c was performed by simply summing the inverse rank of an answer a from each component system s to generate a new score for the answer as follows:</p><formula xml:id="formula_24" coords="4,380.74,506.62,160.22,26.35">score(a) = s 1 r s (a) . (<label>18</label></formula><formula xml:id="formula_25" coords="4,540.96,513.36,4.15,8.97">)</formula><p>For the other question task, no system combination was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Support generation</head><p>As for the TREC2005 evaluation an adapted version of the projection component ProjectAnswer of the Aranea system <ref type="bibr" coords="4,327.44,632.46,11.62,8.97" target="#b4">[5]</ref> was used for generating answer support from the AQUAINT corpus for each of the answers found using the web. Only the (upto) 1000 documents retrieved by the PRISE search engine and provided by NIST were used for searching for support information for each question. The same tool was used for determining support for answers in all 3 tasks and all runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental work</head><p>Three different systems (asked06a,b,c) were submitted for evaluation with characteristics given in Table <ref type="table" coords="5,274.20,390.69,3.74,8.97" target="#tab_0">1</ref>.</p><p>System asked06a uses only systems based on model ONE and using web data combining the results from English, French and Spanish mono-lingual systems where the questions are translated into the target language (as appropriate) and the answers translated back in to English before combination. System asked06b uses a combination of a model ONE and model TWO system and only Web data. System asked06c combines answers from the set of unique runs that make up systems asked06a,b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Question pre-processing</head><p>Conversion from the XML format provided by NIST to that required by our system was elementary. For each question set the target is extracted and each component question extracted. All target and question strings are then mapped to upper-case. All punctuation except for "'S" is removed both from target and question strings. Then, if the target for a question does not appear character-for-character in that question string it is simply appended to the end of the question string. In general, we feel our approach is quite robust to errors in pre-processing so we do not worry too much about it.</p><p>In addition, although the questions in each set are supposed to be part of a dialogue in which subsequent questions can reference prior questions and answers in the same set, we do not attempt to exploit this. Consequently, each question is treated independently of all other questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Target document preparation</head><p>Our system was designed with web-based question answering in mind. The source of documents we used was obtained by passing each pre-processed, upper-cased question as-is to a web search engine; the top 500 text or HTML documents returned were then downloaded and kept separate for each question. (We relied on the web search engine to strip out stop words from the query.) In contrast to other experiments using web data in the literature <ref type="bibr" coords="5,472.29,504.48,11.62,8.97" target="#b0">[1]</ref> none of our experiments has yet found a point at which performance deteriorates after a certain number of documents. We therefore settled on 500 documents for reasons of expediency rather than optimality. Subsequent text processing of the downloaded documents proceeds in essentially the same way as for question pre-processing except that HTML markup is also removed and sentence boundaries are inserted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Factoid question task</head><p>For system development this year we only optimised performance on the TREC2004 and TREC2005 evaluation questions. Apart from several parameter settings the system used was identical to that used in the TREC2005 evaluation. For training the filter model we use 288812 example q-and-a from the Knowledge Master KM data <ref type="bibr" coords="5,492.49,704.20,11.62,8.97" target="#b1">[2]</ref> plus 2408 q-and-a from the TREC-8,9 and TREC2001 questions, and also the TREC2002,3,4 evaluation q-and-a.</p><p>The most frequent |V C A | = 224000 words from the AQUAINT corpus were used to obtain C A for |C A | = 50, 500, 5000 clusters as described in <ref type="bibr" coords="6,204.47,123.99,10.58,8.97" target="#b7">[8]</ref>. The vocabulary V C A covers approximately 90% of the answers in E. The maximum number of features used in the retrieval model was set to l X = 15 for reasons of speed and memory efficiency.</p><p>The results for all 3 submitted runs on all 3 tasks are shown in Table <ref type="table" coords="6,112.99,196.74,3.74,8.97" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">List question task</head><p>This year once again no development was performed on list questions. Using the same system for answering list questions as was used last year we simply selected the top 10 scoring answers output by each system combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Other question task</head><p>In TREC2005 good results were obtained on the other question task using nuggets obtained only on the AQUAINT data (rather than using web data). Consequently, we used the best performing system for other questions from TREC2005 in this year's evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and analysis</head><p>In this year's evaluation our focus was on the factoid task and on system combination for improved performance. In last year's evaluation and during previous development our method of system combination had been found to be robust and effective at boosting overall system performance.</p><p>This year, we also looked at how we could exploit our mono-lingual QA systems in other languages to increase the variation of documents we considered (and hopefully the orthogonality of answers that are considered for combination) as well as possibly increasing our ability to answer questions about events, persons and locations in other countries. Run asked06a combined our mono-lingual web QA systems for English, French and Spanish. Unfortunately this turned out to be our worst run. Inexact or incorrect translations contributed somewhat to this poor result, however, the main culprit was a simple human error that was introduced during the recombination of the multi-lingual results when modifying them to conform to the TREC submission format. This scrambled the order of the final one third of the answers for this run, and consequently lowered the overall score. Unfortunately, the error was not discovered until after the results for the run had been returned from NIST.</p><p>Over the portion of answers that were not corrupted the percentage of correct and supported answers was 19.4% rather than the 15.4% that was obtained over all answers.</p><p>Run asked06b used a similar combination of component runs as one of last year's best runs and gave a performance on the factoid task this year of 23.6%. The inclusion in run asked06c of the translated French and Spanish runs and also a run from a modified version of the open-source Aranea system <ref type="bibr" coords="6,371.47,170.94,11.61,8.97" target="#b4">[5]</ref> improved system performance by 1.5% absolute to 25.1%. Most of this increase probably comes from the inclusion of the Aranea answers rather than the translated multi-lingual runs though this will be more thoroughly investigated in the future.</p><p>The performance of our best run on the factoid questions increased from 21.3% last year to 25.1% this year with only a small increase in the number of ineXact and Unsupported answers. Nonetheless the absolute values of ineXact and Unsupported answers were high as they were last year. This is not particularly surprising as the projection algorithm which generates support for the answers found on the web was identical to last year and has fairly consistently shown that we suffer a loss of around 20% of our potentially correct answers due to incorrect support information. Consequently the small increase in Unsupported answers is in line with the overall increase in correct answers.</p><p>Although last year's system and this year's were not very different, it is difficult to make any hard conclusions since absolute performance is heavily dependent on the questions asked. A subsequent investigation of the relative performance against other groups will allow some assessment of the question difficulty this year compared to last year.</p><p>Answers to list questions showed small but insignificant improvements over last year's results but other answers scored approximately half of what they did last year. This was the primary factor in our signficantly reduced per series scores this year and a change in the weighting scheme that weighted each of the three tasks equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper we have given a preliminary overview of our work for the TREC2006 question answering evaluation. Our primary focus was on the factoid task and our best run gave performance significantly higher than the median performance of all participants and substantially higher than last year's performance in TREC2005. In this evaluation our performance in the list task was not exceptional but comparable to last year's results, due to the overly simplified modelling of list questions and selection of how many answers to output. Other questions showed a marked reduction in score compared to last year and this together with a new weighting scheme that weighted each of the three tasks equally combined to give substantially lower per-series scores than last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Online demonstration</head><p>A demonstration of the system using model ONE supporting questions in English, Japanese, Chinese, French, Spanish, Russian and Swedish can be found online at http://asked.jp/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Acknowledgments</head><p>This research was supported by JSPS and the Japanese government 21st century COE programme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,62.07,73.96,471.09,223.34"><head>Table 1 . Descriptions of systems developed for TREC2006 including the 3 submitted runs</head><label>1</label><figDesc></figDesc><table coords="5,62.07,73.96,453.25,223.34"><row><cell>System</cell><cell cols="2">Data source Which model</cell><cell></cell><cell>Languages</cell><cell></cell><cell>Submitted run</cell></row><row><cell>asked06a</cell><cell>Web</cell><cell>ONE</cell><cell cols="3">English,French,Spanish</cell><cell>yes</cell></row><row><cell>asked06b</cell><cell>Web</cell><cell>ONE+TWO</cell><cell></cell><cell>English</cell><cell></cell><cell>yes</cell></row><row><cell>asked06c</cell><cell>Web</cell><cell>ONE+TWO</cell><cell></cell><cell>English</cell><cell></cell><cell>yes</cell></row><row><cell>asked06A</cell><cell>Web</cell><cell>Aranea</cell><cell></cell><cell>English</cell><cell></cell><cell>no</cell></row><row><cell>asked06S</cell><cell>Web</cell><cell>ONE</cell><cell></cell><cell>Spanish</cell><cell></cell><cell>no</cell></row><row><cell>asked06F</cell><cell>Web</cell><cell>ONE</cell><cell></cell><cell>French</cell><cell></cell><cell>no</cell></row><row><cell>asked06E</cell><cell>Web</cell><cell>ONE</cell><cell></cell><cell>English</cell><cell></cell><cell>no</cell></row><row><cell cols="7">asked06a,b,c and 4 component runs asked06A,S,F,E that were not submitted for evaluation.</cell></row><row><cell></cell><cell></cell><cell>Factoid task</cell><cell></cell><cell>List</cell><cell>Other</cell><cell>Avg. per-</cell></row><row><cell>System</cell><cell>Right</cell><cell>Unsupp.</cell><cell>ineXact</cell><cell>task</cell><cell>task</cell><cell>series score</cell></row><row><cell>asked06a</cell><cell cols="5">62 (15.4%) 12 (3.0%) 24 (6.0%) 0.052 0.064</cell><cell>0.085</cell></row><row><cell>asked06b</cell><cell cols="5">95 (23.6%) 22 (5.5%) 27 (6.7%) 0.074 0.062</cell><cell>0.116</cell></row><row><cell cols="6">asked06c 101 (25.1%) 26 (6.5%) 27 (6.7%) 0.057 0.060</cell><cell>0.116</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,155.47,312.19,284.28,9.22"><head>Table 2 . Performance on the 3 tasks of the 3 submitted runs.</head><label>2</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,323.21,657.97,221.90,7.17;2,308.86,667.43,153.90,7.82"><p>Note that the value of i is simply the base10 number that represents the binary encoding of the active features in X i .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,323.21,677.16,221.90,7.17;2,308.86,686.63,236.25,7.17;2,308.86,696.09,236.25,7.17;2,308.86,705.56,72.61,7.17"><p>A linear interpolation of models, which borrows directly from statistical language modeling techniques for speech recognition, was found to give retrieval performance approximately twice that of a naive-Bayes or log-linear formulation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,66.71,284.06,219.65,8.07;7,66.71,295.02,219.65,8.07;7,66.71,305.98,219.65,8.07;7,66.71,316.94,219.65,8.07;7,66.71,327.89,20.17,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,246.55,284.06,39.81,8.07;7,66.71,295.02,140.20,8.07">Web Question Answering: is more always better?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,219.91,295.02,66.46,8.07;7,66.71,305.98,219.65,8.07;7,66.71,316.94,147.88,8.07">Proceedings of the 25th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on research and development in information retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.71,338.35,219.65,8.07;7,66.71,349.31,219.65,8.07;7,66.71,360.27,20.17,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,129.70,338.35,152.83,8.07">Knowledge Master Educational Software</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hallmarks</surname></persName>
		</author>
		<ptr target="http://www.greatauk.com/" />
	</analytic>
	<monogr>
		<title level="j" coord="7,66.71,349.31,28.62,8.07">PO Box</title>
		<imprint>
			<biblScope unit="volume">998</biblScope>
			<date type="published" when="2002">2002</date>
			<pubPlace>Durango, CO 81302</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.71,370.73,219.65,8.07;7,66.71,381.69,219.65,8.07;7,66.71,392.65,181.58,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,199.75,370.73,86.61,8.07;7,66.71,381.69,203.50,8.07">Automatic speech summarization based on sentence extraction and compaction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,66.71,392.65,82.56,8.07">Proceedings of ICASSP</title>
		<meeting>ICASSP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.71,403.11,219.65,8.07;7,66.71,414.07,219.65,8.07;7,66.71,425.03,219.65,8.07;7,66.71,435.99,59.52,8.07" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,193.13,403.11,93.23,8.07;7,66.71,414.07,117.70,8.07">Automatically Evaluating Answers to Definition Questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno>LAMP- TR-119/CS-TR-4695/UMIACS-TR-2005-04</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,66.71,446.45,219.65,8.07;7,66.71,457.41,219.65,8.07;7,66.71,468.37,219.65,8.07;7,66.71,479.33,207.43,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,137.42,446.45,148.94,8.07;7,66.71,457.41,215.74,8.07">Question Answering from the Web Using Knowledge Annotation and Knowledge Mining Techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,76.54,468.37,209.82,8.07;7,66.71,479.33,181.16,8.07">Proceedings of Twelfth International Conference on Information and Knowledge Management (CIKM 2003)</title>
		<meeting>Twelfth International Conference on Information and Knowledge Management (CIKM 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.71,489.79,219.65,8.07;7,66.71,500.75,219.65,8.07;7,66.71,511.70,219.65,8.07;7,66.71,522.66,209.73,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,155.05,489.79,131.32,8.07;7,66.71,500.75,76.63,8.07">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,161.22,500.75,125.14,8.07;7,66.71,511.70,219.65,8.07;7,66.71,522.66,102.87,8.07">Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on research and development in information retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.71,533.12,219.65,8.07;7,66.71,544.08,219.65,8.07;7,66.71,555.04,219.65,8.07;7,66.71,566.00,20.17,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,245.51,533.12,40.85,8.07;7,66.71,544.08,219.65,8.07;7,66.71,555.04,22.77,8.07">TREC2005 Question Answering Experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.52,555.04,176.91,8.07">Proceedings of the 14th Text Retrieval Conference</title>
		<meeting>the 14th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,66.71,576.46,219.65,8.07;7,66.71,587.42,219.65,8.07;7,66.71,598.38,158.97,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,222.87,576.46,63.49,8.07;7,66.71,587.42,219.65,8.07;7,66.71,598.38,15.34,8.07">A Statistical Pattern Recognition Approach to Question Answering using Web Data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,98.82,598.38,100.40,8.07">Proceedings of Cyberworlds</title>
		<meeting>Cyberworlds</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
