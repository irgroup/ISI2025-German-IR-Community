<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,192.82,71.86,211.67,13.09;1,139.90,87.93,317.34,12.91">The Alyssa System at TREC 2006: A Statistically-Inspired Question Answering System</title>
				<funder ref="#_rnq8WDZ">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_E99cTvd">
					<orgName type="full">German research council DFG</orgName>
				</funder>
				<funder>
					<orgName type="full">International Graduate College &quot;IGK&quot; between Saarland University</orgName>
				</funder>
				<funder ref="#_xFam85r">
					<orgName type="full">BMBF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,125.83,130.81,49.47,10.76"><forename type="first">Dan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Spoken Language Systems Saarland University</orgName>
								<address>
									<postCode>D-66125</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.24,130.81,93.06,10.76"><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Spoken Language Systems Saarland University</orgName>
								<address>
									<postCode>D-66125</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.24,130.81,82.96,10.76"><forename type="first">Andreas</forename><surname>Merkel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Spoken Language Systems Saarland University</orgName>
								<address>
									<postCode>D-66125</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.15,130.81,84.27,10.76"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Spoken Language Systems Saarland University</orgName>
								<address>
									<postCode>D-66125</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,192.82,71.86,211.67,13.09;1,139.90,87.93,317.34,12.91">The Alyssa System at TREC 2006: A Statistically-Inspired Question Answering System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A331738103427AE0F21F2FF07DE2E85C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present our new statistically-inspired open-domain Q&amp;A research system that allows to carry out a wide range of experiments easily and flexibly by modifying a central file containing an experimental "recipe" that controls the activation and parameter selection of a range of widelyused and custom-built components.</p><p>Based on this, we report our experiments for the TREC 2006 question answering track, where we used a cascade of LMbased document retrieval, LM-based sentence extraction, MaxEnt-based answer extraction over a dependency relation representation followed by a fusion process that uses linear interpolation to integrate evidence from various data streams to detect answers to factoid questions more accurately than the median of all participants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes Alyssa, a new open-domain question answering (Q&amp;A) system developed at Saarland University. <ref type="foot" coords="1,162.40,610.49,3.99,10.99" target="#foot_0">1</ref> It was built as a tool for the investigation of Q&amp;A from a more principled, i.e. information theoretically well-founded approach. Another requirement was the construction of a software that can serve as an experimental platform in the longer term. Therefore, our development efforts were focused on flexibility and modularity rather than performance tuning in the first year of Alyssa participation at TREC. We report our experiments for the TREC 2006 question answering track, where we used a cascade of LM-based document retrieval, LM-based sentence extraction, maximum entropy based answer extraction over a dependency relation representation followed by a fusion process that uses linear interpolation to integrate evidence from various data streams to detect answers to factoid questions more accurately than the median of all participants.</p><p>The remainder of this paper is structured as follows: Section 2 describes the architecture of our system. In Section 3, we describe the specific methods used for TREC 2006, and Section 4 presents our results. Finally Section 5 concludes the paper with a summary and possible future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design Choices</head><p>As pointed out in <ref type="bibr" coords="1,387.49,533.37,66.62,15.04" target="#b6">(Leidner, 2003)</ref>, software engineering in speech &amp; language technology is constrained by the development resources available. According to <ref type="bibr" coords="1,369.32,574.02,90.12,15.04" target="#b2">(Cunningham, 2000)</ref>, the "creation of software infrastructure must be undertaken in conjunction with the development of systems on which the infrastructure is based and which will in turn use its services". In this case, our challenge was to construct a reusable and extensible software system while still being able to deliver first experiments for TREC 2006, to the same deadline. As a consequence, we had to find a feasible minimum-overhead architecture without producing yet another ad-hoc "solution".</p><p>To this end, a modular architecture was chosen that comprises an array of components that are remote-controlled by a central driver module. The #----------------------------# Define settings for doc. IR #----------------------------[document_retrieval] # or: lucene, terrier engine=lemur # or: tdidf dfr lm_dir lm_abs lm_jm method=lm_dir top_n=60 index=default # LSVLM options --------------------# 0=TFIDF 1=OKAPI 2=Kullback-Leibler lemur_model=2 dirichlet_prior=1000 # smoothing factor: jelinek_mercer_lambda=0.5 discount_delta=0.5 crucial insight is that the driver knows about the components, but the components need not know about the existence of the driver. Our design was informed by the design of the QED system <ref type="bibr" coords="2,259.13,353.22,26.46,15.04;2,72.00,366.77,65.73,15.04" target="#b5">((Leidner et al., 2004)</ref>) and other systems, where the kind of processing undertaken is encoded as sequences of file name extensions.</p><p>We designed the core driver module so as to read a global configuration file (Figure <ref type="figure" coords="2,243.13,421.64,4.54,15.04" target="#fig_0">1</ref>) describing all component's activation status (on or off) and the settings of all their parameters in a central place. We use many external tools widely shared across NLP researchers, and they often have their own parameter files, which we generate automatically from the master "recipe", which serves a dual purpose: first, the recipe preserves the details of the experiment as it was run for later inspection (provenance). Second, even the master recipe can later be auto-generated (for example in the future by a graphical GUI) using varied parameter settings in order to explore potentially better component constellations than the ones we currently use.</p><p>The file layout and naming conventions together with the recipe form the backbone of the Alyssa system architecture. Next, we describe how this infrastructure is currently populated by components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Result</head><p>Figure <ref type="figure" coords="2,102.67,709.76,5.44,15.04" target="#fig_1">2</ref> shows the resulting architecture of Alyssa with its various component modules and data streams.</p><p>The question first undergoes question analysis, a phase in which several modules are involved independently. The type of the question is determined and a linguistic analysis is carried out, including full and shallow parses and named entity tagging. Then a query is constructed from the question based on this analysis. The query is run against document retrieval on the AQUAINT index and passage retrieval on a Wikipedia index. An optional co-reference step allows pronouns or NPs to be replaced by their antecedents. Alternative sentence extraction strategies have been implemented based on language modeling and windowing techniques, and extracted sets of sentences undergo further linguistic analysis before being fed into two answer extraction module, one based on patterns, another one based on a supervised machine learning method using dependencies (see below). Special modules take care of event questions, definition questions, and list questions. As a result, several candidate answer streams emerge that are integrated in a answer validation and fusion step. This architecture and module inventory is a superset of what was actually used for TREC 2006, as will become evident in the subsequent experimental description, in part because some modules are still in development, in part because some modules were ready but we simply did not have enough time to carefully test their contribution to performance was beneficial. For example, the Web validation and anaphora resolution modules were not used for TREC 2006 yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Analysis</head><p>We process questions as follows:</p><p>Preprocessing: We apply a simple anaphora resolution strategy for a question: 1. We replace all pronouns in the question with main NP of its target (TNP). 2. For each NP in the question, if it has same head word as the target and shorter length than the target, we replace it with TNP. 3. If the question doesn't contain the target after the previous steps, we choose a noun phrase in the question based on rules and replace it with TNP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Answer Type (EAT) Extraction:</head><p>We chunk questions using Abney's chunker <ref type="bibr" coords="2,321.77,710.44,63.97,15.04">(Abney, 1989)</ref>. Based on chunk sequence of question, we identify a NP chunk which indicates EAT of the question using rules, such as, for Q: Which terrorist organization claimed responsibility for the massacre?, which chunk sequence is which np0 vb0 np1 for np2 ?, the first chunk np0(terrorist organization) following which, is identified as EAT. Furthermore, the head of EAT is mapped to a hand-built ontology to obtain the corresponding named entity (NE) type (e.g. organization corresponds to NE type ORGANIZATION). In the current system, about 50 named entity types (listed in Figure <ref type="figure" coords="4,262.19,170.28,4.54,15.04" target="#fig_2">3</ref>) are considered.</p><p>=3&gt;?&lt;;-&lt;&gt;5/;7E/@7&lt;; +=/&gt;@D . A;7B3&gt;?7@D . 0A7927;5 . &lt;@63&gt;,-9&lt;1/@7&lt;; +17@D . 1&lt;A;@&gt;D . 1&lt;;@7;3;@ . :&lt;A;@/7; . 7?9/;2 . 9/83 . &lt;13/; . =3;7;?A9/ . &gt;7B3&gt; . ?@&gt;/7@ . B&lt;91/;&lt; . &lt;@63&gt;,-;A:03&gt; +1&lt;A;@ . &lt;&gt;23&gt; . :&lt;;3D . =3&gt;7&lt;2 . =3&gt;13;@ . 27?@/;13 . ?=332 . @3:=3&gt;/@A&gt;3 . ?7E3 . /&gt;3/ . B&lt;9A:3 . C3756@ . &lt;@63&gt;,-2/@3-1&lt;9&lt;&gt;-1A&gt;&gt;3;1D-9/;5A/53-:A?71-&lt;11A=/@7&lt;;-&gt;39757&lt;;-?=&lt;&gt;@-C/&gt;-/;7:/9 +07&gt;2 . 7;?31@ . :/::/9 . &lt;@63&gt;,-=9/;@ +49&lt;C3&gt; . @&gt;33 . &lt;@63&gt;,-/00&gt;3B7/@7&lt;;-?D:0&lt;9-=&lt;?@1&lt;23-A&gt;9 Question Pattern Matching: Considering that there are questions with very high frequency to be asked in TREC, we build question patterns (QPTN) to map high frequent questions to classes. Different from <ref type="bibr" coords="4,142.97,384.57,126.23,15.04" target="#b4">(Kaisser and Becker, 2004)</ref> and <ref type="bibr" coords="4,72.00,398.12,79.52,15.04" target="#b13">(Wu et al., 2005)</ref>, question classes are defined in terms of meaning but not syntactic structures of questions. For each question class, we also build answer patterns (APTN) to extract answers for questions in the class, as described in Section 3.5.1. Figure <ref type="figure" coords="4,158.30,465.87,5.44,15.04" target="#fig_3">4</ref> shows QPTNs for question class WHAT CREATION. Given a question Q and a question pattern QP, if the chunk sequence of Q is matched to BFORM of QP, and if the key chunks of Q satisfies all constraints CON listed in CONS of QP, then Q is matched to QP and belongs to then corresponding class. Furthermore, SLOTS of QP records which key chunks of Q will be used in answer pattern matching. 23 question classes, such as WHO CREATION, WHEN CREATION, WHEN BORN, are considered in the system. In TREC 2006, 92 among total 403 factoid questions are classified to question classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency Relation Path Extraction:</head><p>For the remaining questions that are not classified, we use a statistical model to rank candidate answers. The core idea of the model <ref type="bibr" coords="4,203.41,696.89,86.57,15.04;4,72.00,710.44,25.41,15.04" target="#b10">(Shen and Klakow, 2006)</ref> is to compare dependency relations between questions and answer sentences. In question analysis, we use MINIPAR <ref type="bibr" coords="4,175.98,737.53,49.96,15.04" target="#b8">(Lin, 1994)</ref> to parse questions and extract dependency relation paths be- tween question words, such as what, who, when, and all key chunks of questions.</p><formula xml:id="formula_0" coords="4,311.53,70.64,225.12,312.23">06;4BB PK\1+E94CF6A74C:&gt;=+2 0@?C=FB7C2 0@?C=2 058&gt;A&lt;2[NGX JT SU. ZH. 30-58&gt;A&lt;2 06&gt;=B2 06&gt;= PK\1+ZH.+2ZGV.0-6&gt;=2 0-6&gt;=B2 0B;&gt;CB2 0B;&gt;C PK\1+WQTX.+2SU.0-B;&gt;C2 0-B;&gt;CB2 0-@?C=2 0@?C=2 058&gt;A&lt;2[NGX HK SU. TL SU/ 30-58&gt;A&lt;2 06&gt;=B2 06&gt;= PK\1+SU.+2ZGV/0-6&gt;=2 0-6&gt;=B2 0B;&gt;CB2 0B;&gt;C PK\1+WQTX.+2SU/0-B;&gt;C2 0-B;&gt;CB2 0-@?C=2 0@?C=2 058&gt;A&lt;2[NGX HK SU. ,W SU/ 30-58&gt;A&lt;2 06&gt;=B2 06&gt;= PK\1+SU/+2ZGV/0-6&gt;=2 0-6&gt;=B2 0B;&gt;CB2 0B;&gt;C PK\1+WQTX.+2SU.0-B;&gt;C2 0-B;&gt;CB2 0-@?C=2 0-@?C=FB7C2 0D4AB2 0D4A PK\1+ZGV.+2LTYSJ^KWXGHQOWN^LTVR^TVMGSO]K^IVKGXKĤ YOQJ^OSZKSX^JOWITZKV^JKWOMS0-D4A2 0D4A PK\1+ZGV/+2LTYSJOSM^IVKGXOTS^OSZKSXOTS^JOWITZKV\0-D4A2 0-D4AB2 0-6;4BB2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Classification and Typing</head><p>For question classification, we use the taxonomy as proposed in <ref type="bibr" coords="4,407.11,518.27,94.39,15.04" target="#b7">(Li and Roth, 2002)</ref>. It uses 6 coarse and 50 fine grained classes. In our system we used the fine grained classification only because we use specific sub-classes (like DATE, which is a specific sub-class of NUMBER) later on in our sentence retrieval. We use the 5,500 questions provided by the Cognitive Computing Group at University of Illinois at Urbana-Champaign<ref type="foot" coords="4,357.57,625.93,3.99,10.99" target="#foot_1">2</ref> for training and the TREC 10 data for evaluation.</p><p>In terms of classification paradigm we start with the Bayes classifier</p><formula xml:id="formula_1" coords="4,359.64,695.85,165.84,16.31">ĉ = argmax c P (Q|c)P (c)<label>(1)</label></formula><p>which is known to produce the minimum number of misclassifications if the correct probabilities are known (Q is the question and c the question type).</p><p>However, the probability P (Q|c) can easily be calculated using a language model (LM) trained on all questions of class c. The major advantage of the language modeling approach is that we can draw on a vast amount of available techniques to estimate and smooth probabilities even if there is very little training data available. On average, there are only about 100 training questions per question type.</p><p>Specifically, we evaluated absolute discounting, linear interpolation and Dirichlet prior as smoothing techniques. It turns out that absolute discounting in a variant known as Kneser-Ney smoothing for bigram language models gives best possible results. On top of this, we employ a count specific discounting technique.</p><p>The prior P (c) can be considered a unigram language model as well. However, as all classes are seen sufficiently often (that is at least 4 times) there is no smoothing issue at all and relative frequencies can be used.  <ref type="bibr" coords="5,216.82,510.02,73.15,15.04;5,72.00,523.57,25.41,15.04" target="#b14">(Zhang and Lee, 2003)</ref> with the proposed language model based approach, denoted by LM in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Tables 1 compares results from <ref type="bibr" coords="5,219.23,561.39,70.76,15.04;5,72.00,574.95,25.41,15.04" target="#b14">(Zhang and Lee, 2003)</ref> with the proposed LM approach for various machine learning algorithms. As <ref type="bibr" coords="5,218.11,588.49,71.85,15.04;5,72.00,602.05,25.41,15.04" target="#b14">(Zhang and Lee, 2003)</ref> use two different feature sets (bag-of-words and bigram features) we always picked the better result from <ref type="bibr" coords="5,124.52,629.15,100.56,15.04" target="#b14">(Zhang and Lee, 2003)</ref>. The LM approach uses bigram features.</p><p>It is interesting to see that the approach is much better than the Naive Bayes approach even though it uses the same independence assumptions. This difference is probably due to the very much different smoothing technique. The SVM is the best algorithm from <ref type="bibr" coords="5,141.24,723.99,100.20,15.04" target="#b14">(Zhang and Lee, 2003)</ref> however it is outperformed by the LM approach by a small margin. Document retrieval is a crucial part in Question Answering systems. In this part we tried to reduce the number of documents to a smaller, more manageable set of relevant documents. In our experiment we considered a two-step-approach necessary in order to meet the problem. First, a kind of query construction was made to optimize the search for documents. Then the document retrieval was done in a second step using standard language model based techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Query construction</head><p>In this first step, we prepared the query to ideally fit the needs of our document retrieval algorithm. As explained in <ref type="bibr" coords="5,413.21,479.44,88.50,15.04" target="#b9">(Miller et al., 1999)</ref> each term in the query can be weighted with a score. This means that repeating a specific term multiple times would give the latter term more importance in form of a higher score. Because this holds also for our language model approach, we expanded the query in this step with the topic of the question. Figure <ref type="figure" coords="5,363.33,574.28,5.44,15.04" target="#fig_4">5</ref> shows the effects on our document retrieval systems when we expand the query with the topic multiple times on TREC 2004 data. On the x-axis the number of included topics is shown whereas on the y-axis the Mean Average Precision (MAP) is plotted. The performance increases until including the topic three times. So, if the topic is added too often it gets too much weight and other possible important keywords are scored too lowly and, therefore, the retrieval system performs worse.</p><p>There is also just a marginal improvement between adding the topic twice or three times so we decided to include the topic only twice for our ex-perimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Document retrieval and fetching</head><p>For document retrieval the Lemur Toolkit for Language Modeling and Information Retrieval<ref type="foot" coords="6,285.79,115.96,3.99,10.99" target="#foot_2">3</ref> was used. Queries as well as the AQUAINT corpus were stemmed with the Porter stemmer and no stop-word removal was done. As mentioned above, we chose a language model based approach for the retrieval step using unigram distributions.</p><p>The smoothing method we used was Bayesian smoothing with Dirichlet priors. As shown in <ref type="bibr" coords="6,72.00,225.08,99.83,15.04" target="#b3">(Hussain et al., 2006)</ref> smoothing with Dirichlet prior performs best in context of document retrieval experiments and even outperforms traditional information retrieval techniques like Okapi and TFIDF. <ref type="bibr" coords="6,128.41,279.28,93.47,15.04" target="#b3">(Hussain et al., 2006</ref>) also suggests an optimal smoothing parameter for TREC question sets which we used within our experiments as well.</p><p>After the retrieval step, we fetched the best 60 relevant documents because this number was sufficient in previous TREC runs to get about 90% of answers within those documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentence Retrieval</head><p>In this section, we describe the setup for retrieving sentences and the actual re-ranking step. Sentence retrieval, which is just a special case of passage retrieval, is a common step in question answering ( <ref type="bibr" coords="6,76.52,473.32,86.33,15.04" target="#b1">(Clarke et al., 2000)</ref>, <ref type="bibr" coords="6,170.98,473.32,84.45,15.04" target="#b11">(Tellex et al., 2003)</ref>). Normally it is necessary to further reduce the size of a document collection in order to improve finding answers. But there also might be very long documents within the collection or perhaps topic changes within a single document. So the document is further split up to smaller chunks to deal with such events. In our case we split up the documents to single sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Experimental setup</head><p>Before we started with the sentence retrieval experiments some preparations had to be done. First of all we took the retrieved document collection from the previous section (3.3.2) and extracted those documents from a specially prepared AQUAINT corpus. In this prepared corpus, we used a sentence boundary detection algorithm<ref type="foot" coords="6,274.09,703.54,3.99,10.99" target="#foot_3">4</ref> to identify possible ends of sentences.</p><p>After the extraction, the sentences as well as the queries were stemmed using the Porter stemmer. Parallel to this stemming process we did a kind of expansion of queries and sentences. If the expected answer type of a query<ref type="foot" coords="6,445.75,115.35,3.99,10.99" target="#foot_4">5</ref> was DATE then the token DATE was added at the end. The sentences were prepared in almost the same manner. Here, patterns were used to identify possible occurrences of time and date information. Due to this expansion, possible answer candidates for the expected answer type DATE were ranked higher.</p><p>To get an optimal score for those kind of queries we introduced a weighting scheme and experimentally determined the specific weight.</p><p>Again an unigram language model based technique was used to rank the sentences in our experiment. In detail we used Bayesian smoothing with Dirichlet prior which is given by the following formula:</p><formula xml:id="formula_2" coords="6,347.04,332.43,178.49,27.52">p µ (w|d) = c(w; d) + µp(w|C) w c(w; d) + µ (2)</formula><p>Whereas c(w;d) means the count of word w in sentence d, C is the collection of sentences and µ is the smoothing parameter.</p><p>We chose this kind of smoothing because it performed already promising for document retrieval. As smoothing parameter we chose the interpolation weight µ = 100. We got this value by a search for the complete parameter space. Figure <ref type="figure" coords="6,519.80,456.19,5.44,15.04" target="#fig_5">6</ref> shows that this method actually performed better than Jelinek-Mercer linear interpolation. <ref type="foot" coords="6,483.60,482.56,3.99,10.99" target="#foot_5">6</ref>For the re-ranking experiments of the sentences the LSVLM toolkit was used. It is the Language Modeling toolkit from LSV 7 and implements standard language modeling techniques. We decided to use this particular toolkit instead of Lemur because it is much more flexible. So it is easily possible to switch between different language models for interpolation or to manipulate the used vocabulary. In our case, we closed the vocabulary over the query to get a better performance as described in <ref type="bibr" coords="6,318.48,632.36,91.51,15.04" target="#b3">(Hussain et al., 2006)</ref>.</p><p>Another preparation step was to include a dynamical list of stop-words. This list consists of the four most commonly used terms of the complete sentence collection. However, these words were not removed but just got a smaller score. Again a weighting scheme was used to optimal score the stop-words.</p><p>Finally, the queries were optimized. This was done by removing the query word. In most cases the query word has no meaning in terms of searching for relevant sentences so removing it gives a higher score for the rest of the possible relevant query words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Sentence Re-ranking</head><p>The settings described in the previous section were used to do the sentence re-ranking.</p><p>Figure <ref type="figure" coords="7,114.34,439.12,5.44,15.04" target="#fig_5">6</ref> shows first results when testing different language model based techniques with different optimization parameter explained in the previous section on TREC 2004 data. The plot shows on the x-axis the number of sentences returned on a logarithmic scale and on the y-axis the accuracy of the system.</p><p>We experimented with two baseline methods, the Jelinek-Mercer linear interpolation (JM_baseline) and a baseline of the Dirichlet prior smoothing (Dirichlet_baseline). The figure show that both distributions perform nearly equally but the Dirichlet baseline is slightly better when returning just a smaller set of sentences. The third smoothing method (Dirichlet_optimized) shows the language model with optimization changes we described in section 3.4.1. It outperforms the baseline distributions for nearly all cases. The interesting point in regard to our experiments is that we had to go back to just a smaller number of sentences to obtain the same accuracy as for a baseline method.</p><p>Table 2 also shows the significant improvement of our smoothing method regarding the Mean Distribution MRR JM baseline 0.29 Dirichlet baseline 0.31 Dirichlet optimized 0.39 Table 2: MRR of baseline and optimized experiments.</p><p>Reciprocal Rank (MRR) of the distributions for TREC 2004 dataset. A performance gain of 25% can be observed between the Dirichlet prior baseline and the optimized language model and a gain of actually more than 34% regarding the Jelinek-Mercer linear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Extraction</head><p>For the sentences retrieved by SR Module, we first process them by using linguistic tools including LingPipe (http://www.alias-i.com/lingpipe/) for named entity recognition, Abney's chunker <ref type="bibr" coords="7,307.28,342.23,65.40,15.04">(Abney, 1989)</ref> for NP chunking and MINIPAR <ref type="bibr" coords="7,307.28,355.78,52.31,15.04" target="#b8">(Lin, 1994)</ref> for dependency parsing. Next, we apply two strategies, which are mainly based on surface text pattern matching and correlation of dependency relation path respectively, to extract exact answers from the processed sentences. We set them as pipeline structure. For each sentence, firstly, answer patterns (Section 3.5.1) are used to match candidate answers. If matched, the candidate answers will be returned. Otherwise, path correlation-based Maximum Entropy model (Section 3.5.2) is applied to rank the candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Chunk-based Surface Text Pattern</head><p>Matching For the questions in question classes, answer patterns <ref type="bibr" coords="7,346.17,574.95,35.73,15.04">(APTN)</ref> indicate what are expected answer positions in surface sentences. APTNs are represented as regular expressions over tokens, using the variables slot, var and ANSWER. slot is bound to the key chunks of questions. A question chunk, expected by certain slots, is assigned in question pattern matching. var is a set of special alternative words, which are usually shared by various patterns and also assigned in question pattern matching. ANSWER indicates the expected answer. Figure <ref type="figure" coords="7,409.85,710.44,5.44,15.04">7</ref> shows APTNs for question class WHAT CREATION. Patterns are manually authored for the system. However, results show that the coverage is not satisfactory since only 12 questions are answered by pattern matching.</p><p>Figure <ref type="figure" coords="8,104.97,340.90,4.24,15.04">7</ref>: Example of answer patterns for class WHAT CREATION.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Correlation of Dependency Relation</head><p>Path If none of candidate answers are matched to APTNs, we further compare dependency relations between candidate answers and mapped question chunks in sentences with corresponding relations in questions. A correlation measure is proposed to calculate distance between two dependency relation paths. The parameters of the measure are estimated on a set of question answer pairs in previous TREC. Next, the correlations are incorporated in a Maximum Entropy-based ranking model which estimates path weights from training. Lastly, topranked candidate answer in each sentence is returned. <ref type="bibr" coords="8,112.74,576.12,118.23,15.04" target="#b10">(Shen and Klakow, 2006)</ref> presents the detailed information of the model. Results show that it extracts 65 among 403 factoid questions in TREC 2006.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Fusion</head><p>One issue with fusion of different processing streams and different modules in a QA system is that scores are calculated in various ways and in particular that they are hardly ever probabilities. This makes fusion a difficult task.</p><p>However, it is well known that probability distributions in natural language applications (but also beyond this) are often Zipf-distributed. As all the modules produce a ranking on one way or other, we use Zipf's Law to convert ranks into probabilities. A simple version of this idea is an arithmetic average of inverse ranks which was proposed in <ref type="bibr" coords="8,346.62,116.08,99.88,15.04" target="#b12">(Whittaker et al., 2005)</ref>.</p><p>Here we use for the probability of an answer created by module i being correct a Mandelbrot distribution which is a refinement of the Zipf distribution. It has one more parameter that allows to tune the flatness of the distribution for top ranked answers. This is important where modules put several good answers on top but can not discriminate between the alternatives. The Mandelbrot function is defined by</p><formula xml:id="formula_3" coords="8,364.91,263.55,160.62,26.67">P i (A) = N (r i (A) + µ) β<label>(3)</label></formula><p>where r i (A) is the rank on which module i puts the answer A and µ and γ are parameters determine on the TREC 2004 data. N is a normalization factor depending on µ and γ.</p><p>The actual fusion is a linear interpolation</p><formula xml:id="formula_4" coords="8,370.04,374.48,155.46,32.19">P (A) = N i=1 λ i P i (A)<label>(4)</label></formula><p>where N is the number of components to be fused and λ i are the interpolation weights satisfying the normalization constraint 1 = N i=1 λ i . All the parameters are optimized on the TREC 2004 data. We fuse the results of our answer extraction with the sentence retrieval results, the Wikipedia system and the output of the web validation module. Note that the web validation modules was not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We carried out our TREC experiments on three nodes part of a Linux Beowulf cluster with 2.6 GHz Intel Xeon multi-core CPUs (512 MB RAM each). While the cluster is equipped with a master node featuring a 1 TB RAID system, we only used the nodel-local 300 GB hard disk drives to avoid delays caused by NFS overhead.</p><p>Table <ref type="table" coords="8,345.31,656.25,5.44,15.04" target="#tab_2">3</ref> shows the three runs we submitted and our results obtained. The first run uses only the AQUAINT corpus to retrieve answer candidates from, whereas the second run adds Wikipedia candidates that are used to validate AQUAINT candidates. The third run uses more answer extraction patterns (in addition to the ones already used in the other runs).  As the numbers show, despite the fact that Alyssawas implemented in just a few months prior and during TREC, it already performed better than the median of participants. We attribute this success to</p><p>• the flexibility of the system architecture, which allowed us to experiment with a variety of approaches systematically (while the system implementation was still in progress), combined with</p><p>• our data-driven approach: at any point in time we try to improve the component that was the bottleneck (in terms of most errors introduced).</p><p>Surprisingly, our LIST and OTHER performance is not dramatically worse that the average despite no specific efforts were made to address these types, which indicates that the state of the art is still very basic. This may be attributed to the fact that we used Wikipedia rather than AQUAINT as the source for definition questions.</p><p>Unlike other groups, we did not observe an increase of unsupported answers. The lesson we learn from this is to use Wikipedia, but to use it wisely, i.e. to support AQUAINT answers rather than to extract candidate answers directly from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Conclusions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Summary</head><p>We have presented the new open-domain Q&amp;A system Alyssa and our experiments for the TREC 2006 question answering track using the system to explore a statistically-inspired approach to question answering.</p><p>Using a flexible software architecture that allowed to switch methods easily we carried out a series of experiments. Our best results outperformed the median over all TREC 2006 Q&amp;A track participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Future Work</head><p>In future work, we are planning to use the existing system to carry out further experiments, such as studying the impact of the substitution of pronouns by their antecedents as a recall-enhancing device.</p><p>We are also planning to improve our system along four directions: First, we would like to develop and integrate more fine-grained named entity tagging. Second, we are planning to investigate more sophisticated learning methods for the integration of the evidence, both at the fusion step and the component level. Third, making use of existing patterns from the answer extraction could be used for precise Web validation patterns. Finally, we would like to carry out an ablation study analyzing the errors made by the systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,259.44,218.00,15.04;2,72.00,272.99,81.54,15.04"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A central experimental description facilitates experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,226.07,772.02,145.22,15.25"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of Alyssa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,122.33,301.26,117.49,15.04"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: List of NE types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,307.28,404.53,218.01,15.04;4,307.28,422.62,87.61,9.82"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of question patterns for class WHAT CREATION.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,307.28,249.50,218.00,15.04;5,307.28,263.05,91.38,15.04"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number of included topics versus MAP for TREC 2004 data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,72.00,220.53,217.97,15.04;7,72.00,234.08,217.97,15.04;7,72.00,247.63,75.95,15.04"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Number of returned sentences versus accuracy of baseline and optimized experiments for TREC 2004 data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,382.95,217.98,142.11"><head>Table 1 :</head><label>1</label><figDesc>Comparison of various algorithms (Naive Bayes, ... SVM) investigated in</figDesc><table coords="5,119.11,382.95,124.00,100.71"><row><cell></cell><cell>Accuracy</cell></row><row><cell>Naive Bayes Neural Network SNoW Decision Tree SVM LM</cell><cell>67.8% 68.8% 75.8% 77.0% 80.2% 80.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,156.30,120.34,284.61,15.04"><head>Table 3 :</head><label>3</label><figDesc>LSV Group Runs and Results Submitted to TREC 2006.    </figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.14,743.26,201.90,12.36;1,72.00,753.22,138.78,12.36"><p>The system is named after Alyssa, the departmental secretary's terrier dog and system mascot.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,323.42,753.22,168.06,12.36"><p>http://l2r.cs.uiuc.edu/∼cogcomp/Data/QA/QC/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,88.14,742.35,104.71,12.36"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,88.14,753.22,154.44,12.36"><p>LingPipe: http://www.alias-i.com/lingpipe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,323.42,721.59,53.97,12.36"><p>see Section 3.2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,323.42,732.47,201.89,12.36;6,307.28,742.43,153.70,12.36"><p><ref type="bibr" coords="6,323.42,732.47,75.75,12.36" target="#b3">(Hussain et al., 2006</ref>) also showed that Jelinek-Mercer performed better than absolute discounting</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Ciprian Raileanu</rs>, <rs type="person">Alexandru Chitea</rs> and <rs type="person">Andrea Heyl</rs> for implementing the modules for Web validation (using the Google Web Service), anaphora resolution module (using GATE's <rs type="programName">ANNIE</rs>) and pattern construction respectively. <rs type="person">Jochen Leidner</rs> was funded in part by a <rs type="grantName">SOCRATES grant</rs> from the <rs type="funder">European Union</rs> and in part by the <rs type="projectName">BMBF</rs> project <rs type="projectName">SmartWeb</rs>. <rs type="person">Dan Shen</rs> was funded by the <rs type="funder">German research council DFG</rs> through the <rs type="funder">International Graduate College "IGK" between Saarland University</rs> and the <rs type="institution">University of Edinburgh</rs>.</p><p>Andreas Merkel was funded by the <rs type="funder">BMBF</rs> project <rs type="projectName">SmartWeb</rs>.</p><p>We thank to <rs type="person">Johan Bos</rs> and <rs type="person">Sebastian Pad</rs> ó for discussions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_rnq8WDZ">
					<orgName type="grant-name">SOCRATES grant</orgName>
					<orgName type="project" subtype="full">BMBF</orgName>
					<orgName type="program" subtype="full">ANNIE</orgName>
				</org>
				<org type="funded-project" xml:id="_E99cTvd">
					<orgName type="project" subtype="full">SmartWeb</orgName>
				</org>
				<org type="funded-project" xml:id="_xFam85r">
					<orgName type="project" subtype="full">SmartWeb</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,148.99,107.99,21.72,7.21;8,72.95,116.67,36.21,7.21;8,83.82,125.36,115.86,7.21;8,83.82,134.05,137.58,7.21;8,83.82,142.74,137.58,7.21;8,83.82,151.43,144.82,7.21;8,83.82,160.12,155.68,7.21;8,83.82,168.81,130.34,7.21;8,83.82,177.50,199.13,7.21;8,83.82,186.19,217.23,7.21;8,83.82,194.88,191.89,7.21;8,83.82,203.57,137.58,7.21;8,83.82,212.26,137.58,7.21;8,83.82,220.95,152.06,7.21;8,83.82,229.64,148.44,7.21;8,83.82,238.33,159.30,7.21;8,83.82,247.01,159.30,7.21;8,83.82,255.70,159.30,7.21;8,83.82,264.39,162.93,7.21;8,83.82,273.08,159.30,7.21;8,83.82,281.77,133.96,7.21;8,83.82,290.46,173.79,7.21;8,83.82,299.15,155.68,7.21;8,72.96,307.84,39.83,7.21;8,72.96,316.53,28.96,7.21;9,307.28,564.97,55.52,10.76;9,307.28,582.15,174.46,13.74;9,489.49,586.29,35.86,8.97;9,318.18,597.25,64.88,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,91.06,160.12,97.75,7.21;8,109.16,177.50,112.24,7.21;8,109.16,186.19,141.20,7.21">AEG=D -WNRZUNIUZWNOKN. JL -VIS4. -RMZJX</title>
		<idno>AEG=D--0Z1.. -VIS4. -JXZRM. TQRU372</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,91.06,238.33,101.38,7.21">9;AEG=D ,T-MOSTU.: -VIS3</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>CFA. Z1.. ;AEG=D ,T TQRU372;CFA9 72;CFAHE=F9 72&lt;@;EE9 References Steven Abney. 1989. Parsing by chunks. The MIT Parsing Volume</note>
</biblStruct>

<biblStruct coords="9,307.28,616.89,218.05,13.74;9,318.18,627.86,207.16,13.74;9,318.18,638.81,145.79,13.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,352.21,627.86,173.14,13.74">Question answering by passage selection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kisman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>MultiText experiments for TREC 9</note>
</biblStruct>

<biblStruct coords="9,307.27,662.60,120.70,13.74;9,437.20,666.73,88.12,8.97;9,318.18,677.68,110.48,8.97;9,436.44,673.55,88.90,13.74;9,318.18,684.51,207.12,13.74;9,318.18,695.47,95.67,13.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,437.20,666.73,88.12,8.97;9,318.18,677.68,106.17,8.97">Software Architecture for Language Engineering</title>
		<author>
			<persName coords=""><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Sheffield, England, UK.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Deptartment of Computer Science, University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="9,307.27,719.25,218.06,13.74;9,318.18,730.21,207.17,13.74;9,318.18,741.17,207.15,13.74;9,318.18,756.26,183.51,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,395.66,730.21,129.69,13.74;9,318.18,741.17,191.75,13.74">Dedicated backing-off distributions for language model based passage retrieval</title>
		<author>
			<persName coords=""><forename type="first">Munawar</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Merkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,318.18,756.26,133.32,8.97">Hildesheimer Informatik-Berichte</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,62.93,218.06,13.74;10,82.91,73.89,207.14,13.74;10,82.91,84.85,28.28,13.74;10,113.15,88.98,176.88,8.97;10,82.91,99.94,55.83,8.97;10,141.24,95.80,23.86,13.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,218.98,62.93,71.08,13.74;10,82.91,73.89,207.14,13.74;10,82.91,84.85,12.23,13.74">Question answering by searching large corpora with linguistic methods</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kaisser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,113.15,88.98,176.88,8.97">Proceedings of the Text Retrieval Conference</title>
		<meeting>the Text Retrieval Conference</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,115.73,218.07,13.74;10,82.91,126.69,207.17,13.74;10,82.91,137.65,207.15,13.74;10,82.91,148.61,207.15,13.74;10,82.91,159.57,70.49,13.74;10,157.86,163.70,132.21,8.97;10,82.91,174.66,147.28,8.97;10,234.62,170.53,55.43,13.74;10,82.91,181.48,207.13,13.74;10,82.91,192.44,43.40,13.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,82.91,148.61,207.15,13.74;10,82.91,159.57,47.80,13.74">The QED open-domain answer retrieval system for TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jochen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tiphaine</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,157.86,163.70,132.21,8.97;10,82.91,174.66,143.00,8.97">Proceedings of the Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text Retrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="595" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,212.37,218.06,13.74;10,82.91,223.32,187.31,13.74;10,272.26,227.46,17.79,8.97;10,82.91,238.42,207.12,8.97;10,82.91,249.38,207.14,8.97;10,82.91,260.33,150.66,8.97;10,236.89,256.20,53.15,13.74;10,82.91,267.16,113.58,13.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,183.64,212.37,106.42,13.74;10,82.91,223.32,171.25,13.74">Current issues in software engineering for natural language processing</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jochen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Leidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,272.26,227.46,17.79,8.97;10,82.91,238.42,207.12,8.97;10,82.91,249.38,207.14,8.97;10,82.91,260.33,146.19,8.97">Proceedings of the Workshop on Software Engineering and Architecture of Language Technology Systems (SEALTS) held at HLT/NAACL 2003</title>
		<meeting>the Workshop on Software Engineering and Architecture of Language Technology Systems (SEALTS) held at HLT/NAACL 2003<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,287.09,218.06,13.74;10,82.91,298.05,31.51,13.74;10,116.90,302.18,101.00,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,189.69,287.09,100.37,13.74;10,82.91,298.05,16.36,13.74">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,116.90,302.18,95.43,8.97">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,317.98,218.05,13.74;10,82.91,328.93,144.43,13.74;10,230.18,333.06,59.88,8.97;10,82.91,344.03,38.97,8.97;10,124.38,339.89,57.47,13.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,153.25,317.98,136.80,13.74;10,82.91,328.93,127.71,13.74">PRINCIPAR-an efficient, broadcoverage, principle-based parser</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,230.18,333.06,59.88,8.97;10,82.91,344.03,33.41,8.97">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="42" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,359.82,218.03,13.74;10,82.91,370.77,207.14,13.74;10,82.91,381.73,47.47,13.74;10,134.58,385.87,155.48,8.97;10,82.91,396.83,207.14,8.97;10,82.91,407.78,137.34,8.97;10,225.44,403.65,64.60,13.74;10,82.91,414.61,81.19,13.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,108.89,370.77,181.15,13.74;10,82.91,381.73,25.83,13.74">A hidden markov model information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,134.58,385.87,155.48,8.97;10,82.91,396.83,207.14,8.97;10,82.91,407.78,133.46,8.97">Proceedings of SIGIR-99, 22nd ACM International Conference on Research and Development in Information Retrieval</title>
		<meeting>SIGIR-99, 22nd ACM International Conference on Research and Development in Information Retrieval<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,434.53,218.05,13.74;10,82.91,445.50,207.14,13.74;10,82.91,456.45,44.77,13.74;10,130.17,460.58,102.34,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,231.58,434.53,58.47,13.74;10,82.91,445.50,207.14,13.74;10,82.91,456.45,29.23,13.74">Exploring correlation of dependency relation paths for answer extraction</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,130.17,460.58,97.30,8.97">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,476.38,218.04,13.74;10,82.91,487.34,207.15,13.74;10,82.91,498.29,138.75,13.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,113.13,487.34,176.92,13.74;10,82.91,498.29,134.42,13.74">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,518.22,218.06,13.74;10,82.91,529.18,207.13,13.74;10,82.91,540.14,165.71,13.74;10,254.06,544.28,36.00,8.97;10,82.91,555.23,207.13,8.97;10,82.91,566.19,55.83,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,110.11,529.18,179.93,13.74;10,82.91,540.14,139.94,13.74">TREC 2005 question answering experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,254.06,544.28,36.00,8.97;10,82.91,555.23,207.13,8.97">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005. TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,581.98,218.07,13.74;10,82.91,592.94,207.16,13.74;10,82.91,603.90,67.19,13.74;10,153.78,608.03,136.28,8.97;10,82.91,618.99,103.83,8.97;10,189.24,614.86,23.86,13.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,152.34,592.94,137.74,13.74;10,82.91,603.90,47.02,13.74">University of Albany&apos;s ILQUA in TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,153.78,608.03,136.28,8.97;10,82.91,618.99,99.55,8.97">Proceedings of the Text Retrieval Conference (TREC 2005)</title>
		<meeting>the Text Retrieval Conference (TREC 2005)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,634.78,218.06,13.74;10,82.91,645.74,161.29,13.74;10,246.40,649.87,43.66,8.97;10,82.91,660.83,207.14,8.97;10,82.91,671.80,207.14,8.97;10,82.91,682.75,86.08,8.97;10,171.49,678.61,52.49,13.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,220.78,634.78,69.28,13.74;10,82.91,645.74,144.90,13.74">Question classification using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wee</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,246.40,649.87,43.66,8.97;10,82.91,660.83,207.14,8.97;10,82.91,671.80,207.14,8.97;10,82.91,682.75,82.20,8.97">SIGIR &apos;03: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
