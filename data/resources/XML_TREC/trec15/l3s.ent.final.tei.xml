<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,233.42,115.96,148.51,12.62;1,194.62,133.89,226.11,12.62">L3S Research Center at TREC 2006 Enterprise Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.50,172.68,66.26,8.74"><forename type="first">Sergey</forename><surname>Chernov</surname></persName>
							<email>chernov@l3s.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">L3S Research Center</orgName>
								<orgName type="institution">University of Hanover</orgName>
								<address>
									<settlement>Hanover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.00,172.68,84.38,8.74"><forename type="first">Gianluca</forename><surname>Demartini</surname></persName>
							<email>demartini@l3s.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">L3S Research Center</orgName>
								<orgName type="institution">University of Hanover</orgName>
								<address>
									<settlement>Hanover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.76,172.68,62.20,8.74"><forename type="first">Julien</forename><surname>Gaugaz</surname></persName>
							<email>gaugaz@l3s.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">L3S Research Center</orgName>
								<orgName type="institution">University of Hanover</orgName>
								<address>
									<settlement>Hanover</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,233.42,115.96,148.51,12.62;1,194.62,133.89,226.11,12.62">L3S Research Center at TREC 2006 Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AB654FCB74AA8A66D952FFF8BB5E353A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The L3S Research Center submitted four runs at Enterprise Track for the first time in 2006, all of them are based solely on the W3C mailing lists. The first run serves as a fully automatically produced baseline. The second run uses a threshold on the document scores to limit the number of documents used for expert ranking. The third uses in addition a threshold on the experts scores in order to decide how many experts to retrieve. Our last run exploits the manually assigned topic specificity values, which predicts a number of relevant expert for each query. The results show that the simple threshold techniques outperform the baseline, while the current definition of query specificity does not improve the result quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We performed experiments within an Expert Search task in the scope of the Enterprise Track 2006. We based our four techniques solely on the W3C mailing lists. The main assumption was that the author of an email is an expert on the subject addressed by the email. We tested different thresholds on the document score as well as the expert score. Using a set of data-driven thresholds on similarity values we cut off different number of experts per each query.</p><p>One finding of our experiments was that the specific information needs do not assume fewer relevant experts. It was an unexpected result, since normally the more specific your question, the less experts you expect to find. This result should be investigated more carefully, since definition of the task specificity is somewhat vague. It would be interesting to agree on one common scheme for topic specificity definition in the expert search community. We also scheduled more experiments with additional dataset, which we are creating in our group. This dataset will include real world documents, publications and wiki pages. The difference with the W3C collections is that it could be enhanced with a specific expert search interface and can allow tracking user logs while searching experts with it.</p><p>Authors are listed in alphabetical order</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Collection Management</head><p>We based our techniques solely on the W3C mailing lists. The main assumption was that the author of an email is an expert on the subject addressed by the email she wrote. To manage the W3C mailing lists first we created a XML valid file<ref type="foot" coords="2,147.50,174.25,3.97,6.12" target="#foot_0">1</ref> containing the structure of the mailing list collection.</p><p>After this step we parsed the file with an XML parser and created a Lucene (an open source information retrieval library<ref type="foot" coords="2,330.41,198.16,3.97,6.12" target="#foot_1">2</ref> ) Index with the fields described in the Table <ref type="table" coords="2,190.95,211.69,3.88,8.74" target="#tab_0">1</ref>, in order to retrieve relevant emails. The mapping between the candidates and the email authors considered only exact match of the email address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs Description</head><p>After building the inverted index of the mailing list, it is possible to retrieve emails relevant to the topics proposed in Enterprise Track 2006 for the Expert Search Task. We used an internal Lucene TFxIDF ranking function, to get the retrieval status values (RSV) for each email. The produced scoring was used to estimate the expert scores. The authors of the majority of the relevant emails (with at least one query term) were considered experts on the topic. The model was tuned on the results from Expert Search task 2005. In the following sections we describe each run in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline (l3s1)</head><p>The topic structure in Enterprise Track 2006 is different from the topics of 2005. In year 2006 for each topic a Title, a Description, and a Narrative is available in contrast to Title part only of topics of 2005. To have at least one fully automatic and data-independent method we decided to use only the Title part of the query in our first run. It makes the run perfectly comparable with the runs from other participants and from the past year.</p><p>We first retrieved all the emails relevant to the query (composed by the keywords in the Title of the topic) and then we ranked the authors according to the number of relevant emails they have written. In this simple scenario the expert score (ES) is given by the number of emails they have written on the topic. After ranking the authors this first algorithm retrieves the top 5 experts. We assume that in real-world task it is unrealistic to browse as many experts as we normally browse documents. While it is common fact that majority of users do not look after the first 10-30 results, we expect that number of top-k expert should lie in the interval of 5-10 experts. This run is considered to be the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using Document Score Threshold (l3s2)</head><p>The second algorithm uses the whole topic description provided by TREC and builds a weighted "OR" query of all the parts (Title, Description, and Narrative) of the original topic. The final query is obtained with an additional boosting of term weights as follows:</p><formula xml:id="formula_0" coords="3,146.73,386.15,333.87,8.74">Query = 3.0 * T itle OR 2.0 * Description OR 1.0 * N arrative (1)</formula><p>The problem is that for such long queries we have in average the 80% of the collection documents relevant to each query. In this case a good ranking of the retrieved documents should put on the top of the list the most relevant ones. To resolve the "too-many-results" problem we decided to fix a document threshold to limit the number of document considered to assess the expertise of the candidates. We assume that with low RSV we need more documents to decide about the expertise. We fixed a value of RSV to be filled by the top-k retrieved documents.</p><p>We learned the parameters from the topics of 2005 test collection (which uses the same document collection as of 2006) where the relevance judgments were available. We compared different possible thresholds to see which one performed better in terms of a Mean Average Precision (see Figure <ref type="figure" coords="3,375.62,536.57,3.88,8.74" target="#fig_0">1</ref>). The best results were achieved when we consider the top 240 documents on average. Instead of using the fixed number of documents, we calculated that the total sum of RSV for the first 240 documents is equal to 76.5 (on average). The performance of other thresholds is shown on the Figure <ref type="figure" coords="3,282.48,584.39,3.88,8.74" target="#fig_0">1</ref>. To smooth differences between popular and rare queries, we used this RSV threshold rather than fixed number of documents. So for every query we took into account only the top documents until their sum of RSV reaches the value of 76.5.</p><p>After ranking the documents and limiting the set of relevant documents, the expert search were computed as the sum of RSVs of their emails and, as in the run l3s1, only the top 5 experts were retrieved. The third algorithm we proposed is based on the second run with an additional improvement. The process of selecting the documents to decide about the expertise of the candidates is the same as in the run l3s2. The enhancement is done using an Expert Score Threshold (EST) to avoid the retrieving of a fixed number of expert for each query. Our assumption is that there are different types of topics and for some of them there will be more experts and of some others less experts depending on the different characteristics of the topic itself. The relationship between the type of topic and the number of experts on it will be considered more in details for the run l3s4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Average Precision on Expert</head><p>In the run l3s3, instead of retrieving the top N (with N fixed) candidates after ranking them, we decided to compute the ES as the RSV sum over all emails in the relevant set written by the expert and to put a threshold in order to retrieve only the experts with scores above a fixed threshold.</p><p>We decided to retrieve 5 experts on average, but for some topics we retrieved less than 5 and for some other topics we retrieved more than 5. Using the 2005 test collection we found that the average expert score at rank 5 is 1.2, so we fixed the EST at this value, and all experts with the ES above 1.2 were retrieved, but minimum one expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Using the Topic Specificity (l3s4)</head><p>In the last run we assume that the number of available experts depends on the specificity of the topic. For example, there could be a lot of experts for the topic "Web Service Architecture", while only a few for "DOM traversal and range". We manually judged the specificity of each topic as 1 (very broad), 2 (usual topic) to 3 (very narrow). The topic specificity was defined as "If you input the query into a search system, do you expect to find big, moderate or small number of experts?". This definition is both collection and user dependent. On the one hand, only a part of all the possible expertises is present in W3C collection, on the other hand, your expectation about the topic specificity is influenced by your background, for example, some people see a "DOM traversal and range" problem as a very broad area.</p><p>In our experiments we considered three assessors, their pairwise disagreement lies in the interval [0.26;0.38] which is very substantial. It indicates that the notion of topic specificity should be defined in a more consistent manner, which does not allow such a vague interpretation. The average real numbers among three available judgments were used. The correlation between user-defined topic specificity and number of relevant experts in the collection for topics from years 2005 and 2006 is shown on the Fig. <ref type="figure" coords="5,292.20,267.61,4.98,8.74">2</ref> and Fig. <ref type="figure" coords="5,340.22,267.61,3.88,8.74">3</ref>. The actual assignment of the specificity values is presented in the Table 3 (see at the end of the report). From the plots we do not observe any correlation between specificity values and number of experts. This can be either the real situation or just a side effect of our not perfect definition of topic specificity. Currently, it depends heavily on user expectations, while we would like to establish more robust measure for prediction of the expected number of experts. In our run l3s4 we multiplied EST with the coefficients 0.5, 1.0, and 1.5 (for specificity 3, 2, and 1 accordingly) to modify the number of the experts retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of query specificity given number of relevant experts per query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion of the results</head><p>In this section we compare the results obtained with our four runs showing which method performs better than the other.</p><p>run MAP bpref P5 l3s1 0.029 0.042 0.167 l3s2 0.131 0.140 0.571 l3s3 0.106 0.112 0.416 l3s4 0.115 0.122 0.444 Table <ref type="table" coords="6,216.74,355.13,3.59,7.86">2</ref>: Average of some measures across the 2006 topics One possible way to compare our 4 runs is using the 2006 results only. We can see which run performed better comparing the average value of the evaluation measures across the 2006 topics. The results are presented in Table <ref type="table" coords="6,427.82,423.26,4.98,8.74">2</ref> where it is possible to see that the run l3s2 had the best performance and that the run l3s1 (the baseline) was the worst one. The results indicate two main facts. First, the variable number of experts was not helpful in the run l3s3. Second, our definition of topic specificity does not work well for the Expert Search task. While it is still interesting to explore broad range of specific and broad information needs, better predictor is needed for the number of expert to return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We consider the Expert Search task as one of the most important directions of a future information retrieval research. In this report we described the L3S Research Center runs at the Enterprise Track 2006. We conducted experiments on the W3C mailing lists and tested several thresholds on the document and on the expert scores. While the thresholds on the document scores proved to be helpful, the threshold on expert scores did not improve the retrieval performance. The manually assigned topic specificity values, for prediction of the number of relevant experts for each topic, did not work for the current setup. We believe, that it can be an effect of a vague definition of the topic specificity and leave the development of better notion of specificity as a future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,149.64,298.21,316.08,7.86"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Performance of different document thresholds on Enterprise Track 2005</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,152.09,446.31,311.17,7.86"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Distribution of the Topic Specificity in Enterprise Track 2005 Queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,186.22,243.55,242.92,147.17"><head>Table 1 :</head><label>1</label><figDesc>The Email Fields Indexed with Lucene.</figDesc><table coords="2,186.22,243.55,242.92,134.81"><row><cell cols="2">Email Field Description</cell></row><row><cell>Body</cell><cell>Full text of the email</cell></row><row><cell>CC</cell><cell>Email addresses in CC</cell></row><row><cell>DocNo</cell><cell>ID of the supporting document for TREC</cell></row><row><cell>From</cell><cell>Email address of the sender</cell></row><row><cell>ID</cell><cell>Unique ID of the email</cell></row><row><cell>InReplyTo</cell><cell>ID of the email to which this one is an answer</cell></row><row><cell>Name</cell><cell>Name of the sender</cell></row><row><cell>Received</cell><cell>Date of mail receiving</cell></row><row><cell>Sent</cell><cell>Date of mail sending</cell></row><row><cell>Subject</cell><cell>Subject of the email</cell></row><row><cell>To</cell><cell>Email address of the receiver</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,141.61,385.80,493.71"><head>Table 3 :</head><label>3</label><figDesc>Topic Specificity Values (TN -Topic Number, TS -Topic Specificity, NRE -Number of Relevant Experts).</figDesc><table coords="7,136.16,141.61,384.41,476.37"><row><cell cols="2">TN Title</cell><cell cols="3">NRE TS TN Title</cell><cell cols="2">NRE TS</cell></row><row><cell>1</cell><cell>Semantic Web Coordination</cell><cell>11</cell><cell cols="2">2.0 52 ontology engineering</cell><cell cols="2">168 2.0</cell></row><row><cell>2</cell><cell cols="2">Research and Development Interest 8</cell><cell cols="2">1.0 53 W3C translation policy</cell><cell cols="2">180 1.7</cell></row><row><cell>3</cell><cell>Cascading Style Sheets (CSS)</cell><cell>26</cell><cell cols="2">1.0 54 xml digital signature</cell><cell cols="2">163 1.7</cell></row><row><cell>4</cell><cell>Web Services Addressing</cell><cell>44</cell><cell cols="2">2.3 55 Semantic Web Rule Language</cell><cell cols="2">234 1.3</cell></row><row><cell>5</cell><cell>Hypertext Coordination</cell><cell>20</cell><cell cols="2">2.3 56 Rich Web Client</cell><cell>88</cell><cell>1.7</cell></row><row><cell>6</cell><cell>Mobile Web Initiative</cell><cell>16</cell><cell cols="2">3.0 57 OWL Lite Specification</cell><cell cols="2">178 2.0</cell></row><row><cell></cell><cell>Workshop Program Committee</cell><cell></cell><cell cols="2">58 text XML query language</cell><cell cols="2">172 2.3</cell></row><row><cell>7</cell><cell>WCAG reviewers</cell><cell>2</cell><cell cols="2">2.3 60 SOAP security considerations</cell><cell cols="2">165 2.0</cell></row><row><cell>8</cell><cell>P3P Specification</cell><cell>24</cell><cell cols="2">1.7 61 VoiceXML Browser Implementation</cell><cell cols="2">148 2.0</cell></row><row><cell>9</cell><cell>XML Query</cell><cell>47</cell><cell cols="2">1.0 62 Mereology</cell><cell cols="2">177 2.0</cell></row><row><cell cols="2">10 XML Schema</cell><cell>28</cell><cell>1.0 63</cell><cell></cell><cell cols="2">172 2.3</cell></row><row><cell cols="2">11 Voice Browser</cell><cell>86</cell><cell cols="2">2.0 64 MathML specification</cell><cell cols="2">154 2.3</cell></row><row><cell cols="2">12 Web Services Description</cell><cell>33</cell><cell cols="2">2.0 65 RSS</cell><cell cols="2">163 1.7</cell></row><row><cell cols="3">13 Web Content Accessibility Guidelines 42</cell><cell cols="2">2.0 66 parsing MathML</cell><cell cols="2">167 2.7</cell></row><row><cell cols="3">14 Rules Workshop program committee 18</cell><cell cols="2">2.3 67 Privacy on the Web</cell><cell cols="2">201 1.3</cell></row><row><cell cols="2">15 XSL/FO Task Force</cell><cell>12</cell><cell cols="2">3.0 68 semantic search</cell><cell cols="2">160 1.3</cell></row><row><cell cols="2">16 Semantic Web Best</cell><cell>56</cell><cell cols="2">2.0 69 CSS3</cell><cell cols="2">187 2.7</cell></row><row><cell></cell><cell>Practices and Deployment</cell><cell></cell><cell cols="2">70 Evaluation and Report Language</cell><cell cols="2">188 2.0</cell></row><row><cell cols="2">17 Education and Outreach</cell><cell>26</cell><cell cols="2">2.0 71 css floating elements</cell><cell cols="2">226 2.3</cell></row><row><cell cols="2">18 Compound Document Formats</cell><cell>31</cell><cell cols="2">2.7 72 PNG specification</cell><cell cols="2">183 1.7</cell></row><row><cell cols="2">19 Device Independence</cell><cell>19</cell><cell cols="2">2.0 73 DOM traversal and range</cell><cell cols="2">184 3.0</cell></row><row><cell cols="2">20 Math Interest</cell><cell>15</cell><cell>1.7 74</cell><cell></cell><cell cols="2">144 2.3</cell></row><row><cell cols="2">21 Internationalization Tag Set (ITS)</cell><cell>13</cell><cell cols="2">2.7 75 W3Photo</cell><cell cols="2">152 2.7</cell></row><row><cell cols="2">22 W3C's Tenth Anniversary</cell><cell>11</cell><cell cols="2">3.0 76 URI Fragment identifier</cell><cell>0</cell><cell>2.3</cell></row><row><cell></cell><cell>Birthday Celebration attendees</cell><cell></cell><cell cols="2">77 XML schema test collection</cell><cell cols="2">115 2.0</cell></row><row><cell cols="2">23 ERCIM employees</cell><cell>8</cell><cell cols="2">2.0 78 rdf ontology</cell><cell cols="2">177 1.0</cell></row><row><cell cols="2">24 Protocols &amp; Format</cell><cell>12</cell><cell cols="2">1.0 79 Semantic Web</cell><cell cols="2">135 1.0</cell></row><row><cell cols="2">25 Patent and Standards Interest</cell><cell>22</cell><cell cols="2">2.0 80 User Agent Accessibility Guidelines</cell><cell cols="2">148 2.0</cell></row><row><cell cols="2">26 Chairs</cell><cell>65</cell><cell cols="4">1.0 81 Description logics in the Semantic Web 163 1.7</cell></row><row><cell cols="2">27 Authoring Tool Guidelines</cell><cell>7</cell><cell cols="2">2.3 82 APPEL A P3P Preference</cell><cell>0</cell><cell>2.7</cell></row><row><cell cols="2">28 Multimodal</cell><cell>56</cell><cell>1.5</cell><cell>Exchange Language</cell><cell></cell></row><row><cell cols="2">29 Internationalization Core</cell><cell>11</cell><cell cols="2">2.0 83 Semantic interpretation for</cell><cell cols="2">234 2.3</cell></row><row><cell cols="2">30 HTML</cell><cell>15</cell><cell>1.0</cell><cell>speech recognition</cell><cell></cell></row><row><cell cols="2">31 XSL</cell><cell>24</cell><cell cols="2">1.0 84 W3C validation services</cell><cell cols="2">233 1.3</cell></row><row><cell cols="2">32 RDF Data Access</cell><cell>31</cell><cell cols="2">2.0 85 Timed text specifications</cell><cell cols="2">133 2.7</cell></row><row><cell cols="2">33 Advisory Committee</cell><cell cols="3">391 2.0 86 RDF graph serialization</cell><cell cols="2">211 2.7</cell></row><row><cell cols="2">34 SVG</cell><cell>37</cell><cell cols="2">1.3 87 XML Processing Model</cell><cell cols="2">202 2.0</cell></row><row><cell cols="2">35 Social Meaning of RDF and</cell><cell>16</cell><cell cols="2">3.0 88 patent policy</cell><cell cols="2">213 1.7</cell></row><row><cell></cell><cell>URIs Task Force</cell><cell></cell><cell cols="2">89 XML interchange</cell><cell cols="2">143 2.3</cell></row><row><cell cols="2">36 Technical Plenary Attendees</cell><cell>4</cell><cell cols="2">2.7 90 CSS test suite</cell><cell cols="2">152 2.7</cell></row><row><cell cols="2">37 XForms</cell><cell>21</cell><cell cols="2">1.7 91 SVG Accessibility</cell><cell cols="2">216 3.0</cell></row><row><cell cols="2">38 Mobile Web Best Practice</cell><cell>31</cell><cell cols="2">2.0 92 Notation 3</cell><cell cols="2">202 2.0</cell></row><row><cell cols="2">39 Technical Architecture</cell><cell>9</cell><cell cols="2">1.0 93 machine translation</cell><cell cols="2">160 1.3</cell></row><row><cell cols="2">40 XML Coordination</cell><cell>14</cell><cell cols="2">2.0 94 Voice Browser</cell><cell cols="2">178 2.0</cell></row><row><cell cols="2">41 Tech Plenary Program Committee</cell><cell>10</cell><cell cols="2">3.0 95 XSL Transformations</cell><cell cols="2">162 1.3</cell></row><row><cell cols="2">42 SYMM</cell><cell>18</cell><cell cols="2">2.7 96 orphaned annotations</cell><cell cols="2">175 3.0</cell></row><row><cell cols="2">43 URI Coordination</cell><cell>6</cell><cell cols="2">2.7 97 Device Independence Principles</cell><cell cols="2">111 2.3</cell></row><row><cell cols="2">44 Advisory Board</cell><cell>10</cell><cell cols="4">2.0 98 RDF Semantics Datatype Interpretations 160 2.0</cell></row><row><cell cols="2">45 Evaluation &amp; Repair Tools</cell><cell>19</cell><cell cols="2">2.0 99 P3P for my website</cell><cell>0</cell><cell>3.0</cell></row><row><cell cols="2">46 XML Binary Characterization</cell><cell>46</cell><cell cols="2">3.0 100 XML Encryption standard</cell><cell>0</cell><cell>2.3</cell></row><row><cell cols="2">47 XML Core</cell><cell>14</cell><cell cols="2">2.0 101 Implementation of EPAL</cell><cell cols="2">182 2.7</cell></row><row><cell cols="2">48 Internationalization Guidelines,</cell><cell>12</cell><cell cols="2">2.7 102 User Agents testing</cell><cell cols="2">167 2.3</cell></row><row><cell></cell><cell>Education &amp; Outreach (GEO)</cell><cell></cell><cell cols="2">103 Annotea server protocol</cell><cell cols="2">153 2.3</cell></row><row><cell cols="2">49 AC Meeting attendees</cell><cell>2</cell><cell cols="2">2.7 104 Web Service Architecture</cell><cell>0</cell><cell>1.0</cell></row><row><cell cols="2">50 MWI Device Description</cell><cell>10</cell><cell cols="2">2.7 105 Authoring tool web accessibility</cell><cell></cell><cell>1.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.84,308.73,7.86"><p>available at http://www.l3s.de/∼demartini/w3c/w3c-lists-supercleaned.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,656.80,103.96,7.86"><p>http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
