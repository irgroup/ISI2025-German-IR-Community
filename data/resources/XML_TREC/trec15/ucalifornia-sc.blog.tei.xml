<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.61,102.95,304.03,15.11">UCSC on TREC 2006 Blog Opinion Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,234.82,135.43,67.15,10.48"><forename type="first">Ethan</forename><surname>Zhang</surname></persName>
							<email>ethanz@soe.ucsc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California Santa Cruz</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.63,135.43,47.80,10.48"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California Santa Cruz</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.61,102.95,304.03,15.11">UCSC on TREC 2006 Blog Opinion Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">34BBEB604298FB9FA9DA48293BC79EF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of California Santa Cruz team submitted three runs for the TREC Blog Track opinion mining task. We developed a two stage retrieval system. We started with retrieving relevant documents from the corpus for each topic, and then ran each retrieved document through a classifier to estimate the probability that the document contains opinion expressions. The documents were ranked according to the product of the retrieval score and the estimated probability. The Lemur search engine, which is based on the language modeling approach, was used for retrieval. A Bayesian Logistic Regression classifier was trained using a noisy training data set from other domains, which include news articles, product reviews and movie reviews. All runs are automatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Retrieval</head><p>The Lemur Toolkit <ref type="bibr" coords="1,185.52,479.04,10.52,8.74" target="#b0">[1]</ref> was used to index and retrieve blog documents.</p><p>The search engine was implemented based on language modeling. The retrieval model used by the software is best documented in http://ciir.cs.umass.edu/ metzler/indriretmodel.html. HTML tags were stripped while the documents were indexed. Porter stemmer was applied to both index documents and search queries. A stop word list containing 587 common terms was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Logistic Regression Classifier</head><p>Bayesian logistic regression model was used to learn a classifier. In this model, we have:</p><formula xml:id="formula_0" coords="1,126.79,704.29,117.37,22.66">P (y = 1|β, x) = 1 1 + e -β T x</formula><p>where x represents the input vector including the bias x 0 = 1, and β is the parameter vector we are to learn. The output y is computed as follows, y = +1 if P (y = 1|β, x) &gt; 0.5 and -1 otherwise.</p><p>In our blog opinion mining runs, the input x is the vector representation of a blog post, in which each component is a binary number indicating whether a term appears in the document. In other words, if we let T = {t 1 , t 2 , ..., t p } denote the feature set, then x i = 1 if t i is present in the document and x i = 0 if t i is absent from the document. The output y = +1 means the input expresses opinions, and y = -1 means the input does not express opinions. </p><formula xml:id="formula_1" coords="1,412.97,509.03,89.27,11.73">β i P (y i |β, x i )P (β)</formula><p>Let V β be a diagonal matrix with each entry on the diagonal equals v β . Thus the max a posterior estimation (MAP) of β is:</p><formula xml:id="formula_2" coords="1,319.59,575.14,210.68,94.04">β M AP = argmax β P (β|D t ) = argmax β t i=1 P (y i |β, x i )P (β) = argmax β t i=1 log(1 + exp(-y i β T x i )) -v β (β -m β ) 2</formula><p>In the experiments, we arbitrarily set m β = (0, 0, ..., 0) and v β = 1. Conjugate gradient descent method was used to find β M AP .</p><p>We used the product of retrieval score and classification probability as the combined score to order the results. The same classifier was used for all our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Data For Opinion Classification</head><p>Since there are no publicly available labeled blog documents for opinion mining, we looked for training data in other domains. We needed a mixture of documents expressing opinions and texts that are objective. Our training set contained 82,437 documents, which were obtained from three sources:</p><p>• Yahoo Movies reviews:</p><p>We downloaded 15,983 user reviews from http://movies.yahoo.com. The reviews were posted for 15 different movies.</p><p>• Epinions Digital Camera reviews: We used 6,454 reviews from Epinions.com on digital cameras.</p><p>• Reuters newswire: 60,000 Reuters finance news stories were used in the training set.</p><p>Documents in the Yahoo Movies reviews and Epinions Digital Camera reviews sets were automatically labeled as "opinion" and Reuters news documents were labeled "non-opinion". It is worth mentioning that this is a very noisy training data set, since people do express opinions in Reuters news and the characteristics of this data set are very different from the TREC blog data.</p><p>Our goal is to learn a domain and topic independent opinion classifier. However, a classifier trained in a particular domain will overfit the particular domain.</p><p>For example, if only Digital Camera reviews and news stories were used as training data, terms such as "focus" and "lens" would get very heavy weights because in this set they appear frequently in opinion text but rarely in non-opinion documents. We used two sets of review documents from different domains so that the classifier learned won't overfit a particular product category too much. Ideally, we would like to gather review documents from as many domains as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Feature Engineering</head><p>We used 6,511 features that contain 5,604 adjectives chosen from the SentiWordNet lexicon [2] and 907 bigrams and trigrams that were automatically selected from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SentiWordNet</head><p>SentiWordNet is a lexical resource in which each WordNet [3] synset s is assigned three scores: Obj(s), P os(s) and N eg(s), indicating how objective, positive and negative the terms contained in the synset are. The scores add up to 1:</p><formula xml:id="formula_3" coords="2,357.14,279.51,135.59,8.74">Obj(s) + P os(s) + N eg(s) = 1.</formula><p>P os(s) + N eg(s) can be regarded as the subjectivity score of the terms.</p><p>We used in our feature set the subjective adjectives extracted from SentiWordNet 1.0. An adjective is considered subjective if any synset that contains it has a subjectivity score above 0.5. Table <ref type="table" coords="2,504.24,371.09,4.98,8.74" target="#tab_0">1</ref> shows some examples of such adjectives. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bigrams and Trigrams</head><p>We extracted from the Yahoo Movies and Epinions Digital Camera reviews bigrams and trigrams that appear in more than four documents. This left with 43,195 terms to form the feature space. Vectoral representations of all documents in the training set were then used to select the most relevant features. We used the Pearson's correlation coefficient method to select 907 features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Preliminary Experimental Results</head><p>We submitted three automatic runs. In each run, we retrieved documents using the Lemur search engine and ran each retrieved document through a Logistic Regression classifer. The classifier, which was trained once using the document set described in Section 4, was reused for all three runs. The difference between the runs is in the retrieval of documents and is described as follows.</p><p>• UCSCAUTO: The title field of the topics was used as the search query. A numeric score is associated with each retrieved document. The classifier then predicts the probability of containing opinion expressions for each document. The product of the retrieval score and the predicted probability was used to order the results.</p><p>• UCSCDESC: The topic description was used as the search query to retrieve documents. Words in the description that are not specific to any topic were treated as stop words and removed from the expanded query. Some examples of such words are "provide", "find", "regarding", and "opinion". The results were then classified the same way as in UCSCAUTO.</p><p>• UCSCRELFB: The ordered results of UC-SCAUTO were used as pseudo relevance feedback for the search engine. We ran search using the topic title as the query with the pseudo relevance feedback. The documents were ordered according to the retrieval score. No additional classification was performed on the results.</p><p>Table <ref type="table" coords="3,99.70,690.15,4.98,8.74" target="#tab_2">3</ref> shows the average R-precisions over all topics for opinion retrieval and topic relevance. UC-SCAUTO performs the best among the three runs.</p><p>This observation is very different from those reported by some past TREC ad hoc retrieval participants, who observed that descriptions can be helpful. This suggest that blog opinion mining queries are very different from traditional TREC queries, and retrieval techniques that used to work well on old TREC ad hoc retrieval data sets may not work well on blog retrieval task and vice versa. In our two stage ranking system, the opinion classifier works separately from the topic retrieval. To tell how well the classifier learned from other domains works in the blog domain, we did some further experiments to run classifier alone on relevant blog documents about at least one target. The UCSCAUTO run produced 11,677 relevant documents that received a qrel score of 1 and above. We ran the classifier on these documents and the confusion matrix is presented in Table <ref type="table" coords="3,339.13,384.75,3.88,8.74">4</ref>. A document is considered true opinion if it received a qrel score of 2 or above. The overall prediction accuracy on the test set is 61%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,310.61,396.48,228.12,8.74;1,310.61,408.41,228.63,9.68;1,310.61,420.39,228.65,8.74;1,310.61,432.35,228.64,8.74;1,310.61,444.30,146.40,8.74;1,346.41,470.15,50.70,8.74;1,414.14,463.41,54.67,8.74;1,412.97,482.66,4.52,6.12;1,420.00,477.08,54.67,8.74;1,389.36,502.13,7.75,8.74;1,418.50,492.29,3.01,6.12;1,418.50,500.29,12.90,6.12;1,433.57,495.23,67.85,9.68"><head></head><label></label><figDesc>Let the prior distribution of the model parameter β follow a Gaussian distribution N (β; m β , V β )). The posterior probability distribution of model parameter β conditional on the training data D can be calculated based on Bayesian theorem. P (β|D) = P (D|β)P (β) β P (D|β)P (β) = t i=1 P (y i |β, x i )P (β)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,310.61,405.67,228.64,153.34"><head>Table 1 :</head><label>1</label><figDesc>Examples subjective adjectives. The scores belong to the most subjective WordNet synset that contains the adjective.</figDesc><table coords="2,348.03,405.67,153.79,107.57"><row><cell></cell><cell></cell><cell>Neg</cell><cell>Obj</cell></row><row><cell>happy</cell><cell>0.875</cell><cell>0</cell><cell>0.125</cell></row><row><cell>disastrous</cell><cell>0.0</cell><cell>0.75</cell><cell>0.25</cell></row><row><cell>poor</cell><cell>0.0</cell><cell>0.75</cell><cell>0.25</cell></row><row><cell>wrong</cell><cell>0.0</cell><cell cols="2">0.875 0.125</cell></row><row><cell>specific</cell><cell cols="3">0.375 0.25 0.375</cell></row><row><cell>counterfeit</cell><cell>0.375</cell><cell>0.5</cell><cell>0.125</cell></row><row><cell>sturdy</cell><cell>0.5</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell cols="2">comparable 0.625</cell><cell>0.0</cell><cell>0.375</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,310.61,702.10,228.64,20.69"><head>Table 2 :</head><label>2</label><figDesc>Examples of high-scoring bigrams and trigrams are shown in Table 2. Most relevant bigram and trigram features</figDesc><table coords="3,116.04,74.82,140.55,119.92"><row><cell>reason i</cell><cell>the settings</cell></row><row><cell>in my opinion</cell><cell>settings and</cell></row><row><cell>the only thing</cell><cell>this one is</cell></row><row><cell>a must</cell><cell>features are</cell></row><row><cell cols="2">recommend this great and</cell></row><row><cell>i purchased</cell><cell>special effects</cell></row><row><cell>i recently</cell><cell>it if you</cell></row><row><cell>my favorite</cell><cell>does have</cell></row><row><cell>well worth</cell><cell>you haven't</cell></row><row><cell>over and over</cell><cell>funny and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,310.61,180.43,238.99,79.62"><head>Table 3 :</head><label>3</label><figDesc>Average R-precisions over all topics for opinion retrieval and topic relevance.</figDesc><table coords="3,316.58,180.43,233.01,45.80"><row><cell></cell><cell cols="2">Opinion Retrieval Topic Relevance</cell></row><row><cell>UCSCAUTO</cell><cell>0.2354</cell><cell>0.3047</cell></row><row><cell>UCSCDESC</cell><cell>0.2198</cell><cell>0.2806</cell></row><row><cell>UCSCRELFB</cell><cell>0.2076</cell><cell>0.2992</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="3,331.08,477.38,17.16,8.74;3,367.75,477.38,28.12,8.74;3,415.38,477.38,31.27,8.74;3,466.16,477.38,11.94,8.74;3,497.60,477.38,41.65,8.74;3,331.08,489.33,40.40,8.74;3,390.90,489.33,16.06,8.74" xml:id="b0">
	<monogr>
		<title level="m" coord="3,331.08,477.38,17.16,8.74;3,367.75,477.38,28.12,8.74;3,415.38,477.38,31.27,8.74;3,466.16,477.38,11.94,8.74;3,497.60,477.38,41.65,8.74;3,331.08,489.33,40.40,8.74;3,390.90,489.33,16.06,8.74">The Lemur Toolkit for Language Modeling and</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
