<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.52,99.04,354.85,12.91">Question Answering with LCC&apos;s CHAUCER at TREC 2006</title>
				<funder>
					<orgName type="full">U.S. Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,81.72,131.27,69.66,10.76"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.41,131.27,71.44,10.76"><forename type="first">John</forename><surname>Williams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.03,131.27,76.68,10.76"><forename type="first">Jeremy</forename><surname>Bensley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.90,131.27,65.54,10.76"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.86,131.27,41.76,10.76"><forename type="first">Ying</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,469.82,131.27,60.07,10.76"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.52,99.04,354.85,12.91">Question Answering with LCC&apos;s CHAUCER at TREC 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">640D5D3ABCDCC2FB79F3D353ADD89813</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CHAUCER is a Q/A system developed for (a) combining several strategies for modeling the target of a series of questions and (b) optimizing the extraction of answers. Targets were modeled by (1) topic signatures;</p><p>(2) semantic types; (3) lexico-semantic patterns; (4) frame dependencies; and (5) predictive questions. Several strategies for answer extraction were also tried. The best-performing strategy was based on the use of textual entailment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As with the TREC 2004 and TREC 2005 Question-Answering Track evaluations, the main task of the TREC 2006 evaluations required systems to answer a series of questions that sought information about a specific target. In order to provide information relevant to a target, two forms of semantic knowledge had to be reconciled: (1) the expected answer types of the questions; and (2) the semantic signatures of the targets. For example, in order to answer a question about the popular television show The Daily Show like What was the title for The Daily Show's 2000 election coverage?, question-answering systems need both the ability to recognize titles in texts as well as access to the forms of contextual knowledge to identify those titles that could potentially correspond to the names of television show segments.</p><p>Q149.6 (The Daily Show) What was the title for The Daily Show's 2000 election coverage?</p><p>Answer: "The Daily Show" is sponsored by cool cars, cell phones and movies -and its big corporate sponsors for its "Indecision 2000" election coverage include Yahoo, Volkswagen and Snapple.</p><p>Table <ref type="table" coords="1,114.81,593.68,3.90,8.97">1</ref>: Question 149.6: The Daily Show Unlike our previous experience with series of questions, in which the target was processed as a pair (lexical-string, semantic-type), in CHAUCER we have developed a methodology of generating the semantic signature of the target and using interactions between this signature and the questions from a given series. In TREC 2006, we focused on the interactions between targets and only two forms of questions, namely (1) factoid questions and (2) "other" questions. In future work, we shall also consider the interaction between target signatures and list questions.</p><p>To be able to answer questions based on semantic signatures of targets, we have also considered (1) a two-tiered passage retrieval system and (2) the use of multiple answer extraction strategies. Answers were extracted by making also use of two novel approaches: (a) the automatic generation of question-answer pairs, known as predictive questions from texts and (b) the recognition of forms textual entailment between a question and a candidate answer.</p><p>The rest of this paper is organized as follows. Section 2 describes the CHAUCER Q/A system. Section 3 discusses how factoid questions were answered while Section 4 shows how we processed "other" questions. Results from CHAUCER's participation in the main task of the TREC 2006 QA track are presented in Section 5; Section 6 summarizes our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The CHAUCER Question-Answering System</head><p>In this section, we describe the architecture of the CHAUCER question-answering system used to answer series of factoid and list questions for the TREC 2006 QA main task. The architecture of CHAUCER is presented in Figure <ref type="figure" coords="1,513.04,469.84,3.77,8.97" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Processing</head><p>CHAUCER begins the process of providing answers to a series of questions by submitting the series' target to a Target Processing module. Targets are initially sent to a Target Type Detection module, which uses a Maximum Entropy classifier in order to associate the target with one of six different target type categories. In our TREC 2006 work, targets were classified as either (1) a PERSON (e.g. Warren Moon), (2) an ORGANIZATION (American Enterprise Institute), (3) a LOCATION (Amazon River), (4) an EVENT (1991 eruption of Mount Pinatubo), (5) an AUTHORED WORK (The Daily Show), or a (6) GENERIC NOUN <ref type="bibr" coords="1,450.00,618.52,42.80,8.97">(avocados)</ref>. Following this classification, keywords were extracted from the target and sent to a Document Retrieval module in order to retrieve a set of documents relevant to the target itself. These documents are then sent to a Topic Representation module which employs two different statistical approaches based on the methods for computing topic signatures <ref type="bibr" coords="1,480.40,684.28,77.54,8.97" target="#b11">(Lin &amp; Hovy 2000)</ref> in order to model the topic of a relevant set of documents. CHAUCER uses a subset of the text passages returned during Target Processing in order to generate a set of predictive questions that could potentially be asked about a given target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing</head><p>Once a set of predictive questions have been generated for a target, CHAUCER sends each question in a question series to a QUESTION PROCESSING module. Questions are initially sent to an Annotation Module, which uses LCC's suite of natural language processing tools to tokenize, part-ofspeech tag, and syntactically parse each question. Questions are also annotated with one of over 300 different named entity classes from LCC's CiceroLite and are also semantically parsed using LCC's PropBank-, NomBank-, and FrameNetbased semantic parsers. Following annotation, questions are first sent to a Question Type Detection module, which uses a set of syntactic heuristics in order to classify individual questions as an example of a factoid, list, or "other" question. Factoid and list questions are then sent to an Answer Type Detection module, which follows <ref type="bibr" coords="2,217.85,505.12,74.68,8.97" target="#b10">(Li &amp; Roth 2002)</ref> and <ref type="bibr" coords="2,72.09,516.16,153.77,8.97" target="#b0">(Chakrabarti, Krishnan, &amp; Das 2005)</ref> in using a twostage Maximum Entropy-based classifier in order to identify the expected answer type (EAT) of the question from LCC's answer type hierarchy. Keywords are then extracted from each question and sent to a Keyword Expansion module designed to identify additional key words and phrases that could be used to enhance the quality of document and passage retrieval for a particular question. CHAUCER also incorporates a Question Coreference module which uses a heuristic-based approach to resolve instances of pronominal and nominal coreference within a question series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Preprocessing and Retrieval</head><p>We preprocessed the AQUAINT corpus with five types of information. First, we used LCC's implementation of the Collins parser to provide a full syntactic parse for every document in the corpus. Second, we used three different semantic parsers in order to identify semantic dependencies imposed by both verbal and nominalized predicates. In addition to LCC's PropBank and NomBank parsers, we also used LCC's FrameNet-based semantic parser to identify instances FrameNet frames in natural language texts; a separate role classifier was used to identify roles associated each FrameNet frame. Third, we used LCC's CICERO-LITE named entity recognition system in order to classify more than 300 different types of names found in the corpus. We also used more than 500 lexicons and gazetteers derived from web-based resources in order to tag additional name types not covered by CICEROLITE. Fourth, we used LCC's TASER temporal normalization system <ref type="bibr" coords="2,506.69,413.68,51.06,8.97;2,319.55,424.72,36.54,8.97">(Lehmann et al. 2005)</ref> in order to map temporal expressions found in documents to a standardized (ISO 8601) format. Finally, as with question series, we used a conservative heuristic-based approach in order to resolve instances of nominal and pronominal coreference.</p><p>Following preprocessing, the AQUAINT corpus was indexed using the Lucene Information Retrieval engine in order to allow documents to be retrieved using queries composed of either literal strings, stemmed words, or any of the entity types identified by CICEROLITE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Extraction and Selection</head><p>CHAUCER uses a battery of six different strategies to extract answers from retrieved passages. (Each of these six strategies are described in detail in Section 3.) Following Answer Extraction, the top five candidate answers identified by each strategy are then sent to a Candidate Answer Reranking module which uses a Maximum Entropy-based reranker (based on <ref type="bibr" coords="2,389.18,629.44,143.92,8.97" target="#b13">(Ravichandran, Hovy, &amp; Och 2003)</ref> in order to provide a single ranked list of candidate answers for a particular question. The re-ranked list of answers were then sent to a final Answer Selection module which uses the stateof-the-art textual entailment system described in <ref type="bibr" coords="2,511.09,673.24,46.75,8.97;2,319.55,684.28,23.48,8.97" target="#b7">(Hickl et al. 2006)</ref> in order to identify the single answer passage whose meaning is most likely to be entailed by the meaning of the original question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List Answer Extraction</head><p>CHAUCER leverages the basic factoid question-answering (Q/A) pipeline we have described in this section in order to answer list questions from a series as well. Table <ref type="table" coords="3,268.22,113.20,5.03,8.97" target="#tab_0">2</ref> lists the final answers given for question 181.3 (List the artists represented in the collection.).</p><p>In TREC 2006, we utilized a method based on web counts from various search engines to determine how much of an association there was between the candidate answer and both the series target and answer type term (in this case Hermitage Museum and artist, respectively). The scores from these methods were then combined to give each candidate answer a final composite score. We then considered all answers above a dynamically-defined threshold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Answering Factoid Questions</head><p>This section describes several of the novel techniques that were introduced into the CHAUCER factoid Q/A pipeline for the TREC 2006 evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Predictive Questions</head><p>Following Target Processing, the top 50 passages taken from the set of target-relevant documents are re-ranked according to a composite score based on (1) the weights associated with T S 1 terms and T S 2 relations found in the passage, (2) the weights associated with the soft patterns and (3) manually-created patterns found in the passage, and (4) the weights assigned to any FrameNet frame detected in the passage. Following <ref type="bibr" coords="3,123.04,552.76,150.15,8.97" target="#b6">(Harabagiu, Lacatusu, &amp; Hickl 2006)</ref>, we used the output of LCC's PropBank-based semantic parser in order to generate natural language questions from each predicate found in the top-ranked passages. Given a set of semantic dependencies associated with a predicate, we used a set of heuristics in order to select a single argument from each predicate to serve as the answer of a generated "factoid" question. Features derived from LCC's CICEROLITE were then used to map the argument to one of the possible WH-phrase (e.g. Who,What,Where) used in natural language questions. The entire passage was then submitted to a Question Generation module which utilized the dependency structure of the passage in order to generate a natural language question. Generated questions were then paired with their original passage-length answers and stored in a Predictive Question Database for later use. Table <ref type="table" coords="3,515.06,68.08,5.03,8.97" target="#tab_1">3</ref> provides examples of the predictive questions generated for Question 149.6, What was the title for The Daily Show's 2000 election coverage?.</p><p>Q149.6 (The Daily Show) What was the title for The Daily Shows 2000 election coverage?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P Q1</head><p>What was just the sort of piece that "The Daily Show," revels in? P Q2</p><p>Who has hired Dole as a guest political commentator for its election coverage? P Q3</p><p>Who best summed up Comedy Central's coverage of the 2000 Republican Convention? P Q4</p><p>Who will join the cast of the "The Daily Show"? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing</head><p>In this section, we describe the three types of Question Processing CHAUCER performs for each question.</p><p>Keyword Expansion Keywords extracted from each question were processed by a Keyword Expansion module that was designed to identify additional synonymous keywords that could be used to augment the query CHAUCER used to retrieve documents. This module used a set of heuristics in order to append synonyms and alternate keywords from a database of similar terms developed by LCC for previous TREC QA evaluations. In addition, we used the topic representations generated by CHAUCER's Target Processing module in two ways. First, we included as keyword expansions all T S 1 terms that were found either in the WordNet synsets for a particular keyword. Second, we also considered all terms found in set of the target-relevant documents that were linked to the question keyword via T S 2 relations with relevance scores above a fixed threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Coreference</head><p>We incorporated a heuristic-based Question Coreference module in order to resolve referring expressions found in the question series to antecedents mentioned in previous questions or in the target description. First, we used heuristics for performing name aliasing and nominal coreference from CICEROLITE in order to identify the full referent for each partial name mention found in the question series. Next, we constructed an antecedent list from all of the named entities that occurred in the question series prior to the current question. Each potential antecedent and referring expression found in the series were then annotated with name class, gender, and number information available from CICEROLITE. We then used the Hobbs Algorithm (Hobbs 1978) in order to match referring expressions to candidate antecedents. When no compatible antecedent could be identified from the antecedent list, we made no further attempt to resolve the referring expression found in the question. Table <ref type="table" coords="3,439.54,651.40,5.03,8.97" target="#tab_3">4</ref> presents an example where CHAUCER was able to resolve the antecedent of the pronoun it correctly; in contrast, Table <ref type="table" coords="3,441.36,673.24,5.03,8.97" target="#tab_4">5</ref> presents an example where our approach is unable to correctly recognize coreference between a noun phrase from the question (the program) and the target phrase (television show Cheers).    Our coarse answer type classifier was trained using the 5500-question UIUC Answer Type Corpus; we re-annotated this corpus with fine answer types from the LCC answer type hierarchy in order to train our fine answer type classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Retrieval</head><p>In CHAUCER, we experimented with a novel two-tiered approach approach to document retrieval which used a conservative entity-based answer extraction strategy in order 1 The HUMAN coarse answer type encompasses the more familiar PERSON and ORGANIZATION entity types. By approximating incorporating the entity constraints on candidate answerhood into its Document Retrieval engine, CHAUCER is able to eliminate documents that may be keyword-dense but may not contain any relevant candidate answers. This approach also enables CHAUCER retain a high level of precision in answering factoid questions while processing fewer documents. In our experiments using the factoid questions from TREC 2005, we found no appreciable improvement in the precision or the coverage of CHAUCER's answers when more than 50 documents were retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Extraction</head><p>CHAUCER uses a total of six different answer extraction strategies in order to identify exact answers from a set of retrieved passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity-Based Answer Extraction CHAUCER's entity-</head><p>based answer extraction strategy takes advantage of the large number of entity types recognized by LCC's CICEROLITE named entity recognition system in order to identify candidate answers to individual questions. Under this approach, only passages that contain entity types associated with the question's expected answer type are considered as candidate answers; remaining passages are then re-ranked based on the distribution and density of question keywords discovered in each passage. The wide coverage of LCC's CICEROLITE allows this strategy to retrieve a surprising number of exact answers, even without incorporating additional lexicosemantic features. With a question like Q181.3 (Who is the manager of Manchester United?), CHAUCER's Answer Type Detection module associates the EAT with a number of entity types related to people and organizations -including POLITICIAN, NOBILITY, and COACH. Here, it is able to return the correct answer without the need for patterns or other types of semantic information.     <ref type="bibr" coords="5,54.00,523.84,141.62,8.97">imdb.com, nndb.com, iplpotus.com)</ref> in order to answer other specific types of questions.</p><p>Soft Pattern-based Answer Extraction Following <ref type="bibr" coords="5,272.08,552.76,20.25,8.97;5,54.00,563.68,80.87,8.97" target="#b1">(Cui, Kan, &amp; Chua 2004)</ref>, we used a soft pattern matching approach in order to automatically generate additional patterns that could be used to extract exact answers to different types of factoid questions. Under this approach, we first organized questions taken from the previous TREC QA evaluations into a set of 30 different categories, based on expected answer type. Once this classification was in place, we used the set of "gold" answer sentences associated with each question in a category in order to train a bigram soft pattern model for each question category. As with <ref type="bibr" coords="5,186.68,662.32,101.48,8.97" target="#b1">(Cui, Kan, &amp; Chua 2004)</ref>, we then used this soft pattern model in order to compute the percentage match between the set of training examples and the the candidate answers retrieved for a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FrameNet-Based Answer Extraction</head><p>We leveraged semantic dependency information from LCC's FrameNetbased parser in order to extract candidate answers from the set of top-ranked passages retrieved for a question. Under this approach, we used LCC's FrameNet parser in order to recognize a set of semantic frame dependencies for each question. Passages retrieved for each question were then ranked based on (1) the distribution of semantic frames detected in each passage and (2) the parser's estimation of the confidence of the frame assignment. For example, as depicted in   A FrameNet parse of the top-ranked passage (Padre Pio, who died in 1968 at the age of 81, was right.), also includes the same two FrameNet frames detected in the question. By aligning the frame slots associated with the AGE frame both found in the question and the answer, we found compelling evidence which could be used to identify to this candidate answer being as the right answer.</p><p>However, the alignment of frames does not always point to the right answer. In an example like question Q141.2 (Where did Moon play in college?), both the question and the top-ranked candidate answer are associated with both a BEING-LOCATED and a COMPETITION frame, yet the location argument identified in the answer points to a location other than the answer to the question.</p><p>Predictive Question-Based Answer Extraction Finally, CHAUCER uses the set of predictive questions generated as a part of Target Processing in order to provide an additional source of candidate answers. Following Predictive Question Generation, CHAUCER uses the question similarity metrics described in <ref type="bibr" coords="5,372.06,673.24,97.92,8.97" target="#b5">(Harabagiu et al. 2005)</ref> in order to select the top 50 most similar predictive question-answer pairs stored in the Predictive Question Network. These question-answer pairs are then ranked based on (1) their overall similarity to the original question, (2) the presence of entity types corresponding to the EAT of the original question, and (3) the distribution of question keywords. After re-ranking, the top 25 question-answer pairs are then sent to the Answer Ranking and Answer Selection modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Ranking</head><p>Following Answer Extraction, CHAUCER uses a Maximum Entropy-based re-ranker (similar to <ref type="bibr" coords="6,195.44,161.80,96.82,8.97;6,54.00,172.84,41.18,8.97" target="#b13">(Ravichandran, Hovy, &amp; Och 2003)</ref>) in order to compile answers from each of the six answer extraction strategies into a single ranked list. This re-ranker was trained on the top ten answers returned by each of CHAUCER's answer extraction strategies for each of the questions taken from the TREC 2004 and TREC 2005 datasets. (Answers were keyed automatically using "gold" answer patterns made available by the TREC organizers and other participating teams.) Five sets of features were used in this re-ranker: (1) the strategy used to extract the answer, (2) the EAT of the original question, (3) the entity type associated with the exact answer, (4) the redundancy of the answer across the top-ranked answers, and (5) the confidence assigned to the answer by each answer extraction strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Selection</head><p>Once a ranking of candidate answers is performed, the top 25 answers were then sent to an Answer Selection module which leverages LCC's state-of-the-art textual entailment system in order to identify the answer which best approximates the semantic content of the original question. Popularized by the recent PASCAL Recognizing Textual Entailment (RTE) Challenges <ref type="bibr" coords="6,167.90,409.00,124.53,8.97;6,54.00,419.92,21.66,8.97" target="#b3">(Dagan, Glickman, &amp; Magnini 2005)</ref>, textual entailment systems seek to identify whether the meaning of a hypothesis can be reasonably inferred from the meaning of a corresponding text. While the RTE Challenges have focused to-date only on the computation of entailment relationships between sentence-length texts and hypotheses, our recent work <ref type="bibr" coords="6,165.10,474.76,105.45,8.97">(Harabagiu &amp; Hickl 2006</ref>) has shown that current systems for recognizing TE can be leveraged to accurately identify entailment relationships between questions and answers -or even questions and other questions.</p><p>CHAUCER uses the entailment system described in <ref type="bibr" coords="6,266.78,530.20,25.53,8.97;6,54.00,541.12,43.84,8.97" target="#b7">(Hickl et al. 2006</ref>) in order to estimate the likelihood that a question entails either ( <ref type="formula" coords="6,132.48,552.04,3.92,8.97">1</ref>) a candidate answer extracted by one of CHAUCER's six answer extraction strategies or (2) a predictive question generated by the Predictive Question Generation module. Following <ref type="bibr" coords="6,167.27,584.92,101.60,8.97">(Harabagiu &amp; Hickl 2006</ref>), we first filtered all candidate answers that were not entailed by the original questions. The remaining candidate answers (including any remaining predictive question-answer pairs) were re-ranked based on the entailment confidence output by the RTE system. The top-ranked answer was then returned as our submitted answer.</p><p>In our TREC 2006 experiments, we found that the establishment of textual entailment between a question and a predictive question or an answer passage to be a powerful tool for the validation of candidate answers.</p><p>In Table <ref type="table" coords="6,364.76,57.16,8.38,8.97" target="#tab_1">13</ref>, we present an example where the top-ranked predictive question generated from text is entailed by the original question; in this case, the correct answer associated with the predictive question is also the answer to the original question.</p><p>Q142.3 (LPGA) How many events are part of the LPGA tour?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P Q1</head><p>How many events did the LPGA expand its schedule to? Answer1</p><p>Under Ritts, the LPGA expanded its schedule from 36 to 43 events; increased purses to $36.2 million Table <ref type="table" coords="6,351.33,168.76,8.55,8.97" target="#tab_1">13</ref>: Correct Answer: Entailed Predictive Question However, this is not always the case: in Table <ref type="table" coords="6,528.76,183.76,8.38,8.97" target="#tab_3">14</ref>, the predictive question (Where was the 82nd Airborne Division formed?) is incorrectly classified as being entailed by the original question (Where in the US is the [82nd Airborne Division] based?). Here, the failure of the TE system to distinguish between the implications of the verbs formed and based results in the selection of an incorrect answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q144.1 (82nd Airborne Division)</head><p>Where in the US is the division based?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P Q1</head><p>Where was 82nd Airborne Division formed? Answer1</p><p>The 82nd Airborne Division was formed in 1917 at Camp Gordon, Ga.</p><p>Table <ref type="table" coords="6,348.33,315.28,8.55,8.97" target="#tab_3">14</ref>: Incorrect Answer: Entailed Predictive Question A similar phenomenon is seen when comparing entailment relationships between questions and answer passages. While both candidate answers are entailed by Q197.1 (What animal was the first mammal successfully cloned from adult cells?), only the first candidate answer passage is correct.</p><p>Q197.1 (cloning of mammals) What animal was the first mammal successfully cloned from adult cells?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct</head><p>Dolly the sheep, the world's first clone of an adult mammal, has made history again to become a mother, its creator, Scotland's Roslin Institute, said Thursday.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorrect</head><p>The University of Hawaii scientists, reporting in Thursday's issue of the journal Nature, describe their work as "the first reproducible cloning of a mammal from adult cells" extending at least three generations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Processing "Other" Questions</head><p>In this section, we describe the approach used to provide answers to the "other" questions associated with each question series in the TREC 2006 Main QA Task. The architecture of the CHAUCER system for answering "other" questions is presented in Figure <ref type="figure" coords="6,398.79,585.64,3.77,8.97" target="#fig_2">2</ref>. CHAUCER begins the processing of answering "other" questions by submitting a question series target to the same Target Processing module used in the factoid and list question-answering pipelines depicted in Figure <ref type="figure" coords="6,515.89,629.44,3.77,8.97" target="#fig_0">1</ref>. As with factoid and list questions, the process of answering "other" questions begins by categorizing targets using the Target Type Detection module. In addition, topic representations (including topic signatures and enhanced topic signatures) are also computed from the top 100 target-relevant documents retrieved from the AQUAINT corpus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nugget Extraction</head><p>We used three different strategies to extract relevant nuggets from the documents retrieved from the AQUAINT corpus. First, we extracted nuggets for each target using libraries of high-precision patterns developed for each of our six different target types. Second, we used our own implementation of the algorithm for automatically generating soft patterns introduced in <ref type="bibr" coords="7,184.41,382.36,107.99,8.97" target="#b1">(Cui, Kan, &amp; Chua 2004;</ref><ref type="bibr" coords="7,54.00,393.28,23.48,8.97">2005)</ref> in order to identify an additional set of patterns that could be used to extract relevant information for a particular target type. Third, we used information derived from the the two different topic representations generated during Target Processing in order to identify sets of sentences that contained information relevant to the topic denoted by the target itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern-Based Nugget Extraction</head><p>In CHAUCER's pattern-based nugget extraction strategy, nuggets were extracted if the target appeared in any of a fixed set of extraction patterns that were defined for a particular target type. While extraction patterns based on the recognition of appositives, relative clauses, parentheticals, and copular constructions were used for each of the six target types, we developed specific patterns (when possible) for each individual target type. Table <ref type="table" coords="7,186.22,562.60,10.06,8.97" target="#tab_16">16</ref> provides examples of the types of patterns used to extract nuggets for targets classified as PERSON.</p><p>In a departure from previous pattern-based approaches to nugget extraction <ref type="bibr" coords="7,127.23,607.48,145.95,8.97" target="#b14">(Xu, Licuanan, &amp; Weischedel 2003)</ref>, we used a large corpus of definitions, descriptions, and biographies extracted from the Web in order to assign weights to each of the extraction patterns associated with each target type. Weights were computed for each individual extraction pattern associated with a target type based on the frequency that the pattern occurred in the corpus of descriptions assembled for each target type. Sentences were then extracted from the set of documents retrieved for the target based on   <ref type="bibr" coords="7,361.51,541.72,102.59,8.97" target="#b1">(Cui, Kan, &amp; Chua 2004)</ref> in order to identify additional patterns that could be used to extract nuggets for a particular target type. As with the soft pattern-based answer extraction strategy used in CHAUCER's factoid Q/A pipeline, we followed <ref type="bibr" coords="7,411.97,585.64,106.55,8.97" target="#b1">(Cui, Kan, &amp; Chua 2004)</ref> in developing a bigram soft pattern model in order to identify potential matches between a set of training sentences and each of the sentences extracted for a particular target. Training sentences were derived for each target type from two different sources: (1) the collection of "gold" nuggets identified for the TREC 2005 "other questions" and a collection of 5,000 biographies, descriptions, and encyclopedia articles that were downloaded from wikipedia.org, s9.com, and biography.com. We used the probablity that a passage was matched by an soft pattern in order to assign confidence weights to each of the sentences retrieved for a target; only the top 50 sentences were considered during Answer Selection.</p><p>Topic-Based Nugget Extraction Following work done by <ref type="bibr" coords="8,66.82,106.72,90.37,8.97" target="#b8">(Lacatusu et al. 2006)</ref> for question-focused summarization, we used weights associated with TS 1 terms and TS 2 relations to compute a composite topic score for each sentence in the set of documents retrieved for a target. Sentences were re-ranked based on their topic score before being submitted to the Answer Selection module. As with the soft pattern nugget extraction strategy, only the top 50 passages were considered during Answer Selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Selection</head><p>Recent work in summarization <ref type="bibr" coords="8,189.33,219.88,103.07,8.97;8,54.00,230.80,23.48,8.97" target="#b12">(Nenkova &amp; Passonneau 2004)</ref> has benefited from the use of content models in selecting a set of relevant sentences for inclusion in a multidocument or question-focused summary. As with summaries, we believe that the set of answers returned in response to an "other" question can be modeled using techniques which are able to evaluate the relevance of each candidate passage (or "nugget") against some approximation of the content a user is seeking when asking this type of question.</p><p>In order to select amongst the set of candidate nuggets identified by our three nugget extraction strategies, we constructed a model of the idealized content of a set of answers to an "other" question based on passages extracted from a set of documents retrieved from number of authoritative sources found on the World Wide Web. (In our TREC 2006 work, we experimented with documents from three web sources: wikipedia.org, s9.com, and biography.com.) The top 10 documents from each site were retrieved with a simple web query, using only stemmed keywords extracted from the series target. Relevant passages were extracted from these downloaded documents by selecting passages that contained target keywords and topic signature (T S 1 ) terms. In order to acquire a set of passages that most closely resembled the the types of nuggets we hoped to select for our final answer submission, we discarded any sentence that contained fewer than 5 tokens or more than 150 tokens. After the model sentences were selected, we discarded remaining stop words, stemmed the remaining words, and built a term vector based on the tf.idf value computed for each word.</p><p>We then used a greedy search algorithm in order to identify the set of extracted nuggets that most closely resembles the content of the relevant passages extracted from the set of Web documents downloaded for that target. We defined an answer submission as any non-zero set of candidate nuggets identified from the set of candidate nuggets sent to the Answer Selection module. Each possible answer submission was then turned into a tf.idf term vector (using the same process as was used in processing passages included in the model). The resulting answer submission vector was then scored against the model using cosine similarity, defined as</p><formula xml:id="formula_0" coords="8,53.99,668.67,95.88,14.32">Sim( x 1 , x 2 ) = x1â€¢ x2 | x1|| x2| .</formula><p>After creating an empty answer submission, the greedy search algorithm considers each candidate nugget is consid-ered in turn; candidate nuggets is added to the answer set only if it would increase the answer submission's similarity when compared to the model. Search halts after a single pass through the nuggets. To prevent the inclusion of a large number of redundant nuggets, we use a heuristic to limit to size of the answer set. Each time a nugget was added to the answer set, we recorded the factor by which the similarity score was increased. Rather than searching until all of the candidate nuggets were considered, the search was terminated when the average of the last 10 score increases fell below a threshold.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation Results</head><p>Table <ref type="table" coords="8,345.82,318.52,10.06,8.97" target="#tab_20">18</ref> presentsCHAUCER's performance on the TREC 2006 factoid Q/A task. We were encouraged by the overall performance of our system, as it suggests that current systems for textual entailment can be used effectively in order to select amongst the output of a multi-strategy approach to factoid Q/A.  While we experimented with a single novel strategy for answering list questions, the bulk of our team's efforts were spent decidedly on factoid questions.  <ref type="table" coords="8,386.10,585.64,10.06,8.97" target="#tab_22">20</ref> shows our precision, recall, and F-Score for Other questions.</p><p>A breakdown of the number of questions lost at each stage of CHAUCER's factoid Q/A processing is provided in Table 21.</p><p>Despite using over 260 fine answer types, CHAUCER only assigns a spurious expected answer type to approximately 10% of the factoid questions. While we would predict that using a coarser answer type hierarchy would reduce some of this loss at both the question analysis and answer extraction stages, we would anticipate that reducing the number of  In addition, we believe that the relatively small number of questions lost at the level of Document Retrieval suggests that our approaches to keyword expansion and passage are well-suite for the factoid Q/A task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we described CHAUCER, the new automatic question-answering system developed at LCC for the TREC 2006 QA evaluations. This system is notable in that it utilizes four new retrieval and answer detection techniques in order to better retrieve passages and extract exact answers from natural language texts. First, CHAUCER features a novel query expansion process which leverages automatically-generated topic representations created specifically for each question series target to identify new keywords for each question. Second, higher precision passage retrieval was achieved for factoid and list questions through a two-phase approach to information retrieval which uses topic signatures to select better candidate passages for answer extraction. Performance of CHAUCER's retrieval components was further enhanced by combining keyword queries with entity types selected from the set of over 300 types recognized by LCC's CICEROLITE named entity recognition system. Third, CHAUCER's system for answering "other" questions exploits a new retrieval approach which exploits language models computed from collections of topical web documents in order to select relevant passages from 5 competing answer extraction modules. Finally, instead of adopting the abductive reasoning framework utilized by several of LCC's past TREC QA submissions, CHAUCER exploits a mechanism for answer validation that incorporates forms of textual inference from a stateof-the-art textual entailment in order to retrieve and select answers to both factoid and list questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,167.64,250.12,276.55,8.97;2,89.80,54.07,431.85,191.54"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the CHAUCER Question-Answering System</figDesc><graphic coords="2,89.80,54.07,431.85,191.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,64.32,340.96,215.85,6.28;5,64.32,350.92,72.68,6.28;5,64.32,363.28,110.80,6.28;5,79.20,373.72,21.70,6.28;5,127.66,373.72,143.90,6.28;5,127.68,383.68,143.91,6.28;5,127.68,393.64,143.78,6.28;5,127.68,403.60,66.08,6.28"><head>Q157. 5</head><label>5</label><figDesc>(United Nations (U.N.)) Who was the President of the U.N. Security Council for August 1999? Fine Answer Type GOVT PERSON Answer Martin Andjaba, Namibian ambassador to the United Nations, will succeed Hasmy Agam of Malaysia as the president of the U.N. Security Council as of August 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,183.48,267.40,244.80,8.97;7,89.80,54.33,431.88,208.18"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the CHAUCER "Other" Q/A System</figDesc><graphic coords="7,89.80,54.33,431.88,208.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,82.32,245.56,157.66,143.25"><head>Table 2 :</head><label>2</label><figDesc>List Extraction Example</figDesc><table coords="3,82.32,245.56,157.66,128.68"><row><cell cols="3">Q181.3 (Hermitage Museum) List the artists represented</cell></row><row><cell cols="2">in the collection.</cell><cell></cell></row><row><cell>Rank</cell><cell>Answer</cell><cell>Result</cell></row><row><cell>1</cell><cell>Vladimir Mayakovsky</cell><cell>Incorrect</cell></row><row><cell>2</cell><cell>Da Vinci</cell><cell>Correct</cell></row><row><cell>3</cell><cell>Michelangelo</cell><cell>Correct</cell></row><row><cell>4</cell><cell>Rembrandt</cell><cell>Correct</cell></row><row><cell>5</cell><cell>Poussin</cell><cell>Correct</cell></row><row><cell>6</cell><cell>Rubens</cell><cell>Correct</cell></row><row><cell>7</cell><cell>van Gogh</cell><cell>Correct</cell></row><row><cell>8</cell><cell>Caspar David Friedrich</cell><cell>Incorrect</cell></row><row><cell>9</cell><cell>Guido Reni</cell><cell>Incorrect</cell></row><row><cell>10</cell><cell>Parmigianino</cell><cell>Incorrect</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,319.56,211.60,238.29,19.89"><head>Table 3 :</head><label>3</label><figDesc>Examples of the Predictive Questions Generated for Question 149.6</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,64.32,115.60,208.68,47.32"><head>Table 4 :</head><label>4</label><figDesc>Correctly-resolved Question Coreference</figDesc><table /><note coords="4,64.32,144.28,98.09,6.28;4,64.32,156.64,122.93,6.28"><p><p>Target 150: television show Cheers</p>What year was the program first broadcast?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,54.00,168.52,238.45,341.08"><head>Table 5 :</head><label>5</label><figDesc>Incorrectly-resolved Question Coreference NUMERIC, and (6) DESCRIPTION. Once a single coarse answer type has been identified for each question, a second classifier is then used to map the question to one of the set of fine answer types associated with each coarse type. In our work, we have used a hierarchy of over 260 fine entity types derivable from the more than 300 different entity types recognized by LCC's CICEROLITE. Table6presents examples of the fine types we associated with each coarse answer type.</figDesc><table coords="4,54.00,190.24,238.45,319.36"><row><cell cols="3">Answer Type Detection CHAUCER follows much re-</cell></row><row><cell cols="3">cent work in Answer Type Detection (Li &amp; Roth 2002;</cell></row><row><cell cols="3">Chakrabarti, Krishnan, &amp; Das 2005) in using a two-stage</cell></row><row><cell cols="3">Maximum Entropy-based classifier in order to recognize the</cell></row><row><cell cols="3">expected answer type of a question. CHAUCER's first an-</cell></row><row><cell cols="3">swer type classifier thecoarse answer type of the question;</cell></row><row><cell cols="3">currently, we consider the following six coarse answer types:</cell></row><row><cell cols="3">(1) HUMAN 1 , (2) LOCATION, (3) ENTITY, (4) ABBREVIA-</cell></row><row><cell>TION, (5) UIUC Coarse</cell><cell>LCC Fine</cell><cell>Examples</cell></row><row><cell>Type</cell><cell>Types</cell><cell></cell></row><row><cell>ABBREVIATION</cell><cell>2</cell><cell>Acronym, Expanded</cell></row><row><cell></cell><cell></cell><cell>Acronym</cell></row><row><cell>DESCRIPTION</cell><cell>2</cell><cell>Death Manner, Quote</cell></row><row><cell>ENTITY</cell><cell>45</cell><cell>Animal, Authored</cell></row><row><cell></cell><cell></cell><cell>Work, Chemical Element</cell></row><row><cell>HUMAN</cell><cell>106</cell><cell>Coach, Writer, Govt</cell></row><row><cell></cell><cell></cell><cell>Person, Medical Org</cell></row><row><cell>LOCATION</cell><cell>61</cell><cell>Country, Mountain,</cell></row><row><cell></cell><cell></cell><cell>Planet, Ocean</cell></row><row><cell>NUMERIC</cell><cell>46</cell><cell>Age, Velocity, Money</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,54.00,515.32,238.44,19.89"><head>Table 6 :</head><label>6</label><figDesc>Distribution of Fine Types in LCC's Answer Type Hierarchy</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,54.00,586.00,238.40,30.93"><head>Table and Table</head><label>and</label><figDesc>presents evaluation results of our systems for answer type detection, as evaluated on the TREC 2006 questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="4,319.56,56.56,238.41,326.96"><head>Table 7 :</head><label>7</label><figDesc>Answer Type Detection on TREC 2006 Questions Passage Retrieval module which was used to extract the most relevant text passages from each docu-</figDesc><table coords="4,319.56,56.56,238.40,206.36"><row><cell></cell><cell>Score</cell><cell></cell></row><row><cell>Coarse</cell><cell>92.9</cell><cell></cell></row><row><cell>Fine</cell><cell>84.0</cell><cell></cell></row><row><cell>Coarse Type</cell><cell>Total Questions</cell><cell>Score</cell></row><row><cell>ENTITY</cell><cell>31</cell><cell>67.7</cell></row><row><cell>DESCRIPTION</cell><cell>2</cell><cell>100</cell></row><row><cell>LOCATION</cell><cell>66</cell><cell>93.9</cell></row><row><cell>NUMERIC</cell><cell>179</cell><cell>91.6</cell></row><row><cell>ABBREVIATION</cell><cell>3</cell><cell>100</cell></row><row><cell>HUMAN</cell><cell>84</cell><cell>92.9</cell></row><row><cell cols="3">Table 8: Fine Answer Type Detection on TREC 2006 Ques-</cell></row><row><cell>tions</cell><cell></cell><cell></cell></row><row><cell cols="3">to augment traditional keyword-and entity-based retrieval</cell></row><row><cell cols="3">queries. First, the top 200 documents were retrieved us-</cell></row><row><cell cols="3">ing an expanded keyword query; these documents were then</cell></row><row><cell>passed to a</cell><cell></cell><cell></cell></row></table><note coords="4,319.56,275.92,238.14,8.97;4,319.56,286.84,238.28,8.97;4,319.56,297.88,238.16,8.97;4,319.56,308.80,238.29,8.97;4,319.56,319.72,238.28,8.97;4,319.56,330.76,238.39,8.97;4,319.56,341.68,238.25,8.97;4,319.56,352.60,238.26,8.97;4,319.56,363.52,238.39,8.97;4,319.56,374.56,124.52,8.97"><p>ment. Next, the top 500 retrieved passages were then sent to an entity-type based Answer Extraction module, which reranked passages according to the distribution of (1) entities matching the EAT of the question, (2) topic signature terms and relations identified during Target Processing, and (3) keywords extracted from the original question. The original set of 200 retrieved documents were then re-ranked based on the distribution of these top-ranked passages; only the top 50 documents were then considered by later Answer Extraction and Answer Selection modules.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="5,54.00,217.96,238.44,110.49"><head>Table 9 :</head><label>9</label><figDesc>Question 183.1: Correct Entity Detection Of course, the performance of this strategy is also ultimately limited by the coverage and quality of CICEROLITE, as well. In TREC 2006, we failed to retrieve an answer to question Q157.5 (Who was the President of the U.N. Security Council for August 1999?): even though CHAUCER correctly identified the expected answer type of the question as a type of PERSON, this strategy failed to extract the correct answer Martin Andjaba because it was not tagged with one of the entity types associated with this question's EAT.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="5,54.00,415.48,238.65,106.41"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note coords="5,106.52,415.48,173.56,8.97;5,54.00,436.24,238.36,8.97;5,54.00,447.16,238.40,8.97;5,54.00,458.08,238.65,8.97;5,54.00,469.12,238.31,8.97;5,54.00,480.04,238.41,8.97;5,54.00,490.96,238.52,8.97;5,54.00,502.00,238.40,8.97;5,54.00,512.92,238.37,8.97"><p>Question 157.5: Failure of Entity Detection Pattern-Based Answer Extraction CHAUCER utilizes two different pattern-based approaches in in order to identify answers to a small set of question types. Hand-crafted extraction patterns are first used to extract answers to the question types frequently asked in past TREC evaluations from the AQUAINT corpus. In addition, we have experimented with using structured web-based sources of information related to people, places, and authored works (e.g.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="5,319.56,166.72,238.40,147.28"><head>Table 11</head><label>11</label><figDesc></figDesc><table coords="5,319.56,166.72,238.40,147.28"><row><cell></cell><cell>, LCC's FrameNet parser identifies two</cell></row><row><cell cols="2">FrameNet frames for a question like Q199.4 (How old was</cell></row><row><cell cols="2">Padre Pio when he died?): (1) a AGE frame, used to encode</cell></row><row><cell cols="2">the age of an entity and (2) a DEATH frame, used to encode</cell></row><row><cell cols="2">information about an event in which a protagonist dies.</cell></row><row><cell cols="2">Q199.4 (Padre Pio) How old was Padre Pio when he died?</cell></row><row><cell>Age</cell><cell>Entity: Padre Pio</cell></row><row><cell>Death</cell><cell>Protagonist: Padre Pio</cell></row><row><cell cols="2">Answer: Padre Pio, who died in 1968 at the age of 81, was right.</cell></row><row><cell>Age</cell><cell>Entity: Padre Pio</cell></row><row><cell></cell><cell>Age: 81</cell></row><row><cell>Death</cell><cell>Protagonist: Padre Pio</cell></row><row><cell></cell><cell>Time: 1968</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="5,326.64,319.60,224.15,118.12"><head>Table 11 :</head><label>11</label><figDesc>Correct Answer based on FrameNet Matching</figDesc><table coords="5,329.88,358.12,125.69,79.60"><row><cell>Being Located</cell><cell>Location: Where</cell></row><row><cell>Competition</cell><cell>Participant: Moon</cell></row><row><cell>Being Located</cell><cell>Location: Kansas City</cell></row><row><cell>Competition</cell><cell>Participant: Warren Moon</cell></row><row><cell></cell><cell>Location: Kansas City</cell></row></table><note coords="5,329.88,347.68,161.07,6.28;5,329.88,380.80,215.85,6.28;5,329.88,390.76,215.86,6.28;5,329.88,400.72,70.63,6.28"><p><p>Q141.2 (Warren Moon) Where did Moon play in college?</p>Answer: Warren Moon did not play at all for Kansas City as coach Gunther Cunningham tried to protect his 43-year-old backup quarterback from a banged-up offensive line.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="5,323.64,443.32,230.14,8.97"><head>Table 12 :</head><label>12</label><figDesc>Incorrect Answer based on FrameNet Matching</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="6,377.88,493.60,121.91,8.97"><head>Table 15 :</head><label>15</label><figDesc>Entailment Example</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="7,319.56,416.44,238.38,134.25"><head>Table 16</head><label>16</label><figDesc></figDesc><table coords="7,319.56,416.44,238.38,134.25"><row><cell>: Top PERSON Patterns</cell></row><row><cell>a composite score equal to the sum of the weights of all</cell></row><row><cell>of the patterns that were extracted from a sentence. Since</cell></row><row><cell>this strategy necessarily favors precision over recall, all sen-</cell></row><row><cell>tences that were assigned a non-zero weight were considered</cell></row><row><cell>during Answer Selection.</cell></row><row><cell>Soft Pattern-Based Nugget Extraction In addition to</cell></row><row><cell>hand-crafted extraction patterns, we also experimented with</cell></row><row><cell>using the probabilistic soft matching techniques first de-</cell></row><row><cell>scribed in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="8,344.64,276.64,187.96,8.97"><head>Table 17 :</head><label>17</label><figDesc>"Other" Answers to Q199: Padre Pio</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="8,352.80,457.96,171.67,8.97"><head>Table 18 :</head><label>18</label><figDesc>TREC 2006 Factoid Q/A Results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="8,319.80,494.92,238.04,99.69"><head>Table 19 :</head><label>19</label><figDesc>Table 19 details the CHAUCER's performance on list questions. TREC 2006 List Q/A Results Finally, Table</figDesc><table coords="8,411.00,528.76,53.75,36.52"><row><cell>Metric</cell><cell>Score</cell></row><row><cell>Recall</cell><cell>0.187</cell></row><row><cell>Precision</cell><cell>0.162</cell></row><row><cell>F(Î²=1)</cell><cell>0.148</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="9,90.84,56.56,164.60,109.36"><head>Table 20 :</head><label>20</label><figDesc>TREC 2006 Other Q/A Results</figDesc><table coords="9,110.16,56.56,124.40,109.36"><row><cell>Metric</cell><cell>Score</cell><cell></cell></row><row><cell>Recall</cell><cell>0.143800</cell><cell></cell></row><row><cell>Precision</cell><cell>0.079760</cell><cell></cell></row><row><cell>F(Î²=3)</cell><cell>0.108387</cell><cell></cell></row><row><cell>Component</cell><cell>Accuracy</cell><cell>Loss</cell></row><row><cell>Question Analysis</cell><cell>89.6</cell><cell>10.4%</cell></row><row><cell>Document Retrieval</cell><cell>86.1</cell><cell>3.4%</cell></row><row><cell>Answer Extraction</cell><cell>75.9</cell><cell>10.2%</cell></row><row><cell>Answer Ranking</cell><cell>53.8</cell><cell>22.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="9,54.00,171.52,238.42,66.09"><head>Table 21 :</head><label>21</label><figDesc>Component Analysis of CHAUCER on TREC 2006 Factoid entity types considered by CHAUCER would make the tasks of Answer Extraction and Answer Ranking sufficiently more difficult.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Sanda Harabagiu</rs>, <rs type="person">Tobias Jungen</rs>, and <rs type="person">Cosmin Adrian Bejan</rs> for their assistance with this work. This material is based upon work funded in whole or in part by the <rs type="funder">U.S. Government</rs> and any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,324.48,70.80,233.36,8.07;9,324.48,80.76,233.37,8.07;9,324.48,90.72,87.66,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,508.33,70.80,49.52,8.07;9,324.48,80.76,216.65,8.07">Enhanced answer type inference from questions using sequential models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,324.48,90.72,82.37,8.07">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,103.44,233.36,8.07;9,324.48,113.40,235.15,8.07;9,324.48,123.36,233.24,8.07;9,324.48,133.32,50.51,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,482.42,103.44,75.42,8.07;9,324.48,113.40,202.34,8.07">Unsupervised Learning of Soft Patterns for Definitional Question Answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,543.56,113.40,16.07,8.07;9,324.48,123.36,204.23,8.07">Proceedings of the Thirteenth World Wide Web conference</title>
		<meeting>the Thirteenth World Wide Web conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,145.92,233.49,8.07;9,324.48,155.88,235.02,8.07;9,324.48,165.84,233.38,8.07;9,324.48,175.80,230.60,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,494.18,145.92,63.79,8.07;9,324.48,155.88,174.87,8.07">Generic Soft Pattern Models for Definitional Question Answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,515.25,155.88,44.25,8.07;9,324.48,165.84,233.38,8.07;9,324.48,175.80,177.72,8.07">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development of Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,188.52,233.26,8.07;9,324.48,198.48,233.37,8.07;9,324.48,208.44,113.33,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,518.05,188.52,39.69,8.07;9,324.48,198.48,145.89,8.07">The pascal recognizing textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,489.73,198.48,68.12,8.07;9,324.48,208.44,109.16,8.07">Proceedings of the PASCAL Challenges Workshop</title>
		<meeting>the PASCAL Challenges Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,221.16,233.37,8.07;9,324.48,231.12,233.36,8.07;9,324.48,241.08,80.96,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,459.51,221.16,98.34,8.07;9,324.48,231.12,179.93,8.07">Methods for Using Textual Entailment in Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,525.48,231.12,32.37,8.07;9,324.48,241.08,76.00,8.07">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,253.68,233.25,8.07;9,324.48,263.64,233.34,8.07;9,324.48,273.60,233.29,8.07;9,324.48,283.56,100.87,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,324.48,263.64,180.75,8.07">Experiments with Interactive Question-Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,525.46,263.64,32.37,8.07;9,324.48,273.60,233.29,8.07;9,324.48,283.56,96.86,8.07">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,296.28,233.25,8.07;9,324.48,306.24,233.39,8.07;9,324.48,316.20,36.56,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,518.52,296.28,39.21,8.07;9,324.48,306.24,161.62,8.07">Answering complex questions with random walk models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,504.02,306.24,53.85,8.07;9,324.48,316.20,32.50,8.07">Proceedings of SIGIR-06</title>
		<meeting>SIGIR-06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,328.92,233.28,8.07;9,324.48,338.88,233.52,8.07;9,324.48,348.84,233.38,8.07;9,324.48,358.80,105.41,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,390.34,338.88,167.66,8.07;9,324.48,348.84,67.39,8.07">Recognizing Textual Entailment with LCC&apos;s Groundhog System</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,408.20,348.84,149.67,8.07;9,324.48,358.80,60.32,8.07">Proceedings of the Second PASCAL Challenges Workshop</title>
		<meeting>the Second PASCAL Challenges Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="9,324.48,371.40,233.38,8.07;9,324.48,381.36,233.26,8.07;9,324.48,391.32,233.38,8.07;9,324.48,401.28,41.14,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,456.89,381.36,100.85,8.07;9,324.48,391.32,163.82,8.07">Lcc&apos;s gistexter at duc 2006: Multi-strategy multi-document summarization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,504.49,391.32,53.37,8.07;9,324.48,401.28,18.95,8.07">Proceedings of DUC</title>
		<meeting>DUC</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,414.00,233.36,8.07;9,324.48,423.96,233.26,8.07;9,324.48,433.92,233.38,8.07;9,324.48,443.88,113.57,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,349.43,423.96,208.31,8.07;9,324.48,433.92,93.04,8.07">TASER: A Temporal and Spatial Expression Recognition and Normalization System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Aarseth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nezda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deligonul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.37,433.92,124.49,8.07;9,324.48,443.88,109.65,8.07">Proceedings of the 2005 Automatic Content Extraction Conference</title>
		<meeting>the 2005 Automatic Content Extraction Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,456.60,235.16,8.07;9,324.48,466.56,233.40,8.07;9,324.48,476.52,41.25,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,436.15,456.60,105.70,8.07">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,324.48,466.56,233.40,8.07;9,324.48,476.52,36.66,8.07">Proc. the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,489.12,233.37,8.07;9,324.48,499.08,233.26,8.07;9,324.48,509.04,233.90,8.07;9,324.48,519.00,191.89,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,434.57,489.12,123.28,8.07;9,324.48,499.08,122.08,8.07">The automated acquisition of topic signatures for text summarization</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,468.99,499.08,88.75,8.07;9,324.48,509.04,145.22,8.07">Proceedings of the 18th conference on Computational linguistics</title>
		<meeting>the 18th conference on Computational linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="495" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,531.72,233.37,8.07;9,324.48,541.68,233.38,8.07;9,324.48,551.64,68.84,8.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,476.79,531.72,81.06,8.07;9,324.48,541.68,163.52,8.07">Evaluating content selection in summarization: the pyramid method</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,504.50,541.68,53.36,8.07;9,324.48,551.64,47.01,8.07">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,564.36,233.37,8.07;9,324.48,574.32,235.11,8.07;9,324.48,584.28,233.29,8.07;9,324.48,594.24,40.54,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,505.21,564.36,52.65,8.07;9,324.48,574.32,166.15,8.07">Statistical qaclassifier vs re-ranker: What&apos;s the difference?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,505.74,574.32,53.85,8.07;9,324.48,584.28,233.29,8.07;9,324.48,594.24,36.48,8.07">Proceedings of the ACL Workshop on Multilingual Summarization and Question Answering</title>
		<meeting>the ACL Workshop on Multilingual Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,324.48,606.84,233.37,8.07;9,324.48,616.80,235.29,8.07;9,324.48,626.88,20.03,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,501.37,606.84,56.48,8.07;9,324.48,616.80,138.74,8.07">Trec 2003 qa at bbn: Answering definitional questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Licuanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,481.21,616.80,78.55,8.07;9,324.48,626.88,16.02,8.07">Proceedings of TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
