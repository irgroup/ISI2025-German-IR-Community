<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,216.60,74.24,178.80,10.80">ASU at TREC 2006 Genomics Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.88,101.36,42.67,10.80"><forename type="first">Luis</forename><surname>Tari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.16,101.36,86.65,10.80"><forename type="first">Graciela</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.20,101.36,71.25,10.80"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.36,101.36,69.89,10.80"><forename type="first">Shawn</forename><surname>Nikkila</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.60,115.16,57.40,10.80"><forename type="first">Ryan</forename><surname>Wendt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.84,115.16,58.42,10.80"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,216.60,74.24,178.80,10.80">ASU at TREC 2006 Genomics Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E80DCF98D4F530FD7ECF12C4481041E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our experiments in the TREC 2006 Genomics track submitted by the ASU BioAI group, as well as experiments based on the improvements made after our submission. Some of the major issues we tried to address in our experiments are how to (1) extract keywords from natural language questions in the biomedical domain and (2) determine the relevancy of passages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task for the TREC 2006 Genomics track is quite different from last year in the sense that the goal is to develop a system that can provide answers with enough context and supporting information to the questions. Another noticeable difference is that HTML full-text articles are provided rather than abstracts of the articles. The system we developed is fully automated: from processing natural language questions to presenting answers in the form of passages. Our main design principle of the system is to answer questions that are not limited to the questions provided by TREC. To achieve this goal, we experimented our system with some innovative approaches, such as taking advantages of the WordNet <ref type="bibr" coords="1,417.48,340.12,11.72,8.96" target="#b0">[1]</ref> resource to recognize keywords from natural language questions. Another highlight of our system is the use of subject-verbobject triplets to realize the relevancy of passages. The paper is organized as follows. Section 2 provides a system overview, with details of some of the major components of the system discussed in the subsections. Section 3 describes our runs submitted to TREC 2006 Genomics track and our revised run after improvement of some of the components. Section 4 describes our revised run and section 5 provides analysis of how each of the major components affects the performance of the overall system. Finally, we summarize the new components we improved for our revised run, and discuss some of the issues and challenges we face in developing our system in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>Our system for TREC 2006 Genomics track can be divided into three major components: preprocessing, document retrieval and passage retrieval. We first provide an overview of these three major components and describe some of the innovative features of our system in details. The role of the preprocessing component is to convert HTML full-text articles provided by TREC into structured XML format of the articles, as shown in figure <ref type="figure" coords="1,204.12,535.60,3.77,8.96">1</ref>. Acronyms are resolved in the course of preprocessing the full-text articles. The details of the preprocessing component are described in subsection 2.1. The document retrieval component, as illustrated in figure <ref type="figure" coords="1,231.00,561.64,3.77,8.96">2</ref>, involves the processing of natural language questions to form queries (subsection 2.2) and expansion of biological entities identified by the question processor (subsection 2.3). Articles are retrieved using variants of queries generated from the original queries (subsection 2.4). Passages are retrieved by the passage retrieval component, which utilizes the subject-verb-object (SVO) triplets of the questions to determine the relevancy of the passages (subsection 2.5). Passages that are determined to be not relevant to the question are filtered out from the final results. The passage retrieval component is illustrated in figure <ref type="figure" coords="1,225.24,639.64,3.77,8.96">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preprocessing</head><p>One of the major obstacles is to process the full-text articles provided by TREC, which are in HTML format. Our goal is to translate the HTML articles into XML format so that it can capture information such as which section a paragraph of text is originated from and the byte start and byte offset for each sentences.</p><p>A na√Øve algorithm is used to do such conversion based on section indicators. Section indicators are defined as a list of words such as "abstract", "introduction" that indicate the beginning of sections in full-text articles. We outline the algorithm in this subsection. For each HTML full-text article, the content of the HTML file is extracted by a tool called HTMLParser 1 . If the article being converted does not have a table of contents, then the article is read line by line to find section indicators. When no such section indicators are found, the first paragraph of the full-text article is considered to be originated from the abstract and the rest of the text is considered as part of the introduction. Since figure captions usually contain rich and concise information, text from figure captions are included into the XML files. The XML files also include byte start and offset for each of the sentences that correspond to the original HTML files. As acronyms are frequently used in the biomedical literature, resolving acronyms is essential to the task of retrieving articles and passages. The acronym resolution algorithm in <ref type="bibr" coords="3,143.16,101.68,11.72,8.96" target="#b1">[2]</ref> is used to resolve acronyms in the full-text articles, and the occurrences of the acronyms are stored in the XML files. The corresponding MeSH terms for each article are obtained from PubMed as well. These XML files are then indexed using Lucene <ref type="bibr" coords="3,307.68,127.60,10.69,8.96" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Keyword extraction</head><p>One of the important aspects of our system is to process the given TREC Genomics questions which are in the form of natural language, so that keywords can be extracted from the questions to construct the corresponding queries. It is clear that extracting the right keywords from natural language questions is critical to the task of retrieval of documents and the resulting passages. The queries we are interested in generating are in the form of Lucene queries, which are boolean keyword queries. Our approach of question processing incorporates the WordNet <ref type="bibr" coords="3,357.24,232.12,11.60,8.96" target="#b0">[1]</ref> dictionary together with the part-ofspeech and biological entity taggers to extract keywords. WordNet is an extensive lexical dictionary for English that provides different meanings of words, commonly referred to as senses, as well as relationships between words. It is a common resource for QA systems, as the WordNet dictionary aims at nonspecialized English words. It is suitable to our task of extracting keywords out of biological questions, as the goal here is to disallow common words in the questions from being selected as keywords. Our assumption behind the idea is that common English words usually have multiple senses. With this assumption, nouns that are not recognized as biological entities are examined using the WordNet dictionary. If an unrecognized noun has none or fewer than k different senses, our question processor considers the noun as a keyword. Using the question for topic 177 "How does Sec61-mediated CFTR degradation contribute to cystic fibrosis?", the tagger recognized "cystic fibrosis" as a disease, but missed Sec61 as a gene. Using our WordNet-based algorithm, "Sec61-mediated CFTR" and "CFTR degradation" are picked up as keywords. Further processing (breaking hyphenated words) resulted in recognition of Sec61. In our experiments using the TREC Genomics questions, we found that the threshold k=3 achieves the best performance for this purpose based on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Expansion of biological entities</head><p>Variation of naming convention for biological entities is one of the challenges in text retrieval and mining of biomedical literature, in particular gene names. Even resources such as Entrez Gene provide comprehensive lists of synonyms of gene names, slight variation of the official gene names can commonly be found in the biomedical literature. To automatically recognize biological entities, we utilize a gene name tagger called Abner <ref type="bibr" coords="3,174.84,505.60,11.72,8.96" target="#b3">[4]</ref> and an efficient exact match of disease names and biological processes utilizing MeSH terms disease category and the Gene Ontology <ref type="bibr" coords="3,307.08,518.68,11.72,8.96" target="#b4">[5]</ref> biological process ontology. Once a word is recognized as a biological entity such as a gene name, it is vital to be able to identify the actual gene name intended in order to find the corresponding synonyms. This is commonly known as gene normalization. Our approach of normalization of gene names is to utilize the fuzzy and proximity queries provided by Lucene. A fuzzy query allows matching words that are within certain edit distance between them. This is useful, for instance, to match the keyword "HNF4" to "HNF-4", in which their edit distance is 1. A proximity query is capable of matching words that are within a specific distance of each other. The gene name "hyprocretin receptor" can be matched to "hyprocretin (orexin) receptor" with 1 extra word in between the words of the gene name. All fields of the human gene names (i.e. genes with taxonomy id of 9606) in the Entrez gene dictionary are first indexed by Lucene. A simple algorithm is then used to find the intended gene name, as follows:</p><p>1. If the given gene name is single-word (we assume this is a gene symbol), then 1.1. form a Lucene query that allows exact matching of the gene name. 1.2. form a Lucene fuzzy query by adding "*~0.6" at the end of the given gene name.</p><p>1.3. form a Lucene fuzzy query by adding "~0.6" at the end of the given gene name. 2. If the given gene name is multi-word (we assume this is a gene name), then 2.1. form a Lucene proximity query with distance of 3 based on the given gene name. Steps 1.2 and 1.3 differ in the sense that the query formed in step 1.2 gives preference to matching gene names that are from the same gene family. For instance, BOP is matched to BOP1 using the query formed in step 1.2, but the query formed in step 1.3 allows to match BOP to BEP. However, simply adding synonyms from a gene name dictionary such as Entrez Gene is not enough, as variations of gene names might be used in the literature. For example, Nur77 can be used in an article instead of Nur-77, but this is not listed as a synonym of Nur77. As a result, potentially important abstracts can be missed if variants of gene symbols are not considered in the formation of queries. It is important to notice that gene symbol variants are names that do not appear in the gene name dictionary as official names or synonyms. We utilized an algorithm adapted from <ref type="bibr" coords="4,311.76,231.64,10.89,8.96" target="#b5">[6,</ref><ref type="bibr" coords="4,326.16,231.64,8.36,8.96" target="#b6">7]</ref> to generate gene symbol variants for a given gene identified as a keyword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Article retrieval and ranking</head><p>Once the keywords are identified and expanded as described in the previous sections, Lucene queries are formed based on the keywords. To increase the precision of the queries, predefined lists of MeSH terms for different kinds of biological entities <ref type="bibr" coords="4,244.20,310.12,11.72,8.96" target="#b7">[8]</ref> are utilized in the formation of query variants. Multiple query variants are generated from the keywords of the original query, which are described as follows:</p><p>Query variant 1: keywords with their synonyms and variants Query variant 2: keywords and their synonyms together with a predefined list of MeSH terms There are cases when there are none or very few (less than 10) articles are retrieved for a topic. When this happens, the retrieval component switches to the "risky" mode automatically, in which we are willing to give up some precision in favor of higher recall by relaxing the queries. The revised queries allow, for example, fuzzy and proximity matches, so that biological process such as "cell growth" can be matched to "growth of cell". For words that are extracted by the question processor but their types are not identified, such as mutation, we relax the query so that a retrieved article may contain the word mutation. The relaxed query enables us to retrieve articles that must contain biological keywords and may contain the nonbiological keywords. To unify and rank the hits retrieved by all query variants, the rank of an article is computed by adding the normalized ranking scores computed by Lucene from each query variants. This allows an article retrieved by multiple query variants to achieve a high rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Passage retrieval and ranking</head><p>A passage is defined as a contiguous list of sentences from a paragraph. In our case, we limit the maximum number of sentences in a passage to be 3. Our passage retrieval component takes top-n ranked articles relevant to the question, and finds sentences that have at least half of the keywords in the article. We call such sentences as important sentences. Neighboring sentences of the important sentences with at least one keyword are merged to form a passage. To avoid having too many passages, we set the limitation of a maximum number of 5 passages from each paragraph of the article. An essential part of our system is the extraction of subject-verb-object (SVO) triplets from sentences of passages as well as questions. The idea of SVO triplets is commonly used in typical QA systems <ref type="bibr" coords="4,480.60,609.64,11.72,8.96" target="#b8">[9]</ref> to deal with the problem of semantic symmetry. The questions "What is the role of X in Y?" and "What is the role of Y in X?" are similar at the word level, where X and Y are some keywords. However, the answers we expect from these two questions can be quite different. Our current method of extracting SVO triplets utilizes constituent trees (also referred to as parse trees) produced by the Link Grammar parser <ref type="bibr" coords="4,475.68,661.60,15.34,8.96" target="#b9">[10]</ref>, which describe the syntactic structure of a sentence. However, generating the constituent trees of the sentences is computationally expensive. We relax the constraint of determining the validity of passages by finding the occurrences of subject and object extracted from the corresponding question. Passages that do not have sentences containing the subject or object of the question are filtered out from being considered as a passage related to the question. With this relaxed constraint, we only require passages that contain subjects and objects of the questions to be valid passages, while keywords such as "mutation" that are neither the subjects nor the objects of the question are not required to be in the valid passages. Once passages are retrieved, we used two approaches for ranking passages. Approach 1 is based on the idea of <ref type="bibr" coords="5,101.28,140.68,16.76,8.96" target="#b10">[11]</ref> that takes into account of the keyword density and distance between keywords. To give preference to longer passages, we extended the approach to take into account of the number of words and sentences in each passage. Approach 2 utilizes the rank of an article to determine relevancy of a passage retrieved from the article. The assumption is that a passage retrieved from a highly ranked article is likely to be a relevant passage. So approach 2 is computed using the reciprocal of the rank of the article in which the passage is retrieved from. The scores of passages computed by the above two approaches are normalized so that the scores can be combined for different runs. The choices of the passage ranking approaches for each run are described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Runs</head><p>We submitted 3 runs for our TREC 2006 Genomics track submission. Our baseline run (run 1) involved retrieving passages from the top 100 relevant articles and filtering out passages that do not have all keywords mentioned in the passages. Our second run includes natural language processing techniques to rank passages and determine the relevancy of the passages by using the SVO triplets of passages. Due to the length of processing SVO triplets from passages, only the top 25 retrieved relevant articles are used to retrieve passages. Our third run only filters out passages that do not have the subject mentioned in the sentences of the passages, and passages are retrieved from the top 100 retrieved relevant articles. In this paper, we emphasize on all new results following the improvements of some of the components in our system. Some of the improved components involve the conversion of HTML to XML, gene synonym finding, filtering of passages, formation of query variants and variants of keywords from the index. We used these improved components to perform evaluation of various aspects. We called this as our revised run, with the results described in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we describe the performance of our revised run for the 26 topics instead of the original 8 topics, as there are no passages for topics 173 and 180 in the gold standard provided by TREC. The document average precision (denoted as Doc AP), passage average precision (denoted as Psg AP) and aspect average precision (denoted as Asp AP) for each of the 26 topics are described in Table <ref type="table" coords="5,496.56,492.64,3.77,8.96">1</ref>. We noticed that there is an improvement of our results by using our new components, and we further investigated the effects of the performance for each of the major component on the overall performance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>In this section, we analyzed how the performance of the major components of our system affects the overall performance of our system. These components include the recognition and expansion of biological entities, HTML to XML conversion, retrieval of articles and extraction of passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Recognition of biological entities</head><p>Finding and recognizing, also known as normalizing, the extracted keywords from natural language questions to the intended biological entities are essential to the formation of queries, which are used to retrieve articles. In this subsection, we evaluated the correctness of the normalization of gene names, disease names and biological processes based on the reference file containing normalized biological entities for each of the questions<ref type="foot" coords="6,188.28,86.41,3.24,5.83" target="#foot_1">2</ref> . Among the 33 genes in the 28 questions, we noticed that 19 of the genes were normalized to the correct gene. In the case of disease names, 10 out of 12 disease names in the 28 questions are normalized correctly. For biological processes, only 4 out of 17 of them were normalized to the correct biological process. Analysis of the normalization of the three types of biological entities indicates that exact matching on biological processes with standardized vocabulary is not an ideal solution to normalize biological entities. On the other hand, exact matching of disease names with MeSH disease terms is adequate, due to the fewer variation of naming conventions for diseases. Normalizing gene names with fuzzy and proximity queries result in a fair performance.  <ref type="table" coords="6,116.88,600.19,4.98,10.29">1</ref> -The document average precision (Doc AP), passage average precision (Psg AP) and aspect average precision (Asp AP) for each of the 26 topics for our revised run</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">HTML to XML conversion</head><p>The variety of the HTML formats for different journals in the TREC 2006 Genomics corpus is an obstacle for our HTML to XML conversion program. Out of the 162,259 articles in the corpus, only 147,723 articles were stored in our Lucene index, i.e. 8.96% of the articles were not utilized by our system. The number of articles stored in our Lucene index indicates that some of the converted XML articles are not in valid XML format, such as articles containing special characters and missing certain XML tags in the course of the conversion. With respect to the topics, there is an average of 16.30% or (a median of 14.29%) of the gold standard articles not being in our index. To investigate on how the articles not indexed by our system affects our retrieval performance, we performed an experiment on evaluating our retrieval performance based on the gold standard articles that were indexed by our system. In Figure <ref type="figure" coords="7,254.64,166.60,3.77,8.96" target="#fig_1">4</ref>, the document average precision for each topic using all 162K articles (denoted as Doc AP) is compared with using the 147K articles that our system indexed (denoted as Doc AP Mod_GS). We noticed that there was a gain in the retrieval performance in each of the topic for Doc AP Mod_GS. In fact, the document MAP for Doc AP Mod_GS went up to 0.2142, as compared to 0.1743 for Doc AP. We can conclude from this experiment that the failure in indexing all articles due to the issues in converting HTML to XML format has an impact on our performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Retrieval of articles</head><p>In TREC Genomics 2006, precision of document retrieval is dependent on the submitted passages. As described in section 2.5, extraction of passages involves a process filtering out passages that are not determined as relevant with respect to the questions. This implies some of the documents can be filtered due to the absence of passages extracted from them, which can possibly be correct documents. We performed an experiment to evaluate the correctness of document retrieval without the passage filtering process. In Figure <ref type="figure" coords="7,165.60,655.48,3.77,8.96" target="#fig_2">5</ref>, we compare the document average precision for each topic without using the passage filtering process (denoted as Bef_Psg), i.e. retain all retrieved documents, and using the passage filtering process (denoted as Aft_Psg). We can see that there is an increase in document average precision for 6 of the 26 topics when the passage filtering process is used, while the document average precision decreases for another 16 topics. The performance remains unchanged for the remaining 4 topics. The document MAP for Bef_Psg is 0.2537, a huge increase from 0.1743 for Aft_Psg. The results of this experiment show that the passage extraction process can be too restricted so that documents that are in fact correct are filtered out as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Extraction of passages</head><p>We performed analysis of our extraction performance, and we realized there is an issue with the correctness of byte start and offset for the passages. This issue is rooted from the conversion of HTML to XML format of the articles. It is unfortunate that we could not do a thorough analysis of this component, but we believe the issue of byte start and offset for the passages contribute to the poor passage average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>From the analysis of our experiments, we realized that the component that performs the conversion of TREC Genomics articles from HTML to XML format is the major source of errors, thus reducing the performance of our system. Due to errors encountered in the course of the conversion, we noticed that almost 9% of the HTML articles were not indexed by our indexer, which expects to take valid XML files as input for indexing. A more serious flaw of the component was that there were cases when the byte start and offset of the sentences could not be computed correctly.</p><p>Other than the programming issues, the methodology of expansion of biological entities needs to be improved. Though there are cases when using the biological entities extracted from the natural language questions to form keywords is enough to form the correct queries, we realized that we performed poorly in questions that involve biological processes. This can imply our current way of expansion of biological processes is insufficient. On the other hand, we recognized from the experiments that some of the correct documents were dropped from the final results due to the absence of passages. That could mean there are issues in the process of filtering passages. Another issue with our system is that our current method of extracting passages tends to extract lengthy passages, which hurts our extraction performance when the gold standard passages tend to be concise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,187.08,254.83,237.68,10.29;2,112.44,123.96,390.12,128.52"><head>Figure 1 -Figure 2 -Figure 3 -</head><label>123</label><figDesc>Figure 1 -An overview of the preprocessing component</figDesc><graphic coords="2,112.44,123.96,390.12,128.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,90.00,529.27,431.65,10.29;7,90.00,540.79,411.50,10.29;7,90.00,552.31,127.76,10.29"><head>Figure 4 -</head><label>4</label><figDesc>Figure 4 -The document average precision of all topics with respect to the gold standard based on all articles (denoted as Doc AP) and the gold standard based on only the articles our system indexed (denoted as Doc AP Mod_GS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,90.00,357.31,418.76,10.29;8,90.00,368.83,217.65,10.29"><head>Figure 5 -</head><label>5</label><figDesc>Figure 5 -The document average precision of all topics with passage filtering (denoted as Aft_Psg) and without passage filtering (denoted as Bef_Psg).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,95.76,710.68,133.69,8.96"><p>http://htmlparser.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,95.76,699.16,352.16,8.96;6,90.00,710.68,327.27,8.96"><p>Normalized representation of the TREC questions -Alex Morgan, Stanford and MITRE http://www.stanford.edu/~alexmo/slides/NormalizedTRECGen2006Questions.xls</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to acknowledge the availability of the <rs type="affiliation">ASU/TGen</rs> supercomputer for our processing tasks.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,93.77,99.43,319.00,10.29" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,160.20,99.43,178.31,10.29">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.77,110.95,428.24,10.29;9,108.00,122.35,380.85,10.29" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,204.96,110.95,317.05,10.29;9,108.00,122.35,18.68,10.29">A simple algorithm for identifying abbreviation definitions in biomedical texts</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,143.88,122.59,224.94,9.34">Proceedings of the Pacific Symposium on Biocomputing</title>
		<meeting>the Pacific Symposium on Biocomputing</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.76,133.87,193.83,10.29" xml:id="b2">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="9,108.00,133.87,30.94,10.29">Lucene</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.77,145.39,428.29,10.29;9,108.00,156.91,223.89,10.29" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,149.04,145.39,373.02,10.29;9,108.00,156.91,55.30,10.29">ABNER: an open source tool for automatically tagging genes, proteins and other entity names in text</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,169.56,157.15,59.43,9.34">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3191" to="3192" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.76,168.43,428.31,10.29;9,108.00,179.95,171.78,10.29" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,334.80,168.43,187.28,10.29;9,108.00,179.95,29.48,10.29">Gene Ontology: tool for the unification of biology</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,144.24,180.19,65.19,9.34">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.76,191.35,428.20,10.29;9,108.00,202.87,297.57,10.29" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,217.56,191.35,300.50,10.29">A hybrid method for relation extraction from biomedical literature</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,108.00,203.11,179.04,9.34">International Journal of Medical Informatics</title>
		<imprint>
			<publisher>In Press</publisher>
		</imprint>
	</monogr>
	<note>Corrected Proof</note>
</biblStruct>

<biblStruct coords="9,93.76,214.39,428.29,10.29;9,108.00,225.91,69.81,10.29" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bhalotia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">I</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<title level="m" coord="9,331.08,214.39,190.98,10.29;9,108.00,225.91,65.62,10.29">BioText team report for the TREC 2003 Genomics track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.76,237.43,428.37,10.29;9,108.00,248.95,413.85,10.29;9,108.00,260.44,261.69,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,171.00,237.43,351.14,10.29;9,108.00,248.95,168.29,10.29">Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,298.32,249.19,161.03,9.34">NIST Text Retrieval Conference (TREC)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.76,271.87,428.24,10.29;9,108.00,283.63,414.08,9.34;9,108.00,295.00,131.49,9.49" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,176.16,271.87,324.98,10.29">Selectively Using Relations to Improve Precision in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,108.00,283.63,409.47,9.34">Proceedings of the EACL 2003 Workshop on Natural Language Processing for Question Answering</title>
		<meeting>the EACL 2003 Workshop on Natural Language Processing for Question Answering<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.38,306.43,423.63,10.29;9,108.00,317.92,50.61,8.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,271.32,306.43,227.25,10.29">A Robust Parsing Algorithm For LINK Grammars</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sleator</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>CMU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,98.38,329.35,423.70,10.29;9,108.00,341.11,413.99,10.29;9,108.00,352.39,326.49,10.29" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,495.00,329.35,27.08,10.29;9,108.00,341.11,413.99,10.29;9,108.00,352.39,16.81,10.29">SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B-H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B-K</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">An</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,143.88,352.63,209.38,9.34">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference<address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
