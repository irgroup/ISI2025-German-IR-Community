<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.46,80.99,269.07,12.90;1,217.13,96.93,177.75,12.90">Experiments at the University of Edinburgh for the TREC 2006 QA track</title>
				<funder ref="#_KfpwDFE">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.35,141.71,79.52,10.75"><forename type="first">Michael</forename><surname>Kaisser</surname></persName>
							<email>m.kaisser@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.90,141.71,68.56,10.75"><forename type="first">Silke</forename><surname>Scheible</surname></persName>
							<email>s.scheible@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.51,141.71,79.14,10.75"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
							<email>bonnie@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.46,80.99,269.07,12.90;1,217.13,96.93,177.75,12.90">Experiments at the University of Edinburgh for the TREC 2006 QA track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">934A5104004A5D8BD120C92CB1339B7E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe experiments carried out at the University of Edinburgh for our TREC 2006 QA participation. Our main effort was to develop an approach to QA that is based on frame semantics. Two algorithms were implemented to this end, building on the lexical resources FrameNet, PropBank and VerbNet. The first algorithm uses the resources to generate potential answer templates to a given question, which are then used to pose exact, quoted queries to a web search engine and confirm which of the results contain an actual answer to the question. The second algorithm bases search queries on key words only, but it can recognize answers in a wider range of syntactic variants in its candidate sentence analysis stage. We discuss both approaches when applied to each of the resources and a combination of these.</p><p>We also describe how-in a later step-the found answer candidates are mapped to the AQUAINT corpus and how we answered other questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year we concentrated mainly on two approaches that employ lexical resources based on frame semantics like FrameNet <ref type="bibr" coords="1,209.80,523.21,89.00,9.46;1,72.00,536.76,23.48,9.46" target="#b2">(Fillmore and Lowe, 1998)</ref>, PropBank <ref type="bibr" coords="1,150.10,536.76,90.41,9.46" target="#b10">(Palmer et al., 2005)</ref> and VerbNet <ref type="bibr" coords="1,72.00,550.31,68.31,9.46" target="#b12">(Schuler, 2005)</ref> for question answering. Compared to WordNet <ref type="bibr" coords="1,124.45,563.86,84.05,9.46" target="#b8">(Miller et al., 1993)</ref> which has been used widely in Question Answering, these other lexical resources are relatively new and therefore their usefulness for QA has still to be proven. In this paper we argue that, by employing these tools, we can indeed search for and extract answers in ways that were not or only indirectly possible beforehand. We describe two new and complementary techniques for using these resources and show the improvements to be gained when they are used individually and then together. We also point out problems that must be overcome to achieve these results.</p><p>Generally, the mentioned resources offer the following features which can be used to gain a better understanding of questions, sentences containing answer candidates, and the relations between them:</p><p>• They all provide verb-argument structures for a large number of lexical entries. • FrameNet and PropBank contain semantically annotated sentences that exemplify the underlying frame. • FrameNet contains not only verbs but also lexical entries for other part-of-speeches.</p><p>• FrameNet provides inter-frame relations that can be used for more complex paraphrasing to link the question and answer sentences.</p><p>By using these features, we are able, for example, to give a complete frame-semantic analysis of the following sentences and to recognize that they all contain an answer to the question "When was Alaska purchased?":</p><p>The United States purchased Alaska in 1867. Alaska was bought from Russia in 1867. In 1867, Russia sold Alaska to the United States. The acquisition of Alaska by the United States in 1867 is known as "Seward's Folly.</p><p>The first algorithm we present uses the three lexical resources to generate potential answercontaining templates. While the templates contain holes-in particular, for the answer-the parts that are known can be used to create exact quoted search queries. Sentences can then be extracted from the output of the search engine and annotated with respect to the resource being used. From this, an answer candidate (if present) can be extracted. The second algorithm analyzes the dependency structure of the annotated example sentences in FrameNet and PropBank. It then poses rather abstract queries to the web, but can in its candidate sentence analysis stage recognize answers in a wider range of syntactic possibilities. As we will see, the two algorithms are nicely complementary.</p><p>We additionally report on two other features of our TREC 2006 system. We describe how we map answer candidates found on the web to supporting AQUAINT documents and also our method for answering other questions, which received the best result this year.</p><p>2 QuALiM at TREC 2006: An Overview Our system, QuALiM, uses four different algorithms to answer factoid questions. The pattern matching and the keyword/keyphrase-based algorithm are essentially the same as in our 2004 system <ref type="bibr" coords="2,72.00,340.18,118.91,9.46" target="#b5">(Kaisser and Becker, 2004)</ref>. New this year are two algorithms based on frame semantics, which we focus on in this paper. The first of these algorithms described in Section 4 was finished at the time of the 2006 evaluation period. The second algorithm (see Section 6) only existed in a rudimentary form and so did not contribute significantly to the result. However, it is now fully functional and produces the results described in Section 7.</p><p>For list questions, we used a modified version of our keyword/keyphrase-based algorithm from the 2004 system. Similarly, other questions were answered with a modified version of the method used in 2004 (with the exception of run 3). However, because the system architecture changed considerably since 2004, both algorithm were reimplemented. Reimplementation of the other question algorithm involved tweaking parameters and paying attention to details etc. because we felt that the algorithm did not perform as well as it could have had in recent years. We did not devote much effort to the list question algorithm. The official TREC results reported in Section 12 mirror this in an interesting manner, especially if compared to our TREC 2004 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FrameNet, PropBank and VerbNet</head><p>FrameNet is a lexical database resource based on frame semantics and supported by corpus evidence.</p><p>It documents the range of semantic and syntactic combinatory possibilities (valences) of target words (lexical units) and contains human-annotated sentences (currently more than 135,000), which exemplify the use of more than 8,900 lexical units organized in more than 625 semantic frames. Frames may also be linked to each other by particular semantic relations. Lexical units in different frames but with similar semantics usually receive identical role labels. For example, the frames for both "invent" and "design" relate the roles (frame elements in FrameNet's terminology) Cognizer and Invention, while those for the verbs "buy" and "sell" relate the frame elements Buyer and Seller.</p><p>PropBank uses the syntactic structure present in the Penn Treebank and adds to it a new layer of semantic annotation. As a result PropBank delivers parsed example sentences where the position of the semantic fillers for a given head verb are specified as nodes in the parse tree. Unlike FrameNet, it uses the argument labels ARG0, ARG1, ARG2 and various versions of ARGM for modifiers. Thus the relations between the arguments of different verbs are not obvious.</p><p>Finally, VerbNet does not list annotated example sentences for a given verb, but rather only specifies an abstract argument structure, as in this entry for "purchase":<ref type="foot" coords="2,364.69,466.74,3.99,6.91" target="#foot_0">1</ref> 4 Method 1: Question Answering by Natural Language Generation</p><p>The first method implemented uses the data available in the resources to generate potential answer templates to the question. While at least one component of such a template (the answer) is yet un-known, the remainder of the sentence can be used to query a web search engine. The results can then be analyzed, and if they match the originally-proposed answer template structure, an answer candidate can be extracted. In this section we will give a short explanation of how this method processes the question "Who purchased YouTube?" First, the incoming question is parsed using Mini-Par <ref type="bibr" coords="3,88.85,184.25,47.51,9.46" target="#b7">(Lin, 1998)</ref>, and the resulting dependency tree is simplified to the following structure: head: purchased(V) subj: Who whn: Who obj: YouTube head indicates that the head of the question is the verb purchased, subj indicates that the deep subject is who (which whn marks as also being a question word) and obj indicates that the deep object is YouTube. Furthermore, our system analyzes the tense as being Simple Past.</p><p>This provides enough information to look up the head verb in (for example) FrameNet, where one lexical unit (LU) for purchase.v can be found. One of the associated annotated sentences is:</p><p>The company had PURCHASED ... FE:Buyer lexical unit ... several PDMS terminals , but ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FE:Goods</head><p>As can be seen, parts of the sentences are annotated with frame elements, here Buyer and Goods. The system will parse and simplify the annotated sentences until we achieve a set of abstract frame structures, similar to those in VerbNet. By doing this we intentionally remove certain levels of information that were present in the original data, i.e. tense, voice, mood and negation. (In a later step we will reintroduce some of it.) For the above example the abstract structure is:</p><formula xml:id="formula_0" coords="3,82.76,602.36,177.54,6.32">Buyer[Subj,NP] VERB Goods[Obj,NP]</formula><p>Other structures similarly extracted from FrameNet are: This shows that usually, in active sentences, the Buyer role is realized as an NP at subject position, while Goods is an NP at object position. As mentioned earlier, the analysis of the question showed that, in a potential (active) answer sentence, the answer should be in subject relation to the verb "purchase". Furthermore, "YouTube" needs to be in object relation to the verb. Taking this information together, it can be concluded that the filler for the Goods frame element is "YouTube", and that the question asks for a Buyer.</p><p>This and the fact that the question was asked in past tense, enables the system to create the following potential answer templates by alternating all possible past tense forms and the voice: The part (or parts) of the templates that are known are quoted and sent to a search engine. For the fourth example, one query would be "YouTube was purchased by". From the snippets returned by the search engine, we extract candidate sentences and match them against the abstract frame structure from which the queries were originally created. In this way, we annotate the candidate sentences and are now able to identify the filler of the answer role. For example, the above query returns "On October 9, 2006, YouTube was purchased by Google for an incredible US$1.65 billion", from which we can extract "Google", because it is the NP filling the buyer role.</p><p>Note that although we described the approach using an example based on FrameNet, we use the data provided in PropBank and VerbNet as well. In the case of PropBank the proceeding is basically the same. When using VerbNet, we can skip one processing step, because VerbNet does not list example sentences, but returns the frame structure directly.</p><p>So far, we have only discussed questions whose answer role is an argument of the head verb. However, for some question classes (especially time-or location-questions) this assumption does not hold.</p><p>Here, the answer to the question is usually realized as an adjunct. This is an important difference for at least three reasons:</p><p>1. FrameNet and VerbNet do not or only sparsely annotate peripheral adjuncts. (PropBank however does.) 2. In English, the position of adjuncts varies much more than those of arguments. 3. In English, different kinds of adjuncts can occupy the same position in a sentence, although naturally not at the same time.</p><p>The following examples illustrate point 2:</p><p>YouTube was purchased by Google on October 9. On October 9, YouTube was purchased by Google.</p><p>YouTube was purchased on October 9 by Google.</p><p>All variations are possible, although they may differ in frequency. PPs conveying other adjuncts could replace all the above temporal PPs, or they could be added at other positions.</p><p>This behavior has to be accounted for, both when annotating the question with semantic roles and when creating and processing potential answer sentences. When annotating the answer role in a question which asks for an peripheral adjunct, the syntax of the question is of little help. Instead, we have to consult the answer type of the question. <ref type="foot" coords="4,246.37,443.93,3.99,6.91" target="#foot_1">2</ref> We match certain answer types to certain roles, e.g. whenever a temporal or location answer type is detected, the answer role becomes, in FrameNet terms, Place or Time, respectively. We then use an abstract frame structure like the following to create the queries:</p><formula xml:id="formula_1" coords="4,82.76,533.74,182.92,16.28">Buyer[Subj,NP,unknown] VERB Goods[Obj,NP,"YouTube"]</formula><p>While this lacks a role for the answer, we can still use it to create, for example, the query "has purchased YouTube". When sentences returned from the search engine are then matched against the abstract structure, we extract all PPs directly before the Buyer role, between the Buyer role and the verb and directly behind the Goods role. Then we check all these PPs on their semantic types and keep only those that match the answer type of the question (if any).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Making use of FrameNet Frames and Inter-Frame Relations</head><p>The methods presented so far can be used with all three resources. But FrameNet goes a step further than just listing verb-argument structures: It organizes all of its lexical entries in frames<ref type="foot" coords="4,486.31,150.23,3.99,6.91" target="#foot_2">3</ref> , with relations between frames that can be used for a wider paraphrasing and inference. In the following we explain how we use this information to generate additional answer templates. The purchase.v lexical unit, for example, is found in a Commerce-buy frame which also contains the lexical units buy.v and purchase.n. Both of these entries list annotated example sentences which use the same frame elements as purchase.v. It is therefore relatively straightforward to produce reformulations based on these related entries:</p><formula xml:id="formula_2" coords="4,323.96,326.74,193.67,46.17">ANSWER[NP] bought YouTube ANSWER[NP] (has|have) bought YouTube YouTube (has|have) been bought by ANSWER[NP] ...</formula><p>It is also possible to generate target paraphrases with heads which are not verbs like</p><formula xml:id="formula_3" coords="4,313.20,419.85,212.54,6.32">ANSWER[NP-Genitive] purchase of YouTube.</formula><p>Handling these is usually easier than sentences based on verbs, because no tense/voice information has to be introduced.</p><p>Furthermore, frames themselves can stand in different relations. The frame Commerce goodstransfer, for example, stands both to the already mentioned Commerce buy frame and to Commerce sell in an is perspectivized in relation. The latter contains the lexical entries retail.v, retailer.n, sale.n, sell.v, vend.v and vendor.n. Here is one annotated example sentence listed in sell.v: As can be seen, the frame elements of these lexical units use the same labels as the frame elements in the purchase.v and buy.v entries. This enables us to create answer templates like YouTube was sold to ANSWER <ref type="bibr" coords="5,210.43,77.96,20.56,6.32">[NP]</ref>. Other templates created from this frame seem odd, e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube has been retailed to ANSWER[NP].</head><p>This is because the verb "to retail" usually takes mass-products as its object argument and not a company. But FrameNet does not make such fine-grained distinctions. However, we did not come across a single example in our experiments where such a phenomenon caused an overall wrong answer. Sentences like the one above will most likely not be found on the web (just because they are in a narrow semantic sense not well-formed). Yet even if we would get a hit, it probably would be a legitimate to count the odd sentence "YouTube had been retailed to Google" as evidence for the fact that Google bought YouTube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Method 2: Combining Frame Semantics and Dependency Paths</head><p>The second method we have implemented compares the dependency structure of example sentences found in PropBank with the dependency structure of candidate sentences. (VerbNet does not list example sentences for lexical entries, so could not be used here.) In a preprocessing step, all example sentences in PropBank are analyzed and the dependency paths from the head to each of the frame elements are stored. For example, in the sentence "The Soviet Union has purchased roughly eight million tons of grain this month", "purchased" is recognized as the head, "The Soviet Union" as ARG0, "roughly eight million tons of grain" as ARG1, and "this month" as an adjunct of type TMP. The stored paths to each are as follows:</p><formula xml:id="formula_4" coords="5,83.90,570.25,122.19,43.79">headPath = /i role = ARG0, paths = {./s, ./subj,} role = ARG1, paths = {./obj} role = TMP, paths = {./mod}</formula><p>This says that the head is at the root, ARG0 is at both surface subject (s) and deep subject (subj) position<ref type="foot" coords="5,107.16,657.49,3.99,6.91" target="#foot_3">4</ref> , ARG1 is the deep object (obj), and TMP is a direct adjunct (mod) of the head.</p><p>Questions are annotated as described in Section 4. Sentences that potentially contain answer candidates are then retrieved by posing an rather abstract query consisting of key words from the question. Once we have obtained a set of candidate-containing sentences, we ask the following questions of their dependency structures compared with those of the example sentences from PropBank: 1a Does the candidate-containing sentence share the same head verb as the example sentence? 1b Do the candidate sentence and the example sentence share the same path to the head? 2a In the candidate sentence, do we find one or more of the example's paths to the answer role? 2b In the candidate sentence, do we find all of the example's paths to the answer role? 3a Can some of the paths for the other roles be found in the candidate sentence? 3b Can all of the paths for the other roles be found in the candidate sentence? 4a Do the surface strings of the other roles partially match those of the question? 4b Do the surface strings of the other roles completely match those of the question?</p><p>Tests 1a and 2a of the above are required criteria: If the candidate sentence does not share the same head verb or if we can find no path to the answer role, we exclude it from further processing.</p><p>Each sentence that passes steps 1a and 2a is assigned a weight of 1. For each of the remaining tests that succeeds, we multiply that weight by 2. Hence an candidate sentence that passes all the tests is assigned a weight 64 times higher than a candidate that only passes tests 1a and 2a. We take this as reasonable, as the evidence for having found a correct answer is indeed very weak if only tests 1a and 2a succeeded and very high if all tests succeed. Whenever condition 2a holds, we can extract an answer candidate from the sentence: It is the phrase that the answer role-path points to. All extracted answers are stored together with their weights. If we retrieve the same answer more than once, we simple add the new weight to the old ones. After all candidate sentences have been compared with all pre-extracted structures, the ones that do not show the correct semantic type are removed. This is especially important for answers that are realized as adjuncts (see Section 4). We choose the answer candidate with the highest score as the final answer.</p><p>We now illustrate this method with respect to our question "Who purchased YouTube?" The roles assignment process produces this result: "YouTube" is ARG1 and the answer is ARG0. From the web we retrieve inter alia the following sentence: "Their aim is to compete with YouTube, which Google recently purchased for more than $1 billion." The dependency analysis of relevant phrases is: headPath = /i/pred/i/mod/pcomp-n/rel/i phrase = "Google", paths = {./s, ./subj,} phrase = "which", paths = {./obj} phrase = "YouTube", paths = {../..} phrase = "for more than $1 billion", paths = {./mod} If we annotate this sentence by using the analysis from the above example sentence ("The Soviet Union has purchased ...") we get the following (partially correct) frame assignment: "Google" is ARG0, "which" is ARG1, "for more than $1 billion" is TMP.</p><p>The following table shows the results of the 8 tests described above:</p><formula xml:id="formula_5" coords="6,86.03,427.96,198.74,18.13">1a 1b - 2a OK 2b OK 3a OK 3b OK 4a - 4b -</formula><p>Test 1a and 2a succeeded, so this sentence is assigned an initial weight of 1. However, only three other tests succeed as well, so its final weight is 8. This rather low weight for a positive candidate sentence is due to the fact that we compared it against a dependency structure which it only partially matched. However, it might very well be the case that another of the annotated sentences shows a perfect fit. In such a case this comparison would result in a weight of 64. If these were the only two sentences that produce a weight of 1 or greater, the final weight for this answer candidate would be 8 + 64 = 72.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation of the Frame Semantics Approaches</head><p>We do not have a separate evaluation of what the above described frame semantics algorithms contributed to our TREC 2006 entry. Rather, we chose</p><p>to evaluate them on the TREC 2002 QA test set because test sets from 2004 and beyond contain question series that pose problems that are separate from the research described in this paper. While the anaphora-resolution component we used for question series performed quite well, we feel that if one wants to evaluate a particular method, adding an additional module, unrelated to the actual problem, can distort the results. The evaluation also reflects searching for answers on the web rather than in the AQUAINT corpus, and so does not distinguish between supported and unsupported judgments.</p><p>Of the 500 questions in the TREC 2002 test set, 236 have be as their head verb. As the work described here essentially concerns verb semantics, such questions fall outside its scope. Evaluation has thus been carried out on only the remaining 264 questions.</p><p>For the first method (cf. Section 4), we evaluated system accuracy separately for each of the three resources, and then together, obtaining the following values:</p><p>FrameNet PropBank VerbNet combined 0.181 0.227 0.223 0.261</p><p>For the combined run we looked up the verb in all three resources simultaneously and all entries from every resource were used. As can be seen, Prop-Bank and VerbNet perform similarly well, while FrameNet's performance is lower. These differences are due to coverage issues: FrameNet is still in development, and further versions with a higher coverage will be released. However, a closer look shows that coverage is a problem for all of the resources. The following table shows the percentage of the head verbs that were looked up during the above experiments based on the 2002 question set, that could not be found (not found). It also lists the percentage of lexical entries that contain no annotated sentences (s = 0), five or fewer (s &lt;= 5), ten or fewer (s &lt;= 10), or more than 50 (s &gt; 50). Finally, the table provides the average (mean) number of lexical entries found per head verb (avg senses) and the average (mean) number of annotated sentences found per lexical entry (avg sent). The problem with lexical entries only containing a small number of annotated sentences is that these sentences often do not exemplify common argument structures, but rather rare ones. As a solution to this coverage problem, we experimented with a cautious technique for expanding coverage. Any head verb, we assumed displays the following three patterns:</p><formula xml:id="formula_6" coords="7,84.67,274.37,166.25,31.14">intransitive: [ARG0] VERB transitive: [ARG0] VERB [ARG1] ditransitive: [ARG0] VERB [ARG1] [ARG2]</formula><p>During processing, we then determined whether the question used the head verb in a standard intransitive, transitive or ditransitive way. If it did, and that pattern for the head verb was not contained in the resources, we temporarily added this abstract frame to the list of abstract frames the system used. This method rarely adds erroneous data, because the question shows that such a verb argument structure exists for the verb in question. By applying this technique, the combined performance increased from 0.261 to 0.284.</p><p>In Section 4 we reported on experiments that make use of FrameNet's inter-frame relations. The next table lists the results we get when (a) using only the question head verb for the reformulations, (b) using the other entries in the same frame as well, (c) using all entries in all frames to which the starting frame is related via the Inheritance, Perspective on and Using relations (by using only those frames which show the same frame elements). all entries in frame 0.204 all entries in related frames (c) (with same frame elements) 0.215</p><p>Our second method described in Section 6, can only be used with FrameNet and PropBank, because VerbNet does not give annotated example sentences. Here are the results: FrameNet PropBank 0.030 0.159</p><p>Analysis shows that PropBank dramatically outperforms FrameNet for three reasons:</p><p>1. PropBank's lexicon contains more entries. 2. PropBank provides many more example sentences for each entry. 3. FrameNet does not annotate peripheral adjuncts, and so does not apply to When-or Where-questions, which are common question types in TREC evaluation sets.</p><p>Of interest is the relatively small overlap found between correct answers returned by method 1 and by method 2. For PropBank, only 3.0% of all questions were answered correctly by both algorithms simultaneously. When the two methods are combined we obtain a accuracy of 0.306. The methods we have described above are complementary. When we combine them (i.e. using method 1 with all three resources and our cautious coverage-extension strategy, with all additional reformulations that FrameNet can produce, and method 2), we achieved an accuracy of 0.367 on the mentioned 264 questions of the TREC 2002 corpus.</p><p>8 Frame Semantics: Related Work So far, there has been little work at the intersection of QA and frame semantics. <ref type="bibr" coords="7,451.64,492.32,69.94,9.46" target="#b3">Fliedner (2004)</ref> describes the functionality of a planned system based on the German version of FrameNet, SALSA <ref type="bibr" coords="7,520.61,519.42,19.39,9.46;7,313.20,532.97,52.26,9.46">(Erk et al., 2003)</ref>, but no so far no paper describing the completed system has been published. <ref type="bibr" coords="7,324.11,560.47,141.28,9.46" target="#b9">Novischi and Moldovan (2006)</ref> use a technique that builds on a combination of lexical chains and verb argument structures extracted from VerbNet to re-rank answer candidates. The authors' aim is to recognize changing syntactic roles in cases where an answer sentence shows a head verb different from the question (similar to work described here in Section 4). However, since VerbNet is based on thematic rather than semantic roles, there are problems in using it for this purpose, illustrated by the following VerbNet pattern for buy and sell:</p><formula xml:id="formula_7" coords="8,82.76,77.96,177.54,16.28">[Agent] buy [Theme] from [Source] [Agent] sell [Recipient] [Theme]</formula><p>Starting with the sentence "Peter bought a guitar from Johnny", and mapping the above roles for buy to those for sell, the resulting paraphrase in terms of sell would be "Peter sold UNKNOWN a guitar". That is, there is nothing blocking the Agent role of buy being mapped to the Agent role of sell, nor anything linking the Source role of buy to any role in sell. There is also a coverage problem: The authors report that their approach only applies to 15 of 230 TREC 2004 questions. They report a performance gain of 2.4% (MMR for the top 50 answers), but it does not become clear whether that is for these 15 questions or for the complete question set.</p><p>The way in which we uses the web in our first method is somewhat similar to <ref type="bibr" coords="8,205.36,302.66,88.74,9.46" target="#b1">(Dumais et al., 2002)</ref>. However, our approach allows control of verb argument structures, tense and voice and thus we can create a much larger set of reformulations.</p><p>Regarding our second method, two papers describe related ideas: Firstly, in <ref type="bibr" coords="8,208.01,371.66,90.78,9.46" target="#b0">(Bouma et al., 2005)</ref> the authors describe a Dutch QA system which makes extensive use of dependency relations. In a preprocessing step they parsed and stored the full text collection for the Dutch CLEF QA-task. When their system is asked a question, they match the dependency structure of the question against the dependency structures of potential answer candidates. Additionally, a set of 13 equivalence rules allows transformations of the kind "the coach of Norway, Egil Olsen" ⇔ "Egil Olsen, the coach of Norway".</p><p>Secondly, <ref type="bibr" coords="8,130.64,521.96,114.69,9.46" target="#b13">Shen and Klakow (2006)</ref> use dependency relation paths to rank answer candidates. In their work, a candidate sentence supports an answer if relations between certain phrases in the candidate sentence are similar to the corresponding ones in the question.</p><p>Our work complements that described in both these papers, based as it is on a huge collection of semantically annotated example sentences: We do not require the structure of the answer candidate to match the question's structure: Instead we require it to match one of the annotated example sentences. This allows us to deal with a much wider range of syntactic possibilities, as the resources we use do not only document verb argument structures, but also the many ways they can be syntactically realized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Frame Semantics: Discussion</head><p>Both methods presented in this paper build on frame semantics but with different aims in mind: The first method focuses on creating obvious answercontaining sentences. Because in these sentences, the head and the semantic roles are usually adjacent, it is possible to create exact search queries that will lead to answer candidates of a high quality. Our second method can deal with a wider range of syntactic variations but here the link to the answer sentences' surface structure is not obvious, thus no exact queries can be posed.</p><p>The first method performs better than the second one. This is largely due to the fact that the web contains such a huge collection of natural-language texts. It is important to obtain a set of high quality answer candidates, something that the first method was purposely build for. The second method, however, is very useful to fill the gaps of the first.</p><p>The overall accuracy we achieved suggest that employing frame semantics for question answering is indeed useful. The evaluation results presented in the last section compare nicely to TREC evaluation results. <ref type="foot" coords="8,344.41,418.18,3.99,6.91" target="#foot_5">6</ref> This is an especially strong point, because many high performing TREC systems combine miscellaneous strategies, most of them already know to perform well. Because the research question driving this work was to determine how frame semantics can benefit QA, we deliberately designed the approaches to only build on frame semantics. We did not chose to extend an already existing approach with a few frame semantic features.</p><p>Our results are convincing qualitatively as well as quantitavely: Detecting paraphrases and drawing inferences is a key challenge in question answering, which our methods achieve in various ways:</p><p>• They both recognize different verb-argument structures of the same verb. • Method 1 controls for tense and voice: Our system will not take a future perfect sentence for an answer to a present perfect question. • For method 1, no answer candidates altered by mood or negation are accepted.</p><p>• Method 1 can create and recognize answer sentences, whose head is synonymous or related in meaning to the answers head. In such transformations, we are also aware of potential changes in the argument structure.</p><p>• The annotated sentences in the resources enables method 2 to deal with a wide range of syntactic phenomena.</p><p>10 Finding Supporting AQUAINT Documents</p><p>All of QuALiM's answer finding approaches use the web as their only source of information. We initially do not search the AQUAINT corpus. However, TREC requires us to return not only an answer but also a supporting AQUAINT document. Hence, in order to be able to participate in TREC, a module to map answer candidates found on the web to an AQUAINT documents is needed. The module we used this year is a new development based on Lucene <ref type="bibr" coords="9,108.95,376.37,147.21,9.46" target="#b4">(Hatcher and Gospodnetić, 2004</ref>) and replaces the module we used in 2004 and 2005.</p><p>For every question we take into account the top 5 answer candidates. Question 181.4 "Who is the captain of Manchester United?", for example, yields the following list:</p><p>1.00: "Gary Neville" 0.74: "Roy Keane" 0.53: "Steve Bruce" 0.44: "David Beckham" 0.43: "Ruud van Nistelrooy"</p><p>The numbers represent the relative confidence values that the system assigns to each candidate. Note that "Gary Neville" is the actual correct answer as of November 2006, though that was not the case at the time the AQUAINT corpus was written. "Roy Keane", "Steve Bruce" and "Ruud van Nistelrooy" were former captains of the club, while "David Beckham" played for Manchester, but was never the team's captain. (He was captain of the English national team until recently.)</p><p>For each of the answer candidates, we create a query that consists of keywords from the question and the answer. For the above example, such queries would be "Gary Neville" captain "Manchester United" or "Roy Keane" captain "Manchester United"</p><p>We then employ Lucene to find promising documents that contain all or many parts of the query. In the 100 best scoring documents we search for sentences containing the answer. We then assign a score to each of the sentences that reflects how many of the question keywords or keyphrases occur in it. Finally, we combine all this evidence using this formula:</p><formula xml:id="formula_8" coords="9,338.45,229.08,176.30,42.64">f inal score = x * web score + y * document score + z * sentence score</formula><p>Prior to this year's evaluation period we conducted an experiment on the TREC 2002 and TREC 2004 corpora in order to find the best assignments for the variables x, y and z. In order to do this, we ran the system several times with all possible assignments for x, y and z ranging from 0 to 10. We then evaluated each of the runs automatically. The assignments for the run that produced the best result was x = 0, y = 10, z = 7.</p><p>Surprising is the assignment of 0 to x. This implies, that the amount of evidence on the web for a particular answer candidate should not be taken into account when searching for the final answer in the corpus. Does this suggest that the web is not a reliable source for question answering? We do not think so.</p><p>Rather, the web is a heterogeneous source of information. Lots of different views are expressed, and the information found has an emphasis on the here and now. (Despite the globalization, the "here" still largely depends on where the user lives, as this will affect the choice of web sites he/she decides to read, e.g. CNN.com vs. BBC.co.uk. Also, Google.com and Google.co.uk do not return the same results.) In contrast, the AQUAINT corpus is a rather homogeneous information source. Its articles have been compiled by mostly American (and partly Chinese) journalists in a period of a few years, which now lies six to ten years in the past. Information regarded as true from that view and at that time period can still be found on the web: At the time of writing, "Bill Clinton is the President of the United States" gets 72 hits on Google, but "George W. Bush is the President of the United States" gets 404. (Consider also the results we obtained for the question "Who is the captain of Manchester United?"). But it is very hard to determine to which view or time period a certain fact found on the web belongs.</p><p>For the above example, our system finally changed its mind and returned "Roy Keane" as the final answer, because the corresponding query received a higher score than the one for "Garry Neville". Indeed, Roy Keane was Manchester captain from 1998 to 2005, so with respect to the AQUAINT corpus, this is the correct answer.</p><p>Hence, strictly speaking, we do not search the web for answers, but for answer candidates. The one that fits the AQUAINT corpus best becomes the final answer. While several researchers use the term web validation to describe their techniques, we might use the term corpus validation for ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Other Questions</head><p>QuALiM's approach to answering other questions in TREC 2006 is very similar to the one we used in TREC 2004 <ref type="bibr" coords="10,128.97,440.82,115.08,9.46" target="#b5">(Kaisser and Becker, 2004</ref>). However we reimplemented the algorithm for this year's contribution and-because we felt that our method potentially could perform better-spend considerable time on parameter tweaking etc. In 2004 we achieved the 10th best result for other question <ref type="bibr" coords="10,221.30,508.57,72.80,9.46" target="#b14">(Voorhees, 2005)</ref>, in 2006 our method performed best.</p><p>In a first step, we create search queries from the targets. For simple targets (e.g. person names), we use the target as it is and an additional quoted version. (For the target "Warren Moon" our queries are: Warren Moon and "Warren Moon".) If the target is an event, we extract its main NP, so that for "John William King convicted of murder" the queries are: John William King convicted of murder and "John William King". If the target ends with a PP we add an additional, quoted query without that PP, so the queries for the target "Great Wall of China" are Great Wall of China, "Great Wall of China" and "Great Wall". In total, we developed eight such rules.</p><p>The queries are sent to a search engine and we count the occurrence of all non-stop words from the snippets returned in the top 50 results. This is the list of words we obtain for the target "Warren Moon": 148: "moon" 145: "warren" 30: "football" 27: "nfl" 20: "houston" "oilers" 18: "autographed" 11: "quarterback", "jerseys" 10: "hall", "time", "18", "throwback" 9: "player", "born", "only", "1956", "pro", "november" "sports" 8: "jersey", "1" 7: "los", "angeles", "team", "authentic", "career", "fame", "free" ...</p><p>We then search for sentences in the AQUAINT corpus that contain the target (or parts of it). If a sentence is regarded as too long (more than 100 non-white space characters) we cut it into smaller parts at punctuation marks it contains, if possible. Every sentence is then assigned a score that is the sum of all weights of all web words occurring in it divided by the length of the sentence in non-white space characters.</p><p>We now select the highest scoring sentence as our first answer sentence. This sentence is then removed from the list and the weight of every word that occurs in it is divided by 5 (with the exception of words from the target -here, "moon" and "warren" -whose weight is divided by 2). Again, all sentences are scored and the one with the highest weight is selected. This is repeated until the length of all answer sentences exceed a certain threshold.</p><p>The following table lists the length threshold of each run in non-whitespace characters and the results they received. Run 1 and run 2 used the method described above. But for run 1 the best scoring sentences up to a limit of 200 non-whitespace characters were omitted. This was done because TREC explicitly wants participants to avoid repeating answers to previous factoid and list questions in the series as answers to an other question. While we do not explicitly exclude sentences that contain answers to previous questions, we observed in recent years that the best scoring sentences often contain the answers to these questions.</p><p>Run 3 reranked the results of the method described above based on importance indicators, such as superlative constructions and a number of other expressions. The assumption that these are useful with respect to answering other questions is based on a recent study of judgment consistency by <ref type="bibr" coords="11,283.65,238.45,15.15,9.46;11,72.00,252.00,128.09,9.46" target="#b6">Lin and Demner-Fushman (2006)</ref>, where relevant other question nuggets were judged as either vital or okay by 10 different judges rather than the single assessor standardly used in TREC. Our investigation of the nuggets in Lin and Demner-Fushman's study yielded some interesting results. First of all, we created a word frequency list from all nuggets judged as vital in TREC 2004 and 2005 (where nuggets judged as vital by more than one assessor were counted as many times as judged vital). Our aim was to identify frequently occurring words that could serve as importance indicators in answering other questions. While the top 20 of frequent words mainly include prepositions and forms of the verb to be or to have, ranks 12, 13, 14, and 20 are occupied by the words first, largest, million and most, respectively. Interestingly, all but one of the instances of "most" judged as vital are part of a superlative construction (i.e., "most" + adjective/adverb). Given that "largest" is also a superlative form, these results suggest that superlatives might be useful indicators of nugget importance. A closer investigation of the nuggets containing superlatives further shows that, on average, at least half of the TREC topics have at least one superlative nugget. Furthermore, of a total of 69 superlative nuggets, 32 (i.e. almost half) are judged vital by more than 9 assessors.</p><p>On the basis of these results, a scoring mechanism was devised for run 3 that reranked the snippets used for run 1 and 2 according to the presence of superlatives. Other potential importance indicators such as the words "first", "the only", and "unique", and a number of frequently occurring content words (such as "million", "oil", and "countries") were also considered, but given less weight.</p><p>As the results above show, run 3 performed worse than the other two runs. This may be due to either or both of the following. First, the evaluation metric employed by TREC seems to favor longer submissions. Secondly, a subsequent analysis of the nuggets containing superlatives <ref type="bibr" coords="11,468.10,307.27,71.90,9.46" target="#b11">(Scheible, 2007)</ref> showed that they can be distinguished by how the question target (T1) relates to the superlative target (T2): T1 and T2 may coincide (class S1 in table below), or T2 may be part of, closely related to or part of the comparison set of T1 (class S2 below). Finally, T1 may be unrelated or only distantly related to T2 (class S3 in These results strongly suggest that while superlatives alone are not a reliable indicator of nugget importance, S1 membership may be. This calls for a more sophisticated approach, where class S1 superlatives can be distinguished from other types of superlatives. We are currently at work on such a method, which can then be employed in the "other" questions task of TREC 2007.</p><p>12 Official TREC results The difference between our factoid runs lies in the number of NIL answers returned (run 3 &gt; run 2 &gt; run 1). The runs for list questions differed in the number of instances returned for each question (run 3 &gt; run 2 &gt; run 1). The difference between the other question runs is described in section 11.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,77.38,655.22,177.54,6.32;3,184.98,665.18,102.21,6.32;3,77.38,675.15,177.54,6.32;3,184.98,685.11,91.45,6.32;3,77.38,695.07,177.54,6.32;3,184.98,705.04,112.97,6.32;3,82.76,715.00,16.14,6.32"><head></head><label></label><figDesc>Buyer[Subj,NP] VERB Goods[Obj,NP] Seller[Dep,PP-from] Buyer[Subj,NP] VERB Goods[Obj,NP] Money[Dep,PP-for] Buyer[Subj,NP] VERB Goods[Obj,NP] Recipient[Dep,PP-for] ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,323.96,288.48,150.64,6.32;3,323.96,298.44,209.81,6.32;3,323.96,308.41,172.16,6.32;3,323.96,318.37,166.78,6.32;3,334.72,328.33,53.80,6.32;3,323.96,338.30,16.14,6.32"><head></head><label></label><figDesc>ANSWER[NP] purchased YouTube ANSWER[NP] (has|have) purchased YouTube ANSWER[NP] had purchased YouTube YouTube (was|were) purchased by ANSWER[NP] ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,72.00,414.59,226.80,144.23"><head></head><label></label><figDesc>The following table shows the first 20 words of the frequency list:</figDesc><table coords="11,92.55,447.43,185.69,111.38"><row><cell cols="2">rank word</cell><cell>freq.</cell><cell>rank</cell><cell>word</cell><cell>freq.</cell></row><row><cell>1</cell><cell>of</cell><cell>1.262</cell><cell>11</cell><cell>from</cell><cell>215</cell></row><row><cell>2</cell><cell>in</cell><cell>1.255</cell><cell>12</cell><cell>first</cell><cell>198</cell></row><row><cell>3</cell><cell>the</cell><cell>895</cell><cell>13</cell><cell>largest</cell><cell>187</cell></row><row><cell>4</cell><cell>to</cell><cell>755</cell><cell>14</cell><cell>million</cell><cell>181</cell></row><row><cell>5</cell><cell>and</cell><cell>498</cell><cell>15</cell><cell>at</cell><cell>175</cell></row><row><cell>6</cell><cell>for</cell><cell>37</cell><cell>16</cell><cell>on</cell><cell>174</cell></row><row><cell>7</cell><cell>a</cell><cell>411</cell><cell>17</cell><cell>with</cell><cell>164</cell></row><row><cell>8</cell><cell>is</cell><cell>341</cell><cell>18</cell><cell>as</cell><cell>157</cell></row><row><cell>9</cell><cell>was</cell><cell>265</cell><cell>19</cell><cell>has</cell><cell>153</cell></row><row><cell>10</cell><cell>by</cell><cell>254</cell><cell>20</cell><cell>most</cell><cell>139</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,313.20,402.11,226.80,320.25"><head></head><label></label><figDesc>table below):Of the 69 nuggets containing superlatives inTREC 2004 and 2005, 46  fall into subclass S1, 15 into subclass S2 and 8 into subclass S3. While we noted earlier that 32/69 (46%) of superlativecontaining nuggets were judged vital by more than 9 assessors, these judgments are not equally distributed over the subclasses: the following table shows that 87% of S1 nuggets are judged as vital, while only 38% of S3 nuggets are.In the TREC 2006 evaluation of other questions, 60 superlative-containing nuggets were judged vitalor okay by the single TREC assessor. But the judgments show a similar distribution, with 91% of S1 superlatives judged as vital, but only 20% of class S3 superlatives:</figDesc><table coords="11,323.04,427.59,207.12,255.58"><row><cell></cell><cell>number</cell><cell>% of</cell><cell>% of</cell></row><row><cell></cell><cell>of</cell><cell>vital</cell><cell>okay</cell></row><row><cell></cell><cell cols="3">instances judgments judgments</cell></row><row><cell>S1</cell><cell>32</cell><cell>91%</cell><cell>9%</cell></row><row><cell>S2</cell><cell>13</cell><cell>54%</cell><cell>46%</cell></row><row><cell>S3</cell><cell>15</cell><cell>20%</cell><cell>80%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>T1</cell><cell>nugget (T2 in italics)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>S1</cell><cell>AARP</cell><cell>Largest seniors organization</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Florence S2 Nightingale</cell><cell>Nightingale Medal highest international nurses award</cell></row><row><cell></cell><cell></cell><cell></cell><cell>S3</cell><cell>Kurds</cell><cell>Irbil largest city controlled by Kurds</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>number</cell><cell>% of</cell><cell>% of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>of</cell><cell>vital</cell><cell>okay</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>instances judgments judgments</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>S1</cell><cell>46</cell><cell>87%</cell><cell>13%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>S2</cell><cell>15</cell><cell>59%</cell><cell>40%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>S3</cell><cell>8</cell><cell>38%</cell><cell>60%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,329.34,704.20,210.66,7.77;2,313.20,714.16,83.79,7.77"><p>A realization for the fourth structure would be: "$50 won't even purchase a dress".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,88.14,704.20,210.66,7.77;4,72.00,714.16,121.01,7.77"><p>See<ref type="bibr" coords="4,103.69,704.20,94.63,7.77" target="#b5">(Kaisser and Becker, 2004</ref>) for an explanation of how QuALiM processes answer types.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,329.34,704.04,210.66,7.94;4,313.20,714.16,53.54,7.77"><p>Note the different meaning of frame in FrameNet and Prop-Bank/VerbNet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,88.14,694.24,210.66,7.77;5,72.00,704.20,226.79,7.77;5,72.00,714.16,163.57,7.77"><p>MiniPar allows more than one path between nodes due, for example, to traces. The given example is MiniPar's way of indicating that this is a sentence in active voice.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,329.34,694.24,210.66,7.77;6,313.20,704.20,226.79,7.77;6,313.20,714.16,215.55,7.77"><p>As VerbNet contains no annotated sentences, it is not listed. Note also, that these figures are not based on the resources in total, but on the head verbs we looked up for our evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,329.34,714.16,191.86,7.77"><p>However, note the differences in the evaluation setup.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by <rs type="funder">Microsoft Research</rs> through the European PhD Scholarship Programme.</p></div>
			</div>
			<div type="funding">
<div><p>Agent[<rs type="programName">NP] VERB Theme[NP] Agent[NP] VERB Theme[NP] Source[PP-from] Agent[NP] VERB Theme[NP] Asset[PP-for] Asset[NP] VERB Theme[NP]</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KfpwDFE">
					<orgName type="program" subtype="full">NP] VERB Theme[NP] Agent[NP] VERB Theme[NP] Source[PP-from] Agent[NP] VERB Theme[NP] Asset[PP-for] Asset[NP] VERB Theme[NP]</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,72.00,638.94,226.80,8.64;12,82.91,649.90,215.89,8.64;12,82.91,660.86,215.89,8.64;12,82.91,671.63,167.99,8.59" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,262.83,649.90,35.97,8.64;12,82.91,660.86,199.79,8.64">Question Answering for Dutch using Dependency Relations</title>
		<author>
			<persName coords=""><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jori</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lonneke</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jrg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,82.91,671.63,163.32,8.59">Proceedings of the CLEF 2005 Workshop</title>
		<meeting>the CLEF 2005 Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.00,691.60,226.80,8.64;12,82.91,702.56,215.89,8.64;12,82.91,713.33,192.94,8.82;12,313.20,76.47,226.80,8.64;12,324.11,87.43,215.89,8.64;12,324.11,98.39,215.89,8.64;12,324.11,109.17,207.06,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,408.81,87.43,131.18,8.64;12,324.11,98.39,215.89,8.64;12,324.11,109.35,82.60,8.64">Towards a Resource for Lexical Semantics: A Large German Corpus with Extensive Semantic Annotation</title>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Bankom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,181.55,702.56,117.25,8.64;12,82.91,713.33,188.46,8.82;12,425.38,109.17,100.94,8.59">Web Question Answering: Is More Always Better? Proceedings of UAI 2003</title>
		<editor>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Kowalski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Pado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
		</imprint>
	</monogr>
	<note>Proceedings of ACL-2003</note>
</biblStruct>

<biblStruct coords="12,313.20,127.59,226.80,8.64;12,324.11,138.37,215.89,8.82;12,324.11,149.33,89.65,8.59" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,353.25,138.55,128.35,8.64">The Berkeley FrameNet Project</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><forename type="middle">F</forename><surname>Baker Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,503.94,138.37,36.07,8.59;12,324.11,149.33,84.17,8.59">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,167.74,226.80,8.64;12,324.11,178.52,215.89,8.82;12,324.11,189.48,215.89,8.59;12,324.11,200.44,134.07,8.59" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,420.53,167.74,119.47,8.64;12,324.11,178.70,81.97,8.64">Towards Using FrameNet for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Gerhard</forename><surname>Fliedner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,433.13,178.52,106.86,8.59;12,324.11,189.48,215.89,8.59;12,324.11,200.44,129.48,8.59">Proceedings of the LREC 2004 Workshop on Building Lexical Resources from Semantically Annotated Corpora</title>
		<meeting>the LREC 2004 Workshop on Building Lexical Resources from Semantically Annotated Corpora</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,218.68,226.80,8.82;12,324.11,229.64,136.43,8.82" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetić</surname></persName>
		</author>
		<title level="m" coord="12,499.61,218.68,40.39,8.59;12,324.11,229.64,24.43,8.59">Lucene in Action</title>
		<imprint>
			<publisher>Manning Publications Co</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,248.05,226.80,8.64;12,324.11,259.01,215.89,8.64;12,324.11,269.79,215.89,8.82;12,324.11,280.75,173.43,8.59" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,486.50,248.05,53.50,8.64;12,324.11,259.01,215.89,8.64;12,324.11,269.97,32.69,8.64">Question Answering by Searching Large Corpora with Linguistic Methods</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kaisser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tilman</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,379.13,269.79,160.87,8.59;12,324.11,280.75,117.31,8.59">The Proceedings of the 2004 Edition of the Text REtrieval Conference</title>
		<meeting><address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,299.17,226.80,8.64;12,324.11,309.95,215.89,8.82;12,324.11,320.90,97.77,8.59" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,522.68,299.17,17.32,8.64;12,324.11,310.13,163.91,8.64">Will Pyramids Built of Nuggets Topple Over?</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,503.94,309.95,36.07,8.59;12,324.11,320.90,92.40,8.59">Proceedings of the HLT/NAACL</title>
		<meeting>the HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,339.32,226.80,8.64;12,324.11,350.10,215.89,8.82;12,324.11,361.06,34.03,8.59" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,404.84,339.32,135.16,8.64;12,324.11,350.28,38.18,8.64">Dependency-based Evaluation of MINIPAR</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,382.89,350.10,157.12,8.59;12,324.11,361.06,29.78,8.59">Workshop on the Evaluation of Parsing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,379.48,226.80,8.64;12,324.11,390.44,215.89,8.64;12,324.11,401.40,215.89,8.64" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Miller</surname></persName>
		</author>
		<title level="m" coord="12,536.12,390.44,3.87,8.64;12,324.11,401.40,211.56,8.64">troduction to WordNet: An On-Line Lexical Database</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,419.63,226.80,8.64;12,324.11,430.59,215.89,8.64;12,324.11,441.37,215.89,8.82;12,324.11,452.33,215.89,8.59;12,324.11,463.29,111.78,8.59" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,504.02,419.63,35.98,8.64;12,324.11,430.59,215.89,8.64;12,324.11,441.55,31.72,8.64">Question Answering with Lexical Chains Propagating Verb Arguments</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.36,441.37,156.64,8.59;12,324.11,452.33,215.89,8.59;12,324.11,463.29,106.66,8.59">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,481.71,226.80,8.64;12,324.11,492.66,215.89,8.64;12,324.11,503.44,215.89,8.82;12,324.11,514.58,56.73,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,357.02,492.66,182.98,8.64;12,324.11,503.62,92.64,8.64">The Proposition Bank: An Annotated Corpus of Semantic Roles</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,429.84,503.44,106.31,8.59">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,532.82,226.80,8.64;12,324.11,543.78,215.89,8.64;12,324.11,554.74,33.11,8.64" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,412.96,532.82,127.04,8.64;12,324.11,543.78,47.40,8.64">A Computational Treatment of Superlatives</title>
		<author>
			<persName coords=""><forename type="first">Silke</forename><surname>Scheible</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis Proposal</note>
</biblStruct>

<biblStruct coords="12,313.20,572.79,226.80,8.82;12,324.11,583.75,215.89,8.82;12,324.11,594.89,110.32,8.64" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,456.03,572.79,83.98,8.59;12,324.11,583.75,157.66,8.59">VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon</title>
		<author>
			<persName coords=""><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,313.20,613.13,226.80,8.64;12,324.11,624.09,215.89,8.64;12,324.11,634.87,215.89,8.82;12,324.11,645.83,215.89,8.59;12,324.11,656.79,80.51,8.59" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,471.45,613.13,68.55,8.64;12,324.11,624.09,215.89,8.64;12,324.11,635.05,29.27,8.64">Exploring Correlation of Dependency Relation Paths for Answer Extraction</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,371.09,634.87,168.91,8.59;12,324.11,645.83,215.89,8.59;12,324.11,656.79,75.39,8.59">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,313.20,675.20,226.80,8.64;12,324.11,685.98,215.89,8.82;12,324.11,696.94,215.89,8.59;12,324.11,707.90,22.42,8.59" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,423.43,675.20,116.57,8.64;12,324.11,686.16,105.74,8.64">Overview of the TREC 2004 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,448.42,685.98,91.58,8.59;12,324.11,696.94,184.19,8.59">The Proceedings of the 2004 Edition of the Text REtrieval Conference</title>
		<meeting><address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2005. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
