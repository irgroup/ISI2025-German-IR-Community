<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.22,75.35,415.57,12.58">Knowledge Transfer and Opinion Detection in the TREC2006 Blog Track</title>
				<funder ref="#_Gv3KSVz">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,128.46,116.04,43.08,9.88"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>huiyang@cs.cmu.edu</email>
						</author>
						<author>
							<persName coords="1,289.50,116.04,29.69,9.88"><forename type="first">Luo</forename><surname>Si</surname></persName>
						</author>
						<author>
							<persName coords="1,433.44,116.04,57.03,9.88"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Inst. School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Language Technologies Inst. School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.22,75.35,415.57,12.58">Knowledge Transfer and Opinion Detection in the TREC2006 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E18AAA6ED37BB060A0794710A33FE74</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper describes the opinion detection system developed in Carnegie Mellon University for TREC 2006 Blog track. The system performed a two-stage process: passage retrieval and opinion detection. Due to lack of training data for the TREC Blog corpus, online opinion reviews provided in other domains, such as movie review and product review, were used as the training data. Knowledge transfer was performed to make the cross-domain learning possible. Logistic regression ranked the sentence-level opinions vs. objective statements. The evaluation shows that the algorithm is effective in the task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The Blog track is a new task in the TREC 2006 evaluation. The main task of the track is "opinion detection" in the domain of the online blogs posted during the period of Dec 2005 to Feb 2006. The posts and the comments are from Technorati, Bloglines, Blogpulse and other web hosts. The system developed in Carnegie Mellon University for the opinion detection task consists of the modules described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Preprocessing</head><p>The data from NIST are mainly xml files with tags similar to previous TREC web collections, such as WT10G or GOV2. Three types of files are provided by NIST: permalinks (html documents containing the posts), RSS feeds, and blog homepages (html documents containing the homepages of the feeds). Permalinks contains the actual content of the corpus, and are the main target of this task. Indexing, retrieval and opinion detection are all performed based on permalink documents. Further study should involve RSS feeds since they reveal the structure and network of multiple blog posts.</p><p>Due to messy nature of online html, data cleaning is an important preprocessing step. Two approaches were tried. The first utilized the built-in html file cleaning functions of the latest Indri 2.3.1 toolkit <ref type="bibr" coords="1,507.83,475.26,10.63,9.02">[1]</ref>. Additional preprocessing was done to handle stylesheets and javascript, which were not handled by the current version of Indri. The index was then built based on the "trecweb" format supported by Indri. The unit for indexing and retrieval is one permalink document, i.e., one blog post with its following comments. However, it was soon realized that taking the raw html files and throwing them into Indri to index limits the flexibility of gathering more information from the raw text, for example, sentence structure, paragraph information, part-of-speech tagging, etc, which could be important for opinion detection in later stages. These could all be done by creating more functions in Indri, however, due to the amount of programming effort and time constraints, the first approach was discarded.</p><p>The second approach was to transform the html files as closely as possible into regular text files. This was done by several steps.</p><p>Removing HTML tags, scripts, stylesheets: A wrapper was created on top of a tool called "striptags" from the REAP project <ref type="bibr" coords="1,200.92,624.72,10.63,9.02" target="#b0">[2]</ref>. The text documents looked much neater; however, there were still advertisements, text from side bars and menus floating around. These are all noise in the main text. To remove them another module with machine learned patterns could possibly remove them. However, such noise would also be filtered out automatically by the retrieval and opinion detection in the later stages, hence leaving them in caused little harm to the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removing Non-English characters:</head><p>The TREC 2006 Blog corpus contains non-English posts. Characters with ASCII code less than 32 and greater than 126 were removed from the corpus.</p><p>Sentence Splitting: A modified version of UIUC's sentence splitter <ref type="bibr" coords="2,376.45,97.32,11.69,9.02">[3]</ref> was used to annotate the corpus with &lt;s&gt; and &lt;/s&gt; tags that identified the beginning and end of each sentence.</p><p>Creating Artificial Paragraphs: Original line breaks from the text were reserved as segmentations of paragraphs. Moreover, for long original paragraphs, an around-100-word paragraph break was introduced with no crossing of the sentence boundaries.</p><p>Removing Dummy Sentences: If a sentence contained only punctuation, or just a single number, it was removed. This step filtered out form counters largely available on the Web documents. Moreover, if within a sentence, any word had an occurrence of more than N times (N=20 in these tests), that sentence was removed. This step filtered out advertisement and web category anchor text, which were not important content of the Web blog and hence are irrelevant to any potential query &lt;DOC&gt; &lt;TEXT&gt; &lt;PARAGRAPH&gt; &lt;s&gt;blah blah blah&lt;/s&gt; &lt;s&gt;blah blah blah&lt;/s&gt; … &lt;/PARAGRAPH&gt; &lt;PARAGRAPH&gt; … &lt;/TEXT&gt; &lt;/DOC&gt; .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Formulation</head><p>The topic file that NIST provided contains topics each with the title, description and narrative parts. Based on the title field, the topics could be classified into 6 categories [Figure <ref type="figure" coords="2,377.37,361.80,3.71,9.02">1</ref>]: The TREC Blog track required at least one run using just the title as the query. The Minipar parser [1] was applied to identify part-of-speech for the title, description and narrative terms. To create the query, only nouns and adjectives were extracted as query terms. A dictionary () of positive and negative verbs and adjectives was downloaded from the web, and manual selection was used so that the list was not long enough to increase retrieval time too much.</p><p>Based on the nouns and adjectives from the title, description and narrative as well as the sentiment word dictionary, four types of queries were formulated by the system:</p><p>• The title query;</p><p>• The title query and the nouns and adjectives from the descriptions;</p><p>• The title query and sentiment words from the dictionary; and</p><p>• The title query, the nouns and adjectives from the descriptions and sentiment words from the dictionary.  <ref type="bibr" coords="3,202.26,109.80,208.07,9.02;3,202.26,121.32,202.52,9.02;3,202.26,132.84,207.51,9.02;3,202.26,144.30,220.37,9.02;3,202.26,155.82,185.79,9.02;3,202.26,167.34,80.62,9.02;3,436.26,109.80,69.80,9.02;3,436.26,121.32,53.64,9.02;3,436.26,132.84,63.11,9.02;3,436.26,144.30,24.46,9.02">Good, best, better, happy, extraordinary, successful, glad, desirable, worthy, remarkable, funny, lovely, entertaining, decent, beautiful, fascinating, brilliant, gorgeous, perfect, nice, fantastic, impressive, fabulous, amazing, desirable, excellent, great, awesome, splendid, distinctive Bad, awful, suck, worse, worst, poor, annoying, stupid</ref> The Indri queries automatically generated by the system were structured queries (see Indri query language in [1]) which treat "title" as a phrase (an ordered window) , "description" words as expanded terms, and "dictionary" words as terms within an unordered window to "title" words. Here are examples for the above four categories:</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexing and Passage Retrieval</head><p>Two kinds of Indri indexes were built, one with stopword removal and krovetz stemming, and the other with neither of them. The indexes were built over the paragraphs created in the earlier stage.</p><p>Passage retrieval was preferred to document retrieval in our system. For opinion retrieval, it was believed that only a small amount of text, probably a paragraph in a post, was needed to show the loves and hates. Moreover, smaller units help to focus on the opinions instead of objective sentences. Therefore, the accuracy of opinion detection should be higher by using passage retrieval than using document retrieval. Another concern was that there could be multiple opinions about one topic in a blog post. If document is used as the unit, there is no chance to identify different opinions within the same document. However, in the TREC blog evaluation, document is used as the unit to evaluate, i.e., as long as a document contains an opinion, it should be returned in the result. We believe that a finer unit than document is necessary for a more accurate evaluation of opinion detection.</p><p>Since there are many duplicated documents in the Web blog collection, duplicate pruning was conducted after documents were retrieved. First, 1000 paragraphs were retrieved for each query, then the duplicate detection module removed any exact duplicates within the retrieved results. The results list was extended, as necessary, so that 1000 passages remained after duplicate removal. Duplicate removal helped the system focus on unique passages and hence improve the quality of retrieved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Transfer and Opinion Detection</head><p>The opinion detection task was considered as a binary text classification problem. Logistic regression was used to rank the sentences from the retrieved passages. The feature space was represented as } ..., , { </p><formula xml:id="formula_0" coords="4,212.28,153.31,297.52,27.84">) exp( 1 1 ) , | 1 Pr( s s S Y L T θ θ - + = = = = (1)</formula><p>where θ is the parameter vector. Gaussian prior ) , 0 ( 2 I N σ are assumed for the parameters, in particular, for components in the parameter vector, independence is assumed among them. For every component, zeromeans and equal variances are applied. The estimated parameter vector θ is obtained by maximizing loglikelihood function via maximum a posterior (MAP): The training data is from movie and product review; however the testing set is from the blog posts. The domain of training set data is different from that of the testing data. The technique of transfer learning <ref type="bibr" coords="4,502.20,401.76,23.36,9.02">[6][7]</ref> is necessary in this case. The knowledge learned from one domain should be able to help the leaning in another domain, as long as they are similar task. Hence opinion detection with knowledge transfer is preferred in this case.</p><p>A simple approach of knowledge transfer was taken. Based on the two training datasets, movie review and product review, features highly ranked in both domains are considered as common features for the task of opinion detection across all opinion-related domains. In another word, they are considered as domainindependent features. The rest of the features in the training data were then removed in the feature selection step. For example, in the movie review unigram model, "film" has a high occurrence of 1006, and "movie" has a high frequency of 733. Both of them are within the top 35 features, however they are not domainindependent, which means that they are not useful for classifying the opinions and non-opinions in other domains. Those domain-dependent high frequency features were removed from the vocabulary.</p><p>Another source was sentences extracted and annotated from the 2006 TREC Blog corpus itself by creating training topics manually. There were 1201 positive examples and 1240 negative examples manually drawn from the training topics. The training topics used to create the training data were: "Steelers", "Christmas", "Mr. and Mrs. Smith", "Harry Potter", "Condoleezza Rice", "Canon camera", "China rise", "Harvard University", "Hash browns", "Seattle", and "Zoloft".</p><p>Bayesian Logistic Regression toolkit (BBR <ref type="bibr" coords="4,273.58,631.74,11.26,9.02" target="#b2">[5]</ref>) was used to rank each retrieved sentence. The passage score was generated by averaging scores of sentences within a passage. Unigram, bigram and trigram models were built for regression. Another feature used in the experiment was the percentage of adjectives in a sentence by parsing the sentence for its part-of-speech tags. It is important because many opinions describe things using adjectives. For example, "the camera is great," "It is an awesome movie," And "Here's the brief synopsis: the phone is tiny, cute, feels kind of plastic-like." The main features used in the runs submitted to TREC summarized in Table <ref type="table" coords="4,276.95,700.73,3.77,9.02" target="#tab_2">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and Experimental Results</head><p>TREC 2006 Blog task contains 50 topics. For each topic, evaluation was done by calculating the mean average precision (MAP), R-precision and precision at 10 (P@10) of returned passages. Early precision is important. Figure <ref type="figure" coords="5,164.67,216.30,5.01,9.02" target="#fig_3">2</ref> shows the MAP and P@10 of opinion detection task and the topic relevance for the first stage of passage retrieval. The top right and top left panel of Figure <ref type="figure" coords="5,388.65,227.82,5.01,9.02" target="#fig_3">2</ref> show average MAP and P@10 of topic relevance respectively for all 50 topics. The 6 runs are compared with the "best" artificial run (created by using the MAP or P@10 of the best performing system for each topic), and the median artificial run (created by using the MAP or P@10 of the median performing system for each topic). Based on the results, the following observations can be made.</p><p>• Our best run was blog06r2. For both topic relevance and opinion detection, it gives the highest MAP. This run is bigram model with no query expansion. However it was not the run selected for submission to TREC. • The MAPs of the 6 runs are above the median run by 20%-53.3% for topic relevance, 40%-60% for opinion detection. • The P@10 of some runs are below the median run for topic relevance, however, the P@10 for all the runs are beyond the median run for opinion detection. This indicates that our knowledge transfer and logistic regression model is effective to recognize opinions from retrieved passages. • The fact that Bigram and trigram models succeed unigram model is not surprising, since bigram and trigram models capture phrases to express opinions, for example, "I love", "is awesome", "really like it", which is more reliable features than just a single word. • Out of our expectation, query formulation does not play a useful role in passage retrieval. The topic relevance for run blog06r1, blog06r2 and blog06r3 are better than the rest three runs, which make use of query expansion with topic description and opinion words. This shows that the description words in the topic given by NIST are not effective in finding the relevant passages in Blog corpus. There may be some vocabulary mismatch in the description and corpus. Since the query expansion is done with both description words and opinion words, the improvement introduced by adding opinion words in the queries are covered. The experiments conducted after TREC submission shows that query expansion with opinion words do increase the chances of finding opinions in the passage retrieval stage and hence improves the opinion detection. • Topic retrieval plays an important part in the final opinion detection results. It constrains the number of passages that are candidate to be opinions. The low recall of the opinion detection is mainly due to low recall of topic retrieval. Useful documents are not retrieved by the search engine from the blog corpus, though we believe our approach of logistic regression with knowledge transfer is effective in this task, the performance of passage retrieval constraints the opinion detection module, which cannot show a more impressive result. It will be the future work to improve the system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP of Opinion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper showed a two-stage opinion detection system developed for TREC 2006 Blog track. In the frist stage, language model passage retrieval were performed to get the relevant passages to the given topic. In the second stage, knowledge transfer across multiple domains was employed and logistic regression was used as the main algorithm to solve the opinions vs. objective sentences binary text classification problem. Our 6 submitted runs performed not the best, but better than the median of all the submitted runs. The TREC evaluation showed that the knowledge transfer and logistic regression approach are effective for recognizing opinions against objective statements in the blog space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,251.94,553.98,108.05,9.02;2,90.00,565.38,431.98,9.02;2,90.00,576.84,432.01,9.02;2,90.00,588.36,431.96,9.02;2,90.00,599.88,431.96,9.02;2,90.00,611.34,173.59,9.02"><head></head><label></label><figDesc>Figure 1: Topic CategoryThe TREC Blog track required at least one run using just the title as the query. The Minipar parser [1] was applied to identify part-of-speech for the title, description and narrative terms. To create the query, only nouns and adjectives were extracted as query terms. A dictionary () of positive and negative verbs and adjectives was downloaded from the web, and manual selection was used so that the list was not long enough to increase retrieval time too much.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,126.00,237.54,198.63,9.02;3,108.00,246.55,295.40,12.26;3,108.00,258.73,107.40,12.26;3,128.52,273.48,164.45,9.02;3,128.52,285.00,162.29,9.02;3,126.00,296.46,10.02,9.02;3,128.52,307.98,167.25,9.02;3,128.52,319.50,187.75,9.02;3,108.00,328.45,107.40,12.26;3,128.52,343.20,164.45,9.02;3,128.52,354.66,162.29,9.02;3,126.00,366.18,10.02,9.02;3,128.52,377.70,167.25,9.02;3,128.52,389.16,184.38,9.02;3,126.00,400.68,74.67,9.02"><head></head><label></label><figDesc>#combine[paragraph](#3(march of the penguins)) • #combine[paragraph]( #3(march of the penguins) documentary film) • #combine[paragraph]( #uw15( #3(march of the penguins) love) #uw15( #3(march of the penguins) like) … #uw15( #3(march of the penguins) great) #uw15( #3(march of the penguins) awesome)) • #combine[paragraph]( #uw15( #3(march of the penguins) love) #uw15( #3(march of the penguins) like) … #uw15( #3(march of the penguins) great) #uw15( #3(march of the penguins) awesome) documentary film)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,407.34,251.73,3.99,10.79;4,366.30,261.15,6.00,10.79;4,373.20,244.29,6.00,10.79;4,350.70,251.73,3.99,10.79;4,339.72,251.73,3.00,10.79;4,310.98,251.73,2.40,10.79;4,268.02,251.73,14.66,10.79;4,251.22,251.73,15.33,10.79;4,233.04,251.73,3.99,10.79;4,212.04,251.73,20.65,10.79;4,195.24,251.73,15.32,10.79;4,402.30,241.40,3.50,6.30;4,402.30,265.28,3.50,6.30;4,381.42,259.16,3.50,6.30;4,169.86,258.14,12.05,6.30;4,237.24,244.60,12.81,22.02;4,356.70,247.87,6.57,14.68;4,325.38,247.87,6.57,14.68;4,292.98,247.87,6.57,14.68;4,186.18,247.87,6.57,14.68;4,390.66,247.18,6.24,15.51;4,371.58,256.60,7.23,15.51;4,342.84,247.18,6.24,15.51;4,162.66,247.18,6.24,15.51;4,211.50,262.15,3.64,9.05;4,334.86,251.73,4.67,10.79;4,315.78,251.73,6.00,10.79;4,303.42,251.73,5.32,10.79;4,282.24,251.73,6.67,10.79;4,496.02,253.08,11.68,9.02;4,90.00,286.80,431.98,9.02;4,90.00,298.26,434.44,9.02;4,90.00,309.78,432.04,9.02;4,90.00,321.30,432.05,9.02;4,90.00,332.76,432.05,9.02;4,90.00,344.28,431.96,9.02;4,90.00,355.80,432.13,9.02;4,90.00,367.26,258.62,9.02"><head></head><label></label><figDesc>lack of training data, two training data sets in different domains were used. The first dataset was movie review data provided by Pang and Lee (http://www.cs.cornell.edu/People/pabo/movie-review-data/ ). It includes 5000 subjective sentences and 5000 objective sentences. The subjective sentences are sentences expressing opinions about a movie. The objective sentences are descriptions or the storytelling of a movie. The second source was the customer review data provided by Hu and Liu (http://www.cs.uic.edu/~liub/FBS/FBS.html). It contains 4258 sentences in total with 2041 positive examples and 2217 negative examples. The customer reviews are from Amazon.com about 5 electronic products including digital cameras, DVD players and jukeboxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,162.00,285.42,323.50,9.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MAP and Precision @10 for topic relevance and opinion detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,95.40,74.46,379.90,55.88"><head>Table 1 : Opinion Word Dictionary</head><label>1</label><figDesc></figDesc><table coords="3,95.40,86.34,379.90,44.00"><row><cell>#positive</cell><cell>#negative</cell><cell>#positive adjective</cell><cell>#negative</cell></row><row><cell>verb</cell><cell>verb</cell><cell></cell><cell>adjective</cell></row><row><cell>Love,</cell><cell>Hate,</cell><cell></cell><cell></cell></row><row><cell>like</cell><cell>dislike</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,101.58,685.74,420.48,16.58"><head></head><label></label><figDesc>is the binary value indicating presence of feature i w in the sentence. The task is to predict labels Y for test set sentence S, Y=1 when S is an opinion and Y= -1 when S is an objective sentence. Logistic regression model is trained based on m labeled training set examples</figDesc><table coords="3,101.58,685.74,420.48,16.58"><row><cell cols="3">represented as</cell><cell>S</cell><cell>=</cell><cell cols="2">{</cell><cell cols="2">X</cell><cell>1</cell><cell>,</cell><cell cols="2">X</cell><cell cols="2">2</cell><cell cols="4">...,</cell><cell>X</cell><cell>n</cell><cell>}</cell><cell>∈</cell><cell>n } 1 , 0 {</cell><cell>, where</cell></row><row><cell></cell><cell></cell><cell cols="2">D =</cell><cell cols="2">{(</cell><cell cols="2">1 s</cell><cell>,</cell><cell cols="3">1 y</cell><cell cols="2">),</cell><cell cols="2">(</cell><cell>s</cell><cell>2</cell><cell cols="2">,</cell><cell>y</cell><cell>2</cell><cell>)...,</cell><cell>(</cell><cell>s</cell><cell>, m y</cell><cell>m</cell><cell>)}</cell><cell>. The likelihood function is:</cell></row><row><cell>F =</cell><cell>1 w</cell><cell>2 w</cell><cell></cell><cell cols="4">n w</cell><cell></cell><cell cols="11">, where i w 's n-gram features, n is the feature space size. A sentence S was</cell></row></table><note coords="4,331.44,83.23,1.95,6.31;4,322.62,76.82,7.34,10.81"><p>i X</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,95.40,74.46,421.69,92.90"><head>Table 2 : Description of Submitted Runs</head><label>2</label><figDesc></figDesc><table coords="5,95.40,86.34,421.69,81.02"><row><cell cols="3">Run tag Submit Priority Description</cell></row><row><cell>blog06r1</cell><cell>6</cell><cell>Title query, unigram model + adjective percentage</cell></row><row><cell>blog06r2</cell><cell>3</cell><cell>Title query, bigram model + adjective percentage</cell></row><row><cell>blog06r3</cell><cell>5</cell><cell>Title query, trigram model + adjective percentage</cell></row><row><cell>blog06r4</cell><cell>4</cell><cell>Title + description + opinion words, unigram model + adjective percentage</cell></row><row><cell>blog06r5</cell><cell>1</cell><cell>Title + description + opinion words, bigram model + adjective percentage</cell></row><row><cell>blog06r6</cell><cell>2</cell><cell>Title + description + opinion words, trigram model + adjective percentage</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,345.93,455.01,159.15,115.98"><head>Detection for All Runs</head><label></label><figDesc></figDesc><table coords="5,345.93,465.45,159.15,105.53"><row><cell></cell><cell>0.35</cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell>r1</cell></row><row><cell></cell><cell></cell><cell>r2</cell></row><row><cell>MAP</cell><cell>0.2</cell><cell>r3 r4 r5</cell></row><row><cell></cell><cell>0.15</cell><cell>r6</cell></row><row><cell></cell><cell></cell><cell>best</cell></row><row><cell></cell><cell>0.1</cell><cell>median</cell></row><row><cell></cell><cell></cell><cell>worst</cell></row><row><cell></cell><cell>0.05</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>runs</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,99.90,699.36,404.16,9.02;1,90.00,710.82,44.80,9.02"><p>This work was done while the author was at the Language Technologies Institute at Carnegie Mellon University.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported by <rs type="funder">NSF</rs> grant <rs type="grantNumber">IIS-0429102</rs>. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsor. The authors are also grateful to <rs type="person">Mark Hoy</rs> and <rs type="person">Jie Lu</rs> for helpful discussions for the system development.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Gv3KSVz">
					<idno type="grant-number">IIS-0429102</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,100.65,498.48,159.80,9.02" xml:id="b0">
	<monogr>
		<ptr target="http://reap.cs.cmu.edu/" />
		<title level="m" coord="6,108.00,498.48,54.77,9.02">REAP project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.00,510.00,306.37,9.02" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Sentence</forename><surname>Uiuc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Splitter</surname></persName>
		</author>
		<ptr target="http://l2r.cs.uiuc.edu/~cogcomp/atool.php?tkey=SS" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,100.65,532.98,372.71,9.02" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><surname>Bbr</surname></persName>
		</author>
		<ptr target="http://www.stat.rutgers.edu/~madigan/BBR/" />
		<title level="m" coord="6,133.30,532.98,156.02,9.02">Bayesian Logistic Regression Software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.00,544.50,413.94,9.02;6,108.00,556.02,414.01,9.02;6,108.00,567.48,135.06,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,327.00,544.50,194.93,9.02;6,108.00,556.02,22.70,9.02">Transfer Learning by constructing informative priors</title>
		<author>
			<persName coords=""><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,141.34,556.02,380.67,9.02;6,108.00,567.48,79.13,9.02">Workshop on Transfer Learning in the Nineteenth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.00,579.00,414.06,9.02;6,108.00,590.52,414.01,9.02;6,108.00,601.98,95.39,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,321.82,579.00,113.20,9.02">Multi-task Feature Selection</title>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,456.79,579.00,65.27,9.02;6,108.00,590.52,414.01,9.02;6,108.00,601.98,36.13,9.02">the workshop of structural Knowledge Transfer for Machine Learning in the 23rd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
