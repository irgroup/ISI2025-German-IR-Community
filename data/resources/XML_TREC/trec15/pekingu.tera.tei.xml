<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,85.50,92.13,424.09,16.19">Peking University at the TREC 2006 Terabyte Track</title>
				<funder ref="#_WNQyZac">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,237.72,116.72,49.07,10.79"><forename type="first">Li</forename><surname>Jingjing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="laboratory">Networks and Distributed Systems Laboratory</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.49,116.72,63.06,10.79"><forename type="first">Yan</forename><surname>Hongfei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="laboratory">Networks and Distributed Systems Laboratory</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,85.50,92.13,424.09,16.19">Peking University at the TREC 2006 Terabyte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CFB6C4EF234A464AB7718B89B6EE81E5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Evaluation</term>
					<term>Search Engine</term>
					<term>Terabyte</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper details the experiments carried out at TREC 2006 Terabyte Track using Indri Search Engine. There were three tasks in the Terabyte track of TREC 2006, i.e. efficiency task, ad hoc task and named page finding task. We participated in two tasks, and submitted 5 runs for ad hoc task and 3 runs for named page task respectively. In ad hoc task, we looked at the importance of term proximity. In named page finding task, we cared more about the information of document structure and document prior.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Indri is a new indexing and retrieval component developed by the University of Massachusetts for the Lemur Toolkit <ref type="bibr" coords="1,243.45,404.48,10.43,7.63" target="#b2">[2]</ref>. It is a scalable Search Engine that combines the language modeling and inference network approaches into a single framework, and it supports a robust query language, based on the INQUERY query language.</p><p>Our goals for this year's track were modest, to complete runs with Indri Search Engine, to gain a good understanding of Indri retrieval model, and to gain more experience in the procedure of evaluations. We have organized Chinese Web Track for several years <ref type="bibr" coords="1,69.76,502.70,10.36,7.63" target="#b1">[1]</ref>. In order to organize the Chinese Web Track better, the experience of TREC is important to us.</p><p>The remainder of the paper details the experimental work and results obtained for each of the two tasks we participated in. We describe the collection and tasks, indexing environment, retrieval techniques and details of the runs we submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">COLLECTION AND TASK SUMMARY</head><p>The GOV2 corpus, which contains a large proportion of the crawlable pages in .gov domain, was used as the collection. It is made up of about 25 million documents comprising about 426GB of document source.</p><p>The TREC 2006 Terabyte track consisted of three tasks. Ad hoc task was classical ad hoc retrieval task, a task which investigated the performance of systems searching on a static set of documents using a set of previously unseen topics. Participants were given 50 new topics, and for each of these topics participants were asked to return a ranked set of the 10,000 most relevant documents.</p><p>The efficiency task was a task whose aim was to provide a means for comparing efficiency and scalability issues in IR systems. We didn't participate in this task.</p><p>The named page finding task was to search a document by name. An effective retrieval system could return the page at or near rank one. Participants were given about 180 topics, each of which specified a document by name, and were asked to return top 1000 results for each of the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INDEXING ENVIRONMENT</head><p>For various reasons, we ended up using a single PC for both indexing and retrieving all the GOV2 corpus documents. The PC had dual Xeon 2.8GHz CPUs with 4GB RAM running Red Hat Linux release 9.</p><p>We partitioned the GOV2 corpus into 9 roughly equal-sized pieces and built a separate index for each subset using Indri 4.2. All documents were stemmed using the Porter stemmer.</p><p>For the ad hoc task this year, we built index for all the documents, with no special document or link structure indexed. It took 1515 minutes to build the index and the size of full-text index was about 202GB.</p><p>For the named page finding task, we indexed title, mainbody, heading, and inlink fields, and it took 2236 minutes to build the index and the resulting index files was about 204GB in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RETRIEVAL TECHNIQUES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ad hoc task</head><p>For ad hoc task, we adopted three techniques, i.e. query likelihood <ref type="bibr" coords="1,343.39,567.14,10.28,7.63" target="#b3">[3]</ref>, dependence model <ref type="bibr" coords="1,437.56,567.14,12.02,7.63" target="#b4">[4]</ref> and pseudo-relevance feedback <ref type="bibr" coords="1,338.49,577.40,11.01,7.63" target="#b5">[5]</ref>. Query likelihood run was baseline run, which simply weighted each word equally. The dependence model is a mechanism for modeling term proximity features. Pseudorelevance feedback, whose basic idea is to extract expansion terms from the top-ranked documents to formulate a new query for a second round retrieval, is a technique commonly used to improve retrieval performance.</p><p>We converted topics from TREC format to Indri structured queries as follows.</p><p>For automatic runs, we only used the title field of each topic. We parsed the title field, removed the stop words using a stop words list provided by Indri Search Engine, and combined the residual words by Indri operator #combine. For example, topic 804, "ban to human cloning", after being removed stop word "to", can be converted into the following query: #combine ( ban human cloning ) For manual runs, all of the three fields of each topic were used to construct an Indri query. Our aim was to see whether manual run would perform better than automatic run. We observed the three fields of each query, selected important words based on our own knowledge, finally combined the selected words by #combine operator. We weighted each word equally. Take topic 804 for example again, we finally got a query as follows:</p><p>#combine(ban human cloning resolutions legislation rationale)</p><p>Our dependence model was only used to the title-only runs. We also removed stop words in this step. The following query is an example of queries in dependence model: We adopted the default pseudo-relevance feedback algorithm in Indri Search Engine. Three of our runs made use of this technique with fbDocs=10 and fbTerms=30. The original and expanded queries were weighted equally.</p><p>Smoothing plays a very important role in language modeling technique. Indri provides several smoothing methods. We used Dirichlet smoothing for all our ad hoc runs with μ=1600 for query likelihood and μ=4500 for dependence model, using GOV2 collection to estimate parameters. For named page finding task, the smoothing parameter will be described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named page finding task</head><p>In this task, we aimed to investigate the document structure and document prior. So we indexed different fields of documents, such as title, mainbody, heading, and inlink fields. We also investigated Pagerank as the document prior.</p><p>For this task, we submitted three runs. One run(TWTB06NP01) made use of all the four indexed fields, one run(TWTB06NP02) used the title field, and another one(TWTB06NP03) used the title field and Pagerank. As an example, we list three query formulations of topic NP903 in Figure <ref type="figure" coords="2,198.45,619.76,3.29,7.63" target="#fig_1">1</ref>, each of which was a formulation used for one of the three runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OFFICIAL RUNS</head><p>We submitted 5 runs for the ad hoc task, two of which were manual runs and the other three were automatic runs, and submitted 3 runs for the named page finding task.</p><p>For the ad hoc task, we submitted: TWTB06AD01: Automatic title-only run using query likelihood, dependence model, and pseudo-relevance feedback.</p><p>TWTB06AD02: Manual run using query likelihood and pseudorelevance feedback.</p><p>TWTB06AD03: Manual run using query likelihood only.</p><p>TWTB06AD04: Automatic title-only run using query likelihood and dependence model.</p><p>TWTB06AD05: Automatic title-only run using query likelihood as our baseline run.</p><p>For the named page finding task, we submitted: TWTB06NP01: A run using title, mainbody, heading and inlink fields. We smoothed the language model with μ=10, 40, 100, 250 for title field, heading field, inlink field and mainbody field respectively.</p><p>TWTB06NP02: A run using the title field of the documents. The smoothing parameter μ equaled 10.</p><p>TWTB06NP03: A run using the title field and pagerank. The smoothing parameter μ equaled 10. Since the submitted automatic runs gave the top 10.000 documents for both topics 701-800 and new topics 801-850, TREC gave each run two evaluation results. One was for the new topics, and the other one was for all 150 topics. Here we only list the results for new topics 801-850.</p><p>From table 1, we can see that manual runs(TWTB06AD02 and TWTB06AD03) performed the same as the baseline run, and TWTB06AD02 , a manual run with pseudo-relevance feedback, performed the same as TWTB06AD03 which was a manual run without pseudo-relevance feedback. TWTB06AD01 and TWTB06AD04 were two best automatic runs, which provided a great increase over the baseline run.</p><p>From table 2, we can see that our three named page finding runs performed not very well. TWTB06NP03 which used title field and Pagerank prior gained no improvement compared with TWTB06NP02 which used only the title field of documents, and TWTB06NP01 which used all of the three fields of documents was the worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>We participated in two tasks, submitted 5 runs for ad hoc task and 3 runs for named page task respectively, and gained much experience in the procedure of evaluations.</p><p>The problems we encountered were the low speed when retrieving the documents on a single PC and the transformation from the query format given by TREC to the format of Indri structured query. Since we used a single PC, the speed was very low. There were some characters which were reserved or not recognized by Indri structured query language, so we had to remove these characters from the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,87.54,295.82,155.81,7.63;2,122.52,306.07,132.57,7.63;2,173.03,316.33,53.47,7.63;2,171.29,326.59,69.99,7.63;2,122.52,336.85,147.66,7.63;2,173.03,347.17,64.15,7.63;2,171.29,357.43,66.66,7.63;2,171.29,367.69,88.02,7.63"><head></head><label></label><figDesc>#weight( 0.8 #combine(ban human cloning) 0.1 #combine(#1(ban human cloning) #1(ban human) #1(human cloning)) 0.1 #combine(#uw12(ban human cloning) #uw8(ban human) #uw8(ban cloning) #uw8(human cloning) ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,319.98,356.78,211.82,7.88;2,389.70,367.03,72.34,7.88"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Topic NP903 and its corresponding three Indri query formulations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,309.12,692.83,188.52,7.63"><head>Table 1 : Runs submitted for TREC Terabyte ad hoc task using TREC topics 801-850 and top 10,000 documents.Table 2 : Runs submitted for TREC Terabyte named page finding task using topics 901-1081 and top 1,000 documents.</head><label>12</label><figDesc></figDesc><table coords="2,309.12,692.83,188.52,7.63"><row><cell>All 8 runs submitted are summarized in table 1 and 2.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENTS</head><p>This work described therein is supported by <rs type="funder">NSFC</rs> grant <rs type="grantNumber">60435020</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WNQyZac">
					<idno type="grant-number">60435020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,313.52,382.87,89.64,10.50" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.64,396.68,153.20,7.63;3,326.64,406.94,74.53,7.63" xml:id="b1">
	<monogr>
		<ptr target="http://www.cwirf.org" />
		<title level="m" coord="3,326.64,396.68,148.94,7.63">Chinese Web Information Retrieval Forum</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.64,422.06,188.03,7.63;3,326.64,432.32,195.07,7.63;3,326.64,442.64,194.41,7.63;3,326.64,452.90,188.59,7.63;3,326.64,463.16,143.30,7.63" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/˜lemur" />
		<title level="m" coord="3,507.40,442.64,13.65,7.63;3,326.64,452.90,188.59,7.63;3,326.64,463.16,28.29,7.63">The Lemur toolkit for language modeling and information retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.64,478.28,211.52,7.63;3,326.64,488.54,200.39,7.63;3,326.64,498.80,210.44,7.63;3,326.64,509.06,62.66,7.63" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,419.14,478.28,119.02,7.63;3,326.64,488.54,71.91,7.63">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,403.93,488.54,123.10,7.63;3,326.64,498.80,206.02,7.63">The 21st Annual Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.64,524.18,213.75,7.63;3,326.64,534.44,215.41,7.63;3,326.64,544.70,75.53,7.63" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,457.74,524.18,82.65,7.63;3,326.64,534.44,100.49,7.63">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,442.66,534.44,95.44,7.63">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.64,559.82,207.05,7.63;3,326.64,570.08,214.73,7.63;3,326.64,580.40,194.82,7.63" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,411.27,559.82,122.42,7.63;3,326.64,570.08,194.84,7.63">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,524.79,570.08,16.58,7.63;3,326.64,580.40,50.35,7.63">Proc. of SIGIR 2002</title>
		<meeting>of SIGIR 2002<address><addrLine>New Orleans, U.S.A.</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
