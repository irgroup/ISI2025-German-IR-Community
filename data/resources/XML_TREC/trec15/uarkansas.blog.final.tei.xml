<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,211.92,59.93,202.55,16.73">UALR at TREC: Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.88,80.96,64.12,10.80"><forename type="first">Hemant</forename><surname>Joshi</surname></persName>
							<email>hmjoshi@ualr.edu</email>
						</author>
						<author>
							<persName coords="1,269.64,80.96,71.81,10.80"><forename type="first">Coskun</forename><surname>Bayrak</surname></persName>
							<email>cxbayrak@ualr.edu</email>
						</author>
						<author>
							<persName coords="1,370.20,80.96,58.32,10.80"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
							<email>xwxu@ualr.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Arkansas at Little Rock</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,211.92,59.93,202.55,16.73">UALR at TREC: Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A5145B9F62D98996A5A35A0F949A8C56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider Opinion Blog retrieval from classification point of view. We used the active learning method with an integrated feature selection to train the Support Vector Machine algorithm. We wanted to study the effect of different types of features on the classification accuracy of the model generated by the classifier algorithm. We considered mainly three different types of features for 5 runs submitted. Feature types include bag-of-words features, seed-words as features and statistical features. Bag-ofwords features are generated from the actual blog data. Seed-words were manually generated specific to the domain of interest. Statistical features studied included the ratio of linguistic features to total number of words. We built models using an iterative process and studied accuracy as well as coverage of each model. Study of different features is important in order to build a better model. Feature selection algorithms can choose the best features among the available ones but different features have costs associated with them. We need features that not only predict class labels or contribute towards prediction but the feature should also be representative of the entire dataset, especially test data. Training the classifier on such features will yield better coverage and training accuracy for the model. We compared the three different models generated by three different feature generation strategies. Our preliminary results indicate that seed-words that are specific to a particular domain or particular type of classification achieve better accuracy and coverage. In general, bag-of-words features are tightly coupled with the data they represent. On the other hand, statistical features are independent of the actual words used. Statistical features are more useful in building robust models that can be used with different languages and for different tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is the first time that the Information Retrieval Group at the University of Arkansas at Little Rock (UALR) has participated in the blog track competition. A dataset of over 3 million blog posts (known as permalinks) was provided in raw form. Along with permalinks, data was also provided as blog feeds and homepages. The task this year was to identify opinionated blog posts for the given target. The target could be an entity, organization, event, person or location. 50 queries were given as targets and participants were asked to submit top 1000 results for each target ranked by opinionated nature of the blog post. The dataset of over 3 million permalink documents consists of blogs as well as nonblogs (news articles etc.). The non-english blogs as well as blogs with offensive content were considered as not-opinionated. From the total of 3,215,171 permalink documents, we removed those which are empty or non-english documents. The remaining 2,806,645 permalink documents were parsed to obtain text documents.</p><p>We divided the task of identifying opinionated blog posts about the target into two components. The first component is an indexing and searching component that obtains all blog posts about the target sorted by their similarity score. We used normalized term frequency (TF) and Inverse Document Frequency (IDF) metric to note the weight of each unique term in the corpus. We obtained 3,691,472 unique words in 2,806,645 permalink documents. Such a high number of unique words is due to the addition of the large number of non-english and non-dictionary words. The words commonly used as the expression blog content such as aaarrgh! etc. are not dictionary words but they indicate the opinionated nature of the content (probably frustration in this example). We also did not use standard stop list available with SMART <ref type="bibr" coords="2,151.44,158.00,13.96,10.80" target="#b1">[1]</ref> system but rather, manually created the list of 271 words with only relevant stop words. Words like i, we are indicative of the subjective nature of the blog and hence were not discarded.</p><p>The second component of the system uses Machine Learning classification techniques to identify opinionated blog posts from not-opinionated posts. We considered using topic detection mechanisms such as the one used in <ref type="bibr" coords="2,444.72,227.00,12.87,10.80" target="#b2">[2]</ref>. Due to lack of time constraints, we assumed that the content of the entire blog post is about the given target if the word appears in the corresponding blog post. Figure <ref type="figure" coords="2,462.48,254.60,6.00,10.80" target="#fig_0">1</ref> shows the detailed description of the two components of our system. The Opinion Miner component is responsible for training the Support Vector Machine (SVM) classifier with linear kernels. It also predicts actual test data with respective probabilities. These probabilities are considered as scores (score-1) for the Opinion Miner component. The higher the score, the more opinionated is the nature of the blog post. The second component is exclusively for searching relevance of the query with blog post. This component provides relevance of each blog post as a score (score-2) and sorts them in descending manner. Ultimately, we integrated the scores from Search Component and scores from Opinion Miner to obtain final scores (score-1 * score-2) of all blogs sorted in ascending order. Top 1000 results were returned for each query submitted to the system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Component</head><p>We used open source lucene search engine <ref type="bibr" coords="3,313.08,91.40,14.08,10.80">[3]</ref> for indexing about 2.8 million text files. Each text file represents the content of the blog post. Lucene supports inverted index structures as well as stemmers (We used Porter stemmer <ref type="bibr" coords="3,388.56,119.00,13.62,10.80" target="#b4">[4]</ref>) and analyzers to index the documents. Standard normalized TF * IDF weighting was implemented after filtering the stop words. The index files were merged to faster access the index (8 GB size). For any given query, lucene can compute scores of all documents being returned as relevant to the query. All returned results are sorted in an ascending order by the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Opinion Miner Component</head><p>The Opinion Miner component is solely responsible for identifying opinionated blog posts. We use linear kernel SVM for training the classifier. As none of the blog posts provided were labeled, we wanted to use minimum training effort (including manual labeling of classes) and obtain maximum training accuracy. In order to accomplish this, we used the active learning paradigm <ref type="bibr" coords="3,325.68,300.68,12.87,10.80" target="#b5">[5]</ref>. We started with a small randomly selected training dataset of 20 blog posts and labeled classes as op or nop for opinionated and not-opinionated blog posts respectively. The samples were chosen such that we had 10 samples from op class and 10 from nop class. Next, we trained linear SVM with libSVM <ref type="bibr" coords="3,228.48,355.88,13.96,10.80" target="#b6">[6]</ref> software on the training set and built the model. Figure <ref type="figure" coords="3,516.00,355.88,6.00,10.80">2</ref> explains active learning process in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure2: Integrated active learning approach</head><p>We also modified the source code for libSVM to predict absolute distance of all test data samples from the generated decision boundary. We sorted all test data according to increasing distance from the decision boundary. This helps us identify those points that are closest to the boundary and thus most difficult for the model to predict. We selected 20 such test data points and labeled them. Now with the new training dataset of 40 samples we continued the same procedure. We noted the training accuracy until we obtained satisfactory training phase. We used 10 fold cross validation to determine training set accuracy. Adding new samples iteratively; increases the coverage of the model built using this approach. We define the coverage as the number of blog posts that can be predicted by the generated model. The higher the coverage of the model, the better it represents the actual test data. We used active learning approach and generated different models with different set of features. We considered 3 different set of features to represent the feature space. In the first approach, we used simple bag-of-words approach to obtain all the words in the training dataset and used Information Gain feature selection technique to rank all features according to their values. We selected top 260 features as the best representative of the entire feature space. One of the important considerations for using feature selection was processing time. By limiting the number of features to 260, we were able to process blogs and generate better models and coverage. We submitted automatic [UALR06a260r2] and manual [UALR06m260r3] runs obtained using Information Gain feature selection method.</p><p>We could not fully integrate feature selection as well into the active learning process due to lack of time. Using just feature selection integrated with active learning <ref type="bibr" coords="4,104.40,365.00,13.96,10.80" target="#b7">[7]</ref> does not produce expected increase in accuracy. This is because the features selected in that particular iteration ranked by Information Gain or any other feature selection technique may not be sufficient to truly represent the test data. Instead, integrating manual feature selection with active learning exhibited to have better results. Hence we used the second set of features totally independent of training set. We compiled a list of 500 adjectives, adverbs and few verbs that we think commonly indicate the opinionated nature of blogs. Words that indicate subjectivity or opinion are good candidates to be in this list. We used this manual seed word list as a feature set and submitted automatic [UALR06a500r4] as well as manual [UALR06m500r5] Runs. Table <ref type="table" coords="4,165.36,489.20,6.00,10.80" target="#tab_0">1</ref> shows the comparison of all runs submitted to the TREC.</p><p>As a third type of feature, we used simple statistical features. We counted the number of adjectives, adverbs, ratio of sum of adjectives and adverbs to total number of words for each blog post. We also added features such as ratio of the number of adjectives in the particular blog post to the highest number of adjectives found in any of the corpus blog posts etc. We used such 5 features and generated the naïve statistical model. The hypothesis for using purely frequency based or statistical features was that presence of adjectives and adverbs may be indicative of the opinion in the blog post. As can be seen from Table <ref type="table" coords="4,330.84,599.60,4.50,10.80" target="#tab_0">1</ref>, this method did not yield very high training accuracy. We submitted only manual run [UALR06m5Tr1] as the fifth run for the competition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiments and Results</head><p>Table <ref type="table" coords="5,138.60,281.24,6.00,10.80" target="#tab_0">1</ref> details the 5 runs submitted this year. Our highest priority runs were UALR06a260r2 and UALR06a500r4 with 260 and 500 features respectively. Figure <ref type="figure" coords="5,516.00,295.04,6.00,10.80" target="#fig_0">1</ref> shows the interpolated precision recall response comparison for all 5 runs. It should be noted that though all runs have almost similar precision-recall response, automatic run with active learning and 260 features [UALR06a260r2] has a little better values than the other runs for interpolated precision at different levels of recall. In the future, we would like to improve active learning methods by incorporating feature selection. Preliminary work done by Raghavan et. al. <ref type="bibr" coords="6,403.44,430.64,14.08,10.80" target="#b7">[7]</ref> indicates that feature selection integrated with active learning does not always yield increasing accuracy especially with text data. We would like to investigate reasons for decrease in performance and improve the active learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolated Precision</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,104.40,727.42,415.19,12.40;2,104.40,417.60,413.88,293.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall view of the system with Search and Opinion Miner components</figDesc><graphic coords="2,104.40,417.60,413.88,293.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,156.00,650.62,314.39,12.40"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Interpolated Precision Recall response of the 5 runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,104.40,380.88,414.00,294.12"><head></head><label></label><figDesc></figDesc><graphic coords="3,104.40,380.88,414.00,294.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,104.40,47.74,396.78,179.75"><head>Table 1 : 5 Runs submitted to TREC: Blog Track</head><label>1</label><figDesc></figDesc><table coords="5,112.56,66.68,388.62,160.80"><row><cell></cell><cell>Run</cell><cell>Type</cell><cell>No. of Features</cell><cell>Coverage</cell><cell>Training Accuracy *</cell></row><row><cell>1</cell><cell>UALR06m5Tr1</cell><cell>Manual</cell><cell>5</cell><cell>2806645</cell><cell>48.21%</cell></row><row><cell>2</cell><cell cols="2">UALR06a260r2 Automatic</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>260</cell><cell>2223295</cell><cell>74.33%</cell></row><row><cell>3</cell><cell>UALR06m260r3</cell><cell>Manual</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell cols="2">UALR06a500r4 Automatic</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>500</cell><cell>2711173</cell><cell>80.67%</cell></row><row><cell>5</cell><cell>UALR06m500r5</cell><cell>Manual</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,127.80,497.64,82.56,13.58" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,121.44,529.52,391.18,11.44;6,104.40,543.32,200.20,10.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,184.68,529.70,272.69,11.26">Implementation of the smart information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<idno>85-686</idno>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="6,121.44,557.12,375.17,11.44;6,104.40,570.92,406.44,11.44" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,290.52,557.30,206.09,11.26;6,104.40,571.10,105.11,11.26">Retrieving Topical Sentiments from Online Document Collections</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,230.40,570.92,192.20,10.80">Document Recognition and Retrieval XI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,121.44,584.72,301.66,10.80;6,104.40,598.52,115.32,10.80" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Apache</forename><surname>Jakarta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucene</forename></persName>
		</author>
		<ptr target="http://lucene.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,121.44,612.32,368.37,10.80" xml:id="b4">
	<monogr>
		<ptr target="http://www.tartarus.org/~martin/PorterStemmer/" />
		<title level="m" coord="6,121.44,612.32,127.38,10.80">Porter stemming algorithm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,121.44,626.12,393.00,11.44;6,104.40,639.92,381.84,11.44;6,104.40,653.72,118.20,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,216.60,626.30,297.84,11.26;6,104.40,640.10,88.82,11.26">Support Vector Machine Active Learning with Applications to Text Classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,200.76,639.92,285.48,10.80;6,104.40,653.72,86.07,10.80">Proceedings of ICML-00, 17th International Conference on Machine Learning</title>
		<meeting>ICML-00, 17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,121.44,667.52,372.59,11.44;6,104.40,681.32,382.78,11.44" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,307.44,667.52,186.59,11.44;6,104.40,681.50,42.88,11.26">LIBSVM : a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,121.44,695.12,372.60,11.44;6,104.40,708.92,410.52,11.44" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,303.12,695.30,190.92,11.26;6,104.40,709.10,111.83,11.26">Active Learning with Feedback on Both Features and Instances</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,225.36,708.92,184.27,10.80">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1655" to="1686" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
