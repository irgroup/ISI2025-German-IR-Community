<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.20,71.96,313.06,16.59;1,54.84,91.88,499.50,16.59">University of Glasgow at TREC 2006: Experiments in Terabyte and Enterprise Tracks with Terrier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,98.64,132.19,75.32,16.49"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.55,132.19,80.73,16.49"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.84,132.19,89.78,16.49"><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
							<email>vassilis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.65,132.19,37.66,16.49"><forename type="first">Jie</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.80,132.19,33.44,16.49"><forename type="first">Ben</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,457.87,132.19,52.85,16.49"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.20,71.96,313.06,16.59;1,54.84,91.88,499.50,16.59">University of Glasgow at TREC 2006: Experiments in Terabyte and Enterprise Tracks with Terrier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2EDA1BF33BA2846FBC1CFD51284F22CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2006, we participate in three tasks of the Terabyte and Enterprise tracks. We continue experiments using Terrier 1 , our modular and scalable Information Retrieval (IR) platform. Furthering our research into the Divergence From Randomness (DFR) framework of weighting models, we introduce two new effective and low-cost models, which combine evidence from document structure and capture term dependence and proximity, respectively. Additionally, in the Terabyte track, we improve on our query expansion mechanism on fields, presented in TREC 2005, with a new and more refined technique, which combines evidence in a linear, rather than uniform, way. We also introduce a novel, low-cost syntacticallybased noise reduction technique, which we flexibly apply to both the queries and the index. Furthermore, in the Named Page Finding task, we present a new technique for combining query-independent evidence, in the form of prior probabilities. In the Enterprise track, we test our new voting model for expert search. Our experiments focus on the need for candidate length normalisation, and on how retrieval performance can be enhanced by applying retrieval techniques to the underlying ranking of documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The research scope underlying our participation in TREC 2006 has been to extend our current robust weighting models and retrieval performance enhancing techniques, in novel ways that are theoretically-sound, modular, low-cost, and most importantly, effective. In terms of weighting models, we present two new Divergence From Randomness (DFR) models. The first model aims at combining evidence from document structure, and we test it in the Named Page Finding task of the Terabyte track. The second model aims at modelling term dependence and proximity, and we test it in the Named Page Finding task of the Terabyte track and the Expert Search task of the Enterprise track. In terms of retrieval performance enhancing techniques, we present (i) a refined query expansion mechanism on fields, which combines document field evidence in a linear way, and (ii) a novel noise reduction mechanism for long queries and the index, which uses syntactically-based evidence (parts of speech). We test these two techniques in the Adhoc task of the Terabyte track. We also present a new technique for combining query-independent evidence, in the form of prior probabilities. We test this technique in the Named Page Finding task of the Terabyte track.</p><p>In the Enterprise track, we test our novel voting model for expert search. Firstly, we experiment on how candidate length normalisation can be used in the voting model to prevent candidates with too much expertise evidence from gaining an unfair advantage in the voting model. Secondly, we examine how a selection of state-ofthe-art retrieval techniques, such as a field-based weighting model, query expansion and term dependence and proximity, can be used to enhance the retrieval performance of the expert search system, by enhancing the quality of an underlying ranking of documents. Conclusions are drawn across two ways of associating documents with candidates to represent their expertise.</p><p>The remainder of this paper is organised as follows. Section 2 presents the weighting models used in the Terabyte and the Enterprise tracks. Section 3 presents the hypotheses tested and techniques applied in the Adhoc and Named Page Finding tasks of the Terabyte track, with a discussion of the results. Section 4 presents the hypotheses tested and techniques applied in the Enterprise track, with a discussion of the results. Section 5 summarises our overall participation in TREC 2006.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODELS</head><p>Following from previous years, our research in Terrier centres in extending the Divergence From Randomness framework (DFR) <ref type="bibr" coords="1,543.20,495.83,9.53,12.36" target="#b1">[1]</ref>. In TREC 2006, we have devised novel, information-theoretic ways of combining evidence from document structure (or fields, such as the title and anchor text), and in modelling term dependence and proximity. Both proposed models are based on the DFR framework, and they are applied very effectively and with little computational overhead.</p><p>The remainder of this section is organised as follows. Section 2.1 presents existing field-based DFR weighting models. Section 2.2 introduces our new field-based DFR weighting model, while Section 2.3 presents our new DFR model, which captures term dependence and proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Field-based Divergence From Randomness (DFR) Weighting Models</head><p>Document structure (or fields), such as the title and the anchor text of incoming hyperlinks, have been shown to be effective in Web IR <ref type="bibr" coords="1,348.58,676.07,9.53,12.36" target="#b4">[4]</ref>. Robertson et al. <ref type="bibr" coords="1,429.05,676.07,14.87,12.36" target="#b24">[23]</ref> observed that the linear combination of scores, which has been the approach mostly used for the combination of fields, is difficult to interpret due to the nonlinear relation between the scores and the term frequencies in each of the fields. In addition, Hawking et al. <ref type="bibr" coords="2,199.35,53.87,10.43,12.36" target="#b5">[5]</ref> showed that the length normalisation that should be applied to each field depends on the nature of the field. Zaragoza et al. <ref type="bibr" coords="2,174.74,74.87,14.87,12.36" target="#b26">[25]</ref> introduced a field-based version of BM25, called BM25F, which applies length normalisation and weighting of the fields independently. Macdonald et al. <ref type="bibr" coords="2,277.79,95.75,14.87,12.36" target="#b11">[11]</ref> also introduced Normalisation 2F in the DFR framework for performing independent term frequency normalisation and weighting of fields.</p><p>In this work, we use two field-based models from the DFR framework, namely PL2F and InL2F. Using the PL2F model, the relevance score of a document d for a query Q is given by:</p><formula xml:id="formula_0" coords="2,57.36,178.08,235.55,24.26">score(d, Q) = X t∈Q qtw • 1 tf n + 1 `tf n • log 2 tf n λ<label>(1)</label></formula><p>+(λ -tf n) • log 2 e + 0.5 • log 2 (2π • tf n) ẃhere λ is the mean and variance of a Poisson distribution, given by λ = F/N ; F is the frequency of the query term t in the whole collection, and N is the number of documents in the whole collection. The query term weight qtw is given by qtf /qtfmax; qtf is the query term frequency; qtfmax is the maximum query term frequency among the query terms.</p><p>For InL2F, the relevance score of a document d for a query Q is given by:</p><formula xml:id="formula_1" coords="2,60.72,315.48,224.93,24.26">score(d, Q) = X t∈Q qtw • 1 tf n + 1 `tf n • log 2 N + 1 nt + 0.5 ´(<label>2</label></formula><formula xml:id="formula_2" coords="2,285.65,320.35,7.26,2.65">)</formula><p>where nt is the number of documents term t occurs in.</p><p>In both PL2F and InL2F, tf n corresponds to the weighted sum of the normalised term frequencies tf f for each used field f , known as Normalisation 2F <ref type="bibr" coords="2,130.05,376.55,13.91,12.36" target="#b11">[11]</ref>:</p><formula xml:id="formula_3" coords="2,61.68,395.95,231.21,24.91">tf n = X f " w f • tf f • log 2 (1 + c f • avg l f l f ) « , (c f &gt; 0) (3)</formula><p>where tf f is the frequency of term t in field f of document d; l f is the length in tokens of field f in document d, and avg l f is the average length of the field across all documents; c f is a hyperparameter for each field, which controls the term frequency normalisation; the importance of the term occurring in field f is controlled by the weight w f . Note that the classical DFR weighting models PL2 and InL2 can be generated by using Normalisation 2 instead of Normalisation 2F for tf n in Equations ( <ref type="formula" coords="2,133.87,510.11,3.48,12.36" target="#formula_0">1</ref>) &amp; (2) above. Normalisation 2 is given by:</p><formula xml:id="formula_4" coords="2,102.00,529.68,187.40,20.85">tf n = tf • log 2 (1 + c • avg l l )(c &gt; 0) (<label>4</label></formula><formula xml:id="formula_5" coords="2,289.40,532.31,3.48,12.36">)</formula><p>where tf is the frequency of term t in the document d; l is the length of the document in tokens, and avg l is the average length of all documents; c is a hyper-parameter that controls the normalisation applied to the term frequency with respect to the document length.</p><p>Note that, following <ref type="bibr" coords="2,136.52,605.15,13.71,12.36" target="#b24">[23]</ref>, we have also devised a simplified variant of Normalisation 2F, which normalises the sum of the weighted term frequencies from different fields, instead of normalising the term frequencies on a per-field basis. Indeed, this simplified variant of Normalisation 2F allows us to reduce training time, because it has less hyper-parameters to train. The simplified Normalisation 2F, which we denote as Normalisation 2FS, is given as follows:</p><formula xml:id="formula_6" coords="2,70.92,696.00,221.96,24.38">tf n = X f `wf • tf f • log 2 (1 + c • avg l l ) ´, (c &gt; 0) (5)</formula><p>Normalisation 2F in Equation ( <ref type="formula" coords="2,427.50,53.87,3.48,12.36">3</ref>) has a hyper-parameter c f for each indexed document field. Unlike Normalisation 2F, Normalisation 2FS has only a single hyper-parameter c for all the indexed document fields. Therefore, we can benefit from having less hyperparameters to train. In our previous experiments for Adhoc retrieval, we found no significant difference between the retrieval performance obtained using PL2F and PL2FS. For example, for the TREC-9 Web Adhoc task, using title-only queries, the optimised mean average precision (MAP) of PL2F and PL2FS is 0.2071 and 0.2062, respectively. The p-value is 0.07858 using the Wilcoxon matched-pairs signed-ranks test, which indicates an insignificant difference at 5% confidence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multinomial Divergence From Randomness (DFR) Weighting Model</head><p>In TREC 2006, we re-investigate the use of document structure (or fields) in the DFR framework. In both BM25F and the DFR models that employ Normalisation 2F (e.g. PL2F, InL2F), it is assumed that the occurrences of terms in the fields follow the same distribution, because the combination of fields takes place in the document length normalisation component, and not in the probabilistic model <ref type="bibr" coords="2,368.14,279.35,13.71,12.36" target="#b19">[18]</ref>.</p><p>In TREC 2006, we take a different approach by considering that the term occurrences in the fields of documents follow a multinomial distribution. In this way, the combination of the term occurrences from the different fields is modelled in a probabilistic way, and is not part of the document length normalisation.</p><p>We introduce a new DFR weighting model, which employs a multinomial randomness model, as follows. The weight of a term in a document (score(d, t)) is equal to the product of the information content of two probabilities. Therefore, the relevance score of a document d for a query Q is computed as follows:</p><formula xml:id="formula_7" coords="2,337.92,407.59,218.03,46.75">score(d, Q) = X t∈Q qtw • score(d, t) = X t∈Q `qtw(-log 2 (P1)) • (1 -P2) ´(6)</formula><p>P1 corresponds to the probability that there is a given number of term occurrences in the fields of a document. P2 corresponds to the probability of having one more occurrence of a term in a document, after having seen it a given number of times. The probability P1 is computed using a multinomial randomness model:</p><formula xml:id="formula_8" coords="2,350.16,527.83,205.79,44.23">P1 = F tf n1 tf n2 . . . tf n k tf n ! •p tf n 1 1 • p tf n 2 2 • • • p tf n k k • p tf n (7)</formula><p>The probability P2 is computed using the Laplace after-effect model.</p><formula xml:id="formula_9" coords="2,398.52,601.99,157.43,24.55">P2 = P f tf n f 1 + P f tf n f<label>(8)</label></formula><p>In the above equations, k is the number of fields, tf n f is the normalised frequency of a term in the field f , which is given by applying Normalisation 2 from Equation (4) to that field. F and N are as defined in Section 2.1. tf n = F -P f tf n f ; p f is the prior probability of having a term occurrence in the field f of a document, and it is equal to</p><formula xml:id="formula_10" coords="2,378.33,686.51,135.27,14.18">p f = 1 k•N ; p = 1 - P f p f = N -1 N .</formula><p>The final score of a document d for a query Q is computed as follows:</p><formula xml:id="formula_11" coords="3,66.96,69.84,340.08,69.42">score(d, Q) = X t∈Q " qtw 1 + P f tf n f • " -log 2 (F !) + X f `log 2 (tf n f !) -tf n f log 2 (p f ) + log 2 (tf n !) -tf n log 2 (p ) ""<label>(9)</label></formula><p>We refer to the multinomial DFR model described in Equation ( <ref type="formula" coords="3,287.14,144.95,3.48,12.36" target="#formula_11">9</ref>) as ML2. In the above equation, the logarithm of the factorial is computed using the Lanzcos approximation of the Γ function <ref type="bibr" coords="3,278.63,165.83,14.15,12.36;3,53.76,176.27,21.59,12.36">[21, p. 213</ref>]. The Lanzcos approximation is preferred over the Stirling approximation because it results in lower error <ref type="bibr" coords="3,223.08,186.71,13.71,12.36" target="#b19">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Term Dependence in the Divergence From Randomness (DFR) Framework</head><p>We believe that taking into account the dependence and proximity of query terms in documents can increase retrieval effectiveness. To this end, we extend the DFR framework with models for capturing the dependence of query terms in documents. Following <ref type="bibr" coords="3,280.07,261.71,9.53,12.36" target="#b2">[2]</ref>, the models are based on the occurrences of pairs of query terms that appear within a given number of terms of each other in the document. The introduced weighting models assign scores to pairs of query terms, in addition to the single query terms.</p><p>The score of a document d for a query Q is given as follows:</p><formula xml:id="formula_12" coords="3,62.04,332.76,230.70,22.73">score(d, Q) = X t∈Q qtw • score(d, t) + X p∈Q 2 score(d, p)<label>(10)</label></formula><p>where score(d, t) is the score assigned to a query term t in the document d; p corresponds to a pair of query terms; Q2 is the set that contains all the possible combinations of two query terms. In Equation <ref type="bibr" coords="3,88.54,390.84,13.71,12.36" target="#b10">(10)</ref>, the score P t∈Q qtw • score(d, t) can be estimated by any DFR weighting model, with or without fields. The weight score(d, p) of a pair of query terms in a document is computed as follows:</p><formula xml:id="formula_13" coords="3,101.52,438.59,191.26,13.62">score(d, p) = -log 2 (Pp1) • (1 -Pp2)<label>(11)</label></formula><p>where Pp1 corresponds to the probability that there is a document in which a pair of query terms p occurs a given number of times. Pp1 can be computed with any randomness model from the DFR framework, such as the Poisson approximation to the Binomial distribution. Pp2 corresponds to the probability of seeing the query term pair once more, after having seen it a given number of times. Pp2 can be computed using any of the after-effect models in the DFR framework. The difference between score(d, p) and score(d, t) is that the former depends on counts of occurrences of the pair of query terms p, while the latter depends on counts of occurrences of the query term t.</p><p>For example, applying term dependence and proximity with the weighting model InL2 (see Equations ( <ref type="formula" coords="3,194.55,580.55,3.48,12.36" target="#formula_1">2</ref>) and ( <ref type="formula" coords="3,222.15,580.55,3.14,12.36" target="#formula_4">4</ref>)), results in a new version of InL2, which we denote by pInL2, where the prefix p stands for proximity. pInL2 estimates score(d, p) as follows:</p><formula xml:id="formula_14" coords="3,84.36,620.40,172.42,20.85">score(d, p) = 1 tf np + 1 `tf np • log 2 N + 1 np + 0.5</formula><p>´ <ref type="bibr" coords="3,265.05,625.27,27.73,2.65" target="#b12">(12)</ref> where np corresponds to the number of documents in which the pair of query terms p appear within dist terms of each other. tf np is the normalised frequency of a query term pair p in document d, which can be obtained from applying Normalisation 2 from Equation <ref type="bibr" coords="3,69.96,686.51,9.53,12.36" target="#b4">(4)</ref>. A different randomness model, which does not consider the collection frequency of pairs of query terms, is based on the binomial randomness model, and computes the score of a pair of query terms in a document as follows:</p><formula xml:id="formula_15" coords="3,324.00,81.24,231.82,48.66">score(d, p) = 1 tf np + 1 • " -log 2 (l -1)! + log 2 tf np! + log 2 (l -1 -tf np)! -tfp log 2 (pp)<label>(13)</label></formula><formula xml:id="formula_16" coords="3,436.44,132.31,111.23,14.15">-(l -1 -tf np) log 2 (p p ) "</formula><p>where pp = 1 l-1 and p p = 1 -pp. We refer to this binomial DFR model described in Equation (13) as pBiL2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TERABYTE TRACK</head><p>In the TREC 2006 Terabyte Track, we participate in the Adhoc and Named Page Finding tasks.</p><p>We index the .GOV2<ref type="foot" coords="3,399.24,223.36,2.99,8.24" target="#foot_1">2</ref> collection using Terrier <ref type="bibr" coords="3,488.82,223.91,13.71,12.36" target="#b16">[16]</ref>, in seven parts (each part having an average size of 3.6 million documents). To support our investigation into the use of document field evidence in retrieval, each of the seven parts consists of three inverted files, one for each of the following document fields: body, title, and anchor text. Standard stopwords are removed from each index. We apply Porter's full stemming for our Adhoc experiments, and Porter's weak stemming for our Named Page Finding experiments. Our choice of stemming is justified by the observation that weak stemming, being less aggressive than full stemming, is better suited for high-precision tasks, such as Named Page Finding.</p><p>Following our experiments in the TREC 2005 Terabyte track <ref type="bibr" coords="3,316.80,349.43,13.71,12.36" target="#b10">[10]</ref>, we use a distributed version of Terrier to reduce up retrieval time. In TREC 2006, we use one broker, and seven query servers, each serving one index part. Moreover, a global lexicon is created in order to speed up the retrieval process, particularly for query expansion.</p><p>In the Adhoc task, we adopt a dual approach that generally aims to boost query informativeness on one hand, and reduce noise on the other hand. We boost query informativeness by incorporating different combinations of document field evidence into our query expansion mechanism. We reduce noise using part-of-speech evidence. Specifically, we investigate the following hypotheses: H1 For the query expansion mechanism on fields, the linear combination of fields can provide a better retrieval performance, than the uniform combination of fields (Section 3.1.1).</p><p>H2 In a collection of documents, low frequency part-of-speech n-grams correspond to noisy sequences of words, which if removed, can enhance retrieval performance (Section 3.1.2).</p><p>In the Named Page Finding task, we investigate a new way of modelling term occurrence in document fields, and a novel theoretically-founded approach for combining multiple sources of query independent evidence. More specifically, we test the following hypotheses:</p><p>H3 Modelling the distribution of term occurrences in document fields as a multinomial distribution is a theoretically-sound and robust approach, which performs at least comparably to other field-based weighting models (Section 2.2).</p><p>H4 Modelling the dependence and proximity of query terms in documents can enhance retrieval effectiveness (Section 2.3).</p><p>H5 Using the conditional combination of multiple sources of query independent evidence, in the form of prior probabilities, can improve retrieval performance over using either source of evidence alone (Section 3.2.1).</p><p>The remainder of Section 3 is organised as follows. Section 3.1 presents the linear combination of fields for query expansion (Section 3.1.1), and syntactically-based noise reduction (Section 3.1.2). Section 3.1.3 presents our Adhoc experiments. Section 3.2 presents our participation in the TREC 2006 Named Page Finding task, with an introduction of the techniques tested in Section 3.2.1, and a discussion of the experiments in Section 3.2.2. Section 3.3 summarises our participation in the TREC 2006 Terabyte Track, with conclusions and lessons learnt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adhoc Task</head><p>In TREC 2006 we extend our Terrier retrieval platform and implement two retrieval performance enhancing techniques, namely (i) query expansion, which combines document fields in a linear way, and (ii) syntactically-based noise reduction, which is applied to long queries and the index. We experiment with short (Title), and long (Title + Description + Narrative) queries, and report on our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Query Expansion on Document Fields</head><p>Continuing our experimentation in the TREC 2005 Terabyte Adhoc task, we aim to further improve our query expansion mechanism on document fields, by appropriately combining field evidence available in corpora (hypothesis H1, Section 3). This finegrained query expansion mechanism uses statistics from various document fields, such as the title, the anchor text of the incoming links, and the body of documents. In TREC 2005, we applied a uniform combination of evidence from different document fields (QEFU) <ref type="bibr" coords="4,85.17,397.07,13.71,12.36" target="#b10">[10]</ref>. In TREC 2006, we replace this uniform combination with a more refined linear combination of evidence from different weighted fields (QEFL).</p><p>Our query expansion mechanism on document fields is built on top of the Bo1 term weighting model <ref type="bibr" coords="4,195.65,438.95,9.53,12.36" target="#b1">[1]</ref>, which is based on the Bose-Einstein statistics. Using this model, the weight of a term t in the exp doc top-ranked documents is given by:</p><formula xml:id="formula_17" coords="4,94.92,486.24,197.86,20.85">w(t) = tfx • log 2 1 + Pn Pn + log 2 (1 + Pn)<label>(14)</label></formula><p>where exp doc usually ranges from 3 to 10 <ref type="bibr" coords="4,210.01,508.79,9.53,12.36" target="#b1">[1]</ref>. Another parameter involved in the query expansion mechanism is exp term, the number of terms extracted from the exp doc top-ranked documents.</p><p>exp term is usually larger than exp doc <ref type="bibr" coords="4,202.68,540.23,9.53,12.36" target="#b1">[1]</ref>. Pn is given by F N ; F is the frequency of the term in the collection, and N is the number of documents in the collection; tfx is the frequency of the query term in the exp doc top-ranked documents.</p><p>We extend the above Bo1 term weighting model to deal with document fields by a linear combination of the frequencies of the query term in different fields:</p><formula xml:id="formula_18" coords="4,133.92,629.51,158.87,21.62">tfx = X f wq f • tf xf<label>(15)</label></formula><p>We call the term weighting model in Equation ( <ref type="formula" coords="4,229.89,655.19,6.86,12.36" target="#formula_17">14</ref>), where tfx is given by Equation ( <ref type="formula" coords="4,125.20,665.63,6.86,12.36" target="#formula_18">15</ref>), the Bo1F term weighting model. In Equation <ref type="bibr" coords="4,70.08,676.07,13.71,12.36" target="#b15">(15)</ref>, wq f is the weight of a field f in the exp doc top-ranked documents, which reflects the relative importance of the associated field in the top-ranked documents. tf xf is the frequency of the query term in field f of the exp doc top-ranked documents.</p><p>Terrier employs a parameter-free function to determine qtw, the query term weight of a query term, which is given as follows:</p><formula xml:id="formula_19" coords="4,334.08,91.56,221.75,45.09">qtw = qtf qtfmax + w(t) lim F →tfx w(t) (16) = Fmax log 2 1 + Pn,max Pn,max + log 2 (1 + Pn,max)</formula><p>where qtf is the query term frequency of term t, and qtfmax is the maximum qtf among all the query terms in the expanded query;</p><p>lim F →tfx w(t) is the upper bound of w(t); Pn,max is given by F max/N , where F max is the F value of the term with the maximum w(t) in the exp doc top-ranked documents. If a query term does not appear in the most informative terms from the top-ranked documents, its query term weight remains equal to the original one.</p><p>The above formula is parameter-free in the sense that the parameter in Rocchio's query expansion function <ref type="bibr" coords="4,457.36,223.43,14.87,12.36" target="#b25">[24]</ref> has been omitted. Using a field-based weighting model, e.g. PL2F (Equations ( <ref type="formula" coords="4,548.87,233.87,3.48,12.36" target="#formula_0">1</ref>) and ( <ref type="formula" coords="4,335.65,244.31,3.14,12.36">3</ref>)), together with Bo1F, there are six field weights involved, namely the weights (w f ) of the three document fields in the weighting model, and the weights (wq f ) of the three document fields in Bo1F. Since it would be very time-consuming to optimise all of these six field weights, we make the following assumptions to reduce the number of field weights to two:</p><p>1. For a given field f , we assume that w f = wq f . This is reasonable because the weight of a field reflects the contribution of the field to the document ranking, which should be consistent in both retrieval and query expansion.</p><p>2. Following <ref type="bibr" coords="4,377.98,364.31,14.87,12.36" target="#b10">[10]</ref> and <ref type="bibr" coords="4,410.36,364.31,14.87,12.36" target="#b24">[23]</ref> , we set the weight of the body field to 1.</p><p>By making the above two assumptions, we reduce the number of field weights from six to two, namely the weights of the anchor text and title fields. In addition, we apply the simplified Normalisation 2FS in Equation ( <ref type="formula" coords="4,381.75,425.15,3.18,12.36">5</ref>), instead of Normalisation 2F in Equation (3), so that we have only one c hyper-parameter.</p><p>In order to train the hyper-parameter c, the field weights, and the parameters exp doc and exp term, we adopt two different training strategies. The first training strategy (T1) optimises the parameters over all the 100 old topics used in the TREC 2004 and 2005 Terabyte Adhoc tasks. The parameter values that give the best MAP are used. The second training strategy (T2) splits these 100 old topics into two parts. Each part consists of the 50 topics used in the TREC 2004 or 2005 Terabyte Adhoc task. T2 optimises the parameters over each of the two parts of the old topics. The average of the optimised parameter values for the two parts of the old topics is used. We expect T2 to result in a better retrieval performance than T1 because T2 prevents the training process from being biased towards the set of topics that performs better. Indeed, on the TREC 2005 topics it is easier to achieve high retrieval performance than for the TREC 2004 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Syntactically-based Noise Reduction</head><p>This section describes our technique for reducing estimated noise from long queries and documents. We use part-of-speech (POS) ngrams <ref type="bibr" coords="4,340.90,644.75,9.71,12.36" target="#b3">[3,</ref><ref type="bibr" coords="4,352.89,644.75,7.43,12.36" target="#b7">7]</ref> to detect noise in text.</p><p>POS n-grams are n-grams (or blocks) of parts of speech, which are extracted from a POS-tagged sentence in a recurrent and overlapping way. For example, for a sentence ABCDEFG, where parts of speech are denoted by the single letters A, B, C, D, E, F, G, and where POS n-gram length l = 4, the POS n-grams extracted are ABCD, BCDE, CDEF, and DEFG. The order in which the POS n-grams occur in the sentence is ignored. For each sentence, all possible POS n-grams are extracted.</p><p>Our technique is based on the fact that high-frequency POS ngrams correspond mostly to sequences of words that include relatively little noise, whereas low-frequency POS n-grams correspond mostly to sequences of words that include relatively more noise <ref type="bibr" coords="5,280.06,257.63,9.53,12.36" target="#b7">[7]</ref>. To test the hypothesis that reducing noise from text using POS ngrams can enhance retrieval performance (H2, Section 3), firstly we reduce estimated noise from long queries in order to enhance retrieval performance by providing more informative queries <ref type="bibr" coords="5,280.04,299.39,9.53,12.36" target="#b9">[9]</ref>. We refer to this as N Rq. Secondly, we reduce estimated noise from the collection before it is indexed, in order to improve retrieval precision, at no detrimental cost to retrieval recall <ref type="bibr" coords="5,222.12,330.83,9.53,12.36" target="#b8">[8]</ref>. We refer to this as N Ri. The only resources needed are a POS tagger and a collection of documents. This can be any collection of documents of a reasonable size <ref type="bibr" coords="5,116.83,362.15,9.53,12.36" target="#b9">[9]</ref>, not necessarily the collection from which we retrieve relevant documents.</p><p>Our methodology is as follows. We extract POS n-grams from a collection of documents and count their frequency. We refer to these POS n-grams as global POS n-grams. We rank these global POS n-grams according to their frequency in the collection (in decreasing order). We refer to this ranked list as global list. We empirically set a cutoff threshold θ of POS n-gram rank in the global list and we assume that everything below this threshold corresponds to estimated noise (Figure <ref type="figure" coords="5,152.58,456.35,3.24,12.36" target="#fig_0">1</ref>). We extract POS n-grams from the text we wish to process, i.e. a long query (for N Rq), or a document from the collection to be indexed (for N Ri). For each POS n-gram drawn from the text, we determine its position in the global list. Whenever this rank is below the threshold, we remove the POS n-gram and its corresponding sequence of words from the query or the document, regardless of any other POS n-grams that overlap it. In N Rq, we reduce estimated noise from long queries in two ways: firstly, uniformly for all queries (N RqU ); and secondly, in-dividually on a per query basis (N RqL). For N RqU , we use the same threshold θ for all queries. For N RqL, we use different values of θ according to query length. The intuition behind varying noise reduction according to query length is that the shorter the query, the less noise it is likely to contain. The values of θ according to different query lengths used are displayed in Table <ref type="table" coords="5,522.95,106.19,3.36,12.36" target="#tab_0">1</ref>.</p><p>In N Ri we reduce estimated noise from the index from which relevant documents are retrieved. Again, we set the threshold θ, so that everything below θ is considered noisy and removed. We remove POS n-grams in a uniform way, i.e. by setting θ to the same value for all documents (Table <ref type="table" coords="5,448.71,158.51,3.24,12.36" target="#tab_0">1</ref>).</p><p>After noise has been reduced using either of the noise reduction techniques described above, we treat the query or index as we would normally treat them. We use the TreeTagger<ref type="foot" coords="5,505.56,189.40,2.99,8.24" target="#foot_2">3</ref> for the POS tagging of WT10G and .GOV2. The POS n-grams extracted from these collections provide us with two separate global lists of POS n-grams. Overall we extract 25,070 POS n-grams from WT10G and 47,018 POS n-grams from .GOV2. We use the POS n-grams extracted from WT10G or .GOV2 to reduce noise from the queries, and the POS n-grams extracted from WT10G to reduce noise from the index of .GOV2. We note that there is not much difference in the POS n-gram ranking between the two collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Experiments and Results</head><p>We submitted five runs to the Adhoc task. The first two submitted runs test the query expansion mechanism on fields with two different training strategies, respectively (as described in Section 3.1.1). The third submitted run tests the query expansion mechanism on fields with the first training strategy (T1), as well as noise reduction from long queries. The last two submitted runs test the query expansion mechanism on the body of documents only, with noise reduction from long queries and the index. Our collective submitted runs, and their salient features, are summarised in Table <ref type="table" coords="5,532.28,388.31,3.36,12.36" target="#tab_1">2</ref>. The parameter values used in our submitted runs are given in Table <ref type="table" coords="5,549.09,398.75,3.36,12.36">9</ref>. A full description of the submitted runs follows.</p><p>• uogTB06QET1 uses the PL2FS weighting model with the simplified Normalisation 2FS; applies query expansion on fields (QEFL) using the Bo1F term weighting model, with training method T1, on short queries.</p><p>• uogTB06QET2 uses the PL2FS weighting model with the simplified Normalisation 2FS; applies query expansion on fields (QEFL) using the Bo1F term weighting model, with training method T2, on short queries.</p><p>• uogTB06S50L uses the PL2FS weighting model with the simplified Normalisation 2FS; applies query expansion on fields (QEFL) using the Bo1F term weighting model, with training method T1, on long queries; applies uniform noise reduction from the queries (N RqU ), with POS n-grams drawn from WT10G, and θ = 50.</p><p>• uogTB06SS10L uses the PL2 weighting model with Normalisation 2; applies query expansion on the documents (QE) using the Bo1 term weighting model, on long queries; applies uniform noise reduction from the queries (N RqU ), with POS n-grams drawn from .GOV2, and θ = 10; applies noise reduction in the index (N Ri), with POS n-grams drawn from WT10G, and θ = 17, 070.</p><p>• uogTB06SSQL uses the PL2 weighting model with Normalisation 2; applies query expansion on the documents (QE) For our query expansion mechanism on fields, Table <ref type="table" coords="6,251.77,255.23,4.44,12.36" target="#tab_2">3</ref> compares the use of the linear combination of fields (QEFL) with the use of the uniform combination of fields (QEFU). The related additional runs corresponding to training method T1 (resp. T2) use the same parameter values applied in run uogTB06QET1 (resp. uogTB06QET2) (see Table <ref type="table" coords="6,157.36,307.55,3.24,12.36">9</ref>). In Table <ref type="table" coords="6,208.21,307.55,3.36,12.36" target="#tab_2">3</ref>, we see that the two different training methods have little impact on retrieval performance. Both training methods result in similar MAP performances for both QEFL and QEFU. Moreover, we observe that QEFL outperforms QEFU for both the 50 new topics, and all the 150 topics used. This indicates that our newly proposed linear combination of fields achieves a better retrieval performance than the uniform combination of fields.</p><note type="other">Run Weighting Model Retrieval Features Settings Topic Fields uogTB06QET1</note><p>In Table <ref type="table" coords="6,94.29,391.31,4.44,12.36" target="#tab_3">4</ref> we see that reducing estimated noise from the queries improves retrieval performance, compared to using no noise reduction, without query expansion (top part of Table <ref type="table" coords="6,230.17,412.19,3.36,12.36" target="#tab_3">4</ref>, first row). The parameter values of the related additional runs are the same as those used in run uogTB06S50L (see Table <ref type="table" coords="6,196.36,433.07,3.24,12.36">9</ref>). With query expansion on the body of documents only, query noise reduction results in slightly worse retrieval performance, compared to using query expansion without noise removal (second part of Table <ref type="table" coords="6,247.56,464.51,3.36,12.36" target="#tab_3">4</ref>, first row). This could be due to the fact that we have trained our query expansion mechanism on long queries before noise reduction, but not on long queries after noise reduction. Query noise reduction reduces query length (from 47.22% to 63.69%, Table <ref type="table" coords="6,226.34,506.27,3.36,12.36" target="#tab_0">1</ref> tion, marked †). Retraining the query expansion mechanism on the reduced queries could provide fairer grounds for comparing the effect of query noise reduction with query expansion. Additionally, in Table <ref type="table" coords="6,347.74,201.11,3.36,12.36" target="#tab_3">4</ref>, we see no marked difference between using query noise reduction with query expansion on the body of the documents only, and using query noise reduction with query expansion on more document fields. Finally, we observe that removing noise from the index slightly damages MAP. However, it appears to benefit highprecision retrieval, as it provides the 2nd highest P@10 score of all official runs of all groups, namely P@10 = 0.6720.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Named Page Finding Task</head><p>The objective of the Named Page Finding task is to find a particular page, given a topic that describes it. A high precision task such as this can benefit from deploying a field-based weighting model that takes into account document structure. For TREC 2006, we test modelling the distribution of term occurrences in document fields as a multinomial distribution (hypothesis H3), using our new multinomial field-based DFR weighting model, ML2 (Section 2.2, Equation ( <ref type="formula" coords="6,337.22,371.03,3.14,12.36" target="#formula_11">9</ref>)). Furthermore, we model term dependence and proximity (hypothesis H4) using the pBiL2 binomial model (Section 2.3, Equation ( <ref type="formula" coords="6,355.42,391.91,6.71,12.36" target="#formula_15">13</ref>)). Lastly, we investigate a novel approach for com- bining sources of query independent evidence, in the form of prior probabilities (hypothesis H5), which is described in Section 3.2.1. We describe and discuss our experimental runs in Section 3.2.2.</p><formula xml:id="formula_20" coords="6,332.28,426.12,7.32,8.97">N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Query-Independent Prior Probabilities</head><p>Various sources of query-independent evidence, in the form of prior probabilities, have been shown to be important for Web IR <ref type="bibr" coords="7,53.76,140.75,9.53,12.36">[6]</ref>. In this paper, we consider the following three sources of query independent evidence: (i) the information-to-noise ratio of a document [26], (ii) the static absorbing model <ref type="bibr" coords="7,216.50,161.75,13.71,12.36" target="#b21">[20]</ref>, which is a way of providing authority to documents on the basis of their incoming links, and (iii) the number of incoming links to each document (inlinks). When using query independent evidence for retrieval, the relevance score of a retrieved document d for a query Q is altered in order to take the document prior probability into account as follows:</p><formula xml:id="formula_21" coords="7,96.36,239.39,196.28,12.36">score(d, Q) = score(d, Q) + log(P (E))<label>(17)</label></formula><p>where P (E) is the prior probability of the query independent source of evidence E in document d. However, it is not clear how several document priors should be combined in a principled way. In particular, some previous work considered the priors to be independent <ref type="bibr" coords="7,195.51,296.15,9.53,12.36">[6]</ref>, while other hand-tuned linear combinations of priors <ref type="bibr" coords="7,159.05,306.59,13.71,12.36" target="#b15">[15]</ref>. Moreover, the independence assumption does not always hold: For example, consider the absorbing model and inlinks priors -while both of these priors increase retrieval accuracy, they are likely to be correlated, because a document with a high number of inlinks is likely to have a high absorbing model score. Therefore, to combine several prior probabilities in a principled manner, we propose a novel combination of prior probabilities. The combination of prior probabilities is given by:</p><formula xml:id="formula_22" coords="7,111.72,395.75,181.06,12.36">P (E1, E2) = P (E2|E1) • P (E1)<label>(18)</label></formula><p>where P (E1) is the prior probability of the query independent source of evidence E1; P (E2|E1) is the conditional probability of the query independent source of evidence E2, given E1; P (E1, E2) is the probability that both E1 and E2 occur <ref type="bibr" coords="7,208.42,443.03,13.71,12.36" target="#b18">[17]</ref>. Naturally, we can can extend this technique for more than two priors. When using the combination of prior probabilities described in Equation ( <ref type="formula" coords="7,91.78,474.47,7.43,12.36" target="#formula_22">18</ref>) for retrieval, the score of a retrieved document d for a query Q is altered, in order to take the combined prior probabilities into account as follows:</p><formula xml:id="formula_23" coords="7,87.00,511.31,205.78,12.36">score(d, Q) = score(d, Q) + log(P (E1, E2)) (19)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Experiments and Results</head><p>We submitted three runs to the TREC 2006 Named Page Finding task. The first run tests the effectiveness of the new ML2 fieldbased DFR weighting model, described in Section 2.2. The second run tests the effectiveness of the pBiL2 term dependence and proximity model, described in Section 2.3. The third run tests the combination of prior probabilities using the second run as baseline. A full description of the submitted runs follows:</p><p>• uogTB06M uses the multinomial DFR weighting model ML2.</p><p>• uogTB06MP also uses the multinomial DFR weighting model ML2, and adds the term dependence and proximity model pBiL2.</p><p>• uogTB06MPIA uses the multinomial DFR weighting model ML2 and the term dependence and proximity model pBiL2, while also combining information-to-noise ratio and static absorbing model prior probabilities.</p><p>After submitting the above official runs, we discovered that when we approximated the ML2 field-based weighting model, we used the natural logarithm, instead of the correct log2 in the Lanzcos approximation of the Γ function. We retrained and repeated the submitted runs with the correct logarithm. Table <ref type="table" coords="7,497.41,95.75,8.88,12.36" target="#tab_9">10</ref> gives the parameter settings applied in this task. Moreover, Table <ref type="table" coords="7,507.11,106.19,4.44,12.36" target="#tab_4">5</ref> displays the Mean Reciprocal Rank (MRR) of the official submitted runs, and their replacement runs with the corrected logarithm. In addition to the runs submitted, we also experimented with using a different field-based weighting model, namely PL2F, as well as applying each of the three sources of query independent evidence alone, (using Equation ( <ref type="formula" coords="7,380.73,168.95,6.71,12.36" target="#formula_21">17</ref>)), instead of combined as per Equations ( <ref type="formula" coords="7,544.54,168.95,7.43,12.36" target="#formula_22">18</ref>) &amp; <ref type="bibr" coords="7,326.04,179.39,13.71,12.36" target="#b20">(19)</ref>.</p><p>The conclusions we draw from Table <ref type="table" coords="7,458.66,189.95,4.44,12.36" target="#tab_4">5</ref> are as follows. Firstly, regarding our hypothesis H3, concerning modelling the distribution of term occurrences in document fields as a multinomial distribution, we observe that ML2 (uogTB06M) performs comparably to PL2F (uogTB06PL). This means that ML2 is not only an elegant and theoretically-sound model, but also a readily deployable model, on a par with existing state-of-the-art field-based weighting models, such as PL2F, despite ML2 employing less parameters than PL2F.</p><p>Secondly, modelling term proximity appears to assist the retrieval process. In particular, applying proximity to our baselines of uog-TB06M and uogTB06PL increases MRR (see uogTB06MP with MRR 0.466 and uogTB06PLP with MRR 0.478 respectively). This validates our hypothesis H4 on the usefulness of term dependence and proximity in the Named Page Finding task.</p><p>Thirdly, regarding the application of prior evidence, we see that all three priors applied alone -namely information-to-noise, absorbing model and inlinks -decrease performance compared to the baseline (comparing uogTB06MI, uogTB06MA and uogTB06ML to uogTB06M respectively). However, regarding hypothesis H5 on the combination of query-independent evidence, we observe that retrieval performance can be improved if we choose appropriate document priors (uogTB06MIL). In particular, MRR is improved over the use of no priors (uogTB06M), as well as over the use of any single prior alone (uogTB06MI or uogTB06ML).</p><p>Lastly, using both term proximity and the appropriate document priors, we see that retrieval performance is again enhanced compared to the baseline and the combination of priors. In particular, the unofficial run uogTB06MPIL achieves a 5% increase in MRR over our best submitted run (uogTB06MP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Terabyte Track Conclusions</head><p>In the 2006 Terabyte Track, we participated in the Adhoc and Named Page Finding tasks. We extended our modular Terrier retrieval platform, and tested the following hypotheses. For the Adhoc task, we hypothesised that, for query expansion on document fields, the linear combination of fields can provide better retrieval performance, than the uniform combination of fields. We tested this hypothesis with short queries (Section 3.1.3, Table <ref type="table" coords="7,505.93,581.99,3.24,12.36" target="#tab_2">3</ref>), and found it to be valid. For the same task, we hypothesised that low frequency part-of-speech n-grams found in text, correspond mostly to noise, which if removed, can enhance retrieval performance. We tested this hypothesis on long queries and on the test collection to be indexed, and found it to be valid when query expansion is not applied (Section 3.1.3, Table <ref type="table" coords="7,440.33,644.75,3.24,12.36" target="#tab_3">4</ref>). Query expansion combined with noise reduction lead to a small deterioration in retrieval performance, which could be due to the effect of noise reduction on query length (for noise reduction from the queries). For the Named Page Finding task, we tested the hypotheses that: (i) modelling in a refined way the distribution of term occurrences in document fields, namely as a multinomial distribution, is a theoretically-sound and robust approach, which performs comparably to other field-based weighting models; (ii) modelling the dependence and proximity of query terms in documents can enhance retrieval performance; (iii) using a conditional combination of multiple sources of query independent evidence, in the form of prior probabilities can improve retrieval performance, over using a single source of such evidence. We found hypotheses (i) and (ii) to be valid (Section 3.2.2, Table <ref type="table" coords="8,53.76,475.31,3.24,12.36" target="#tab_4">5</ref>), while further work is needed to establish the best combination of priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ENTERPRISE TRACK</head><p>In TREC 2006, we participate in the Expert Search task of the Enterprise track, where we aim to develop and experiment using our novel voting model for Expert Search <ref type="bibr" coords="8,212.66,547.79,13.71,12.36" target="#b14">[14]</ref>. Firstly, a set of documents is associated with each candidate to represent the candidate's expertise to the system. Then our voting model considers the ranking of documents with respect to the query, in order to generate an accurate ranking of candidates. For TREC 2006, we experiment to validate the following hypotheses:</p><p>1. Candidate Length Normalisation: the profiles of candidates can be of various lengths. We hypothesise that our voting model requires to account for candidate profiles of varying lengths.</p><p>2. Document Ranking: in our voting model, we hypothesise that the accuracy of the candidate ranking model depends on the extent to which documents retrieved by the underlying document ranking represents the topic.</p><p>To validate our two hypotheses, our research is directed in two areas: firstly, we propose and integrate into the voting model a new theoretically-driven way of combining document votes for candidates, that accounts for the length of each candidate's profile; secondly, to test our document ranking hypothesis, we employ three techniques, namely (i) the use of a field-based weighting model; (ii) query expansion; and (iii) the term dependence and proximity model. These techniques should increase the quality of the document ranking, and we hypothesise that the accuracy of the generated candidate ranking will also be increased.</p><p>The remainder of this section is as follows: Section 4.1 describes our voting approach for Expert Search; Section 4.2 discusses the need for candidate profile length normalisation in Expert Search; Section 4.3 describes the effect of the document ranking in the voting approach, and defines techniques which can be applied to increase the quality of the document ranking. In Section 4.4, we present the experimental setup for our runs. We discuss the submitted runs and their results in Section 4.5. We present additional runs in Section 4.6, and give some closing comments in Section 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Voting Approaches for Expert Search</head><p>Our newly-proposed approach models Expert Search as a voting process <ref type="bibr" coords="8,346.54,284.39,13.71,12.36" target="#b14">[14]</ref>. In our model, a candidate's expertise is represented by a profile, which is a set of documents associated with each candidate, to represent that candidate's expertise.</p><p>In our voting model for Expert Search, instead of directly ranking candidates, we consider the ranking of documents, with respect to the query Q, which we denote R(Q). We propose that the ranking of candidates can be modelled as a voting process, from the retrieved documents in R(Q) to the profiles of candidates: every time a document is retrieved and is associated with a candidate, then this is a vote for that candidate to have relevant expertise to Q. The votes for each candidate are then appropriately aggregated to form a ranking of candidates, taking into account the number of voting documents for that candidate, and the relevance score of the voting documents. Our voting model is extensible and general, and is not collection or topics dependent.</p><p>In <ref type="bibr" coords="8,337.19,441.35,13.71,12.36" target="#b14">[14]</ref>, we defined eleven voting techniques for aggregating votes for candidates, adapted from existing data fusion techniques. For TREC 2006, we experiment using two voting techniques, namely CombSUM and expCombMNZ. For CombSUM, the score of a candidate C's expertise to a query Q is given by:</p><formula xml:id="formula_24" coords="8,367.80,503.03,187.98,34.46">score cand CombSU M (C, Q) = X d ∈ R(Q)∩ prof ile(C) score(d, Q)<label>(20)</label></formula><p>where score(d, Q) is the score of document d in the initial ranking of documents R(Q), as given by a suitable document weighting model. In all our runs, we use the DFR InL2 document weighting model, or its field-based variant InL2F to generate score(d, Q)see Equations ( <ref type="formula" coords="8,371.78,584.87,3.18,12.36" target="#formula_1">2</ref>), ( <ref type="formula" coords="8,386.95,584.87,3.48,12.36">3</ref>) &amp; ( <ref type="formula" coords="8,408.59,584.87,3.18,12.36" target="#formula_4">4</ref>). Secondly, we apply the expCombMNZ voting technique. For expCombMNZ, the score of a candidate C's expertise to a query Q is given by:</p><formula xml:id="formula_25" coords="8,318.00,636.12,237.75,34.46">score cand expCombM N Z (C, Q) = R(Q) ∩ prof ile(C) • X d ∈ R(Q)∩ prof ile(C) exp(score(d, Q))<label>(21)</label></formula><p>where R(Q) ∩ prof ile(C) is the number of documents from the profile of candidate C that are in the ranking R(Q). In the next section, we introduce our candidate length normalisation technique which can be applied to either the voting techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Candidate Length Normalisation for Expert Search Voting approach</head><p>Document length normalisation has been studied in IR for some time, in order to fairly retrieve documents of all lengths. State-ofthe-art document weighting models, such as BM25 <ref type="bibr" coords="9,245.29,99.35,14.87,12.36" target="#b23">[22]</ref> or those from the DFR framework (for instance PL2 or InL2) <ref type="bibr" coords="9,254.14,109.79,9.53,12.36" target="#b1">[1]</ref>, all include document length normalisation components. This normalisation component prevents long documents from gaining an unfair advantage in the document ranking. However, our voting model may be susceptible to favouring candidates which have a large profile: consider a candidate with many associated documents in its profile -this candidate has a higher chance of achieving a vote at random from the document ranking, than another candidate that has a smaller profile with fewer associated documents. Hence we hypothesise that we should account for candidate length in our model, so that candidates of all lengths are retrieved fairly.</p><p>For TREC 2006, we extend our model to introduce a new technique that explicitly accounts for candidate profile length while ranking candidates. We supplement a voting technique (denoted M), by adding a candidate length normalisation. This normalisation is an adaption of Normalisation 2 from the DFR framework -see Equation ( <ref type="formula" coords="9,114.26,277.19,3.18,12.36" target="#formula_4">4</ref>). Normalisation 2 is used to control any bias towards candidates with longer profile lengths. The combination of a technique M with candidate length normalisation is denoted MNorm2, and is calculated as follows:</p><formula xml:id="formula_26" coords="9,65.76,327.72,227.01,33.33">score candMNorm2(C, Q) = score candM (C, Q) • log 2 (1 + cpro • avg len pro lC ), (cpro &gt; 0)<label>(22)</label></formula><p>where lC is the number in tokens in all the documents belonging to the profile of candidate C, and avg len pro is the average length of all candidate profiles, in tokens. cpro is a hyper-parameter, used to control the influence of normalisation. For TREC2006, we test the use of candidate length normalisation with both CombSUM and expCombMNZ, denoted by CombSUMNorm2 and expComb-MNZNorm2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of the Document Ranking</head><p>In our voting approach, the quality of the document ranking R(Q) directly affects how well the approach performs. We hypothesise that if we are able to produce a document ranking with many on-topic documents at the top of the document ranking, then we are able to accurately convert the ranking of documents into an effective ranking of candidates. In TREC 2006, we test this hypothesis, using three retrieval techniques to increase the quality of the document ranking. Firstly, we know that taking into account the structure of documents can allow increased precision for document retrieval, particularly on the W3C collection <ref type="bibr" coords="9,162.07,560.99,13.71,12.36" target="#b12">[12]</ref>. Hence, we apply a field-based weighting model from the DFR framework, to take a more refined account of each document field into account when ranking the documents. Namely, we experiment with applying the InL2F fieldbased weighting model (see Equations ( <ref type="formula" coords="9,195.15,602.87,3.48,12.36" target="#formula_1">2</ref>) &amp; ( <ref type="formula" coords="9,215.80,602.87,3.14,12.36">3</ref>)). Using this should increase the number of on-topic documents at the top of the document ranking, compared to using the InL2 model (Equations ( <ref type="formula" coords="9,285.83,623.75,3.48,12.36" target="#formula_1">2</ref>) &amp; ( <ref type="formula" coords="9,66.14,634.19,3.14,12.36" target="#formula_4">4</ref>)).</p><p>Secondly, we use the novel information theoretic model, based on the DFR framework, for incorporating the dependence and proximity of the query terms in the documents, as described in Section 2.3. We apply the term dependence and proximity model to improve the number of on-topic documents at the top of the document ranking, as we believe that on-topic documents will have term-dependencies between query terms, and by modelling these, we can bring these to the top of the document ranking. In particular, we use the pInL2 term dependence and proximity model -see Equation <ref type="bibr" coords="9,351.46,74.87,13.71,12.36" target="#b12">(12)</ref>.</p><p>Thirdly, we investigate the use of query expansion (QE) in the expert search setting. We assume that the top-ranked documents in the document ranking are on-topic to the expertise query. By performing query expansion using these top-ranked documents, we aim to bring more on-topic documents into the document ranking <ref type="bibr" coords="9,330.48,137.63,13.71,12.36" target="#b13">[13]</ref>.</p><p>Query expansion is applied using Bo1 (Equation ( <ref type="formula" coords="9,505.68,148.07,7.14,12.36" target="#formula_17">14</ref>)) to weight terms from the top exp doc ranked documents in R(Q). For Bo1, tfx is the term frequency of term t in the top exp doc ranked documents. The exp term top-ranked terms are then added to query Q and the document ranking R(Q) regenerated. We use the default settings of exp term = 10 and exp doc = 3 <ref type="bibr" coords="9,482.14,200.39,9.53,12.36" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Setup</head><p>We index the W3C collection using the Terrier IR platform <ref type="bibr" coords="9,538.55,231.35,13.71,12.36" target="#b16">[16]</ref>, by removing standard stopwords and applying Porter's weak stemming. Only documents which were associated with at least one candidate were indexed, which leaves only 52,129 documents in the index. We also index the anchor text of incoming hyperlinks from the entire W3C collection and add these to the documents.</p><p>We used two techniques to identify documents from the W3C collection to associate with candidates to represent each candidate's expertise. As described for the Occurrences profile sets of our TREC 2005 participation <ref type="bibr" coords="9,413.95,325.55,13.71,12.36" target="#b10">[10]</ref>, we generate queries which were used to identify documents that mentioned each candidate, based on the occurrences of variations of the candidate's name and email address in the collection. These documents form the OccurrencesA profile set of each candidate. All our official runs use this profile set.</p><p>Secondly, we use the Unix grep command to identify documents from the collection which contain an exact match of the candidate's full name. Each matching document is added to the candidate's profile, to form their OccurrencesB profile set. On average, it appeared that the OccurrencesB profile set finds more documents for each candidate than OccurrencesA. We note that this is counterintuitive, as OccurrencesB should be a subset of OccurrencesA, so we theorise that a bug affected the creation of OccurrencesA for TREC 2005. OccurrencesB is created using a simpler approach, than OccurrencesA.</p><p>All our experiments were performed using Terrier. We trained using the 50 TREC 2005 Enterprise track queries. Our optimisation system uses simulated annealing processes to find settings for c and cpro that maximise mean average precision (MAP). Table <ref type="table" coords="9,521.62,524.27,8.88,12.36" target="#tab_10">11</ref> details the parameter values used for the Expert Search task in TREC 2006.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments and Results</head><p>We submitted 4 runs to the Expert Search task of the Enterprise track, which test our two hypotheses for this task. All official runs used the OccurrencesA profile sets to represent the candidate expertise, and only the title field of the topics. The first three runs test our candidate length normalisation technique. Moreover, they each test a different way of increasing the topicality of the document ranking. The fourth run is a baseline run. More specifically, we submitted the following runs:</p><p>• uogX06csnP generates a document ranking using the InL2 document weighting model, and applies our CombSUMNorm2 expert search technique described above. Moreover, the pInL2 term dependence and proximity model is applied to increase the topicality of the document ranking. pendence to test the document ranking hypothesis.</p><p>• uogX06csnQE also applies InL2 and CombSUMNorm2, but applies query expansion using Bo1 to increase the topicality of the document ranking. This run also tests the candidate length normalisation technique, but uses QE to test the document ranking hypothesis.</p><p>• uogX06csnQEF is similar to uogX06csnQE, but instead the document ranking takes document structure into account, by using the field-based InL2F weighting model. This run tests the candidate length normalisation technique, and also applies fields and QE to test the document ranking hypothesis.</p><p>• uogX06ecm uses the expCombMNZ expert search technique, which applies no candidate length normalisation.</p><p>Table <ref type="table" coords="10,76.18,380.27,4.44,12.36" target="#tab_7">6</ref> summaries the salient features of each submitted run, and some additional runs that we will describe in Section 4.6. Table <ref type="table" coords="10,68.15,401.27,4.44,12.36" target="#tab_6">7</ref> shows the results of the submitted runs, in terms of Mean Average Precision (MAP), binary Preference (bPref) and Precision at 10 (P@10). We also show the overall best and median runs achieved across all participants, as well as two additional baseline runs, namely uogX06csn and uogX06csnF. uogX06csn is the baseline run using InL2 and CombSUMNorm2; uogX06csnF uses InL2F and CombSUMNorm2. Adding term dependence to the baseline run (uogX06csn vs uog-X06csnP) increases retrieval performance, as do fields (uogX06-csnF). In particular, adding QE (uogX06csn vs uogX06csnQE) provides the best submitted run. Note that using QE and fields (uog-X06csnQEF) does not increase MAP or bPref when compared to QE alone (uogX06csnQE), though Precision at 10 is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Additional Runs</head><p>As explained in Section 4.4, it appears that our OccurrencesA candidate profile sets was affected by a bug, and did not contain as much expertise evidence for each candidate as OccurrencesBnormally, OccurrencesB would be expected to be a subset of Oc-currencesA.</p><p>For our additional runs, we use only the OccurrencesB candidate profile sets, and perform a selection of runs using this, to allow us to draw firm conclusions, especially concerning the usefulness of candidate length normalisation. We also experiment across all three topic lengths. The salient features of the additional runs are also shown in Table <ref type="table" coords="10,127.04,662.39,3.36,12.36" target="#tab_7">6</ref>.</p><p>The results in terms of MAP are shown in Table <ref type="table" coords="10,244.33,672.83,4.44,12.36">8</ref> <ref type="foot" coords="10,248.88,672.28,2.99,8.24" target="#foot_3">4</ref> . From the shown results, we can see that our MAP is markedly improved With regard to our document ranking hypothesis, this seems to be validated, because applying known techniques for increasing the quality of the document ranking were shown to increase the retrieval performance of the ranking of candidates. Moreover, on the OccurrencesB candidate profile sets, applying more than one technique (fields and query expansion) resulted in a improvement over either technique alone in most cases.</p><p>Our results show that the exact technique applied to associate documents to candidate to represent their expertise has a marked effect on the retrieval performance of the system. Choosing the correct candidate profile set results in a marked increase in performance of our expert search system compared to our submitted run, and the median run of all participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In TREC 2006, we participated in the Adhoc and Named Page Finding tasks of the Terabyte track, and the Expert Search task of the Enterprise track. Having such a variety of retrieval tasks to address, ranging from classical adhoc retrieval, to enterprise-oriented expert seach, we focussed on devising new, theoretically-driven, and effective weighting models and retrieval boosting techniques, which would be generic enough, so as to be easily and effectively applied in as many retrieval tasks as possible. Specifically, we extended our Terrier Information Retrieval platform to accommodate two new Divergence From Randomness (DFR) weighting models, which combine evidence on document structure and capture term dependence and proximity, respectively. We used these models in the Terabyte and the Enterprise tracks, and found them to be effective. Additionally, we presented a new query expansion mechanism on fields, which successfully combines evidence in a linear, rather than uniform way and a novel syntactically-based noise reduction technique for long queries and the index. We presented a new theoretically-driven way of combining query independent evidence, in the form of prior probabilities, which we tested in Named Page Finding. In the Expert Search task, we further enhanced our understanding of our model for expert search, and through experimentation, generated some very promising results. Overall, our participation in TREC 2006 includes parts of our ongoing research in weighting models and retrieval performance enhancing techniques, which are effectively combined as part of the DFR framework, and easily implemented in our Terrier retrieval platform. The good results reported in our participation pave the way for further research.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,78.72,681.96,172.57,8.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: POS n-grams ranked by frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,53.76,51.96,239.15,126.86"><head>Table 1 : Syntactically-based Noise Reduction Settings. θ dis- plays the value of the threshold in the POS n-gram ranking used. † and ‡ denote reduction in query length (in tokens) and in document pointers in the postings list, respectively.</head><label>1</label><figDesc></figDesc><table coords="5,66.81,51.96,214.93,75.55"><row><cell>Noise Reduction</cell><cell>θ</cell><cell>POS n-grams extracted from</cell><cell>Reduction</cell></row><row><cell>N Rq uniform N Rq: query length ≤ 40 N Rq: query length 50 -100 N Rq: query length &gt; 100 N Riindex</cell><cell>50 10 50 10 5 17,070</cell><cell>WT10G .GOV2 .GOV2 .GOV2 .GOV2 WT10G</cell><cell>47.22% † 63.13% † 63.69% † 9.39% ‡</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,76.20,73.77,425.87,171.15"><head>Table 2 : Salient features of submitted Adhoc runs. using</head><label>2</label><figDesc>the Bo1 term weighting model, on long queries; applies noise reduction per query length (N RqL), with POS blocks drawn from .GOV2. For queries of less than 40 words θ = 50; for queries of 41 -100 words, θ = 10; for queries of more than 100 words, θ = 5 (see Table1). Applies noise reduction in the index (N Ri), with POS n-grams drawn from WT10G, and θ = 17, 070.</figDesc><table coords="6,85.50,73.77,416.57,50.46"><row><cell>PL2FS (Eq. 1 &amp; 5) Bo1F (Eq. 14 &amp; 15) PL2FS (Eq. 1 &amp; 5) Bo1F (Eq. 14 &amp; 15) PL2FS (Eq. 1 &amp; 5) Bo1F (Eq. 14 &amp; 15), query noise reduction uogTB06SS10L PL2 (Eq. 1 &amp; 4) uogTB06QET2 uogTB06S50L Bo1 (Eq. 14), query &amp; index noise reduction NRqU, NRiU QEFL: Training T1 QEFL: Training T2 QEFL: Training T1, NRqU TDN T T TDN uogTB06SSQL PL2 (Eq. 1 &amp; 4) Bo1 (Eq. 14), query &amp; index noise reduction NRqL, NRiU TDN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,53.76,506.27,239.15,201.99"><head>Table 3 : MAP of the linear combination of fields vs. the uni- form combination of fields. Title-only queries. The weight- ing model used is PL2F, with Bo1F for query expansion on fields. QEFL+T1 (resp. QEFL+T2) corresponds to our official run</head><label>3</label><figDesc>, column Reduc-</figDesc><table coords="6,78.00,536.03,190.38,78.96"><row><cell cols="3">Training QEFU QEFL diff. (%) p-value 50 New topics</cell></row><row><cell>T1 T2 T1 T2</cell><cell>0.3220 0.3459 +7.42 0.3248 0.3456 +6.40 All 150 topics 0.3335 0.3558 +6.69 0.3338 0.3594 +7.67</cell><cell>0.3396 0.1549 3.756e-04 9.001e-03</cell></row></table><note coords="6,69.72,668.76,222.95,8.07;6,53.76,679.32,238.90,8.07;6,53.76,689.76,239.03,8.07;6,53.76,700.20,124.71,8.07"><p>uogTB06QET1 (resp. uogTB06QET2). Submitted runs are in boldface. QEFU+T1 and QEFU+T2 are baselines for comparison with QEFL. p-values are computed by the Wilcoxon matched-pairs signed-ranks test.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,316.80,422.99,239.22,286.95"><head>Table 4 : MAP, P@10, and bPref of Adhoc runs with Title + Description + Narrative queries. The weighting model is PL2 (PL2F with fields) and Bo1 for query expansion (Bo1F with fields). ⊕ is our official submitted run uogTB06S50L. ⊗ is our official submitted run uogTB06SS10L. is our official submit- ted run uogTB06SSQL</head><label>4</label><figDesc></figDesc><table coords="6,322.56,422.99,228.19,162.24"><row><cell>Rq</cell><cell>N Ri</cell><cell cols="2">Features MAP</cell><cell>P@10</cell><cell>bPref</cell></row><row><cell>none U-WT10G U-GOV2 L-GOV2 none U-WT10G U-GOV2 L-GOV2 U-GOV2 L-GOV2 none U-WT10G U-WT10G L-WT10G</cell><cell cols="2">none none none none none none none none WT10G QE⊕ QE QE QE QE WT10G QE⊗ none QEFL none QEFL none QEFL none QEFL</cell><cell cols="3">0.3355 0.6240 0.3772 0.3613 0.6320 0.4023 0.3409 0.6240 0.3813 0.3485 0.6400 0.3891 0.3966 0.6680 0.4446 0.3853 0.6640 0.4399 0.3806 0.6460 0.4325 0.3898 0.6540 0.4423 0.3686 0.6540 0.4290 0.3728 0.6720 0.4404 0.3878 0.6560 0.4398 0.3893 0.6580 0.4411 0.3770 0.6380 0.4177 0.3804 0.6460 0.4257</cell></row></table><note coords="6,402.59,648.96,153.43,8.97;6,316.80,659.40,239.13,8.97;6,316.80,670.56,239.00,8.07;6,316.80,680.40,239.04,8.97;6,316.80,691.44,238.91,8.07;6,316.80,701.88,43.54,8.07"><p>. N Rq and N Ri denote noise reduction in the query and the index, respectively. U and L denote uniform noise reduction and reduction per query length, respectively. QE is query expansion on body only (QEF L is query expansion on fields). Submitted runs are shaded and best scores are in bold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,53.76,51.71,239.03,314.43"><head>Table 5 : MRR of the Named Page runs. Submitted are the offi- cial submitted runs. Corrected log are the same runs, using the correct logarithm function. The field-based weighting models used are ML2 (denoted by</head><label>5</label><figDesc></figDesc><table coords="8,90.72,51.71,164.80,231.48"><row><cell>Run Name</cell><cell cols="3">Submitted Corrected log</cell></row><row><cell cols="3">Weighting model only</cell><cell></cell></row><row><cell>uogTB06M uogTB06PL</cell><cell>0.448</cell><cell>0.454</cell><cell>0.449</cell></row><row><cell></cell><cell>Proximity</cell><cell></cell><cell></cell></row><row><cell>uogTB06MP uogTB06PLP</cell><cell>0.466</cell><cell>0.478</cell><cell>0.467</cell></row><row><cell></cell><cell>Single Priors</cell><cell></cell><cell></cell></row><row><cell>uogTB06MI uogTB06MA uogTB06ML</cell><cell></cell><cell>0.440 0.431 0.422</cell><cell></cell></row><row><cell cols="3">Combined Priors</cell><cell></cell></row><row><cell>uogTB06MIA uogTB06MIL</cell><cell></cell><cell>0.413 0.465</cell><cell></cell></row><row><cell cols="3">Proximity + Priors</cell><cell></cell></row><row><cell>uogTB06MPIA uogTB06MPIL</cell><cell>0.463</cell><cell>0.489</cell><cell>0.454</cell></row><row><cell>best median</cell><cell cols="2">0.7779 0.3706</cell><cell></cell></row></table><note coords="8,162.66,326.16,130.12,8.97;8,53.76,336.60,239.03,8.97;8,53.76,347.64,238.99,8.07;8,53.76,358.08,138.53,8.07"><p>M ), and PL2F (denoted by P L). Term dependence and proximity is denoted by P . I, A and L denote the priors of information-to-noise ratio, static absorbing model and inlinks, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,53.76,358.08,239.04,28.95"><head>best and median are the best and median runs submitted among all participants, respec- tively. Submitted runs are shaded. Our best run is in boldface.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,339.24,696.95,216.48,22.92"><head>Table 7 : The mean average precision (MAP), binary preference (bPref), and precision at 10 (P@10) of our submitted runs, as well as that achieved by all participants, and two additional runs. P@10 achieved by all participants is not available. All runs use the OccurrencesA profile sets, and title only topics.</head><label>7</label><figDesc>This run tests the candidate length normalisation technique, and uses term de-</figDesc><table coords="10,91.08,51.71,164.22,97.20"><row><cell>Run Name Best Median uogX06csnP uogX06csnQE uogX06csnQEF 0.3011 0.3208 0.4551 MAP bPref P@10 0.7507 0.7542 -0.3412 0.3602 -0.2881 0.3120 0.4510 0.3024 0.3292 0.4429 uogX06ecm 0.2685 0.2991 0.4143 uogX06csn 0.2784 0.3222 0.4224 uogX06csnF 0.2830 0.3195 0.4306</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,53.76,276.48,455.01,57.96"><head>Table 6 : Salient features of submitted and additional runs of the expert search task of the Enterprise track. on</head><label>6</label><figDesc>the OccurrencesB set. Further experimentation using additional candidate profile sets would provide solid conclusions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,316.80,274.44,239.11,185.40"><head>Table 10 : The parameter values used in our TREC 2006 Ter- abyte track Named-page task runs. Figures in brackets were before being retrained using the correct logarithm function base.</head><label>10</label><figDesc></figDesc><table coords="12,346.68,321.47,179.37,138.36"><row><cell>Parameter pInL2 cp InL2 c Norm2 cpro with InL2 InL2F c body InL2F c title InL2F catext InL2F w body InL2F w title InL2F watext Norm2 cpro with InL2F (22) Equation(s) (12) &amp; (4) (2) &amp; (4) (22) (2) &amp; (3) (2) &amp; (3) (2) &amp; (3) (2) &amp; (3) (2) &amp; (3) (2) &amp; (3) QE exp term (14) QE exp doc (14)</cell><cell>Value 0.5 0.124 3.690 0.171 1.131 2.598 0.772 1.320 1.334 12.0000 10 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,316.80,471.84,238.90,99.96"><head>Table 11 : The parameter values used in our submitted runs to the TREC 2006 Enterprise Track, Expert Search task.</head><label>11</label><figDesc>Hard Tracks. In Proceedings of the TREC-2004, Gaithersburg, USA, 2004. [26] X. L. Zhu and S. Gauch. Incorporating Quality Metrics in Centralized/Distributed Information Retrieval on the World Wide Web. In Proceedings of SIGIR'2000, 288-295, Athens, Greece, 2000.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.32,698.51,140.17,12.36;1,53.76,707.51,102.16,12.36"><p>Information on Terrier can be found at: http://ir.dcs.gla.ac.uk/terrier/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,321.24,698.51,150.13,12.36;3,316.80,707.51,204.00,12.36"><p>Information on .GOV2 can be found from http://ir.dcs.gla.ac.uk/test collections/gov2-summary.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,321.24,698.51,234.32,12.36;5,316.80,707.51,19.67,12.36"><p>Details on the tagger parameters and tagset used can be found in<ref type="bibr" coords="5,326.04,707.51,10.43,12.36" target="#b7">[7]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="10,58.32,698.51,234.44,12.36;10,53.76,707.51,238.92,12.36"><p>Note that all runs using OccurrencesB were made using a full index of all 331,037 documents in the W3C collection. This should</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>by using the OccurrencesB candidates profiles set, compared to the submitted runs in Table <ref type="table" coords="10,421.99,321.23,3.36,12.36">7</ref>. In particular, the performance of uogX06csn jumps to MAP 0.4647 using short topics, and uogX06ecm to 0.5430. Applying either QE or fields to either baseline results in an improvement in terms of MAP. For example, comparing uogX06csn with uogX06csnQE; uogX06csn with uogX06-csnF; and uogX06ecm with uogX06ecmQE. In each case, applying a technique resulted in an increase in MAP, which validates our document ranking hypothesis. Moreover, in most cases, applying two techniques in runs uogX06csQEF, uogX06csnQEF, and uogX06ecmnQEF improves over applying either QE or fields alone (the exceptions here are uogX06ecmQEF on short and long queries, and uogX06csQEF on long queries). This appears to validate our document ranking hypothesis. Further improvements are obtainable if the exp doc and exp term parameters are varied <ref type="bibr" coords="10,521.69,457.19,13.71,12.36" target="#b13">[13]</ref>.</p><p>Next, we examine the usefulness of candidate length normalisation. Comparing uogX06cs with uogX06csn, and uogX06ecm with uogX06ecmn, shows a decrease in MAP across all three topic types. This is mirrored across other runs -for instance, comparing uogX06csQEF with uogX06csnQEF. Note however that decreases in MAP are less marked when applying QE and fields.</p><p>Comparing CombSUM and expCombMNZ, we can see that exp-CombMNZ is at least as good as, and usually better than Comb-SUM. This mirrors our evaluation using TREC 2005 data <ref type="bibr" coords="10,525.00,551.39,13.71,12.36" target="#b14">[14]</ref>.</p><p>Finally, we examine the effect of topic length on MAP. On average, using title description and narrative topic fields (TDN) is better than title and description (TD), which is better than title only (T). However, the margins between topic types are very narrow, so no solid conclusions can be drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Expert Search Task Conclusions</head><p>Overall, we demonstrated that our expert search model performs in a stable manner.</p><p>With regard to our first hypothesis, we require further research to establish the usefulness of candidate length normalisation in expert search. Candidate length normalisation did not appear to be useful have little effect on the results, as unassociated documents are not considered by the voting techniques for ranking the experts.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,321.29,398.27,96.47,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.79,405.59,205.56,12.36;11,331.80,416.03,186.84,12.36;11,331.80,426.47,104.93,12.36" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,369.56,409.32,167.79,8.07;11,331.80,419.76,139.24,8.07">Probabilistic Models for Information Retrieval based on Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="11,331.78,437.99,220.19,12.36;11,331.80,448.43,218.37,12.36;11,331.80,458.87,215.88,12.36" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,480.85,437.99,71.12,12.36;11,331.80,448.43,117.80,12.36">Italian Monolingual Information Retrieval with Prosit</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<editor>C. Peters, M. Braschler, J. Gonzalo, and M. Kluck</editor>
		<imprint>
			<date type="published" when="2001">2001. 2002</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.78,470.39,211.41,12.36;11,331.80,480.83,192.57,12.36;11,331.80,491.27,217.33,12.36" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,382.03,480.83,142.34,12.36;11,331.80,491.27,33.25,12.36">Class-Based n-Gram Models of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,371.60,495.00,94.17,8.07">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.78,502.67,217.40,12.36;11,331.80,513.23,206.37,12.36;11,331.80,523.67,20.03,12.36" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,440.06,502.67,109.12,12.36;11,331.80,513.23,16.78,12.36">Overview of TREC-2004 Web track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,363.81,516.96,96.47,8.07">Proceedings of TREC-2004</title>
		<meeting>TREC-2004<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.78,535.07,201.69,12.36;11,331.80,545.51,194.87,12.36;11,331.80,556.07,109.86,12.36" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,480.48,535.07,52.99,12.36;11,331.80,545.51,77.49,12.36">Towards better Weighting of Anchors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Upstill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,425.34,549.24,97.20,8.07">Proceedings of SIGIR&apos;2004</title>
		<meeting>SIGIR&apos;2004<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="512" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.78,567.47,222.78,12.36;11,331.80,577.91,214.17,12.36;11,331.80,588.35,162.13,12.36" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,487.90,567.47,66.66,12.36;11,331.80,577.91,144.77,12.36">The Importance of Prior Probabilities for Entry Page Search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,492.36,581.64,53.61,8.07;11,331.80,592.08,41.31,8.07">Proceedings of SIGIR&apos;2002</title>
		<meeting>SIGIR&apos;2002<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.78,599.87,198.01,12.36;11,331.80,610.31,185.02,12.36;11,331.80,620.75,219.95,12.36" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,416.33,599.87,113.46,12.36;11,331.80,610.31,171.84,12.36">Examining the Content Load of Part-of-Speech Blocks for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,331.80,624.48,107.57,8.07">Proceedings of COLING/ACL</title>
		<meeting>COLING/ACL<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.78,632.27,200.27,12.36;11,331.80,642.71,190.67,12.36;11,331.80,653.15,111.53,12.36" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,416.33,632.27,115.72,12.36;11,331.80,642.71,119.45,12.36">Light Syntactically-Based Index Pruning for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,468.86,646.44,53.60,8.07;11,331.80,656.88,38.74,8.07">Proceedings of ECIR-2007</title>
		<meeting>ECIR-2007<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,331.78,664.67,189.47,12.36;11,331.80,675.11,186.22,12.36;11,331.80,685.55,210.23,12.36" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,416.33,664.67,104.92,12.36;11,331.80,675.11,182.76,12.36">A Syntactically-Based Query Reformulation Technique for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,331.80,689.28,148.59,8.07">Information Processing and Management</title>
		<imprint>
			<publisher>In Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,336.22,696.95,219.45,12.36;11,331.79,707.51,204.23,12.36;12,68.76,154.67,220.27,12.36;12,68.76,165.11,93.30,12.36" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,518.26,696.95,37.42,12.36;11,331.79,707.51,204.23,12.36;12,68.76,154.67,104.20,12.36">University of Glasgow at TREC 2005: Experiments in Terabyte and Enterprise tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,188.18,158.40,96.35,8.07">Proceedings of TREC-2005</title>
		<meeting>TREC-2005<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.19,176.63,189.83,12.36;12,68.76,187.07,186.01,12.36;12,68.76,197.51,195.95,12.36;12,68.76,207.95,220.65,12.36;12,68.76,218.51,220.31,12.36;12,68.76,228.95,78.21,12.36" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,102.10,187.07,152.67,12.36;12,68.76,197.51,195.95,12.36;12,68.76,207.95,65.82,12.36">University of Glasgow at WebCLEF 2005: Experiments in Per-Field Normalisation and Language Specific Stemming</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kluck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="page" from="898" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.19,240.35,175.23,12.36;12,68.76,250.79,211.28,12.36;12,68.76,261.35,107.10,12.36" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,174.76,240.35,73.65,12.36;12,68.76,250.79,94.28,12.36">Combining Fields in Known-Item Email Search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,178.71,254.52,97.20,8.07">Proceedings of SIGIR&apos;2006</title>
		<meeting>SIGIR&apos;2006<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="675" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.19,272.75,207.82,12.36;12,68.76,283.19,209.73,12.36;12,68.76,293.63,20.03,12.36" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,174.76,272.75,106.24,12.36;12,68.76,283.19,48.95,12.36">Using Relevance Feedback in Expert Search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,133.38,286.92,94.63,8.07">Proceedings of ECIR-2007</title>
		<meeting>ECIR-2007<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,305.15,218.51,12.36;12,68.76,315.59,193.41,12.36;12,68.76,326.03,187.57,12.36" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,174.76,305.15,116.93,12.36;12,68.76,315.59,179.91,12.36">Voting for Candidates: Adapting Data Fusion Techniques for an Expert Search Task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,68.76,329.76,98.94,8.07">Proceedings of CIKM&apos;2006</title>
		<meeting>CIKM&apos;2006<address><addrLine>Arlington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,337.55,212.02,12.36;12,68.76,347.99,183.23,12.36;12,68.76,358.43,140.67,12.36" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,259.06,337.55,26.14,12.36;12,68.76,347.99,100.76,12.36">Indri at TREC 2005: Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,185.31,351.72,66.68,8.07;12,68.76,362.16,40.58,8.07">Proceedings of the TREC-2005</title>
		<meeting>the TREC-2005<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,369.83,218.25,12.36;12,68.76,380.39,190.17,12.36;12,68.76,390.83,221.24,12.36" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,107.02,380.39,151.91,12.36;12,68.76,390.83,109.57,12.36">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,193.95,394.56,73.74,8.07">Proceedings of OSIR</title>
		<meeting>OSIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,68.76,401.27,109.01,12.36" xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Workshop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Seattle, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,412.67,205.57,12.36;12,68.76,423.23,208.90,12.36;12,68.76,433.67,66.20,12.36" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,149.82,412.67,128.93,12.36;12,68.76,423.23,94.62,12.36">Combination of Document Priors in Web Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,178.84,426.96,94.51,8.07">Proceedings of ECIR-2007</title>
		<meeting>ECIR-2007<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.19,445.07,192.49,12.36;12,68.76,455.51,223.90,12.36;12,68.76,466.07,111.53,12.36" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,173.08,445.07,92.60,12.36;12,68.76,455.51,154.96,12.36">Multinomial Randomness Models for Retrieval with Document Fields</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,239.05,459.24,53.61,8.07;12,68.76,469.80,38.74,8.07">Proceedings of ECIR-2007</title>
		<meeting>ECIR-2007<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,477.47,217.29,12.36;12,68.76,487.91,222.34,12.36;12,68.76,498.35,221.60,12.36;12,68.76,508.91,42.45,12.36" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,200.65,477.47,89.82,12.36;12,68.76,487.91,222.34,12.36;12,68.76,498.35,41.39,12.36">University of Glasgow at TREC2004: Experiments in Web, Robust and Terabyte tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,125.35,502.08,109.54,8.07">Proceedings of the TREC-2004</title>
		<meeting>the TREC-2004<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,520.31,216.45,12.36;12,68.76,530.75,210.72,12.36;12,68.76,541.19,20.03,12.36" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,213.13,520.31,76.51,12.36;12,68.76,530.75,65.30,12.36">The Static Absorbing Model for the Web</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,140.95,534.48,98.91,8.07">Journal of Web Engineering</title>
		<imprint>
			<biblScope unit="page" from="165" to="186" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,552.71,181.80,12.36;12,68.76,563.15,216.58,12.36;12,68.76,573.59,198.01,12.36" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<title level="m" coord="12,123.44,566.88,161.90,8.07;12,68.76,577.32,37.67,8.07">Numerical Recipes in C: The Art of Scientific Computing</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct coords="12,73.18,585.11,181.07,12.36;12,68.76,595.55,218.72,12.36;12,68.76,605.99,134.07,12.36" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,165.89,595.55,60.92,12.36">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,243.23,599.28,44.25,8.07;12,68.76,609.72,36.48,8.07">Proceedings of TREC-4</title>
		<meeting>TREC-4<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,617.51,204.71,12.36;12,68.76,627.95,219.60,12.36;12,68.76,638.39,178.83,12.36" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,227.51,617.51,50.38,12.36;12,68.76,627.95,137.47,12.36">Simple BM25 Extension to Multiple Weighted Fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,221.56,631.68,66.80,8.07;12,68.76,642.12,18.18,8.07">Proceedings of the CIKM</title>
		<meeting>the CIKM<address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.19,649.79,216.56,12.36;12,68.76,664.08,199.07,8.07;12,68.76,670.79,222.20,12.36;12,68.76,681.23,128.68,12.36" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,115.52,649.79,161.17,12.36">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,68.76,664.08,199.07,8.07;12,68.76,674.52,76.81,8.07">The Smart Retrieval system-Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Salton, G., Ed; Prentice-Hall Englewood Cliffs, N.J., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,73.18,692.63,180.93,12.36;12,68.76,703.19,209.76,12.36" xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,119.49,703.19,111.41,12.36">Microsoft Cambridge at TREC</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<publisher>Web</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
