<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.25,157.84,377.50,18.08">York University at TREC 2006: Legal Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,241.31,193.75,41.21,9.41"><forename type="first">Miao</forename><surname>Wen</surname></persName>
							<email>mwen@cs.yorku.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.66,193.75,58.85,9.41"><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
							<email>jhuang@yorku.ca</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.25,157.84,377.50,18.08">York University at TREC 2006: Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6753D011FFF0657BBB43AF803F0B5433</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>York University participated in the legal track this year. For this track, we developed an Okapi-based Legal Search Engine (LSE) v1.0. Our experiments mainly focused on evaluating the effect of a probabilistic text retrieval model on the legal domain. In order to address the special problems in legal text retrieval, new automatic feedback methods and term weighting methods are proposed and tested.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Legal text retrieval is a particular problem of information retrieval among a wide range of retrieval tasks in different domains. Retrieval in the legal domain is case oriented. Lawyers need to retrieve huge amount of evidence from accessible resources, which is relevant to the problem in litigation. Increasingly, lawyers use automated search and retrieval tools to find useful information from the vast amount of evidence in electronic form. To our best knowledge, most computer aided legal systems are kinds of expert system historically. So far almost all the legal information retrieval systems are based on the boolean retrieval model.</p><p>Probabilistic Information Retrieval (IR) model is one of the most classical models in IR. Sound statistic background of the model brings its outstanding performance. Based on this model, term weighting functions are proposed and evolved over the decades. The utilization of relevance information and query expansion are the most important factors of IR, which has been studied almost from the very beginning of IR. The efficiency of it to improve the performance of IR has been affirmed widely. However, applying the probabilistic IR model into legal text retrieval is relatively new. The 2006 legal track provides an uniform simulation of legal text requests in real litigation, which allows IR researchers to evaluate their retrieval systems in the legal domain. One major goal of us is to evaluate the effect of a probabilistic retrieval model on the legal domain.</p><p>The other major goal of us this year is to evaluate our new automatic pseudo-relevance feedback process in the legal text retrieval. Pseudo-relevance feedback, also known as blind feedback, is a practical technique commonly used to improve retrieval performance <ref type="bibr" coords="1,402.30,664.42,10.52,10.46" target="#b2">[3,</ref><ref type="bibr" coords="1,416.83,664.42,7.01,10.46" target="#b7">8]</ref>. The basic idea is to extract expansion terms from the top-ranked documents to formulate a new query term set for the second round retrieval. Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. As one of the most popular and practical relevance feedback approaches, it provides an easy way to obtain relevance document automatically. However, the negative side of blind feedback is its uncontrolled quality on relevance, which could degrades the performance of the retrieval greatly. A more robust and error tolerant feedback algorithm is investigated in our experiments.</p><p>Under the MLSRF-pack architecture, we developed an Okapi-based legal search engine LSE1.0 to process all the topics. The whole collection was only indexed at the document level in the experiments. We totally built 6 parallel indexes by using Okapi to facility the query process. A new feedback approach of β-approximation was also tested in our experiments. We submitted two automatic runs in this year. The run without feedback was submitted as our primary run. The other run with a new feedback term selection (TS) and a new term weighting (RW) method was also submitted</p><p>In the next section, we describe the overall system architecture. More details on query formulation, parallel databases, new relevant weighting and term selection methods are discussed in algorithm section. Experiments and results are provided in section 4. In the final section, more discussions on the experimental results and future work are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Figure <ref type="figure" coords="2,120.64,320.39,4.98,10.46" target="#fig_1">1</ref> depicts the overall structure of our legal search system. We use Okapi 2.31 as the underlying retrieval system, based on which we develop our own multi-database utility model, topic processing and feedback modules.  The LSE v1.0 was developed under the frame of MLSRF-pack. The differences between the LSE1.0 and our previous search engine are mainly in two aspects. First, no passage level index was built and used in LSE version 1.0. So the original dual level index and re-ranking was simplified into a single document level index retrieval. However, a room for dual index was reserved and could be implemented. Secondly, we found a lot of OCR errors in the corpus when we index the whole corpus. Because of the OCR errors, over 10 thousands tokens could be generated at the pseudorelevance feedback stage. Therefore, a new term selection method was designed and evaluated at the pseudo-relevance feedback stage. Details will be given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-database Utility</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm</head><p>In this section, we first discuss how to generate the query terms automatically for each topic. Then we describe the indexing and related okapi-based parallel database. Finally, we present our new term weighting and term selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query formulation</head><p>Total forty-six document production requests (ref as 'topics' in the rest) in five 'Complaint' sections were given in this year's legal track. Different from other tracks, topics were not in traditional TREC format, but in a complex XML format with extensive topic description information. We formulated our query by extracting terms from &lt;BooleanQuery&gt; element. Precisely, three elements, &lt;ProposalByDefendant&gt;, &lt;RejoinderByPlaintiff&gt; and &lt;FinalQuery&gt;, were utilized in our query formulation. General algorithm of query formulation is shown as follows:</p><p>• 1. Remove all term or phrase defined by 'NOT'.</p><p>• 2. Remove all non-literal characters, such as '"', ' !', '(',')'</p><p>• 3. Remove all Boolean operators, including "NOT/not", "OR/or", "W/x", etc.</p><p>• 4. Tokened the remain string and formulate each term as a array of occurrence in above three elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallel databases retrieval model</head><p>To improve the flexibility and capability of Okapi interface, we implemented the parallel databases retrieval model so that the upper layer system could access multiple Okapi databases. These databases were indexed under the same schema as if they were indexed in a whole piece. We will discuss this issue in three parts: 1. Indexing parallel databases; 2. Extracting global statistic information and weighting; 3. Merging final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Indexing parallel databases</head><p>The first task was to separate corpus and generated and indexed okapi databases with same schema.</p><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Extracting probabilistic information and weighting</head><p>The traditional BM25 weighting function is shown as follows:</p><formula xml:id="formula_0" coords="4,172.64,357.87,349.35,21.70">ω = (k1 + 1) * tf K + tf * w (1) * (k3 + 1 ) * qtf k 3 + qtf ⊕ k2 * nq * (avdl -dl ) (avdl + dl )<label>(1)</label></formula><p>w (1) = log</p><formula xml:id="formula_1" coords="4,302.87,391.06,214.89,24.03">N -n + 0.5 n + 0.5 (<label>2</label></formula><formula xml:id="formula_2" coords="4,517.76,397.80,4.24,10.46">)</formula><p>where w is the weight of a query term, N is the number of indexed documents in the collection, n is the number of documents containing the term, R is the number of documents known to be relevant to a specific topic, r is the number of relevant documents containing the term, tf is within-document term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the average document length, nq is the number of query terms, the k i s are tuning constants (which depend on the database and possibly on the nature of the queries and are empirically determined), K equals to k 1 * ((1 -b) + b * dl/avdl), and ⊕ indicates that its following component is added only once per document, rather than for each term. In our experiments, the values of k 1 , k 2 , k 3 and b in the BM25 function are set to be 1.2, 0, 8 and 0.75 respectively. In parallel databases solution, the key issue was to extract statistic information of query terms from all databases, globally, but from any individual database, so BM25 function could maintain valid. We abstracted : N i Number of record in i th partition database n i Number of units containing a specific term in i th partition database By substituting N and n in equation 2 with following:</p><formula xml:id="formula_3" coords="4,281.08,630.13,240.92,69.16">N = n k=1 N i (3) n = n k=1 n i (4)</formula><p>We obtained precise global statistic information of query terms, so that the term weighing of BM25 still holds in our parallel database solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Merging final results</head><p>After obtaining the global weight for each query term, the whole set of terms are queried upon each partition database. Each one will retrieve a ranked list of relevant records within each partition. The final result is then generated by merging all sub-results from all the partitions and ranking according to their relevant scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A new method for term weighting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Relevant weight of BM25</head><p>Without relevant information, term weighting function <ref type="bibr" coords="5,324.05,241.08,12.80,10.46" target="#b1">(2)</ref>, was simplified to IDF-like function. However, the utilization of relevant information was one of the most important component in Probabilistic retrieval model. RSJ relevance weighting of query terms was proposed in 1976 <ref type="bibr" coords="5,435.45,265.00,10.52,10.46" target="#b4">[5]</ref> as an alternative term weighting of 2 when relevant information is available. As shown in 5,</p><formula xml:id="formula_4" coords="5,228.14,297.38,293.86,24.03">w (1) = log (r)/(R -r) (n -r)/(N -n -R + r) (5)</formula><p>R and r were introduced into term weighting, in which:</p><p>• R is the number of documents known to be relevant to a specific topic,</p><p>• r is the number of relevant documents containing the</p><p>The above weighting was applicable with idea relevance information available, and under the assumption of: (1) "The term distribution in the relevant items previously retrieved is the same as the distribution for the complete set of relevant items";(2) "all non-retrieved items can be treated as non-relevant" <ref type="bibr" coords="5,157.16,422.97,12.71,10.46" target="#b0">[1]</ref>. However, the idea relevance information only existed theoretically, to make the weighting function more practical, a point-5 version of approximation is suggested, when not all the relevance information is available.</p><formula xml:id="formula_5" coords="5,203.24,467.30,318.77,24.03">w (1) = log (r + 0.5)/(R -r + 0.5) (n -r + 0.5)/(N -n -R + r + 0.5)<label>(6)</label></formula><p>The function 6 is consistent with 2 when relevant information is not available, which R and r are reduce to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">β approximation</head><p>For the same reason of point-5 version of approximation in RSJ, relevant information was never ideal in practical case. The universal 0.5 approximation to every terms is somewhat an arbitrary solution. So we tried to find an alternative solution, which can adjust approximation according to terms' own probabilistic character in collection, so that weighting of terms could be more accurate.</p><p>Our new method was also based on the original theoretical principle of term relevance weighting <ref type="bibr" coords="5,90.00,613.28,10.52,10.46" target="#b4">[5,</ref><ref type="bibr" coords="5,103.83,613.28,7.01,10.46" target="#b0">1]</ref>, same as BM25 relevance weighting was:</p><formula xml:id="formula_6" coords="5,269.59,634.20,252.42,24.03">w = log p(1 -q) (1 -p)q (7)</formula><p>• p is the probability of a document contains a term, given that it is relevant.</p><p>• q is the probability of a document contains a term, given that it is not relevant.</p><p>Considering the property of relevance information obtained by pseudo-relevance feedback and the deduction fashion in <ref type="bibr" coords="6,201.06,134.11,9.96,10.46" target="#b8">[9]</ref>, the approximation of p = 0.5 without any relevance information, we could more safely approximate that p = β, (0.5,1) for each terms in feedback information. So the weighting function with relevance information could be deduced as:</p><formula xml:id="formula_7" coords="6,265.98,177.89,256.02,24.03">w β = log N -nβ (1 -β)n<label>(8)</label></formula><p>The left question was how to approximate β for each term based on relevant information. One of our proposed method was to utilize the change of term's 'density' in whole data set and in feedback information. Given following definition:</p><p>• N is the number of documents in feedback collection,</p><p>• n is the number of documents containing the term in feedback collection, We believed that the density holds certain association to p, and if we assumed that there was a linear relationship between them. Then we have:</p><formula xml:id="formula_8" coords="6,263.92,327.00,258.08,51.88">n N : p = n N : kβ (9) β = N • n N • n × p × 1 k<label>(10)</label></formula><p>Where p=0.5 Ideally, β should be in range of (0.5, 1). Because of the size of feedback information ware normally far smaller than that of original collection's, it was practically in range of (0, +∞). To adapt it for formula <ref type="bibr" coords="6,126.56,417.99,11.62,10.46" target="#b4">(5)</ref>. We applied following:</p><formula xml:id="formula_9" coords="6,280.10,438.33,237.48,25.10">β = e β 1 + e β (<label>11</label></formula><formula xml:id="formula_10" coords="6,517.57,446.14,4.43,10.46">)</formula><p>The above was the general idea of β approximation on term weighting with unreliable relevance information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A new term selection method</head><p>Term selection was a practical as well as critical problem of feedback process, which related to query expansion strategy. Technically, more than hundreds of terms could be abstracted from feedback information, even only top 10 documents were chosen. To identify and select the most "useful" terms amount them automatically, selection criteria was needed to be designed very carefully. Robertson talked about criteria in <ref type="bibr" coords="6,194.38,573.51,10.52,10.46" target="#b3">[4,</ref><ref type="bibr" coords="6,208.23,573.51,7.01,10.46" target="#b6">7]</ref>, shown in .</p><formula xml:id="formula_11" coords="6,266.74,597.42,255.25,11.35">a t = w t * (p t -q t ) (12)</formula><p>• w t is the weight of term t,</p><p>• p t is the probability of a document contains a term, given that it is relevant.</p><p>• q t is the probability of a document contains a term, given that it is not relevant.</p><p>in which, p t and q t may be estimated from relevance feedback information. When considering above criteria within RSJ weighting schema, a t was actually equivalent to w t mathematically, and by R-r relevant weighting, only partial probabilistic information of terms within feedback documents was used. More fully utilizing of terms' statistic information was explored in our experiment as a complement to traditional methods.</p><formula xml:id="formula_12" coords="7,199.46,130.63,322.55,65.32">a t = w β * m * 1 n f b i=1 (o ti -m) 2 * (k 3 + 1) * qtf k 3 + qtf (13) m = n f b i=1 o ti n f b (<label>14</label></formula><formula xml:id="formula_13" coords="7,517.57,177.76,4.43,10.46">)</formula><p>where</p><p>• n f b is the number of selected feedback documents in feedback collection. Traditionally it is 10 and can be increased to improve performance according different topics.</p><p>• o ti is the number of occurrence of term t, in the i th feedback document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>For the 2006 legal track, we submitted two runs "york06la01" and "york06la02" in total. The difference between those two runs is shown in Table <ref type="table" coords="7,326.89,319.81,3.87,10.46">3</ref>. Their evaluation results are presented in Table <ref type="table" coords="7,117.60,331.77,3.87,10.46" target="#tab_2">4</ref>. Although the retrieval results we submitted are based on all the 46 topics, only 39 topics are counted in the final evaluation. Therefore, the evaluation results shown in According to the evaluation results of these 39 topics, the average number of relevant documents per topic is 111. Topic 19 has 502 relevant documents. There are 15 topics that have over 100 relevant documents. Comparing to the 2004 and 2005 topics of HARD track, the 2006 topics of legal track are much more rich in terms of the number of relevant documents. However, for the "Average Precision" measure, the upper boundary among all the 39 topics is 0.2539. Similarly, the median value is 0.0499. In terms of "R-prec", the upper boundary is 0.3270, while the median value is just 0.0863.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>There are a lot of spaces to improve the retrieval performance. First, we find that there are many OCR errors in the 2006 legal corpus. Some errors can be recognized. But many errors are difficult to deal with. We tried to clean those errors by defining some rules. However, we found that it is almost impossible to find all the errors by this method. Without finding an effective method to handle this problem, those OCR errors will be put into our Okapi indexes. However, the incorrect statistic information for a term will have a negative impact on the retrieval performance. Secondly, the average length of legal documents is long. For example, the average length of documents for the 2004 HARD corpus is 2188 <ref type="bibr" coords="8,211.72,146.07,14.61,10.46" target="#b9">[10]</ref>, while the average length of documents for the 2006 legal corpus is 4549. Therefore, the original settings for those tuning constants k 1 , k 2 , k 3 and b in the BM25, which were set to 1.2, 0, 8 and 0.75 in our experiment, might not be a good setting any more. Finally, to find a better term selection method is also our next step work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This research is supported by research grants from the Natural Sciences and Engineering Research Council (NSERC) of Canada and Atkinson Faculty of York University.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,238.06,598.23,132.57,10.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,505.93,432.02,228.73"><head>Table 1 :</head><label>1</label><figDesc>legal track 2006 case, we were given about 7 million records data as retrieval collection in approximately 57G of uncompressed XML files. After preprocessing XML files, the whole data set ware split into 6 partitions equally. The detail split schema is shown in 1.</figDesc><table coords="3,232.20,550.73,147.59,96.93"><row><cell></cell><cell>file ID</cell><cell>Num of records</cell></row><row><cell>P1</cell><cell>a.a-e.f</cell><cell>1105668</cell></row><row><cell>P2</cell><cell>e.g-i.l</cell><cell>1105702</cell></row><row><cell>P3</cell><cell>i.m-m.r</cell><cell>1105596</cell></row><row><cell>P4</cell><cell>m.s-q.x</cell><cell>1105645</cell></row><row><cell>P5</cell><cell>q.y-w.d</cell><cell>1381973</cell></row><row><cell>P6</cell><cell>w.e-z.z</cell><cell>1105608</cell></row><row><cell>total</cell><cell></cell><cell>6910192</cell></row></table><note coords="3,258.23,659.07,135.12,10.46;3,104.94,691.17,417.07,10.46;3,90.00,703.12,432.00,10.46;3,90.01,715.08,170.05,10.46;3,104.94,724.20,412.30,10.46"><p><p><p><p>2006 Legal corpus split schema</p>All partitions shared same extraction pattern and okapi indexing schema. For each document in the corpus, eleven contents were abstracted from source collection if it was presented. Abstraction and indexing schema were shown in 2.</p>More detailed discussion about XML elements were given on the 2006 legal track Web site</p><ref type="bibr" coords="3,506.72,724.20,10.52,10.46" target="#b1">[2]</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,189.77,278.72,232.46,10.46"><head>Table 2 :</head><label>2</label><figDesc>2006 Legal abstraction and indexing schema</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.01,343.72,432.00,160.90"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="7,463.68,343.72,58.33,10.46"><row><cell>are generated</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,224.47,516.02,163.05,10.46"><head>Table 4 :</head><label>4</label><figDesc>2006 Legal York runs' result</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,303.43,416.50,10.46;8,105.50,315.38,274.88,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,223.80,303.43,230.66,10.46">Improving retrieval performance by relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,461.53,303.43,60.47,10.46;8,105.50,315.38,179.83,10.46">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,335.31,333.00,10.46" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,105.50,335.31,92.14,10.46">Legal discovery track</title>
		<ptr target="http://trec-legal.umiacs.umd.edu" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,355.23,416.50,10.46;8,105.50,367.19,43.72,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,124.19,355.23,130.90,10.46">Efthimiadis Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,275.38,355.23,207.77,10.46">Annual Review of Info. Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="121" to="187" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,387.11,416.50,10.46;8,105.50,399.07,22.69,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,180.75,387.11,164.35,10.46">On term selection for query expansion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,352.82,387.11,114.08,10.46">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,418.99,416.50,10.46;8,105.50,430.94,233.63,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,271.06,418.99,152.93,10.46">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,431.38,418.99,90.62,10.46;8,105.50,430.94,157.24,10.46">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,450.87,416.50,10.46;8,105.50,462.83,297.93,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,241.95,450.87,280.05,10.46;8,105.50,462.83,129.67,10.46">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,300.41,462.83,39.70,10.46">SIGIR 94</title>
		<meeting><address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,482.76,416.50,10.46;8,105.50,494.71,395.88,10.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,274.28,482.76,184.56,10.46">Simple, proven approaches to text retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<idno>no. 356</idno>
		<imprint>
			<date type="published" when="1994">1994 updated 1996,1997. 2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,105.50,514.64,416.52,10.46;8,105.50,526.59,416.50,10.46;8,105.50,538.54,242.72,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,280.11,514.64,241.91,10.46;8,105.50,526.59,74.69,10.46">A multi-system analysis of document and term selection for blind feedback</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,188.15,526.59,333.85,10.46;8,105.50,538.54,168.53,10.46">Proceedings of the thirteenth ACM international conference on Information and knowledge management CIKM &apos;04</title>
		<meeting>the thirteenth ACM international conference on Information and knowledge management CIKM &apos;04</meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,558.47,416.52,10.46;8,105.50,570.42,253.36,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,222.02,558.47,300.00,10.46;8,105.50,570.42,49.83,10.46">Using probabilistic models of information retrieval without relevance information</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,163.18,570.42,114.33,10.46">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="285" to="295" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,590.35,411.52,10.46;8,105.50,602.30,277.46,10.46" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,224.71,590.35,297.29,10.46;8,105.50,602.30,246.83,10.46">York University at TREC 2004: HARD and genomics Tracks The proceedings of the Thirteenth Text REtrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
