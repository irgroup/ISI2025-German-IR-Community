<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,207.66,104.18,43.03,10.80"><forename type="first">Min</forename><surname>Wu</surname></persName>
							<email>minwu@cs.albany.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">ILS Institute University at Albany SUNY</orgName>
								<address>
									<addrLine>1400 Washington Ave. Albany</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.70,104.18,105.57,10.80"><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILS Institute University at Albany SUNY</orgName>
								<address>
									<addrLine>1400 Washington Ave. Albany</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C6A09E54F6D05B73D3C2DFFB4CCB45DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ILQUA At TREC 2006 1 Introduction</head><p>This year, we made changes to the passage/sentence retrieval component of ILQUA in handling factoid and list questions. All the other components remain same.</p><p>ILQUA is an IE-driven QA system. To answer "Factoid" and "List" questions, we apply our answer extraction methods on NE-tagged passages or sentences. The answer extraction methods adopted here are surface text pattern matching, n-gram proximity search, and syntactic dependency matching. Although surface text pattern matching has been applied in some previous TREC QA systems, the patterns used in ILQUA are better since they are automatically generated by a supervised learning system and represented in a format of regular expressions which contain multiple question terms. In addition to surface pattern matching, we also adopt n-gram proximity search and syntactic dependency matching. N-grams of question terms are matched around every named entity in the candidate sentences or passages and a list of named entities are generated as answer candidate. These named entities then go through a multi-level syntactic dependency matching component until a final answer is generated. To answer "Other" questions, we parsed the answer sentences of "Other" questions in previous main task and built syntactic patterns combined with semantic features. These patterns are later applied to the parsed candidate sentences to extract answers of "Other" questions.</p><p>Figure <ref type="figure" coords="1,114.50,637.97,5.49,9.57" target="#fig_0">1</ref> shows the diagram of the ILQUA architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Analysis</head><p>ILQUA classifies questions by syntactic structure and answer target. This year, we use Stanford parser instead of the ME (Maximum Entropy) parser which we applied in our previous ILQUA systems.</p><p>Syntactic chunking splits questions into a list of question terms with syntactic tags. For example, the question "When were the first postage stamps issued in the United States" is chunked with the syntactic structure of "When_Be_NP_VP_NP". However, certain questions with special answer patterns are not chunked in this manner. They are categorized as "Born_When", "Born_Where", "Die_When", "Die_Where", "Abbreviation", etc.</p><p>Since the answer extraction is applied on NEtagged passages, the answer targets of questions are classified into named entity types. We use BBN's Identifinder that can identify 24 types of named entities to annotate AQUAINT corpus. In addition, we developed a small NE tagger to annotate three types of named entities that are not included in Identifinder's list. The following shows all the named entity types that our system can process.</p><p>For questions beginning with the words "When", "Where" and "Who", the answer target assigned is "Date", "Location" and "Person/Organization", respectively. For questions beginning with pattern "How+Adj.", there are handcrafted rules to assign answer target. For questions beginning with "What_Be", "What_Entity", "Which_Be", "Which_Entity", a key term of noun phrase is extracted from the question and mapped to appropriate answer target type. We set up a noun-target map of 7885 entries to map noun to named entity type which can be processed by the system. The assigned entity type is set as the major answer target type while the noun is set as the minor answer target type. For example, if the major answer target is "Quantity", the minor answer target could be "age", "distance", "height", "speed", etc. This two-level answer target categorization is helpful to answer validation.</p><p>Query expansion is done with the aid of Word-Net to find the morphological forms and synonyms of verbs. Some common verbs are filtered out to increase the retrieval precision. We didn't use the noun synonyms to expand the query mainly because the noise introduced by some synonyms would reduce the retrieval precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Passage/Sentence Retrieval</head><p>The IR engine we used is Inquery developed at University of Massachusetts at Amherst. The top 100 documents retrieved by Inquery are tagged and segmented into passages. These passages are then filtered by answer target types, question terms, and topic terms. Passages without named entity of answer target type are filtered out. All the NE tags in the passages, except the tags of answer target, are filtered out for later processing.</p><p>In our previous ILQUA system, the answer selection component was designed to work on passage level based on the observation that in many occasion answers to questions and question terms occur in consecutive sentences of same passage. However in some cases, this passage-level answer selection gave wrong answer candidate. So this year we introduced two-phase answer selection: we applied answer selection on candidate sentences first; in case the selected answer score was too low, we applied the answer selection on candidate passages as well. Accordingly the information retrieval component in this year's ILQUA prepares </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Surface Text Pattern Matching</head><p>Surface text pattern matching has been adopted by some researchers <ref type="bibr" coords="3,170.44,159.30,123.45,9.57" target="#b14">(Ravichandran &amp; Hovy 2002</ref><ref type="bibr" coords="3,293.89,159.30,4.95,9.57;3,72.00,171.96,65.39,9.57">, Soubbotin 2002</ref>) in building QA system during the last few years. The patterns used in ILQUA are automatically learned and extracted. They are sorted according to question types and can handle more anchor terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pattern Format</head><p>Patterns are represented as regular expressions with terms of "NP", "VP", "VPN", "ADVP", "be", "in", "of", "on", "by", "at", "which", "when", "where", "who", ",", "-", "(", etc. Since most TREC questions contain more than one noun phrase, we numbered these noun phrases according to their occurring order in the question. The following gives some sample patterns of question type "when_do_np_vp_np".</p><p>When applying these patterns to specific question, the terms such as "NP", "VP", "VPN", "ADVP" and "be" should be replaced with the corresponding question terms. The replaced patterns can be matched directly to the candidate passages and answer candidate can be extracted quickly with Java tools. The number of patterns depends on the specific question type. Some question have up to 500 patterns. Only patterns with score greater than some empirically determined threshold are applied in pattern matching.</p><p>Patterns are sorted by question types and stored in pattern files. We have patterns for more than 50 question types. Figure <ref type="figure" coords="3,181.94,697.49,5.49,9.57">2</ref> lists question types of "When" questions. Although adverb phrases as question terms are necessary in the patterns, to simplify categorization, we didn't include them in question type labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pattern Extraction and Supervised Iterative Optimization</head><p>We used TREC11, TREC12 and TREC13 questions and answers as sample question-answer pairs. These questions are classified into question types. Questions with complex syntactic structure are omitted and questions whose answer target cannot be tagged by ILQUA are skipped too. Some frequently asked question types contain more sample questions than others. In order to overcome the data sparseness problem, we formulated some sample question-answer pairs for those question types with fewer questions.</p><p>The sample question is analyzed by the question analysis component of ILQUA and the expanded query includes answers to this question. In addition to retrieving documents from AQUAINT corpus, we also mined the web data. For each questionanswer pair, we chose the top fifty documents retrieved by Google. We did not do deep mining beyond 50 documents, as it does not add to performance. Some questions with more than one correct answer will be retrieved many times with each answer in the query from web. The retrieved documents are tagged with Identifinder and filtered. The filtered passages are prepared as input of pattern extraction.</p><p>For every named entity in the passage, a pattern is extracted and validated. If the currently extracted pattern is new, it is appended to the pattern list that contains all the patterns extracted so far. The activation count of the pattern is thus increased by one. If the pattern is generated by the correct answer, the correct activation count is increased by one. Figure <ref type="figure" coords="4,106.64,429.29,5.49,9.57" target="#fig_1">3</ref> shows how patterns are extracted and Figure <ref type="figure" coords="4,103.94,441.95,5.49,9.57">4</ref> shows how to score the extracted patterns by their precision and frequency. During the experiment, it was observed that some rare patterns get high precision scores. This is caused by data sparseness. We introduced precision tuning parameter ε here to adjust the precision score.</p><p>Rather than stopping at only automatic pattern extraction, we found that supervised iterative optimization is necessary to get more accurate pattern distribution. In our experiment, we applied the patterns with score greater than 0.5 to the sample questions and examined the answer extracted. For some questions, correct answers not included in the training question-answer pairs were found. For example, question "When was Jerusalem invaded by the general Titus" was processed with "70 A.D.", "A.D.70" and "70 Ad" as answers. However, among the answers extracted by the learned patterns, we found answer "70AD" occur several times. Such case occurred frequently in our experiment. So we added the newly extracted answer to the training question-answer pairs and retrained the system. Retraining is done until no new patterns are found.</p><p>Finally, patterns are manually refined by merging similar patterns and removing bad patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">N-gram Syntactic Dependency Matching</head><p>Proximity search as an IR method has been used in QA too <ref type="bibr" coords="4,360.74,617.58,120.42,9.57" target="#b7">(Han, Chung and Kim 2004</ref>). To answer questions whose answer cannot be extracted by surface text pattern matching, we applied a combined method of n-gram proximity search and syntactic dependency matching. N-gram proximity search is an effective method to quickly filter out irrelevant information and focus the answer selection on viable candidates. </p><formula xml:id="formula_0" coords="4,326.64,340.25,182.19,167.50">ε + ≈ i i i A C Pr ε = 0 if A i ≥5; ε = 1 if 3≤A i ≤4; ε = 2 if A i ≤2 Frequency: ∑ = = n k k i i A A F 1 Final Score: ) 1 ( Pr i i i F S + × = Figure 4 Pattern Scoring</formula><p>Around every named entity in the filtered candidate passages, question terms as well as topic terms are matched as n-grams. A question term is tokenized by word. We matched the longest possible sequence of tokenized word within the 100 word sliding window around the named entity. Once a sequence is matched, the corresponding word tokens are removed from the token list and the same searching and matching is repeated until the token list is empty or no sequence can be matched. The candidate named entity is scored by the average weighted distance score of question terms and topic terms.</p><p>Let </p><formula xml:id="formula_1" coords="5,74.28,514.80,207.22,103.45">j i t t j i j i j i t t Num t t W t t W t t E d QTerm E D j i ∑ × = N QTerm E D E S N i i ∑ = ) , ( ) (</formula><p>The n-gram proximity search generates a list of named entities as answer candidates. The syntactic dependency matching component takes the top 20 as input and gives the final answer. Figure <ref type="figure" coords="5,262.83,671.82,5.49,9.57">5</ref> shows the top 5 answer candidates of question "When was the first Burger King restaurant opened?" after the n-gram proximity search.</p><p>We use MINIPAR (DeKang Lin) to parse the question and the sentences containing the answer candidates. The syntactic relation triples in the question are matched one by one against the parsed sentences. We use the dependency-based word similarity list (developed by DeKang Lin) to match the synonyms or highly related words. To improve the matching accuracy, we introduced the forward matching propagation and the backward matching propagation. If there are two syntactic relations A:R1:B and B:R2:C in the question, suppose A:R:B is not matched against any relations in the parsed sentence, the forward propagation will consider the relation A:R1:C or A:R2:C. Suppose A:R1:B is matched with the parsed sentence and B:R2:C is not matched with any relations in the parsed sentences, the matching score of A:R1:B will be adjusted according to the rule of backward propagation.</p><p>The parsed dependency relation triples of the question in the Figure <ref type="figure" coords="5,412.19,694.49,5.49,9.57">5</ref> are listed as follows.</p><p>When was the first Burger King restaurant opened?</p><p>1. The number of Burger King fast-food restaurants have reached 100 throughout Turkey since first was opened in 1995, reported the Anatolia News Agency on Sunday. 2. Coke has supplied Burger King for most of the restaurant chain's history, starting with the first Burger King that opened in Miami in 1954. 3. ``The company chose an available Pillsbury pancake mix brand name, Hungry Jack's, in its place. . . . Burger King opened its first four company-owned restaurants (under the Burger King name) in Sydney, New South Wales . . . in December 1997.'' 4. why some BK look-alike restaurants in Australia are named Hungry Jack's while others bear the Burger King moniker. ``The Burger King brand name was not available for use by Burger King Corp. in 1971,'' BK says. ``The company chose an available Pillsbury pancake mix brand name, Hungry Jack's, in its place. . . . Burger King opened its first four company-owned restaurants ..... 5. When the recall was first announced Dec. 27, Burger King placed an ad in USA Today, posted signs in its restaurants and sent out notices to 56,000 pediatricians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5 Answer Candidate List</head><p>There are two main syntactic dependency relations: the first relation is between "when" and "open" and the second relation is between "open" and "the first Burger King restaurant".</p><p>The relations are then matched with the parsed sentences in the Figure <ref type="figure" coords="6,177.49,269.22,4.12,9.57">5</ref>. In the first round of syntactic matching, answer candidates "1995", "1954" and "December 1997" are matched. In the second round of matching, answer candidate "1954" get higher score because "the first Burger King" is more close to the question term "the first Burger King restaurant". So the final answer will be "1954".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answering Definition Questions with Syntactic-Semantic Patterns</head><p>We applied syntactic patterns to answer "Other" questions. The patterns are learned from the previous TREC QA topics and answer sentences.</p><p>There are 65 topics in 2004 QA main task. We split the topics into two sets for training and testing. For each topic, the answer nuggets and a list of sentences containing answer nuggets (created by Ken Litkowski) are provided on TREC website.</p><p>The answer sentences are parsed and the parse trees are bottom-up traversed. At each level of the parse tree, the answer nuggets are compared and the syntactic patterns are extracted if the nuggets are matched. The syntactic patterns are scored according to the answer nuggets matched. If the "Vital" answer nuggets are matched, the syntactic patterns are assigned higher scores. After the pattern extraction, we found that some common syntactic patterns such "NP VP", "NP NP", "NP PP" get high scores. These common patterns extract useful information as well as a lot of irrelevant information. To address this problem, we append semantic features to the patterns. These semantic features include comparative adjectives, digits, topic related verbs and topic phrases.</p><p>These syntactic patterns with semantic features are applied to the test questions. The results are compared with the answer nuggets. The scores of the patterns are adjusted. The patterns that extract more "Vital" or "OK" information get higher scores and patterns which extract more irrelevant information get lower score or be removed. Finally we kept 34 patterns. Here are some sample patterns:</p><p>VBD PP PP_t PP_d NP JJS NN NN_t NP JJS NN NNS_t</p><p>When answer "Other" question, we select the top 7 documents retrieved from the "Factoid" and "List" questions in the topic series. The documents are split into sentences and filtered by topic key words.</p><p>The candidate sentences are parsed and the parse trees are traversed bottom-up to do pattern matching. Perfect match is not always guaranteed. The matching score is calculated according to how well the semantic features are matched. The final score is the product of the pattern score and matching score. Redundancy filtering removes the duplicate information nuggets and the information nuggets with length greater than 125 bytes are filtered out. Finally we chose the top 30 nuggets as the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments and Evaluation Results</head><p>We submitted one run this year and Table <ref type="table" coords="6,534.58,500.82,5.49,9.57">1</ref> shows the evaluation result.</p><p>Compared with last year's evaluation result, the system performance is lowered. Except the F-score of "List" questions improved by 1%, the accuracy of "Factoid" questions dropped nearly 4% and the F-score of "Other" questions dropped nearly 8%. We think the decreasing of "Factoid" questions accuracy was caused mainly by the more difficult questions and more strict evaluation standard because the number of "inexact" answers of this year witnessed an increasing of 25%. However, the obviously decreasing of F-score of "Other" questions shows the incompleteness caused by the syntacticsemantic patterns. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,245.28,441.67,128.83,7.18"><head>Figure 1</head><label>1</label><figDesc>Figure 1 ILQUA System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,235.92,262.89,154.72,8.10;4,326.64,280.98,122.61,9.02;4,326.64,292.50,149.35,9.02;4,326.64,303.96,179.62,9.02;4,326.64,327.12,84.79,9.02"><head>Question:Figure 3</head><label>3</label><figDesc>Figure 3 Pattern Extraction Illustration Pi (i=1,2,... n) ---pattern list Ai (i=1,2,... n) ---activation counts Ci (i=1,2,... n) ---correct activation counts Expected Precision:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,239.40,226.93,327.01"><head></head><label></label><figDesc>Num(t i ...t j ) denotes the number of all matched n-grams, d(E, t i ...t j ) denotes the word distance between the named entity and the matched ngram, W1(t i ...t j ) denotes the topic weight of the matched n-gram, W2(t i ...t ) denotes the length weight of the matched n-gram. If t i ...t j contains topic terms or question verb phrase, 0.5 is assigned to W1, otherwise 1.0 is assigned. The value assigned to length weight W2 is determined by λ, the ratio value of matched n-gram length to question term length. How to assign the value of W2 is illustrated as follows.</figDesc><table coords="5,72.00,403.86,226.80,162.55"><row><cell></cell><cell></cell><cell cols="4">W2(t i ...t j )=0.4 if λ&lt;0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">W2(t i ...t j )=0.6 if 0.4 ≤λ&lt;0.6</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">W2(t i ...t j )=0.8 if λ&gt;0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">W2(t i ...t j )=0.9 if λ&lt;0.75</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">The weighted distance score D(E,QTerm) of the</cell></row><row><cell cols="10">question term and the final score S(E) of the named</cell></row><row><cell cols="6">entity are calculated as follows.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(</cell><cell>,</cell><cell>...</cell><cell>)</cell><cell></cell><cell cols="2">( 1</cell><cell>...</cell><cell>)</cell></row><row><cell>(</cell><cell>,</cell><cell>)</cell><cell>...</cell><cell></cell><cell>( 2</cell><cell>(</cell><cell>... ...</cell><cell>)</cell><cell>)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">when_be_np</head><p>When was the first space shuttle flight? When was the Hellenistic Age?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">when_be_np_pp_np</head><p>When was the U.S. invasion of Haiti? When was the battle of Shiloh?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">when_be_np_pos_np</head><p>When is Mexico's independence?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">when_be_np_vp</head><p>When was "Cold Mountain" written? When was Carlos the Jackal captured?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">when_be_np_pp_np_vp</head><p>When was the battle of Chancellorsville fought? When was the city of New Orleans founded?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">when_be_np_vp_pp_np</head><p>When was Jim Inhofe first elected to the senate? When was the Panama Canal returned to Panama?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">when_do_np_vp</head><p>When did the first American lighthouse open? When did the shuttle Challenger explode?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">when_do_np_vp_np</head><p>When did the United States enter the World War II? When did Amtrak begin operations? 9. when_do_np_vp_pp_np When did "The Simpsons" first appear on television? When did Jack Welch retire from GE?  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="3,101.91,391.84,136.07,7.85;3,83.64,402.16,156.32,7.85" xml:id="b0">
	<monogr>
		<title level="m" coord="3,101.91,391.84,66.04,7.85;3,205.03,391.84,32.96,7.85;3,83.64,402.16,86.29,7.85">&lt;\/Date&gt; NP1 VP NP2 on &lt;Date&gt;</title>
		<imprint/>
	</monogr>
	<note>VP NP2 in &lt;Date&gt;</note>
</biblStruct>

<biblStruct coords="3,97.18,412.48,165.12,7.85;3,83.64,422.86,164.91,7.85;3,83.64,433.18,169.35,7.85" xml:id="b1">
	<monogr>
		<idno>NP1 VP.{1</idno>
		<title level="m" coord="3,101.69,412.48,90.64,7.85;3,229.38,412.48,32.93,7.85;3,83.64,422.86,52.54,7.85;3,173.64,422.86,74.91,7.85;3,83.64,433.18,24.71,7.85">&lt;\/Date&gt;.{1,15}NP1 &lt;Date&gt;</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>{1,15}VP NP2 in &lt;Date&gt;</note>
</biblStruct>

<biblStruct coords="3,83.64,443.56,189.13,7.85;3,83.64,453.88,166.90,7.85" xml:id="b2">
	<monogr>
		<idno>NP1.{1</idno>
		<title level="m" coord="3,113.41,443.56,89.28,7.85;3,239.79,443.56,32.98,7.85;3,83.64,453.88,96.88,7.85">&lt;\/Date&gt; NP1.{1,15}NP2 on &lt;Date&gt;</title>
		<imprint/>
	</monogr>
	<note>15}VP.{1,30} on &lt;Date&gt;</note>
</biblStruct>

<biblStruct coords="3,97.17,464.26,153.38,7.85;3,83.64,474.58,175.51,7.85;3,83.64,484.96,145.63,7.85;3,83.64,495.28,169.37,7.85;3,83.64,505.66,185.19,7.85;7,72.00,274.40,56.04,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,146.32,474.58,112.82,7.85;3,83.64,484.96,75.66,7.85">&lt;\/Date&gt;.{1,15}NP1.{1,50}VP NP1&apos;s NP2 in &lt;Date&gt;</title>
		<author>
			<persName coords=""><surname>{1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,217.59,464.26,32.95,7.85;3,83.64,474.58,25.07,7.85;3,196.34,484.96,32.92,7.85;3,83.64,495.28,24.97,7.85;3,146.06,495.28,106.96,7.85;3,83.64,505.66,115.21,7.85;3,235.90,505.66,32.93,7.85;7,72.00,274.40,56.04,10.80">&lt;\/Date&gt;.{1,15}NP1 VP NP2 NP1 VPN.{1,15}NP2 in &lt;Date&gt;</title>
		<imprint/>
	</monogr>
	<note>&lt;\/Date&gt; References</note>
</biblStruct>

<biblStruct coords="7,72.00,306.51,216.11,8.74;7,83.34,317.97,204.63,8.74;7,83.34,329.49,204.64,8.74;7,83.34,340.95,24.75,8.74;7,108.12,338.77,5.04,5.65;7,115.68,340.95,53.12,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,127.98,317.97,159.98,8.74;7,83.34,329.49,121.28,8.74">A Multi-Strategy and Multi-Source Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carrol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,227.92,329.49,60.06,8.74;7,83.34,340.95,24.75,8.74;7,108.12,338.77,5.04,5.65;7,115.68,340.95,22.47,8.74">Proceedings of the 11 th TREC</title>
		<meeting>the 11 th TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,358.47,216.07,8.74;7,83.34,369.37,204.61,9.36;7,83.34,381.45,204.79,8.74;7,83.34,392.97,35.55,8.74;7,118.92,390.79,5.04,5.65;7,126.48,392.97,53.08,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,87.73,369.37,200.22,9.36;7,83.34,381.45,130.72,8.74">National University of Singapore at the TREC 13 Question Answering Main Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,239.21,381.45,48.92,8.74;7,83.34,392.97,35.55,8.74;7,118.92,390.79,5.04,5.65;7,126.48,392.97,22.41,8.74">Proceedings of the 13 th TREC</title>
		<meeting>the 13 th TREC</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,410.43,216.04,8.74;7,83.34,421.95,204.74,8.74;7,83.34,433.41,204.78,8.74;7,83.34,444.93,35.55,8.74;7,118.92,442.75,5.04,5.65;7,126.48,444.93,53.08,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,221.97,421.95,66.10,8.74;7,83.34,433.41,131.39,8.74">Multiple-Engine Question Answering in TextMap</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Melz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,239.21,433.41,48.91,8.74;7,83.34,444.93,35.55,8.74;7,118.92,442.75,5.04,5.65;7,126.48,444.93,22.41,8.74">Proceedings of the 12 th TREC</title>
		<meeting>the 12 th TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,462.39,216.14,8.74;7,83.34,473.91,204.69,8.74;7,83.34,485.37,204.65,8.74;7,83.34,496.89,54.79,8.74;7,138.12,494.71,5.04,5.65;7,145.62,496.89,53.14,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,177.60,473.91,110.43,8.74;7,83.34,485.37,119.90,8.74">Korea University Question Answering System at TREC</title>
		<author>
			<persName coords=""><forename type="first">K.-S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-C</forename><surname>Rim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
		</imprint>
	</monogr>
	<note>In Proceedings of the 13 th TREC</note>
</biblStruct>

<biblStruct coords="7,72.00,514.35,216.22,8.74;7,83.34,525.87,204.73,8.74;7,83.34,537.33,204.72,8.74;7,83.34,548.85,134.77,8.74;7,218.10,546.67,5.04,5.65;7,225.66,548.85,53.00,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,205.64,525.87,82.43,8.74;7,83.34,537.33,204.72,8.74;7,83.34,548.85,40.25,8.74">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,145.86,548.85,72.24,8.74;7,218.10,546.67,5.04,5.65;7,225.66,548.85,22.41,8.74">Proceedings of 12 th TREC</title>
		<meeting>12 th TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,566.31,216.01,8.74;7,83.34,577.83,204.74,8.74;7,83.34,589.29,81.98,8.74;7,165.30,587.11,5.04,5.65;7,172.86,589.29,53.14,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,119.23,577.83,148.69,8.74">Question Answering in Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,83.34,589.29,81.98,8.74;7,165.30,587.11,5.04,5.65;7,172.86,589.29,22.50,8.74">Proceedings of the 9 th TREC</title>
		<meeting>the 9 th TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,606.81,216.05,8.74;7,83.34,618.27,204.76,8.74;7,83.34,629.79,204.64,8.74;7,83.34,641.25,204.68,8.74;7,83.34,652.77,82.80,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,162.51,606.81,125.53,8.74;7,83.34,618.27,181.06,8.74">Selectively Using Relations to Improve Precision in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,83.34,629.79,204.64,8.74;7,83.34,641.25,204.68,8.74;7,83.34,652.77,29.91,8.74">Proceedings of the EACL-2003 Workshop on Natural Language Processing for Question Answering</title>
		<meeting>the EACL-2003 Workshop on Natural Language Processing for Question Answering</meeting>
		<imprint>
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,670.23,216.05,8.74;7,83.34,681.75,204.70,8.74;7,83.34,693.21,204.66,8.74;7,83.34,704.73,22.58,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,190.56,670.23,97.49,8.74;7,83.34,681.75,204.70,8.74;7,83.34,693.21,63.50,8.74">Logical Form Transformation of WordNet and its Applicability to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,177.10,693.21,105.26,8.74">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,324.00,262.05,216.07,8.74;7,335.34,273.51,204.61,8.74;7,335.34,285.03,177.72,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,516.48,262.05,23.59,8.74;7,335.34,273.51,164.95,8.74">Question-Answering by Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,522.75,273.51,17.19,8.74;7,335.34,285.03,94.95,8.74">Proceedings of SIGIR 2000</title>
		<meeting>SIGIR 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,324.00,302.49,216.07,8.74;7,335.34,314.01,204.74,8.74;7,335.34,325.47,204.76,8.74;7,335.34,336.99,69.98,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,503.96,302.49,36.11,8.74;7,335.34,314.01,204.74,8.74;7,335.34,325.47,101.33,8.74">Question Answering Using Constraint Satisfaction: QA-By-Dossier-With-Constraints</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,462.01,325.47,78.09,8.74;7,335.34,336.99,39.41,8.74">Proceedings of the 42nd ACL</title>
		<meeting>the 42nd ACL</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,324.00,354.45,216.09,8.74;7,335.34,365.97,204.68,8.74;7,335.34,377.43,83.08,8.74;7,418.44,375.25,5.04,5.65;7,425.94,377.43,47.84,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,469.24,354.45,70.85,8.74;7,335.34,365.97,195.61,8.74">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,346.19,377.43,72.23,8.74;7,418.44,375.25,5.04,5.65;7,425.94,377.43,16.91,8.74">Proceedings of 40 th ACL</title>
		<meeting>40 th ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,324.00,394.95,216.01,8.74;7,335.34,406.41,204.69,8.74;7,335.34,417.93,115.32,8.74;7,450.66,415.75,5.04,5.65;7,458.16,417.93,53.14,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,466.16,394.95,73.85,8.74;7,335.34,406.41,204.69,8.74;7,335.34,417.93,21.28,8.74">Patterns of Potential Answer Expressions as Clues to the Right Answers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,378.46,417.93,72.20,8.74;7,450.66,415.75,5.04,5.65;7,458.16,417.93,22.50,8.74">Proceedings of 11 th TREC</title>
		<meeting>11 th TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,324.00,435.39,216.02,8.74;7,335.34,446.91,204.68,8.74;7,335.34,458.37,131.73,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,389.81,435.39,150.21,8.74;7,335.34,446.91,181.72,8.74">Using Question Series to Evaluate Question Answering System Effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,335.34,458.37,102.21,8.74">Proceedings of HLT 2005</title>
		<meeting>HLT 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,324.00,475.89,216.01,8.74;7,335.34,487.35,204.65,8.74;7,335.34,498.87,159.41,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,335.34,487.35,204.65,8.74;7,335.34,498.87,13.09,8.74">ILQUA -An IE-Driven Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,366.16,498.87,98.46,8.74">Proceedings of TREC-14</title>
		<meeting>TREC-14</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,324.00,516.33,216.07,8.74;7,335.34,527.85,204.70,8.74;7,335.34,539.31,160.32,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,442.95,516.33,97.11,8.74;7,335.34,527.85,147.09,8.74">Utilizing Co-occurrence of Answers in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,504.46,527.85,35.58,8.74;7,335.34,539.31,91.18,8.74">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006-07">2006. July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
