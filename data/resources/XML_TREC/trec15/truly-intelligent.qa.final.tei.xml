<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,179.40,56.87,285.01,23.98">TREC 2006 Q&amp;A Factoid: TI Experience</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,102.00,99.77,71.20,9.00"><forename type="first">Satish</forename><surname>Balantrapu</surname></persName>
							<email>satishrao.balantrapu@trulyintelligent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Division</orgName>
								<orgName type="institution">TrulyIntelligent Technologies Pvt. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.90,99.77,50.00,9.00"><forename type="first">Monis</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Division</orgName>
								<orgName type="institution">TrulyIntelligent Technologies Pvt. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,443.80,99.77,82.98,9.00"><forename type="first">Ayyappa</forename><surname>Nagubandi</surname></persName>
							<email>ayyappa@trulyintelligent.com</email>
							<affiliation key="aff2">
								<orgName type="department">Artificial Intelligence Division</orgName>
								<orgName type="institution">TrulyIntelligent Technologies Pvt. Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,179.40,56.87,285.01,23.98">TREC 2006 Q&amp;A Factoid: TI Experience</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7294A8256D15D7164FF73E77D00B1AB1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the first attempt of Artificial Intelligence Division of TrulyIntelligent Technologies Pvt. Ltd. at the TREC2006 Question Answering track. As any Question Answering (QA) system typically involves Question Analysis, Document Retrieval, Answer Extraction and Answer Ranking and this being our first attempt and with certain constraints of time and resources, we developed some modules of our QA system in line with already existing approaches, for example document retrieval, pattern based answer extraction and web boosting. But there are areas where we tried our ideas and got quite encouraging results particularly, Question Analysis, Constraint based Answer Extraction and Answer Ranking which are described in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Question Answering System:</head><p>In this year's QA Track, we participated in the Main task. We put the most effort in Factoid questions and very little on List and Other questions. Our system basically has two QA cores, one is typically pattern based and the other one implements constraint based heuristics for retrieving the final answer. If pattern based core fails to retrieve answer then only constraint based core is applied. Both these cores share modules but differ from answer extraction part. As this being our first attempt and with the time constraint, we used different tools like GATE, MINIPAR, LUCENE, WORDNET, STANFORD TAGGER, LT CHUNKER and GOOGLE API of different Universities/Organizations for developing our QA system. We extracted the Target text, Question ID, Question type and Question from the given XML file. Then the question is mapped onto the "MAP_QUESTIONS" file which is an XML file containing the answer type of the question, the Lucene search pattern and Google search pattern. The keywords for Lucene search pattern and Google search pattern are obtained from the target text. If the target text is Named Entity, the target text itself forms the Lucene and Google search patterns. On the other hand, if the target text is an event, Minipar is run on the target text and extracts the nouns of the event which form the Lucene and Google search patterns. Thus, the system is integration of two different approaches one is pattern based (offline system as decision for answer extraction is taken before hand i.e. when pattern are prepared manually) and another is online system, where answer extraction is done dynamically using various scoring mechanisms. To see distinction between two, let us look at modules that a vanilla QA system consists:</p><p>1 .Question Analysis 2. Document Retrieval 3. Answer Extraction 4. Answer Ranking. The online system differs in last two modules i.e. Answer extraction and ranking. We will describe each module separately and mark differences for offline and online system and then we will picture our overall system. An index is developed for the available documents using Lucene and the documents are stored in the database. The keywords of the question and questions series target (generated by Question Analysis module) are sent to the index and in turn obtain the relevant document numbers. These document numbers are used to retrieve actual documents from database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Question Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Answer Extraction:</head><p>At this step offline (pattern based approach) and online system (constraint based heuristics approach) branch out, both adapt completely different approaches to extract potential answers, we will describe each separately.</p><p>Offline Answer Extraction: </p><formula xml:id="formula_0" coords="3,56.70,410.12,25.43,16.55">After</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Text Snippet:</head><p>ALBANY, N.Y. _ We`re making much of the turn of the century this year. With William Shakespeare`s birthday coming up on April 23, an Academy Awardwinning film about him playing in the theaters, and Harold Bloom`s new book, ``Shakespeare: The Invention of the Human,`` on bestseller lists around the country, let`s pause for a moment to consider the turbulent times that Shakespeare faced in the waning days of the 16th century. When Shakespeare was born in 1564, Queen Elizabeth had been on the throne for six years, and she would remain the only monarch Shakespeare knew for most of his life. It was a heady time in England. In 1588, the small navy of that small island had defeated Spain, the greatest sea power on Earth. The religious conflicts that plagued the continent and even plagued Britain before Elizabeth`s reign were largely absent due to the queen`s shrewd tolerance of religious differences…………………</p><p>The pattern: "Shakespeare\W*.{0,50}born.{0,50}" extracts the sentence "Shakespeare was born in 1564, Queen Elizabeth had been on the throne f" from which the required Answer type (Date in this case) is taken as the possible answer.</p><p>With target text as the Google search pattern, the top ranked document's text is extracted from the web and again the above patterns are applied on these documents to extract the environments. Now using web boosting strategy, the final right answer is declared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online answer extraction:</head><p>Before extracting the possible answers of the given question, potential sentences containing possible answers are extracted. In sentence extraction step following sentence pruning conditions are applied:</p><p>1. sentences containing minimum number(two in our experiment) of keywords, 2. the required Answer Type NE should be in sentence and 3. most important keyword should be in sentence . This is done by semantic ranking keywords with Answer Type. Now from the final list of sentences we extract all those NE s which match our Answer Type, these become our potential answers.</p><p>Finally we have list of potential answers, extracted from the documents. From both offline and online systems we have list of potential answers, these answers are passed to answer ranking module to decide actual answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Answer Ranking</head><p>Like answer extraction module, we have different approaches for offline and online answer ranking, but one common technique that both systems share for answer ranking is web boosting. So we will first discuss web boosting, then we will go on describing the actual answer ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web boosting:</head><p>After extracting the potential answers, we go to web and apply same process of answer extraction described above to the documents retrieved from the web and get potential answers from the web. Now we have, a. potential answers from the documents b. potential answers from the web.</p><p>Web boosting technique is used to find evidence for the right answer, based on the assumption 'the frequency of right answer will be more as compared to wrong ones'. So in web boosting step we add frequencies of 1 and 2 for all answers, thus getting the boosted list of potential answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offline answer ranking:</head><p>Ranking in this case is quite simple. We web boost the potential answers extracted from documents and declare highest frequency answer as the correct answer.</p><p>Online Answer Ranking:</p><p>The rank of potential answer is calculated based on three parameters 1 F, frequency of answer (web boosted frequency) 2 R, rank no. of index -assumption being that indexer will retrieve more relevant docs first. 3 K, The no. of keywords matched. RANK OF POTENTIAL ANSWER = F + (1/R) + K all parameters are normalized to 1. The highest ranked potential answer is declared as correct answer. Now we have seen each module separately, let us look at the overall architecture of system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Overall Architecture:</head><p>As seen in Figure <ref type="figure" coords="5,147.80,560.82,4.50,16.55">1</ref>, once question analysis and document retrieval is done the offline answer extraction and ranking try to obtain answer, but if offline system fails to get answer we run online system. The reason for running offline system before online system is that, since offline system uses manually prepared patterns, if it is able to get answer there is a greater possibility that answer will be exact. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results TIQA2006</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Conclusions and Future work</head><p>As this being our first attempt, we concentrated mainly in answering Factoid questions and give little importance for List and Other questions. In the future, we have to come up with new approaches for answering List and Other questions along with improving the existing approach for Factoid questions. Another area where we want to improve is Named Entity identifier so that we can identify quantities, more detailed job titles etc. We would like to incorporate logical prover in our system for answer ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. References</head><p>Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael Junk, and ChinYew Lin. 2000. Question answering in Webclopedia. In Voorhees, E. and Buckland, L.P., <ref type="bibr" coords="6,493.10,423.82,55.89,16.55">Eds. (2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the Thirteenth Text Retrieval</head><p>Conference TREC 2004.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,320.40,56.66,59.87,16.78;6,405.90,56.66,9.34,16.78;6,440.80,56.66,14.73,16.78;6,481.10,56.66,26.50,16.78;6,533.20,56.66,20.74,16.78;6,320.40,73.46,152.70,16.78;6,320.40,106.92,234.80,16.55;6,320.40,123.42,234.80,16.55;6,320.40,139.86,233.54,16.91;6,320.40,156.66,157.60,16.78;6,320.40,190.12,234.83,16.55;6,320.40,206.72,234.85,16.55;6,320.40,223.16,233.54,16.78;6,320.40,239.96,119.60,16.78;6,320.40,273.42,234.77,16.55;6,320.40,289.92,234.83,16.55;6,320.40,306.36,233.54,16.91;6,320.40,323.16,171.60,16.78;6,320.40,356.92,233.60,16.55;6,320.40,373.36,233.34,16.78;6,320.40,390.16,55.33,16.78;6,437.40,390.16,29.25,16.78;6,528.30,390.16,27.10,16.78"><head></head><label></label><figDesc>Voorhees and Dawn M. Tice. 2000. Overview of the TREC9 question answering track. In Proceedings of the Ninth Text REtrieval Conference (TREC9). Ellen M. Voorhees. 2001. Overview of the TREC 2001question answering track. In Proceedings of the 2001 Text REtrieval Conference (TREC 200). Soubbotin and S.M. Soubbotin. 2001. Patterns of potential answer expressions as clues to the right answers. In Proceedings of the 2001 Text REtrieval Conference(TREC 2001). Voorhees, E. and Buckland, L.P., Eds. (2003). Proceedings of the Twelve Text Retrieval Conference TREC 2003.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,56.70,56.72,498.63,667.85"><head></head><label></label><figDesc>Based on the question, the subject, predicate or object is extracted by running Parser and its semantic closeness to different entities like Person, Organization, and Location etc. (qTarget List) is obtained by using WordNet. The closest entity to which it matches is considered as the Target of the question.</figDesc><table coords="2,56.70,124.42,498.63,598.65"><row><cell>answer type.</cell></row><row><cell>Answer Type: Location</cell></row><row><cell>2. Document Retrieval:</cell></row><row><cell>Analyzing the question involves two subtasks. a. Keyword extraction Keywords extracted are used for document retrieval. b. Answer Type identification Answer type is the category of the answer for the question asked. Answer type is extremely important information for answer extraction and ranking. a. Keyword Extraction: The keywords of the question are obtained by running Parts Of Speech (POS) tagger on the question. The Nouns, Verbs and Adjectives are considered as the keywords of the question. Example: Question: What was the date of the 1999 AllStar Game? Keywords**: date, 1999, allstar, game ** aux verbs are ignored Same keyword extraction technique is used for extracting keywords from question series target. Both keywords from question and question series target are used for document retrieval. b. Answer Type identification: Here, the question is analyzed to know of what it is asking i.e. whether the answer type of the question is a person, Organization, Location, Quantity, Date etc. (For Example, in general: When questions refer to Date, Who questions usually refer to Person, Where questions refer to Location etc.). Example: Question: Which country received the largest loan ever granted by the IMF? Parser Output (Minipar): fin C:whn:N country country N:det:Det which fin C:i:V receive receive V:subj:N country receive V:obj:N loan loan N:det:Det the loan N:mod:A large loan N:vrel:V grant grant V:modbefore:A ever grant V:obj:N loan grant V:bysubj:Prep by by Prep:pcompn:N IMF IMF N:det:Det the In case of which questions directly followed by noun phrase, we pick subject i.e. country in this case as the answer type. Semantic closeness to qTarget list: Below is the semantic closeness scores of each NE type with country Person 0.142857142857143 Location 0.333333333333333 Product 0.125 Number 0.142857142857143 Currency 0.111111111111111 Organization 0.25 Date 0.125 Occupation 0.125 Money 0.111111111111111 Title 0.125 Since location has got highest score it becomes</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
