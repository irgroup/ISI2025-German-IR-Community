<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.50,72.36,408.71,16.84">MonetDB/X100 at the 2006 TREC TeraByte Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.59,118.08,77.34,11.09"><forename type="first">S</forename><surname>Ándor H Éman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CWI Kruislaan</orgName>
								<address>
									<postCode>413</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.64,118.08,86.05,11.06"><forename type="first">Marcin</forename><surname>Zukowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CWI Kruislaan</orgName>
								<address>
									<postCode>413</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.87,118.08,73.31,11.06"><forename type="first">Arjen</forename><surname>De Vries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CWI Kruislaan</orgName>
								<address>
									<postCode>413</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.60,118.08,64.52,11.06"><forename type="first">Peter</forename><surname>Boncz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CWI Kruislaan</orgName>
								<address>
									<postCode>413</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.50,72.36,408.71,16.84">MonetDB/X100 at the 2006 TREC TeraByte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A459DE9DAA53E4F78F4D790F1F422A7F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Requirements of database management (DB) and information retrieval (IR) systems overlap more and more. Database systems are being applied to scenarios where features such as text search and similarity scoring on multiple attributes become crucial. Many information retrieval systems are being extended beyond plain text, to rank semi-structured documents marked up in XML, or maintain ontologies or thesauri. In both areas, these new features are usually implemented using specialized solutions limited in their features and performance.</p><p>Full integration of DB and IR has been considered highly desirable, see e.g. <ref type="bibr" coords="1,131.26,346.55,9.72,7.86" target="#b5">[5,</ref><ref type="bibr" coords="1,145.84,346.55,7.16,7.86" target="#b1">1]</ref> for some recent advocates. Yet, none of the attempts into this direction has been very successful. The explanation can be sought in what has been termed the 'structure chasm' <ref type="bibr" coords="1,175.89,377.94,9.21,7.86" target="#b8">[8]</ref>: database research builds upon the idea that all data should satisfy a pre-defined schema, and the natural language text documents of concern to information retrieval do not match this database application scenario. Still, the structure chasm does not explain why IR systems do not use database technology to alleviate their data management tasks during index construction and document ranking. In practice however, custom-built information retrieval engines have always outperformed generic database technology, especially when also taking into account the trade-off between run-time performance and resources needed.</p><p>To investigate the feasibility of running terabyte scale information retrieval tasks on top of a relational engine, our team from CWI participated in the 2006 TREC Terabyte Track, using its experimental MonetDB/X100 database system <ref type="bibr" coords="1,72.60,555.77,9.73,7.86" target="#b3">[3,</ref><ref type="bibr" coords="1,85.87,555.77,10.75,7.86" target="#b11">11]</ref>. This system, is designed for high performance on data-intensive workloads, whereof TREC-TB is an excellent example. Furthermore, we believe that standard relational algebra provides enough flexibility to express most IR retrieval models, and show that, by employing a hardwareconscious DBMS architecture, it is possible to achieve per-Text Retrieval Conference (TREC) November 2006, Gaithersburg, Maryland, USA.</p><p>formance, both in terms of efficiency and effectiveness, that is competitive with leading, customized IR systems. This notebook is organized as follows. Section 2 describes the distinguishing features of MonetDB/X100 that allow it to run large-scale data processing tasks efficiently. Section 3 then explains the process of indexing the TREC-TB collection, and the resulting relational schema. This is followed by a description of the TREC-TB runs we submitted, together with the hardware platforms used to run them. Effectiveness and efficiency results for these runs are then presented in Sections 6 and Section 7, respectively, before concluding in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MonetDB/X100 OVERVIEW</head><p>MonetDB/X100 is an experimental relational database kernel, optimized for high performance on data-and queryintensive workloads. It relies on the concept of vectorized incache query execution to achieve good CPU utilization <ref type="bibr" coords="1,543.63,407.82,9.21,7.86" target="#b3">[3]</ref>, and a column-oriented storage manager that provides transparent light-weight data compression <ref type="bibr" coords="1,470.83,428.75,14.33,7.86" target="#b11">[11]</ref> to improve I/Obandwidth utilization. An overview of the system architecture is presented in Figure <ref type="figure" coords="1,426.69,449.67,3.58,7.86" target="#fig_0">1</ref>.</p><p>Figure <ref type="figure" coords="1,347.50,470.59,4.61,7.86" target="#fig_0">1</ref> shows an operator tree, being evaluated within MonetDB/X100 in a pipelined fashion, using the traditional open(), next(), close() interface. However, each next() call within MonetDB/X100 does not return a single tuple, as is the case in most traditional DBMSs, but a vector of tuples. A vector is a unary array, containing a small slice of a single column. Vectorization of the iterator pipeline allows Mon-etDB/X100 primitives, which are responsible for computing core functionality such as addition and multiplication, to be implemented as simple loops over vectors. This results in function call overheads being amortized over a full vector of values instead of single tuple, and allows the compiler to produce data-parallel code that can be executed efficiently on modern CPUs. Furthermore, the size of a vector is chosen in such a way, that all vectors needed by a query fit the CPU cache. This way, we avoid materialization of tuples that are being passed from one operator to the next, minimizing main memory access overheads. Such a vectorized in-cache architecture allows MonetDB/X100 query evaluation to be orders of magnitude faster than existing technology on dataand query-intensive workloads.</p><p>The processing power of MonetDB/X100 can make the system extremely I/O-hungry on certain queries. If the database does not fit main memory, the only solution to this problem is to increase the available I/O bandwidth. This can be done by adding more hardware, or by optimizing the DBMSs buffer manager for bandwidth utilization. With respect to the latter, MonetDB/X100 employs a buffer manager, called ColumnBM, that relies on a column-oriented storage scheme to avoid reading unnecessary columns from disk. Further, the granularity of disk accesses is in blocks of several megabytes, to optimize for fast sequential I/O.</p><p>In MonetDB/X100 we take the point of I/O-bandwidth utilization even further, by integrating ultra light-weight RAM-CPU cache compression into our system. The idea is, that by reading compressed blocks from disk, we can increase the perceived I/O bandwidth, as the actual data size of a block after decompression is assumed to be larger than the compressed block read from disk. For such an approach to be applicable even in the context of RAID storage systems that are capable of delivering data at several hundreds of megabytes per second, it should be clear that we need decompression routines that are capable of producing uncompressed data at speeds in the order of several gigabytes per second. To reach such speeds, we recently introduced three novel compression algorithms, PFOR, PFOR-DELTA and PDICT <ref type="bibr" coords="2,104.50,690.26,13.51,7.86" target="#b11">[11]</ref>, that are designed to sacrifice some performance in terms of compression ratio, in exchange for fast decompressibility. Furthermore, these compression schemes are integrated into the DBMS in such a way, that data blocks are stored in compressed form in RAM, and data is only decompressed on-demand, at vector granularity, directly into the CPU cache, where it is fed directly into the operator pipeline, without writing the uncompressed data back to main memory, as can be seen in Figure <ref type="figure" coords="2,477.86,109.94,3.58,7.86" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INDEXING</head><p>Indexing the 426GB TREC-TB document collection, consisting of roughly 25 million web documents, entails three main phases: parsing, constructing an inverted index structure using relational tables, and index compression. Parsing is done using an external program, that scans the collection and filters out markup and stopwords. For the remaining text, the parser returns (docid, term) pairs (DT ) for each term it encounters, with terms being stemmed using a Porter stemmer <ref type="bibr" coords="2,393.82,229.03,9.21,7.86">[9]</ref>, converted to lowercase, and scrambled into 64bit integers. Our scrambling function produces a one-to-one mapping from its input string to its integer representation, as long as no other input string has the same thirteen character long prefix. This is achieved by iterating over the input string, and on each iteration multiplying the current integer result by twenty-seven and adding the i'th characters offset from the character 'a' + 1. Furthermore, the parser generates a unique docid identifier for each document encountered, and outputs it, together with the documents name and length (in number of terms).</p><p>To index the data, we used the inverted list data-structure, which can be easily represented as a relational table. To build this index from the DT output of the parser, the following relational query, which sorts and then aggregates on (term, docid) pairs, was used: The [term,docid,tf ] (T D) table, holds for each term, the IDs of the documents the term appears in (docid), and the number of times the term occurs within a given document (tf). The table is ordered on (term,docid), which allows the term column to be replaced by a range index onto [docid,tf ], and allows the occurrence lists of two arbitrary terms to be combined efficiently using merge-join. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Compression</head><p>As Table <ref type="table" coords="2,355.82,648.42,4.61,7.86" target="#tab_0">1</ref> shows, the full index (the D, T and TD tables), occupies approximately 29 GB uncompressed when we ignore the term column in T D, and replace it with a range index of negligible size. We applied MonetDB/X100's PFOR and PFOR-DELTA light-weight column compression algorithms <ref type="bibr" coords="2,346.21,700.73,14.33,7.86" target="#b11">[11]</ref> to reduce the total size of this index to roughly 9GB. The benefit of this is twofold. First of all, due to the  Values outside this range, are stored in uncompressed form, to make PFOR robust against outliers that would unnecessarily increase b. PFOR-DELTA is similar to PFOR, with the difference that PFOR-DELTA operates on the differences (deltas) between subsequent values in a column. This makes PFOR-DELTA well-suited for clustered or (partially) sorted columns. The rightmost column of Table <ref type="table" coords="3,109.60,549.32,4.61,7.86" target="#tab_0">1</ref> shows the reduction in storage requirements after applying compression.</p><p>Both PFOR and PFOR-DELTA are relatively generic database compression mechanisms; neither has been optimized for IR needs. Furthermore, a single column is always fully compressed with a fixed set of compression parameters, often resulting in suboptimal compression ratios. We are looking into ways of making compression more adaptive, and better suited for the IR domain, without loosing too much of our decompression efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BM25 Score Materialization</head><p>The [term,docid,tf ] (T D) structure presented in Section 3, together with per-document and per-term information, is in itself sufficient to compute a ranked document list for a given query. In our experiments, we use the Okapi BM25 formula for document ranking:</p><formula xml:id="formula_0" coords="3,325.43,83.22,230.49,20.56">S (D) BM 25 = X T ∈Q ωD,T<label>(1)</label></formula><formula xml:id="formula_1" coords="3,332.28,109.37,223.64,24.01">ωD,T = log( fD fT,D ) • (k1 + 1) • fD,T fD,T + k1 • ((1 -b) + b • |D| avgdl )<label>(2)</label></formula><p>Evaluation of this formula is rather expensive in terms of CPU time, and can be avoided. The score for each (term, docid) pair is independent of a query, and can thus be precomputed. This means, that we can extend the T D table with a score column, which contains the precomputed scores. MonetDB/X100 uses column-wise storage, and only reads those columns from disk that are actually needed for a query. So, in case the score column is used, the tf column is left untouched. However, as the BM25 scores are 32 bit floating point numbers, as opposed to the 5.91 bit tf values, the storage requirements for our index increase from 18 bits to 44 bits per tuple. To avoid such an increase in index size, we decided to replace the floating point scores by so-called score ranks <ref type="bibr" coords="3,365.86,276.20,9.21,7.86" target="#b2">[2]</ref>, which quantize the range of floating point numbers into small, compressed integer numbers. We used the following linear Global-By-Value quantization:</p><formula xml:id="formula_2" coords="3,379.00,311.73,114.72,20.82">ω D,T = - q • ωD,T -L U -L + + 1,</formula><p>where L and U are the minimum and maximum values of ωD,T in the entire collection. The formula produces integer numbers between 1 and q. We chose q to be equal to 256, resulting in 8-bit integer scores, as this provided the best trade-off in precision, and run-time efficiency. This resulted in our final index occupying roughly 10GB, as the quantized scores occupy somewhat more space than the compressed tf values. We aim to investigate possibilities to generalize floating point quantization into a dictionary based compression scheme that provides bounds on the error introduced when mapping floating point ranges to integer numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QUERYING 4.1 Relational Query Plan</head><p>Keyword search in a DBMS boils down to retrieving all the documents in which some or all of the query terms occur, and then ranking this list of documents. Given that we have our pre-computed, materialized and quantized BM25 scores, the ranking is simply a summation of these scores for each (queryterm, docid) pair. In relational algebra, this could look as follows for a two term query: First, for each query term, a RangeSelect is used on the T D table, to retrieve the list of documents the query term appears in, together with its score on each document. These lists of (docid, scoreQ) pairs are then joined, producing the relation [TD1.docid, TD1.scoreQ, TD2.docid, TD2.scoreQ].</p><p>One should note that, in case a document contains only one of the query terms, MergeOuterJoin pads the other side of the join result with NULL values, thereby producing a disjunctive boolean restriction on query term presence, i.e. a document should contain one or more of the query terms to propagate into the join result. On the other hand, a regular MergeJoin would only propagate those documents that contain all query terms, thereby producing a boolean conjunctive evaluation. The output of the join is then fed into the Project operator, which sums the scores for both query terms, on a per document basis. Finally, TopN filters out the N documents with the highest overall score, in descending order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Two-Pass Evaluation</head><p>By default, The BM25 retrieval model scores each document, regardless the number of matching query terms. This coincides with the query plan that uses MergeOuterJoin, introduced in Section 4.1. Given the observation that we are only interested in the top-N most relevant documents, we can refrain from computing the score for documents that are highly unlikely to make it into the top-N. Relying on a heuristic that those documents that contain more query terms are likely to obtain a better score <ref type="bibr" coords="4,214.93,301.85,9.21,7.86" target="#b4">[4]</ref>, we can obtain a significant performance improvement by following a two-pass strategy. In the first pass, we retrieve only the documents that contain all query terms, using a conventional MergeJoin instead of a MergeOuterJoin. Only if the first pass does not return N results, we execute a second pass using the less restrictive MergeOuterJoin. On the full efficiency run, such a second pass was required in only 15.5% of the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL SETUP</head><p>We evaluated the performance of our DBMS driven IR engine on top of three different hardware architectures, resulting in the following four configurations: DISK1 A mid-end server architecture, consisting of a single 3.0GHz Intel Xeon CPU, 4GB RAM, and a 10 disk RAID0 I/O subsystem, capable of storing 1TB of data. This configuration is intended to evaluate I/O dominated performance, as the whole index does not fit main memory.</p><p>MEM1 A main-memory oriented high-end (but relatively old, 2003) server architecture, containing four 1.4GHz AMD Opteron CPUs and 16GB of RAM. For experiments on this machine, data was first loaded into RAM over the network from the raid connected to the DISK1 system. In this setup, only a single query stream was used.</p><p>MEM4 Exactly the same setup as MEM1, with the only difference that this configuration always uses four query streams instead of one. This configuration was added to evaluate scalability of our system, by utilizing all four CPUs, without network interference.</p><p>DIST8 A cluster of eight modern desktop machines, built from off-the-shelf hardware components. Each node contains a 2GHz AMD64 X2 dual-core 3800+ CPU, has 2GB RAM, and 2 disks configured for RAID0. The document collection is partitioned over all 8 nodes, All listed systems run the Linux operating system (Fedora Core 4).</p><p>We participated in both the ad-hoc task, measuring effectiveness, and in the efficiency task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EFFECTIVENESS RESULTS</head><p>We submitted two different runs for the ad-hoc task. One to evaluate single node effectiveness, and one to investigate any differences in a distributed setting. If some documents outside the TopN obtain the same score as the TopN document, there can be slight variations in the results returned by these configurations, since the distributed run relies on the broker to merge the per-node local TopN's into a global TopN. As Table <ref type="table" coords="4,382.50,527.42,4.61,7.86" target="#tab_2">2</ref> shows, the differences between the DISK1 and DIST8 runs are minor. The The final run in Table <ref type="table" coords="4,412.42,590.18,3.58,7.86" target="#tab_2">2</ref>, labeled DISK1ah*, was not submitted officially. This run was conducted after the relevance judgments were released. As can be seen in table 1, our officially submitted runs used BM25 parameters different from the commonly used k1 = 1.2 and b = 0.5, as these turned out to work better on the 2004 and 2005 topics. The DISK1ah* run uses these more common parameters. As the results show, our modified parameters performed worse on P@10. However, P@20 and MAP scores are better using the modified parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EFFICIENCY RESULTS</head><p>For the efficiency runs, we measured both index creation time and index size. Furthermore, we evaluated average query response time (latency) for returning the top-20 results on all 100.000 efficiency topics and the total time needed to execute all queries (reflecting query throughput). Results are summarized in Table <ref type="table" coords="5,156.55,109.94,3.58,7.86">3</ref>.</p><p>The comparison between MEM1 and MEM4 shows good system scalability. The fact that MEM4 uses four query streams, keeps all four CPUs in the system busy. Average query response time hardly suffers, and total query execution time is roughly divided by four, as we hoped. This also means, that using four CPUs, the system does not suffer from any contention of the memory bus, which can become an issue when increasing SMP parallelism. Note that our use of database tables that are kept compressed in RAM and are only decompressed in a small vector-granularity (in the CPU cache), helps keep to the memory bandwidth usage down (and increases the amount of inverted list data that can be cached). In the DIST8 configurations, we used four parallel query streams to hide any network latencies, thereby increasing query throughput significantly.</p><p>To have a point of reference, we also participated in the comparative TREC-TB task. We chose to do this on the Zettair system <ref type="bibr" coords="5,83.91,319.16,13.51,7.86" target="#b10">[10]</ref>, as we expected this system to be the most competitive in terms of efficiency, which was our main focus. On our DISK1 architecture, Zettair achieved an indexing time of 460 minutes, which beats us by more than a factor two. However, it has to be mentioned that our indexing times include generation of many optional index structures that are not needed for the runs submitted to TREC, and should therefore be lower if this is cleaned out. We plan to do this for the final version of this notebook. In terms of query processing speed, Zettair required 9304 seconds to execute the 10.000 comparative queries. Of this total, 8094 seconds was CPU time. This means that the system was almost CPU bound, and therefore almost at its peak performance on our 3.0GHz Intel Xeon CPU. Comparing our 0.1971s average query execution time against Zettairs 0.9304s, we beat them by a factor 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Precision of Efficiency Runs</head><p>After submission of our final efficiency results, we noticed a slight deviation with respect to the official rules. The rules state, that the top-20 as returned by the efficiency runs, should be the same as the twenty topmost results in the top-10000 submitted for the ad-hoc runs. Due to our twopass policy (see section 4.2), this is not the case though.</p><p>For the ad-hoc runs, we executed a second (disjunctive) pass only if the first (conjunctive) pass did not fill the full top-10000. For the efficiency runs, the second pass was triggered in case of less than 20 results. This means, that the ad-hoc results are more often based on the disjunctive query plan, as the second pass is triggered more often. The effect of this is, that the top-20s of the ad-hoc and the efficiency runs can show slight differences. For the 150 combined adhoc topics, a second pass was needed 100 times, while for efficiency runs on the same 150 topics, it has been triggered only three times. Overall, for the full efficiency run with 100,000 topics, a second pass was needed in 15.5% of the cases, with the conjunctive plan returning 86838 documents on average.</p><p>Table <ref type="table" coords="5,382.11,64.33,4.13,7.89">4</ref>: Precision of efficiency runs Label P@10 P@20 MEM1-50 0.5360 0.4720 MEM1-150 0.5651 0.5168 We believe, that in a real-world system, execution of only the conjuctive plan is sufficient to satisfy an end-user's need. The system can first present the conjunctive results to the end-user, and only execute the disjunctive plan in case the user desires to see more results than the amount returned by the conjunctive plan. One should note that in a database system like X100 it easy to write a query that complies with the rules, namely, one can return C ∪ (D \ C), where C and D are the results of the conjunctive and disjunctive runs, respectively. To confirm that our efficiency runs did not compromise precision, we present precision scores in table 4. These numbers are very similar to the ad-hoc effectiveness scores in Table <ref type="table" coords="5,384.15,252.00,3.58,7.86" target="#tab_2">2</ref>, confirming that the efficiency runs we submitted are capable to satisfy the end-users need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>By participating in the 2006 TREC TeraByte Track, we have shown that it is possible to run terabyte scale information retrieval tasks on top of a relational database engine, and that such an approach can rival customized IR systems in terms of performance. This work presents a step towards the integration of DB and IR systems, identifying some of the key ingredients needed to achieve this result being: Mon-etDB/X100's raw speed, light-weight data compression, and distributed execution. In the future, we plan to investigate the effect of light-weight compression schemes that are better suited for IR tasks, such as improved inverted list compression and generalized floating point quantization. A closely related research direction investigates how an array database system with an IR-researcher friendly query language <ref type="bibr" coords="5,342.37,444.71,9.72,7.86" target="#b6">[6]</ref> can generate these highly efficient MonetDB/X100 query plans automatically from a declarative specification of the retrieval model <ref type="bibr" coords="5,389.16,465.63,12.20,7.86" target="#b7">[7]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,82.84,407.49,181.02,7.89;2,57.13,53.80,232.44,338.92"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MonetDB/X100 architecture</figDesc><graphic coords="2,57.13,53.80,232.44,338.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,316.81,417.05,105.85,6.64;2,325.28,426.01,21.17,6.64;2,333.75,434.98,21.17,6.64;2,342.22,443.95,105.85,6.64;2,342.22,452.91,76.21,6.64;2,333.75,461.88,67.75,6.64;2,333.75,470.85,76.21,6.64"><head></head><label></label><figDesc>, [docid, term] ), [ term, docid ] ), [ term, docid ], [ tf = count() ] )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,54.52,276.61,8.45,7.86;3,88.73,277.29,8.70,6.99;3,126.63,277.29,82.92,6.99;3,54.52,286.55,3.95,7.86;3,88.73,287.23,4.70,6.99;3,126.63,287.23,82.92,6.99;3,54.52,296.48,10.44,7.86;3,88.73,297.16,144.79,6.99;3,54.52,306.42,186.67,7.86;3,54.52,319.21,244.50,7.01;3,53.80,350.56,239.11,7.86;3,53.80,361.02,239.11,7.86;3,53.80,371.48,239.10,7.86;3,53.80,381.94,239.10,7.86;3,53.80,392.40,239.10,7.86;3,53.80,402.87,239.11,7.86;3,53.80,413.33,209.12,7.86;3,53.80,434.25,239.11,7.86;3,53.80,444.71,239.10,7.86;3,53.80,455.17,239.10,7.86;3,53.80,465.63,111.06,7.86;3,164.85,463.86,3.28,5.24;3,170.97,465.63,121.93,7.86;3,53.80,476.09,78.30,7.86"><head></head><label></label><figDesc>k1 k1 BM25 parameter (0.8) b b BM25 parameter (0.3) fD numdocs number of documents (25M ) avgdl avgdoclen average document length (491) compression: PF=PFOR, PFD=PFOR-DELTA, all with base=0 minimal decompression overhead of the compression algorithms, I/O bandwidth utilization is improved, as the data gets decompressed only when it is used, upon crossing the RAM-CPU Cache boundary. Second, the compressed index requires less memory to make it fully main-memory resident. Even if it does not fit fully, more data can be cached in RAM in compressed form, improving overall performance. In short, Patched Frame of Reference (PFOR) compression stores a column of n-bit integer values as a b-bit integer offset from an arbitrary base value, with b &lt; n. All values in the range [base, base + 2 b -1] are stored in b bits, with b being minimized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,325.28,571.74,21.17,6.64;3,333.75,580.70,33.87,6.64;3,342.22,589.67,63.51,6.64;3,350.69,598.63,156.66,6.64;3,350.69,607.60,156.66,6.64;3,350.69,616.57,97.39,6.64;3,342.22,625.53,156.66,6.64;3,350.69,634.50,148.20,6.64;3,333.75,643.47,80.45,6.64"><head></head><label></label><figDesc>= TD2.docid), [ S.docid = MAX(TD1.docid,TD2.docid), score = TD1.scoreQ + TD2.scoreQ ]), [ score DESC ], 20)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,560.72,239.11,60.17"><head>table 1 .</head><label>1</label><figDesc>Additionally, perdocument information is kept in a separate [docid, name, length] document table D, and per term information [term, ftd] in table T . The relational table layout, together with the amount of storage each field occupies, is summarized in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,54.52,64.33,244.50,209.61"><head>Table 1 :</head><label>1</label><figDesc>Database tables and constants used</figDesc><table coords="3,54.52,72.69,244.50,201.26"><row><cell cols="2">symbol column</cell><cell>semantic</cell><cell cols="3">sorted compression</cell></row><row><cell></cell><cell>name</cell><cell></cell><cell>type</cell><cell cols="2">scheme bits</cell></row><row><cell cols="3">DT -12.3 Gtuples, output of parsing</cell><cell></cell><cell></cell></row><row><cell>D</cell><cell>docid</cell><cell>document id</cell><cell cols="2">int Y none</cell><cell>32</cell></row><row><cell>T</cell><cell>term</cell><cell>term code</cell><cell cols="2">long N none</cell><cell>64</cell></row><row><cell cols="4">TD -3.5 Gtuples, document-level index</cell><cell></cell></row><row><cell>T</cell><cell>term</cell><cell>term code</cell><cell cols="2">long Y PFD b=1</cell><cell>2.13</cell></row><row><cell>D</cell><cell>docid</cell><cell>document id</cell><cell cols="2">int Y PFD b=8</cell><cell>11.98</cell></row><row><cell>fD,T</cell><cell>tf</cell><cell cols="3">frequency of T in D int N PF b=5</cell><cell>5.91</cell></row><row><cell>ωD,T</cell><cell>score</cell><cell>score of T in D</cell><cell cols="2">float N none</cell><cell>32</cell></row><row><cell>ω D,T</cell><cell>scoreQ</cell><cell>quantized score</cell><cell cols="2">int N PF b=8</cell><cell>8.00</cell></row><row><cell cols="6">D -25 Mtuples, output of parsing, per-document information</cell></row><row><cell>D</cell><cell>docid</cell><cell>document id</cell><cell cols="2">int Y none</cell><cell>32</cell></row><row><cell></cell><cell cols="2">docname document name</cell><cell cols="2">str Y none</cell><cell>88</cell></row><row><cell>|D|</cell><cell>doclen</cell><cell>document length</cell><cell cols="2">int N none</cell><cell>32</cell></row><row><cell cols="3">T -12 Mtuples, per-term information</cell><cell></cell><cell></cell></row><row><cell>T</cell><cell>term</cell><cell>term code</cell><cell cols="2">long Y none</cell><cell>64</cell></row><row><cell>fT,D</cell><cell>ftd</cell><cell cols="3">#documents with T int N none</cell><cell>32</cell></row><row><cell cols="2">Global constants</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,316.81,64.33,239.11,288.49"><head>Table 2</head><label>2</label><figDesc>Queries are submitted to a broker program, which broadcasts each query to all eight nodes, and merges the local document rankings returned by each node into a global ranking. As the partial indices of each node fit into main-memory, these experiments don't involve any I/O. In this setting, we used four query streams to hide network latencies induced by communication with the broker.</figDesc><table coords="4,316.81,64.33,239.11,215.27"><row><cell></cell><cell></cell><cell cols="3">: Effectiveness Results</cell></row><row><cell></cell><cell>Label</cell><cell></cell><cell cols="3">P@10 P@20 MAP InfAP</cell></row><row><cell cols="6">DISK1ah-50 0.5340 0.4780 0.2770 0.2299</cell></row><row><cell cols="5">DISK1ah-150 0.5591 0.5171 0.2952</cell><cell>NA</cell></row><row><cell cols="6">DIST8ah-50 0.5380 0.4750 0.2766 0.2308</cell></row><row><cell cols="5">DIST8ah-150 0.5577 0.5138 0.02952</cell><cell>NA</cell></row><row><cell cols="6">DISK1ah*-50 0.5440 0.4690 0.2677 0.2183</cell></row><row><cell cols="6">Table 3: Efficiency Results, with indexing time in</cell></row><row><cell cols="5">minutes, and query times in seconds</cell></row><row><cell>Label</cell><cell cols="2">Index</cell><cell></cell><cell>Query</cell></row><row><cell></cell><cell cols="5">Size Time Streams Avg. Time Total Time</cell></row><row><cell cols="3">DISK1 10GB 1000</cell><cell>1</cell><cell>0.1971</cell><cell>19708</cell></row><row><cell cols="3">MEM1 10GB 1000</cell><cell>1</cell><cell>0.0790</cell><cell>7914</cell></row><row><cell cols="3">MEM4 10GB 1000</cell><cell>4</cell><cell>0.0805</cell><cell>2052</cell></row><row><cell cols="2">DIST8 10GB</cell><cell>185</cell><cell>4</cell><cell>0.0132</cell><cell>539</cell></row><row><cell cols="6">with each node indexing its own partition, in paral-</cell></row><row><cell>lel.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,316.81,537.88,239.11,39.25"><head>table furthermore</head><label>furthermore</label><figDesc></figDesc><table coords="4,316.81,537.88,239.11,39.25"><row><cell>distin-</cell></row><row><cell>guishes between efficiency results obtained on both the 50</cell></row><row><cell>2006 topics and on the 150 combined topics of 2004, 2005</cell></row><row><cell>and 2006.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,321.29,488.35,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.63,500.94,189.60,7.86;5,335.63,511.40,203.91,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amer-Yahia</surname></persName>
		</author>
		<title level="m" coord="5,400.41,500.94,124.82,7.86;5,335.63,511.40,123.46,7.86">Report on the DB/IR Panel at Sigmod 2005. SIGMOD Record</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="71" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.63,529.96,190.46,7.86;5,335.63,540.42,197.18,7.86;5,335.63,550.88,203.88,7.86;5,335.63,561.34,217.81,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,443.65,529.96,82.45,7.86;5,335.63,540.42,106.01,7.86">Simplified Similarity Scoring Using Term Ranks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,460.82,540.42,71.99,7.86;5,335.63,550.88,203.88,7.86;5,335.63,561.34,56.29,7.86">Proceedings of the International Conference on Information Retrieval (ACM SIGIR)</title>
		<meeting>the International Conference on Information Retrieval (ACM SIGIR)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.63,579.91,216.39,7.86;5,335.63,590.37,213.93,7.86;5,335.63,600.83,193.16,7.86;5,335.63,611.29,206.54,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,335.63,590.37,138.34,7.86">Hyper-Pipelining Query Execution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,492.85,590.37,56.70,7.86;5,335.63,600.83,193.16,7.86;5,335.63,611.29,29.04,7.86">Proceedings of the Conference of Innovative Database Research (CIDR)</title>
		<meeting>the Conference of Innovative Database Research (CIDR)<address><addrLine>Asilomar, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="225" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.63,629.86,220.29,7.86;5,335.63,640.32,206.67,7.86;5,335.63,650.78,212.79,7.86;5,335.63,661.24,205.56,7.86;5,335.63,671.70,184.04,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,369.04,640.32,173.26,7.86;5,335.63,650.78,63.80,7.86">Efficient query evaluation using a two-level retrieval process</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,417.97,650.78,130.44,7.86;5,335.63,661.24,200.48,7.86">Proceedings of the Conference of Information and Knowledge Management (CIKM)</title>
		<meeting>the Conference of Information and Knowledge Management (CIKM)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.63,690.26,201.87,7.86;5,335.63,700.73,200.69,7.86;5,335.63,711.19,211.25,7.86;6,72.62,57.64,215.05,7.86;6,72.62,68.10,155.75,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,335.63,700.73,200.69,7.86;5,335.63,711.19,123.64,7.86">Integrating DB and IR Technologies: What is the Sound of One Hand Clapping?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,474.89,711.19,71.99,7.86;6,72.62,57.64,210.21,7.86">Proceedings of the Conference of Innovative Database Research (CIDR)</title>
		<meeting>the Conference of Innovative Database Research (CIDR)<address><addrLine>Asilomar, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.62,85.73,193.39,7.86;6,72.62,96.19,202.97,7.86;6,72.62,106.65,210.42,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,212.48,85.73,53.53,7.86;6,72.62,96.19,112.16,7.86">A declarative DB-powered approach to IR</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cornacchia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,203.60,96.19,71.99,7.86;6,72.62,106.65,181.62,7.86">Proceedings of the European Conference on IR Research (ECIR)</title>
		<meeting>the European Conference on IR Research (ECIR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.62,124.29,185.40,7.86;6,72.62,134.75,206.66,7.86;6,72.62,145.21,203.85,7.86;6,72.62,155.67,20.99,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,182.25,134.75,97.03,7.86;6,72.62,145.21,89.66,7.86">Flexible and efficient IR using Array Databases</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cornacchia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Héman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Boncz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Submitted for publication</note>
</biblStruct>

<biblStruct coords="6,72.62,173.30,186.98,7.86;6,72.62,183.76,220.29,7.86;6,72.62,194.22,217.04,7.86;6,72.62,204.69,191.48,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,258.48,183.76,34.42,7.86;6,72.62,194.22,78.26,7.86">Crossing the structure chasm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tatarinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,170.04,194.22,119.62,7.86;6,72.62,204.69,162.57,7.86">Proceedings of the Conference of Innovative Database Research (CIDR)</title>
		<meeting>the Conference of Innovative Database Research (CIDR)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.62,222.32,190.90,7.86;6,72.62,232.78,121.81,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,129.48,222.32,130.19,7.86">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,72.62,232.78,31.78,7.86">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.62,250.41,211.84,7.86;6,72.62,260.87,137.94,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bernstein</surname></persName>
		</author>
		<title level="m" coord="6,150.13,250.41,134.33,7.86;6,72.62,260.87,109.65,7.86">RMIT University at TREC 2005: Terabyte and Robust Track</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.62,278.51,190.80,7.86;6,72.62,288.97,194.97,7.86;6,72.62,299.43,209.04,7.86;6,72.62,309.89,214.54,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,72.62,288.97,178.96,7.86">Super-Scalar RAM-CPU Cache Compression</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Héman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,72.62,299.43,209.04,7.86;6,72.62,309.89,105.10,7.86">Proceedings of the International Conference of Data Engineering (IEEE ICDE)</title>
		<meeting>the International Conference of Data Engineering (IEEE ICDE)<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
