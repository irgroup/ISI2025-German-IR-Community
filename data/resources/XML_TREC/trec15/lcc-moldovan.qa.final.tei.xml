<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.52,81.04,330.86,12.91">A Temporally-Enhanced PowerAnswer in TREC 2006</title>
				<funder>
					<orgName type="full">Disruptive Technology Office&apos;s (DTO) Advanced Question Answering for Intelligence (AQUAINT) Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.76,134.03,72.70,10.76"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.40,134.03,86.34,10.76"><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.17,134.03,58.86,10.76"><forename type="first">Marta</forename><surname>Tatu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.52,81.04,330.86,12.91">A Temporally-Enhanced PowerAnswer in TREC 2006</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">963472A265BF8EF4EA7878500B3A939C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software (Question Answering)</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Open-domain Question Answering, Questions beyond factoids</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on Language Computer Corporation's participation in the Question Answering track at TREC 2006. An overview of the PowerAnswer 3 question answering system and a description of new features added to meet the challenges of this year's evaluation are provided. Emphasis is given to temporal constraints in questions and how this affected the outcome of the systems in the task. LCC's results in the evaluation are presented at the end of the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language Computer Corporation participated in the QA track of the 15th annual TREC evaluation. TREC 2006 brought new modifications and challenges from the previous year, which we addressed through new developments in our QA system, PowerAnswer. The format of the main task was largely unchanged when compared to 2005. There were 75 target sets composed of both entity and event targets, each set including FACTOID, LIST and OTHER questions, for a total of 567 questions.</p><p>While the format remained the same as last year, there were extra challenges included in this year's question set.</p><p>There was an increased focus on temporality with timedependent questions. These are questions for which there is an implicit or explicit specification of a temporal range from which the correct answer must come. This time frame can be expressed in a variety of ways. Present tense questions are seeking the most up-to-date information available in the corpus. Past tense questions declare the time frame explicitly (What position did she <ref type="bibr" coords="1,514.72,367.00,25.16,8.97;1,313.20,379.00,24.45,8.97">[Janet Reno]</ref> have immediately prior to 1993?) or implicitly through the target (the Queen Mum's 100th Birthday).</p><p>In light of the new temporal constraints in the question set, there is a new judgement category for responses this year -correct has become locally correct and globally correct. A response is judged locally correct if the response is correct in the context of the source document, but the total corpus contains more up-to-date information which is considered globally correct. Locally correct answers do not contribute positively to the final system score.</p><p>Additionally, LIST and OTHER questions are unchanged from last year, though the final per-series score has been updated to give equal weight to each question type. This gives a higher weight to the more subjective and open OTHER questions than previous years.</p><p>This paper describes the PowerAnswer 3 system used in TREC 2006, the improvements made from the previous year and a deep analysis of the system's performance. A discussion on new directions for PowerAnswer inspired by the TREC evaluation will conclude this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of PowerAnswer 3</head><p>The architecture of the PowerAnswer 3 Question Answering system is illustrated in Figure <ref type="figure" coords="1,463.88,677.32,3.77,8.97">1</ref>. Automatic question answering requires a system that has a wide range of tools available. There is no one monolithic solution for all question types or even data sources. In realization of this, PowerAnswer comprises a set of strategies that are selected based on advanced question processing, and each strategy is developed to solve a specific class of questions either independently or together. A Strategy Selection module automatically analyzes the question and chooses a set of strategies with the algorithms and tools that are tailored to the class of the given question. Pow-erAnswer can distribute the strategies across workers in the case of multiple strategies being selected, alleviating the increase in the complexity of the question answering process by splitting the workload across machines and processors <ref type="bibr" coords="2,116.46,504.04,149.55,8.97" target="#b9">(Moldovan, Munirathnam et al. 2006)</ref>.</p><p>The major processing steps highlighted in Figure <ref type="figure" coords="2,279.01,521.92,5.03,8.97">1</ref> are question processing (QP), passage retrieval (PR) and answer processing (AP). The role of the QP module is to determine (1) temporal constraints, (2) how the target (if given) should relate to the current question, (3) the expected answer type and (4) to select the keywords used in retrieving relevant passages. The PR module ranks passages that are retrieved based on lexical similarity, while the AP determines the extraction of the candidate answers and performs semantic matching and scoring as well as introduces a number of post-processing steps for re-ranking. All modules have access to a syntactic parser, a named entity recognizer and a reference resolution system among many other NLP tools. LCC utilizes a generic set of NLP tools and interfaces that allow us to quickly develop or integrate new tools and processing modules in a pipeline of text processing and understanding.</p><p>To improve the methods used for answer selection, we take advantage of redundancy in large corpora, specifically in this case, the Internet. As the size of a document collection grows, a question answering system is more likely to pinpoint a candidate answer that closely resembles the surface structure of the question. Such an intuition has been verified by <ref type="bibr" coords="2,415.07,342.76,76.70,8.97" target="#b1">(Breck et al., 2001)</ref> and empirically re-enforced by several QA systems <ref type="bibr" coords="2,473.92,354.76,43.36,8.97" target="#b5">(Lin, 2002)</ref>. The web-boosting features have the role of correcting the errors in answer processing that are produced by the selection of keywords, by syntactic and semantic processing and by the absence of pragmatic information. The ultimate decision for selecting answers is based on logical proofs from LCC's COGEX inference engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">What's new from TREC2005</head><p>To meet the challenges of the new complex questions and temporal constraints, LCC developed or included several new innovations into the latest version of PowerAnswer 3. The following subsections will detail these new additions from what was used at TREC 2005 <ref type="bibr" coords="2,492.53,518.56,47.57,8.97;2,313.19,530.44,87.10,8.97" target="#b9">(Harabagiu, Moldovan et al. 2005)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Understanding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Resolution</head><p>Before strategy selection occurs, some initial question processing takes place. The target and unmodified question are sent to the temporal resolution module, shown in Figure <ref type="figure" coords="3,113.60,123.04,5.03,8.97">2</ref> which analyzes the target and question together to resolve any ambiguous temporal context and use this information in the reformulated question to be further processed.</p><p>If the target is labelled as an event, any explicit dates are identified, extracted from the target and added to the reformulated question. If there are no explicit temporal contexts in the event target, the module performs a limited question answering procedure to answer "When was TARGET?" The resulting temporal answer is then added to the refomulated question, exemplified in Table <ref type="table" coords="3,267.25,242.56,3.77,8.97" target="#tab_1">1</ref>. If the scores of the answers to this "When" question are similar enough to indicate ambiguity, a date range is created among the top answers and that range is then added to the reformulated question. Our temporal context processing methods will correctly match a date that falls within the desired range. If the question itself contains an explicit temporal context, then none of the above processing is performed and question understanding continues.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Understanding</head><p>The next step in question processing is to understand how the target relates to the question. There are times when the target simply provides context and other times when the question directly references the whole target, or only a part. Furthermore, at TREC, the references to answers from previous questions of the same target is constantly increasing. The first step of the module described in Figure <ref type="figure" coords="3,274.11,713.20,5.03,8.97" target="#fig_2">3</ref> is to examine the possibility of references to previous answers, exemplified in Table <ref type="table" coords="3,399.15,87.16,3.77,8.97" target="#tab_3">2</ref>. Steps, such as type, gender and number matching, are taken to examine the current question, the previous questions within the target set, and their answers and to determine if a previous answer should be inserted. If this succeeds, the new question is saved to a list of reformulations. Next, the module attempts to insert information from the target or use it as context. If no point of insertion is found in the question, the target is added as the question's context which restricts the document retrieval. Several steps are taken to resolve where and in what capacity the whole target or a portion of it should be used in the question. For all the above steps that succeed, a new question is saved in the refomulation list.  Next, all the reformulated questions are sent through the remainning processing pipeline and, at the end, voting is performed to determine which of the ambiguous target understanding reformulations have higher confidence. The top answer of the most unambiguous reformulated question is selected and returned as the final result. The information about target insertions is stored in a cache which shall be used for subsequent questions in the same target set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insertion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer type detection</head><p>We extended PowerAnswer's answer type detection module by moving it to a hybrid system which takes advantage of precise heuristics as well as machine learning algorithms for ambiguous questions. A maximum entropy model was trained to detect both answer type terms and answer types. The learner's features for answer type terms include part-of-speech, lemma, head information, parse path to WH-word, and named entity information. Answer type detection uses a variety of attributes such as additional answer type term features and set-to-set lexical chains derived from eXtended WordNet 1 which links the set of question keywords to the set of potential answer type nodes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal context/temporal ranking of answers and documents</head><p>For TREC 2006, we extended the temporal context mechanisms which originally relied on signal words such as "in", "of", "prior", and "during", and the detection of SUMO <ref type="bibr" coords="4,118.41,288.76,98.89,8.97" target="#b10">(Niles and Pease 2001)</ref> events to flag if a temporal context should be marked within a text snippet <ref type="bibr" coords="4,87.58,312.64,95.38,8.97" target="#b9">(Moldovan et al. 2005)</ref>. Although this approach is precise, the coverage for attaching a temporal context to an event was lower than required for the TREC 2006 task. For this reason, LCC introduced a more robust default temporal context mechanism that operates at several levels of granularity: phrase, sentence, passage, and document. If a date that unifies with the temporal requirements of the input question is detected in the discourse surrounding a candidate answer, a temporal boost is applied, which is scaled according to the syntactic proximity of the detected date to the candidate answer.</p><p>To handle the new distinction between "locally correct" and "globally correct", we also included temporal post-processing of the answer list. Dates for documents and the temporal context of the answer are maintained through question answering and after initial ranking, answers are given a boosting factor on top of their current relevance score that is intended to give greater priority to strong answers that are more recent than other strong answers. Answers that appear further down the response list and have lower relevance scores will not be affected by this boosting. One disadvantage to this is that the scoring of a response as "locally correct" was not exclusive to out-of-date information. There can be misreported information in the document, which might be much more recent than a correct answer found in an older document. There were several cases of later documents contradicting correct answers with incorrect ones.</p><p>For example, Q195.2 What percentage of the vote was for (East Timor) independence? has the exact correct answer of "78.5 percent". Participant responses of "78 percent" to "79 percent" were returned and marked as correct from documents ranging from 1999/09/04-07. One document (NYT19991001.0212) dated 1999/10/01 states "80 percent of the voters chose independence", a more recent response with misreported data, and marked as "locally correct". LCC had a total of 11 locally correct factoids.</p><p>Because temporal answers can have a range of granularity, when pre-processing the data collection, the named entities stored in the IR index are extracted in a greedy fashion, so both "March 14, 1592" and "2000" will be tagged as date to give PowerAnswer the best flexibility for entity selection. During answer processing, if the question is seeking just a month, or a year, then the excess information from the date entity selected is removed after a more fine-grained NE recognition is performed on the answer nugget. <ref type="bibr" coords="4,391.22,364.96,64.95,8.97">Questions 182.4</ref> In what month is the Edinburgh Fringe held <ref type="bibr" coords="4,404.98,376.96,48.07,8.97">? and 182.6</ref> In what year was the Edinburgh Fringe begun? demonstrated the utility of this method. Otherwise, if a simple "When was ..." question is asked, the entity with the most detailed temporal information would be the final answer. This method operated on 38 questions seeking year, month, or day in this year's factoid set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">eXtended WordNet Knowledge Base</head><p>eXtended WordNet Knowledge Base (XWN-KB) is the result of LCC's ongoing research which captures and stores the rich world knowledge encoded in WordNet's glosses into a knowledge base.</p><p>To achieve this goal, LCC used the parsed and sense disambiguated glosses of WordNet (eXtended WordNet) and its semantic parser, Polaris <ref type="bibr" coords="4,437.02,567.76,73.93,8.97" target="#b0">(Bixler et al. 2005)</ref>, to derive a highly semantic representation of WordNet's definitions. For example, the first sense of the word "avocado" defined as a pear-shaped tropical fruit with green or blackish skin and rich yellowish pulp enclosing a single large seed is represented in XWN-KB as shown in Figure <ref type="figure" coords="4,341.73,639.52,3.77,8.97" target="#fig_1">4</ref>.</p><p>Given XWN-KB's semantic clusters, the accuracy of our lexical chains module increased greatly. The ambiguous GLOSS relation which used to link a concept w1 with any concept w2 present in w1's gloss was replaced by chains which describe the exact association between w1 and w2. For instance:</p><p>(n-avocado#1, alligator pear#1, avocado pear#1, aguacate#1) GLOSS (n-seed#2) became:</p><p>(n-avocado#1, alligator pear#1, avocado pear#1, aguacate#1) ISA (n-fruit#1) PART-WHOLE (nseed#2) whose meaning and strength is very different than:</p><p>(n-avocado#1, alligator pear#1, avocado pear#1, aguacate#1) GLOSS (a-rich#2) which is explicited as:</p><p>(n-avocado#1, alligator pear#1, avocado pear#1, aguacate#1) ISA (n-fruit#1) PART-WHOLE (npulp#2, flesh#3) PROPERTY (a-rich#2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">COGEX -Natural Language Logic Prover</head><p>LCC's natural language logic prover, COGEX, is used by PowerAnswer to provide the final re-ranking of the candidate answers based on the degree of semantic entailment between the candidate answer passage (CAP ) and the question (Q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Knowledge Representation Model</head><p>COGEX uses a three-layered logical representation which captures the syntactic, semantic and temporal propositions encoded in a text fragment (either question or answer passage). The details of the logic transformation methodology are presented in <ref type="bibr" coords="5,228.17,384.52,70.47,8.97;5,72.00,396.52,22.88,8.97" target="#b7">(Moldovan et al., 2003;</ref><ref type="bibr" coords="5,96.92,396.52,86.14,8.97" target="#b9">Moldovan et al. 2005)</ref>. Several extra modules were added to this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negation detection and representation</head><p>In cases similar to the best ranked candidate answer passage for Q141.1, If the Seattle Seahawks do not increase their oneyear, $1.8 million offer, ... 2 , the system removes not RB(x3,e1) and negates the verb's predicate (-increase VB(e1,x12,x17)) unless the verb is modified by an adverb. For example, one of the top passages retrieved for Q143.3, Ms. Kirkpatrick's spokeswoman at the institute (American Enterprise Institute) did not immediately return a call for comment is represented as: Similarly, for nouns whose determiner is no, for example, for No NFL starter has more experience than Moon, ... (candidate answer passage for Q141.4), the verb's predicate is negated:</p><formula xml:id="formula_0" coords="5,319.20,87.84,194.69,30.93">NFL NN(x1) &amp; organization NE(x1) &amp; starter NN(x2) &amp; -have VB(e1,x2,x3) &amp; more JJ(x7,x3) &amp; experience NN(x3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantification detection</head><p>The quantification of entities plays a role when a proof's final score is computed. If we assume that both the candidate answer passage and question are existential, then CAP Q if and only if CAP 's entities are a subset of Q's entities, such as: Some smart people read Do people read ? (yes) and penalizing a pair whose Q contains predicates that cannot be inferred is a correct way to ensure entailment: Some people read Do smart people read ? (yes). But, if both CAP and Q are universally quantified, then the groups mentioned in Q must be a subset of the ones from CAP All people read Do all smart people read? (yes) and:</p><p>All smart people read Do all people read? (yes). Thus, COGEX's proof-scoring module adds back the points for the modifiers dropped from Q and subtracts points for CAP 's modifiers not present in Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional detection and representation</head><p>To ensure the correctness of the logic form which impacts the prover's reasoning process, we adapted the logic form representation of a text snipet to reflect the conditional dependencies between two or more clauses.</p><p>For example, the sentence from one of the top passages returned for Q164.6:</p><p>If you have a farm, bet it is represented as</p><formula xml:id="formula_1" coords="5,319.20,469.08,194.93,42.93">(you PRP(x3) &amp; have VB(e1,x3,x1) &amp; farm NN(x1) &amp; AGENT SR(x3,e1) &amp; THEME SR(x1,e1)) -&gt; (bet VB(e2,x3,x1) &amp; AGENT SR(x3,e2) &amp; THEME SR(x1,e2)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Natural Language Axioms</head><p>Our NLP axioms are linguistic rewriting rules that help break down complex logic structures and express syntactic equivalence. After analyzing the logic form and the parse trees of each text fragment, the system, automatically, generates NLP axioms to break down complex nominals and coordinating conjunctions into their constituents so that other axioms can be applied, individually, to the components. These axioms are made available only to the (CAP ,Q) pair that generated them. For example, the noun compound Ben Cohen (related to Q172. In this year's TREC competition, we introduced a new type of NLP axioms which capture the equivalence of names. In the passage</p><p>The jury of 11 whites and one African-American, whom they elected foreman, had convicted Bill King of capital murder ..., Bill King refers to the same entity as John William King from Q145.1 How many non-white members of the jury were there of John William King convicted of murder ?</p><p>Using the keyword alternation information provided by PowerAnswer's Question Processing (QP) module, CO-GEX automatically created and used during inference the axiom:</p><formula xml:id="formula_2" coords="6,78.00,298.56,167.82,42.93">bill NN(x7) &amp; king NN(x8) &amp; nn NNC(x9,x7,x8) -&gt; john NN(x2) &amp; william NN(x3) &amp; king NN(x4) &amp; nn NNC(x5,x2,x3,x4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Lexical Chain Axioms</head><p>For the task of identifying the semantic entailment between a question and its candidate answer, the ability to recognize two semantically-related words is an important requirement. Therefore, we, automatically, construct lexical chains of WordNet relations annotated between the synsets, from the words in CAP to Q's constituents <ref type="bibr" coords="6,123.54,439.36,127.49,8.97" target="#b6">(Moldovan and Novischi 2002)</ref>. For each relation in the best chain<ref type="foot" coords="6,180.72,449.68,3.48,6.28" target="#foot_0">3</ref> , the system generates, on demand, an axiom with the predicates of the synsets in the WordNet relation. For example, given the XWN lexical chain (v-call#4,send for#1) -DERIVATION-&gt; (ncaller#5) -HYPERNYM-&gt; (n-announcer#1) -  between call/VB and announce/VB, the system constructs three axioms: We adopted this new one-axiom-per-chain-relation approach (compared with one-axiom-per-chain) because XWN-KB contains a much larger set of semantic relations and it is very difficult to reduce and entire lexical chain to only one implication which captures its entire meaning. By assigning a set of axiom templates to each semantic relation (examples in Table <ref type="table" coords="6,225.07,672.76,3.63,8.97" target="#tab_4">3</ref>), a lexical chain is broken down into several axioms whose relations are combined by the logic prover as it sees fit. Moreover, the weight of each axiom depends on the relation that defines it and the weight of the chain where it originated from.</p><p>Other improvements added to the lexical chain builder module include lexical chain axioms which append the entity name of the target concept, whenever it exists, to the created axiom. For example, COGEX uses the axiom</p><formula xml:id="formula_3" coords="6,319.20,181.44,179.70,19.05">south african JJ(x1,x2) -&gt; south africa (x1) &amp; country NE(x1)</formula><p>when it tries to infer president of South Africa (Q183.1) from South African president.</p><p>We ensured the relevance of the lexical chains by limiting the path length to three relations and the set of WordNet relations used to create the chains by discarding the paths that contain certain relations in a particular order. For example, the automatic axiom generation module did not consider chains with an IS-A relation followed by a HYPONYMY link. Without removing these types of chains, our system inferred, for instance, John lives in Detroit from John lives in Chicago (Chicago is-a -→ city hyponymy -→ Detroit). Similarly, the system rejected chains with more than one HYPONYMY relations. Although this relation links semantically related concepts, the questions are more general than the candidate answer passages and too many HYPONYMY relations can lead to a too specific concept in Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Semantic Calculus</head><p>The Semantic Calculus axioms combine two semantic relations. Their purpose is three fold: (1) increase the semantic connectivity of a text fragment by combing relations identified in text, (2) which can make explicit unstated relationships and longer dependencies within a text fragment, and (3) pinpoint the semantic association between concepts linked by lexical chains. Once the system breaks down the generic lexical chain (w1) -SR1-&gt; ( <ref type="formula" coords="6,526.10,514.72,9.17,8.97">w2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5">Temporal Axioms</head><p>One of the types of temporal axioms that we load in our logic prover links specific dates to more general time intervals. For example, October 2000 entails the year 2000 time TMP(BeginFn(e1), 2000, 10, 1, 0, 0, 0) &amp; time <ref type="bibr" coords="6,419.25,667.92,75.57,7.05;6,319.20,679.92,134.47,7.05">TMP(EndFn(e1), 2000, 10, 31, 23, 59, 59)</ref> → time TMP(BeginFn(e1), 2000, 1, 1, 0, 0, 0) &amp; time <ref type="bibr" coords="6,403.06,703.80,107.72,7.05;6,319.20,715.80,100.04,7.05">TMP(EndFn(e1), 2000, 12, 31, 23, 59, 59)</ref>.  These axioms are automatically generated before the search for a proof starts. Additionally, the prover uses a SUMO knowledge base of temporal reasoning axioms that consists of axioms for a representation of time points and time intervals, Allen primitives, and temporal functions. For example, during is a transitive Allen primitive, giving:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Relation Axiom Templates</head><p>during TMP(e1,e2) &amp; during TMP(e2,e3) -&gt; during TMP(e1,e3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Challenges at TREC 2006</head><p>The largest challenge introduced for this year's evaluation is the new emphasis on time-dependent questions. These are FACTOID questions for which the target set or question explicitly or implicitly describes a temporal range in which the answer must lie. The result of this is the new type of response judgement, "locally correct" where the response is correct for the local document but not "globally" for the whole data collection.  Another complexity added in this year's evaluation were questions in the same target set that are syntactically very similar with only one or two words changed that completely alter the goal of the question. Examples can be seen in Table <ref type="table" coords="7,159.50,689.32,3.77,8.97" target="#tab_8">5</ref>. If the QA system was not able to fully understand the difference or if the important keyword was incorrectly relaxed from the IR query because of insufficient documents retrieved, the semantic difference is lost and the results of the two questions is the same.   Evaluating question answering results continues to be a challenge as well, as each judge holds differing opinions on what makes an insufficient or oversufficient response. This subjectivity in evaluation in general tends to affect all systems to the same extent but does lead to discussion about correctness of responses. Examples this year involving "inexact": for one question, a response of only "Washington" (meaning District of Columbia) was judged correct, whereas a reponse of "Salzberg" (Austrian city) was judged inexact and only "Salzburg, Austria" marked as correct. For Q180.6 How many years are in a term of the Lebanese Parliament?, responses of "four years" were judged inexact and only an answer of "four" was correct. This leads to a discussion of what information is needed to be sufficient, especially in cases of location answers and does clarifying information such as "times" or "years" make a response over-specified. LCC had a total of 32 inexact responses, as seen in Table <ref type="table" coords="8,279.47,383.20,3.77,8.97" target="#tab_10">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Questions of increasing complexity will continue to demand greater use of contextual information to decrease ambiguity and increase precision for similar questions and targets. More advanced reasoning is required to operate on these contexts to find the right answer through the noise of the collection. Future directions for Power-Answer include better detection and usage of contexts in text, more complex hierarchical representation of knowledge, and problem solving using question answering. This year's TREC yet again helped to push the envelope of what kinds of questions current systems are able to handle. The question sets with close syntactic similarity test the semantic understanding of the participating systems and the emphasis on temporal correctness demands better reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,356.04,698.56,141.10,8.97"><head></head><label></label><figDesc>Figure 2: QP: Temporal Resolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,79.56,360.00,201.50,8.07;3,79.56,369.96,124.12,8.07;3,79.56,380.28,198.70,8.07;3,79.56,390.72,65.13,8.07;3,79.56,401.04,25.42,8.07;3,126.94,401.04,161.89,8.07;3,126.96,411.00,49.06,8.07;3,79.56,421.32,27.81,8.07;3,126.93,421.32,160.39,8.07;3,126.96,431.28,157.36,8.07"><head>Q175. 4 :</head><label>4</label><figDesc>(repatriation of Elian Gonzales) Who was the U.S. Attorney General at the time? Prelim. Q When was repatriation of Elian Gonzales? Prelim. A 2000 New Q Who was the U.S. Attorney General at the time in 2000? Answer "Earlier Thursday, U.S. Attorney General Janet Reno said ..." (from a 2000 document)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,113.64,689.68,143.72,8.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: QP: Target Understanding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,325.80,712.08,149.22,9.33"><head>1</head><label></label><figDesc>http://xwn.hlt.utdallas.edu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,187.92,187.60,235.78,8.97"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Semantic representation of avocado in XWN-KB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,78.00,582.60,171.06,7.05;5,78.00,594.48,176.34,7.05;5,80.64,606.48,173.33,7.05;5,78.00,618.48,173.22,7.05;5,78.00,630.36,137.71,7.05;5,78.00,642.36,173.21,7.05;5,78.00,654.24,178.50,7.05;5,78.00,666.24,200.20,7.05;5,78.00,678.24,197.32,7.05"><head></head><label></label><figDesc>ms NN(x1) &amp; kirkpatrick NN(x2) &amp; nn NNC(x3,x1,x2) &amp; human NE(x3) &amp; s POS(x3,x4) &amp; spokeswoman NN(x4) &amp; at IN(x4,x5) &amp; institute NN(x5) &amp; -immediately RB(x8,e1) &amp; return VB(e1,x4,x7) &amp; call NN(x7) &amp; for IN(x7,x6) &amp; comment NN(x6) &amp; PART-WHOLE SR(x4,x5) &amp; AGENT SR(x4,e1) &amp; THEME SR(x7,e1) &amp; PURPOSE SR(x6,x7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,336.91,656.44,156.53,8.97;5,313.20,668.44,46.86,8.97;5,319.20,679.08,154.98,7.05;5,319.20,691.08,157.03,7.05;5,319.20,702.96,51.70,7.05;5,313.20,713.20,14.50,8.97;6,78.00,74.76,154.98,7.05;6,78.00,86.64,157.03,7.05;6,78.00,98.64,62.50,7.05;6,72.00,108.88,226.56,8.97;6,72.00,120.88,139.17,8.97"><head></head><label></label><figDesc>3) is broken down into Ben and Cohen. The axioms nn NNC(x3,x1,x2) &amp; human (x3) &amp; ben NN(x1) &amp; cohen NN(x2) -&gt; ben NN(x3) and nn NNC(x3,x1,x2) &amp; human (x3) &amp; ben NN(x1) &amp; cohen NN(x2) -&gt; cohen NN(x3) help the system infer the answer for Q172.3 What is Ben's last name of Ben &amp; Jerry's ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,78.00,554.52,181.12,7.05;6,78.00,565.36,190.11,8.97;6,78.00,578.52,162.42,7.05;6,78.00,590.40,56.49,7.05"><head></head><label></label><figDesc>call VB(e1,x1,x2) -&gt; caller NN(x1), caller NN(x1) -&gt; announcer NN(x1) and announcer NN(x1) -&gt; announce VB (e1,x1,x2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,535.27,514.72,4.59,8.97;6,313.20,526.60,226.91,8.97;6,313.20,538.60,226.78,8.97;6,313.20,550.60,226.66,8.97;6,313.20,562.48,226.61,8.97;6,313.20,574.15,226.69,9.30"><head></head><label></label><figDesc>) -SR2-&gt; (w3) into w1(x1) -&gt; w2(x2) &amp; SR1(x1,x2) and w2(x2) -&gt; w3(x3) &amp; SR2(x2,x3), the semantic calculus axiom SR1(x1,x2) &amp; SR2(x2,x3) -&gt; SR3(x1,x3) defines the semantic connection S3 = (SR1 • SR2) between w1 and w3 across the lexical chain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="7,142.56,86.78,12.47,6.46;7,224.27,86.04,86.25,7.05;7,224.28,96.00,150.90,7.05;7,142.56,107.18,46.76,6.46;7,224.24,106.44,202.37,7.05;7,224.28,116.40,107.84,7.05;7,224.28,126.36,107.72,7.05;7,224.28,136.32,107.72,7.05;7,142.56,147.38,25.07,6.46;7,224.27,146.64,245.43,7.05;7,142.56,157.70,26.02,6.46;7,224.26,156.96,180.89,7.05;7,142.56,168.14,32.01,6.46;7,224.26,167.40,91.53,7.05"><head></head><label></label><figDesc>) -&gt; v(e1,x1,x2) &amp; AGENT SR(x1,e1) n(e1) -&gt; v(e1,x1,x2) v(e1,x1,x2) -&gt; n(x1) v(e1,x1,x2) -&gt; n(e1) CAUSE v1(e1,x1,x2) -&gt; v2(e2,x2,x3) &amp; CAUSE SR(e1,e2) AGENT n1(x1) -&gt; n2(x2) &amp; AGENT SR(x1,x2) PERTAIN a(x1,x2) -&gt; n(x1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,107.76,451.84,155.23,8.97"><head>Table 1 :</head><label>1</label><figDesc>Temporal Constraint Example</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,356.04,367.72,141.20,8.97"><head>Table 2 :</head><label>2</label><figDesc>Previous Answer Insertion</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,195.36,187.36,221.40,8.97"><head>Table 3 :</head><label>3</label><figDesc>Semantic Relation -Axiom Template mapping</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,72.00,443.80,226.82,154.43"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="7,72.00,443.80,226.82,154.43"><row><cell></cell><cell>lists several</cell></row><row><cell cols="2">examples of these types of questions. In TREC 2005,</cell></row><row><cell cols="2">16% of the FACTOID and LIST questions set temporal</cell></row><row><cell cols="2">constraints. For this year, that number rose to 20.1% and</cell></row><row><cell cols="2">included more complexity. Errors in temporal context un-</cell></row><row><cell cols="2">derstanding and processing comprised 14% of the errors</cell></row><row><cell>this year.</cell><cell></cell></row><row><cell cols="2">Q149.3: (The Daily Show) Who is host of The Daily</cell></row><row><cell>Show?</cell><cell></cell></row><row><cell>Locally</cell><cell>NYT19980810.0417 -Kilborn</cell></row><row><cell>Correct:</cell><cell></cell></row><row><cell>Globally</cell><cell>NYT19991220.0172 -John Stewart (who re-</cell></row><row><cell>Correct:</cell><cell>placed Craig Kilborn)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,86.16,615.76,198.26,8.97"><head>Table 4 :</head><label>4</label><figDesc>Questions with "locally correct" answers</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,313.20,364.24,226.89,321.71"><head>Table 5 :</head><label>5</label><figDesc>Syntactically-similar questionsTarget understanding remained a challenge this year as seen in the 18% error rate in Table6. Complex targets such as Pakistani government overthrown in 1999 and cloning of mammals (from adult cells) require better precision from PowerAnswer's question processing module to determine the relation between the target and the information being sought in the question. Questions that refer to answers from previous questions complicate the processing even more, as in the target series: Q197.1 What animal was the first mammal successfully cloned from adult cells?, the answer to which is used in Q197.2 What year was this animal born? and answers from both these is further required as a context to Q197.3 At what institute was this procedure done?</figDesc><table coords="7,368.76,572.28,113.23,113.67"><row><cell>Error</cell><cell>Dist</cell></row><row><cell>Answer Ranking</cell><cell>20%</cell></row><row><cell>Target Understanding</cell><cell>18%</cell></row><row><cell>Temporal Context</cell><cell>14%</cell></row><row><cell>Semantic Selection</cell><cell>12%</cell></row><row><cell cols="2">Answer Type Detection 11%</cell></row><row><cell>Keyword Expansion</cell><cell>9%</cell></row><row><cell>Keyword Selection</cell><cell>8%</cell></row><row><cell>NIL Answer</cell><cell>4%</cell></row><row><cell>Strategy Selection</cell><cell>3%</cell></row><row><cell>Passage Selection</cell><cell>1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,353.88,698.32,145.54,8.97"><head>Table 6 :</head><label>6</label><figDesc>ErrorDistribution (Factoid)    </figDesc><table coords="8,136.08,73.20,96.33,61.95"><row><cell>Type</cell><cell>Num</cell></row><row><cell>(Globally) Correct</cell><cell>233</cell></row><row><cell>Inexact</cell><cell>32</cell></row><row><cell>Locally Correct</cell><cell>11</cell></row><row><cell>Unsupported</cell><cell>12</cell></row><row><cell>Wrong</cell><cell>115</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,107.64,147.52,155.60,8.97"><head>Table 7 :</head><label>7</label><figDesc>Answer Distribution(Factoid)    </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="8,72.00,427.00,226.79,143.73"><head>Table 8 :</head><label>8</label><figDesc>Table 8 illustrates the final results of Language Computer's efforts in the 2006 TREC main QA track obtained by PowerAnswer 3. As seen in Table 8, PowerAnswer 3 was the top performer in 2 of 3 question types and best overall system. Results for the main QA task</figDesc><table coords="8,89.16,497.88,190.00,51.51"><row><cell></cell><cell>PowerAnswer 3</cell><cell>Best</cell><cell>Median</cell></row><row><cell>Factoid Acc.</cell><cell>0.578</cell><cell>0.578</cell><cell>0.186</cell></row><row><cell>List F-score</cell><cell>0.433</cell><cell>0.433</cell><cell>0.087</cell></row><row><cell>Other F-score</cell><cell>0.167</cell><cell>0.250</cell><cell>0.125</cell></row><row><cell>Overall</cell><cell>0.394</cell><cell>0.394</cell><cell>0.134</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="6,88.08,693.96,210.57,8.07;6,72.00,703.92,224.48,8.07;6,72.00,713.88,41.74,8.07"><p>Shorter chains are better than longer ones. The relations are not equally important and their order in the chain influences its strength.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the researchers and engineers from <rs type="affiliation">Language Computer Corporation</rs> for their invaluable contributions to this work. This work, and also our broader research in QA, is supported by the <rs type="funder">Disruptive Technology Office's (DTO) Advanced Question Answering for Intelligence (AQUAINT) Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,313.20,275.56,226.63,8.97;8,323.16,286.60,216.71,8.97;8,323.16,297.52,216.70,8.97;8,323.16,308.44,216.81,8.97;8,323.16,319.48,84.13,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,483.37,275.56,56.46,8.97;8,323.16,286.60,216.71,8.97;8,323.16,297.52,120.05,8.97">Using Knowledge Extraction and Maintenance Techniques to Enhance Analytical Performance</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bixler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,464.35,297.52,75.51,8.97;8,323.16,308.44,216.81,8.97;8,323.16,319.48,9.71,8.97">Proceedings of the 2005 International Conference on Intelligence Analysis</title>
		<meeting>the 2005 International Conference on Intelligence Analysis<address><addrLine>Washington D.C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,338.08,226.54,8.97;8,323.16,349.12,216.67,8.97;8,323.16,360.04,216.69,8.97;8,323.16,370.96,216.82,8.97;8,323.16,382.00,216.72,8.97;8,323.16,392.92,216.71,8.97;8,323.16,403.84,45.06,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,489.86,349.12,49.97,8.97;8,323.16,360.04,216.69,8.97;8,323.16,370.96,60.54,8.97">Looking under the hood: Tools for diagnosing your question answering engine</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rooth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,408.26,370.96,131.72,8.97;8,323.16,382.00,216.72,8.97;8,323.16,392.92,216.71,8.97;8,323.16,403.84,40.55,8.97">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001) Workshop on Open-Domain Question Answering</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001) Workshop on Open-Domain Question Answering</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,422.56,226.68,8.97;8,323.16,433.48,86.43,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,404.53,422.56,135.35,8.97;8,323.16,433.48,34.72,8.97">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,452.20,226.64,8.97;8,323.16,463.12,216.70,8.97;8,323.16,474.16,216.85,8.97;8,323.16,485.08,216.71,8.97;8,323.16,496.00,60.27,8.97" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Stephan</surname></persName>
		</author>
		<title level="m" coord="8,419.15,463.12,120.71,8.97;8,323.16,474.16,216.85,8.97;8,323.16,485.08,216.71,8.97;8,323.16,496.00,31.32,8.97">Applying COGEX to Recognize Textual Entailment In Prooceedings of the PAS-CAL Challenges Workshop on Recognising Textual Entailment</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,514.72,226.51,8.97;8,323.16,525.64,216.70,8.97;8,323.16,536.68,216.62,8.97;8,323.16,547.60,22.64,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,401.90,525.64,137.96,8.97;8,323.16,536.68,108.61,8.97">Employing Two Question Answering Systems in TREC-2005</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.62,536.68,87.15,8.97;8,323.16,547.60,18.11,8.97">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,566.20,226.66,8.97;8,323.16,577.24,216.82,8.97;8,323.16,588.16,216.69,8.97;8,323.16,599.08,149.70,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,372.16,566.20,167.70,8.97;8,323.16,577.24,152.46,8.97">The Web as a Resource for Question Answering: Perspectives and Challenges</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,490.49,577.24,49.49,8.97;8,323.16,588.16,216.69,8.97;8,323.16,599.08,93.95,8.97">Proceedings of the third International Conference on Language Resources and Evaluation</title>
		<meeting>the third International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,617.80,226.53,8.97;8,323.16,628.72,216.60,8.97;8,323.16,639.76,73.54,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,478.36,617.80,61.37,8.97;8,323.16,628.72,95.44,8.97">Lexical Chains for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,440.03,628.72,99.74,8.97;8,323.16,639.76,18.11,8.97">Proceedings of COLING 2002</title>
		<meeting>COLING 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="674" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,658.36,226.76,8.97;8,323.16,669.40,216.72,8.97;8,323.16,680.32,216.70,8.97;8,323.16,691.24,216.71,8.97;8,323.16,702.28,216.61,8.97;8,323.16,713.20,54.15,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,349.28,669.40,190.61,8.97;8,323.16,680.32,11.50,8.97">COGEX: A Logic Prover for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,359.48,680.32,180.38,8.97;8,323.16,691.24,216.71,8.97;8,323.16,702.28,189.51,8.97">Proceedings of the Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference</title>
		<meeting>the Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
	<note>HLT-2003</note>
</biblStruct>

<biblStruct coords="9,72.00,75.16,226.67,8.97;9,81.96,86.08,216.82,8.97;9,81.96,97.12,176.19,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,103.05,86.08,195.73,8.97;9,81.96,97.12,46.05,8.97">PowerAnswer-2: Experiments and Analysis over TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,146.69,97.12,106.94,8.97">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,115.96,226.74,8.97;9,81.96,127.00,216.69,8.97;9,81.96,137.92,216.69,8.97;9,81.96,148.84,87.75,8.97;9,72.00,167.80,226.67,8.97;9,81.96,178.72,216.82,8.97;9,81.96,189.76,203.62,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,267.70,115.96,31.04,8.97;9,81.96,127.00,166.71,8.97">Temporal Context Representation and Reasoning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">S</forename><surname>Munirathnam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,262.46,127.00,36.19,8.97;9,81.96,137.92,216.69,8.97;9,81.96,148.84,87.75,8.97;9,72.00,167.80,58.84,8.97;9,170.04,178.72,128.74,8.97;9,81.96,189.76,112.60,8.97">Proceedings of the Nineteenth Internation Joint Conference on Aritificial Intelligence D. Moldovan</title>
		<meeting>the Nineteenth Internation Joint Conference on Aritificial Intelligence D. Moldovan<address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
		</imprint>
	</monogr>
	<note>Synergist: Tools for Intelligence Analysis. NIMD Conference</note>
</biblStruct>

<biblStruct coords="9,72.00,208.72,226.64,8.97;9,81.96,219.64,216.92,8.97;9,81.96,230.56,216.71,8.97;9,81.96,241.48,150.88,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,197.97,208.72,100.67,8.97;9,81.96,219.64,51.07,8.97">Towards a Standard Upper Ontology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.71,219.64,147.17,8.97;9,81.96,230.56,216.71,8.97;9,81.96,241.48,16.59,8.97">Proceedings of the 2nd International Conference on Formal Ontology in Information Systems</title>
		<meeting>the 2nd International Conference on Formal Ontology in Information Systems<address><addrLine>Ogunquit, Maine</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">2001. October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,260.44,226.63,8.97;9,81.96,271.48,216.71,8.97;9,81.96,282.40,206.88,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,206.94,260.44,91.69,8.97;9,81.96,271.48,197.03,8.97">A Logic-based Semantic Approach to Recognizing Textual Entailment</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,81.96,282.40,202.51,8.97">Proceedings of the COLING/ACL 2006 Conference</title>
		<meeting>the COLING/ACL 2006 Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,301.36,226.56,8.97;9,81.96,312.28,216.82,8.97;9,81.96,323.20,216.59,8.97;9,81.96,334.24,216.85,8.97;9,81.96,345.16,43.50,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,160.81,312.28,137.97,8.97;9,81.96,323.20,134.50,8.97">COGEX at the Second Recognizing Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Iles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Slavick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.27,323.20,60.28,8.97;9,81.96,334.24,216.85,8.97;9,81.96,345.16,39.15,8.97">Proceedings of the PASCAL Second Recognising Textual Entailment Challenge</title>
		<meeting>the PASCAL Second Recognising Textual Entailment Challenge</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,364.12,226.75,8.97;9,81.96,375.04,216.70,8.97;9,81.96,385.96,216.69,8.97;9,81.96,397.00,211.07,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,201.44,364.12,97.31,8.97;9,81.96,375.04,216.70,8.97;9,81.96,385.96,31.74,8.97">A bootstrapping method for learning semantic lexicons using extraction pattern contexts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,136.25,385.96,162.39,8.97;9,81.96,397.00,206.85,8.97">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
