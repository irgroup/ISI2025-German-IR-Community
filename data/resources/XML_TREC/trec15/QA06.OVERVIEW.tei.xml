<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.81,112.60,388.38,14.93">Overview of the TREC 2006 Question Answering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.86,144.91,79.92,10.37"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
							<email>hoa.dang@nist.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.49,144.91,52.15,10.37"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.60,144.91,57.80,10.37"><forename type="first">Diane</forename><surname>Kelly</surname></persName>
							<email>dianek@email.unc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.81,112.60,388.38,14.93">Overview of the TREC 2006 Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C5EC361BFF25CF9D8EED2855110C104</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC 2006 question answering (QA) track contained two tasks: the main task and the complex, interactive question answering (ciQA) task. As in 2005, the main task consisted of series of factoid, list, and "Other" questions organized around a set of targets; in contrast to previous years, the evaluation of factoid and list responses distinguished between answers that were globally correct (with respect to the document collection), and those that were only locally correct (with respect to the supporting document). The ciQA task provided a framework for participants to investigate interaction in the context of complex information needs, and was a blend of the TREC 2005 QA relationship task and the TREC 2005 HARD track. Multiple assessors were used to judge the importance of information nuggets used to evaluate the responses to ciQA and "Other" questions, resulting in an evaluation that is more stable and discriminative than one that uses only a single assessor to judge nugget importance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the TREC question answering (QA) track is to foster research on systems that return answers themselves, rather than documents containing answers, in response to a natural language question. Since its inception in <ref type="bibr" coords="1,506.24,515.83,33.76,8.64;1,72.00,527.78,24.90,8.64">TREC-8 (1999)</ref>, the track has steadily expanded both the type and difficulty of the questions asked. The first several editions of the track focused on factoid questions. A factoid question is a fact-based, short answer question such as How many calories are there in a Big Mac? The task in the TREC 2003 QA track contained list and definition questions in addition to factoid questions <ref type="bibr" coords="1,200.27,563.65,67.24,8.64" target="#b7">(Voorhees, 2004)</ref>. A list question asks for different answer instances that satisfy the information need, such as List the names of chewing gums. Answering such questions requires a system to assemble a response from information located in multiple documents. A definition question asks for interesting information about a particular person or thing such as Who is Vlad the Impaler? or What is a golden parachute? Definition questions also require systems to locate information in multiple documents, but in this case the information of interest is much less crisply delineated.</p><p>In TREC 2004 <ref type="bibr" coords="1,148.62,635.38,71.25,8.64">(Voorhees, 2005a)</ref>, factoid and list questions were grouped into different series, where each series was associated with a target (a person, organization, or thing) and the questions in the series asked for some information about the target. In addition, the final question in each series was an explicit "Other" question, which was to be interpreted as "Tell me other interesting things about this target I don't know enough to ask directly". This last question was roughly equivalent to the definition questions in the TREC 2003 task.</p><p>Since the beginning of the QA track, the document returned with an answer had been used to determine the time frame for a question. For example, "Ronald Reagan" was considered a correct answer for the question Who is the President of the United States? if that answer was supported by a document from 1987, even if more recent documents supported "George Bush" as the answer. Such guidelines were appropriate because questions were primarily phrased in the present tense without specifying an explicit time frame. However, in the TREC 2005 main task, events were added as a possible target for the question series, and it became clear that the time frame implied by the series could not be ignored when judging the correctness of answers. Event targets and temporally-constrained questions required that questions be interpreted in the temporal context explicit in the question or implicit in the series.</p><p>The main task for the TREC 2006 QA track was the same as the main task in 2005, except that the implicit time frame for questions phrased in the present tense was the date of the last document in the document collection, rather than the document returned with the answer. Thus, systems were required to give the most up-to-date answer supported by the document collection. This restriction brought TREC QA more closely in line with question answering in the real world, where users would want the best answer to their question in the document collection, rather than just any answer found in any document. The evaluation of the question series in 2006 also down-weighted factoid questions, which had been tested for many years, by giving equal weight to each of the 3 question types in the final per-series score.</p><p>In addition to the main task, the TREC 2006 QA track also contained a complex, interactive QA (ciQA) task. The 2006 ciQA task was a blend of the TREC 2005 relationship task <ref type="bibr" coords="2,341.80,278.71,112.48,8.64" target="#b6">(Voorhees and Dang, 2006)</ref> and the TREC 2005 HARD track, which focused on single-iteration clarification dialogues <ref type="bibr" coords="2,363.25,290.67,53.29,8.64" target="#b0">(Allan, 2006)</ref>. The goals of the ciQA task were to push the frontiers of question answering away from "factoid" questions towards more complex information needs that exist within richer user contexts, and to move away from the one-shot interaction model implicit in previous evaluations towards a model based at least in part on interactions with users. Two metrics were introduced to evaluate answers to complex questions in the ciQA task: modified F-scores based on nugget pyramids and recall plots based on response length.</p><p>The remainder of this paper describes each of the two tasks in the TREC 2006 QA track in more detail. Section 2 describes the questions, evaluation methods, and results for the main task, while Section 3 discusses the ciQA task. The final section looks at the future of the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Main Task</head><p>The scenario for the main task in the TREC 2006 QA track was that an adult, native speaker of English was looking for information about a target of interest. The target could be a person, organization, thing, or event. The user was assumed to be an "average" reader of U.S. newspapers. Serving as surrogate users, NIST assessors developed the questions and judged the system responses.</p><p>The main task required systems to provide answers to a series of related questions. A question series, which focused on a target, consisted of several factoid questions, one to two list questions, and exactly one Other question. The order of questions in the series and the type of each question (factoid, list, or Other) were all explicitly encoded in the XML format used to describe the test set. Example series (minus the XML tags) are shown in Figure <ref type="figure" coords="2,532.53,519.91,3.74,8.64">1</ref>. The final test set contained 75 series; the targets of these series are given in Table <ref type="table" coords="2,416.66,531.87,3.74,8.64" target="#tab_0">1</ref>. Of the 75 targets, 19 were PERSONs, 19 were ORGANIZATIONs, 19 were EVENTs, and 18 were THINGs. The series contained a total of 403 factoid questions, 89 list questions, and 75 Other questions. Each series contained 6-9 questions (counting the Other question), with most series containing 8 questions.</p><p>Participants were required to submit results within one week of receiving the test set. All processing of the questions was required to be strictly automatic. Systems were required to process series independently from one another, and to process an individual series in question order. That is, systems were allowed to use questions and answers from earlier questions in a series to answer later questions in the same series, but could not "look ahead" and use later questions to help answer earlier questions. Thus, question series can be viewed as an abstraction of an information-seeking dialogue between the user and the system; cf. <ref type="bibr" coords="2,256.27,639.46,71.29,8.64" target="#b3">(Kato et al., 2004)</ref>. The document collection from which answers were to be drawn was the AQUAINT Corpus of English News Text (LDC catalog number LDC2002T31). As a convenience for track participants, NIST made available document rankings of the top 1000 documents per target as produced using the PRISE document retrieval system, with the target as the query. In total, 59 runs from 27 participants were 145.1 FACTOID How many non-white members of the jury were there?</p><p>145. The evaluation of a single run can be decomposed into component evaluations for each of the question types and a final per-series score. Each of the three question types has its own response format and evaluation method. The individual component evaluations in 2006 were identical to those used in the TREC 2005 QA track, except that a distinction was made between locally correct answers (supported in the associated document, but contradicted in later documents in the collection) and globally correct answers. An aggregate score was computed for each series in a run using a simple average of the component scores of questions in that series, and the final score for the run was computed as the average of its per-series scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Factoid questions</head><p>The system response to a factoid question was either exactly one [doc-id, answer-string] pair or the literal string 'NIL'. Since there was no guarantee that a factoid question had an answer in the document collection, NIL was returned by the system when it believed there was no answer. Otherwise, answer-string was a string containing precisely an answer to the question, and doc-id was the id of a document in the collection that supported answer-string as an answer.</p><p>Each response was independently judged by two human assessors. When the two assessors disagreed in their judgments, a third adjudicator made the final determination. Each response was assigned exactly one of the following five judgments:</p><p>incorrect: the answer string does not contain a correct answer or the answer is not responsive; not supported: the answer string contains a correct answer but the document returned does not support that answer; not exact: the answer string contains a correct answer and the document supports that answer, but the string contains more than just the answer or is missing bits of the answer;</p><p>locally correct: the answer string consists of exactly a correct answer that is supported by the document returned, but a more recent document contradicts the answer;</p><p>globally correct: the answer string consists of exactly the correct answer, that answer is supported by the document returned, and there are no later documents that contradict the answer.</p><p>To be responsive, an answer string was required to contain appropriate units and to refer to the correct "famous" entity (e.g., the Taj Mahal casino is not responsive if the question asks about "the Taj Mahal"). Questions also had to be interpreted in the time frame implied by the question series. For example, if the target was the event "France wins World Cup in soccer" and the question was Who was the coach of the French team? then the correct answer must be "Aime Jacquet", the name of the coach of the French team in 1998 when France won the World Cup, and not just the name of any past or current coach of the French team. NIL responses were correct only if there was no known answer to the question in the collection. NIL was correct for 17 of the 403 factoid questions in the test set. For 26 questions, no system returned the correct answer, although those questions did have a correct answer found by the assessors.</p><p>The main evaluation metric for the factoid component was accuracy, the fraction of questions judged to be globally correct. Table <ref type="table" coords="5,130.78,547.81,4.98,8.64">2</ref> shows the most accurate run for the factoid component for each of the top 10 groups. Also reported are the recall and precision of recognizing when no answer exists in the document collection. NIL precision is the ratio of the number of times NIL was returned and correct to the number of times it was returned; NIL recall is the ratio of the number of times NIL was returned and correct to the number of times it was correct in the entire test set (17). If NIL was never returned, NIL precision is undefined and NIL recall is zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">List questions</head><p>A list question asks for different instances of a particular type. The correct answer for a list question is the set of all such distinct instances in the document collection. A system's response to a list question consists of an unordered set of <ref type="bibr" coords="5,82.79,669.27,92.22,8.82">[doc-id, answer-string]</ref> pairs such that each answer-string represents a correct answer instance.</p><p>During the evaluation process, the assessor was given an entire system's run at a time. Each instance was evaluated in the same manner as the factoid questions, i.e., assigned one of the following judgments: incorrect, unsupported, not exact, locally correct, and globally correct. In addition to judging for correctness, the assessor also marked the answer instances for distinctness. The assessor arbitrarily chose any one of equivalent responses to be distinct, and the remainder were considered not distinct. Thus, systems were not rewarded (and in fact, penalized) for returning equivalent answer instances multiple times. Only globally correct responses could be marked as distinct.</p><p>The final set of globally correct answers for a list question was compiled from the union of distinct globally correct answers across all runs plus instances the assessor found during question development. For the 89 list questions in the test set, the average number of answers per question was 10, with a minimum of 2 and a maximum of 50. A system's response to a list question was scored using instance precision (IP) and instance recall (IR) based on the complete list of known distinct instances. Let S be the number of such instances, D be the number of globally correct, distinct responses returned by the system, and N be the total number of responses returned by the system. Then IP = D/N and IR = D/S. Precision and recall were then combined to produce an F-score with equal weight given to recall and precision:</p><formula xml:id="formula_0" coords="6,266.73,240.54,77.27,22.31">F = 2 × IP × IR IP + IR</formula><p>The score for the list component of a run was the average F-score over the 89 questions. Table <ref type="table" coords="6,462.26,268.82,4.98,8.64">3</ref> gives the average F-score of the run with the best list component score for each of the top 10 groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other questions</head><p>The Other questions were evaluated using the methodology originally developed for the TREC 2003 definition questions. A system's response for an Other question consisted of an unordered set of <ref type="bibr" coords="6,403.09,342.40,92.64,8.82">[doc-id, answer-string]</ref> pairs. The answer strings were presumed to contain interesting "nuggets" about the series target that had not yet been covered by earlier questions in the series. The requirement to not repeat information already covered by earlier questions in the series made answering Other questions more difficult than answering TREC 2003 definition questions.</p><p>Judging the quality of the systems' responses was performed in two steps. In the first step, all of the answer strings from all of the systems were presented to an assessor in a single list. Using all the answer strings and searches done during question development, the assessor created a list of information nuggets about the target. An information nugget in the context of an Other question is defined as an atomic piece of information about the target that is interesting (in the assessor's opinion) and is not part of an earlier question in the series or an answer to an earlier question in the series. An information nugget is considered atomic if the assessor could make a binary decision as to whether the nugget appears in a response. Once the nugget list was created for a target, the assessor decided which were vital, meaning that the information must be returned for a response to be good. Non-vital ("okay") nuggets acted as "don't care" conditions in that the assessor believed the information in the nugget to be interesting enough that returning the information was acceptable in, but not necessary for, a good response.</p><p>In the second step of the evaluation process, the assessor went through each system's output in turn and marked which nuggets appeared in the response. An answer string contained a nugget if there was a conceptual match between the answer string and the nugget; that is, the match was independent of the particular wording used in either the nugget or the system output. A nugget match was marked at most once per response-if the system output contained more than one match for a nugget, an arbitrary match was marked and the remainder were left unmarked. A single [doc-id, answer-string] pair in a system response could match 0, 1, or multiple nuggets.</p><p>Given the nugget list and the set of nuggets matched in a system's response, nugget recall was computed as the ratio of the number of matched nuggets to the total number of vital nuggets in the list. Nugget precision was much more difficult to compute since there was no effective way of enumerating all the concepts contained in a particular answer string. Instead, a measure based on length (in non-whitespace characters) was used as an approximation to nugget precision. The length-based measure granted an allowance of 100 characters for each (vital or non-vital) nugget matched. If the total system output was less than this number of characters, the value of nugget precision was 1.0. Otherwise, the measure's value decreased as the length increased according to the following formula:</p><formula xml:id="formula_1" coords="6,252.62,672.78,106.76,22.31">1 - length -allowance length .</formula><p>The final score for an Other question was an F-score, with nugget recall weighted more heavily than nugget precision:</p><formula xml:id="formula_2" coords="7,226.66,94.81,158.68,24.10">F (β) = (β 2 + 1) × precision × recall β 2 × precision + recall .</formula><p>The score for the Other questions component was the average F-score (β=3) over the 75 Other questions. Table <ref type="table" coords="7,535.02,130.31,4.98,8.64">4</ref> gives the F-score for the best scoring Other question component for each of the top 10 groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Nugget Pyramids</head><p>The vital/okay distinction has previously been identified as a weakness in the TREC nugget-based evaluation methodology <ref type="bibr" coords="7,98.00,199.43,101.84,8.64" target="#b2">(Hildebrandt et al., 2004)</ref>. Since only vital nuggets affect nugget recall, it is difficult for systems to achieve non-zero scores on topics with few vital nuggets in the answer key. Thus, scores are easily affected by assessor errors and other random variations in evaluation conditions. One direct consequence is that in previous TREC evaluations, the median score for many questions turned out to be zero <ref type="bibr" coords="7,315.03,235.29,72.58,8.64">(Voorhees, 2005b)</ref>. A binary distinction on nugget importance is insufficient to discriminate between the quality of runs that return no vital nuggets but different numbers of okay nuggets. To address many of these issues, <ref type="bibr" coords="7,282.56,259.20,136.45,8.64" target="#b4">Lin and Demner-Fushman (2006)</ref> proposed an extension called "nugget pyramids", in which multiple assessors provide judgments of whether a nugget is vital or simply okay.</p><p>To examine the effectiveness of the pyramid approach, NIST also computed F-scores for Other responses using the pyramid extension. Nine different sets of vital/okay judgments were solicited from eight unique assessors (the primary assessor who originally created the nuggets later assigned vital/okay labels again). Each assessor was given all the questions for the series, as well as the nuggets created by the primary assessor. Using the pyramid procedure, a weight was assigned to each nugget based on the number of assessors who marked it as vital. These nugget weights were then incorporated into the nugget recall computation.</p><p>The left graph in Figure <ref type="figure" coords="7,183.74,354.84,4.98,8.64">2</ref> plots the average F-scores for each run as computed using a single assessor vs. using the nugget pyramid. Even though the nugget pyramid does not represent any single real user, average pyramid F-scores do correlate highly with average single-assessor F-scores; the Pearson's correlation is 0.987, with a 95% confidence interval of [0.980, 1.00].</p><p>While the average F-score for a particular run is stable given a large enough number of questions, the F-score for a single Other question does vary depending on the assessor. The right graph in Figure <ref type="figure" coords="7,433.69,414.62,4.98,8.64">2</ref> plots the single-assessor and pyramid F-scores for each individual Other question from all submitted runs. The Pearson correlation between single-assessor and pyramid F-scores in this case is 0.870, with a 95% confidence interval of [0.863, 1.00]. For 16.4% of the questions, the nugget pyramid assigned a non-zero F-score where the original single-assessor F-score was zero. Thus, from the perspective of system developers, the F-scores from the nugget pyramids may be more useful since they are more discriminative. For a more detailed analysis of the nugget pyramids extension, please refer to <ref type="bibr" coords="7,498.47,474.39,41.53,8.64;7,72.00,486.35,40.26,8.64" target="#b1">(Dang and Lin, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Per-series Combined Weighted Scores</head><p>The three component scores measure a system's ability to process each type of question, but may not reflect the system's overall usefulness to a user. Since each individual series is an abstraction of a single user's interaction with the system, taking the individual series as the basic unit of evaluation should provide a more accurate representation of the effectiveness of the system from an individual user's perspective. Since each series is a mixture of different question types, we can compute a weighted average of the scores of the three question types on a per-series basis, and take the average of the per-series weighted scores as the final score for the run <ref type="bibr" coords="7,409.98,596.04,72.12,8.64">(Voorhees, 2005b)</ref>. In 2006, the weighted average of the three component scores for an individual series was computed as:</p><formula xml:id="formula_3" coords="7,192.20,627.21,227.60,22.31">WeightedScore = 1 3 × Factoid + 1 3 × List + 1 3 × Other.</formula><p>To compute the weighted score for an individual series, only the scores for questions belonging to that series were included in the computation. Since each of the component scores ranges between 0 and 1, the weighted score is also in that range. In contrast to previous years, when factoid questions were weighted more heavily than the other questions, equal weight was given to the three components in 2006. The final per-series score of each run is simply the average of individual per-series scores.</p><p>Table <ref type="table" coords="9,111.82,99.39,4.98,8.64" target="#tab_1">5</ref> shows the final per-series score for the best run from each group. We fit a two-way analysis of variance model with the target type and the best run from each group as factors, and the final per-series score as the dependent variable; we found significant differences between target types (p = 0.005) and runs (p essentially equal to 0). To determine which runs were significantly different from each other, we performed a multiple comparison using Tukey's honestly significant difference criterion and controlling for the experiment-wise Type I error so that the probability of declaring a difference between two runs to be significant when it is actually not, is at most 5%. Table <ref type="table" coords="9,491.79,159.16,4.98,8.64" target="#tab_1">5</ref> shows the results of the multiple comparison; runs sharing a common letter are not significantly different. A similar multiple comparison showed that PERSON targets had significantly higher scores than EVENTs, but no significant differences between any of the other target types were found.</p><p>System scores on the main task have declined since TREC 2004 even though the question series format of the main task has been the same. This is not surprising given that the questions have become increasingly more difficult, with "simple" factoid questions requiring higher levels of reasoning to extract the correct answer from the documents. Assessors also have become more strict about disallowing inexact answers as correct answers.</p><p>3 The Complex, Interactive QA (ciQA) Task</p><p>The goal of the complex, interactive question answering (ciQA) task is to push the frontiers of question answering in two directions:</p><p>• A move away from "factoid" questions towards more complex information needs that exist within richer user contexts. (Question series in the main task also exemplify this shift in evaluation focus.)</p><p>• A move away from the one-shot interaction model implicit in previous evaluations towards a model based at least in part on interactions with users.</p><p>In terms of implementation, the 2006 ciQA task was a blend of the TREC 2005 relationship task <ref type="bibr" coords="9,482.41,389.27,57.59,8.64;9,72.00,401.23,49.56,8.64" target="#b6">(Voorhees and Dang, 2006)</ref> and the TREC 2005 HARD track, which focused on single-iteration clarification dialogues <ref type="bibr" coords="9,484.07,401.23,51.64,8.64" target="#b0">(Allan, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Complex "Relationship" Questions</head><p>The complex information needs explored by ciQA represented an extension and refinement of so-called "relationship" questions piloted in TREC 2005. This choice provided some continuity and training data for participants.</p><p>The concept of a "relationship" is defined as the ability of one entity to influence another, including both the means to influence and the motivation for doing so. Evidence for both the existence or absence of ties is relevant. The particular relationships of interest naturally depend on the context.</p><p>A relationship question in the ciQA task, which we refer to as a topic, is composed of two parts. Consider an example:</p><p>Template: What evidence is there for transport of [drugs] from [Mexico] to [the U.S.]? Narrative: The analyst would like to know of efforts to curtail the transport of drugs from Mexico to the U.S. Specifically, the analyst would like to know of the success of the efforts by local or international authorities.</p><p>The question template is a stylized information need that has a fixed structure and free slots (items in square brackets) whose instantiation varies across different topics. The narrative is free-form natural language text that elaborates on the information need, providing, for example, user context, a more articulated statement of interest, focus on particular topical aspects, etc. Five template types were developed for the ciQA task, enumerated in Figure <ref type="figure" coords="9,532.53,635.81,3.74,8.64" target="#fig_0">3</ref>. For the final test set, NIST assessors developed a total of 30 topics, with 6 topics for each of these templates.</p><p>Answers to ciQA topics consisted of [doc-id, answer-string] pairs, and were evaluated using the same nugget-based methodology that was employed for the main task Other questions. However, the total length of system responses was limited to 7,000 non-whitespace characters. Two metrics were employed to quantify answer quality:  • The first and primary metric was the F-score (β = 3) with the "nugget pyramid" extension.</p><p>• The second metric was new for the ciQA task and attempted to graphically capture the tradeoffs between conciseness and completeness <ref type="bibr" coords="11,208.70,353.22,45.09,8.64" target="#b5">(Lin, 2007)</ref>. The basic idea is to quantify weighted nugget recall (what we call pyramid recall) as a function of answer length (in non-whitespace characters). By the nugget pyramid building process, each nugget is assigned a weight between zero and one. Weighted nugget recall is the sum of weights of all nuggets retrieved divided by the sum of all weights of all nuggets in the assessor's answer key.</p><p>Implementing this metric required two important changes to the previous evaluation protocol:</p><p>1. Answer strings must be rank ordered, with best first.</p><p>2. Assessors must mark the first instance of a nugget in the response set of answer strings.</p><p>For the recall plots, the scoring methodology was as follows (character counts do not include whitespaces):</p><p>1. For each topic, NIST recorded the cumulative character length and pyramid recall after each answer string had been assessed.</p><p>2. Each data point was interpolated to the nearest 100 character increment (longer than the current length). For example, a pyramid recall of 0.25 at 168 characters would be interpolated to (200, 0.25). Plotting these points yielded pyramid recall as a function of answer length for a particular topic.</p><p>3. To arrive at the recall plot for a particular system run, the mean of the recall values was taken across all topics at each length increment, i.e., mean pyramid recall over all topics at 100 characters, at 200 characters, at 300 characters, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interactive Question Answering</head><p>The purpose of the interactive aspect of ciQA was to provide a framework for participants to investigate interaction in the QA context and to provide an opportunity for non-QA researchers to become involved in this area. We consider an interactive system to be a system that gives users control over all or a portion of displayed content. Using this definition, the smallest possible interaction unit consists of the user responding to the system and the system using the user's response to produce new content. The interactive aspect of ciQA was concerned with this interaction unit and was modeled in part after the HARD track's clarification forms.</p><p>The HARD track's clarification forms allowed participants to elicit information from assessors through a single interaction. This interaction consisted of assessors completing forms (i.e., Web pages) that had been created by track participants. The results of these interactions were then returned to the participants so that revised results could be generated-comparison of output before and after the clarification quantified the effects of the interaction.</p><p>Although many participants took advantage of the opportunity provided by the HARD track to investigate traditional relevance feedback techniques, this was not a goal of the HARD track nor a condition for participation; there were, in fact, some participants who used clarification forms in novel ways. In the ciQA task, we explicitly encouraged innovative ways of using forms that go beyond traditional relevance feedback. The question answering community has yet to reach common ground on the role of interaction in QA, and the ciQA task was meant to provide a forum for continued dialogue.</p><p>The rationale for studying the smallest interaction unit rests on the idea that a good QA system should return relevant information with a minimum amount of interaction. Furthermore, given the potential complexities that are likely to arise with coordinating cross-site interactive evaluations, we believe that using the smallest interaction unit is a reasonable starting point in the exploration of interactive QA. Previous experiences with the TREC interactive track demonstrated that coordinating multi-site interactive IR system evaluation is a challenge and that results are difficult (if not impossible) to compare.</p><p>In more detail, interaction forms were HTML pages created by participants that solicited user input via CGI. Although NIST placed no restrictions on the type of content, there were technical restrictions (see below). Each question was associated with a unique form, and each site was limited to two sets of interaction forms (which provided the ability to evaluate two different interaction techniques).</p><p>NIST assessors completed the interaction forms on Redhat Enterprise Linux workstations with 20-inch LCD monitors (1600×1200 resolution and millions of colors) using the Firefox Web browser (v1.5.0.2). The machines at NIST were disconnected from all networks and participants were required to provide all necessary information as part of their forms. If a form required multiple files, then it was necessary for such files to be contained within the submitted directory structure. These forms were not allowed to invoke any CGI scripts or write to disk. Javascript was allowed, but Java was not.</p><p>Assessors spent no more than three minutes completing each interaction form. This duration included the time needed to load the form, initialize any content, and then render it. At the end of three minutes, if the assessor had not submitted the form, the form timed out and was forcibly submitted. The CGI variable bindings associated with the forms captured the results of the interactions, which NIST returned to the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The ciQA evaluation proceeded as follows:</p><p>1. Participants submitted initial runs and interaction forms.</p><p>2. NIST assessors interacted with the forms.</p><p>3. NIST returned results of the interaction (i.e., the CGI bindings).</p><p>4. Participants submitted final runs based on the results of the interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">NIST evaluateed both initial and final runs.</head><p>As with the main task, the AQUAINT collection of newswire articles served as the official corpus. To support the individual goals of participants, ciQA was entirely independent of the main task; the interactive aspect was also optional, which allowed participants to focus solely on complex QA if they desired. Finally, both automatic and manual runs were allowed. A manual run was defined as any run where human intervention occurred in any part of the process (except assessor interaction with the submitted interaction forms).</p><p>The ciQA task drew participation from six groups. NIST received ten initial runs and eleven final runs. A total of ten sets of interaction forms were submitted by the six participants. In addition, a pair of initial/final runs that used simple sentence retrieval techniques was submitted as a baseline implementation (described below). A set of interaction forms was also associated with this run pair.</p><p>We constructed a rotation specifying the order in which interaction forms would be presented to assessors to minimize learning and order effects, and to insure that each form would occupy each position in the rotation (e.g., first, second, third) as equal a number of times as possible. This rotation is shown in Table <ref type="table" coords="13,446.49,123.30,3.74,8.64" target="#tab_2">6</ref>. Row headings show topic numbers, while column headings represent forms. Cell numbers indicate the presentation order of the form; for example, for Topic 26, CLR1 was the fourth form presented and strath3 was the first. This rotation is based on a basic Latin square rotation; the relationship between forms is preserved, but the position of the form is shifted across topics. For example, strath2 always followed CLR1 except when it was the first form in the rotation, and strath2 and CLR1 were each the first, second, third, etc. form in the rotation an equal number of times. To construct the order, forms and topics were randomly assigned to column and row headings, and an order of 1, 2, 3, 4, etc. was assigned to the first row, 2, 3, 4, 5, etc. was assigned to the second row, etc. The table has been sorted according to topic, but one can see that Topics 31, 34 and 39 appeared in either the 1st, 12th, or 23rd row of the randomized table.</p><p>In total, there were eleven different initial-final run pairs. The pyramid F-scores of these run pairs are shown in Table <ref type="table" coords="13,96.10,242.85,3.74,8.64">7</ref>. Pyramid F-scores were computed using the methodology outlined in <ref type="bibr" coords="13,379.43,242.85,133.92,8.64" target="#b4">(Lin and Demner-Fushman, 2006)</ref>. Nine different sets of vital/okay judgments were solicited from eight unique assessors (the assessor who originally created the nuggets later assigned vital/okay labels again).</p><p>In addition to runs submitted by the participants, the University of Maryland separately prepared a sentence retrieval baseline. For each topic, the verbatim question template was used as a query to Lucene, which returned the top 20 documents. These documents were then tokenized into individual sentences. Sentences that contained at least one non-stopword from the question were retained and returned as the initial run (up to the 7,000 character limit). Sentence order within each document and across the ranked list was preserved. The interaction forms associated with this run asked the assessor for relevance judgments on each of the sentences (relevant, not relevant, don't know). The final run was prepared by removing sentences judged not relevant-this had the effect of pulling in more sentences from documents lower in the ranked list. The performance of this sentence retrieval baseline is also shown in Table <ref type="table" coords="13,532.53,362.40,3.74,8.64">7</ref>.</p><p>Surprisingly, the sentence retrieval baseline performed exceedingly well. Only two initial runs received a higher score, one of which was a manual run. Only two final runs received a higher score, one of which was a manual run. The high baseline performance is consistent with findings from previous TREC results <ref type="bibr" coords="13,428.48,398.27,67.59,8.64" target="#b7">(Voorhees, 2004)</ref>. Figure <ref type="figure" coords="13,535.02,398.27,4.98,8.64">4</ref> shows a scatter plot of the initial and final F-scores for all eleven run pairs. Points below the reference line y = x represent cases in which interaction actually decreased performance-there were two such cases.</p><p>Plots of pyramid recall as a function of response length are shown in Figure <ref type="figure" coords="13,395.84,434.13,3.74,8.64">5</ref>. These graphs attempt to quantify how quickly a user is able to acquire relevant nuggets by reading system responses. Naturally, curves that rise more quickly represent "better" systems. In the top graph, the sentence retrieval baseline is compared against the best automatic run. In the bottom graph, the sentence retrieval baseline is compared against the best manual run. It is interesting to note that for the automatic runs, these recall plots paint a different picture of performance than the pyramid F-scores. Although UWATCIQA4 achieved a higher pyramid F-score than the final submission of the sentence retrieval baseline, the recall plots suggest that the sentence retrieval baseline is able to deliver more information given the same response length. For the manual run, although the recall plots show little difference between the nugget content of the pre-and post-interaction system responses, the pyramid F-scores suggest a difference in answer quality. More work is needed to understand the divergences between pyramid F-scores and these recall plots.</p><p>These results appear to suggest that the complex QA task is difficult, but that off-the-shelf IR systems provide a strong baseline. The effective use of linguistic analysis techniques for complex questions remains an open research question. For a more in-depth exploration of these issues and the evaluation methodology, see <ref type="bibr" coords="13,448.03,577.59,43.58,8.64" target="#b5">(Lin, 2007)</ref>.</p><p>4 Future of the QA Track At the TREC 2006 workshop, participants indicated that they would like to have longer, more complex interactions in the ciQA task rather than short interactions via cached interaction forms. Participants proposed trying "live interactions" for 2007. Under this setup, the interactive QA system would be located at a URL on the participant's machine, and NIST assessors would simply navigate to the URL. The advantage would be that participants would be able to host more complex interaction interfaces. On the other hand, this setup would put additional burden on each partici-  <ref type="table" coords="15,97.54,264.59,3.88,8.64">7</ref>: Performance of the eleven initial-final pairings for the ciQA task, along with the sentence retrieval baseline.</p><p>pant; if the NIST assessor could not reach a site for any reason during the interaction period -even due to problems outside the control of the site -the assessor would simply ignore the site. A straw poll indicated preference for live interactions, and the ciQA task will be repeated in 2007 with live URLs and a longer interaction period. Based on the successful application of the nugget pyramid evaluation method in TREC 2006, the pyramid method will be the official evaluation method for both the ciQa and the Other questions in TREC 2007. Since the main task had been run largely unchanged for three years, a radical change was proposed to push the state of the art forward. The series format has supported the evaluation of different types of questions (factoid, list and Other) while providing an abstraction of a real user session with a QA system; therefore, rather than changing the series format, it was decided to move the main task forward by changing the genre of the document collection. The main task for the TREC 2007 QA Track will again be series of factoid, list, and Other questions, but the document collection will be a combination of newswire and blogs. Mining blogs for answers will introduce significant new challenges in at least two aspects that are very important for functional QA systems: 1) being able to handle language that is not well-formed, and 2) dealing with discourse structures that are more informal and less reliable than newswire. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,179.16,289.30,253.68,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The five templates used in the TREC 2006 ciQA task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="17,127.11,510.45,357.78,8.64"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Scatter plot showing initial and final pyramid F-scores for submitted ciQA runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,224.14,468.00,340.01"><head>Table 1 :</head><label>1</label><figDesc>Targets of the 75 question series. submitted to the main task.</figDesc><table coords="3,72.00,224.14,468.00,340.01"><row><cell cols="2">2 FACTOID Who was the foreman for the jury?</cell></row><row><cell cols="2">145.3 FACTOID Where was the trial held?</cell></row><row><cell cols="2">145.4 FACTOID When was King convicted?</cell></row><row><cell cols="2">145.5 FACTOID Who was the victim of the murder?</cell></row><row><cell>145.6 LIST</cell><cell>What defense and prosecution attorneys participated in the trial?</cell></row><row><cell>145.7 OTHER</cell><cell></cell></row><row><cell>185 Iditarod Race</cell><cell></cell></row><row><cell cols="2">185.1 FACTOID In what city does the Iditarod start?</cell></row><row><cell cols="2">185.2 FACTOID In what city does the Iditarod end?</cell></row><row><cell cols="2">185.3 FACTOID In what month is it held?</cell></row><row><cell cols="2">185.4 FACTOID Who is the founder of the Iditarod?</cell></row><row><cell>185.5 LIST</cell><cell>Name people who have won the Iditarod.</cell></row><row><cell cols="2">185.6 FACTOID How many miles long is the Iditarod?</cell></row><row><cell cols="2">185.7 FACTOID What is the record time in which the Iditarod was won?</cell></row><row><cell>185.8 LIST</cell><cell>Which companies have sponsored the Iditarod?</cell></row><row><cell>185.9 OTHER</cell><cell></cell></row><row><cell>212 Barry Manilow</cell><cell></cell></row><row><cell cols="2">212.1 FACTOID What year was he born?</cell></row><row><cell cols="2">212.2 FACTOID How many times has he married?</cell></row><row><cell cols="2">212.3 FACTOID What is the name of the musical that he wrote about the Harmonistas?</cell></row><row><cell cols="2">212.4 FACTOID What music school did he attend?</cell></row><row><cell cols="2">212.5 FACTOID For what female singer was he the musical director and pianist in the 70's?</cell></row><row><cell cols="2">212.6 FACTOID What record label did he sing for in 2000?</cell></row><row><cell>212.7 LIST</cell><cell>List the songs he recorded.</cell></row><row><cell>212.8 OTHER</cell><cell></cell></row><row><cell cols="2">Figure 1: Sample question series from the test set. Series 145 has an EVENT as the target, series 185 has a THING as</cell></row><row><cell cols="2">the target, and series 212 has a PERSON as the target.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,72.00,113.92,468.00,564.43"><head>Table 5 :</head><label>5</label><figDesc>Figure 2: Other F-score computed using a single primary assessor vs. using multiple assessors, by individual question (right), and averaged over all questions for each submitted run (left). Multiple comparison of the best run from each group, based on ANOVA of per-series score. What evidence is there for transport of [goods] from [entity] to [entity]? Example: What evidence is there for transport of [drugs] from [Mexico] to [the U.S.]? What [relationship] exist between [entity] and [entity]? (where [relationship] ∈ {"financial relationships", "organizational ties", "familial ties", "common interests"}) Example: What [financial relationships] exist between [drug companies] and [universities]? What is the position of [John McCain] with respect to [the Moral Majority or the Christian Coalition]? Is there evidence to support the involvement of [entity] in [event/entity]? Example: Is there evidence to support the involvement of [China] in [human organ transplants from Chinese prisoners]?</figDesc><table coords="10,89.29,113.92,396.28,542.78"><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average pyramid Other score</cell><cell>0.05 0.10 0.15 0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pyramid Other score</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0.25</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell cols="4">Average primary-assessor Other score</cell><cell></cell><cell></cell><cell cols="3">Primary-assessor Other score</cell></row><row><cell></cell><cell></cell><cell>RunID</cell><cell></cell><cell cols="2">Per-series score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>lccPA06</cell><cell></cell><cell></cell><cell>0.3938</cell><cell>A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>LCCFerret</cell><cell></cell><cell></cell><cell>0.2644</cell><cell>B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">cuhkqaepisto</cell><cell></cell><cell>0.2310</cell><cell>B C</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ed06qar1</cell><cell></cell><cell></cell><cell>0.2066</cell><cell cols="2">B C D</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">FDUQAT15A</cell><cell></cell><cell>0.1918</cell><cell cols="2">C D E</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">NUSCHUAQA3</cell><cell></cell><cell>0.1908</cell><cell cols="2">C D E</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">QACTIS06A</cell><cell></cell><cell>0.1853</cell><cell cols="2">C D E F</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ILQUA1</cell><cell></cell><cell></cell><cell>0.1713</cell><cell cols="3">D E F G</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>QASCU1</cell><cell></cell><cell></cell><cell>0.1588</cell><cell cols="3">D E F G H</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Roma2006run3</cell><cell></cell><cell>0.1571</cell><cell cols="3">D E F G H</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>InsunQA06</cell><cell></cell><cell></cell><cell>0.1568</cell><cell cols="3">D E F G H</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MITRE2006C</cell><cell></cell><cell>0.1485</cell><cell></cell><cell cols="2">E F G H</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ISL2</cell><cell></cell><cell></cell><cell>0.1430</cell><cell></cell><cell cols="3">E F G H I</cell><cell></cell></row><row><cell></cell><cell></cell><cell>csail02</cell><cell></cell><cell></cell><cell>0.1344</cell><cell></cell><cell cols="3">E F G H I</cell><cell></cell></row><row><cell></cell><cell></cell><cell>shef06ss</cell><cell></cell><cell></cell><cell>0.1344</cell><cell></cell><cell cols="3">E F G H I</cell><cell></cell></row><row><cell></cell><cell></cell><cell>lsv2006c</cell><cell></cell><cell></cell><cell>0.1298</cell><cell></cell><cell cols="3">F G H I J</cell><cell></cell></row><row><cell></cell><cell></cell><cell>asked06c</cell><cell></cell><cell></cell><cell>0.1156</cell><cell></cell><cell></cell><cell cols="2">G H I J</cell><cell></cell></row><row><cell></cell><cell></cell><cell>uw574</cell><cell></cell><cell></cell><cell>0.1083</cell><cell></cell><cell></cell><cell cols="3">H I J K</cell></row><row><cell></cell><cell></cell><cell cols="2">DLT06QA02</cell><cell></cell><cell>0.0871</cell><cell></cell><cell></cell><cell></cell><cell cols="2">I J K L</cell></row><row><cell></cell><cell></cell><cell cols="2">TIQA200601</cell><cell></cell><cell>0.0851</cell><cell></cell><cell></cell><cell></cell><cell cols="2">I J K L</cell></row><row><cell></cell><cell></cell><cell>clr06m</cell><cell></cell><cell></cell><cell>0.0763</cell><cell></cell><cell></cell><cell></cell><cell cols="2">J K L</cell></row><row><cell></cell><cell></cell><cell cols="2">TWQA0601</cell><cell></cell><cell>0.0725</cell><cell></cell><cell></cell><cell></cell><cell cols="3">J K L M</cell></row><row><cell></cell><cell></cell><cell>irstqa06</cell><cell></cell><cell></cell><cell>0.0573</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">K L M</cell></row><row><cell></cell><cell></cell><cell>Dal06e</cell><cell></cell><cell></cell><cell>0.0459</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">L M</cell></row><row><cell></cell><cell></cell><cell>lexiclone06</cell><cell></cell><cell></cell><cell>0.0458</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">L M</cell></row><row><cell></cell><cell></cell><cell>lf10w10g5</cell><cell></cell><cell></cell><cell>0.0312</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">L M</cell></row><row><cell></cell><cell></cell><cell cols="2">TREC06ST01</cell><cell></cell><cell>0.0167</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,72.00,175.97,468.00,413.09"><head>Table 6 :</head><label>6</label><figDesc>Form rotation according to topic. As a specific example: for Topic 26, the form CLR1 was presented fourth and the form strath3 was presented first.</figDesc><table coords="14,82.53,175.97,449.55,369.43"><row><cell cols="2">Topic CLR1</cell><cell>strath2</cell><cell>csaili2</cell><cell cols="4">UMDA1 UMAS1 UWAT1 CLR2</cell><cell>csaili1</cell><cell>strath3</cell><cell>UMDM1 Baseline1</cell></row><row><cell>26</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell></row><row><cell>27</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>28</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell></row><row><cell>29</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>30</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>31</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>32</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>33</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell></row><row><cell>34</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>35</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>36</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>37</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>38</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>39</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>40</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>41</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell></row><row><cell>42</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>43</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>44</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>45</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>46</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>47</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell></row><row><cell>48</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>49</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>50</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>51</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>52</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell></row><row><cell>53</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>54</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>55</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="15,72.00,499.64,468.00,8.82;15,86.94,511.60,233.63,8.59" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,154.44,499.82,316.48,8.64">HARD track overview in TREC 2005: High accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,490.65,499.64,49.35,8.59;15,86.94,511.60,175.24,8.59">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2006. TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,530.83,468.00,8.64;15,86.94,542.61,453.05,8.82;15,86.94,554.56,158.02,8.59" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,237.03,530.83,302.97,8.64;15,86.94,542.78,185.35,8.64">Different structures for evaluating answers to complex questions: Pyramids won&apos;t topple, and neither will human assessors</title>
		<author>
			<persName coords=""><forename type="first">Trang</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,290.74,542.61,249.25,8.59;15,86.94,554.56,106.01,8.59">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,573.79,468.00,8.64;15,86.94,585.57,453.06,8.82;15,86.94,597.52,400.80,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,308.76,573.79,231.24,8.64;15,86.94,585.75,28.32,8.64">Answering definition questions with multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,135.14,585.57,404.86,8.59;15,86.94,597.52,341.45,8.59">Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004)</title>
		<meeting>the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,616.75,468.00,8.64;15,86.94,628.53,453.06,8.82;15,86.94,640.48,329.42,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,387.74,616.75,152.26,8.64;15,86.94,628.71,333.59,8.64">Handling information access dialogue through QA technologies-A novel challenge for open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Tsuneaki</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun'ichi</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fumito</forename><surname>Masui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,441.20,628.53,98.80,8.59;15,86.94,640.48,247.25,8.59">Proceedings of the HLT-NAACL 2004 Workshop on Pragmatics of Question Answering</title>
		<meeting>the HLT-NAACL 2004 Workshop on Pragmatics of Question Answering</meeting>
		<imprint>
			<date type="published" when="2004-05">2004. May</date>
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,659.54,468.00,8.82;15,86.94,671.49,453.06,8.59;15,86.94,683.45,260.48,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,258.93,659.72,170.78,8.64">Will pyramids built of nuggets topple over?</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,444.61,659.54,95.39,8.59;15,86.94,671.49,453.06,8.59;15,86.94,683.45,191.17,8.59">Proceedings of the 2006 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2006)</title>
		<meeting>the 2006 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,75.48,468.00,8.64;16,86.94,87.25,453.05,8.82;16,86.94,99.21,445.91,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,152.38,75.48,387.62,8.64;16,86.94,87.43,58.55,8.64">Is question answering better than information retrieval? A task-based evaluation framework for question series</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,166.50,87.25,373.50,8.59;16,86.94,99.21,376.60,8.59">Proceedings of the 2007 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2007)</title>
		<meeting>the 2007 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,119.13,468.00,8.82;16,86.94,131.09,223.39,8.59" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,247.93,119.31,214.62,8.64">Overview of the TREC 2005 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,480.54,119.13,59.46,8.59;16,86.94,131.09,165.00,8.59">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2006. TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,151.01,468.00,8.82;16,86.94,162.97,199.54,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,179.60,151.19,216.62,8.64">Overview of the TREC 2003 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,414.96,151.01,125.04,8.59;16,86.94,162.97,140.19,8.59">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="54" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,182.89,468.00,8.82;16,86.94,194.85,218.77,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,184.73,183.07,217.21,8.64">Overview of the TREC 2004 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,421.03,182.89,118.97,8.59;16,86.94,194.85,159.42,8.59">Proceedings of the Thirteenth Text REtreival Conference (TREC 2004)</title>
		<meeting>the Thirteenth Text REtreival Conference (TREC 2004)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,214.77,468.00,8.82;16,86.94,226.73,453.06,8.59;16,86.94,238.69,196.38,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,182.36,214.95,290.35,8.64">Using question series to evaluate question answering system effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,490.65,214.77,49.35,8.59;16,86.94,226.73,453.06,8.59;16,86.94,238.69,127.07,8.59">Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005)</title>
		<meeting>the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
