<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,62.34,72.36,485.03,16.84;1,271.79,92.29,66.14,16.84">Correlating Topic Rankings and Person Rankings to Find Experts</title>
				<funder ref="#_EqygrrF">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="2,227.14,107.93,12.89,6.30;2,227.14,121.19,12.89,6.30"><forename type="first">Lori</forename><surname>Lori</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Thijs Westerveld CWI</orgName>
								<address>
									<postBox>PO Box 94079</postBox>
									<postCode>INS1, NL-1090 GB</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,227.14,141.50,12.89,6.30;2,227.14,154.76,16.28,6.30"><forename type="first">Lori</forename><surname>Ellen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Thijs Westerveld CWI</orgName>
								<address>
									<postBox>PO Box 94079</postBox>
									<postCode>INS1, NL-1090 GB</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,227.14,176.62,9.85,6.30;2,227.14,189.88,12.89,6.30;2,228.72,214.87,12.89,6.30"><forename type="first">Ian</forename><forename type="middle">Lori</forename><surname>Lori</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Thijs Westerveld CWI</orgName>
								<address>
									<postBox>PO Box 94079</postBox>
									<postCode>INS1, NL-1090 GB</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,227.14,235.18,16.28,6.30;2,227.14,248.44,12.89,6.30"><forename type="first">Ellen</forename><surname>Lori</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Thijs Westerveld CWI</orgName>
								<address>
									<postBox>PO Box 94079</postBox>
									<postCode>INS1, NL-1090 GB</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Thijs Westerveld CWI</orgName>
								<address>
									<postBox>PO Box 94079</postBox>
									<postCode>INS1, NL-1090 GB</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,377.75,243.61,9.85,6.30"><surname>Ian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Thijs Westerveld CWI</orgName>
								<address>
									<postBox>PO Box 94079</postBox>
									<postCode>INS1, NL-1090 GB</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,62.34,72.36,485.03,16.84;1,271.79,92.29,66.14,16.84">Correlating Topic Rankings and Person Rankings to Find Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CB4AD4E3B49DCEA3298F7AAB3F19315F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Expert search is about finding people rather than documents. The goal is to retrieve a ranked list of candidates with expertise on a given topic. The task is studied in the context of the enterprise track.</p><p>We describe an approach that compares topic profiles and candidate profiles directly. These profiles are not based on unordered sets of documents, but on ranked lists. This allows us to differentiate between documents that are highly related to a topic or a candidate and documents that are only marginally related. The ranked lists for topics and candidates are obtained by simple retrieval queries. The correlation between the ranked list of documents for a topic and the ranked list for a candidate is used as an indicator of the candidate's expertise on the topic. We study different ways to rank documents for the candidate profiles as well as various ways of comparing the document and candidate based ranked lists. Experiments show that starting from the right candidate profiles, reasonable results can be obtained. Furthermore, it seems important to take a correlation measure that takes into account the orderings of documents in both the candidate profile and the documents profile.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>TREC's enterprise track focuses on information retrieval problems in an intranet setting. The track tackles the problems of dealing with a heterogeneous collection containing web pages, email archives, meeting reports, technical documents, memos and more. The tasks studied are the following Discussion search Retrieve arguments, pro's and con's around a given topic, e.g., blocking pop-ups or Is XHTML more accessible than HTML Expert search Retrieve experts on a given topic, e.g., RDF Data Access, SVG, or privacy on the web.</p><p>We only participated in the expert search task, a task that is common in enterprises and other organisations. The goal of this task is to retrieve a ranked list of at most 100 experts for each topic, and to provide up to 20 documents with each expert that support their expertise. The experts come from a predefined set of candidates, each candidate has an identifier plus a name and email address. Topics for the 2006 expert search task are created by the participating groups and have the typical TREC format with title, description and narrative.</p><p>The approach we take is based on comparing topic and candidate profiles. We produce a ranked list for each topic and one for each candidate expert and correlate them. The lists with highest correlations indicate experts on topics.</p><p>The remainder of this paper is organised as follows. Section 2 discusses other approaches to the same task. Section 3 explains how correlations between document rankings can help to identify experts. Experimental results are discussed in Section 4. The paper ends with a summary of the main conclusions and a discussion of possible points for improvement and future research in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Two typical approaches to expert ranking exist, one based on document ranking, the other on candidate profiles. The document ranking approach <ref type="bibr" coords="1,434.93,386.90,9.72,7.86" target="#b8">[8,</ref><ref type="bibr" coords="1,448.71,386.90,7.16,7.86" target="#b1">1]</ref> starts with a document ranking for the topic on which we want to find experts. The top ranked documents are then analysed to locate occurrences of candidates, for example by looking for their names and email addresses. Candidates are subsequently ordered by decreasing number of occurrences. Figure <ref type="figure" coords="1,500.95,439.21,4.10,7.86" target="#fig_0">1</ref>-a illustrates this scheme.</p><p>The candidate profile approach <ref type="bibr" coords="1,449.42,470.59,9.73,7.86" target="#b7">[7,</ref><ref type="bibr" coords="1,463.40,470.59,7.16,7.86">6]</ref> starts by creating a profile for each candidate, for example by gathering all documents authored by the candidate, or all documents mentioning the candidate. These profiles are then treated as (large) documents and ranked according to the topic of interest. The highest ranked profiles are assumed to correspond to the experts on the topic. Figure <ref type="figure" coords="1,499.32,533.35,4.27,7.86" target="#fig_0">1</ref>-b illustrates this strategy.</p><p>The approach discussed here can be viewed as a mix of the document ranking and candidate profiling approaches. While we start from a document ranking, we do not directly identify candidates in the retrieved documents. Instead, we investigate whether the retrieved documents are included in the candidate's profile. In our approach, candidate profiles are rankings rather than sets. That means documents can be included in a candidate's profile to different degrees. In a sense the relation between documents and candidates is weighted. Others have studied this weighting on the side of topical ranking: candidates matching documents retrieved at a higher rank are more likely to be experts than candidates matching documents further down the topical ranking, see for example <ref type="bibr" coords="1,383.58,700.73,9.21,7.86" target="#b5">[5]</ref>. The use of a similar weighting on the candidate side of the coin is something we have not seen</p><p>The document ranking based approach: Rank documents based on topic (Copyright forms), and locate candidates in the top ranked documents. The number of candidate occurrences is assumed to be an indicator of the candidates expertise.</p><p>The candidate profile based approach: Construct a profile for each candidate expert, and rank the profiles as if they were documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CORRELATING RANKINGS</head><p>Our approach to expert search is based on correlating candidate profiles and topic profiles. Each of these profiles is a simple document ranking. The correlation between the rankings is an indication of the candidate's topical expertise. The intuition behind this approach is the following. If person X is an expert on topic Y, the documents retrieved when searching for person X can be expected to overlap with the documents retrieved when searching for topic Y. Of course, not all documents on topic Y will mention person X, and not all documents that mention the person will be on topic, but a reasonable overlap can be expected, at least an overlap bigger than for non experts on Y. We use this intuition and compare the topic based ranking and the candidate based ranking directly.</p><p>In our setting, a topical profile is the document ranking obtained from a query on a language modelling based IR system using only the topic's title field. For candidate profiles, we experimented with different variants as explained in the following sub-section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Profiles</head><p>The candidate profiles we use, are ranked lists of documents. We investigate various ways of constructing these. Our base-line is to rank the documents using simple queries based on the names and email addresses of the candidates. No special processing of names or email addresses in collection or queries is used. This means these are tokenised as any other string. This makes this approach a relatively poor baseline since persons sharing a first or last name, or a domain name in their email address are easily confused.</p><p>In a second document ranking for the candidates, we tackle these problems by using the W3C corpus with candidate annotation as created by Jianhan Zhu at the Open University. This corpus is a copy of the original W3C collection with the occurrences of the candidates annotated with special tags. A small sample is shown in Figure <ref type="figure" coords="2,466.21,562.29,3.58,7.86">2</ref>. We transform this collection to a collection of only candidates. In this candidate collection, all text from the documents is removed and the candidate tags are replaced by candidate terms (see Figure <ref type="figure" coords="2,333.05,604.13,3.58,7.86" target="#fig_1">3</ref>). Candidate queries against this candidate collection can easily retrieve the most relevant documents for a given candidate based on term frequency and inverse document frequency of the candidate term.</p><p>The two ways of constructing candidate profiles are topic independent. Since a person may be an expert on more than a single topic, this may be problematic. Therefore, we try to focus the candidate profiles by making them topic specific. This third candidate ranking is based on queries consisting of the original title queries as provided by the  track organisers, augmented with a single special candidate term. For example, for topic EX67 and candidate-0001, the query would be privacy on the web candidate0001. This query is issued against a modified version of the tagged W3C corpus, where candidate tags are replaced with candidate terms and the rest of the text is left untouched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparing Rankings</head><p>To compare topic profiles and candidate profiles, or rather the two corresponding document rankings, we use two basic approaches, Spearman's rank correlation, and a blind feedback like approach that assumes the top N for one of the rankings (topic based or candidate based) are relevant and measures average precision on the other ranking. The techniques are detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spearman Rank Correlation</head><p>Spearman's rank correlation, or Spearman's ρ is a non-parametric test that measures the strength of the correlation between two variables:</p><formula xml:id="formula_0" coords="3,122.00,606.68,166.98,21.51">ρ = 1 -6 X D 2 N (N 2 -1) , (<label>1</label></formula><formula xml:id="formula_1" coords="3,288.98,614.24,3.93,7.86">)</formula><p>where D is the difference in rank of corresponding variables, and N is the number of paired values. Our variables are the topic based score and candidate based score for a given document. To limit computational costs, we compute Spearman's ρ based only on the top K returned documents for topic queries. Documents that do not occur in the candidate rankings are assumed to be at rank K + 1. Note that this is a best case scenario assumption, thus we over-estimate ρ.</p><p>Spearman's ρ measures the correlation for the complete ranked list. In our setting, we would like to emphasise correlations at the top of the ranked list and down-weight the correlations further down. To this end, we use a log based variant of Spearman's rho. We first perform a log transform on the ranks before computing the correlation. Note that while a log transformation of the scores would not influence the ranking, such a transformation on the ranks does influence the correlation. We refer to the traditional Spearman correlation as SP and to the log-based variant as log-SP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Blind Feedback AP</head><p>Another approach to compare the rankings is similar to blind feedback. Here, we assume the top K documents for the topic based ranking are relevant, and we compute the average precision (AP) of the candidate ranking based on that. This will reward candidate queries that have retrieved many of the documents in the topic ranking, especially if these are retrieved at the top ranks.</p><p>We also tested the reverse approach: assuming the top K documents of the candidate based rankings are relevant and computing AP on the topic based ranking. In this setting, we directly compare absolute performance of a single topic ranking on different tasks. The tasks are to retrieve documents relating to candidate-0001, relating to candidate-0002, etc. In a normal retrieval setting, comparing absolute numbers across tasks does not make sense. Here we keep the system fixed (the topic run), and effectively try to find the easy topics for this system (what are the easy candidates for this topic?). This is less elegant than the reverse approach, but it may still help to locate the experts.</p><p>In the following we will refer to these AP based variants as AP-Trel and AP-Crel, for the runs with Topic rankings assumed relevant and Candidate rankings assumed relevant respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>For our experiments, we use the open source pf/tijah system <ref type="bibr" coords="3,316.81,648.42,9.21,7.86" target="#b4">[4]</ref>, a flexible blend of an XQuery engine and an XML IR retrieval systems that supports many information retrieval models. To be able to use this system with the W3C data, we cleaned the collection to make sure all documents are valid XML. While the pf/tijah system supports retrieval at the element level (e.g., retrieve paragraphs or sections), we only use full document queries. Documents and queries are stemmed and stopped and language models are used to perform the ranking of documents. For the topic specific ranking pf/tijah was not able to perform the 60,000+ queries (55 topics times 1092 candidates) in the available time. For this variant, we had to use the X100 based retrieval system as used by our group in the terabyte track <ref type="bibr" coords="4,211.39,109.94,10.93,7.86" target="#b3">[3]</ref>.</p><p>While the official track results include measures based on retrieved experts and measures based on experts retrieved with supporting documents, we ignore the supporting documents and only report on the ability of the various approaches to identify the experts. The metric used is mean average precision (MAP) over the ranked list of experts. We use K = 1500, that means the topical profiles consist of the top 1500 documents retrieved for the topic. These 1500 documents are the basis for computing the Spearman correlation measures and they are regarded relevant in the AP-Trel and AP-Crel approaches. The influence of K on the results is studied below. Table <ref type="table" coords="4,141.15,245.93,4.61,7.86" target="#tab_1">1</ref> shows the results. <ref type="foot" coords="4,219.97,244.16,3.65,5.24" target="#foot_0">1</ref> .</p><p>One notable result is the low score across the board for the topic specific candidate rankings. Intuitively, these rankings are expected to be better, more focused, and more likely to yield appropriate correlations. The low scores in practise can be explained by the fact that the specific candidate terms that were added to the topic query often have little influence on the ranking. One reason for this is that it is just a single extra term on a title query of on average 2.9 terms. More important though, is that some candidates appear only in very few documents, these few may end up on the top of the ranked list for title+query, but the rest of the list remains unchanged. The effect is that the title+query rankings are often very similar to the title only rankings in particular in the cases where the candidate term matches few or no documents. This means the correlation between the two rankings is higher for candidates that hardly appear in the collection. Clearly, these are not necessarily the experts.</p><p>Also, between the other two candidate rankings, there is a remarkable difference. The annotated candidate run clearly outperforms the name and email variant. This was expected as the annotated variant does not suffer of confusion between people with similar names or email addresses.</p><p>Finally, the log-SP runs give lower scores than the other correlation variants. A reason could be that the top ranks are emphasised too much now. Further analysis is needed to confirm this.</p><p>In comparison to the other expert search submissions, these numbers are relatively low. One reason for this is that we did not use any collection specific knowledge. Many runs that performed well treated emails different than other documents and included special treatment or weighting for specific fields like an email's sender, receiver or subject. Also, many successful approaches ranked and analysed information at the sub-document level, creating candidate profiles based on smaller windows around candidate occurrences rather than on complete documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Varying the size of the profiles</head><p>The number of documents in the topic and candidate profiles is likely to have an impact on the results. The top ranked documents are probably better indicators of expertise than the ones further down the ranked list. Our comparison of rankings takes this into account to some extend, and we tried to emphasise this by using a log-transform on the ranks. Still, it may be useful to limit the size of the profiles and look only at the top documents. Figure <ref type="figure" coords="4,473.37,376.44,4.61,7.86" target="#fig_2">4</ref> shows the influence of K on mean average precision values for the SP, AP-Trel and AP-Crel correlations.</p><p>First of all we notice that while the performance for the different correlation measures on the top 1500 (as reported in Table <ref type="table" coords="4,353.45,439.21,4.09,7.86" target="#tab_1">1</ref>) are the same, large differences exist for smaller values of K. The AP-Crel line shows that taking fewer than 1000 documents as a candidate's profile harms the MAP (Note that for the two other correlation measures K relates to the size of the topic profile; the size of the candidate profile for these is fixed at 1500). The size of the topic profile can be significantly smaller; the optimum seems to be around 100 or 200 documents. It seems the experts need to have only the top ranked topical documents in their profile, but it could also be the case that for many topics fewer than 1500 documents are relevant and that the profile for these includes random documents to reach a profile size of 1500 documents. Furhter investigation is needed to check this.</p><p>The emphasis on the top ranked documents in the profiles that we wanted to attain with the log-SP run, seems indeed important, but it is not as symmetric as the (failing) log-SP run treated it: for the candidate profiles we need many documents, for the topical profiles we need to focus on the top ranks. Finally, Spearman (SP) based comparison of the profiles gives much higher scores than the average precision (AP) based runs. This indicates that it is important to not only look at the set of documents related to a candidate or a topic, but that the ordering of these documents plays an important role. A candidate is an expert on a topic, if the top ranked documents in the topical profile have a similar ordering in the candidates profile. The experiments for the expert search task showed that the approach is feasible given that the initial rankings are good enough. The baseline, name and email address based candidate ranking is clearly not good enough, but a candidate ranking based on the tagged candidates gives good results when used in our correlation method<ref type="foot" coords="5,205.61,324.36,3.65,5.24" target="#foot_1">2</ref> . The exact correlation method is of minor importance in such a setting, but log-SP is a particular bad choice.</p><p>Topic specific candidate rankings harm results significantly.</p><p>The main reason for this is the relatively low number of appearances in the collection for many candidates. More research is needed to investigate ways to compensate for this. A possible solution would be to compare the title + candidate based ranking not only to the topic based ranking, but also to the candidate based ranking, or to measure the overlap between all sub-queries cf. Carmel et al <ref type="bibr" coords="5,249.05,441.20,9.21,7.86" target="#b2">[2]</ref>.</p><p>In comparison with other expert search approaches the performance of the approach described here is relatively poor. One reason for this is we do not use any collection specific information, another is we treat documents as a single entity. The same techniques could easily be adapted to work on windows, XML elements or passages. Also in creating the rankings for the topic and candidate profiles, collection specific creation could be incorporated. Correlating rankings on top of the successful expert search approaches is an interesting direction for future research.</p><p>As a collection independent approach, the correlation based profile comparison gives good results, provided the parameters are chosen correctly. Spearman based correlation is than better than the average precision based measures, indicating that it is important to take the ordering of documents in both the topical profile and the candidate profile into account.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,185.93,396.63,237.86,7.89"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two typical approaches to expert search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,53.80,336.53,239.10,7.89;3,53.80,346.99,94.92,7.89"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A sample document from the W3C based candidate collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,316.81,235.94,239.10,7.89;4,316.81,246.40,239.11,7.89;4,316.81,256.86,86.76,7.89"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The influence of K on mean average precision (MAP) for different correlations (candidate annotation profile)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.80,64.83,502.11,195.93"><head>Table 1 :</head><label>1</label><figDesc>Expert search results (MAP) for various base candidate rankings (rows) and various correlation measures (columns).</figDesc><table coords="5,53.80,95.05,375.31,165.72"><row><cell></cell><cell></cell><cell cols="2">Correlation Type</cell></row><row><cell>Candidate profile</cell><cell>SP</cell><cell cols="3">log-SP AP-Trel AP-Crel</cell></row><row><cell>Name + email</cell><cell cols="2">0.1154 0.0841</cell><cell>0.1336</cell><cell>0.1375</cell></row><row><cell cols="3">Candidate annotation 0.2193 0.1577</cell><cell>0.2190</cell><cell>0.2135</cell></row><row><cell>Topic Specific</cell><cell cols="2">0.0017 0.0017</cell><cell>0.0016</cell><cell>0.0016</cell></row><row><cell>5. CONCLUSIONS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Directly comparing topic profiles and candidate profiles based</cell><cell></cell><cell></cell></row><row><cell cols="2">on the correlation between document rankings is a novel</cell><cell></cell><cell></cell></row><row><cell cols="2">and interesting approach. It is different from traditional ap-</cell><cell></cell><cell></cell></row><row><cell cols="2">proaches that either construct a descriptive document for</cell><cell></cell><cell></cell></row><row><cell cols="2">each candidate and rank these, or rank documents based on</cell><cell></cell><cell></cell></row><row><cell cols="2">topics only and then extract candidates from the top ranked</cell><cell></cell><cell></cell></row><row><cell cols="2">documents. The correlation based approach directly works</cell><cell></cell><cell></cell></row><row><cell cols="2">on the ranked lists and uses information of the entire lists.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,58.41,684.29,234.50,7.86;4,53.80,693.25,239.10,7.86;4,53.80,702.22,239.11,7.86;4,53.80,711.19,74.08,7.86"><p>The official (lower) SP and log-SP results are based on Spearman ρ's for varying set sizes N , and hence not directly comparable; here we report the numbers for the runs after fixing this mistake</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,58.41,702.22,234.50,7.86;5,53.80,711.19,192.44,7.86"><p>At this moment it is unclear how this compares to other approaches on the TREC 2006 enterprise track.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The 15th <rs type="projectName">Text Retrieval Conference</rs> (<rs type="grantNumber">TREC-2006</rs>) Notebook, 2006.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_EqygrrF">
					<idno type="grant-number">TREC-2006</idno>
					<orgName type="project" subtype="full">Text Retrieval Conference</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,58.28,664.08,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,67.22,675.86,219.18,6.99;5,67.22,684.82,225.68,6.99" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Research on expert</note>
</biblStruct>

<biblStruct coords="5,330.23,191.32,212.74,6.99;5,330.23,200.28,214.74,6.99;5,330.23,209.25,199.59,6.99;5,330.23,218.22,214.67,6.99;5,330.23,227.18,54.10,6.99" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,522.04,191.32,20.93,6.99;5,330.23,200.28,88.27,6.99">What makes a query difficult?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,432.84,200.28,112.13,6.99;5,330.23,209.25,199.59,6.99;5,330.23,218.22,188.28,6.99">SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="390" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.23,240.14,201.76,6.99;5,330.23,249.10,220.46,6.99;5,330.23,258.07,212.40,6.99" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,330.23,249.10,170.08,6.99">Monetdb/x100 at the 2006 trec terabyte track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Héman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,517.25,249.10,33.45,6.99;5,330.23,258.07,186.33,6.99">The 15th Text Retrieval Conference (TREC-2006) Notebook</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.23,271.02,218.40,6.99;5,330.23,279.99,224.94,6.99;5,330.23,288.95,215.37,6.99;5,330.23,297.92,138.09,6.99" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,520.78,271.02,27.86,6.99;5,330.23,279.99,140.68,6.99">Pftijah: text search in an xml database system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rode</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Van Os</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Flokstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,488.26,279.99,66.91,6.99;5,330.23,288.95,215.37,6.99;5,330.23,297.92,62.21,6.99">Proceedings of the 2nd International Workshop on Open Source Information Retrieval (OSIR)</title>
		<meeting>the 2nd International Workshop on Open Source Information Retrieval (OSIR)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.23,310.87,224.25,6.99;5,330.23,319.84,216.97,6.99;5,330.23,328.80,219.74,6.99;5,330.23,337.77,224.13,6.99;5,330.23,346.74,149.06,6.99" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,436.59,310.87,117.89,6.99;5,330.23,319.84,176.39,6.99">Voting for candidates: adapting data fusion techniques for an expert search task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,523.56,319.84,23.64,6.99;5,330.23,328.80,219.74,6.99;5,330.23,337.77,161.73,6.99">CIKM &apos;06: Proceedings of the 15th ACM international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.23,359.69,195.98,6.99;5,330.23,368.65,208.72,6.99;5,330.23,377.62,112.58,6.99" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,458.11,359.69,68.10,6.99;5,330.23,368.65,58.52,6.99">Bupt at trec 2006: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,405.68,368.65,133.27,6.99;5,330.23,377.62,86.52,6.99">The 15th Text Retrieval Conference (TREC-2006) Notebook</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.23,390.57,210.79,6.99;5,330.23,399.54,187.49,6.99;5,330.23,408.50,156.85,6.99" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,459.68,390.57,81.33,6.99;5,330.23,399.54,81.57,6.99">Ricoh research at trec 2006: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,428.73,399.54,88.99,6.99;5,330.23,408.50,130.79,6.99">The 15th Text Retrieval Conference (TREC-2006) Notebook</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,330.23,421.46,222.07,6.99;5,330.23,430.42,190.05,6.99;5,330.23,439.39,112.58,6.99" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,359.28,421.46,193.03,6.99;5,330.23,430.42,39.84,6.99">Open university at trec 2006 enterprise track expert search task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,387.02,430.42,133.27,6.99;5,330.23,439.39,86.52,6.99">The 15th Text Retrieval Conference (TREC-2006) Notebook</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
