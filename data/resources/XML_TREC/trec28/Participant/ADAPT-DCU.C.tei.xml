<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,166.45,115.96,282.45,12.62;1,248.51,133.89,118.34,12.62">DCU at the TREC 2019 Conversational Assistance Track</title>
				<funder ref="#_mbqVWsY">
					<orgName type="full">ADAPT Centre at Dublin City University</orgName>
				</funder>
				<funder>
					<orgName type="full">Science Foundation Ireland</orgName>
					<orgName type="abbreviated">SFI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.51,171.61,56.46,8.74"><forename type="first">Piyush</forename><surname>Arora</surname></persName>
							<email>piyush.arora@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.97,171.61,79.53,8.74"><forename type="first">Abhishek</forename><surname>Kaushik</surname></persName>
							<email>abhishek.kaushik@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.19,171.61,81.65,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gareth.jones@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,166.45,115.96,282.45,12.62;1,248.51,133.89,118.34,12.62">DCU at the TREC 2019 Conversational Assistance Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AE8828F9A981DB5BE6048B4126E7E181</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conversational Assistance</term>
					<term>Question Answering</term>
					<term>Understanding Dialogue</term>
					<term>Query Formulation</term>
					<term>Sentence Selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the DCU-ADAPT team's participation in the TREC 2019 Conversational Assistance Track (CAsT) track. The CAsT track focuses on two main aspects: i) system understanding of information needs in a conversational format, and ii) finding relevant responses using contextual information. In our participation in the CAST track, we focused on the second aspect of finding relevant information using contextual information from the queries for a conversational search system. We carried out two main investigations: i) Query formulation using syntactic analysis, and ii) Data Fusion of results for re-ranking top candidates retrieved from three different data sources used in the CAsT track. We find that using only query formulation and data fusions techniques attains average results in comparison to other submissions, which are not sufficient to answer questions in a conversational setting reliably.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC CAsT track is a conversational search benchmark focusing on understanding the evolution of user information need in a conversational setting comprising of several turns in a search dialogue. The goal of this track is to identify and retrieve salient information needed for the current turn in the conversation. The overall objective of the CAsT benchmark being to improve understanding and research on conversational information seeking (CIS).</p><p>Interest in the development of conversational methods is motivated by analysis of the use of current search systems. These existing systems can be referred to as "single shot", since the user is required to enter a single query with the intention of retrieving documents sufficient to satisfy their information need in a single search operation. This can be observed to pose a number of challenges <ref type="bibr" coords="1,134.77,609.90,10.52,8.74" target="#b3">[4]</ref> including:</p><p>-The user must completely describe their information need in a single query.</p><p>-The user may not be able to adequately describe their information need.</p><p>-High cognitive load on the user in forming their query.</p><p>-A poorly specified query can make it difficult for the search engine to return relevant content.</p><p>While current search systems are able to perform information seeking and retrieval effectively with suitably defined queries, their ability to support CIS is very limited. The goal of the CAsT track is to promote and develop research activities in CIS and to create large-scale reusable test collections to enable empirical research in CIS. The main objectives of the CAsT track are: i) system understanding of information needs in a conversational format, and ii) finding relevant responses using contextual information. The CAsT track is motivated by complex search tasks requiring multiple turns (possibly across multiple sessions), e.g. as explored in the TREC Session tracks <ref type="bibr" coords="2,329.87,249.08,9.96,8.74" target="#b0">[1]</ref>.</p><p>TASK Definition: In the CAsT track, participants are given a set of topics and their descriptions, and have to return potentially relevant responses for the given subsequent questions for each topic based on multiple turns as in a dialogue, as shown in Table <ref type="table" coords="2,202.87,303.31,3.87,8.74" target="#tab_0">1</ref>. A response for each of the questions in the conversation needs to be generated by performing retrieval over a large collection of paragraphs to identify relevant information. The main challenges of this track lie in understanding the query context and extracting salient information (e.g. named entities) from the dialogue. We examine some examples which illustrate these challenges of this track:</p><p>-Query Formulation: Building effective queries for conversational search systems is a complex task. Some of the key challenges associated with this are described below using examples presented in Table <ref type="table" coords="2,393.32,629.50,7.75,8.74" target="#tab_0">1:</ref> 1. Pronoun Resolution: Are angora goats good for it?, identifying it refers to meat (Question-4).</p><p>2. Named Entity Identification: What are pygmies used for?, identifying that 'pygmies' refers to 'goats' as a specific entity (Question-6). 3. Query terms extraction: How long do Angora goats live?, identifying important aspects and terms to formulate effective queries (Question-8). Just considering three terms, 'angora', 'goats', 'live', there can be different query representations: 'Angora', 'goats', 'live' -each word as a separate query 'Angora goats','live' -the named entity as a single unit "angora goats" 'Angora goats live' -all words as a single phrase -Data Fusion: There are three different data collections used in this track (described in detail in Section 2). How to effectively combine results from these three different collections is a major challenge in the track which we investigate in our work.</p><p>We attempt to address the challenges described above to retrieve potential relevant responses for questions in a conversational search system. The two main questions that we investigate in this work are:</p><p>-Query formulation: How to effectively form queries from conversational topics spanning over multiple questions as turns and different dialogue utterances? -Data Fusion: How to combine and re-rank documents from multiple data collections for the CAsT track?</p><p>The remainder of this paper is organised as follows: Section 2 introduces the dataset, tools used and the evaluation strategy of the CAsT track, Section 3 describes the approach adopted in our participation in this track, Section 4 gives results and analysis of our submissions to the track, and finally Section 5 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset &amp; Resources</head><p>In this section we outline the dataset, tools, resources used and the evaluation criteria adopted in this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>Three data collections are used in the CAsT track as follows:</p><p>-MS MARCO: Microsoft Machine Reading Comprehension (MS MARCO) <ref type="foot" coords="3,476.12,601.31,3.97,6.12" target="#foot_0">1</ref>is a large scale dataset focused on machine reading comprehension, question answering, passage ranking, keyphrase extraction, and conversational search studies.</p><p>-TREC CAR paragraph collection: The TREC CAR paragraph collection is a corpus<ref type="foot" coords="4,220.10,129.37,3.97,6.12" target="#foot_1">2</ref> of 20 million paragraphs. These are harvested from paragraphs on Wikipedia pages from a snapshot gathered in 2016 (with hyperlinks preserved). Given the large amount of duplication on Wikipedia pages, the collection was de-duplicated before the data release. These de-duplicated paragraphs are then available for passage ranking tasks. -TREC Washington Post Corpus (WAPO): The TREC Washington Post Corpus<ref type="foot" coords="4,206.21,200.94,3.97,6.12" target="#foot_2">3</ref> contains 608,180 news articles and blog posts from January 2012 through to August 2017. The articles are stored in JSON format, and include title, byline, date of publication, kicker (a section header), article text broken into paragraphs and links to embedded images and multimedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topics</head><p>Conversation evolves through turns (similar to dialogues) for a given topic. We were provided with training and validation topics with each topic consisting of about 10 questions (turns).</p><p>-Training queries: The training queries were divided into topic and turns. In total, there were 30 topics with each topic having around 7-11 turns. Each turn was associated with the previous turns as shown in Table <ref type="table" coords="4,420.14,355.66,3.87,8.74" target="#tab_2">2</ref>. The queries have two sets of turns: one set associated with turns using the pronoun, while the second set was expanded (as shown in Table <ref type="table" coords="4,363.62,379.57,3.87,8.74" target="#tab_2">2</ref>). The provided expanded queries were formed from the initial queries by the organisers by performing pronoun resolution and resolving ambiguous entity mentions. -Validation (test) queries: The validation queries were also divided into topics and turns. In total, there were 50 topics with each topic having 7-11 turns. Similar to the training queries, we were provided with the raw query as well as the expanded query with the initial query modified after performing pronoun resolution and resolving entity mentions. We used the expanded queries for our investigations, since we did not have access to a suitable mechanism for resolution of pronouns and ambiguities available to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tools and Data Processing</head><p>We used the spacy library <ref type="foot" coords="4,249.79,555.04,3.97,6.12" target="#foot_3">4</ref> to perform query extraction and syntactic analysis of the queries (described later in Section 3). We used the whoosh library<ref type="foot" coords="4,462.98,567.00,3.97,6.12" target="#foot_4">5</ref> to perform data indexing and passage retrieval over the indexed collection for the given input queries (described later in Section 3). We used the probabilistic Best Match (BM25 ranking model <ref type="bibr" coords="4,265.00,604.44,10.79,8.74" target="#b6">[7]</ref>) for our work.  The Macro and Wapo datasets contain duplicate paragraphs. We followed the standard procedure provided by the TREC organizers to remove the duplication of records <ref type="foot" coords="5,178.29,307.88,3.97,6.12" target="#foot_5">6</ref> . We remove special characters, stop words and numbers from the data and the input query. We perform stemming over the data collection and input query using Porter stemming <ref type="bibr" coords="5,291.46,333.37,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation strategies</head><p>The CAsT track is similar to the task of passage retrieval and sentence selection, however the context of the query changes depending on the previous questions and the answers for these previous questions, as in a conversational setting. The top k passages retrieved for a given question are evaluated using two main evaluation mechanisms: N DCG@k [3] and M AP @k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section we describe our system pipeline and the models explored in our work. The three main components of our pipeline are shown in Figure <ref type="figure" coords="5,431.59,503.59,3.87,8.74" target="#fig_0">1</ref>: indexing, syntactic analysis and data fusion, these components are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing</head><p>We indexed the dataset using the python whoosh API<ref type="foot" coords="5,366.07,566.20,3.97,6.12" target="#foot_6">7</ref> . As outlined above, there were three datasets to be indexed with the schema (Docid and content). Due to the data collection size being quite large, we maintained a separate indexed object for each of the three datasets: MSMARCO, CAR and WAPO. Having three different indexes enables us to maintain data collection specific term and document statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntactic Analysis</head><p>As described above in Section 2, we use expanded queries provided by the track organisers in our work. We performed rich syntactic analysis of the queries. Each query was parsed and lexicon objects extracted. A combination of these lexicon objects was used to frame effective queries. The following four lexicon objects were extracted using the spacy library: Noun, Noun Phrase, Verb Phrase and Adjective. Tables <ref type="table" coords="6,212.16,415.62,4.98,8.74">3</ref> and<ref type="table" coords="6,239.82,415.62,4.98,8.74" target="#tab_3">4</ref> show input queries and corresponding lexicon objects extracted using the spacy library.</p><p>After our initial investigations with the training topics, we decided to use the eight query making models (M1-M8) listed below. These models were built using a combination of the lexicon objects. We used the "AND" boolean operator to combine the lexicon objects for a model. Based on the specificity of the query from specific to general, these eight query models were ranked from M1 to M8 based on our investigation with the training queries. While searching for a query q, we first searched the output of M 1(q); if there were no results, we then searched for M 2(q), and subsequently other model outputs M [1 -8] as necessary.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Fusion</head><p>We submitted four runs for the CAsT track. We used query formulation as described in Section 3.2 while performing search for potentially relevant passages. As indicated in Section 3.1, we used three separate indexes in our work. Thus for each query we retrieved three sets of ranked passages. Next, we focused on investigating different mechanisms for combining these three ranked result lists for each query. We investigated different data fusion techniques which formed our four system submissions for this track. Details of our four system submissions are as follows:</p><p>Model-1 (combination): This is the baseline run, where we performed NLP based query extraction using the spacy library to perform passage retrieval. As stated earlier, we used the officially provided expanded queries. We searched for each query using the three datasets separately using a BM25 retrieval model, and merged the retrieval results obtained for the three datasets. For merging, we performed score normalization for each ranked list and combined the ranked lists, sorting the passages in the combined list in decreasing order of relevance scores for the given topic. In cases where the scores between passages from different collections were the same, we outputted the passages in the following order of the document collection: "marco","wapo", and "car" (the collection order was chosen based on the proportion of relevant documents in the training set). Thus all the documents from a single collection such as "marco" were placed first, then all the documents from "wapo" in cases where passages from "marco" and "wapo" had the same score.</p><p>Model-2 (datasetreorder): Similar to model-1, we performed score normalization for each ranked list and sorted the passages in a merged list in decreasing order of relevance scores for the given topic. In situations where the scores between passages from different collections were the same, we outputted the passages one at a time in the following order: "marco","wapo", and "car". Thus in this model we combined output results in a sequential manner to have mixed ranked results combined from different data collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-3 (rerankingorder):</head><p>This model is similar to model-2, however we changed the document collection order to "car","wapo","marco", when combining documents from the different data collections with identical scores.</p><p>Model-4 (topicturnsort): In all of the above three models, we present the order of the document collections when combining ranked results retrieved from different collections. However, this may not be the optimum strategy. Thus in this model we performed re-ranking using the percentage of potentially relevant documents returned. For each query we searched across all the document collections, the collection which retrieved the most results was ranked first while combining results across the three collections in cases where the scores of passages are identical. This model is different from the other three models as it is based on dynamic file ordering depending on descending order of retrieved documents.</p><p>A worked example of the four alternative data fusion strategies investigated in this work is shown in Figure <ref type="figure" coords="8,273.50,450.84,3.87,8.74" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>In this section, first we discuss the nature of the qrels and then describe the results of our submissions to the CAsT track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">QRELs/Gold judgments</head><p>For each question within a topic, the submitted systems returned about 1000 ranked passages. A pooling technique was then adopted to generate the qrels. In this pooling process, the top 10 results from each different system submission were manually evaluated. Each passage was judged on a five point graded relevance scale: Fails to meet (0), Slightly meets (1), Moderately meets (2), Highly meets (3), and Fully meets (4), the information need.</p><p>Since pooling exercises are costly time wise and financially, the track organisers performed a manual evaluation exercise for 20 topics rather than the complete Table <ref type="table" coords="9,293.13,445.30,4.13,7.89">5</ref>. Qrel details 50 topics provided in the validation set. Table <ref type="table" coords="9,335.67,471.17,4.98,8.74">5</ref> provides a basic overview of the topics for which manual evaluation was carried There were issues in the de-duplication process for the WAPO dataset, hence the track organisers only considered the MS-MARCO and CAR datasets for the final evaluation of the submitted systems. The results from the WAPO dataset were omitted from the submitted system results, with the revised ranking comprising only of the Marco and CAR dataset passages for the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submission results</head><p>Table <ref type="table" coords="9,162.06,596.04,4.98,8.74" target="#tab_4">6</ref> presents the results of our four submitted systems. For comparison, the organizers provided the results of the median system results averaged over all the turns within a topic.</p><p>All the scores of our submitted systems are below the scores of the median systems. From analysis of these results, it appears that using only syntactic information based query formulation and data fusion techniques is not that effective NDCG@5 MAP@5 NDCG@1000 MAP@1000 Median System 0.2960 0.0420 for retrieving potentially relevant passages for conversational search. We carried out further investigation using our system's datasetreorder run which obtained the highest NDCG@5 scores. Table <ref type="table" coords="10,294.63,252.54,4.98,8.74" target="#tab_5">7</ref> presents topic wise results for retrieving and ranking passages for conversational search systems. The results are quite varied across the different topics. This shows that our systems do well for some topics and not so well for others. Note that these results are averaged across all the turns.</p><p>Topic-ID NDCG@5 MAP@5 NDCG@1000 MAP@1000 31 As the results in Table <ref type="table" coords="10,253.65,620.25,4.98,8.74" target="#tab_5">7</ref> are averaged across different turns, we were motivated to calculate the retrieval scores averaged across all topics but for different turns. We hypothesize that:As we go deeper in conversation it should be more difficult to understand and hence answer the question successfully. Table <ref type="table" coords="10,437.51,656.12,4.98,8.74" target="#tab_6">8</ref> presents results averaged across all the turns. The nature of topics can vary significantly, further as the number of turns is not consistent across topics and varies considerably, linearly averaging NDCG scores across topics does not seem to be appropriate. We do not observe a clear trend to support our hypothesis that as we go deeper it becomes harder to understand and answer questions successfully. We plan to investigate these variations of results as shown in Tables <ref type="table" coords="11,443.79,178.77,4.98,8.74" target="#tab_5">7</ref> and<ref type="table" coords="11,472.84,178.77,3.87,8.74" target="#tab_6">8</ref>, across different topics and turns in detail in future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this report we describe our submissions for the CAsT track at TREC 2019. We focus on understanding of information needs in a conversational format to find relevant responses using contextual information. We investigate query formulation and data fusion techniques. Our results show that our query formulation and data fusion techniques attain average results, which are not sufficient to answer questions in a conversational setting effectively. We plan to study the variation in results across different topics and across different turns to understand the challenges to support conversational search systems better. We anticipate a need to leverage effective semantic representation of queries, context and the passages to capture relevant information effectively. In future, we plan to explore recent work using deep learning models such as BERT <ref type="bibr" coords="11,467.30,556.03,9.96,8.74" target="#b1">[2]</ref>, XLNET <ref type="bibr" coords="11,172.48,567.98,10.52,8.74" target="#b7">[8]</ref> and OPEN-GPT-2 model <ref type="bibr" coords="11,297.82,567.98,10.52,8.74" target="#b5">[6]</ref> for the task of ranking passages for conversational search systems as they have shown good results in the benchmarking tasks on question-answer ranking and machine reading comprehension <ref type="bibr" coords="11,444.27,591.89,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="11,458.11,591.89,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="11,469.18,591.89,7.00,8.74" target="#b7">8]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,251.06,300.69,113.23,7.89;6,134.77,115.84,345.82,170.08"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System Architecture</figDesc><graphic coords="6,134.77,115.84,345.82,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,138.97,546.40,341.62,8.77;6,151.70,558.38,41.54,8.74;6,138.97,570.52,287.34,8.77;6,138.97,582.69,306.96,8.77;6,138.97,594.86,300.26,8.77;6,138.97,607.03,255.71,8.77;6,138.97,619.20,275.33,8.77;6,138.97,631.37,229.23,8.77;6,138.97,643.54,197.59,8.77"><head>1 .</head><label>1</label><figDesc>M1: This model was the combination of noun phrase, verb and adjective in the query 2. M2: This model was the combination of noun phrase and verb 3. M3: This model was the combination of noun phrase and adjective 4. M4: This model was the combination of noun, verb and adjective 5. M5: This model was the combination of noun and verb 6. M6: This model was the combination of adjective and noun 7. M7: This model only considered the noun phrase 8. M8: This model only considered the noun</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,180.63,357.37,254.10,7.89;9,226.62,379.75,35.11,7.89;9,362.63,379.77,13.82,7.86;9,226.62,391.11,29.54,7.89;9,362.63,391.13,9.22,7.86;9,226.62,402.46,65.90,7.89;9,362.63,402.49,11.78,7.86;9,226.62,413.82,154.44,7.89;9,226.62,425.18,101.14,7.89;9,362.63,425.20,23.04,7.86;9,134.77,115.83,345.82,226.77"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Worked example of alternative Data Fusion approaches Queries 173 Topics 20 Average turns 8.5 Number of relevant passages 8120 Total passages judged 29350</figDesc><graphic coords="9,134.77,115.83,345.82,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,151.13,360.84,310.02,171.28"><head>Table 1 .</head><label>1</label><figDesc>What are the main breeds of goat? CAsT track sample topic</figDesc><table coords="2,151.13,405.47,212.40,106.52"><row><cell>Question-2 Tell me about boer goats?</cell></row><row><cell>Question-3 What breed is good for meat?</cell></row><row><cell>Question-4 Are angora goats good for it?</cell></row><row><cell>Question-5 What about boer goats?</cell></row><row><cell>Question-6 What are pygmies used for?</cell></row><row><cell>Question-7 What is the best for fiber production?</cell></row><row><cell>Question-8 How long do Angora goats live?</cell></row><row><cell>Question-9 Can you milk them?</cell></row><row><cell>Question-10 How many can you have per acre?</cell></row><row><cell>Question-11 Are they profitable?</cell></row></table><note coords="2,166.12,360.84,25.36,7.89;2,209.26,360.87,46.15,7.86;2,152.11,372.20,246.85,7.89;2,209.26,383.18,251.89,7.86;2,153.79,394.52,50.03,7.89"><p><p>Topic</p>goat breeds Description Interested in buying goats that implies interest in different breeds of goats and their use (milk, meat and fur). Question-1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,229.23,241.37,156.91,7.89"><head>Table 2 .</head><label>2</label><figDesc>Initial and Expanded queries</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,252.40,355.25,110.55,7.89"><head>Table 4 .</head><label>4</label><figDesc>Lexicon objects-2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,143.33,129.14,328.71,72.22"><head>Table 6 .</head><label>6</label><figDesc>Results for our submissions with Median System results for comparison</figDesc><table coords="10,161.34,129.14,260.42,52.10"><row><cell></cell><cell></cell><cell></cell><cell>0.3840</cell><cell>0.1740</cell></row><row><cell>Combination</cell><cell>0.2481</cell><cell>0.0378</cell><cell>0.2869</cell><cell>0.1306</cell></row><row><cell cols="2">Dataset Reorder 0.2512</cell><cell>0.0360</cell><cell>0.2923</cell><cell>0.1356</cell></row><row><cell cols="2">Reranking Order 0.2488</cell><cell>0.0367</cell><cell>0.2937</cell><cell>0.1379</cell></row><row><cell>Topic Turn Sort</cell><cell>0.2427</cell><cell>0.0357</cell><cell>0.2926</cell><cell>0.1367</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,194.94,343.83,225.43,247.17"><head>Table 7 .</head><label>7</label><figDesc>Topic based results for the datasetreorder run</figDesc><table coords="10,194.94,343.83,223.46,227.04"><row><cell></cell><cell>0.1555</cell><cell>0.0142</cell><cell>0.1149</cell><cell>0.0531</cell></row><row><cell>32</cell><cell>0.0827</cell><cell>0.0119</cell><cell>0.1079</cell><cell>0.0482</cell></row><row><cell>33</cell><cell>0.0165</cell><cell>0.0041</cell><cell>0.0481</cell><cell>0.0202</cell></row><row><cell>34</cell><cell>0.0627</cell><cell>0.0118</cell><cell>0.0605</cell><cell>0.0257</cell></row><row><cell>37</cell><cell>0.0480</cell><cell>0.0091</cell><cell>0.1010</cell><cell>0.0528</cell></row><row><cell>40</cell><cell>0.0588</cell><cell>0.0106</cell><cell>0.0608</cell><cell>0.0198</cell></row><row><cell>49</cell><cell>0.0319</cell><cell>0.0052</cell><cell>0.0506</cell><cell>0.0238</cell></row><row><cell>50</cell><cell>0.1059</cell><cell>0.0113</cell><cell>0.1184</cell><cell>0.0687</cell></row><row><cell>54</cell><cell>0.0255</cell><cell>0.0055</cell><cell>0.0466</cell><cell>0.0126</cell></row><row><cell>56</cell><cell>0.0814</cell><cell>0.0075</cell><cell>0.0761</cell><cell>0.0395</cell></row><row><cell>58</cell><cell>0.1033</cell><cell>0.0101</cell><cell>0.0903</cell><cell>0.0272</cell></row><row><cell>59</cell><cell>0.0761</cell><cell>0.0079</cell><cell>0.0748</cell><cell>0.0307</cell></row><row><cell>61</cell><cell>0.0309</cell><cell>0.0046</cell><cell>0.0249</cell><cell>0.0096</cell></row><row><cell>67</cell><cell>0.0764</cell><cell>0.0055</cell><cell>0.0763</cell><cell>0.0198</cell></row><row><cell>68</cell><cell>0.0710</cell><cell>0.0116</cell><cell>0.0799</cell><cell>0.0474</cell></row><row><cell>69</cell><cell>0.0894</cell><cell>0.0162</cell><cell>0.1225</cell><cell>0.0578</cell></row><row><cell>75</cell><cell>0.0264</cell><cell>0.0052</cell><cell>0.0698</cell><cell>0.0177</cell></row><row><cell>77</cell><cell>0.0876</cell><cell>0.0205</cell><cell>0.1290</cell><cell>0.1077</cell></row><row><cell>78</cell><cell>0.0403</cell><cell>0.0142</cell><cell>0.0692</cell><cell>0.0278</cell></row><row><cell>79</cell><cell>0.0768</cell><cell>0.0081</cell><cell>0.0580</cell><cell>0.0254</cell></row><row><cell>all</cell><cell>0.2512</cell><cell>0.0360</cell><cell>0.2923</cell><cell>0.1356</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,208.67,223.18,198.02,148.96"><head>Table 8 .</head><label>8</label><figDesc>Turnwise results for datasetreorder run</figDesc><table coords="11,252.17,223.18,107.94,128.84"><row><cell cols="3">Turns NDCG@5 Topics</cell></row><row><cell>1</cell><cell>0.2115</cell><cell>20</cell></row><row><cell>2</cell><cell>0.2176</cell><cell>20</cell></row><row><cell>3</cell><cell>0.2648</cell><cell>20</cell></row><row><cell>4</cell><cell>0.3532</cell><cell>20</cell></row><row><cell>5</cell><cell>0.2243</cell><cell>20</cell></row><row><cell>6</cell><cell>0.2298</cell><cell>20</cell></row><row><cell>7</cell><cell>0.2620</cell><cell>19</cell></row><row><cell>8</cell><cell>0.1969</cell><cell>20</cell></row><row><cell>9</cell><cell>0.1925</cell><cell>7</cell></row><row><cell>10</cell><cell>0.3937</cell><cell>4</cell></row><row><cell>11</cell><cell>0.5299</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,102.74,7.86"><p>http://www.msmarco.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,623.92,109.40,7.86"><p>http://trec-car.cs.unh.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,634.88,141.74,7.86"><p>https://trec.nist.gov/data/wapost/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,645.84,90.52,7.86"><p>https://spacy.io/usage</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,656.80,167.91,7.86"><p>https://whoosh.readthedocs.io/en/latest/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,144.73,645.84,97.09,7.86"><p>http://www.treccast.ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,144.73,656.80,167.91,7.86"><p>https://whoosh.readthedocs.io/en/latest/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research is supported by <rs type="funder">Science Foundation Ireland (SFI)</rs> as a part of the <rs type="funder">ADAPT Centre at Dublin City University</rs> (Grant No: <rs type="grantNumber">12/CE/I2267</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mbqVWsY">
					<idno type="grant-number">12/CE/I2267</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,155.24,142.90,325.35,8.74;12,155.24,154.86,325.35,8.74;12,155.24,166.81,325.35,8.74;12,155.24,178.77,181.94,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,254.62,142.90,225.97,8.74;12,155.24,154.86,90.91,8.74">Evaluating Retrieval over Sessions: The TREC Session Track 2011-2014</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,274.68,154.86,205.91,8.74;12,155.24,166.81,325.35,8.74;12,155.24,178.77,42.82,8.74">Proceedings of the 39th International ACM SI-GIR Conference on Research and Development in Information Retrieval. SIGIR &apos;16</title>
		<meeting>the 39th International ACM SI-GIR Conference on Research and Development in Information Retrieval. SIGIR &apos;16<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="685" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,155.24,190.72,325.35,8.74;12,155.24,202.68,325.35,8.74;12,155.24,215.35,246.61,8.58" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,247.12,190.72,233.47,8.74;12,155.24,202.68,137.06,8.74">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,155.24,226.59,325.35,8.74;12,155.24,238.55,325.35,8.74;12,155.24,250.50,109.59,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,336.52,226.59,144.07,8.74;12,155.24,238.55,70.29,8.74">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,253.31,238.55,227.28,8.74">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,155.24,262.46,325.35,8.74;12,155.24,274.41,325.34,8.74;12,155.24,286.37,293.65,9.30" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,246.96,262.46,165.29,8.74">Dialogue-Based Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Kaushik</surname></persName>
		</author>
		<idno>978-3-030-15719-7</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,440.60,262.46,40.00,8.74;12,155.24,274.41,105.78,8.74">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="364" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,155.24,298.32,321.52,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,238.26,298.32,140.43,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Program</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,155.24,310.28,325.34,8.74;12,155.24,322.23,121.85,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,243.75,310.28,228.51,8.74">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,170.46,322.23,55.93,8.74">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,155.24,334.19,325.35,8.74;12,155.24,346.14,88.56,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,240.45,334.19,73.35,8.74">Okapi at TREC-3</title>
		<author>
			<persName coords=""><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,342.32,334.19,104.88,8.74">NIST special publication</title>
		<imprint>
			<biblScope unit="volume">500225</biblScope>
			<biblScope unit="page" from="109" to="123" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,155.24,358.10,325.35,8.74;12,155.24,370.05,325.35,9.02;12,155.24,382.72,220.46,8.58" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,238.40,358.10,242.19,8.74;12,155.24,370.05,92.37,8.74">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<ptr target="http://arxiv.org/abs/1906.08237" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
