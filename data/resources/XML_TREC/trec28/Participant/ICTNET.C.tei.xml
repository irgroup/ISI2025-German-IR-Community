<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.17,71.79,337.20,12.90">ICTNET at Trec 2019 Conversational Assistance Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,122.36,111.83,80.05,10.75"><forename type="first">Changying</forename><surname>Hao</surname></persName>
							<email>haochangying@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.85,111.83,88.26,10.75"><forename type="first">Yuanyuan</forename><surname>Zhang</surname></persName>
							<email>zhangyuanyuan@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.55,111.83,70.93,10.75"><forename type="first">Weifeng</forename><surname>Yang</surname></persName>
							<email>yangweifeng@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,410.92,111.83,56.80,10.75"><forename type="first">Heng</forename><surname>Zhao</surname></persName>
							<email>zhaoheng18s@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.17,71.79,337.20,12.90">ICTNET at Trec 2019 Conversational Assistance Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D111F2532CDD068F4270AC17C80B81BD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we report on our participation in the TREC 2019 Conversational Assistance Track which focuses on Conversational Information Seeking(CIS)namely understand the dialogue context and retrieve candidate response information from collections provided. We convert the CIS task into a standard information retrieval task and use both traditional IR model and neural IR model to rerank the baseline official evaluation results. We compare the results of models in two categories(four models in total), and give a summary for the solution of our work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversational Information Seeking is timely and important with increased adoption of a new generation of conversational 'assistant' systems. The focus of the TREC 2019 Conversational Assistance is on understanding of information needs in a conversational format and finding relevant responses using contextual information. Baseline retrieval run on 30 example training topics, limited training data for the 30 topics judged from the baseline retrieval run, as well as baseline retrieval results of evaluation data for 50 evaluation topics were provided as data. There are 10 queries in each topic proposed in terms of conversation, and every query has 1000 relevant documents retrieved advance in baseline retrieved results for both training data and evaluation data. This task is similar to conversational question answering or conversational information retrieving, so we see mainstream approaches in these area as follows:</p><p>1. Traditional matching methods basing on the idea of TF-IDF, such as BM25. Many open source search engines provide good support for the first approach, and BERT <ref type="bibr" coords="1,491.42,264.96,12.72,9.46" target="#b0">[1]</ref> provides a strong support for the latter two methods. Based on the above research backgroundï¼Œwe implement the following four models for the TREC 2019 Conversational Assistance track:</p><p>1. For each query, we use elasticsearch to rerank the baseline retrieval run on evaluation data.</p><p>2. For each query, we split each of its preselected top 1000 documents in baseline retrieval run into short segments, then using BERT to get vectorized representation.</p><p>For each document, we calculate each of its short segments' vectorized representation with query vectorized representation and get a match socre, and select the highest score as the document score.</p><p>3. We concatenate a query and a pre-selected document and get the vectorized representation with BERT. With those representations which have ranking label, we construct and train a pair-wise ranking model.</p><p>4. We use the representations for all tokens in BERT's last layer as the documents' and queries' word vector representation, then using Conv-knrm to calculate the final match score.</p><p>In terms of model performance, the fourth model performs better than the third model on training data set. When actually evaluating 50 topics, we select part of the results for manual analysis and find that the first model has the best performance. A detailed discussion will be covered in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>First, we use coreference resolution for each question in multi-turn conversations. Then we use the disambiguated questions as queries, and convert the Conversational Information Seeking task into a standard retrieval task. In this section, we introduce our four models designed for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Elasticsearch</head><p>Elasticsearch <ref type="bibr" coords="2,129.10,199.31,13.18,9.46" target="#b1">[2]</ref> is a full-text search engine.It is because Elasticsearch contains many advantages that it becomes the most popular enterprise search engine. That is, all kinds of documents can be applied with Elasticsearch, meanwhile, with a given keyword, it can do scalable search and the results it returns usually are closer to real-time than other search engines. More importantly, with the rapid development of Internet, we have to face the phenomenon of sharp expansion of data volume, so it is important to Search in massive amounts of data and get results quickly. The distribution of Elasticsearch makes it possible to support massive, petabytes of big data searches, especially, Elasticsearch also has near real-time (second-level) performance support at massive data levels, as well as grammatical support for powerful search and aggregation analysis. All of these advantages make Elasticsearch more suitable for data analysis applications in big data scenarios.</p><p>In this task, for a variable queryï¼Œwe should find the most 1000 releavent document in three collections, that is, MS MARCO Passage Ranking collection, the TREC CAR paragraph collection v2.0 and the TREC Washington Post Corpus version 2, which consist of over ten million documents. First, we process all these documents into xml format with TREC-CAsT Tools. Next, we use Elasticsearch to do further search sorting based on the search results and queries given by the baseline.</p><p>Elasticsearch's scoring ideas are based primarily on bm25 <ref type="bibr" coords="2,122.85,633.92,16.10,9.46" target="#b2">[3]</ref> and tfidf <ref type="bibr" coords="2,180.35,633.92,11.21,9.46" target="#b3">[4]</ref>. Lucene <ref type="bibr" coords="2,230.81,633.92,14.94,9.46" target="#b4">[5]</ref> (and thus Elasticsearch) uses the Boolean model to find matching documents, and a formula called the practical scoring function to calculate relevance. This formula borrows concepts from term frequency/inverse document frequency and the vector space model but adds more-modern features like a coordination factor, field length normalization, and term or query clause boosting. The formula of Lucene calculateing for the score is as fol-lows: score(q, d) =queryN orm(q) â€¢ coord(q, d)</p><formula xml:id="formula_0" coords="2,308.84,104.51,216.70,271.04">â€¢ tâˆˆq (tf (t, d) â€¢ idf (t) 2 â€¢ t.getBoost() â€¢ norm(t, d)) (1) queryN orm = 1 âˆš SumOf SquaredW eights (2) coord(q, d) = overlap maxoverlap (3) tf (t, d) = f requency (4) idf (t) = 1 + log numDocs docF req + 1 (5) sumOf SquaredW eights =idf (t 1 ) 2 + idf (t 2 ) 2 + ... + idf (t n ) 2<label>(6)</label></formula><formula xml:id="formula_1" coords="2,359.91,387.32,165.63,24.43">norm(d) = 1 numT erms<label>(7)</label></formula><p>where the score(q, d) is the relevance score of document d for query q. queryN orm(q) is the query normalization factor. coord(q, d) is the coordination factor.Where overlap is to retrieve the number of terms in the hit query, maxoverlap is the total number of terms in the query. tf (t, d) is the term frequency for term t in document d. idf (t) is the inverse document frequency for term t, numDocs is the acount of all documents, docF req is the amount of documents that contains term t. t.getBoost() is the boost that has been applied to the query,and can be seen as the weight of each term. norm(t, d) is the field-length norm, combined with the index-time field-level boost, if any.numT erms is terms' counts of a document.Finally, sum all of the weights for each term t in the query q for document d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BERT Based Models</head><p>We have seen a rapid growth of pre-train neural language models recently, such as ELMo <ref type="bibr" coords="2,479.19,688.12,16.47,9.46" target="#b5">[6]</ref>, Ope-nAI GPT <ref type="bibr" coords="2,344.02,701.67,15.45,9.46" target="#b6">[7]</ref>, BERT. They promote the development of a lot of tasks in natural language understanding. We select BERT as our base model. Unlike traditional word embedding, BERT is contextual -the representation of a word is a function BERT is pretrained on a large corpus and thus, it can encode many language features in its contextual representation. BERT is also well suited for the retrieval task as its NSP task can help the interaction-based model judge the relationship between two pieces of text very well.</p><p>In this paper, we use the fine-tuned BERT released by <ref type="bibr" coords="3,119.00,390.97,12.72,9.46" target="#b7">[8]</ref> who augment the primitive BERT with search knowledge by continuing to train it on a large sample of Bing web search log. We use this Bing-augment BERT as our feature-extraction model and try three methods to rerank the top-k baseline results based on the contextual representation produced by this Bing-augment BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">BERT + Sim</head><p>For most documents, the length of a document is always too long compared to a query's length, and it may be inaccurate if we compute the cosine similarity between a query and a document directly. We thus split each document into many short segments. We set the length of each segment to be twice the length of a correspond query and the last segment's length may be shorter than that. We use BPE <ref type="bibr" coords="3,95.76,619.94,12.72,9.46" target="#b8">[9]</ref> to tokenize each token in each sentence and record the length of each tokenized sentence. We add token [CLS] at the start of the tokenized sentence and add [SEP] at the end of the sentence whether it is a query or a segment of a document. We set the max input length of the BERT to be a fixed number. We fill the token [PAD] if the length of the tokenized sentence is shorter than the max input length.</p><p>As shown in figure <ref type="figure" coords="3,168.98,742.31,4.09,9.46" target="#fig_1">1</ref>, for each query-document pair, we first get split tokenized segments of the document and get their tokens' embedding at the first layer of the model. Then we feed the tokens' embedding of the query and each segment into the Bing-augment BERT. We use average pooling for tokens' representations of the BERT's last layer to get the final representation of a sentence(a query or a segment) at the model's second layer.</p><p>We compute the final representation's cosine similarity between a query and each segment of a document at the last layer of the model. Then we get some similarity values for a query-document pair. For the ith segment of document, the similarity is</p><formula xml:id="formula_2" coords="3,356.55,446.39,168.99,10.68">sim i = cos(R Q , R D seg i)<label>(8)</label></formula><p>We use the max similarity value as the final similarity score for the document with respect to the query.</p><formula xml:id="formula_3" coords="3,315.73,521.30,209.81,10.63">Sim = max{sim i }(i = 1, 2, .., num seg) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">BERT + MLP</head><p>For this model, we don't split documents into a lot of segments, we concatenate a query and a document with a [SEP] token as shown in figure <ref type="figure" coords="3,504.64,592.27,5.45,9.46" target="#fig_2">2</ref> instead.</p><p>First, we construct a lot of positive-negative document pairs for each query according to their different relative score for the same query. For example, two documents of relative score 2 and 0, we regard the document of score 2 as a positive sample and the document of score 0 as a negative sample.</p><p>Second, we get the BPE tokenized sentences' embedding of the two documents and padding them to the same length. For both documents, we concatenate the sequence embedding of the query Third, we get the token [CLS]'s representation of the last BERT layer for both query-document pairs and feed two representations into a same MLP network. The MLP's output unit's dimension is one. So we get two predicted score Å·1 and Å·2 for two query-document pairs. We use the hinge loss to train our BERT + MLP model.</p><formula xml:id="formula_4" coords="4,82.42,377.19,207.85,56.88">l = q d + ,d -âˆˆD +,- q max(0, y(d + ) -y(d -) -(f (q, d + ) -f (q, d -)))<label>(10)</label></formula><p>where D +,- q are q's pairwise preferences, d + ranks higher than d -, y(d + ) and y(d -) are their true relative scores, f (q, d + ) and f (q, d -) are the predicted relative scores. For the sake of brevity, we use Å·1 for f (q, d + ) and Å·2 for f (q, d -) in figure <ref type="figure" coords="4,111.39,516.10,4.09,9.46" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">BERT + Conv-knrm</head><p>In information retrieval task, the query and document often match at n-grams, such as phrases, concepts and entities. However, people usually treat n-grams identically to words, which greatly explode the parameter space, and suffer from data sparsity. In this paper, we use the Convknrm, a Convolutional Kernel-based Neural Ranking Model that models n-gram soft matches for adhoc search <ref type="bibr" coords="4,118.62,661.02,17.73,9.46" target="#b9">[10]</ref>, which uses convolutional neural networks to represent n-grams of various lengths and soft matches them in a unified embedding space. The n-gram soft matches are then utilized by the kernel pooling and learning-to-rank layers to generate the final ranking score.</p><p>In our work, we combine the Bing-augment BERT and Conv-knrm model, in which BERT provides the query and document embedding representations and then employ Conv-knrm model on the query and document words' embeddings to get the final ranking score as showing in Figure <ref type="figure" coords="4,324.88,272.87,4.09,9.46" target="#fig_3">3</ref>. In detail, we use the query and document as input of the Bing-augment BERT respectively, and generate the output of the last layer as corresponding word embedding. While the query words' embedding and document words' embedding have been obtained, the convolutional layer applies convolution filters to compose n-grams from the text(query or document). To be detail, we will get h max kinds n-grams(1, 2, ..., h max ), for each h-grams h âˆˆ {1, ..., h max }, the CNN layer converts the text embedding into h-gram embedding G h q or G d . And the same set of convolution filters is used to compose all n-grams thus the model only needs to learn the CNN weights for combining word-level embeddings, which have much fewer parameters. Then the cross-match layer matches query n-grams and document ngrams of different lengths. The unified embedding representations allow cross-matching n-grams of different lengthsï¼Œand it generates h 2 max translation matrices. After that, kernel-pooling which has K Gaussian kernels is applied to count the soft matches of word or n-gram pairs at K different strength levels. In this step, we get K soft-TF features for each of the h 2 max translation matrices in M. Finally, a ranking score is been computed by the learning-to-rank layer which combines the soft-TF ranking features through neural network layer. For an overview, we refer to <ref type="bibr" coords="4,460.05,652.24,16.72,9.46" target="#b9">[10]</ref>.</p><p>All of the BERT and Conv-knrm layers are differentiable, and standard pairwise learning-torank is used to train the combined model jointly. where D +,- q are q's pairwise preferences: d + ranks higher than d -. In our work, as show in the train data, there are three categories rank scores namely 0, 1, 2.</p><formula xml:id="formula_5" coords="4,307.28,725.52,226.30,39.80">l = q d + ,d -âˆˆD +,- q max(0, 1-f (q, d + )+f (q, d -)) (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In our experiments, we convert all sentences of the whole data set to lowercase form.</p><p>We use the training data with judgments released by Trec cast as our experiment data, there are 1930 labeled query-document pairs in total. We split the labeled data into train set, validation set and test set with the proportion of 8:1:1. For BERT+MLP model and BERT+Conv-knrm model, we use train set and validation set during the training process and test the models' ability on the test data set. Finally, we use our trained model to rerank the baseline results provided by trec cast using indri on the evaluation data containing 50 evaluation topics. For Elastic search model and BERT+Sim model, we directly rerank the baseline evaluation results provided by trec cast using the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>For Elastic search model, We use the default sorting algorithm of Elasticsearchï¼Œwhich the result is returned in reverse order according to the degree of relevance (matching degree) of the document and the query. The larger the score ( score), the higher the correlation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/BODY&gt; &lt;/HTML&gt; &lt;/DOC&gt;</head><p>After we getting the converted data,before using elasticsearch for search sorting, we need to create corresponding indexs for all documents.What's more,considering the large amount of data, we used the method of importing large documents specified by elasticsearch--divide large documents into smaller chunks of bulk document then import. The code are as follows:</p><p>#!/bin/bash #split -l 10000 marco.json ./tmp /macro_bulk #BULK_FILES=./tmp/marco_bulk * for f in ./tmp2/ * ; do curl -H "Content-Type: appli cation/json" -XPOST "localho stï¼š9200/test/_bulk" --databinary "@$f" &gt;&gt; /dev/null temp_file=${f:16} part1="localhost:9200/" part2="/_bulk" #curl -H "Content-Type:a ppliaction/json" -XPOST $part1$temp_file$part2 --data-binary "@$f" &gt;&gt; /d ev/null echo $f &gt;&gt; ./import.log done Then we can set the query to search for elasticsearch. The search settings are given by the following code: seach_body = {"query": {"match":</p><p>{"id":query}}} res = es.search(index=[all index s], body=seach_body) for hit in res['hits']['hits']: id = hit["_source"]["id"] body = hit["_source"]["body"]</p><p>All of the three BERT-based models are based on the Bing-augment BERT. It is augmented based on the primitive BERT-base-uncased model.The number of BERT layers is 12, the hidden size is 768 and the number of attention heads is 12. For BERT+Sim model, the max length of all the queries is 20 and thus we set the length of a short segment to be 40. We set the max input length of BERT to be 100. For BERT+MLP model and BERT+Conv-knrm model, we set the max input length of BERT to be 512. We use adam optimiser to train the MLP model and the Conv-knrm model with learning rate of 5e-5 , betas of (0.9,0.98) and epsilon of 1e-8. In BERT+Conv-knrm model, the max n-grams number is 3, the number of convolution filters is 128, and the number of Gaussian kernels is 11. We use MRR, MAP, NDCG@3 and NDCG@5 as automatic metrics for BERT+MLP model and BERT+Conv-knrm model as those two models need training. As can be seen from table 1, We can get competitive performance using only BERT+MLP model. The Bing-augment BERT has a strong ability to model the interaction of query and document. The representations extracted by BERT help the MLP model to give a more accurate match score for query-document pairs. When we use BERT+Conv-knrm model, we have a bigger improvement in the performance of ranking retrieved documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Finally, we compare the performance of all the models on the evaluation data set by human evaluation. We firstly sample 10 evaluation topics with ranking provided by four models. For each topic, we judge whether the top-10 retrieved documents for each query match the query well for all the four models. We find that the results provided by Elasticsearch has the best performance however. We finally submit the results using Elasticsearch model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Experimental result evaluation is generated by the assessed documents. The evaluation consists of MAP,NDCG and so on, which contains 91 metrics.And the results are as follows: According to the results from the official return, we made another statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we use a variety of models aiming for tackle the problem of the TREC 2019 Conversational Assistance Track, including the traditional model such as Elasticsearch and the combination BERT-based models i.e. cosine similarity, MLP and Conv-knrm. The BERT+Conv-knrm model perform better than the BERT+MLP model on the relavant documents reranking task on the training dataset. On the evaluatation dataset, we find that the Elasticsearch model has the best performance by human evaluation however. Traditional models still have strong ability in information retrieval tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,80.66,742.31,209.62,9.46;1,93.82,755.86,78.61,9.46;1,315.93,226.40,195.54,9.46"><head>2 . 3 .</head><label>23</label><figDesc>Document matching extraction based on semantic extraction. Query and document's pair wise learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,241.58,234.73,114.39,8.64;3,95.61,62.81,406.32,160.08"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: BERT+Sim Model</figDesc><graphic coords="3,95.61,62.81,406.32,160.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,239.37,184.81,118.82,8.64;4,125.25,62.81,347.04,110.16"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BERT+MLP Model</figDesc><graphic coords="4,125.25,62.81,347.04,110.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,224.35,309.86,148.86,8.64;5,96.87,62.81,403.80,235.20"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: BERT + Conv-knrm Model</figDesc><graphic coords="5,96.87,62.81,403.80,235.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,307.28,343.71,218.27,420.86"><head></head><label></label><figDesc>The documents used for Elastic search are processed as the following examples:</figDesc><table coords="5,307.28,381.50,216.00,383.07"><row><cell>er the impressive achievement</cell><cell></cell></row><row><cell>of the atomic researchers and</cell><cell></cell></row><row><cell>engineers is what their succe</cell><cell></cell></row><row><cell>ss truly meant;hundreds of th</cell><cell></cell></row><row><cell>ousands of innocent lives obl</cell><cell></cell></row><row><cell>iterated.</cell><cell></cell></row><row><cell>&lt;/BODY&gt;</cell><cell></cell></row><row><cell>&lt;/HTML&gt;</cell><cell></cell></row><row><cell>&lt;/DOC&gt;</cell><cell></cell></row><row><cell>&lt;DOC&gt;</cell><cell></cell></row><row><cell>&lt;DOCNO&gt;WAPO_b2e89334-33f9-11e</cell><cell></cell></row><row><cell>1-825f-dabc29fd7071-1&lt;/DOCNO&gt;</cell><cell></cell></row><row><cell>&lt;DOCHDR&gt;https://www.washingto</cell><cell></cell></row><row><cell>npost.com/sports/colleges/dan</cell><cell></cell></row><row><cell>ny-coale-jarrett-boykin-are-a</cell><cell></cell></row><row><cell>-perfect-1-2-punch-for-virgin</cell><cell></cell></row><row><cell>ia-tech/2011/12/31/gIQAAaW4SP</cell><cell></cell></row><row><cell>_story.html</cell><cell></cell></row><row><cell>&lt;/DOCHDR&gt;</cell><cell></cell></row><row><cell>&lt;HTML&gt;</cell><cell></cell></row><row><cell>&lt;BODY&gt;</cell><cell></cell></row><row><cell>&lt;span class="dateline"&gt;NEW OR</cell><cell></cell></row><row><cell>LEANS ?&lt;/span&gt; Whenever a &lt;a href="http:// www.washingtonp ost.com/blogs/hokies-journal" title="www.washingtonpost.com "&gt;Virginia Tech&lt;/a&gt; offensive coach is asked how the most p</cell><cell>&lt;DOC&gt; &lt;DOCNO&gt;CAR_000000afe1da525b3d b17db77f350b187441a9ed &lt;/DOCNO&gt; &lt;DOCHDR&gt;</cell></row><row><cell>rolific receiving duo in scho ol history came to be, inevit ably the first road game in 2 008 against North Carolina co mes up.</cell><cell>&lt;/DOCHDR&gt; &lt;BODY&gt;The 1913 Johannisthal A ir Disaster happened close to the air field, killing all 28 passengers.The German astrona</cell></row><row><cell></cell><cell>ut Rein hard Furrer died on S</cell></row><row><cell></cell><cell>eptember 9,1995 during histor</cell></row><row><cell></cell><cell>ic flight.</cell></row><row><cell></cell><cell>&lt;/BODY&gt;</cell></row><row><cell></cell><cell>&lt;/DOC&gt;</cell></row><row><cell></cell><cell>&lt;DOC&gt;</cell></row><row><cell></cell><cell>&lt;DOCNO&gt;MARCO_0&lt;/DOCNO&gt;</cell></row><row><cell></cell><cell>&lt;DOCHDR&gt;</cell></row><row><cell></cell><cell>&lt;/DOCHDR&gt;</cell></row><row><cell></cell><cell>&lt;HTML&gt;</cell></row><row><cell></cell><cell>&lt;BODY&gt;The presence of communi</cell></row><row><cell></cell><cell>cation amid scientific minds</cell></row><row><cell></cell><cell>was equally important to the</cell></row><row><cell></cell><cell>success of the Manhattan Proj</cell></row><row><cell></cell><cell>ect as scientific intellect w</cell></row><row><cell></cell><cell>as. The only cloud hanging ov</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,307.28,701.69,218.28,59.81"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons for BERT+MLP and BERT+Conv-knrm on labeled test data set.</figDesc><table coords="6,313.37,735.62,207.64,25.87"><row><cell>Model</cell><cell>MRR</cell><cell cols="3">MAP NDCG@3 NDCG@5</cell></row><row><cell>BERT+MLP</cell><cell cols="2">0.3372 0.3060</cell><cell>0.2314</cell><cell>0.2605</cell></row><row><cell cols="3">BERT+CONV-KNRM 0.4368 0.4007</cell><cell>0.2452</cell><cell>0.2713</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,473.31,218.28,59.81"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons for our results and median results of all submissions.</figDesc><table coords="7,78.09,507.25,167.94,25.87"><row><cell>results</cell><cell cols="3">MAP@5 NDCG@5 NDCG@1000</cell></row><row><cell>median</cell><cell>0.0337</cell><cell>0.2656</cell><cell>0.3622</cell></row><row><cell>elasticsearch</cell><cell>0.0193</cell><cell>0.1641</cell><cell>0.2574</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,93.82,590.59,196.45,9.46;7,93.82,604.14,98.16,9.46"><p>On MAP@5,We have 75 turns with a higher score than the median.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,93.82,627.62,196.45,9.46;7,93.82,641.17,99.98,9.46"><p>On NDCG@5,We have 70 turns performing better than the median.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,93.82,664.64,196.45,9.46;7,93.82,678.19,192.70,9.46;7,82.91,701.67,207.36,9.46;7,72.00,715.22,218.27,9.46;7,72.00,728.77,218.27,9.46;7,72.00,742.32,218.27,9.46;7,72.00,755.86,218.27,9.46;7,307.28,66.67,218.27,9.46;7,307.28,80.22,218.27,9.46;7,307.28,93.76,218.27,9.46;7,307.28,107.31,218.27,9.46;7,307.28,120.86,218.27,9.46;7,307.28,134.41,209.98,9.46"><p>On NDCG@1000,the scores of 58 turns in our method are higher than those in median.There are 173 turns in total on the evaluation dataset, and it seems that our results is a little worse on the above three metrics. According to the model results we submitted, we can conclude that only using traditional search model solely may be not a good idea in information retrieval task because of not utilizing the ground truth. However, in this TREC, the ground truth is too few to obtain a better results for neural IR model. Thus, the lack of ground truth is a dilemma for us to train the neural IR model and get a promising results.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,318.89,403.51,206.66,8.64;7,318.19,414.47,207.37,8.64;7,318.19,425.43,207.37,8.64;7,318.19,436.21,207.36,8.82;7,318.19,447.17,207.37,8.59;7,318.19,458.13,207.37,8.59;7,318.19,469.08,192.86,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,409.91,414.47,115.64,8.64;7,318.19,425.43,207.37,8.64;7,318.19,436.39,11.42,8.64">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,358.19,436.21,167.36,8.59;7,318.19,447.17,207.37,8.59;7,318.19,458.13,207.37,8.59;7,318.19,469.08,87.53,8.59">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,318.89,491.64,206.66,8.64;7,318.19,502.42,169.13,8.82" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Clinton</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Elasticsearch</surname></persName>
		</author>
		<title level="m" coord="7,318.19,502.42,139.56,8.82">The definitive guide. Oreilly Media</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,318.89,524.97,206.66,8.64;7,318.19,535.93,35.14,8.64;7,369.68,535.93,155.87,8.64;7,318.19,546.71,207.36,8.82;7,318.19,557.67,207.37,8.59;7,318.19,568.63,46.77,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,369.68,535.93,155.87,8.64;7,318.19,546.89,61.48,8.64">Simple bm25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taylor</forename><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,405.12,546.71,120.42,8.59;7,318.19,557.67,207.37,8.59;7,318.19,568.63,17.49,8.59">Thirteenth Acm International Conference on Information &amp; Knowledge Management</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,318.89,591.18,206.66,8.64;7,318.19,602.14,207.37,8.64;7,318.19,612.92,207.37,8.82;7,318.19,623.88,207.37,8.59;7,318.19,634.84,207.37,8.59;7,318.19,645.80,192.77,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,352.89,602.14,172.67,8.64;7,318.19,613.10,11.42,8.64">Stemming and its effects on TFIDF ranking</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Kantrowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Behrang</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vibhu</forename><forename type="middle">O</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,347.46,612.92,178.09,8.59;7,318.19,623.88,207.37,8.59;7,318.19,634.84,168.49,8.59">SIGIR 2000: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">July 24-28, 2000. 2000</date>
			<biblScope unit="page" from="357" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,318.89,668.35,206.65,8.64;7,318.19,679.31,207.37,8.64;7,318.19,690.27,207.37,8.64;7,318.19,701.05,114.57,8.82" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,481.99,679.31,43.57,8.64;7,318.19,690.27,203.30,8.64">Integrating the probabilistic models BM25/BM25F into lucene</title>
		<author>
			<persName coords=""><forename type="first">JoaquÃ­n</forename><surname>PÃ©rez-Iglesias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>JosÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">VÃ­ctor</forename><surname>PÃ©rez-AgÃ¼era</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuval</forename><forename type="middle">Z</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Feinstein</surname></persName>
		</author>
		<idno>CoRR, abs/0911.5046</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,318.89,723.60,206.66,8.64;7,318.19,734.56,207.37,8.64;7,318.19,745.52,207.37,8.64;7,318.19,756.30,205.51,8.82" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m" coord="7,395.13,745.52,130.43,8.64;7,318.19,756.48,38.49,8.64">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,83.62,67.28,206.66,8.64;8,82.91,78.24,207.37,8.64;8,82.91,89.02,207.36,8.82;8,82.91,99.98,179.88,8.59;8,82.91,110.94,218.23,8.59;8,82.91,121.90,124.72,8.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,248.33,67.28,41.95,8.64;8,82.91,78.24,107.04,8.64;8,206.98,78.24,83.30,8.64;8,82.91,89.20,170.59,8.64">Improving language understanding by generative pre-training</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct coords="8,83.62,142.00,206.66,8.64;8,82.91,152.96,207.37,8.64;8,82.91,163.74,207.36,8.82;8,82.91,174.70,207.37,8.59;8,82.91,185.66,207.37,8.82;8,82.91,196.80,49.69,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,224.16,142.00,66.12,8.64;8,82.91,152.96,207.37,8.64;8,82.91,163.92,35.67,8.64">Deeper text understanding for IR with contextual neural language modeling</title>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,137.98,163.74,152.29,8.59;8,82.91,174.70,207.37,8.59;8,82.91,185.66,133.60,8.59">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,83.62,216.72,206.66,8.64;8,82.91,227.68,207.37,8.64;8,82.91,238.46,207.36,8.82;8,82.91,249.42,207.37,8.59;8,82.91,260.38,99.07,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,117.27,227.68,173.01,8.64;8,82.91,238.64,75.69,8.64">Neural machine translation of rare words with subword units</title>
		<author>
			<persName coords=""><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,177.80,238.46,112.47,8.59;8,82.91,249.42,207.37,8.59;8,82.91,260.38,69.53,8.59">Proceedings of the 54th Annual Meeting of the Association for Computational Linguisticsï¼ŒACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguisticsï¼ŒACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.60,280.48,201.68,8.64;8,82.91,291.44,207.37,8.64;8,82.91,302.22,207.36,8.82;8,82.91,313.18,207.37,8.59;8,82.91,324.14,207.37,8.82;8,82.91,335.28,69.62,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,147.23,291.44,143.05,8.64;8,82.91,302.40,163.80,8.64">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,272.46,302.22,17.81,8.59;8,82.91,313.18,207.37,8.59;8,82.91,324.14,152.28,8.59">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
