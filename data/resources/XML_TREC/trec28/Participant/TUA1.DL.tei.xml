<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.60,90.19,290.83,12.64">TUA1 at the TREC 2019: Deep Learning Track</title>
				<funder ref="#_bVZZmdV #_rT4GmEg">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.98,125.33,45.06,10.53"><forename type="first">Yun</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.74,125.33,58.59,10.53"><forename type="first">Hai-Tao</forename><surname>Yu</surname></persName>
							<email>yuhaitao@slis.tsukuba.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.01,125.33,49.48,10.53"><forename type="first">Xin</forename><surname>Kang</surname></persName>
							<email>kang-xin@is.tokushima-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.46,125.33,44.82,10.53"><forename type="first">Fuji</forename><surname>Ren</surname></persName>
							<email>ren@is.tokushima-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokushima University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.60,90.19,290.83,12.64">TUA1 at the TREC 2019: Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2551D54207984251243B37F16679BDAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conv-KNRM</term>
					<term>BERT re-ranking</term>
					<term>Learning to rank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper details our participation in the TREC 2019 Deep Learning Track task including Passage Ranking task. In the Top-1000 Re-ranking subtask of Passage Ranking task, we performed passage ranking based on the technique of Conv-KNRM and BERT. In order to get a better ranking result, we divided the task into two stages. In the first stage we use Conv-KNRM model for initial ranking, then in the second stage we combine the results with a state-of-the-art BERT re-ranker.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TUA1 group from The University of Tokushima participated in Passage Ranking task of the TREC 2019 Deep Learning Track Task, i.e., the Passage Ranking Top-1000 Re-ranking subtask. For detailed introductory information of this subtask, please refer to the task homepage<ref type="foot" coords="1,273.03,543.50,3.49,6.13" target="#foot_0">1</ref> , the TREC Tracks homepage<ref type="foot" coords="1,155.64,554.46,3.49,6.13" target="#foot_1">2</ref> , and the overview of the TREC 2019 Deep Learning Paper <ref type="bibr" coords="1,163.14,566.16,87.57,9.65" target="#b0">[Craswell et al., 2019]</ref>. The Deep Learning Track has two tasks: Passage Ranking and Document Ranking. The passage ranking task has a full ranking and re-ranking subtasks. We participate in top-1000 re-ranking subtask. This subtask provides with an initial ranking of 1000 passages and re-ranks these passages based on their likelihood of containing an answer to the question. We investigated the traditional IR model, such as BM25 <ref type="bibr" coords="1,82.94,653.83,93.11,9.65" target="#b3">[Robertson et al., 1994]</ref>, neural IR model and BERT <ref type="bibr" coords="1,54.00,664.79,79.03,9.65" target="#b2">[Devlin et al., 2018]</ref>. In this task, we use the most advanced neural retrieval model Conv-KNRM <ref type="bibr" coords="1,470.60,218.47,68.34,9.65" target="#b1">[Dai et al., 2018]</ref> and BERT to rank passages. This article will elaborate on our experimental process and results.</p><p>In the remainder of this paper, we outline the Deep Learning Track tasks in Section 2. Section 3 and Section 4 details the approaches proposed for top-1000 re-ranking subtask respectively.We conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Learning Track Tasks</head><p>The deep learning track has two tasks: Passage Ranking and Document Ranking. Both tasks use a large human-generated set of training labels, from the MS-MARCO<ref type="foot" coords="1,491.83,347.43,3.49,6.13" target="#foot_2">3</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Passage Ranking Task</head><p>The passage ranking task has a full ranking and re-ranking subtasks.In context of full ranking subtask, given a question to rank passages from the full collection in terms of their likelihood of containing an answer to the question, and submit up to 1000 passages for this end-to-end retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Top-1000 re-ranking subtask</head><p>In context of top-1000 re-ranking subtask, an initial ranking of 1000 passages is provided and expected to re-rank these passages based on their likelihood of containing an answer to the question. In this subtask, we can compare different reranking methods based on the same initial set of 1000 candidates, with the same rationale as described for the document re-ranking subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We used the training set provided by the organizer to train a Conv-KNRM model, and then used it to rank in the first stage. After that, we used a BERT pre-training model, and then reranked in the second stage to get the final ranking result. An overview of the proposed method is shown in Figure <ref type="figure" coords="1,526.99,616.64,3.74,8.76" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conv-KNRM</head><p>Conv-KNRM is a Convolutional Kernel-based Neural Ranking Model that models n-gram soft matches for ad-hoc search. of various lengths and soft matches them in a unified embedding space. The n-gram soft matches are then utilized by the kernel pooling and learning-to-rank layers to generate the final ranking score. Conv-KNRM can be learned endto-end and fully optimized from user feedback.The architecture of our Conv-KNRM model is identical to the base model described in <ref type="bibr" coords="2,106.41,257.10,66.54,9.65" target="#b1">[Dai et al., 2018]</ref>, we used train.triple.small.tsv of the MS-MARCO dataset as the training set. It has three columns of data, namely query, positive passage and negative passage. We train with a batch size of 64 tokens, and vocab size of 851354.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It uses Convolutional Neural Networks to represent n-grams</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BERT Re-rank</head><p>The re-ranker is to estimate a score s i of how relevant a candidate passage d i is to a query q. We use BERT as our re-ranker. Using the same method used by <ref type="bibr" coords="2,54.00,370.04,90.68,9.65" target="#b3">[Nogueira et al., 2019]</ref>.The query as sentence A and the passage text as sentence B. We use a BERT LARGE model as a binary classification model. Using the [CLS] vector as input to a single layer neural network to obtain the probability of the passage being relevant. We independently calculate the probabilities of each article and rank them according to these probabilities to obtain the final list of passages. We optionally re-rank these retrieved documents using BERT described in <ref type="bibr" coords="2,64.24,457.71,102.79,9.65" target="#b3">[Nogueira and Cho, 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conv-KNRM + BERT re-rank</head><p>First, a large number of possibly relevant documents to a given question are retrieved from Conv-KNRM Model. In the second stage, passage re-ranking, each of these documents is scored and re-ranked by a more computationally-intensive method.</p><p>We retrieve documents as in Conv-KNRM and further rerank the documents with BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup 4.1 Data</head><p>To train and evaluate the models, we use the following dataset: MS MARCO is a passage re-ranking dataset with 8.8M passages obtained from the top-10 results retrieved by the Bing search engine <ref type="bibr" coords="2,114.04,661.63,83.70,9.65" target="#b3">[Nguyen et al., 2016]</ref>. The training set contains approximately 500k pairs of query and relevant documents. On average each query has one relevant passage. The development and test sets contain approximately 6,900 queries each, but relevance labels are made public only for the development set.</p><p>The training set contains approximately 400M tuples of a query, relevant and non-relevant passages. The development set contains approximately 6,900 queries, each paired with the top 1,000 passages retrieved with BM25 from the MS MARCO corpus.</p><p>An evaluation set with approximately 6,800 queries and their top 1,000 retrieved passages without relevance annotations is also provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ranking Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Metrics</head><p>To evaluate the effectiveness of the methods on MS MARCO, we use its official metric, mean reciprocal rank of the top-10 documents (MRR@10). For TREC Deep Learning task, we use normalized discounted cumulative gain (NDCG), precision (P), and mean average precision (MAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">runs</head><p>We submitted one run for the re-ranking subtask.</p><p>â€¢ TUA1-1. We combine Conv-KNRM results with BERT re-ranker, achieve our team's best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental Results</head><p>Table <ref type="table" coords="2,339.65,478.65,4.98,8.76" target="#tab_0">1</ref> shows the performance on the MS-MARCO dev set, where MMR@l is used with a cut-off value of 10.</p><p>MRR@10 BM25 0.184 Conv-KNRM 0.224 Conv-KNRM + BERT 0.366 Table <ref type="table" coords="2,349.49,611.26,4.98,8.76" target="#tab_1">2</ref> shows the performance of the run, where P@l and NDCG@l are used as the evaluation metrics with a cut-off value of 10. P@l represents the precision at l, NDCG@l represents the normalised discounted cumulative gain.MAP represents the mean average precision.</p><p>Run NDCG@10 P@10 MAP TUA1-1 0.7314 0.6372 0.4571 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we described our approaches to solve the Top-1000 Re-ranking subtask in the TREC 2019 Deep Learning Track task. Our current implementation is based on integrating three open-source toolkits: Kernel-Based-Neural-Ranking-Models, Anserini(BM25), and dl4marcobert(BERT re-ranker). Among the neural approaches, the best-performing methods tended to use transfer learning, employing a pre-trained language model such as BERT. Although the neural IR model has good results, there are also some limitations of our current work. For example, (1) the low-frequency term is a recurring challenge for information retrieval model, especially the neural IR framework is difficult to fully capture the word not often observed. (2) although BERT re-ranking have a good result, but it is very time-consuming. In the future work, we will investigate the effectiveness of word embedding on ranking results, and improve the speed of document re-ranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,54.00,143.30,243.00,7.88;2,54.00,153.26,243.00,7.88;2,54.00,163.23,105.85,7.88;2,55.58,54.01,239.80,78.28"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Given documents, Conv-KNRM model predicts document ranking results, acorrding the ranking results, BERT re-ranker predicts the final ranking results.</figDesc><graphic coords="2,55.58,54.01,239.80,78.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,315.00,189.20,175.82,8.76;2,315.00,200.02,208.95,8.90;2,527.04,198.52,3.49,6.13;2,534.10,200.16,23.90,8.76;2,315.00,211.12,243.00,8.76;2,315.00,222.08,243.01,8.76;2,315.00,233.04,157.60,8.76;2,315.00,243.86,243.00,8.90;2,315.00,254.95,199.59,8.76;2,516.83,253.32,3.49,6.13;2,523.07,254.95,34.93,8.76;2,315.00,265.91,84.28,8.76;2,315.00,276.73,235.69,8.90;2,554.02,275.24,3.49,6.13;2,315.00,287.83,64.92,8.76;2,315.00,298.65,243.00,8.90;2,315.00,309.75,229.53,8.76"><head></head><label></label><figDesc>We evaluate the following ranking methods: BM25: We use the Anserini open-source IR toolkit 4 (Yang et al., 2017, 2018) to index the original (non-expanded) documents and BM25 to rank the passages. During evaluation, we use the top1000 re-ranked passages. Conv-KNRM: We use the open-source Neural Kernel Match IR method Kernel-Based-Neural-Ranking-Models 5 to train a Conv-KNRM model. BERT re-ranker: We use the open-source dl4marco-bert 6 as our re-ranker. Conv-KNRM+BERT: The ranking result of Conv-KNRM is put into BERT re-ranker to obtain the final ranking result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,323.40,576.64,226.19,7.88"><head>Table 1 :</head><label>1</label><figDesc>Mean reciprocal rank scores for MS-MARCO dev set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,80.19,100.07,190.61,7.88"><head>Table 2 :</head><label>2</label><figDesc>TREC evaluation scores for re-rank subtask.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,70.14,685.19,200.87,7.88"><p>https://microsoft.github.io/TREC-2019-Deep-Learning/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,70.14,696.06,111.70,7.88"><p>https://trec.nist.gov/tracks.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,331.14,696.06,89.41,7.88"><p>http://www.msmarco.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,331.14,674.32,129.41,7.88"><p>https://github.com/castorini/anserini</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,331.14,685.19,234.77,7.88"><p>https://github.com/thunlp/Kernel-Based-Neural-Ranking-Models</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,331.14,696.06,145.21,7.88"><p>https://github.com/nyu-dl/dl4marco-bert</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>This research has been supported by <rs type="funder">JSPS KAKENHI</rs> Grant Number <rs type="grantNumber">19K20345</rs> and Grant Number <rs type="grantNumber">19H04215</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bVZZmdV">
					<idno type="grant-number">19K20345</idno>
				</org>
				<org type="funding" xml:id="_rT4GmEg">
					<idno type="grant-number">19H04215</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,54.00,410.26,55.56,10.53;3,54.00,427.57,243.00,9.65;3,65.62,439.42,231.38,8.76;3,65.62,450.38,231.38,8.76;3,65.62,461.34,56.35,8.76" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,189.46,439.42,107.54,8.76;3,65.62,450.38,74.01,8.76">Overview of the trec 2019 deep learning track</title>
		<author>
			<persName coords=""><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,156.01,450.47,140.99,8.55;3,65.62,461.43,27.51,8.55">Proceedings of the 28th TREC Conference</title>
		<meeting>the 28th TREC Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,54.00,476.57,243.01,9.65;3,65.62,488.43,231.39,8.76;3,65.62,499.39,231.38,8.76;3,65.62,510.35,22.42,8.76" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,173.05,488.43,123.96,8.76;3,65.62,499.39,176.78,8.76">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName coords=""><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,265.74,499.48,25.01,8.55">WSDM</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,54.00,525.58,243.01,9.65;3,65.62,537.43,231.38,8.76;3,65.62,548.39,231.38,8.76;3,65.62,559.35,108.81,8.76" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,211.04,537.43,85.96,8.76;3,65.62,548.39,231.38,8.76;3,65.62,559.35,11.42,8.76">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,95.22,559.44,48.88,8.55">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,54.00,574.58,243.00,9.65;3,65.62,586.44,231.39,8.76;3,65.62,597.40,231.38,8.76;3,65.62,608.35,231.38,8.76;3,65.62,619.31,22.42,8.76;3,54.00,634.54,142.23,9.65;3,220.88,635.44,37.07,8.76;3,282.61,635.44,14.39,8.76;3,65.62,646.40,231.38,8.76;3,65.62,657.36,90.77,8.76;3,54.00,672.59,243.00,9.65;3,65.62,684.45,231.38,8.76;3,65.62,695.40,165.47,8.76;3,315.00,56.47,243.01,9.65;3,326.62,68.33,231.38,8.76;3,326.62,79.28,188.85,8.76" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="3,128.58,597.40,168.42,8.76;3,65.62,608.35,124.54,8.76;3,143.22,646.40,116.52,8.76;3,174.52,684.45,122.48,8.76;3,65.62,695.40,38.99,8.76">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><surname>Nguyen</surname></persName>
		</author>
		<idno>ArXiv, abs/1904.08375</idno>
		<imprint>
			<date type="published" when="1994">2016. 2016. 2019. 2019. 2019. 2019. 1994. 1994</date>
			<publisher>Micheline Hancock-Beaulieu</publisher>
		</imprint>
	</monogr>
	<note>Document expansion by query prediction. and Mike Gatford. Okapi at trec-3. In TREC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
