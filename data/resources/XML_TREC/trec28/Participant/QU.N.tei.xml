<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,229.68,84.23,154.83,15.44;1,100.57,104.15,410.86,15.44">bigIR at TREC 2019: Graph-based Analysis for News Background Linking</title>
				<funder ref="#_MgFgNFa">
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.46,134.31,68.40,10.59"><forename type="first">Marwa</forename><surname>Essam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department Qatar University Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,316.82,134.31,71.72,10.59"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
							<email>telsayed@qu.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department Qatar University Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,229.68,84.23,154.83,15.44;1,100.57,104.15,410.86,15.44">bigIR at TREC 2019: Graph-based Analysis for News Background Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">75EB7655FF584B8B21CD28AC54EAD5CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, it is very rare to find an online news article that is selfcontained with everything a reader would want to know about the article's story. Therefore, it became vital for any article to contain links to other articles or resources that provide the background and contextual knowledge required to conceptualize the article's story. However, finding useful background and contextual links can be a challenging problem. In this paper, we address this problem in the context of the participation of the bigIR team at Qatar University in the news background linking task of the TREC 2019 news track. Our methods mainly relied on a graph-based analysis of the query-article's text to extract its most representative and influential keywords, and then use these keywords as a search query to retrieve the article's background links from a collection of news articles. All of our submitted runs outperformed the TREC hypothetical run that achieved a median effectiveness over all queries. Moreover, our best submitted run was ranked second among 28 runs submitted to the task, indicating the potential effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the last decade, online news reporting services have played a vital role in changing the way people receive and share news. Often though, an author of any news article assumes that readers have a certain background knowledge on the article's topic or story. This is, of course, not the case for many readers. Therefore, to help readers conceptualize a news article, it is necessary to add links in each article to other articles or resources that provide the desired background knowledge <ref type="bibr" coords="1,140.96,504.23,9.34,7.94" target="#b7">[8]</ref>. Links in articles are typically added to point to other articles that are written by the same author, or ones that are most-frequently viewed by the readers. However, adding useful background links to articles can be a challenging problem.</p><p>Motivated to find solutions to this problem, a news background linking task was recently introduced as a new challenge in the news track in TREC 2018 <ref type="bibr" coords="1,150.27,569.99,9.52,7.94" target="#b7">[8]</ref>. A number of teams participated in this challenge in 2018 proposing different methods to solve the problem. Most of the proposed methods relied mainly on an ad-hoc search approach to retrieve the background links for query articles. In this approach, a search query/queries are constructed from an input article, and these queries are issued against an index created for the news collection to retrieve the background links for the input article. Upon the retrieval of an initial set of background links, some of the earlier proposed methods further adopted other approaches to extend their initial results or even to diversify the background links presented to the reader <ref type="bibr" coords="1,208.60,679.57,7.37,7.94" target="#b1">[2]</ref><ref type="bibr" coords="1,215.97,679.57,3.68,7.94" target="#b2">[3]</ref><ref type="bibr" coords="1,219.65,679.57,7.37,7.94" target="#b3">[4]</ref><ref type="bibr" coords="1,229.26,679.57,10.13,7.94" target="#b9">10]</ref>.</p><p>In this paper, we demonstrate the participation of our bigIR team at Qatar University in the background linking task in TREC 2019.</p><p>Our approach relied also on an ad-hoc search approach to find the required background links; nonetheless, we base the construction of the search query mainly on a graph-based analysis of the query article's text <ref type="bibr" coords="1,364.96,236.69,9.38,7.94" target="#b4">[5]</ref>. Precisely, we adopted graph construction and decomposition methods to extract a set of keywords from the query article, and used a weighted representation of those keywords as our search query. Our motivation for using graph decomposition methods for keyword extraction is that it catches important information in the text like the order, co-occurrence, and context relations between the different terms, allowing the most influential and representative keywords to stand out. We submitted four runs to this task using four different methods; yet, all our methods relied on the same graph-based analysis idea.</p><p>The rest of this paper is organized as follows. Section 2 describes, in detail, our proposed approach to solve the problem. Section 3 presents the official runs setup and results as reported back by TREC. Finally, section 4 concludes the paper and outlines potential future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACH</head><p>We map the original news background linking problem to the following problem:</p><p>Given a collection of news articles D and a query article A, construct a search query Q to retrieve background articles of A from D.</p><p>To construct the search query Q, we extract keywords from the query article A that best indicates its relevant topics, and concatenate these keywords in one query. Specifically, the required keywords are obtained using a graph-based analysis of the query article A. Graph-based analysis of text allows to capture the dependency between different terms. Dependency relations may exist between terms in a document (e.g., grammatical dependency), and thus can help quantify the importance of those terms by relative weights. Consequently, those weights may help in choosing keywords that better represent the document. Recently, graph representation of text was explored to deal with term dependency in various Information Retrieval (IR) tasks, and was proved effective <ref type="bibr" coords="1,512.15,598.92,9.33,7.94" target="#b5">[6,</ref><ref type="bibr" coords="1,523.72,598.92,6.14,7.94" target="#b6">7,</ref><ref type="bibr" coords="1,532.11,598.92,6.22,7.94" target="#b8">9]</ref>.</p><p>In this section, we first present the core architecture of our background links retrieval system, then we discuss each component in the system in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Design</head><p>Figure <ref type="figure" coords="1,342.93,668.62,4.09,7.94">1</ref> depicts the high-level architecture of our proposed system. Initially, all the articles in the news collection are indexed. To index an article, its metadata is extracted first. This may include the article's title, URL, author's name and publishing date. For now, we Figure <ref type="figure" coords="2,188.03,241.64,3.45,7.70">1</ref>: A high level overview of the background links retrieval system used this metadata to filter out duplicate articles or articles that represent personal opinions or views. Nonetheless, this metadata can further be used in the re-ranking process of the retrieved results to support the diversification and the freshness of the final articles presented to the reader. Each article then goes to a preprocessing phase where a token stream is generated and fed to the indexer.</p><p>Having all articles in the collection indexed, we proceed to process query articles. For each query article, a co-occurrence graph is constructed from the article's title and body concatenated together. The article's graph is then decomposed into subgraphs that help assign weights to the terms, and the keyword set of the required query text Q is extracted accordingly. Q is finally used to retrieve the required background links from the collection index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Construction</head><p>After preprocessing the query article A into a stream of terms, we construct a co-occurrence graph G for the article. The underlying assumption of creating such graph is that terms co-occurring within a relatively small window of text have some kind of semantic relatedness regardless of their roles in a sentence, and that this relationship influences the importance of each single term within the article. Each node in G represents a single (unigram) term, and each edge connects two terms that co-occur (within a sliding window) at least once in the article. Edges are weighted by the co-occurrence frequency of the term pair. Figure <ref type="figure" coords="2,169.29,534.87,4.17,7.94" target="#fig_0">2</ref> shows the graph constructed for an example short article. We opted to construct an undirected graph as recommended in <ref type="bibr" coords="2,125.32,556.78,9.27,7.94" target="#b4">[5]</ref>, since using directed graphs was not proved to achieve significant impact on keyword extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Decomposition</head><p>After constructing the graph, weights of nodes (representing terms) can be computed using different measures. In this work, we select keywords based on graph decomposition methods <ref type="bibr" coords="2,246.02,624.78,9.52,7.94" target="#b8">[9]</ref>. The idea behind those methods is to decompose the co-occurrence graph into a hierarchy of nested subgraphs using graph degeneracy methods. Each subgraph contains a cohesive subset of nodes from the initial graph. Going down this hierarchy of subgraphs reveals nodes that are placed at the core of the initial graph. This is based on the assumption that keywords in an article's graph are not necessarily the ones with high number of connections or high frequency, but  the ones that are at the core of the graph and can reach many other nodes (spreaders).</p><p>There are several ways to decompose a graph into subgraphs for keyword extraction. In this work, we adopt two methods: k-core <ref type="bibr" coords="2,547.78,690.53,10.42,7.94" target="#b4">[5]</ref> and k-truss <ref type="bibr" coords="2,361.29,701.49,10.51,7.94" target="#b8">[9]</ref> graph decomposition. k-core decomposition relies on peeling away weakly connected nodes to gradually get some understanding of the core of the graph. The k-core of a graph is the maximal subgraph such that every node has a degree at least k, where the degree of a node is the sum of the weights on its connecting edges. k-truss decomposition is an extension to k-core decomposition, in which edges are pruned first if they are weak, and then nodes with no more connecting edges are peeled off. ktruss basically prunes an edge from the k-1 subgraph if it is not supported by at least k-2 other edges that form triangles with that edge. Figure <ref type="figure" coords="3,100.56,186.42,4.17,7.94" target="#fig_1">3</ref> shows an example of both decomposition methods.</p><p>In the decomposition process, both the k-core and the k-truss methods continue increasing k and prune nodes to create more cohesive subgraphs until they reach k-max, which is the deepest level they can reach in decomposition (after which the resulting subgraph becomes empty). A core/truss number of a node is then defined as the maximum core/truss number at which this node exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Keyword Extraction</head><p>Assuming that the graph was decomposed into cores/trusses, we assign a score to each node that is equal to the sum of the core/truss numbers of its neighbors in the the 0-core/0-truss graph. This scoring is based on the assumption that nodes in the same core/truss are as good as spreaders in the graph <ref type="bibr" coords="3,192.59,342.97,9.44,7.94" target="#b0">[1]</ref>. If a score is assigned instead based only on the node's own core/truss number, then many nodes will end up having the same weight. After assigning scores to nodes, keywords then can be selected using different methods. In this work, we first set the number of keywords N to be selected to P% of the number of nodes in the article's full graph. To account for short articles, we test if N is less than a predefined minimum min. If yes, it is set to min. Next, we sort the nodes given their core/truss scores and select the query keywords as the top N terms having the highest scores. A term with a weight equal to the N th node is also selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Retrieving Background Articles</head><p>The set of selected keywords constructs the search query Q, where each keyword is weighted/boosted by its score. We issue this query against the collection index to retrieve a potential set of background articles. Finally, duplicate articles from the results (that have the same author, title and publishing date but different identifier) are removed, and the top 100 links found are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL EVALUATION 3.1 Dataset Preprocessing and Indexing</head><p>We used version 2 of the Washington Post news test collection released for the news track by TREC<ref type="foot" coords="3,176.56,603.85,3.38,6.44" target="#foot_0">1</ref> in 2018. To index the collection, we used Lucene<ref type="foot" coords="3,109.88,614.81,3.38,6.44" target="#foot_1">2</ref> ver. 8.0. During our indexing process, we excluded articles that are "Opinions", "Letters to the Editor", or "The Post's View" as they were declared by TREC to be non-relevant. The articles in this version were provided as JSON objects in a single file. For each article, we extracted the metadata (title, author, publishing date, and URL) and indexed them as separate string fields in Lucene.</p><p>For the article's body, we first concatenated the HTML text contents from the JSON object (marked by a "sanitized_html" type). We then used JSOUP library<ref type="foot" coords="3,389.81,107.56,3.38,6.44" target="#foot_2">3</ref> to extract the raw text from the HTML text. Afterwards, we lower-cased the text and removed stop words and all non-alphabetical characters. The final preprocessed text was indexed as a text field in Lucene along with the article's metadata.</p><p>For the query articles, we first extracted its title and body, concatenated both in one string, then we preprocessed it the same way like the other indexed articles. For retrieving background links, we used Lucene's default ranking model (a variation of the Vector Space Model), and we submitted the first 100 background articles retrieved (after removing duplicates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Measures</head><p>To evaluate the effectiveness of the proposed solutions to the background linking problem, nDCG@5 was used as a primary metric by TREC. The gain value for each retrieved background article was calculated as 2 r where r indicates how much background and context knowledge the retrieved background article provides to the query article. The gain value r ranges from 0 (the article provides little or no useful background information) to 4 (the article must appear in the sidebar otherwise critical context is missing).</p><p>As participants were asked to submit up to 100 background links for each query article, other metrics than nDCG@5 were also reported back by TREC for each submitted run (e.g., precision at different levels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Official Runs</head><p>We experimented with the two graph decomposition methods explained in the previous section in solving the background linking problem. For each method, we tuned a number of parameters using the TREC 2018 queries and relevant judgments. The parameters we tuned were:</p><p>• The sliding window size used when creating the article's graph. • The keyword selection parameters, which are the percentage of terms to be selected P and the minimum number min of keywords to be selected for short articles. • The title words boost factor. After the analysis of the article and the selection of keywords to construct the query, keywords that are part of the article's title are given a boost factor that is multiplied by their weight before issuing the query to Lucene. This is driven by our belief that authors carefully select title words to mostly reflect the message/story of the article.</p><p>As stated in the guidelines for this task last year and also this year, the user is assumed to read the query article in the present time. This allows articles published at any time to be candidate background articles to the query article. Nonetheless, during our tuning and testing phase using the TREC 2018 set of query articles and relevant judgements, we always found that excluding articles that were published after the query article (denoted as forward links) from the results exhibits better performance. Therefore, for each of the graph decomposition methods we used for analyzing the query article, we submitted two runs for TREC 2019: one that does not include forward links, and another that does. The runs we submitted were as follows:</p><p>• QU _KCore: In this run, we used k-core as our graph decomposition method with a window size set to 4, the percentage of terms P set to 20% , min for short articles set to 70, and a title boost set to 1.7. In this run, we excluded forward links from the results. • QU _KCore_F : In this run, we used the same settings as in QU _KCore, but we included forward links in the results. • QU _KT russ: In this run, we used k-truss as our graph decomposition method with a window size set to 6, the percentage of terms P set to 20%, and min for short articles set to 70. We also excluded forward links from the results. • QU _KT russ_F : In this run, we used the same settings as in QU _KT russ, but we included forward links in the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>Table <ref type="table" coords="4,76.19,289.60,4.25,7.94" target="#tab_0">1</ref> shows the nDCG@5 values achieved by our methods. In general, all our runs outperform the TREC median for this task with our best run, (QU _KCore), ranked second among 28 total submitted runs. This supports, to a great extent, the effectiveness of using graph-based text analysis in solving the news background linking problem. We also notice that using the k-core method for the article's graph analysis and decomposition produced a higher nDCG@5 score compared to k-truss.</p><p>We can also notice from Table <ref type="table" coords="4,175.73,377.27,3.06,7.94" target="#tab_0">1</ref>, as expected, that the runs that excluded forward links produced better results using both graph decomposition methods. Nonetheless, as mentioned before, other metrics than nDCG@5 were also measured. In our analysis of the precision scores that our methods achieved at different cutoffs, we found that, as shown in Figure <ref type="figure" coords="4,165.11,432.07,3.01,7.94" target="#fig_2">4</ref>, including the forward links to the results set generally yielded retrieval of more relevant articles. This measure of performance, however, ignores how much useful is the retrieved background article to the query article.</p><p>Run nDCG@5 Rank </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we described the approach we adopted to tackle the problem of news background linking as part of our participation in TREC 2019 news track. Our solution relied basically on extracting representative keywords from the news query article, and using them to find the article's background links. We used a graph-based analysis of articles for keyword extraction, and varied the methods used for constructing the graph and analyzing it. The official results reported by TREC showed that our proposed approach, in different settings, outperformed the TREC median for this task. Moreover, our best submitted run was ranked second among 28 total submitted runs to the task, indicating the potential effectiveness of our approach. Our future work includes conducting more deep experiments to confirm the effectiveness of adopting graph-based text analysis in solving the news background linking problem. We will also investigate the idea of expanding the extracted keywords using other methods such as word embedding similarities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,317.96,387.50,240.25,7.70;2,317.96,398.45,240.25,7.73;2,317.96,409.41,240.25,7.73;2,317.96,420.37,174.92,7.73;2,334.08,436.99,208.00,149.60"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A graph constructed for an example article: "Online newspaper is the online version of a newspaper, either as a standalone publication or as the online version of a printed periodical", with a sliding window of size 2.</figDesc><graphic coords="2,334.08,436.99,208.00,149.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,317.96,600.59,241.85,7.70;2,317.96,611.55,240.25,7.70;2,317.96,622.51,240.24,7.70;2,317.96,633.46,163.36,7.70"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graph decomposition: (a) the main graph. (b) the 3core decomposition. (c) the 3-truss decomposition. Note that the node * is removed in the truss decomposition because its connecting edge is not part of a triangle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,317.96,257.88,240.25,7.70;4,317.96,268.84,203.09,7.70;4,322.88,83.69,230.40,160.20"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The precision scores obtained for all the submitted runs at different numbers of retrieved documents.</figDesc><graphic coords="4,322.88,83.69,230.40,160.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,53.50,501.41,240.54,92.34"><head>Table 1 :</head><label>1</label><figDesc>nDCG@5 scores for the submitted runs compared to TREC'19 median and best runs.</figDesc><table coords="4,102.03,501.41,138.08,69.05"><row><cell cols="2">TREC'19-Median 0.5295</cell><cell>-</cell></row><row><cell>TREC'19-Best</cell><cell>0.6064</cell><cell>1 st</cell></row><row><cell>QU _KCore</cell><cell>0.5918</cell><cell>2 nd</cell></row><row><cell>QU _KTruss</cell><cell>0.5807</cell><cell>3 rd</cell></row><row><cell>QU _KCore_F</cell><cell>0.5723</cell><cell>7 th</cell></row><row><cell>QU _KTruss_F</cell><cell>0.5689</cell><cell>9 th</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,56.72,694.38,94.99,6.18"><p>https://trec.nist.gov/data/wapost/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,56.84,702.79,71.10,6.18"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,321.00,702.79,49.42,6.18"><p>https://jsoup.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was made possible by NPRP grant# <rs type="grantNumber">NPRP 11S-1204-170060</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of <rs type="funder">Qatar Foundation</rs>). The statements made herein are solely the responsibility of the authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MgFgNFa">
					<idno type="grant-number">NPRP 11S-1204-170060</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,333.39,471.08,224.81,6.18;4,333.39,479.00,224.81,6.23;4,333.39,486.97,143.91,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,456.49,471.08,101.71,6.18;4,333.39,479.05,162.08,6.18">Identifying and ranking influential spreaders in complex networks by neighborhood coreness</title>
		<author>
			<persName coords=""><forename type="first">Joonhyun</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sangwook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,500.83,479.00,57.38,6.23;4,333.39,486.97,85.32,6.23">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="page" from="549" to="559" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,494.99,224.81,6.18;4,333.39,502.96,225.88,6.18;4,333.39,510.93,141.13,6.18" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Agra</forename><surname>Bimantara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michelle</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Gerwert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Gottschalk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Lukosz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shenna</forename><surname>Piri</surname></persName>
		</author>
		<title level="m" coord="4,447.72,502.96,111.55,6.18;4,333.39,510.93,116.97,6.18">Nima Saken Shaft, and Klaus Berberich. 2018. htw saar @ TREC 2018 News Track</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,518.90,225.58,6.18;4,333.39,526.87,225.99,6.18;4,333.39,534.84,91.60,6.18" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,434.21,526.87,125.17,6.18;4,333.39,534.84,67.37,6.18">Using clustering to filter results of an Information Retrieval system</title>
		<author>
			<persName coords=""><forename type="first">Pilar</forename><surname>Lopez-Ubeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Diaz-Galiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teresa</forename><forename type="middle">Martin</forename><surname>Valdivia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Alfonso Urena-Lopez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,542.81,225.99,6.18;4,333.39,550.78,91.61,6.18" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,424.58,542.81,134.80,6.18;4,333.39,550.78,67.80,6.18">Paragraph as Lead -Finding Background Documents for News Articles</title>
		<author>
			<persName coords=""><forename type="first">Kuang</forename><surname>Lua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,558.75,224.81,6.18;4,333.39,566.68,224.81,6.23;4,333.39,574.65,123.15,6.23" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,489.62,558.75,68.58,6.18;4,333.39,566.73,156.29,6.18">Main core retention on graph-of-words for single-document keyword extraction</title>
		<author>
			<persName coords=""><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,501.28,566.68,56.92,6.23;4,333.39,574.65,66.66,6.23">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="382" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,582.67,225.99,6.18;4,333.39,590.64,224.81,6.18;4,333.15,598.61,226.23,6.18;4,333.39,606.53,204.74,6.23" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Guokan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wensi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zekun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Lorré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05271</idno>
		<title level="m" coord="4,519.91,590.64,38.29,6.18;4,333.15,598.61,226.23,6.18;4,333.39,606.58,91.11,6.18">Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,614.55,225.88,6.18;4,333.39,622.52,224.81,6.18;4,333.39,630.44,224.81,6.23;4,333.39,638.41,119.65,6.23" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,333.39,622.52,224.81,6.18;4,333.39,630.49,97.36,6.18">Fusing document, collection and label graph-based representations with word embeddings for text classification</title>
		<author>
			<persName coords=""><forename type="first">Konstantinos</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fragkiskos</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,443.58,630.44,114.62,6.23;4,333.39,638.41,80.50,6.23">NAACL-HLT Workshop on Graph-Based Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,646.43,224.99,6.18;4,333.39,654.40,50.12,6.18" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="4,493.08,646.43,65.30,6.18;4,333.39,654.40,25.73,6.18">TREC 2018 News Track Overview</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,662.37,224.81,6.18;4,333.39,670.29,224.81,6.23;4,333.39,678.26,216.46,6.23" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,534.90,662.37,23.30,6.18;4,333.39,670.34,146.41,6.18">A graph degeneracy-based approach to keyword extraction</title>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fragkiskos</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,492.58,670.29,65.63,6.23;4,333.39,678.26,180.70,6.23">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1860" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,686.28,224.81,6.18;4,333.39,694.25,88.52,6.18" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="4,431.99,686.28,126.21,6.18;4,333.39,694.25,64.41,6.18">Anserini at TREC 2018: CENTRE, Common Core, and News Tracks</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
