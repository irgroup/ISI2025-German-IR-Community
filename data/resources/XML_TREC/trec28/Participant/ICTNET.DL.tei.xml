<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.15,84.23,338.20,15.44">ICTNET at TREC 2019 Deep Learning Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.83,114.37,63.57,10.60"><forename type="first">Jiangui</forename><surname>Chen</surname></persName>
							<email>chenjiangui18z@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.68,114.37,63.75,10.60"><forename type="first">Yinqiong</forename><surname>Cai</surname></persName>
							<email>caiyinqiong18s@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.71,114.37,72.60,10.60"><forename type="first">Haoquan</forename><surname>Jiang</surname></persName>
							<email>jianghaoquan18g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.15,84.23,338.20,15.44">ICTNET at TREC 2019 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6CA8EE9B2C3299218104FC716B197CC8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the Deep Learning Track at TREC 2019. We built a ranking system which combines a search component based on BM25 and a semantic matching component using pretraining knowledge. Our best run achieves NDCG@10 of 0.6650, MAP of 0.2035.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Deep Learning Track aims to find the most relevant text for a given query. Normally, users send a query to the system, and the system should be able to retrieval a considerable number of documents involved with the query. After which, various methods are implemented to determine the final best answer.</p><p>Passage ranking can be seen as a text matching problem. Text matching is essential in information retrieval and natural language processing. A large majority of tasks can be formalized as a matching problem. In question answering, we need to match questions with answers. In ad-hoc retrieval, we are required to find relevant documents for a query. And in natural language inference, we expected to discover the relationship between premises and hypothesis.</p><p>Most passage ranking task has a full ranking and re-ranking subtasks. Generally, full ranking consists of two steps. The first step is to narrow the search space when concerning with efficiency. And the next step is required to dig the deep semantic information of queries and passages. Traditional methods largely rely on hand-craft features, which is inadequate and time-consuming. With the blooming of deep learning technology, a wide range of neural ranking models are applied to tackle the matching problems of queries and passages <ref type="bibr" coords="1,127.90,514.08,11.09,7.95" target="#b4">[5]</ref>.</p><p>In this paper, we propose a ranking system which combines a search component based on BM25 and a semantic matching component using pretraining knowledge. Given a query Q, The search component is responsible to return relatively relevant passages. Based on the passages retrieved by search component, semantic matching component models the matching information between the query and passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEM DESCRIPTION 2.1 Overview</head><p>As Figure1 shows, given query Q, we first retrieve top 1000 relevant passages from the extended passage collection through Search Component. Then we use Semantic Matching Component to rerank the 1000 relevant passages. By comparing the final top 10 relevant passages and ground truth, we can get the evaluation metrics on query Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Component</head><p>Inspired by <ref type="bibr" coords="1,363.20,185.26,9.52,7.95" target="#b8">[9]</ref>, we trained a model that predicts corresponding queries for passages and then expand the passages with those predictions. After enriching passages with extra information, we then index them. The search strategy is rough BM25 <ref type="bibr" coords="1,492.03,218.13,13.31,7.95" target="#b11">[12]</ref>. Results show that the performance (MRR) of BM25 can improve over 0.03 by expanding passages.</p><p>Concretely, we processed training data to construct query-passage pairs. The passage is the corresponding positive examples of the query. Then, we used these pairs to train a machine translation model with the help of OpenNMT <ref type="bibr" coords="1,442.34,283.89,10.43,7.95" target="#b5">[6]</ref> <ref type="foot" coords="1,452.76,281.74,3.38,6.45" target="#foot_0">1</ref> , in which passage is viewed as source sequence while query is viewed as target sequence. The machine translation model uses a transformer-based encoder-decoder architecture. The encoder processes source sequence to a sequence of hidden vectors and the decoder combines hidden representations of previously generated words with source hidden vectors to predict the probability distribution of next word. The encoder and decoder are composed of six layers of transformer respectively.</p><p>After training the machine translation model, we utilized it to predict queries for each passage in the collection. In the test-time, decoding is done through beam search where multiple hypothesis target predictions are considered at each time step. Eventually we can get several predicted queries for each passage, then we concatenate its predicted queries after each passage.</p><p>Subsequently, we index these passages using Anserini <ref type="bibr" coords="1,521.15,437.31,13.22,7.95" target="#b13">[14]</ref>. Then BM25 is used to retrieve top 1000 relevant passages for each query in training and development dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Matching Component</head><p>The semantic matching component aims to re-rank the documents retrieved by the search component. We have seen a rapid growth of pretrain neural models recently, such as ELMo <ref type="bibr" coords="1,503.79,516.08,17.36,7.95" target="#b9">[10]</ref>, OpenAI GPT <ref type="bibr" coords="1,330.16,527.04,16.27,7.95" target="#b10">[11]</ref>, BERT <ref type="bibr" coords="1,370.00,527.04,13.24,7.95" target="#b1">[2]</ref> and XLNet <ref type="bibr" coords="1,420.06,527.04,15.92,7.95" target="#b14">[15]</ref>. Models pretrained on a language modeling task achieve promising results on various natural language tasks. To leverage the ability of BERT, we introduced it as the base unit of our semantic matching component.</p><p>Given a query Q consisting of m tokens, we denotes it as Q = q 1 q 2 . . . q m , where q i is the i-th tokens of Q. Similarly, a passage P can be denotes as P = p 1 p 2 . . . p n , where p i is the i-th tokens of P and n is the number of tokens of the passage.</p><p>The semantic matching component that we integrated has two types, which are BERT and Conv-KNRM. <ref type="bibr" coords="1,459.58,642.17,9.52,7.95" target="#b1">[2]</ref>, which apples the bidirectional of Transformer <ref type="bibr" coords="1,409.10,653.13,16.48,7.95" target="#b12">[13]</ref> to train language model. BERT obtains new state-of-the-art results on a diversity of natural language processing tasks. The pretrained BERT model can be used to create state-of-the-art models for a wide range of task by only fine- Inspired by <ref type="bibr" coords="2,107.20,260.95,9.39,7.95" target="#b7">[8]</ref>, we used BERT as re-ranker to leverage the substantial ability of BERT. The query Q was marked as sentence A, and the passage P was marked as sentence B. Following the original implementation, we then fed the concatenate representation into BERT. We use BERT-base model as a regression task. It uses the representation of '[CLS]' in last layer as the matching features and combines them linearly with weight w. The ranking score from BERT includes all term pair interactions between the query and passage via the cross-match attentions of transformer. Thus it is an interaction-based matching model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">BERT. BERT was proposed by</head><p>For each query and passage, BERT would eventually output a value denoting the matching score. Hence we obtain:</p><formula xml:id="formula_0" coords="2,139.54,404.83,154.51,9.78">s i j = BERT (Q i , P j )<label>(1)</label></formula><p>For a query Q i , which has k relevant passages. For each passage with respect to Q i , we gained a list of scores, [s i1 , s i2 , . . . , s ik ]. The top-1 passage was selected by sorting scores in descent order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Conv-KNRM.</head><p>Conv-KNRM, which stands for a Convolutional Kernel-based Neural Ranking Model, is a state-of-the-are neural ranking model. It was proposed by <ref type="bibr" coords="2,205.30,482.31,9.27,7.95" target="#b0">[1]</ref>. Conv-KNRM utilizes the ability of convolutional neural networks to represent n-grams of various lengths and soft matches them in a unified embedding space. Before generating the final score of each query and passage pair, kernel pooling and learning-to-rank layers are applied by feeding the n-gram soft matches signals. The architecture of Conv-KNRM is showed in Figure <ref type="figure" coords="2,127.29,548.06,3.07,7.95">2</ref>.</p><p>Query Q and passage P were first fed into embedding layer, which maps tokens of Q i , i.e. [q 1 , q 2 , . . . , q m ], and tokens of P i , i.e.[p 1 , p 2 , . . . , p n ] into distributed representations. Convolutional layer was followed by embedding layer to generate n-grams embedding, which takes multiple tokens into account. The key to advantages of Conv-KNRM is to use cross-matching n-grams. The query n-grams and passage n-grams of various lengths, which capture different granularity of semantic information different, were matched by cross-match layer. Translation matrices produced by cross-match layer were fed into kernel pooling layer to generate soft-TF features. In the end, learning-to-rank layer combines the highly extracted signals to generate ranking score.</p><p>Identical to Section 2.3.1, Conv-KNRM produces a list of ranking scores. After sorting scores, we obtain the highest relevant passages.</p><p>We train BERT and Conv-KNRM by using cross-entropy loss:</p><formula xml:id="formula_1" coords="2,365.78,260.80,192.42,19.92">L = - j ‚ààS pos log(s j ) - j ‚ààS ne–¥ log(1 -s j ) (2)</formula><p>where S pos is the set of relevant passage with respect to query, while S ne–¥ is the set of non-relevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENT 3.1 Experimental Setup</head><p>We conducted a series of experiments on TREC-Deep Learning passage ranking datasets.</p><p>Data. The passage ranking dataset is built using technology and data from Microsoft's Bing <ref type="bibr" coords="2,412.72,380.31,10.98,7.95" target="#b6">[7]</ref>. Its passage collection includes 8.8M passages extracted from 3.5M URLs. There are 1,010,916 unique real queries that were generated by sampling and anonymizing Bing usage logs. These queries are divided into three parts for train, dev and test, as Table <ref type="table" coords="2,383.78,424.15,4.23,7.95" target="#tab_0">1</ref> shows. It also provides qrels files which has a QID to PID mapping of when a question has had a passage marked as relevant. The average length of queries and passages is 5.9 and 57. <ref type="bibr" coords="2,328.82,457.03,3.62,7.95" target="#b6">7</ref>  Evaluate Metrics. We adopt four evaluation measures, i.e. Precision (P), Recall (R), MAP and NDCG, to evluate the performance of our proposed system.</p><p>Experimental Details.</p><p>We first used BM25 to retrieve top 1000 relevant passages for each query in train dataset. The parameter of BM25 is set to b1=0.8, k=0.6. At training time, we constructed the training set in the following way. The passages from qrels file were positive samples and BERT2 applies BERT on the concatenated query and passage sequence. The length of query and passage is fixed as 20 and 150 respectively. BERT is fine-tuned from the pretrained BERT-BASE model released by Google. AdamW optimizer is applied to optimize the model, with 5e-5 of learning rate, 0.9 and 0.98 of betas. The warmup step is set to 6 during each epoch and batch size was 16.</p><formula xml:id="formula_2" coords="3,159.25,216.47,209.68,80.95">0.2, ‚Ä¶ 0.1, ‚Ä¶ 0.5, ‚Ä¶ 0.4, ‚Ä¶ -0.1, ‚Ä¶ 0.1, ‚Ä¶ 0.8, ‚Ä¶ hamlet best lines to be or not ùëî ‚Éó # # ‚Ä¶ ùëî ‚Éó $ # ùëî ‚Éó # % ‚Ä¶ ùëî ‚Éó $ % ùê∫ ' # (unigrams) ùê∫ ' % (bigrams) ùëî ‚Éó # # ‚Ä¶ ùëî ‚Éó ( # ‚Ä¶ ùëî ‚Éó # % ‚Ä¶ ùëî ‚Éó ( % ‚Ä¶ ùê∫ ' # (unigrams) ùê∫ ' %<label>(bigrams)</label></formula><p>CKNRM_B uses the sequence output of last layer in BERT as input. BERT was fine-tuned with Conv-KNRM end-to-end. For Conv-KNRM, max n-gram is 3, the number of Gaussian kernels is 11 and we use 128 filters in each convolution layer. The model is trained using Adam optimizer on a typical GPU, and batch size is 64. Initial learning rate is set to 1e-3. The learning rate decays with factor 0.9 every three epochs.</p><p>CKNRM_B50 uses the same model as CKNRM_B, but only utilizes top 50 relevant passages of BM25 at prediction time. Table <ref type="table" coords="3,350.62,489.64,4.25,7.95" target="#tab_2">2</ref> shows the results of our submitted runs. As shown in the table, BERT2 achieves best results although we only added a learning-to-rank layer and fine-tuned on it. This demonstrates that the effectiveness and robustness of pretraining models.</p><p>When combining pretraining language models like BERT and traditional neural ranking model Conv-KNRM, we can see a slight decrease in the evaluation results. This is because the representation produced by BERT is already highly extracted, which includes the semantic information of two original texts and the interactions between them. When feeding them into Conv-KNRM, the duplicate operations of Conv-KNRM may disturb the information that BERT has distilled previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>We participated in the Deep Learning Track at TREC 2019. We proposed a ranking system which combines a search component based on BM25 and a semantic matching component using pretraining knowledge. The search component narrows down searching space for a given query, while the semantic matching component models the matching signals. Future work includes introducing external knowledge to help modeling the matching signals between query and passages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,228.39,211.07,155.22,7.70;2,135.92,83.69,340.16,113.39"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of Ranking System</figDesc><graphic coords="2,135.92,83.69,340.16,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,334.68,457.03,173.71,138.53"><head>Table 1 :</head><label>1</label><figDesc>Passage ranking dataset</figDesc><table coords="2,334.68,457.03,173.71,124.31"><row><cell>respectively.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>#</cell><cell>#Sample</cell></row><row><cell>collection</cell><cell>total</cell><cell>8841823</cell></row><row><cell>4*query</cell><cell>total</cell><cell>1010916</cell></row><row><cell></cell><cell>train</cell><cell>808731</cell></row><row><cell></cell><cell>dev</cell><cell>101093</cell></row><row><cell></cell><cell>test</cell><cell>101092</cell></row><row><cell>2*qrel</cell><cell>train</cell><cell>532761</cell></row><row><cell></cell><cell>dev</cell><cell>59273</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.80,352.49,504.40,356.48"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for runs based on automatic judgements</figDesc><table coords="3,53.80,701.12,155.08,7.85"><row><cell>2 https://github.com/NTMC-Community/MatchZoo-py</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,320.88,702.79,126.84,6.19"><p>https://github.com/OpenNMT/OpenNMT-py</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,69.23,131.73,224.81,6.19;4,69.23,139.03,224.81,6.97;4,69.23,147.00,225.58,6.97;4,69.07,155.64,24.81,6.19" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,254.17,131.73,39.87,6.19;4,69.23,139.70,172.79,6.19">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,254.30,139.03,39.75,6.97;4,69.23,147.00,204.25,6.97">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,69.23,163.61,225.63,6.19;4,69.23,170.91,224.81,6.97;4,69.23,178.88,91.11,6.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,281.34,163.61,13.53,6.19;4,69.23,171.58,205.00,6.19">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,69.23,187.52,225.88,6.19;4,69.23,194.82,223.54,6.97" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianpeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07270</idno>
		<title level="m" coord="4,115.43,195.49,86.15,6.19">A toolkit for deep text matching</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,69.23,203.46,225.58,6.19;4,69.23,210.76,224.81,6.97;4,69.23,218.73,224.81,6.97;4,69.23,226.70,225.63,6.97;4,69.01,235.34,97.00,6.19" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,230.24,203.46,64.57,6.19;4,69.23,211.43,172.71,6.19">MatchZoo: A Learning, Practicing, and Developing System for Neural Text Matching</title>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331403</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331403" />
	</analytic>
	<monogr>
		<title level="m" coord="4,254.52,210.76,39.52,6.97;4,69.23,218.73,224.81,6.97;4,69.23,226.70,89.22,6.97">Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)</title>
		<meeting>the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1297" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,69.23,243.31,224.81,6.19;4,68.90,251.28,225.14,6.19;4,69.23,258.58,204.03,6.97" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06902</idno>
		<title level="m" coord="4,200.44,251.28,93.60,6.19;4,69.23,259.25,90.70,6.19">A deep look into neural ranking models for information retrieval</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,69.23,267.22,225.88,6.19;4,69.23,275.19,224.81,6.19;4,69.23,282.49,137.68,6.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,103.30,275.19,180.44,6.19">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-4012</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-4012" />
	</analytic>
	<monogr>
		<title level="m" coord="4,69.23,282.49,25.46,6.97">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,69.23,291.13,224.81,6.19;4,69.23,299.10,224.81,6.19;4,333.39,89.09,96.35,6.19" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="4,163.01,299.10,131.03,6.19;4,333.39,89.09,93.47,6.19">MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,97.06,225.88,6.19;4,333.39,104.36,108.33,6.97" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="4,467.87,97.06,87.86,6.19">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,113.00,224.81,6.19;4,333.39,120.30,201.70,6.97" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<title level="m" coord="4,528.69,113.00,29.51,6.19;4,333.39,120.97,88.06,6.19">Document Expansion by Query Prediction</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,128.94,224.94,6.19;4,333.39,136.91,224.81,6.19;4,333.39,144.21,156.42,6.97" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m" coord="4,480.78,136.91,77.42,6.19;4,333.39,144.88,42.73,6.19">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,152.85,224.81,6.19;4,333.39,160.82,50.92,6.19;4,404.69,160.82,154.69,6.19;4,333.39,168.79,53.34,6.19;4,404.02,168.12,154.88,6.97;4,333.39,176.09,225.57,6.97;4,333.39,184.06,30.92,6.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="4,404.69,160.82,154.69,6.19;4,333.39,168.79,50.51,6.19">Improving language understanding by generative pre-training</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,192.70,224.81,6.19;4,333.39,200.00,224.81,6.97;4,333.39,208.64,56.72,6.19" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="4,478.66,192.70,79.54,6.19;4,333.39,200.67,82.93,6.19">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,422.00,200.00,136.20,6.97">Foundations and Trends¬Æ in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,216.61,225.58,6.19;4,333.15,224.58,225.06,6.19;4,333.39,231.88,212.44,6.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="4,513.37,224.58,44.83,6.19;4,333.39,232.55,24.64,6.19">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,370.40,231.88,139.53,6.97">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,240.52,224.81,6.19;4,333.39,247.82,224.81,6.97;4,333.39,255.79,225.58,6.97;4,333.23,264.43,31.30,6.19" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="4,456.35,240.52,101.85,6.19;4,333.39,248.49,94.71,6.19">Anserini: Enabling the use of Lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,440.34,247.82,117.87,6.97;4,333.39,255.79,204.15,6.97">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,272.40,225.58,6.19;4,333.39,280.37,225.99,6.19;4,333.39,287.67,173.64,6.97" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m" coord="4,397.18,280.37,162.20,6.19;4,333.39,288.34,59.34,6.19">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
