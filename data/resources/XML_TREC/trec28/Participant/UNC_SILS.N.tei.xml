<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.78,115.90,266.22,12.68">UNC SILS at TREC 2019 News Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,248.27,153.69,51.04,8.80"><forename type="first">Jiaming</forename><surname>Qu</surname></persName>
							<email>jiaming@ad.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel</orgName>
								<address>
									<addrLine>Hill Chapel Hill</addrLine>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.01,153.69,44.82,8.80"><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<email>wangyue@email.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel</orgName>
								<address>
									<addrLine>Hill Chapel Hill</addrLine>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.78,115.90,266.22,12.68">UNC SILS at TREC 2019 News Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F9F7E1A174829982C0A0008900E5B57A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the Background Linking task of TREC 2019 News Track. Our approach has two directions. After processing the corpus, we use Lucene to index and run the initial retrieval. Then we leverage the learning-to-rank idea to train a re-ranker using the 2018 relevance judgement files as ground truth, and the reranker is applied to the initial retrieval results to generate a new ranked list. Experiment results prove that the re-ranker significantly improves the retrieval performance (NDCG@5) compared to the initial retrieval results without the re-ranking step.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The News Track started in TREC 2018, and it is the second year for this track. This track consists of two tasks: Background Linking (BL) and Entity Ranking (ER). The BL task aims to find relevant news from the whole corpus given a target news, which provides background information or further understanding for readers. The ER task aims to rank the named entity in the news according to their importance in the context, which highlights the most significant entities.</p><p>The School of Information and Library Science (SILS) at the University of North Carolina at Chapel Hill (UNC) participated in the BL task in this year's News Track. In this paper, we discuss our strategy to tackle the problem. In section 2, we provide an overview of the dataset and how we process the text. In section 3, we demonstrate in detail our retrieval strategy, especially the feature generation to train the re-ranker. In section 4, we report the retrieval performances evaluated by TREC. In section 5, we summarize our work and propose potential improvements and directions for future BL tasks or relevant research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Processing</head><p>This year's News Track continues to use the Washington Post corpus as in 2018, which consists of more than 590,000 news articles published by The Washington Post. Even though TREC provides a version without duplication, we still notice that there remain some duplicate articles in the corpus. However, we do not remove duplicate articles as the first step of pre-processing, as recognizing duplicates for all articles in the corpus will involve significant computation. Since the main task is news retrieval, we leave news deduplication to the final final ranking step (Section 3.3).</p><p>First of all, we parse the original "JSON-line" format file into separate JSON files, with each file representing a piece of news in the corpus. Secondly, for each JSON file, we parse the file by extracting four fields which are ID, title, contents, and category, and discard other information. Note that the publishing date is no longer useful, as is officially stated in last year's overview <ref type="bibr" coords="2,377.82,214.58,10.40,8.80" target="#b0">[1]</ref> that we are assuming the user is reading the news now, irrespective of when the news was published.</p><p>When parsing the news content which is a list of paragraphs, in last year's News Track, Lu and Fang <ref type="bibr" coords="2,257.04,250.44,10.63,8.80" target="#b1">[2]</ref> propose treating each paragraph separately to generate keywords for retrieval. However, we simply concatenate each paragraph and treat the merged text as news content. When processing news titles and contents, we remove HTML tags, punctuation and stop words. During processing, we also remove news with types of "Opinion", "Letters to the Editor", or "The Post's View", for they are identified as irrelevant to the task <ref type="bibr" coords="2,407.36,310.22,10.08,8.80" target="#b0">[1]</ref>, which finally leaves 571,963 news in the corpus.</p><p>Since the BL task aims to find relevant news given a target one, we assume that relevance judgement should be not only on the text level, but also on the context level. We do not take the Deep Learning approach to learn the contexts in this task, instead we simply assume that two news are more likely to be contextually relevant if they share more named entities in common. Therefore, before cleaning the text as mentioned above, we use Spacy <ref type="bibr" coords="2,399.61,393.90,10.08,8.80" target="#b2">[3]</ref>, which is a free open-source library to do the Named Entity Recognition (NER) task to recognize entities from the news titles and contents. We discard eight kinds of entities which are "LANGUAGE", "DATE", "TIME", "PERCENT", "MONEY", "QUANTITY", "ORDINAL" and "CARDINAL", as we identify these entities, e.g., "May 1st", "250 million US dollars", "154" are not helpful to represent the context. We kept the rest kinds of entities, such as "ORG", "PERSON", "EVENT", and etc<ref type="foot" coords="2,434.22,464.08,3.97,6.16" target="#foot_0">1</ref> .</p><p>To sum up, we parse the Washington Post corpus by dividing it into separate JSON files, extracting four useful fields, then recognizing named entities from titles and contents with Spacy, finally cleaning the text and removing irrelevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we provide detailed demonstration of our approach and intuitions behind, as is shown in Figure <ref type="figure" coords="2,264.67,579.20,3.83,8.80">1</ref>. In general, we use Apache Lucene 2 , which is an open-source, high-performance and publicly-available searching engine toolkit in Java to index the corpus and run the initial searching which retrieves 1,000 documents per query. Then we generate features for each query, document pairs, and leverage the 2018 BL task relevance judgements to train a re-ranker. Finally for the 2019 topics, we initially retrieve 1,000 documents for each query and re-rank the result using the trained re-ranker. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Retrieval Results</head><p>Re-ranker 2019 Queries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Retrieval Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Generation 2019 Retrieval Results</head><p>Fig. <ref type="figure" coords="3,272.29,532.34,4.13,7.93">1</ref>. General Framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing</head><p>For each news article, four fields are extracted as mentioned above, which are title, content, ID and category. For titles and contents, we use the standard analyzer built in Lucene to analyze and stem the text. We do not process ID but simply treat it as identifiers, and the category field is not used at this step but at re-ranking instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initial Retrieval</head><p>One problem during the initial retrieval is how to generate the query. Instead of throwing in the whole content as an extremely long query, we extract top-K keywords with the highest TF-IDF weights as query terms. We then tune the parameter K using the 2018 data and find K=80 gives the highest NDCG@5 score. When run the search in Lucene, we use the OR operator between all the terms, and use the SHOULD operator for both the title and content field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Re-ranking</head><p>To re-rank the initial retrieval results, we train a re-ranker based on the 2018 relevance judgements. This is a ranking problem because each query, document pair will have a relevance score to be sorted. We transform this ranking problem to a multi-class classification problem <ref type="bibr" coords="4,301.52,287.58,9.95,8.80" target="#b3">[4]</ref>, and calculate the score of each querydocument pair (q, d) as:</p><formula xml:id="formula_0" coords="4,214.68,334.07,265.91,20.59">score(q, d) = r∈{0,2,4,8,16} w r • p θ (y = r|q, d)<label>(1)</label></formula><p>where y is predicted relevance grade using a multiclass classifier p θ (•|q, d). The idea is that we treat relevance grades r = 16, 8, 4, 2, 0 as class labels without order, instead of its original representing relevance order. w r are weights for each relevance grade r in the scoring function; higher weights should be assigned to higher relevance grade. Here we set w 0 = 0, w 2 = 1, w 4 = 2, w 8 = 3, w 16 = 4.</p><p>The formula shows that we sum up the product of a weight of a relevance grade and the probability of that relevance grade. Afterwards, we train an one-vs-all Logistic Regression classifier to do the multi-class classification.</p><p>We generate five features to train the classifier, which are similarity of title, similarity of content, similarity of the first 100 words, similarity of mentions, similarity of category. Below are detailed demonstration of these features.</p><p>similarity of title We construct a corpus of all the news titles, then vectorize each instance into TF-IDF vectors and measure similarity of title by the Cosine similarity between two vectors. similarity of content We construct a corpus of all the news contents, then vectorize ach instance into TF-IDF vectors and measure similarity of content by the Cosine similarity between two vectors.</p><p>similarity of the first 100 words We construct a corpus of all the news' first 100 words, then vectorize ach instance into TF-IDF vectors and measure similarity of the first 100 words by the Cosine similarity between two vectors. The reason of using the first 100 words is that we assume the introduction part of a news has covered the main idea or main entities in the content.</p><p>similarity of mentions As is mentioned in Section 2, for each news we use Spacy to extract meaningful entities. Then for each pair of news, we measure similarity of mentions by the Jaccard similarity between two entity sets. similarity of category In the original JSON-line file, some news have a sub-field in its content which indicates the category it belongs to, e.g., "Europe", "Technology", "Sports", "Movies" and etc. There are 179 categories in total, but some news do not have such a sub-filed. To obtain a probability distribution of categories as well as to fill missing categories for each news, we generate text features from news contents, and use the basic bag-of-words model with unigrams to train a multi-class classifier, by which we predict the probability distribution of categories for each news. We measure the similarity of category between two news articles d 1 , d 2 by the total variation of the two predicted news category distributions:</p><formula xml:id="formula_1" coords="5,223.94,247.93,252.41,26.80">δ(d 1 , d 2 ) = 1 2 • c∈C |p φ (c|d 1 ) -p φ (c|d 2 )| (<label>2</label></formula><formula xml:id="formula_2" coords="5,476.35,254.61,4.24,8.80">)</formula><p>where C is the set of all news categories; p φ (c|d) is the predicted probability of article d having category c; φ is the parameter of the multiclass classifier. The formula shows that we sum up the absolute value of element-wise differences, and divide the sum by 2. One nice property of this metric is that it is bounded in [0, 1], which is on the same scale as the cosine similarity and Jaccard similarity. Note that since it actually measures the distance, thus the higher the total variation is, the lower the similarity, which is in contrast of the cosine and Jaccard similarity. We generate these five features for each query, document pair from the initial retrieval, and use the ground truth from 2018 as labels to train a multi-class classifier. We apply the same approach on the 2019 retrieval results, which could be viewed as a test set. Based on the five features generated, the re-ranker predicts a probability distribution which is converted to a re-ranking score. We test three methods using the new (re-ranking) score to re-rank. The first is purely based on the new score. The second is to normalize the raw score from Lucene and the re-ranking score separately using the min-max scaler to get a number between 0 and 1, and then add up two scores as a final score. The third is to assign different weights to the raw and re-ranking scores when adding up, and based on the ground truth from 2018, we find the optimal weight is 9 for the raw and 1 for the new. The evaluations of these three methods are shown in Section 4.</p><p>Another important contribution of similarity of title, similarity of content, similarity of the first 100 words is to find duplicate news. As is discussed in Section 2, there are some duplicate news in the corpus, but it is impossible to check every pair to find duplicate ones. Thus, at the re-ranking step, we identify a retrieved news is a duplicate to the target news if any of similarity of title, similarity of content, similarity of the first 100 words, or similarity of mentions is 1, as the first row shown in Figure <ref type="figure" coords="5,295.58,583.83,3.80,8.80">2</ref>. Then we simply give a final score of 0 to a duplicate news.</p><p>The coefficients in the multi-class Logistic Regression are shown in Figure <ref type="figure" coords="5,461.50,608.24,3.80,8.80" target="#fig_1">3</ref>. In general, we could see that similarity of content is the most important feature for each label, but different labels have a different order of feature importance. Take two extremes as an example, we can see that content similarity and first 100 words similarity features positively correlate with being perfectly relevant (label Fig. <ref type="figure" coords="6,283.98,189.91,4.13,7.93">2</ref>. Feature Table <ref type="table" coords="6,134.77,224.27,18.32,8.80">= 16</ref>), while negatively correlate with being non-relevant (label = 0), which makes sense. However we found that not all learned coefficients are easily interpretable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We submit 4 runs for the BL task and the evaluation results are summarized in the table below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Evaluation Results</head><p>Run Name NDCG@5 sils_news_run1 0.3482 sils_news_run2 0.5502 sils_news_run3 0.5472 sils_news_run4 0.3805 sils_news_run1 is the baseline which only uses the raw BM25 score from Lucene to rank the retrieval result.</p><p>sils_news_run2 only uses the re-ranking score to rank the initial retrieval result.</p><p>sils_news_run3 uses both the raw BM25 score from Lucene and the re-ranking score by adding up to rank the initial retrieval result.</p><p>sils_news_run4 uses both the raw BM25 score from Lucene and the re-ranking score by adding up with weights to rank the initial retrieval result.</p><p>By comparing sils_news_run1 and sils_news_run2 which rank the retrieval results based on two different scores, we could see that the re-ranking score reflect the true ranking better than the raw BM25 score.</p><p>Also, by comparing sils_news_run2 and sils_news_run3 , we could see that adding the raw BM25 score to the re-ranking score does not differ much from only using the re-ranking score.</p><p>Finally, by comparing sils_news_run3 and sils_news_run4 , we could see that adding up two scores with different weights hurt the performance much, which is to our surprise because these weights improves NDCG@5 on the 2018 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We participate in the 2019 News track and submit 4 runs for the Background Linking task. Our best run incorporates two steps, which are the initial retrieval and re-ranking. We use Lucene to run the initial retrieval, and then leverage the learning-to-rank idea to train a classifier to re-rank the initial retrieval result. We convert the ranking problem to a classification problem and generate five features from both text and context. Results show that the learnt re-ranker improves performance in terms of NDCG@5 compared to the baseline model without re-ranking.</p><p>In future work, we aim to find a better approach to generate the query which maximize the performance at the initial retrieval, as the re-ranking will not be so effective if there are few relevant news in the initial retrieval result. Also, we aim to do feature engineering to generate better both high-level and query-specific features to improve the classifier. Finally, ablation studies can be performed to better understand the impact of different features in this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,398.29,345.82,7.93;6,134.77,409.25,82.73,7.92;6,134.77,268.37,345.82,115.18"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Learned coefficients for different similarity features in the 5 one-vs-all logistic regression classifiers.</figDesc><graphic coords="6,134.77,268.37,345.82,115.18" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.79,128.83,7.92"><p>https://spacy.io/api/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,656.74,103.94,7.92"><p>http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,578.96,344.03,7.92;7,146.68,589.91,20.99,7.92" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="7,361.47,578.96,116.84,7.92">Trec 2018 news track overview</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,601.41,342.25,7.92;7,146.91,612.37,99.63,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,249.86,601.41,230.74,7.92;7,146.91,612.37,27.82,7.92">Paragraph as lead-finding background documents for news articles</title>
		<author>
			<persName coords=""><forename type="first">Kuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,193.81,612.37,22.95,7.92">TREC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,623.87,342.24,7.92;7,146.91,634.83,333.68,7.92;7,146.32,645.79,336.06,7.92;7,146.57,656.74,172.59,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,299.89,623.87,180.70,7.92;7,146.91,634.83,89.49,7.92">An improved non-monotonic transition system for dependency parsing</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,254.55,634.83,226.04,7.92;7,146.32,645.79,122.57,7.92">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="1373" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,119.62,342.24,7.92;8,146.91,130.58,333.68,7.92;8,146.91,141.54,163.59,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,347.90,119.62,132.69,7.92;8,146.91,130.58,181.65,7.92">Mcrank: Learning to rank using multiple classification and gradient boosting</title>
		<author>
			<persName coords=""><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,350.11,130.58,130.48,7.92;8,146.91,141.54,72.38,7.92">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
