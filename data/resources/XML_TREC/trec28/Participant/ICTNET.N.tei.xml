<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,186.37,107.03,222.49,12.90">ICTNET at TREC 2019 News Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,56.09,147.01,63.42,10.37"><forename type="first">Yuyang</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology {DingYuyang18s</orgName>
								<orgName type="laboratory">.CAS Key Lab of Network Data Science and Technology</orgName>
								<address>
									<addrLine>LianXiaoying18s</addrLine>
									<postCode>ZhouHouquan18s</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,136.08,146.63,74.66,10.48"><surname>Xiaoyinglian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology {DingYuyang18s</orgName>
								<orgName type="laboratory">.CAS Key Lab of Network Data Science and Technology</orgName>
								<address>
									<addrLine>LianXiaoying18s</addrLine>
									<postCode>ZhouHouquan18s</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,186.37,107.03,222.49,12.90">ICTNET at TREC 2019 News Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B575B5C0DA62D20F80D690E26C6192F8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our work in the background linking task and entity ranking task in TREC 2018 News Track. We explore four methods in background linking task and two methods in entity ranking task. All of our methods largely rely on off-the-shell open-source components(e.g., Solr for indexing the documents), and follow the basic thoughts-BM25 and relevance feedback. These methods differ in how they analyze the given input to obtain a query and to what extent the returned results are re-ranked taking meta data of the document(e.g., publish dates) into account.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The News track focuses on information retrieval in the service of helping people read the news. In this TREC, there are two tasks-background linking and entity ranking, containing 600,000 news articles aiming to find how news is presented on the web.</p><p>The goal of the background linking task is to develop evaluation data to support researchers in developing systems that can help users contextualize news articles as they are reading them. For example, a user is reading a specific article (the query article), algorithms should recommend articles that this person should read next that are the most useful for providing context and background for the query article. It is reasonable to view this task as a specific kind of news recommendation task that would be useful in any news reading context, including the Posts website.</p><p>The second task, entity ranking, aims at separating important entities from non-important ones within an article. In addition to providing links to articles that give the reader background or contextual information, journalists sometimes link mentions of concepts, artifacts, and entities to internal or external pages with in depth information that will help the reader better understand the article. Given a news article with title and content that is annotated with entity links to a set of entities, we are supposed to rank the the given entities by importance for the article (i.e., saliency ).</p><p>To solve this two tasks, we follow the basic thoughts in information retrieval systems-relevance feedback and BM25. Also, we support information retrieval research using the popular open-source Solr search library which allows researchers to easily replicate results with modern ranking models on diverse test collections. These methods turn out to perform well. In our participation at the news track, we submitted four different runs: three for background linking and the last for entity ranking.</p><p>The rest of this paper is structured as follows. Section 2 and 3 present the basic frameworks of the two news tasks: background linking and entity ranking. Each section contains several subsections explain the data analysis, related methods, results and the final analysis. Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background Linking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Kicker</head><p>As mentioned in Guideline, articles from the "Opinion", "Letters to the Editor", or "The Post's View" sections labeled in the"kicker" field, are not relevant. In order not to miss information, we observe the distribution of documents according to the "kicker" filed. The results of the observation are shown in Figures 1.</p><p>There are a total of 184 different types of "kickers". Documents without "kicker" filed are also counted as one category named "None". The "kicker" field can recall up to 39,885 documents and 3,323 documents on average. The median number of recalled documents is 1585. The number of documents that can be recalled by the three types of kickers that need to be ignored is 23074.</p><p>Figure <ref type="figure" coords="1,350.79,644.74,4.98,8.64">1</ref> shows the distribution of documents recalled by the kicker field. The horizontal axis is the number of recalled documents, and the vertical axis is the number of "kicker" types. There are 22 types of "kicker" recalled documents below 10, and 90 types of "kicker" recalled between 1000 and 10,000. It can be seen that the number of docu-Figure <ref type="figure" coords="2,77.31,200.69,3.36,7.77">1</ref>. The distribution of documents recalled by the kicker field. The horizontal axis is the number of recalled documents, and the vertical axis is the number of "kicker" types. ments recalled by most "kicker" is huge, so the types of documents recalled by same "kicker" can be very different. Since there are many differences in documents belonging to the same "kicker", and documents belonging to different "kicker" also have many similar documents. Therefore, we believe that the information brought by the "kicker" field is too confusing to help retrieve the relevant documents. In the following work, the information of "kicker" field is abandoned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Methods</head><p>Background linking is similar to ad-hoc search task. We use the main article of one topic as query to create a candidate set of background documents with high retrieve scores. Unfortunately, we don't have enough labelled data. Therefore, the keys to retrieve better background documents are constructing a suitable query clause and making good use of output documents with high confidence from unsupervised method. Based on these motivations, here are our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">BM25 Baseline</head><p>BM25 <ref type="bibr" coords="2,77.93,524.56,11.62,8.64" target="#b4">[5]</ref> is one of the most popular and also the best models in the history of information retrieval. We use the default implementation of BM25 in solr to make our experiments. Different from ad-hoc search task with a short query as input, an article as query could have two defects. Firstly, the influence of some relevant words maybe impaired by a large set of irrelevant words. Secondly, the weights of the paragraphs and titles of articles should be set differently to simulate the attention of human readers.</p><p>As a consequence, we try a lot combinations of paragraphs and titles to make better query clause: only title, only first paragraph, title+first paragraph, concatenation of every paragraphs, first sentences, all paragraphs. We find that the best one was still the concatenation of all paragraphs. We also search optimal weights combinations and finally set the weight of content as 0.7 and the weight of title as 0.3.</p><p>The other methods are developed from this baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">BM25 with Rocchio</head><p>Except the dimension of query clause, we can also improve performance by making use of results of unsupervised method, which is also called pseudo relevance feedback. The main idea of relevance feedback is that a user can't fully express his intention on the first try. So we need to expand the inaccurate query to its accurate semantic meaning by user feedback. Specially, we don't have labelled feedback. So we need to do pseudo relevance feedback by the high-confidence subset of output documents from BM25.</p><p>In this method, we use the well-known Rocchio algorithm <ref type="bibr" coords="2,334.23,258.29,10.58,8.64" target="#b1">[2]</ref>. Rocchio algorithm is a combination of Vector Space Model(VSM) and pseudo relevance feedback model. We see query articles and candidate articles as tf-idf feature vectors. Then the task of retrieve is converted into calculating the similarity between two vectors. As figure <ref type="figure" coords="2,505.62,306.11,3.74,8.64">1</ref>, the distribution of feature vectors in space of relevant documents is different from irrelevant documents. So we can use them to modify the representation of query articles. The formula for Rocchio relevance feedback is as follows:</p><formula xml:id="formula_0" coords="2,308.86,600.98,236.26,40.44">q opt = (1-α-β) q org +α 1 |C r | dj ∈Cr d j -β 1 |C ir | dj ∈Cir d j (1)</formula><p>We use BM25 to produce top 100 candidate documents and select the top 20 relevant documents as C r and bottom 20 irrelevant documents as C ir to create a new query which is closer to the relevant and apart from the irrelevant. In the last, we use the new query to retrieve top 20 documents by BM25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">BM25 with Bert</head><p>Another way to make use of BM25 is to learn its pattern of rank of candidate documents. Although there are no labelled relevant document list data, we think it still meaningful to apply learning to rank method on high-confidence output of BM25.</p><p>Bert <ref type="bibr" coords="3,81.61,156.74,11.62,8.64" target="#b0">[1]</ref> is the state of art neural network model which is applied to almost all fields in NLP. So we apply pretrained Bert model on the top 10 documents from BM25. Specifically, we use some tricks to improve the result. First we select the first paragraph of query and then randomly select some other paragraphs to create a query. We do the same thing to create a candidate document. Then Bert model learns the score of the concatenation of the query and candidate. Second according to the rule of pair-wise learning to rank, we also create some "weak queries" which not include the most important first paragraph but include the other paragraphs and then pair them with the former "strong queries". Bert model learns from the pair and tries to make the weak ones score lower than the strong ones. These tricks help to create more data and build a more robust model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Query likelihood and Relevance Model 3</head><p>Query likelihood(QL) <ref type="bibr" coords="3,143.22,376.48,11.62,8.64" target="#b3">[4]</ref> is a well-know information retrieval method. Based on language models built from each document, QL ranks documents according to the likelihood of the given query.</p><p>QL is usually combined with relevance feedback approaches, especially RM3. RM3 is an variant of relevancebased language model(usually called RM) <ref type="bibr" coords="3,223.34,449.18,14.11,8.64" target="#b2">[3]</ref>, and most frequently-used in information retrieval tasks. RM assumes that there exists a relevant language model and builds a language model based on these results. The retrieval is achieved by standard QL language model and this auxiliary language model.</p><formula xml:id="formula_1" coords="3,56.35,534.14,230.01,20.68">P rm1 (t | q) ∝ D∈D R P (t | t) qi∈q P (q i | D)<label>(2)</label></formula><formula xml:id="formula_2" coords="3,56.35,560.40,230.01,20.91">P rm3 (t | q) = (1 -λ) • P M LE (t | q) + λ • P RM 1 (t | q)<label>(3)</label></formula><p>We test the standard QL model and QL+RM3 model. In the former, we use Jelinek-Mercer smoothing method in language model and apply the parameter λ to 0.7 for queries are long. In the latter, we use top-20 results returned by QL as relevance documents and apply the rm3 interpolation parameter to 0.4. We test constructing query in different ways. Since the title and the first paragraph usually contains useful information about the news, we try constructing query with title and title+first paragraph respectively. Furthermore, we make the full text as query as well. Table <ref type="table" coords="3,331.38,304.56,3.36,7.77">1</ref>. Results for all topics (median) and our four methods on the background linking task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Result</head><p>We report results of our runs in the background linking task in Table <ref type="table" coords="3,381.39,378.64,3.74,8.64">1</ref>. We observer that the BM25 baseline run performs best on nDCG@5 in total 60 topics, whereas the RM3 and Query likelihood respectively perform best on MAP and RPEC.</p><p>The precision-recall curves in Figure <ref type="figure" coords="3,476.93,427.14,4.98,8.64" target="#fig_2">4</ref> shows that the performance of RM3, Query likelihood and BM25 are nearly similar, and the Rocchio method where the pseudorelevance feedback is used based on BM25 significantly performs worse than the other methods. This indicates that the news takes a deep and complex knowledge relevance with its related background news. As a result of this, simply expanding query with similar semantic words cannot improve the performance of the query model.  and explain how pseudo-relevance feedback degrade performs of BM25, cases analysis was performed on the result for each topic. We list the topics where our methods gain highest and lowest nDCG@5 scores in Table <ref type="table" coords="4,252.89,278.73,3.74,8.64" target="#tab_0">2</ref>. It has been observed that Rocchio method where expand query for BM25 gains less score than the original BM25 on the topic 874. After reading the title and content of the article in the topic 874, we find that some metaphor are used in the title to describe the main idea of the article, i.e., the phrase "house flipping" to describe a housing exchanging. It is clear that the semantic of the word "flipping" has been changed greatly from its original definition when it combines with the word "house". However, when pseudorelevance feedback is used to expand the query which is based on the article title, it has large probability to add synonyms whose semantic is same as the original definition of the word "flipping" to the query. Hence, more articles which contains the word with its original semantic are retrieved to the results, and the performance of the Rocchio is definitely worse than the method without pseudo-relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Entity Linking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method 1</head><p>Entity linking task aims to sort entities in a news article according to their salience. Each entity has a wiki id. In this method, we relate each entity with the corresponding wiki page and regard the entity linking task as retrieval in these wiki pages. The returned ranking is used as the entity ranking.</p><p>Given the official wikidump data, we first build index on wiki pages of entities appeared in the dataset. It is efficient since we do not need to build index on all wiki pages. We consider the news article as "query" and wiki pages of entities as documents to be retrieved. Similar to background linking task, we try different query constructing methods including title, title+first paragraph and the full text. We use BM25 as our retrieval model and the parameters are tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M1</head><p>M2 nDCG@5 0.6800 0.7191 Table <ref type="table" coords="4,331.74,110.33,3.36,7.77" target="#tab_1">3</ref>. nDCG@5 for all topics (median) and our two methods (M1, M2) on the news track 2018 dataset</p><p>The retrieval results are filtered to make sure they only contains entities occured in this news article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Method 2</head><p>Method 2(M2) and Method 1(M1) have the same central idea. They all think that the more related to the query document the wiki page is, the more important is the entity in the query document which owns the wiki page. That is, if the wiki page owned by the entity is ranked higher in the result set returned by the query document, then the importance of the entity is higher, and the final score should be larger. The correspondence between the entity and the wiki page is determined by the "enwiki" owned by each wiki page, and a wiki page will contain multiple "enwiki".</p><p>M2 assumes that if the "enwiki" of an entity e i is in the "enwiki" of a wiki page W i , then considers that the wiki page W i corresponds to the entity e i . That is, the entity e i owns the wiki page W i , and the wiki page W i also points to the entity e i . The biggest difference between M2 and M1 is that each entity e i selects multiple related wiki pages W i to build an index. In a query, the final ranking rank i of each entity e i is only related to the ranking of the highest ranked wiki page W ibest in the query result set.</p><p>The specific process of M2 is as follows. First, the top M wiki pages of each entity e i are retrieved according to "enwiki", and the full-text index of the body of all wiki pages is constructed. At the same time, each wiki page W i uniquely points to an entity e i . The entire body concatenate with title of each document is used as the query. Then retrieve N related wiki pages in the full-text index constructed in the previous step as the result set R. Find the rank i of the wiki page W ibest in the result set R of the M wiki pages owned by each entity e i . Finally, the entity e i scores M -rank i in the document.</p><p>The pre-processing of Washington Post and Wiki dumps in M2 only includes uppercase to lowercase, eliminating stop words and stemming. By extending the wiki pages owned by each entity, M2 have become our best way to perform on entity ranking tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Result</head><p>Since at the time of writing, entity ranking runs for other participants are not yet available, we observed the performance of each method on the news track 2018 dataset, using nDCG@5 as the primary effectiveness measure.</p><p>The performance of each method is shown in proved the performance compared with M1. M2 mainly benefits from Pseudo relevance feedback, all the wiki pages are point to one entity, which intangibly enriches the information carried by each entity, so that the query can better match the most relevant aspects of the entities, thereby more accurately measuring the entity importance in the document.</p><p>To further observe how our method performs, we list some of the highest and lowest nDCG@5 scores in Table <ref type="table" coords="5,278.89,287.99,3.74,8.64" target="#tab_2">4</ref>. M1 and M2 performed poorly on both the 810 and 818 topics, and performed well on both the 433 and 802 topics. It has been observed that it is easy to obtain low scores when there is a majority of unrelated entities in a document, and it is easier to obtain high scores when most related entities are available. This is mainly because our method use rank to distinguish whether the entity and the topic are strongly or weakly related. An entity will always have its final rank whether it is related or not. As a result, our approach tends to think that entities are related to topic, so it performs well on topics with many related entities, but performs poorly on topics with few related entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this year's News Track, we take part in all of the two tasks. In the task of background linking, we investigate using paragraphs in different ways to construct our query. We use BM25 to retrieve document and combine Rocchio, Bert and other methods to enhance its performance. Results show that the performance on long text is unsatisfactory.</p><p>In the task of entity ranking, we follow the basic idea that the more related to the query document the wiki page is, the more important is the entity in the query document which owns the wiki page. We select multiple related wiki pages for each entity to build an index. It perform well on topics with many related entities while perform poorly on topics with few related entities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,376.18,545.15,101.62,7.77;2,339.49,364.03,175.00,175.00"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Rocchio algorithm</figDesc><graphic coords="2,339.49,364.03,175.00,175.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,325.48,220.42,203.01,7.77;3,332.49,72.00,189.00,142.30"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Relevance Language Model 3(RM3) algorithm</figDesc><graphic coords="3,332.49,72.00,189.00,142.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,328.44,680.10,197.10,7.77;3,332.49,544.74,189.00,129.24"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Precision-Recall Curves for the four methods</figDesc><graphic coords="3,332.49,544.74,189.00,129.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,320.82,704.51,224.30,8.64"><head>Table 2 .</head><label>2</label><figDesc>Topics for which our methods achieved the highest gain (+) or loss (-) on the background linking track</figDesc><table coords="3,320.82,704.51,224.30,8.64"><row><cell>To further investigate the performance of our methods</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,308.86,692.56,236.25,20.59"><head>Table 3 .</head><label>3</label><figDesc>As can be seen from the table, M2 has significantly im-</figDesc><table coords="5,63.02,74.28,210.43,58.85"><row><cell cols="2">Method Gain/Loss</cell><cell>Topics</cell></row><row><cell>M1</cell><cell>-</cell><cell>810, 818</cell></row><row><cell>M1</cell><cell>+</cell><cell>433, 802, 805, 813, 816</cell></row><row><cell>M2</cell><cell>-</cell><cell>362, 810, 818</cell></row><row><cell>M2</cell><cell>+</cell><cell>347, 433, 802, 804, 806, 819</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,50.11,146.20,236.25,18.73"><head>Table 4 .</head><label>4</label><figDesc>Topics for which our methods achieved the highest gain (+) or loss (-) on the entity linking track</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,65.55,672.29,220.81,7.77;5,65.55,683.24,39.51,7.77;5,123.15,683.24,163.22,7.77;5,65.55,694.04,220.82,7.93;5,65.55,705.00,90.16,7.93" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="5,153.15,683.24,133.21,7.77;5,65.55,694.20,151.80,7.77">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,324.30,76.13,220.81,7.77;5,324.30,87.08,220.82,7.77;5,324.30,98.04,220.81,7.77;5,324.30,109.00,20.17,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,402.35,76.13,142.76,7.77;5,324.30,87.08,151.92,7.77;5,324.30,98.04,217.24,7.77">A probabilistic analysis of the rocchio algorithm with tfidf for text categorization</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Carnegie-mellon univ pittsburgh pa dept of computer science</note>
</biblStruct>

<biblStruct coords="5,324.30,120.96,220.81,7.77;5,324.30,131.76,220.81,7.93;5,324.30,142.71,220.82,7.73;5,324.30,153.67,220.81,7.93;5,324.30,164.79,105.34,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,467.66,120.96,77.45,7.77;5,324.30,131.92,48.88,7.77">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,394.79,131.76,150.32,7.73;5,324.30,142.71,220.82,7.73;5,324.30,153.67,134.39,7.93">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,324.30,176.75,220.81,7.77;5,324.30,187.55,220.81,7.93;5,324.30,198.67,162.88,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,360.86,187.55,134.98,7.73">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,324.30,210.62,220.81,7.77;5,324.30,221.42,220.82,7.93;5,324.30,232.38,200.53,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,483.41,210.62,61.70,7.77;5,324.30,221.58,149.23,7.77">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,484.03,221.42,61.09,7.73;5,324.30,232.38,121.32,7.73">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
