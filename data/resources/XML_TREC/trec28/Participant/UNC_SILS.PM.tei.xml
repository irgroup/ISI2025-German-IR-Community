<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.75,115.90,315.86,12.68;1,288.18,133.83,38.85,12.68">UNC SILS at TREC 2019 Precision Medicine Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,248.27,173.14,51.04,8.80"><forename type="first">Jiaming</forename><surname>Qu</surname></persName>
							<email>jiaming@ad.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel</orgName>
								<address>
									<addrLine>Hill Chapel Hill</addrLine>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.01,173.14,44.82,8.80"><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<email>wangyue@email.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel</orgName>
								<address>
									<addrLine>Hill Chapel Hill</addrLine>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.75,115.90,315.86,12.68;1,288.18,133.83,38.85,12.68">UNC SILS at TREC 2019 Precision Medicine Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">864ADFD316541E829391922A18AAF176</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the scientific abstract retrieval task of TREC 2019 Precision Medicine Track. Our approach has two major components. First, we expand the original disease and gene terms using biomedical knowledge bases to improve recall of the initial retrieval. We then improve precision by promoting treatmentrelated publications to the top using a machine learning reranker trained on 2017 and 2018 relevance judgments combined. Batch evaluation results show that the proposed approach effectively improves P@10 compared to the baseline model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC Precision Medicine track aims to address an important medical retrieval problem: given a patient's disease, gene variation, demographic or other information, how to effectively retrieve relevant scientific papers or clinical trials for physicians to make decisions. This is the third year of the Precision Medicine track, which consists of two tasks as previous years. Both tasks are about document retrieval, but one for scientific abstracts in PubMed/MEDLINE <ref type="foot" coords="1,455.65,499.17,3.97,6.16" target="#foot_0">1</ref> and another for clinical trials in ClinicalsTrial.gov <ref type="foot" coords="1,333.08,511.12,3.97,6.16" target="#foot_1">2</ref> .</p><p>The School of Information and Library Science (SILS) at the University of North Carolina at Chapel Hill (UNC) participated in the scientific abstract retrieval task in this year's PM Track. In this paper, we discuss our strategy to tackle the problem. In section 2, we provides a general overview of our strategy, including how to parse and index the documents. In section 3, we demonstrate in detail our retrieval strategy which consists of a two-direction approach. In section 4, we report the retrieval performances evaluated by TREC. In section 5, we summarize our work and propose potential improvements and directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework Overview</head><p>This year's scientific abstract retrieval task continues using the MEDLINE corpus which is a snapshot of PubMed abstracts, but does not include conference papers as extra corpus as previous years. All the PubMed article abstracts are in XML files, with rich information such as ID, title, abstract content, author, headings, journal information, and etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document processing and Indexing</head><p>The first step is to index the corpus, for which we use Apache Lucene<ref type="foot" coords="2,458.16,228.61,3.97,6.16" target="#foot_2">3</ref> , an open-source, high-performance and publicly-available searching engine toolkit in Java. In this task, for each article we only index three fields, as is shown in Table <ref type="table" coords="2,134.27,266.03,3.80,8.80" target="#tab_0">1</ref>. Throughout the whole task, we use the Okapi BM25<ref type="foot" coords="2,369.24,264.47,3.97,6.16" target="#foot_3">4</ref> ranking algorithm which has been widely used in various industry-level applications. In this paper, we use Lucene's default parameter settings of Okapi BM25 (k = 1.25 and b = 0.75). Check: and standard analyzer with lowercasing and removing English stopwords. There are documents with slightly different abstract but the same document ID (PMID), for a paper is added into the corpus every time after it is revised. To prevent duplicate document IDs, we simply index a document at the first hit of its PMID and ignore the later versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Two-stage retrieval framework</head><p>Our retrieval framework consists of two major components, namely query expansion and re-ranking, as is shown in Figure <ref type="figure" coords="2,339.00,520.65,3.92,8.80" target="#fig_0">1</ref>. Since the standard number of retrieved documents for each topic is 1000, we aim to maximize Recall@1000 by query expansion at the first step, during which we also tune weights of expansion terms. Then we train a pointwise learning-to-rank model (logistic regression model) that predicts the probability of a document's being relevant to the query, and the probability is used to re-rank the initially retrieved documents. All the training and parameter tuning are done on the 2017 and 2018 topics, using the released relevance judgement files with the true relevance grades. On 2019 topics, we first apply the query expansion module in the initial retrieval stage, and then rerank the retrieved results using the trained reranker.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe our proposed retrieval framework. First we discuss how we consult external knowledge bases to expand disease and gene terms in the query to solve the mismatch problem. Then we discuss how we train a relevance model to further refine the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query Expansion</head><p>The goal of query expansion is to use external knowledge bases to expand the original term which helps the query to match more documents which do not contain that term. Since both disease and gene information are given regarding to a patient, we expand both two fields in knowledge bases separately.</p><p>Diseases In this task we use a public knowledge base API called Lexigram<ref type="foot" coords="4,462.93,117.38,3.97,6.16" target="#foot_4">5</ref> . It contains not only the MeSH ontology, but also other knowledge bases like the Systematic Nomenclature of Medicine Clinical Terms (SNOMED CT) and the International Classification of Diseases (ICD). By integrating three professional knowledge bases together, this knowledge base provides broader and more accurate terms for disease expansion.</p><p>In selecting an appropriate query expansion structure, at first we included as many expansion terms as possible, but later we found that including a disease's "parent" and "children" terms bring too many irrelevant terms into the query. Therefore, these terms are not included as expansion terms for a disease, and we only include the preferred term and synonyms of a disease for query expansion. The table below illustrates how expansion terms for a disease term are different in two medical knowledge bases (see Table <ref type="table" coords="4,323.35,262.93,3.87,8.80" target="#tab_2">2</ref>). Another reason of using Lexigram is that it can recognize disease terms from input text and automatically generate expansion terms. However, if using the MeSH knowledge base, we need to look up the disease first to get a unique identifier, and then retrieve expansion terms according to that identifier. This brings a problem that when the disease in the query topic is a rare disease or does not match the standard disease name in MeSH, the identifier is hard to be found without human efforts or domain knowledge.</p><p>Apart from the preferred term and synonyms, acronyms are also used as expansion terms in this paper. Consider such a snippet in a paper's abstract which is relevant to lung cancer : "Microarray analyses have revealed significantly elevated expression of the proto-oncogene ROS1 receptor tyrosine kinase in 20-30% of non-small cell lung carcinomas (NSCLC). Selective and potent ROS1 kinase inhibitors have recently been developed and oncogenic rearrangement of ROS1 in NSCLC identified. We performed immunohistochemical evaluation of expression of ROS1 kinase and its downstream molecules in 399 NSCLC cases. ROS1 expression in primary and recurring lesions of 92 recurrent NSCLC cases was additionally analyzed."</p><p>In this text, we can see that the disease non-small cell lung carcinomas is only written in its full name when it appears in the paragraph for the first time, then it is written in its acronym form NSCLC afterwards for simplicity. This is very common in biomedical scientific papers that people use short acronyms instead of full names. For this paper, we use the simple regular expression &lt;Disease Name&gt; \([A-Z]+\) in the corpus to match a pattern that a disease name is followed by a capitalized term in parentheses to retrieve acronyms for each disease.</p><p>Genes For query expansion of the gene field, we enrich the original gene term with the genetic dataset provided by the National Center for Biotechnology Information (NCBI)<ref type="foot" coords="5,225.76,251.68,3.97,6.16" target="#foot_5">6</ref> . We simply include the aliases for each gene term by looking it up in the dataset (see Table <ref type="table" coords="5,305.03,265.19,3.87,8.80" target="#tab_3">3</ref>). </p><formula xml:id="formula_0" coords="5,139.82,324.70,332.65,45.80">NCBI Gene List Original Gene Term KRAS Aliases C-K-RAS|CFC2|K-RAS2A|K-RAS2B|K-RAS4A|K-RAS4B K-Ras|KI-RAS|KRAS1|KRAS2|NS|NS3|RALD|c-Ki-ras2</formula><p>To summarize, for each query topic the disease term is expanded to its preferred term, acronym and synonyms, and the gene term is expanded to its aliases. This helps enriching the information in the original query and matching those potentially relevant documents which do not mention the raw query terms but terms referring to the same disease or gene. We then tune grid search? the weights of expanded terms on the 2017 and 2018 topics to optimize R@1000, and the result is shown in Table <ref type="table" coords="5,258.91,475.08,3.87,8.80" target="#tab_4">4</ref>. Overall Query Structure For each topic, we expand the query in terms of disease and gene, and other fields are not used in this paper. After these two fields are expanded in knowledge bases, weights are assigned to each term. We use the OR operator between all the terms, because it is impossible for a paper to contain all the query terms after expansion. We use the SHOULD operator for the title field and the MUST operator for the content field, which means it is not required for a document to match the query in its title but required to match the query in its content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Re-ranking</head><p>After expanding the original query to optimize R@1000, we aim to refine the ranking by pushing the most relevant documents to the top. We first explore a heuristic approach, and then discuss features to train a logistic regression classifier. The training step is done on the 2017 and 2018 data.</p><p>A Heuristic Approach Before starting training the reranker which helps to judge how much a document is relevant to the PM topics, we explore a heuristic approach to re-rank the retrieved results. By exploring the title and content of top 10 retrieved results after the query expansion, we notice that almost all the relevant articles have the disease term in the title and most of the non-relevant articles do not. Therefore, we hypothesize that a relevant article should have the disease term in the title, otherwise it should be penalized to a lower relevance score. Based on this idea, we explore a heuristic approach of punishing those articles without the disease term by multiplying their original scores with a penalty factor between 0 to 1.</p><p>Based on the 2017 and 2018 data, we heuristically set the penalty factor to 0.6, and the performance of this retrieval shown in Section 4 proves that simple as this heuristic approach is, it does help to push relevant documents upwards and to improve P@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to Rank</head><p>The heuristic approach is the starting point of re-ranking the retrieved result to enhance P@10. However, this approach is based on human efforts of reading and exploring the results, therefore, we aim to leverage the pointwise learning-to-rank idea and machine learning techniques to learn a ranker which can automatically tell whether a paper is relevant to the PM track or not. According to the PM track overview <ref type="bibr" coords="6,323.63,560.42,10.08,8.80" target="#b0">[1]</ref>, an article which is identified as relevant to the PM track should focus on treatments. Thus, we hypothesize that a relevant article should cover more words related to treatments instead of laboratory experiments. Also, in this year the treatments of each article are also extracted and provided as a new resource, and we include the count of treatments as a feature.</p><p>Table <ref type="table" coords="6,177.70,632.15,5.08,8.80" target="#tab_5">5</ref> lists query-document features and document-specific features used in the ranker. The features of whether the disease term appears in the title is categorical, and all the other features are numerical as we simply count the term frequency of each keywords. We choose high-level features instead of pure word features from the bag-of-words model because such features would be too sparse to have a good performance. The next step is to generate the positive keywords and negative keywords. Oleynik et al. use the Latent Dirichlet Allocation model to do Topic Modeling among the relevant articles and non-relevant articles to find positive and negative keywords <ref type="bibr" coords="7,178.65,341.68,10.06,8.80" target="#b1">[2]</ref>. These keywords are supposed to have the strongest indication of two labels. The lists of positive and negative keywords are shown in Table <ref type="table" coords="7,461.35,353.63,3.87,8.80" target="#tab_6">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ranker Construction</head><p>We train a Logistic Regression model to predict the probability that a document is relevant to the PM track using the scikit-learn<ref type="foot" coords="7,355.67,529.00,3.97,6.16" target="#foot_6">7</ref> library. For each retrieved article, the feature vector is generated based on the feature table above and the probability of a positive (relevant) label is predicted using the model. We use both the predicted probability of how relevant an article is to the PM track and the raw BM25 score for re-ranking. However, since the probability is a number between 0 to 1 and the raw score could be as large as 70, we should keep them at the same scale. Therefore, we do the min-max transformation on both scores and they are scaled to a number between 0 to 1. Then for each article, a new score is generated by adding up the raw BM25 score and the probability from the classifier, and is used for re-ranking.</p><p>However, in experiments we find that applying the reranker to all the 1000 results do not improve the precision, but hurt the performance instead. We suppose that it is because the positive instances in the training set merely come from past teams' top retrieved results, which are much fewer than the negative instances. Thus, the ad-hoc relevance judgment by experts are only applied on documents which rank at top in each team's submitted results, which leads to a different distribution of instances in the training and testing set. Therefore, instead of applying the reranker on all the 1000 retrieved documents to do reranking, we only re-rank the top 50 documents, after testing different top K documents to re-rank by optimizing P@10 on the 2017 and 2018 topics.</p><p>In summary, we train the reranker on the 2017 and 2018 topics and find that the optimal number to re-rank is 50. When we run queries from the 2019 topics, we follow the heuristic approach first, i.e., we punish a document which does not mention the disease term in its title by multiplying its raw score by a penalty factor of 0.6, based on which the results are re-ranked for the first time. We then transform the raw BM25 scores using a min-max scaler to a number between 0 and 1. Afterwards, we only input the top 50 documents into the reranker and add up the BM25 score and probability. Based on new scores, the re-ranking is done for the second time and a final ranked list is returned for relevance judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We submit 4 runs for the scientific abstract retrieval task and the evaluation results are summarized in the table below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7. Evaluation Results</head><p>Run Name P@10 R-prec infNDCG sils_run1 0.5225 0.2858 0.4490 sils_run2 0.5925 0.2805 0.4692 sils_run3 0.5925 0.2757 0.4620 sils_run4 0.5900 0.2757 0.4625 sils_run1 is the baseline which does not re-rank the results after retrieval. This run expands the query and returns a ranked list from Lucene according to BM25 relevance scores.</p><p>sils_run2 adds the heuristic re-ranking on the baseline. sils_run3 adds the trained re-ranker on the baseline, with features of "Whether the disease name appears in the title", "Count of positive/negative keywords in title/abstract".</p><p>sils_run4 adds the trained re-ranker on the baseline, compared to sils_run3 , this run has one more feature of "count of unique treatments" in the classifier.</p><p>By comparing sils_run1 and sils_run2 or sils_run3 , we could see that re-ranking improves P@10 by pushing the most relevant documents to the top. However, by comparing sils_run2 and sils_run3 , we could see that P@10 does not differ too much from the heuristic approach or the learning-to-rank approach. Finally, by comparing sils_run3 and sils_run4 , we could see that adding the new feature in 2019 does not help too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We participate in the 2019 Precision Medicine track and submit 4 runs for the scientific abstract retrieval task. Our best run incorporates two steps, which are query expansion and re-ranking. We expand disease and gene query terms by knowledge bases to improve recall, then train a Logistic Regression classifier to predict how much an article is relevant, based on which we re-rank the retrieval results. Results show that the proposed approach effectively improves the performance in terms of P@10 compared to the baseline model.</p><p>This work opens up interesting questions for future studies. In terms of learning-to-rank models, one could explore richer features (ranking scores produced by different relevance models) and more expressive function family (ensembles such as gradient boosted trees). It would also be interesting to study the actual information need of oncologists. For instance, it would be interesting to study if PM searches are mostly high-precision tasks (as indicated by the name precision medicine), or high-recall tasks (more like a systematic literature review of relevant treatments), or the recall levels being case-specific. A system that's aware of the desired level of recall would make informed decision in presenting (whether ranking or selecting) documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,209.22,438.99,196.92,7.93"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the retrieval architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,197.77,331.66,219.81,70.06"><head>Table 1 .</head><label>1</label><figDesc>Three fields indexed for each scientific paper</figDesc><table coords="2,249.02,355.91,114.24,45.80"><row><cell cols="3">Fields Analyzed Stored</cell></row><row><cell>Title</cell><cell>True</cell><cell>True</cell></row><row><cell cols="2">Abstract True</cell><cell>True</cell></row><row><cell>ID</cell><cell>False</cell><cell>True</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,136.16,296.02,355.19,91.68"><head>Table 2 .</head><label>2</label><figDesc>Example of disease expansion terms in two different knowledge bases</figDesc><table coords="4,136.16,319.98,355.19,67.72"><row><cell></cell><cell>Lexigram</cell><cell>MeSH</cell></row><row><cell cols="2">Original Disease Term cholangiocarcinoma</cell><cell>cholangiocarcinoma</cell></row><row><cell cols="3">Preferred Disease Term cholangiocarcinoma of biliary tract cholangiocarcinomas</cell></row><row><cell>Synonyms</cell><cell>bile duct carcinoma,</cell><cell>Intrahepatic Cholangio-</cell></row><row><cell></cell><cell>bile duct adenocarcinoma,</cell><cell>carcinoma, Extrahepatic</cell></row><row><cell></cell><cell>cholangiocellular carcinoma</cell><cell>Cholangiocarcinoma</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,175.25,300.74,264.85,7.93"><head>Table 3 .</head><label>3</label><figDesc>Example of gene expansion terms in the NCBI gene list</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,209.45,508.13,196.46,102.64"><head>Table 4 .</head><label>4</label><figDesc>Weights for each expanded term group</figDesc><table coords="5,220.45,532.09,171.39,78.68"><row><cell>Field</cell><cell>Query Term</cell><cell>Weight</cell></row><row><cell cols="3">Diseases Original Disease Term 1</cell></row><row><cell></cell><cell cols="2">Disease Preferred Term 0.1</cell></row><row><cell></cell><cell>Disease Synonyms</cell><cell>0.1</cell></row><row><cell></cell><cell>Disease Acronyms</cell><cell>0.5</cell></row><row><cell cols="2">Genes Original Gene Term</cell><cell>1</cell></row><row><cell></cell><cell>Gene Aliases</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,160.44,173.65,291.40,102.94"><head>Table 5 .</head><label>5</label><figDesc>Features in the learning-to-rank model</figDesc><table coords="7,160.44,197.91,291.40,78.68"><row><cell>Feature Description</cell><cell>Data Type</cell></row><row><cell cols="2">Whether the disease name in the query appears in the title Categorical</cell></row><row><cell>Number of positive keywords in the title</cell><cell>Numerical</cell></row><row><cell>Number of positive keywords in the abstract</cell><cell>Numerical</cell></row><row><cell>Number of negative keywords in the title</cell><cell>Numerical</cell></row><row><cell>Number of negative keywords in the abstract</cell><cell>Numerical</cell></row><row><cell>Number of unique treatments in the article</cell><cell>Numerical</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,141.08,384.44,330.11,69.76"><head>Table 6 .</head><label>6</label><figDesc>Positive, negative and heading keywords</figDesc><table coords="7,141.08,408.40,330.11,45.80"><row><cell>Positive Keywords treatment, survival, prognostic, clinical, prognosis, therapy</cell></row><row><cell>outcome, resistance, targets, therapeutic, immunotherapy</cell></row><row><cell>Negative Keywords pathogenesis, tumor, development, model, tissue, mouse</cell></row><row><cell>specific, staining, dna, case, combinations</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,646.48,169.42,7.47"><p>https://www.ncbi.nlm.nih.gov/pubmed/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,657.44,127.07,7.47"><p>https://clinicaltrials.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,645.79,103.94,7.92"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,656.74,179.09,7.92"><p>https://en.wikipedia.org/wiki/Okapi_BM25</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,657.44,112.95,7.47"><p>https://www.lexigram.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,144.73,656.74,161.36,7.92"><p>ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="7,144.73,656.74,96.88,7.92"><p>https://scikit-learn.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,457.35,342.25,7.92;9,146.91,468.31,333.68,7.92;9,146.91,479.27,61.45,7.92" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>William R Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shubham</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>precision medicine track</note>
</biblStruct>

<biblStruct coords="9,138.35,490.23,342.25,7.92;9,146.91,501.19,333.68,7.92;9,146.91,512.15,256.18,7.92" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,191.84,512.15,182.34,7.92">Hpi-dhc at trec 2018 precision medicine track</title>
		<author>
			<persName coords=""><forename type="first">Michel</forename><surname>Oleynik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Faessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ariane</forename><forename type="middle">Morassi</forename><surname>Sasso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arpita</forename><surname>Kappattanavr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harry</forename><surname>Freitas Da</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Philipp</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suparno</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erwin</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Böttinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
