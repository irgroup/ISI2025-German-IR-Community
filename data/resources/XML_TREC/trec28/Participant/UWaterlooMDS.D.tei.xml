<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,80.70,105.06,318.56,12.85">UWaterlooMDS at the TREC 2019 Decision Track</title>
				<funder ref="#_gw2K2zb">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,80.70,133.30,120.17,10.69"><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.65,133.30,122.41,10.69"><forename type="first">Fuat</forename><forename type="middle">C</forename><surname>BeylunioÄŸlu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.84,133.30,102.85,10.69"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,476.57,133.30,55.05,10.69;1,80.70,147.25,65.14,10.69"><forename type="first">P</forename><forename type="middle">Robert</forename><surname>Duimering</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,80.70,105.06,318.56,12.85">UWaterlooMDS at the TREC 2019 Decision Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBC14239D7C09F094E8088D90557C3C6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this report, we discuss the experiments we conducted for the TREC 2019 Decision Track. This year, our goal was to investigate the effect of document credibility on the quality of automatic runs. To address credibility, we combined scores from a spam classifier and a credibility classifier trained to detect non-trustworthy websites. The results from both classifiers were then used to modify a baseline BM25 ranking. In addition to the automatic runs, we also submitted manual runs using the HiCAL [2] system. Our manual runs modify a baseline BM25 ranking using manually judged documents found using the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One of the goals of the TREC 2019 Decision Track is to drive researchers to implement retrieval methods that promote correct information over incorrect information. This is motivated by previous research showing that incorrect information presented on a search engine result page (SERP) can drive the user to make incorrect and potentially harmful decisions <ref type="bibr" coords="1,199.32,327.67,10.30,8.83" target="#b3">[4]</ref>. To evaluate the performance of retrieval methods, submitted runs are evaluated not only on relevance, but also on correctness and credibility of documents returned.</p><p>According to the track's assessing guidelines, relevance is assessed in a manner similar to previous tracks at TREC. Assessing the correctness of information in a document is arguably harder to determine than relevance. Correctness depends on the correspondence between the claims made in the retrieved document and those of an independent source deemed to represent the truth. Credibility, on the other hand, is related to the intentions of the information content provider and the person reading the document. A content provider wants their content to be regarded as trustworthy and accepted by readers, whereas the reader wants the information to be true <ref type="bibr" coords="1,499.93,411.35,31.36,8.83;1,80.70,423.31,7.02,8.83">[5, page 5]</ref>. We hypothesize that a credible document, whether it is judged as credible because it is from a trustworthy source or because of its quality, is most likely to promote correct information over incorrect information. Based on these arguments, we focus our effort on retrieving relevant and credible documents and anticipate that such effort will also be useful for retrieving correct information.</p><p>We submitted automatic and manual runs to the track. Both types of runs depended on a BM25 ranking, and then modified the BM25 ranked documents to filter out documents likely to be non-relevant or non-credible. For automatic runs, our method of determining credibility was based on a combination of scores from a spam classifier and a credibility classifier trained to detect non-trustworthy documents. Manual runs were constructed using the HiCAL system, with two assessors manually judging documents for each of the track's topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SUBMITTED RUNS 2.1 Automatic Runs</head><p>Our runs used three types of scores on (i) relevance, (ii) credibility and (iii) spaminess. Relevance scores were based on the BM25 retrieval method with default parameters as implemented in Anserini<ref type="foot" coords="1,441.34,583.31,3.38,6.44" target="#foot_0">1</ref> . To assess credibility, we trained a logistic regression classifier on a health corpus subsetted from the ClueWeb12 dataset. Lastly, we filtered spam using spaminess scores proposed by Cormack et al. <ref type="bibr" coords="1,339.93,609.07,10.30,8.83" target="#b2">[3]</ref>. While spaminess scores based on a classifier capturing features that signal whether or not a document is spam, the credibility classifier aims to capture the tone that signals whether or not the document is trustworthy. The spam and credibility scores are used to adjust the ranking by elevating the position of credible information relative to the baseline BM25 ranking. Note that both spaminess and credibility classification rely on similar learning algorithms. As with the classifier used to generate spam scores in Cormack et al. <ref type="bibr" coords="2,270.85,394.97,10.31,8.83" target="#b2">[3]</ref>, the credibility classifier employs logistic regression inputting all available character 4-grams in a document, coded based on binary features (i.e. presence or absence) rather than frequencies of occurrence as we will detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Training and Test Collection.</head><p>We prepared two different corpora subsets of ClueWeb12-B13 for (i) training the credibility classifier, and (ii) and measuring the algorithms' performance to determine final runs. For the former, we defined a set of 25 topics, and a number of queries to retrieve documents from ClueWeb12 for each topic. The created topics are similar to the track's topics but are on a different set of medical interventions and treatments. The topics cover a variety of health issues from cancer to diabetes and scoliosis, and were chosen based on different levels of controversy, from lower (e.g. exercise for scoliosis) to higher (e.g. vaccines for hepatitis B) and different target groups (such as vinpocetine for dementia, antioxidants for female subfertility). We then constructed a set of queries in the form of "[treatment] for [issue]" and its variation using synonyms and different modifiers (e.g. "antidepressants for tinnitus", "can antidepressants help tinnitus", "antidepressants for ringing in the ear"). We used Anserini with default BM25 parameters to retrieve the top 1000 documents per query and filtered out malicious pages with an open source anti-virus software, ClamAV, resulting in 40753 unique documents.</p><p>For the second corpus, we selected topics based on their popularity to ensure sufficient credible and non-credible content (e.g., "acupuncture for autism", "antibiotics for otitis media", "pilates for lower back pain", "lycopene for prostate cancer", "green tea cancer"). We retrieved 1000 documents per topic using the procedure described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Annotating the Corpora.</head><p>To prepare an annotated corpus, we used HiCAL <ref type="bibr" coords="2,410.18,634.06,14.42,8.83" target="#b1">[2]</ref>, a system for high-recall retrieval, to assess documents on their credibility. Given a seed query, HiCAL initially ranks the collection and 2.1.3 Credibility Classifier and Scores. The supervised classifier aims to detect deep patterns in web pages that signals credibility independent from the topic. These patterns can be a combination of features that gives all sorts of information from page design to colors and content. We trained a logistic regression model over raw documents by first converting text to lower case, then tokenizing into all sequential character 4-grams. For example the word "HonCode" is parsed into "honc", "onco", "ncod" and "code". We trained the classifier under binary setting, i.e. 1 if the feature is present, 0 if absent.</p><p>Each document d i in the training collection was converted into a binary vector of N features, X i = [1, X i1 , . . . , X i j ] T for X i j âˆˆ {0, 1}, j âˆˆ {1, . . . , N } and for {i âˆˆ 1, . . . , D} documents. We fit standard logistic regression model:</p><formula xml:id="formula_0" coords="3,80.70,572.75,313.81,38.60">P(d i âˆˆ Credible) = 1 1 + e -Z i , for Z i = w T X i for w T = [w 0 , w 1 , . . . , w N ].</formula><p>With the above model trained on the full set of 25 topics, we computed probabilities, P(d i âˆˆ Credible), for each document in the test set of 5 topics and later for the entire TREC Decision collection.</p><p>2.1.4 Spam Scores. Spam may not attempt directly to deceive the content consumer but is an obvious type of a non-credible document. Spam also influences search engine ranks through various deceptive means, thereby improving the rank of a page regardless of the content's relevance. For example, content spam modifies the page simply by adding more keywords to increase its score for a given query.</p><p>To detect with spam documents, Cormack et al. <ref type="bibr" coords="4,284.31,155.86,11.73,8.83" target="#b2">[3]</ref> developed a spam model on the ClueWeb09 dataset which was later used to generate spam scores for ClueWeb12 collection <ref type="foot" coords="4,341.14,165.96,3.38,6.44" target="#foot_2">3</ref> . The dataset contains spamminess percentiles ranging between 0 to 99 with 0 being the most spammy. We used these scores to evaluate the spaminess of each document.</p><p>2.1.5 Runs. In our runs, we aimed first to find all relevant documents and then adjust their positions with respect to their credibility. By combining the relevance, credibility and spam scores we prepared several test runs with different parameters and evaluated on the 5 test topics described above. Then we determined the best-performing algorithms to generate the final runs.</p><p>All the runs were prepared by transforming our baseline, UWaterMDS_BM25, by adding credibility reward and filtering highly spammy documents. Below we describe these transformations and present their performance results.</p><p>2.1.6 UWaterMDS_BM25. In this run, we used the BM25 retrieval algorithm as implemented in Ansereni. The parameters used for this run were the default parameters set by Ansereni. We used the query field of the topic as input to the algorithm.</p><p>2.1.7 UWatMDS_BM25_ZS, UWatMDS_BM25_Z, UWatMDS_BMZBS10 . In the test runs, using the credibility classifier probabilities to filter out documents or adjust their ranks directly (e.g. by computing BM25 * P(d âˆˆ Credible)) resulted in discarding many useful documents. Besides the probabilities are not appropriate for linear transformations for re-ranking documents. Therefore we transformed the probabilities to Z -scores using logit function and used them to linearly combine with other scores:</p><formula xml:id="formula_1" coords="4,204.18,408.08,201.45,23.85">p = P(d âˆˆ Credible) = 1 1 + e -Z , and Z = ln p 1 -p</formula><p>To combine with BM25, we rescaled Z to 0-1. As we aim to improve BM25 scores to favour credibility, we added credibility reward proportionate to relevance as BM25 * (1 + Z ). If Z is close to 0 (non-credible) then the score remains unchanged whereas if it is close to 1, it sums up to double. Using this approach, the interaction between credibility and relevance prevents a non-relevant but credible document from occupying a higher position in the results. We also added a parameter, Î² to Z -score to control the relative weight of credibility judgments:</p><formula xml:id="formula_2" coords="4,272.26,505.51,67.38,9.96">BM25 * (1 + Î²Z )</formula><p>Further, we used spam scores (SPAM) as filters. In the test runs it filtered out too many necessary documents when the threshold was set to 70. The best performances were reached when 10 &lt; SPAM &lt; 40 varying with respect to the test topic. Hence we used the spam filter with a threshold of 10 to filter out "junk" documents and adjusted scores using the above rule.</p><p>After a series of trials with SPAM âˆˆ {10, 20, . . . , 70} and Î² âˆˆ {1, 1.1, . . . , 2} we decided on the rules listed below, which generated relatively higher precision and more stable results.  <ref type="table" coords="5,213.56,361.81,4.73,8.83" target="#tab_1">2</ref> presents the performance of automatic runs on 5 test topics. As shown in the table, the credibility classifier improves BM25 scores. BM25 filtered with 90% level (UWatMDS_BMF_C90) yielded inconsistent results, outperforming all other algorithms for Topic 3, but only marginally improving on BM25 baseline for Topic 1. The combinations of BM25, Z and SPAM (UWatMDS_BM25_Z, UWatMDS_BM25_ZS, UWatMDS_BM25ZBS10) performed better than standard BM25, and even doubling its precision for some topics.</p><formula xml:id="formula_3" coords="4,94.30,599.52,282.11,33.87">1. UWatMDS_BM25_ZS = IF SPAM &gt; 10, BM25 * (1 + Z ) ELSE 0 2. UWatMDS_BM25_Z = BM25 * (1 + Z ) 3. UWatMDS_BMZBS10 = IF SPAM &gt; 10, BM25 Ã— (1 + Z * 2) ELSE 0</formula><p>For the 5th topic, as expected, other algorithms performed worse than UWatMDS_BM25. As described earlier in Section 2.1.2, assessment of this topic gives BM25 an advantage and a better algorithm would precisely reduce the ranks of non-credible or non-relevant documents. As Table <ref type="table" coords="5,336.16,445.50,4.59,8.83" target="#tab_1">2</ref> shows, spam and credibility classifiers reduced precision sharply when used as filters (UWatMDS_BMF_C90, UWatMDS_BMF_C95, UWatMDS_BMF_S30) suggesting that these algorithms filtered out many true positive documents. On the other hand, UWatMDS_BM25_Z caused the least distortion to the overall position of positive instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Manual Runs</head><p>We submitted 3 manual runs for the track. We used the HiCAL system for manually assessing documents. Currently, HiCAL only supports rendering structured documents (e.g. news articles, tweets, etc). We extended HiCAL to render HTML based documents, as shown in Figure <ref type="figure" coords="5,330.41,543.04,3.34,8.83" target="#fig_0">1</ref>. We provided the topic's query as the seed query for HiCAL's classifier. To mitigate the cold-start problem and for the classifier to "learn" the topic of interest, we modified the system to make the assessor start the assessment process with the top 20 documents returned by BM25. Judgments on the first 20 documents were then used to retrain the classifier, and the most likely relevant document is shown to the assessor. The assessment is stopped once the assessor has judged 200 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Collection (Documents Set).</head><p>The current version of HiCAL operates entirely on memory. For HiCAL to work, the entire ClueWeb12-B13 collection (1.95 TB) needs to be loaded into memory. This requirement was not feasible with our resources at the time of assessing. Instead, we operated on a subset of the collection that was manually constructed. To construct the subset, we collected the top 10,000 documents returned by BM25 for each of the 51 queries in the topics file, resulting in 356,635 unique documents. We also added 10,000 Ã— 51 random documents to allow tf-idf features in HiCAL to be more representative of the entire collection. In total, there were 866,635 documents in the constructed subset of the collection. Training HiCAL's classifier was done on the raw HTML document without any pre-processing.</p><p>2.2.2 Assessors. In total, there were 51 topics. Assessment of the topics was divided between two graduate students who are involved in this task. One assessor judged 15 topics and the other judged 36 topics. We randomly assigned the topics to each assessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Judging Criteria.</head><p>As important as relevance is for this task, other criteria are also important, namely the credibility of the document and the correctness of the information. While credibility is possible to judge (based on the assessor's knowledge of reliable sources of information, for example), it is much harder to judge the accuracy of information medical treatments towards health issues, especially for assessors who are not medical experts and without any medical accreditation. Our relevance scale, therefore, is modified to account for relevance and credibility only. Another option we considered was having separate judging criteria for relevancy and credibility. This would slow down the assessment process, however, and due to time limits, we opted for a single relevance scale.</p><p>Before the assessing process, both assessors were informed of the following information when judging:</p><p>â€¢ Highly Relevant: Documents that contain information about the medical treatment and the health issue and appear to be from reliable sources or contain reliable content (e.g., mention of doctor or clinic name, or providing reliable references).</p><p>â€¢ Relevant: Documents that contain information about the medical treatment and the health issue, but unsure of the credibility of the webpage. â€¢ Not Relevant: Documents that are either off topic, or completely not trustworthy (e.g., spam documents). While documents judged as relevant may be considered not credible by NIST assessors, judging them as not relevant could hurt HiCAL's classifier. In particular, a relevant document may contain relevant terms that are useful for the training step in HiCAL. Having the classifier trained on such documents is useful to find other, potentially highly relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Number of Relevant Documents.</head><p>Figure <ref type="figure" coords="7,265.50,199.97,4.57,8.83">2</ref> indicates the number of relevant documents found using HiCAL. We expected highly relevant documents (e.g. documents we consider relevant and credible) to be harder to find. Across all topics, a total of 535 highly relevant and 1345 relevant documents were found.</p><p>One of the challenges we encountered by using HiCAL for finding relevant documents was the quality of documents returned by the classifier. While HiCAL has been shown to be successful in achieving high recall on datasets such as news articles <ref type="bibr" coords="7,222.13,259.74,10.49,8.83" target="#b0">[1,</ref><ref type="bibr" coords="7,235.66,259.74,6.99,8.83" target="#b5">6]</ref>, using the system on a web collection, such as ClueWeb, introduced unanticipated challenges. For example, while using HiCAL, both assessors agreed that the classifier returned many documents that were completely off topic but from a website with a document they judged as relevant earlier during the assessment process. A possible explanation is due to the nature of the HTML documents. Boilerplate content associated with web pages judged as relevant could negatively affect the quality of the classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.5</head><p>UWatMDSBM25_HC1. To construct this run, we first append documents judged as highly relevant, then by relevant documents, sorted in reverse chronological order of appearance. The remainder of the list was filled by unjudged BM25 documents, sorted by their associated score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.6</head><p>UWatMDSBM25_HC2. We filled the list with our known judged relevant document the same way as in UWatMDSBM25_HC1. The remainder of the list was filled using a combination of BM25 ranking and a HiCAL classifier built for each topic. For each topic with â‰¥ 20 relevant documents, we built a final classifier to score each document in the sub-collection. We use the top 1,000 most-likely relevant documents returned by the classifier to alter the ranking produced by BM25. The premise behind this approach is that documents that appeared both in the top 1,000 most likely relevant documents set by the classifier and in BM25 top 1,000 documents should be ranked higher than those that appeared only in BM25's set. This approach allows potentially relevant documents that are low in BM25 ranked list to be pushed higher in the list if they were found to be most-likely relevant by the classifier. As the classifier is also trained with non-relevant documents, unjudged non-relevant documents that are ranked high in BM25 can be pushed even lower, effectively increasing the quality of the list. Documents judged as non-relevant are automatically skipped.</p><p>Training the classifier with few documents is not practical. Therefore, topics with less than 20 documents have the same ranking as in UWaterMDS_BM25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.7</head><p>UWatMDSBM25_HC3. In an effort to determine the effect of boilerplate content in a webpage, and whether simple pre-processing of the collection can improve the quality of HiCAL's classifier, we processed our subcollection to remove all HTML related tags, remove content of all &lt;a&gt; tags, and only keep the content of &lt;body&gt; tag. Following this step, the construction of the run was the same as UWatMDSBM25_HC2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULT AND DISCUSSION</head><p>Table <ref type="table" coords="8,103.79,524.02,4.54,8.83" target="#tab_3">3</ref> shows the performance of our runs under the track's evaluation measures. NLRE and CAM are combination measures designed to combine different assessing aspects into a single score. Here, both NLRE and CAM used all three aspects to compute its final score. The combination measures proposed by the track's organizers make it difficult to interpret the performance of the runs in one aspect over the other (e.g., how does a run optimized for credibility perform in terms of credibility alone?). In the next section, we look into the performance of runs under different combinations of aspects and under all aspects by modifying the notion of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Under Different Aspects</head><p>To understand how the runs perform in terms of correctness and credibility, we computed MAP scores for each aspect separately (relevance, credibility, and correctness). It is important to note that based on the track's assessing guidelines, treatment efficacy (from which the correctness of information is computed) and credibility are only assessed if the document is relevant. Therefore, by considering each of the aspects separately, we are actually measuring relevance with regards to another aspect. For example, in measuring performance in terms of correctness, a document is relevant only if it is relevant to the topic and contains correct information (similarly for credibility). Figure <ref type="figure" coords="10,171.33,418.78,4.58,8.83">3</ref> and Table <ref type="table" coords="10,219.99,418.78,4.58,8.83" target="#tab_4">4</ref> show MAP evaluated using relevance, as provided by NIST, and correctness and credibility evaluated separately using the provided assessments from NIST (qrels). Here, we assume any document that is not relevant or has not been judged in terms of relevance, to be incorrect and not credible, following the same assumption made by the track organizers for evaluating NLRE and CAM measures.</p><p>Our automatic run, UWatMDS_BM25_Z, which uses BM25 scores and Z-scores from our credibility classifier, has the highest performance in terms of credibility. It is interesting to note that credibility does not seem to indicate correctness, as the same run performs below expected in terms of correctness. We leave this question for future work.</p><p>In terms of correctness, our manual runs perform best. Because both assessors were heavily involved in the creation of the track's topics and knew the efficacy of the various treatments, it is possible that their knowledge of the topics may have influenced their judgments towards correct documents.</p><p>One of the task's goal is to drive search engines to provide correct and credible information to users. To obtain another view on performance, we computed MAP with relevance defined such that a document is relevant if and only if it is 1) relevant to the topic, 2) provides correct information and 3) is considered a credible document. Note that by enforcing such a rule, we effectively disregard documents that are relevant and credible but contain incorrect information, which we argue are documents that are most harmful to show to users. Column "All" in Table <ref type="table" coords="10,117.54,610.06,4.73,8.83" target="#tab_4">4</ref> shows the MAP scores when such a requirement is enforced. Here, we see that our baseline run, UWaterMDS_BM25, which ranks first in terms of CAM, underperforms and is ranked second to lowest among our runs. Our manual runs, as well as UWatMDS_BM25_Z automatic run seem to perform best. In light of these results, NLRE and CAM may not appropriately reflect the true performance of runs with respect to the track's goals. <ref type="table" coords="11,177.10,108.03,4.67,8.83" target="#tab_4">4</ref> provide information of overall performance. To highlight the quality of the produced ranking from a user perspective, we computed nDCG@10 scores for all runs using different types of assessments as we did in Table <ref type="table" coords="11,156.29,131.95,3.41,8.83" target="#tab_4">4</ref>. The nDCG@10 scores for all runs are shown in Table <ref type="table" coords="11,384.92,131.95,3.41,8.83" target="#tab_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP scores in Table</head><p>The scores for manual runs are the same because each manual run used the documents found using HiCAL and were manually judged as highly relevant or relevant as the initial set of documents in the ranked list, and modified the remaining non-judged documents based on the methods described in Section 2.2.5-2.2.7. In terms of relevance only, nDCG@10 scores are slight lower than the baseline. If we consider correctness and credibility assessment separately, manual runs improve over the baseline. Similarly for "All", where a documents is valid only if it is relevant, correct and credible.</p><p>When using relevance assessments only, both manual and automatic runs underperform compared to the baseline. The only exception is UWatMDS_BMF_S30, which does not negatively influence nDCG@10 scores in terms of relevance alone, and can also improve nDCG@10 scores in terms of correctness more than the other automatic runs and more than the baseline by 11.59%. To find more credible information, UWatMDS_BMF_C95 (which uses a credibility filter with a 95% threshold) performs slightly better than re-ranking methods (e.g. UWatMDS_BM25_Z) and manual runs. When considering a document to be valid only if it is relevant, correct, and credible (column "All"), all runs improve over the baseline, with most contribution is made by UWatMDS_BMF_C95 with 36% improvement, though none of the differences are statistically significant at 5% significant level.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,80.70,340.66,450.60,8.02;6,80.70,351.62,450.61,8.02;6,80.70,362.58,450.61,8.02;6,80.70,373.54,102.26,8.02;6,115.74,107.99,383.01,215.55"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. HiCAL's<ref type="bibr" coords="6,140.88,340.66,10.72,8.02" target="#b1">[2]</ref> assessment interface modified to render HTML-based collections. Here, an example document on the topic "acupuncture for insomnia" is shown to the user. Users can judge the document using a three-level relevance scale provided on the top of the page. Once a user judges a document, HiCAL shows the next likely relevant document based on an active-learning classifier.</figDesc><graphic coords="6,115.74,107.99,383.01,215.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.87,106.39,434.26,230.76"><head>Table 1 .</head><label>1</label><figDesc>List of submitted runs and their description</figDesc><table coords="2,88.87,132.90,434.26,182.33"><row><cell>Run identifier</cell><cell>Type</cell><cell cols="2">Judging Precedence Description</cell></row><row><cell cols="2">UWaterMDS_BM25 Automatic</cell><cell>3</cell><cell>A baseline BM25 run based on Ansereni's default parameters</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(k1:0.9, b:0.4).</cell></row><row><cell cols="2">UWatMDS_BM25_ZS Automatic</cell><cell>1</cell><cell>SPAMSCORE &gt; 10, BM25 * (1+Z) ELSE 0</cell></row><row><cell cols="2">UWatMDS_BM25_Z Automatic</cell><cell>2</cell><cell>BM25 * (1+Z)</cell></row><row><cell cols="2">UWatMDS_BMZBS10 Automatic</cell><cell>3</cell><cell>IF SPAMSCORE &gt; 10, BM25 * (1+Z*2) ELSE 0</cell></row><row><cell cols="2">UWatMDS_BMF_C90 Automatic</cell><cell>4</cell><cell>IF P(Doc == NonCredible) &lt; 0.90, BM25 ELSE 0</cell></row><row><cell cols="2">UWatMDS_BMF_C95 Automatic</cell><cell>5</cell><cell>IF P(Doc == NonCredible) &lt; 0.95, BM25 ELSE 0</cell></row><row><cell cols="2">UWatMDS_BMF_S30 Automatic</cell><cell>Other</cell><cell>IF SPAMSCORE &gt; 30 BM25, ELSE 0</cell></row><row><cell>UWatMDSBM25_HC1</cell><cell>Manual</cell><cell>1</cell><cell>Used HiCAL to find relevant documents, then return remaining</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unjudged BM25 documents.</cell></row><row><cell>UWatMDSBM25_HC2</cell><cell>Manual</cell><cell>2</cell><cell>Used HiCAL to find relevant documents, then return remaining</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unjudged BM25 documents, with documents ranking pushed</cell></row><row><cell></cell><cell></cell><cell></cell><cell>higher if HiCAL classifier also found the same document.</cell></row><row><cell>UWatMDSBM25_HC3</cell><cell>Manual</cell><cell>1</cell><cell>Used HiCAL to find relevant documents, then return remaining</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unjudged BM25 documents, with a document ranking pushed</cell></row></table><note coords="2,296.36,318.24,226.77,7.94;2,296.36,329.20,182.25,7.94"><p>if the classifier also found it. Here, the classifier was trained on a cleaner (only html body, no &lt;a&gt; tags) collection.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,80.70,106.39,450.61,264.33"><head>Table 2 .</head><label>2</label><figDesc>Mean average precision (map) and geometric map of the methods on our self-created tuning topics.</figDesc><table coords="5,116.43,132.94,379.15,109.47"><row><cell>Topic</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5*</cell><cell>All</cell><cell>All (gm_map)</cell></row><row><cell cols="7">Rel / Ret 9 / 1000 70 / 1000 17 / 1000 14 / 1000 77 / 1000 187 / 5000</cell><cell></cell></row><row><cell>BM25</cell><cell>0.160</cell><cell>0.377</cell><cell>0.065</cell><cell>0.016</cell><cell>0.514</cell><cell>0.226</cell><cell>0.126</cell></row><row><cell>BM25_Z</cell><cell>0.345</cell><cell>0.456</cell><cell>0.132</cell><cell>0.013</cell><cell>0.470</cell><cell>0.283</cell><cell>0.165</cell></row><row><cell cols="2">BM25_ZS 0.346</cell><cell>0.462</cell><cell>0.141</cell><cell>0.015</cell><cell>0.393</cell><cell>0.271</cell><cell>0.167</cell></row><row><cell>BMZBS10</cell><cell>0.336</cell><cell>0.463</cell><cell>0.140</cell><cell>0.014</cell><cell>0.384</cell><cell>0.268</cell><cell>0.165</cell></row><row><cell>BMF_C90</cell><cell>0.255</cell><cell>0.298</cell><cell>0.157</cell><cell>0.005</cell><cell>0.170</cell><cell>0.177</cell><cell>0.100</cell></row><row><cell>BMF_C95</cell><cell>0.362</cell><cell>0.302</cell><cell>0.141</cell><cell>0.004</cell><cell>0.146</cell><cell>0.191</cell><cell>0.097</cell></row><row><cell>BMF_S30</cell><cell>0.163</cell><cell>0.380</cell><cell>0.083</cell><cell>0.010</cell><cell>0.321</cell><cell>0.191</cell><cell>0.111</cell></row></table><note coords="5,80.70,268.16,450.60,8.96;5,80.70,279.22,450.61,9.81;5,80.70,291.18,450.41,9.81;5,80.75,303.13,72.69,9.81;5,153.69,303.13,25.05,10.64;5,178.99,303.13,344.83,10.64;5,94.30,318.13,290.74,9.81;5,94.30,330.09,290.74,9.81;5,94.30,342.98,225.01,8.83;5,80.70,361.76,129.78,8.96"><p>2.1.8 UWatMDS_BMF_C90, UWatMDS_BMF_C95 and UWatMDS_BMF_S30. When combined with relevance scores as filters, the logistic regression classifier's computed P(d âˆˆ NonCredible) and spam filter could also improve results in some cases. To determine parameters, we generated runs for SPAM âˆˆ {10, 20, . . . , 70} and P(d âˆˆ NonCredible) &lt; p 0 for p 0 âˆˆ {0.89, 0.90, . . . , 0.99}, and chose the following rules based on their performances: 4. UWatMDS_BMF_C90 = IF P(d âˆˆ NonCredible) &lt; 0.90 BM25, ELSE 0 5. UWatMDS_BMF_C95 = IF P(d âˆˆ NonCredible) &lt; 0.95 BM25, ELSE 0 6. UWatMDS_BMF_S30 = IF SPAM &gt; 30 BM25, ELSE 0 2.1.9 Test Performances. Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,80.27,106.39,452.42,239.22"><head>Table 3 .</head><label>3</label><figDesc>Performance of the runs under the track's evaluation measures. Bold values indicate highest value in the column. While the result here show that our baseline run UWaterMDS_BM25 (gray shaded) performs best in CAM, our analysis in Section 3.1 show that CAM may not reflect the true performance of runs with respect to the track's goal. Numbers in parentheses indicate percent of change over the baseline score.</figDesc><table coords="8,131.54,165.82,348.92,179.79"><row><cell>Run</cell><cell>MAP</cell><cell>nDCG@10</cell><cell>NLRE</cell><cell>CAM</cell></row><row><cell></cell><cell></cell><cell>Manual Runs</cell><cell></cell><cell></cell></row><row><cell cols="5">UWatMDSBM25_HC1 0.403 (+6.99%) 0.450 (-9.67%) 0.997 (+0.14%) 0.536 (-2.14%)</cell></row><row><cell cols="5">UWatMDSBM25_HC2 0.391 (+3.91%) 0.450 (-9.67%) 0.998 (+0.18%) 0.534 (-2.57%)</cell></row><row><cell cols="5">UWatMDSBM25_HC3 0.411 (+9.14%) 0.450 (-9.67%) 0.998 (+0.25%) 0.539 (-1.66%)</cell></row><row><cell></cell><cell></cell><cell>Automatic Runs</cell><cell></cell><cell></cell></row><row><cell>UWatMDS_BM25_Z</cell><cell cols="4">0.345 (-8.40%) 0.443 (-11.15%) 0.997 (+0.10%) 0.547 (-0.18%)</cell></row><row><cell cols="5">UWatMDS_BM25_ZS 0.310 (-17.51%) 0.430 (-13.72%) 0.997 (+0.11%) 0.510 (-6.96%)</cell></row><row><cell cols="5">UWatMDS_BMF_C90 0.156 (-58.50%) 0.425 (-14.78%) 0.999 (+0.33%) 0.309 (-43.60%)</cell></row><row><cell cols="5">UWatMDS_BMF_C95 0.170 (-54.86%) 0.445 (-10.75%) 0.999 (+0.33%) 0.334 (-39.04%)</cell></row><row><cell cols="5">UWatMDS_BMF_S30 0.285 (-24.15%) 0.500 (+0.28%) 0.998 (+0.20%) 0.456 (-16.74%)</cell></row><row><cell cols="5">UWatMDS_BMZBS10 0.283 (-24.89%) 0.392 (-21.36%) 0.997 (+0.13%) 0.492 (-10.21%)</cell></row><row><cell>UWaterMDS_BM25</cell><cell>0.376</cell><cell>0.499</cell><cell>0.996</cell><cell>0.548</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,80.46,106.39,450.85,250.18"><head>Table 4 .</head><label>4</label><figDesc>MAP result using relevance, correctness, and credibility assessments separately. "All" indicate MAP score by enforcing documents to be considered valid only if it is relevant, correct and credible. UWaterMDS_BM25 (gray shaded) is our baseline run. Bold values indicate highest value in the column for manual and automatic runs. Numbers in parentheses indicate percent of change over the baseline score. Statistical significance over the baseline is computed for bolded values (* indicates statistical significance at p&lt;0.05).</figDesc><table coords="9,112.54,176.78,386.93,179.79"><row><cell>Run</cell><cell>Relevance</cell><cell>Correctness</cell><cell>Credibility</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell>Manual Runs</cell><cell></cell><cell></cell></row><row><cell cols="2">UWatMDSBM25_HC1 0.403 (+6.99%)</cell><cell>0.167 (+27.49%)</cell><cell>0.283 (+10.07%)</cell><cell>0.128 (+32.47%)</cell></row><row><cell cols="5">UWatMDSBM25_HC2 0.391 (+3.91%) UWatMDSBM25_HC3 0.411 (+9.14%) 0.171 (+31.01%)  *  0.289 (+12.49%) 0.131 (+36.20%) 0.163 (+24.81%) 0.277 (+7.86%) 0.125 (+30.08%)</cell></row><row><cell></cell><cell></cell><cell>Automatic Runs</cell><cell></cell><cell></cell></row><row><cell cols="2">UWatMDS_BM25_Z 0.345 (-8.40%)  *</cell><cell>0.122 (-6.58%)</cell><cell cols="2">0.306 (+19.06%)  *  0.119 (+23.86%)</cell></row><row><cell cols="2">UWatMDS_BM25_ZS 0.310 (-17.51%)</cell><cell>0.113 (-13.25%)</cell><cell>0.291 (+13.22%)</cell><cell>0.117 (+21.47%)</cell></row><row><cell cols="2">UWatMDS_BMF_C90 0.156 (-58.50%)</cell><cell>0.074 (-43.42%)</cell><cell>0.202 (-21.28%)</cell><cell>0.095 (-1.87%)</cell></row><row><cell cols="2">UWatMDS_BMF_C95 0.170 (-54.86%)</cell><cell>0.080 (-39.13%)</cell><cell>0.217 (-15.75%)</cell><cell>0.101 (+5.08%)</cell></row><row><cell cols="2">UWatMDS_BMF_S30 0.285 (-24.15%)</cell><cell>0.108 (-17.38%)</cell><cell>0.241 (-6.26%)</cell><cell>0.100 (+3.84%)</cell></row><row><cell cols="2">UWatMDS_BMZBS10 0.283 (-24.89%)</cell><cell>0.104 (-20.44%)</cell><cell>0.275 (+7.08%)</cell><cell>0.111 (+14.73%)</cell></row><row><cell>UWaterMDS_BM25</cell><cell>0.376</cell><cell>0.131</cell><cell>0.257</cell><cell>0.096</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,80.46,370.12,451.01,250.18"><head>Table 5 .</head><label>5</label><figDesc>nDCG@10 result using relevance, correctness, and credibility assessments separately. "All" indicate nDCG@10 score by enforcing documents to be considered valid only if it is relevant, correct and credible. UWaterMDS_BM25 (gray shaded) is our baseline run. Bold values indicate highest value in the column for manual and automatic runs. Numbers in parentheses indicate percent of change over the baseline score. No statistical significance (at significance level of 0.05) was found over baseline. Fig.3. MAP result using relevance, correctness, and credibility assessments separately (c.f. Table4). Halo circles indicate manual runs.</figDesc><table coords="9,119.43,440.51,373.14,179.79"><row><cell>Run</cell><cell>Relevance</cell><cell>Correctness</cell><cell>Credibility</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell>Manual Runs</cell><cell></cell><cell></cell></row><row><cell cols="2">UWatMDSBM25_HC1 0.450 (-9.79%)</cell><cell>0.232 (+23.76%)</cell><cell>0.420 (+6.06%)</cell><cell>0.180 (+33.48%)</cell></row><row><cell cols="2">UWatMDSBM25_HC2 0.450 (-9.79%)</cell><cell>0.232 (+23.76%)</cell><cell>0.420 (+6.06%)</cell><cell>0.180 (+33.48%)</cell></row><row><cell cols="2">UWatMDSBM25_HC3 0.450 (-9.79%)</cell><cell>0.232 (+23.76%)</cell><cell>0.420 (+6.06%)</cell><cell>0.180 (+33.48%)</cell></row><row><cell></cell><cell></cell><cell>Automatic Runs</cell><cell></cell><cell></cell></row><row><cell>UWatMDS_BM25_Z</cell><cell>0.443 (-11.28%)</cell><cell>0.180 (-4.06%)</cell><cell cols="2">0.452 (+14.04%) 0.175 (+29.92%)</cell></row><row><cell cols="2">UWatMDS_BM25_ZS 0.430 (-13.84%)</cell><cell>0.181 (-3.31%)</cell><cell cols="2">0.446 (+12.60%) 0.177 (+31.25%)</cell></row><row><cell cols="2">UWatMDS_BMF_C90 0.425 (-14.90%)</cell><cell>0.193 (+3.15%)</cell><cell cols="2">0.444 (+12.12%) 0.178 (+32.37%)</cell></row><row><cell cols="2">UWatMDS_BMF_C95 0.445 (-10.88%)</cell><cell cols="3">0.201 (+7.26%) 0.463 (+16.86%) 0.183 (+36.01%)</cell></row><row><cell cols="4">UWatMDS_BMF_S30 0.500 (+0.14%) 0.209 (+11.59%) 0.426 (+7.62%)</cell><cell>0.161 (+19.52%)</cell></row><row><cell cols="3">UWatMDS_BMZBS10 0.392 (-21.47%) 0.163 (-12.76%)</cell><cell>0.418 (+5.50%)</cell><cell>0.162 (+20.49%)</cell></row><row><cell>UwaterMDS_BM25</cell><cell>0.499</cell><cell>0.187</cell><cell>0.396</cell><cell>0.135</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,83.93,646.60,119.72,7.06"><p>https://github.com/castorini/anserini</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,84.07,646.98,422.81,7.06"><p>For example a session may start with scoliosis, immediately change into otitis media or dementia and visit the full range of topics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,84.07,646.68,281.71,7.06"><p>Spam scores are available at https://www.mansci.uwaterloo.ca/~msmucker/cw12spam/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs> (<rs type="grantNumber">RGPIN-2014-03642</rs>), and in part by the <rs type="funder">University of Waterloo</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gw2K2zb">
					<idno type="grant-number">RGPIN-2014-03642</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,94.06,384.31,437.24,7.06;11,94.06,394.23,291.98,7.13" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,173.67,394.27,178.68,7.06">UWaterlooMDS at the TREC 2018 Common Core Track</title>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amira</forename><surname>Ghenai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shahin</forename><surname>Rahbariasl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,366.43,394.23,15.69,7.13">TREC</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.06,404.24,437.24,7.06;11,94.06,414.16,437.24,7.13;11,93.58,424.12,297.60,7.13" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,500.28,404.24,31.02,7.06;11,94.06,414.20,106.47,7.06">A System for Efficient High-Recall Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210176</idno>
		<ptr target="https://doi.org/10.1145/3209978.3210176" />
	</analytic>
	<monogr>
		<title level="m" coord="11,213.97,414.16,317.33,7.13;11,93.58,424.12,33.00,7.13">The 41st International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;18)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1317" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.06,434.12,437.24,7.06;11,94.06,444.05,167.72,7.13" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,317.57,434.12,213.73,7.06;11,94.06,444.09,24.79,7.06">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Gordon V Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles La</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,124.82,444.05,66.08,7.13">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.06,454.05,437.24,7.06;11,94.06,463.97,437.24,7.13;11,94.06,473.93,120.67,7.13" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,359.25,454.05,172.06,7.06;11,94.06,464.01,205.32,7.06">The Positive and Negative Influence of Search Results on People&apos;s Decisions about the Efficacy of Medical Treatments</title>
		<author>
			<persName coords=""><forename type="first">Frances</forename><forename type="middle">A</forename><surname>Pogacar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amira</forename><surname>Ghenai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles La</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,313.64,463.97,217.66,7.13;11,94.06,473.93,66.50,7.13">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.06,483.90,158.06,7.13" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Wierzbicki</surname></persName>
		</author>
		<title level="m" coord="11,174.05,483.90,75.11,7.13">Web Content Credibility</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.06,493.90,438.47,7.06;11,94.06,503.82,438.11,7.13;11,94.06,513.79,381.34,7.13" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,113.54,503.86,179.13,7.06">UWaterlooMDS at the TREC 2017 Common Core Track</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angshuman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec26/papers/UWaterlooMDS-CC.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,306.77,503.82,184.25,7.13">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<meeting>The Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-15">2017. 2017. November 15-17, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
