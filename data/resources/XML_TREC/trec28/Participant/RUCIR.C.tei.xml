<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,224.06,83.69,166.08,15.44;1,179.62,103.61,253.26,15.44">Generative Adversarial Networks for face generation: A survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computing Machinery (ACM)</publisher>
				<availability status="unknown"><p>Copyright Association for Computing Machinery (ACM)</p>
				</availability>
				<date type="published" when="2022-03-31">2022-03-31</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amina</forename><surname>Kammoun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rim</forename><surname>Slama</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hedi</forename><surname>Tabia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tarek</forename><surname>Ouni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohmed</forename><surname>Abid</surname></persName>
						</author>
						<title level="a" type="main" coord="1,224.06,83.69,166.08,15.44;1,179.62,103.61,253.26,15.44">Generative Adversarial Networks for face generation: A survey</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Computing Surveys</title>
						<title level="j" type="abbrev">ACM Comput. Surv.</title>
						<idno type="ISSN">0360-0300</idno>
						<idno type="eISSN">1557-7341</idno>
						<imprint>
							<publisher>Association for Computing Machinery (ACM)</publisher>
							<date type="published" when="2022-03-31" />
						</imprint>
					</monogr>
					<idno type="MD5">EBC65215D05AD4B3B7B22F90641CE57E</idno>
					<idno type="DOI">10.1145/1122445.1122456</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>conversational search</term>
					<term>information retrieval</term>
					<term>reference resolution</term>
					<term>neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we give a brief overview of the RUCIR group's participation in the TREC 2019 Conversational Assistance Track. All our runs for the Conversational Assistance Track are on the full MS MARCO Conversational Search Sessions dataset and use the online Indri retrieval system hosted at CMU.</p><p>For the Conversational Assistance Track, our runs try to solve conversational retrieval problems from two directions: One is to improve the search results by modifying the user's current query, including query reference resolution and incorporate the information from user's history queries in the same session. Run 1, Run 2 and Run 4 use this method. The other direction is to design a neural network to model user's global search intent and current search intent to get the retrieval results and run3 uses this method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>document and requires the user to read and select. This kind of interaction makes the conversational retrieval and the traditional information retrieval task have significant differences, from the traditional unilaterally dominant interaction mode to the mutual interaction mode. On the one hand, the search engine gradually clarifies the user's query intention according to a series of queries provided by the user and returns a document more in line with the intention. On the other hand, the user also judges the search intention understood by the search engine according to the information fed back by the search engine, and adjusts the expression of the query according to the deviation between the understanding of the search engine and the actual intention, thereby providing a query that is more suitable for the true search intention.</p><p>Taking the query sequence in the actual session as an example, for the current query "What training is required for a PA", if you do not rely on the external knowledge base, it is difficult to understand what the PA here means. However, if you see "What is a physician assistant" and "physician assistant average salary" in the history query, it is not difficult to know that PA is "physician assistant". In this regard, the session-based retrieval model can also understand the meaning of the entity. In fact, the query of the conversational search will be more concise on the basis of this, for example, the current query is simplified to "What training is required", and even the previous query is simplified to "their average salary". At this time, only the first query has the word "physician assistant" which expresses the core search intent.Therefore, in the conversational information retrieval problem, it is particularly important to fully exploit the information of the user clicked document in the previous queries in the case where the amount of information queried by the user is limited.</p><p>On the other hand, the intent of all queries in a session-based information retrieval system does not vary much, but the query intent within the same session in a conversational information retrieval system can sometimes vary greatly. For example, the first query in a session is "What are the different types of macromolecules?", the second query is "Tell me about the characteristics of carbohydrates", and the sixth query is "Tell me about lipids". The six queries not only omit a lot of information as mentioned above, but also have a large difference from the intention of the second query. Therefore, the related information of the second query will be biased when The first occurrence of the query term in the passage 7</p><p>The last occurrence of the query term in the passage 8</p><p>The distance between the fist and the last occurrence of the query term in the passage applied to the sorting of the documents of the sixth query. To address this issue, a model needs to be designed to model the specific intent of the current query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CONVERSATIONAL ASSISTANT TRACK 2.1 Question Definition</head><p>Similar to the session-based information retrieval, the user query of the conversational information retrieval study is also derived from the same session. Define session history S = {q 1 , q 2 , ..., q t -1 }, the corresponding user history clicked documents as:</p><formula xml:id="formula_0" coords="2,98.84,411.73,149.84,14.12">D c = {{d 1i } n 1 i=1 , {d 2i } n 2 i=1 , ..., {d t -1i } n t -1 i=1 }</formula><p>n i indicates the number of clicked documents related to the ith query. Given current query q t and candidate document d c , we need to calculate the score of the candidate document: Score(q t , d c ) = p(d c |q t , S, D c ). Then sort the list of documents based on the score and return the results to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Run 1</head><p>Run 1 focuses on passage ranking based on the official rewritten queries which avoid coreference and conversational ambiguity. We design several text matching features manually and utilize learning to rank <ref type="bibr" coords="2,79.07,553.04,11.76,7.94" target="#b2">[3]</ref> framework. The features are shown in Table <ref type="table" coords="2,257.28,553.04,3.07,7.94" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Run 2</head><p>Run 2 focuses on query rewriting to solve coreference and ambiguity in the conversational setting. The inputs are original query and query generated by running AllenNLP <ref type="bibr" coords="2,195.08,613.82,12.84,7.94" target="#b0">[1]</ref> coreference resolution. We extract the most important keyword in the original query and combine it with AllenNLP processed query and keywords from query of the last turn to form a new query. We input the three types of query into Solr and obtain three sets of documents(top-k ranked by BM25). We extract top-k keywords from all documents by TF-IDF and combine them with the keywords of original query. Then we input the keywords set into Indri to get the final ranked documents list. The flowchart is shown in Figure <ref type="figure" coords="2,234.71,701.49,3.10,7.94" target="#fig_0">1</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Run 3</head><p>Run 3 uses a neural network model based on key-value memory network <ref type="bibr" coords="2,346.36,112.45,12.17,7.94" target="#b4">[5]</ref> and attentive kernel based method. The model is divided into three parts: model user's overall query intent, model user's current query specific intent and the introduction of statistical features. The overall structure of the model is shown in Figure <ref type="figure" coords="2,548.55,145.33,3.07,7.94" target="#fig_1">2</ref>. model user's overall query intent. The user's overall intent needs to be obtained through the history information of the session, especially the information from user clicked documents. But due to the distraction of the topic during the dialogue, not all historical clicked documents are related to the current query intent, so we use The key-value memory neural network to store historical information, wherein the historical query is used as a key and the document information is used as a value. It is thus possible to select document information that may be advantageous for understanding the current query intent by the degree of relevance of the historical query to the current query.</p><p>First of all, we need to represent the sentence in queries and documents. For query q = {w 1 , w 2 , ...w l }, w t represents the tth word in query. We use graph embedding <ref type="bibr" coords="2,472.84,306.86,10.68,7.94" target="#b5">[6]</ref> to get the representation of the word x t , then we use bi-LSTM <ref type="bibr" coords="2,481.86,317.82,10.68,7.94" target="#b1">[2]</ref> to get the hidden vector h t , which is the concatenation of hidden vectors from the forward LSTM and the backward LSTM model. Finally we get the representation of query q t by attention mechanism:</p><formula xml:id="formula_1" coords="2,360.05,369.79,155.70,43.13">r q = H q A T = l i=1 α i h i , H q = [h 1 , h 2 , ..., h l ] A = Softmax(v T H q W a ), A = [α 1 , α 2 , ..., α l ]</formula><p>Similarly, we can get the representation of all historical query R q = {r q 1 , r q 2 , ...r q t -1 }, the representation of all historical clicked documents</p><formula xml:id="formula_2" coords="2,361.74,443.95,149.92,14.12">R d = {{r d 1i } n 1 i=1 , {r d 2i } n 2 i=1 }, ..., {r d t -1i } n t -1</formula><p>i=1 }, the representation of current query r q t and the representation of candidate document r t d . All the historical clicked documents corresponding to the same historical query are averaged as the value matrix V stored in the key-value memory neural network:</p><formula xml:id="formula_3" coords="2,370.74,510.19,134.10,29.13">v i = 1 n i n i k =1 r d ik , V = [v 1 , v 2 , ..., v t -1 ]</formula><p>The corresponding key value matrix K is the representation of the historical query, it means K = R q = {r q 1 , r q 2 , ..., r q l -1 }. After calculating the representation r q t of the current query, in order to better understand the intent of the current query through the historical knowledge through the memory neural network, historical queries that similar to the current query are selected, and the memory units with the corresponding key are read, then the memory cells are represented by weighting the memory cells as r m :</p><formula xml:id="formula_4" coords="2,411.02,644.40,52.06,14.26">r m = V (r q t K) T</formula><p>Finally, the obtained memory vector representation interacts with the candidate document representation vector to measure the similarity between the candidate document and the user's overall query intent. The similarity is the first matching feature model user's current query specific intent. In the conversational retrieval problem, the user's query intention in the whole session is relatively scattered. Therefore, in the process of understanding the user's current query intent, it is impossible to fully refer to the similarity with the past query process, and also needs to analyze the difference between the current query and the historical query. Some words that appear frequently in historical queries, if they appear in the current query, may reflect the user's query intent, but these words have a relatively low amount of information relative to the current query, their filtering effect of the candidate document is often not as obvious as some new words. In response to this situation, we employs a kernel-based neural network model to model the current specific query intent.</p><p>Referring to the KNRM model <ref type="bibr" coords="3,175.39,494.89,9.37,7.94" target="#b6">[7]</ref>, our model interacts with the candidate document in a similar manner, but in order to emphasize the words representing the current specific intent, we introduce weight for each query word in the KNRM model, the weight a new measures the freshness of the corresponding word, which is calculated as follows:</p><formula xml:id="formula_5" coords="3,64.92,566.64,217.71,20.28">a new t = 1 -max x h i ∈X h (x h i W 2 x t ), a new = [a new 1 , a new 2 , ..., a new l ]</formula><p>X h indicates the representation of words that appear in the historical queries. The larger the value of a new t , the more the word x t represents the specific intent of the current query, that is, it contains a higher amount of information. In addition, the idf of the word can also reflect the amount of information contained in the word, so we add idf to the calculation of the weight:</p><formula xml:id="formula_6" coords="3,139.22,668.34,67.75,12.57">a new t = a new t * id f t</formula><p>In order to calculate the correlation between the current query and the candidate document, it is first necessary to interact with the word vector matrix to obtain the similarity matrix M:</p><formula xml:id="formula_7" coords="3,408.42,367.71,57.28,14.95">M i j = x q i W 3 x d T j</formula><p>Then, referring to the use of the kernel method in the KNRM model, multiple Gaussian kernels are used to calculate the similarity of word vectors under different distributions, and the we obtain kdimensional matching features ϕ(M) = [K 1 (M), K 2 (M), ...K k (M)]:</p><formula xml:id="formula_8" coords="3,387.34,438.61,101.48,51.03">K k (M) = i log(a new i F i ) F i = j exp(- (M i j -µ k ) 2 2σ 2 k )</formula><p>Finally, the obtained k-dimensional matching feature is passed through the fully connected layer to obtain the similarity between the current query and the candidate document:</p><formula xml:id="formula_9" coords="3,317.49,518.47,240.71,19.79">f 2 = tanh(W 4 ϕ(M)+ b).</formula><p>statistical features. In order to more directly measure the relationship of candidate documents to the entire query sequence, we introduce 114-dimensional statistical features. Considering that the first sentence of each document is often of a summary nature, the first 57-dimensional feature measures the relationship between the entire candidate document and the user query sequence, and the last 57-dimensional feature measures the the relationship between the first sentence of the candidate document and the user query sequence.</p><p>Among them, there are 3-dimensional feature associated with the basic statistical features of the document, 4-dimensional feature associated with the word frequency, 5-dimensional feature associated with the tf-idf value of the document, 4-dimensional feature associated with the standardized word frequency, 7-dimensional feature associated with the query word, and the vector similarity feature between the document and the query word has 24 dimensions. Here, the overall word vector of the document and the query has two calculation methods, one is to directly add all the word vectors, the other is to add the word vectors by using the idf of each word as the weight. Since we use word2vec <ref type="bibr" coords="4,215.65,426.61,10.60,7.94" target="#b3">[4]</ref> and graph embedding <ref type="bibr" coords="4,72.04,437.56,10.44,7.94" target="#b5">[6]</ref> to represent word vectors, there are 4 calculation methods for each similarity, and we use 6 similarity calculation methods. Therefore, the vector similarity feature between the document and the query word has a total of 24 dimensions. Finally, the current query words are respectively connected with the previous i queries to obtain 10 recombined queries, and the 10 recombination queries respectively calculate the BM25 value and the Rouge-L value with the document to obtain 20-dimensional features. Therefore, we obtain a total of 57×2 = 114 dimensions. We pass the 114-dimensional feature through a multi-layer perceptron to obtain a 1-dimensional matching feature f 3 . document ranking. Finally, we combine the three matching features obtained before and get the final matching score through the fully connected layer: Score(q t , d c ) = Leaky_ReLU(W 5 [f 1 ; f 2 ; f 3 ])</p><p>The model training process adopts the pairwise method, that is, a pair of documents in the candidate document sequence, the positive example document is d + c , and the negative example document is d - c , and the training goal is to make the score difference of the pair of documents as large as possible. The loss function is: L = max(0, 1 -Score(q t , d + c ) + Score(q t , d - c ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Run 4</head><p>Run 4 focuses on query rewriting to solve coreference and ambiguity in the conversational setting. The input is original query. We extract the most important keyword in the original query and combine it with keywords from query of the last turn to form a new query. We input the two types of query into Solr and obtain two sets of documents(top-k ranked by BM25) and combine them with documents retrieved by query of the last turn. We extract top-k keywords from all documents by TF-IDF and combine them with the keywords of original query. Then we input the keywords set into Indri to get the final ranked documents list. The flowchart is shown in Figure <ref type="figure" coords="4,379.60,506.06,3.20,7.94" target="#fig_0">1</ref>(b).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,204.94,324.46,202.12,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two methods for coreference resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,225.99,354.80,160.01,7.70"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Model Structure of Run 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,63.03,85.73,223.10,126.07"><head>Table 1 :</head><label>1</label><figDesc>Handcrafted features in this task.</figDesc><table coords="2,63.03,111.66,223.10,100.13"><row><cell>Index</cell><cell>Description</cell></row><row><cell>1</cell><cell>Covered query terms number</cell></row><row><cell>2</cell><cell>Covered query terms ratio</cell></row><row><cell>3</cell><cell>Passage length</cell></row><row><cell>4</cell><cell>Minimum length of passage which covers query</cell></row><row><cell></cell><cell>terms</cell></row><row><cell>5</cell><cell>Passage length normalized minimum length of pas-</cell></row><row><cell></cell><cell>sage which covers query terms</cell></row><row><cell>6</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,330.15,539.24,229.13,6.18;4,330.15,547.21,228.88,6.18;4,329.90,554.50,228.30,6.97;4,329.94,563.15,160.61,6.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,529.83,547.21,29.20,6.18;4,329.90,555.18,162.33,6.18">AllenNLP: A Deep Semantic Natural Language Processing Platform</title>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07640</idno>
		<ptr target="http://arxiv.org/abs/1803.07640" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,330.15,570.44,228.05,6.97;4,330.15,578.41,101.21,6.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,464.12,571.12,70.14,6.18">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,539.77,570.44,18.43,6.97;4,330.15,578.41,36.24,6.97">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,330.15,586.38,228.05,6.97;4,330.15,594.35,162.00,6.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,398.25,587.06,120.53,6.18">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,523.92,586.38,34.28,6.97;4,330.15,594.35,103.22,6.97">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,330.15,603.00,229.23,6.18;4,330.15,610.29,209.68,6.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,513.17,603.00,46.21,6.18;4,330.15,610.97,127.70,6.18">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,330.15,618.94,228.05,6.18;4,330.15,626.91,228.05,6.18;4,330.15,634.20,228.06,6.97;4,330.15,642.17,228.82,6.97;4,329.99,650.82,149.91,6.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,419.36,626.91,138.85,6.18;4,330.15,634.88,30.08,6.18">Key-Value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weston</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D16-1147/" />
	</analytic>
	<monogr>
		<title level="m" coord="4,372.81,634.20,185.40,6.97;4,330.15,642.17,56.18,6.97">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01">2016. 2016. November 1-4, 2016</date>
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,330.15,658.79,228.05,6.18;4,330.15,666.08,140.35,6.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,483.91,658.79,74.30,6.18;4,330.15,666.76,68.08,6.18">DeepWalk: online learning of social representations</title>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,410.42,666.08,11.46,6.97">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,330.15,674.73,229.13,6.18;4,330.15,682.02,226.23,6.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,330.15,682.70,158.86,6.18">End-to-End Neural Ad-hoc Ranking with Kernel Pooling</title>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,501.22,682.02,14.18,6.97">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
