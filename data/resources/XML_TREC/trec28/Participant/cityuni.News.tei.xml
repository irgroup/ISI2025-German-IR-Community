<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,213.30,72.06,185.41,12.90">DMINR at TREC News Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,226.79,111.33,69.84,8.64"><forename type="first">Sondess</forename><surname>Missaoui</surname></persName>
							<email>sondess.missaoui@city.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for HCI Design City</orgName>
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country>England, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.45,111.33,81.76,8.64"><forename type="first">Andrew</forename><surname>Macfarlane</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for HCI Design City</orgName>
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country>England, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.96,133.24,61.94,8.64"><forename type="first">Stephann</forename><surname>Makri</surname></persName>
							<email>stephann.makri.1@city.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for HCI Design City</orgName>
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country>England, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.86,133.24,103.18,8.64"><forename type="first">Marisela</forename><surname>Gutierrez-Lopez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for HCI Design City</orgName>
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country>England, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,213.30,72.06,185.41,12.90">DMINR at TREC News Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35F1A7D0D3E7AFAC087CF02C1AFBA740</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the use of the DMINR Named entity extraction and linking system in TREC 2019. The track entered for are: News track, involves both Background Linking and Entity Ranking. Our approach to each of these tasks draws on prior work done by City, University of London at the TREC conference. In the background linking task, we treated the problems as an adhoc search task, using Named Entities (NEs) from a set of documents identified in pseudo relevance feedback, and optimizing these using a Hill-Climbing algorithm to provide a set of related articles. In the Entity Ranking task, we compared an approach using the BM25 ranking method with a probabilistic model that uses Wikipedia data as a resource to rank entities. The probabilistic model utilises an entity modeling approach to disambiguate NEs. Then , it utilises a scoring model which uses the given entities to provide a score for them based on evidence from the news articles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our participation in the Background Linking and Entity Ranking tasks of the TREC 2019 News Track-the second competitive evaluation campaign for IR systems in adversarial online news domain. This track focused on two complementary tasks (i.e., Background Linking and Entity Ranking) that aim to help the user understand the context of a news story, wherever they are reading it. We focus on the users' understanding of a story in relation to key entities for that story.</p><p>The participants were asked to <ref type="bibr" coords="1,212.72,454.88,11.62,8.64" target="#b0">(1)</ref> recommend articles that provided context and background information for the given topic (i.e., query article) and (2) determine which entities are linkable for the given article and rank them in terms of "usefulness", i.e., according to their influence on user's understanding of the topic. The TREC Washington Post Collection, five years of articles (2012 -2017), were provided for the development and evaluation of systems that participated in this benchmarking activity.</p><p>In this year's News Track, we focus on both tasks and propose methods that use 'focused' Named Entities (NEs) as leads to find the background articles; and to rank provided entities.</p><p>More specifically, our proposed methods are based on the structure of news articles. Given, our experience in working in the journalism domain within DMINR project <ref type="foot" coords="1,299.43,551.29,3.49,6.05" target="#foot_0">1</ref> , we assert that a news article usually has a main story, often reported as an event that can be effectively summarised by the five Ws: Who, What, When, Where and Why. Many of the five Ws can be associated with appropriate Named Entities in the article. Moreover, an article encompasses different aspects that explain how the event happened. These aspects are represented through a set of concepts, artifacts, and entities. All this information is important in providing a background to the news story as well as for ranking NEs based on their "usefulness". In all the proposed methods, we focused in using most topical named entities among all entities in the query article and/or recommended articles. These focused named entities have proven to be useful for many natural language processing applications, including summarization, topic identification, and background linking <ref type="bibr" coords="1,520.28,636.64,15.27,8.64" target="#b17">[18]</ref>.</p><p>In the next section, we will explain our assumptions and our methods for identifying and using focused NEs for both News Track tasks.</p><p>2 Prior work at City: OKAPI and PLIERS at TREC City has long been associated with the TREC conference. In our work on the News Track, the prior work at City was reviewed in order to find appropriate methods to use in the News track tasks, and it was clear that there were some ideas that were highly useful in this context. This relates both to the OKAPI work <ref type="bibr" coords="2,403.62,129.28,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="2,417.14,129.28,12.45,8.64" target="#b14">15,</ref><ref type="bibr" coords="2,432.34,129.28,8.85,8.64" target="#b15">16</ref>] and MacFarlane's PhD work done with the PLIERS system <ref type="bibr" coords="2,215.94,141.23,10.79,8.64" target="#b8">[9,</ref><ref type="bibr" coords="2,228.97,141.23,11.83,8.64" target="#b9">10]</ref>. This work developed a series of hill-climbing algorithms (simple heuristic learning methods) to address the selective dissemination of information problem dealt with in the TREC routing <ref type="bibr" coords="2,528.39,153.19,11.62,8.64" target="#b5">[6]</ref> and filtering tracks <ref type="bibr" coords="2,149.82,165.14,10.58,8.64" target="#b6">[7]</ref>. These methods take the basic assumption that relevance assessments are available for a given topic, and that these relevant documents can be used to extract a set of useful terms to represent the users' profiles e.g. using the Robertson term selection value (RSV) to rank terms based on probabilistic techniques <ref type="bibr" coords="2,471.74,189.06,15.27,8.64" target="#b13">[14]</ref>. This simple technique can be enhanced by optimizing on the term set. The top 300 terms or so can be chosen, but this is very costly. There is, however, clear evidence that optimizing on the topic 20-30 terms is sufficient <ref type="bibr" coords="2,446.67,212.97,15.27,8.64" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hill Climbing algorithms</head><p>Once a set of terms has been chosen by the initial relevance feedback mechanism (or pseudo relevant feedback method), the terms set can be optimized in various ways; it is a good idea to start with a set of seed terms for the algorithm i.e. the top 3 ranked terms from in initial term selection stage. There are many ways of doing this using various machine learning techniques e.g. deep learning <ref type="bibr" coords="2,292.27,310.61,15.27,8.64" target="#b12">[13]</ref>, but there is evidence that Hill Climbers are much quicker than other schemes and provide the same level of retrieval effectiveness. MacFarlane et al. <ref type="bibr" coords="2,446.34,322.56,16.60,8.64" target="#b10">[11]</ref> compared Genetic Algorithms with Hill Climbers demonstrating this clearly. There are four algorithms that can be considered as follows:</p><p>1. Find Best (FB): An iteration examines the whole term set available and chooses the best term from that set.</p><p>Iterations are long and the method can be computationally expensive. 2. Choose First Positive (CFP): An iteration stops when a single term shows sufficient improvement on the set already chosen. In this method the number of iterations is increased, but each is quite quick in comparison. 3. Choose All Positive (CAP): All terms that improve the term set within one iteration are accumulated and kept in the terms set for the next iteration. In this method the iterations are much longer, but there are many few of them. 4. Choose Some Positive (CSP): This is a compromise between CFP and CAP. A maximum number of terms per iteration can be selected (e.g. 3) and the iteration stops and moves to the next iteration. Computationally this algorithm is a half-way house.</p><p>The most suitable choice depends on the context e.g. document collection, topics etc. There does not appear to be a single best algorithm: in OKAPI at TREC-3 CAP produced the best results <ref type="bibr" coords="2,397.59,493.93,15.27,8.64" target="#b15">[16]</ref>. However, FB was found to be the most useful with PLIERS at TREC-8 <ref type="bibr" coords="2,240.11,505.88,15.27,8.64" target="#b9">[10]</ref>. In each of these algorithms, different term selection strategies can be used within a given iteration: terms can be added only, terms can be deleted only -or terms can be added or removed. In practical terms, adding and/or removing terms is most useful as it allows more term combinations to be examined. Further schemes on weights can also be considered in conjunction with the term selection strategy (see section 2.2 below). Because the algorithms are heuristics, some form of stopping criteria needs to be set otherwise the algorithms may keep going forever (or until the computer crashes). Several criteria can be used including:</p><p>1. A maximum number of iterations for positive term additions (e.g. 3 iterations where results of adding terms were no better or worse). 2. A threshold for improvements on term addition has been reached (e.g. an improvement in evaluation score below 0.01). 3. The elapsed time for term selection as exceeded a given maximum length (e.g. 2 hours). 4. A maximum number of terms is set (e.g. 20 terms).</p><p>Any or all of these stopping criteria can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Term weighting schemes</head><p>In all the prior work at City, the underlying scheme used for term weighting was the BM25 model <ref type="bibr" coords="3,469.88,96.32,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="3,483.46,96.32,7.47,8.64" target="#b8">9,</ref><ref type="bibr" coords="3,493.73,96.32,12.45,8.64" target="#b9">10,</ref><ref type="bibr" coords="3,508.98,96.32,12.45,8.64" target="#b14">15,</ref><ref type="bibr" coords="3,524.23,96.32,11.83,8.64" target="#b15">16]</ref>. It is entirely possible that machine learning heuristics could be applied to learning the weightings for each term e.g. learning to rank <ref type="bibr" coords="3,139.66,120.23,10.58,8.64" target="#b1">[2]</ref>, but this would be even more time consuming. In practice the City work either used the BM25 weight or attempted to reduced or increase the term by a third when adding a given term to the term set. This simple scheme was more than sufficient for requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metrics for evaluation and optimization</head><p>A score needs to be generated in order to evaluate the improvement or otherwise of applying that term (adding, removing, and reweighting). A number of metrics were used at OKAPI at TREC 4 <ref type="bibr" coords="3,406.99,207.73,16.60,8.64" target="#b14">[15]</ref> -including average precision (AVEP), which proved to be the best predictor of results; AVEP improved the results on other metrics, but other metrics did not provide the same level of prediction. This work was carried about before the B-PREF and nDCG metrics were advocated, but such metrics could also be used for term selection optimization using any of the term selection algorithms outlined in section 2.1 above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Expansion for Background Linking: Using Named Entities as leads</head><p>In this section, we describe our approach to the Background Linking task in news domain. We developed a baseline system that does not rely on any external sources of evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head><p>Our method treats the Background Linking task as an adhoc search task, for which Named Entities (NEs) are extracted from the query article, and these are then used as leads to find background articles, i.e., retrieve and recommend relevant news stories.</p><p>As discussed above (see section 1), we assume that NEs within the query article represent core aspects of the news story. Therefore, the relatedness between a named entity and a background article is similar to the relevance concept in Information Retrieval (IR). In this method, recommending background articles is tackled as a retrieval problem. We extracted a set of useful NEs from the relevant documents and optimized on them for query expansion based ranking method <ref type="bibr" coords="3,138.07,460.63,15.27,8.64" target="#b16">[17]</ref>. In particular, we used pseudo-relevance feedback for each query article, i.e., topic, to extract a set of relevant NEs. Then, we used the NEs to expand the search query and retrieve relevant articles as a background, covering a diverse set of aspects of the original story.</p><p>As news articles may encompass many NEs, expanding the search query using all NEs will make the query too long. Therefore using it directly for retrieval might lead to noisy results <ref type="bibr" coords="3,372.02,508.63,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="3,386.43,508.63,7.19,8.64" target="#b7">8]</ref>. Given that, distinguishing focused NEs, topical NEs that are most representative of the meaning of the news article, is fundamental to expand the initial query and retrieve relevant results. A key question for us is therefore: how to identify the focused NEs from within the pseudo-relevance feedback?</p><p>We argue that this problem is a term selection optimization problem for query expansion in information retrieval and we tackle it using a heuristic learning method, i.e., hill-climbers <ref type="bibr" coords="3,351.32,568.59,15.27,8.64" target="#b10">[11]</ref>. This is a two-stage process: (i) apply the Robertson term selection value to rank NEs and extract a set of candidate named entities to represent the news story; and (ii) optimize on the NEs set using a "Find Best" extension of Hill Climber algorithm (see section 2).</p><p>In the first stage, we select a set of candidate named entities, that we define as "focused NEs", the NEs which maximize Robertson's Term Selection Value <ref type="bibr" coords="3,251.99,616.60,16.60,8.64" target="#b13">[14]</ref> using the following term selection scheme:</p><formula xml:id="formula_0" coords="3,267.07,638.56,269.06,9.65">T SV ei = R * W rsi (<label>1</label></formula><formula xml:id="formula_1" coords="3,536.13,638.88,3.87,8.64">)</formula><p>Where R is the number of pseudo-relevant documents, W rsi is the Robertson's weight given by:</p><formula xml:id="formula_2" coords="3,134.24,683.31,405.76,9.65">W rsi = log [((r + 0.5) * (N -n -R + r + 0.5)) / ((n -r + 0.5) * (R -r + 0.5))]<label>(2)</label></formula><p>where r is the number of pseudo-relevant documents where the entity e i occurs in; R is the number of pseudo-relevant document for the query article. n is the number of documents entity e i occurs in; N is the number of documents in the collection.</p><p>In the second stage, we ran the Hill climbers, on the set of top k entities selected based on T SV ei <ref type="bibr" coords="4,475.16,111.34,15.27,8.64" target="#b10">[11]</ref>. The fitness function used for Hill climber optimization was the standard TREC average precision measure (AVEP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted Run</head><p>Pre-processing of the datasets All of our methods largely rely on the Elasticsearch search engine <ref type="bibr" coords="4,471.64,172.41,16.60,8.64" target="#b11">[12]</ref> for indexing the dataset, i.e., the Washington Post collection provided by TREC.</p><p>In method, cityuni 1, we used the classic tokenizer, a grammar based tokenizer, for which words were lowercased but not stemmed. We treated all fields from the JSON documents separately. For the retrieval model, we relied on the default scoring method in Elasticsearch, which is BM25.</p><p>For Named Entities Recognition, there are many existing approaches including machine learning techniques e.g. Stanford NER <ref type="bibr" coords="4,132.31,244.14,10.58,8.64" target="#b2">[3]</ref>, but SpaCy demonstrated good performance in term of accuracy and run time in the datasets. We employed the SpaCy<ref type="foot" coords="4,157.32,254.42,3.49,6.05" target="#foot_1">2</ref> tool to automatically annotate the news story's text (including the title and all the contents fields).</p><p>Background Linking This method makes the basic assumption that relevance assessments are available for the given topic. However, this is not the case for 2019 Topics. To overcome this limitation, we followed a query formulation scheme using the topic's title. For each topic we employ the NEs extracted from the title as query Q, then, we retrieve a set of relevant documents that are used as relevance feedback. The top-m documents, with m = 10, provide a pool from which candidate query terms , i.e., in our case the query NEs, are chosen based on the term selection value following eq.1. Based on our previous research <ref type="bibr" coords="4,260.69,357.36,15.27,8.64" target="#b10">[11]</ref>, we determined that optimizing on a set of 30 NEs is sufficient for the value of K. Then, based on evidence from our previous work <ref type="bibr" coords="4,330.61,369.32,15.27,8.64" target="#b9">[10]</ref>, we applied the "Find Best" algorithm to choose the best NEs from the k elements. The algorithm started with the top three ranked NEs for each topic to calculate the mean average precision measure; Then, it selected a NE if it yielded an increase in mean average precision. For instance, we adopted a selection strategy where NEs are only added. Moreover, a maximum number of NEs equal to 20 was set as the stopping criteria.</p><p>Finally, NEs that were selected from relevance feedback, were employed to expand a query Q. A new query Q e is then formulated and used to retrieve background articles using Elasticsearch as the retrieval system. For the length of expanded query Q e , we set |Q e | = 25 (without considering the rank). After retrieving relevant documents, the final step is to filter the results, by removing all the articles from the "Opinion", "Letters to the Editor", or "The Post's View" categories, as labeled in the "kicker" field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entity Ranking Task</head><p>Given a topic with title and content that is annotated with a list of mentioned entities in the article, i.e., a set of entities E = e 1 , e 2 , ...e n , the task is to rank the predefined entities by importance for the news article. In this section, we describe the multiple approaches taken by the DMINR team towards the entity ranking task. Run 1, City ER1, is anchored in a BM25 based ranking methodology. While run 2, City ER2, is based on a probabilistic model that uses the Wikipedia Dump as external resources to rank the entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BM25 for Entities Ranking</head><p>In this method, we rank the set of predefined Named Entities based on how often they appear in each News story compared to all other NEs in the document, using the BM25 model. In particular, we rank the mentioned Named Entities, based on how much they are representative of the meaning of the News Article. We define these NEs as the key NEs that maximize the BM25 score.</p><p>As with the Background Linking task, no external sources of evidence were deployed. We followed the below steps:</p><p>-Pre-processing and indexing the datasets using Elasticsearch (i.e., same settings as the Background Linking task).</p><p>-Analyzing the input News story by performing a simple Named Entities Recognition algorithm. More precisely, extracting NEs using SpaCy. -Ranking the list of mentioned Named Entities against the documents (represented as a set of NEs) using BM25 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Wikipedia as a Knowledge Base for Entities Ranking</head><p>Our second method, operating on the index built using Elasticsearch, relies on the Wikipedia dump to rank the Named Entities. In particular, the Wikipedia dump is used as the Knowledge Base (KB) <ref type="bibr" coords="5,393.62,223.18,11.62,8.64" target="#b4">[5]</ref> to represent the list of mentioned entities.</p><p>Methodology Our approach assumed that the features to be used to rank entities also require knowledge on the entities semantic meaning and how this meaning is representative of the news story. We also assumed that entities are produced from a mixture of mention strings. Those mentions are highly relevant for matching an entity against a news article. For this reason, we developed a twofold-based approach: (i) an 'entity modeling' method (see algo.1) to map the entities to their Wikipedia Dump. (ii) a 'scoring model' that uses the entity model to score it against the news article text (more details are provided below). The 'entity model' is a mapping model that grounds entity mentions to their corresponding node in a Knowledge Base (i.e., Wikipedia Dump in this experiment), which allows the representation of an entity e through a set of terms/entities. The main motivation behind the 'entity modeling' method, is that entities can be referred to by many mention strings inside the news article, and the same mention string may be used to refer to multiple entities. Hence, the mapping method will identify information about entities that help to disambiguate them and spot them semantically within the news article.</p><p>The second step, the 'scoring model' represents a topic as a mixture over an underlying set of Named Entities probabilities using a Bayesian model. Furthermore, the NEs probabilities provide an explicit representation of the News article, which allows ranking according to their importance. In this approach, all our model's components are represented as a set of Named Entities. Formally, we define:</p><p>-Wikipedia Dump denoted as W iki is used as a Knowledge Base to enhance the entity semantic representation.</p><p>-A Named Entity is a sequence of n "similar" entities (i.e., semantically similar) extracted from its corresponding node in the Knowledge Base (i.e., Wikipedia Dump) and denoted by e, e ← S e where S e = s 1 , s 2 , ..., s n (i.e., list of similar entities extracted from W iKi), where s n is the nth entity in the sequence. -News article is a document, a sequence of m Named Entities denoted by D = e w1 , e w2 , ..., e wm Using these defined elements, we score a Named Entity e based on its probability to fit in the News article D, when it occurs in the Wikipedia Dump W iki. Formally:</p><formula xml:id="formula_3" coords="5,72.00,569.06,352.43,39.54">score(e, D) = P (D|e, W iki) = tf (e, D) * tf (W iki, D) |D|<label>(3)</label></formula><p>Where tf (e, D) denotes the frequency of e in D. Given, e ← S e , this frequency is cumulative for all s i ∈ S e . Formally:</p><formula xml:id="formula_4" coords="5,253.11,665.13,286.89,30.32">tf (e, D) = n i=1 tf (s i , D)<label>(4)</label></formula><p>Similarly, tf (W iki, D) in eq.3 denotes the co-occurence between D and wiki, it is the count of Named Entities in W iki that belongs to D, and</p><formula xml:id="formula_5" coords="6,189.19,87.11,350.81,51.21">|D| is total count of Named Entities in D tf (W iki, D) = p j=1 tf (t j ) , t j ∈ {D ∩ W iki}<label>(5)</label></formula><p>Submitted Run Run 2, City ER2, of which steps are summarized in Algorithms 1 and 2, shares the same initial steps with City ER1, i.e., the dataset pre-processing and named entities recognition scheme is identical. Each topic in the Entity Ranking task consists of a news article D from the Washington post, that is linked to a mentioned list of entities E. Moreover, each entity e ∈ E is linked to its Wikipedia identifier (as explained above) and modeled as described in the following algorithm 1.</p><p>Algorithm index W e</p><p>The W e is the Wiki. dump provided for e 3:</p><p>extract the set of Named Entities S We from W e 4:</p><p>Rank S We using the RSV (see eq. 1)</p><p>5:</p><p>extract S e set of top 20 NEs in S We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>e ← S e 7:</p><p>return S e 8: end procedure After the 'entity modelling' step is realised, we exploit the entities models in the Entities Ranking task as described in algorithm 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall results</head><p>At the time of writing, runs for other TREC News Track participants are not yet available. Hence, we just report results of our runs in the Entity Ranking and Background Linking tasks without any baseline comparison. The effectiveness of the submitted runs are reported in Tables <ref type="table" coords="6,256.17,623.85,4.98,8.64" target="#tab_2">1</ref> and<ref type="table" coords="6,282.63,623.85,3.74,8.64" target="#tab_3">2</ref>. We focus on nDCG@5 and P@5 as a primary effectiveness measures across all topics. For the Background Linking task, our method achieves a relatively low nDCG@5 score. This can be explained by the fact it relies mainly on pseudo-relevance feedback as relevance judgements are not not available for this year topics. To address this problem, we used NEs extracted from the topic's title to retrieve a pool for pseudo-relevance feedback. However, the title contains only few named entities, yielding shorter queries, losing the gist of the input topic. This arguably sacrifices some retrieval effectiveness.</p><p>We noticed that our second method City ER2 for the Entity Ranking task , which relies on the Wikipedia Dump outperforms the first method City ER1. This can be explained by the fact that the KB-based representation of NEs allows to semantically evaluate the NEs relevance within the news article. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Failure Analysis</head><p>The overall results of our methods in both tasks are close to the median performance of all submitted runs to the track in terms of nDCG@5. The main observation is that our hypothesis that using entity as a main feature for both tasks (Entity Ranking and Background Linking) seems to be supported. However, our runs perform poorly for some topics. To clarify in which cases our methods work the best, we performed a failure analysis. In particular, for each of our runs, we identify the topic for which the related method achieves the highest gain or loss in terms of nDCG@5 comparing to the median.</p><p>For the Entity Ranking task, our methods City ER1 and City ER2 achieve their highest nDCG@5 for over the same 7 topics as Shown in Fig <ref type="figure" coords="7,195.35,431.98,14.82,8.64">1(a)</ref>, where the biggest gain was in topic '852' with 31.2% over the median (0.76215). In the other hand, both methods achieve the highest loss comparing to the median with two different topics '839' and '878'.</p><p>(a) Topics for which our methods achieved the highest nDCG@5 (b) Topics for which our methods achieved the lowest nDCG@5 Fig. <ref type="figure" coords="7,89.44,586.79,7.75,8.64">1:</ref> A Comparison over the topics for which our methods achieved the highest and lowest nDCG@5 for Entity Ranking task For the Background Linking task, our method city uni1 achieves the highest gain comparing to the median over the topic '847' and '853' with more than 70% of gain. As shown in fig 2(b), the method was less accurate in many topics including 828, 832, 835, etc. In more in depth analysis, we observe that our methods suffer from biggest losses in topic that relatively has short title or title that encompass few Named Entities in them. This fact makes it harder for the method to create a pool of relevance feedback based on the topic title only.</p><p>(a) Topics for which our methods achieved the highest nDCG@5 (b) Topics for which our methods achieved the lowest nDCG@5 Fig. <ref type="figure" coords="8,89.44,174.46,3.88,8.64">2</ref>: A Comparison over the topics for which city uni1 method achieved the highest and lowest nDCG@5 for Background Linking task All our proposed methods for both tasks are heavily based on the basic assumption that relevance assessments are available for a given topic, and that these relevant documents can be used to extract a set of useful Named Entities to represent the topic. However, given the evaluation results and the analysis we made, we can argue that our methods could perform better in the case of having a real pseudo-relevance feedback. For this reason, we are planning to conduct further experiments with this year TREC results to re-investigate the accuracy of our three methods comparing to some other baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this year's TREC News Track, we investigated using Named Entities in different ways to find background news articles and to rank the mentioned list of entities. For the Background Linking task, we experimented with a simple heuristic learning method that has been optimized on an initial set of entities to retrieve background documents. Most notably, our enhancement for efficiency shows that the computational complexity of the Hill Climber approach can be reduced with using NEs instead of terms.</p><p>We approached the Entity Ranking task using two different methods; Named Entities were used in both and the Wikipedia Dump was the key success for the City ER2 run.</p><p>To this end, we adapted the state-of-the-art BM25 algorithm along with our previous work in TREC. Overall our adaptation is promising, as all our runs yield reasonable results.</p><p>Our methods are heavily based on the basic assumption that relevance assessments are available for a given topic. An in depth analysis of our adapted methods and results has shown that using the topic's title to create a pool of relevance feedback may arguably sacrifices some retrieval effectiveness. To overcome this problem, we are planning a future experimentation using the real pseudo-relevance feedback provided by News Track for 2019 topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,163.43,141.51,285.15,58.14"><head>Table 1 :</head><label>1</label><figDesc>nDCG@5 and P@5 for submitted runs for Entity Ranking task</figDesc><table coords="7,255.50,153.81,101.01,45.83"><row><cell>Run</cell><cell cols="2">nDCG@5 P@5</cell></row><row><cell cols="2">City ER1 0.5158</cell><cell>0.4115</cell></row><row><cell cols="2">City ER2 0.5582</cell><cell>0.4538</cell></row><row><cell cols="2">median 0.537</cell><cell>0.4326</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,178.97,256.50,254.06,31.44"><head>Table 2 :</head><label>2</label><figDesc>nDCG and Precision for the Background Linking Runs</figDesc><table coords="7,256.50,268.80,99.01,19.13"><row><cell>Run</cell><cell cols="2">nDCG@5 P@5</cell></row><row><cell cols="2">city uni1 0.3251</cell><cell>0.4982</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,81.96,684.11,315.72,7.93"><p>A Google News Initiative (DNI) funded R&amp;D project(see https://blogs.city.ac.uk/dminr)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,81.96,684.27,56.33,7.77"><p>https://spacy.io/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,93.58,127.10,446.42,8.82;9,93.58,139.06,169.26,8.82" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangji</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Williams</surname></persName>
		</author>
		<title level="m" coord="9,414.12,127.10,125.89,8.82;9,93.58,139.06,74.81,8.59">Okapi at trec-5. NIST SPECIAL PUBLICATION SP</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="143" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,151.19,446.42,8.64;9,93.58,162.97,446.42,8.82;9,93.58,174.92,162.43,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,112.47,163.15,157.43,8.64">Learning to rank using gradient descent</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ari</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicole</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><forename type="middle">N</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,289.15,162.97,250.85,8.59;9,93.58,174.92,77.84,8.59">Proceedings of the 22nd International Conference on Machine learning (ICML-05)</title>
		<meeting>the 22nd International Conference on Machine learning (ICML-05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,187.06,446.42,8.64;9,93.58,198.83,446.42,8.82;9,93.58,210.79,368.07,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,351.85,187.06,188.15,8.64;9,93.58,199.01,182.33,8.64">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,298.59,198.83,241.40,8.59;9,93.58,210.79,99.89,8.59">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,222.75,443.77,8.82;9,93.58,234.70,191.62,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,260.27,222.92,166.20,8.64">Information retrieval with verbose queries</title>
		<author>
			<persName coords=""><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,434.00,222.75,103.34,8.59;9,93.58,234.70,95.31,8.59">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="209" to="354" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,246.83,446.42,8.64;9,93.58,258.61,234.84,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,439.93,246.83,100.07,8.64;9,93.58,258.79,57.81,8.64">Evaluating entity linking with wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,159.15,258.61,83.03,8.59">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="130" to="150" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,270.57,446.42,8.82;9,93.58,282.70,22.42,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,171.83,270.57,282.21,8.82">Overview of the third text retrieval conference (TREC-3). Number 500</title>
		<author>
			<persName coords=""><forename type="first">Donna K</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>DIANE Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,294.48,446.42,8.82;9,93.58,306.61,52.30,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,170.87,294.48,339.79,8.82">The trec-7 filtering track: description and analysis. NIST SPECIAL PUBLICATION SP</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="45" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,318.39,446.42,8.82;9,93.58,330.34,446.42,8.59;9,93.58,342.30,47.32,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,198.90,318.57,272.31,8.64">Paragraph as lead -finding background documents for news articles</title>
		<author>
			<persName coords=""><forename type="first">Kuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,490.65,318.39,49.35,8.59;9,93.58,330.34,192.69,8.59">Proceedings of the Twenty-Seventh Text REtrieval Conference</title>
		<meeting>the Twenty-Seventh Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11-14">2018. November 14-16, 2018, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,354.25,446.42,8.82;9,93.58,366.21,60.60,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,371.63,354.43,53.09,8.64">Pliers at vlc2</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,436.22,354.25,103.78,8.59">NIST Special Publication</title>
		<imprint>
			<biblScope unit="page" from="500" to="242" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,378.16,446.42,8.82;9,93.58,390.12,60.60,8.82" xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,369.96,378.16,170.04,8.82">NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="500" to="246" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Pliers at trec</note>
</biblStruct>

<biblStruct coords="9,93.58,402.25,446.42,8.64;9,93.58,414.03,385.20,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,378.10,402.25,161.90,8.64;9,93.58,414.21,183.54,8.64">An experimental comparison of a genetic algorithm and a hill-climber for term selection</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Secker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Timmis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,284.49,414.03,100.78,8.59">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="531" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,425.98,269.52,8.82" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,150.69,425.98,93.46,8.59">ElasticSearch cookbook</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Paro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,438.12,446.42,8.64;9,93.58,449.89,446.42,8.82;9,93.58,461.85,178.18,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,223.48,450.07,283.56,8.64">A survey on deep learning: Algorithms, techniques, and applications</title>
		<author>
			<persName coords=""><forename type="first">Samira</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saad</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yilin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haiman</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yudong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><forename type="middle">Presa</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mei-Ling</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shu-Ching</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,519.27,449.89,20.73,8.59;9,93.58,461.85,109.12,8.59">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,473.80,443.42,8.82" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,183.78,473.98,151.07,8.64">On term selection for query expansion</title>
		<author>
			<persName coords=""><surname>Stephen E Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,342.71,473.80,100.78,8.59">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,485.76,446.42,8.82;9,93.58,497.71,446.42,8.82;9,93.58,509.85,43.45,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,429.80,485.94,57.61,8.64">Okapi at trec-4</title>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alison</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,503.94,485.76,36.06,8.59;9,93.58,497.71,169.19,8.59">Proceedings of the fourth text retrieval conference</title>
		<meeting>the fourth text retrieval conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="73" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,521.80,446.42,8.64;9,93.58,533.58,213.35,8.82" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,515.65,521.80,24.35,8.64;9,93.58,533.76,31.73,8.64">Okapi at trec-3</title>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,132.57,533.58,107.66,8.59">Nist Special Publication Sp</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,545.53,446.42,8.82;9,93.58,557.67,22.42,8.64" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="9,173.24,545.53,173.39,8.59">Query Expansion for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="2254" to="2257" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.58,569.45,446.42,8.82;9,93.58,581.40,446.42,8.59;9,93.58,593.53,114.72,8.64" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,243.74,569.62,228.83,8.64">Focused named entity recognition using machine learning</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,490.65,569.45,49.35,8.59;9,93.58,581.40,442.79,8.59">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
