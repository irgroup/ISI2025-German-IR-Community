<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.73,115.96,288.29,12.62">Poznań Contribution to TREC-PM 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.74,153.71,72.33,8.74"><forename type="first">Artur</forename><surname>Cieślewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Poznań University of Medical Sciences</orgName>
								<address>
									<addrLine>Collegium Maius, Fredry 10</addrLine>
									<postCode>61-701</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.63,153.71,78.05,8.74"><forename type="first">Jakub</forename><surname>Dutkiewicz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Poznań University of Technology</orgName>
								<address>
									<addrLine>Plac Marii Sk lodowskiej-Curie 5</addrLine>
									<postCode>60-965</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.61,153.71,78.53,8.74"><forename type="first">Czes</forename><surname>Law Jȩdrzejek</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Poznań University of Technology</orgName>
								<address>
									<addrLine>Plac Marii Sk lodowskiej-Curie 5</addrLine>
									<postCode>60-965</postCode>
									<settlement>Poznań</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.73,115.96,288.29,12.62">Poznań Contribution to TREC-PM 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">057B4835520567619CB65147F5559B09</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Query Expansion</term>
					<term>Word Embedding</term>
					<term>Learning to rank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes Poznań contribution to the Precision Medicine track of the TREC 2019. In this submission we present several novelties. We cover the motivation for the hand-picked values of the weights assigned to the expanded query terms. We propose a result fusion method -slightly modified version of Borda Count algorithm. Additionally we use a learning to rank environment, we analyze the effectiveness of such an approach in combination with our other methods and analyze the achieved results. We also discuss our dedicated document processing methods. We achieve an improvement of up to 0.02 (infNDCG measure) over the baseline for Clinical Trials with our proposed methods, however the evaluation value of our baseline is much lower than the median of all contributions. The reverse effect happens in the Scientific Abstracts task, the baseline we propose is much stronger than the median, but the default setting of learning to rank proposition lowers the overall evaluation score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Information Retrieval(IR) task in this setting is defined as follows: given a set of documents, return a list of documents sorted by relevance of each document to the given query. TREC-PM <ref type="bibr" coords="1,243.91,517.67,15.87,8.74" target="#b4">[5]</ref>[6] is a specific track, which evaluates the systems performing the IR tasks on a specific sets of documents and queries. Here, each query is defined as a description of a potential patient. It is divided into several fields: disease, related gene and its properties, such as variant or function; the query also contains the demographical description of a patient given by his age and gender. There are two document collections, one which consists of scientific abstracts of medical publication, and a second one which consists of formalized descriptions of clinical trials. We propose several improvements to the classical approach if executing an IR task. Namely, we propose a method of expanding a query with terms, which are similar to the query terms. We calculate the similarity between various terms upon the word embedding space. We use a Borda Count based method for combining retrieved sets of documents by various systems. In this submission we also test the default setting of the learning to rank environment. We describe the system architecture and discuss the steps we take in order to produce the retrieved lists of documents. We specifically focus on our dedicated document processing methods. We provide a description of the query processing. We also describe the settings we use in the Terrier tool, which performs the IR task. Next, we define and describe the runs we submitted and finally discuss the evaluation values we received.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The TREC-PM task consists of two types of documents -Scientific Abstracts (SA) and Clinical Trials (CT). Each type has its unique features and we take advantage of it. Scientific Abstracts is a set of PubMed article abstracts. This is a large set, it consists of 27564896 documents, which we deemed valid and a total size of 235GB. We use an efficient C++ code<ref type="foot" coords="2,370.80,318.04,3.97,6.12" target="#foot_1">3</ref> in order to process the abstracts, dedicated scripts for specialized analysis and Terrier engine in order to perform the IR task <ref type="foot" coords="2,234.09,341.95,3.97,6.12" target="#foot_2">4</ref> .</p><p>As the document collection for Clinical Trials is fairly smaller, we use a classical XML parser. As we process the documents, we put parts of its contents into an SQL database. We use a word embedding space in order to expand the queries with specific terms. We use the learning to ranking environment in order to enhance the quality of the system. Finally, various results are aggregated with a Borda count method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scientific Abstracts</head><p>This section contains details of implementation of retrieval setup used for the Scientific Abstracts task. It consists of three major parts:</p><p>-Document processing -Query processing -Retrieval Document processing. In order to process the set of documents, we use a fast, dedicated XML processor. The program starts by creating a specified number of processing threads. Each thread receives an output file name, a set of input file names and a list of key tags it is supposed to store. Program iteratively progresses trough the file and seeks the tags with names, which are on the list of key tags. Once it encounters the document tag, it creates an empty map, which consists of pairs (key feature, empty string). The program starts to retrieve data from this specific document. When a key tag is encountered, it starts to save its content into a proper map field until it encounters the same closing tag. Once the parser encounters closing /document tag, it stores the contents of processed document into a specified file. It allows us to process documents fast, but we omit the syntax checking feature of the parser -we assume all of the files have correct syntax. We use the following key tags:</p><formula xml:id="formula_0" coords="3,140.99,211.56,90.47,69.72">-PMID -ArticleTitle -AbstractText -Keyword -NameOfSubstance -DescriptorName</formula><p>If a document contains at least two of the mentioned tags (PMID and one additional tag) it is deemed valid. Optionally we can assign the parser to skip documents with specific tags. In this particular case, we skip documents which contain CommentsCorrectionsList tag, as it indicates that a document directly references another document (e.g. it is a comment, erratum or a technical note on another document).</p><p>Query processing There is one set of queries for both the Scientific Abstracts task and Clinical Trials task. Each query consists of three fields -disease name, related genes and their description (such as variant or gene function) and patient demographics (gender and age). In hope to expand the queries for the Scientific Abstracts task we have retrieved the word embeddings using a classical Word2Vec <ref type="bibr" coords="3,199.03,443.80,10.52,8.74" target="#b3">[4]</ref> approach and a collection of Scientific Abstracts documents. We use a relatively simple idea of finding a query expansion candidate terms. For each term in the query we look for the most similar vectors in the word embedding space to the vector of this term. We use a cosine definition of similarity given by <ref type="bibr" coords="3,174.62,491.62,11.62,8.74" target="#b0">(1)</ref>.</p><formula xml:id="formula_1" coords="3,236.15,503.28,244.44,26.22">sim(w 1 , w 2 ) = i v 1i v 2i i v 2 1i i v 2 2i<label>(1)</label></formula><p>In (1) w 1 and w 2 relate to the words, which are being compares. Word embeddings collection Ω ∈ R N contains vectors v 1 ∈ Ω and v 2 ∈ Ω which are embeddings for the w 1 and w 2 words. In this setup we are able to find very interesting, new queries, however, the newly generated queries are not equivalent to the original query. This is due to the nature of a replaceability of a term.</p><p>The word embedding space we use is built upon the word contexts. Thus, words which appear in similar contexts are supposed to be similar. We wish those words to be semantically related as synonyms, but it's often not the case. In most cases the words are semantically parallel (e.g. name of a different type of cancer, name of gene which relate to a very similar, but different type of disease, name of gene which could be related to this disease, but the query does not ask for this gene).  <ref type="table" coords="4,163.11,253.44,4.13,7.89" target="#tab_0">1</ref>. Examples of the similarity check for the query terms. MS stands for the most similar, similarity value is given in brackets. In the examples we omit various inflection forms of the original term (usually it is a plural form of the same words).</p><p>Retrieval In order to perform the Retrieval we use the Terrier tool. We follow the standard procedure of indexing the processed documents. We also employ Terrier to generate the default learning to rank environment. In this environment we use the following features as input:</p><p>-BM25 calculated for each field in the document, -Length of each field in the document, -Proximity features: DFR dependence score and MRF dependence score.</p><p>We train the model on the TREC-PM 2018 data. We use this setup along with the divergence from randomness retrieval models:</p><formula xml:id="formula_2" coords="4,140.99,465.23,49.17,20.46">-LGD [1], -DPH [1],</formula><p>in order to create an output ranking. We use the created ranking lists to test our data Borda count based fusion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clinical Trials</head><p>We test a slightly different approach for the Clinical Abstracts task. This section contains the details of implementation of our method for this task. Document Processing As the document collection for Clinical Trials is fairly smaller, we use a classical XML parser. Thus, we examine the syntax of each document within the set. As we process the documents, we put parts of its contents into an SQL database. Specifically, we use the summary and description fields as the body of a document. We also take use of the demographics data stored within the documents. A complete list of processed tags is as follows:</p><p>brief title official title brief summary detailed description primary outcome</p><formula xml:id="formula_3" coords="5,140.99,178.98,95.78,80.78">-secondary outcome -condition -arm group -condition browse -intervention browse -keyword -criteria</formula><p>The documents to be indexed are generated with use of an automatic script. The script puts contents of each field into respectively body and the title of a document. We additionally split the criteria tag into two parts -exclusion and inclusion criteria. The inclusion criteria is put into the document body, while the exclusion criteria is put into a special "negative" tag. We additionally replace every term "KIT" in the collection with "gene kit" string. The difference between gene KIT and kits used for isolation of DNA or ELISA is easily recognizable with use of the upper cased letters. As the case is lost during the indexing and generation of the word embedding space, we differentiate "kit" from "KIT" at this level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Processing</head><p>We start the query processing by calculating the word embedding space. We use a corpus generated upon the Clinical Trials documents collection by concatenating of all the processed fields. Instead of searching for an expansion for each term in the query separately, we aggregate the similarities before judging whether a term is a valid expansion candidate. The aggregation is an average of similarities of all query terms to the potential expansion term. We choose the expansion terms with computed similarity above the threshold of 0.6. In the query processing section for Scientific Abstracts, we've shown that the expansion terms might distort the results as they change the sense of a query. We hypothesise, that adding terms to the query with much lower weight would improve the quality of a retrieval. This is due to the fact that terms added with much lower weight potentially wouldn't change the order of ranking for the top scoring documents, they would however add an information to the tail of the retrieved list. For example, if we look for documents about the Meningioma, and we find no such document within the entire collection, it's better to retrieve a document related to Ependymoma, than it is to retrieve a randomly selected document. Thus we add the new terms with the following weights:</p><p>-Expanded query term weight : 1; Original query term : 120 (used for the Borda Count fusion method) -Expanded query term weight : 1; Original query term : 140 (this is a default setting)</p><p>-Expanded query term weight : 1; Original query term : 160 (used for the Borda Count fusion method) -Expanded query term weight : 1; Original query term : 180 (used for the Borda Count fusion method)</p><p>Retrieval We use the Terrier system in order to create a document index. We retrieve the documents with BB2 retrieval model. We also use Terrier to implement the learning to rank environment. We train the model on data from TREC-PM 2018 and TREC-PM 2017. We use the same set of features as in the Scientific Abstracts task:</p><p>-BM25 calculated for each field in the document, -Length of each field in the document, -Proximity features: DFR dependence score and MRF dependence score.</p><p>Finally, we exclude the trials with inadequate description of demographics from the lists of retrieved documents,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results fusion</head><p>One can observe, that systems performance vary on the query it processes. If we took only the best system for each query, the averaged result would be much better than an average of a best system. Thus we believe it is reasonable to fuse the results obtained by various systems. We implement the Borda Count <ref type="bibr" coords="6,470.08,392.26,10.52,8.74" target="#b2">[3]</ref> function in order to retrieve the final ranking of documents. The Borda Count method we use is given by</p><formula xml:id="formula_4" coords="6,232.30,436.18,248.30,26.80">s C,Q (D) = t∈T k -r t,C,Q (D) log 2 (r t,C,Q (D) + 1)<label>(2)</label></formula><p>Final score s of a document D given collection of documents C and a query Q, is a sum of components produced by various systems t ∈ T . Here, the r t,C,Q (D) is a rank of a document D retrieved by system t given a query Q and a document collection C, and k is a size of the retrieved list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results Analysis</head><p>TREC-PM currently uses three evaluation measures:</p><p>inferred Normalized Discounted Cumulative Gain (infNDCG) -an inferred version of the normalized cumulative information gain. This measure takes into account the position of a document on the retrieved list <ref type="bibr" coords="6,418.65,608.44,9.96,8.74" target="#b6">[7]</ref>. -Precision at 10 (P@10) -proportion of relevant documents within the top ten retrieved documents <ref type="bibr" coords="6,251.85,632.28,13.81,8.74" target="#b1">[2]</ref>. -R-precision (R-prec) -precision at R, where R is a number of relevant documents within the collection for a given query <ref type="bibr" coords="6,349.27,656.12,12.56,8.74" target="#b1">[2]</ref>.</p><p>Often, the evaluation values are aggregated over the set of queries as an average. We believe this is a necessary operation when comparing systems head to head in order to pick a better one. However, we also believe, that this aggregation is a cause of an information loss. By averaging we lose an information of how the system performs on a various queries. We have prepared a set of tables, in which we can observe our system performs much better than the median on a subset of queries; as well as a subset of queries in which our system performed much worse. This issue is specifically disturbing for our version of Clinical Trials system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted runs</head><p>We submit a total of nine runs, four runs for Scientific Abstracts and five runs for Clinical Trials. The purpose of the Scientific Abstracts runs is to test our document processing methods as well as to test learning to rank environment and the Borda Count results fusion method:</p><p>1. SAsimpleLGD -a default setting retrieval. The purpose of this run is to test the quality of the document processing. We hypothesise that the LGD retrieval model is a very strong baseline. Thus we employ it to perform the retrieval. 2. SA LGD letor and SA DPH letor -these two runs use the default learning to rank environment, purpose of these runs is to test the quality of learning to rank with the default setting as well as create comparable data for the results fusion method. 3. SA bc -a run, which uses the Borda Count method in order to concatenate two retrieval rankings. The Borda Count is calculated upon the SA LGD letor and SA DPH letor runs.</p><p>We submit five runs for the Clinical Trials task. We use the BB2 retrieval model in all of them In addition to the goals described in the above section, the purpose of these runs is to test our hypothesis of improving the quality of retrieval by adding word embedding based terms with much lower weights.</p><p>1. simple -a default setting retrieval. We use its as a comparison. We hope to achieve results better than the ones generated by this run. 2. simple letor -a default setting run with learning to rank employed. We use no word embedding based expansion here. 3. w2v noletor -a run with word embedding based expansion terms. 4. w2v letor -a combination of learning to rank environment with word embedding based expansions. 5. bc -a run, which uses a default setting run without learning to run and four different runs with word embedding based expansion terms. A final result is a combination of results with the Borda Count function.</p><p>The aggregated results of those runs are presented in the table 2. We can see that for the Scientific Abstracts task, the baseline proved to be very strong.</p><p>We believe that our method of document processing, did well and it is one of the reasons of a decent evaluation value. Unfortunately the learning to rank environment did not work properly. Whether it is due to a wrong selection of the features -it requires further investigation. However, we observe that the Borda Count method did particularly well for this task.</p><p>As for the Clinical Trials, we can see that the baseline we chose is, contrary to the one picked for Scientific Abstracts, relatively weak. However, every enhancement to the baseline method we propose seems to improve the evaluation values. In particular, the word embedding based query expansion in combination with learning to rank environment seems to improve results by the largest margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Further analysis</head><p>We also provide a detailed "per-query" results. Figures <ref type="figure" coords="8,380.80,274.56,4.43,8.74" target="#fig_0">1</ref><ref type="figure" coords="8,385.23,274.56,4.43,8.74" target="#fig_1">2</ref><ref type="figure" coords="8,385.23,274.56,4.43,8.74" target="#fig_2">3</ref><ref type="figure" coords="8,385.23,274.56,4.43,8.74" target="#fig_3">4</ref><ref type="figure" coords="8,385.23,274.56,4.43,8.74" target="#fig_4">5</ref><ref type="figure" coords="8,389.65,274.56,4.43,8.74" target="#fig_5">6</ref>in the Appendix A illustrate specific results for each run. We observe that our proposal for the Scientific Abstracts correlates well with an a median submission. The runs perform well for easy queries and perform worse for harder queries. Runs with learning to rank implemented perform better for some queries (e.g. query no 12, 15, 25) but on average the simple version gives the best results. The Clinical Trials set of results is very different. We observe that the system we propose works really well for majority of the queries, however there are some queries for which it returns nonsensical lists of documents. In particular queries no 4,5,12,13,18,27,36 and 38 are problematic. If those queries were excluded the overall evaluation value would drastically go up. It is an issue which requires emergent investigation.</p><p>Note: how to read the figures The top row (labeled as 0) of each figure is an average of the remainder of results. Each figure is split into three parts. The middle part -a column labeled as Trec median -is a reference point. It is a median evaluation value for all submitted runs. The lower this value gets (the more it is red), the harder the query for an average system is. The left part of the figure illustrates how well did our system do on that query. The right part of the figure illustrates how well did our system do on that query compared to an average system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This submission highlights an important issue. Some systems vastly underperform in specific settings. There are two solutions to this problem. We could either formulate the description of such settings, so we know not to use those systems when these specific conditions are met. We suppose that the conditions would be easily described with use of the annotated set, which is not very useful, but we also believe there is a correlation between the features of annotated set (such as number of annotated relevant documents) and a shape of the retrieval score distribution. We plan to analyze this issue. The second way of solving this issue is to use a combination of methods. In particular, the proposed Borda Count method works fairly well and it improves the overall evaluation value. We have also examined two of our methods of document processing. The document processing proposed for Scientific Abstracts seem to work fairly well. The processing we proposed for the Clinical Trials requires further investigation, as it might be a cause of the low baseline score.</p><p>A Query specific results      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,134.77,514.35,345.83,7.89;11,134.77,525.33,345.83,7.86;11,134.77,536.29,345.82,7.86;11,134.77,547.25,222.60,7.86;11,134.77,162.75,369.90,336.83"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Query specific results for Clinical Trials. Evaluation measure: infNDCG. The first row, labeled as 0, is an average over all queries. Column labeled as diff1 is equal to Trec median minus simple letor. The following columns labeled as diff represent the difference between Trec median and the following runs.</figDesc><graphic coords="11,134.77,162.75,369.90,336.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,134.77,563.50,345.83,7.89;12,134.77,574.49,345.82,7.86;12,134.77,585.45,345.83,7.86;12,134.77,596.41,222.60,7.86;12,134.77,174.48,411.00,374.25"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Query specific results for Clinical Trials. Evaluation measure: P@10. The first row, labeled as 0, is an average over all queries. Column labeled as diff1 is equal to Trec median minus simple letor. The following columns labeled as diff represent the difference between Trec median and the following runs.</figDesc><graphic coords="12,134.77,174.48,411.00,374.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,134.77,563.50,345.82,7.89;13,134.77,574.49,345.82,7.86;13,134.77,585.45,345.83,7.86;13,134.77,596.41,222.60,7.86;13,134.77,174.48,423.00,374.25"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Query specific results for Clinical Trials. Evaluation measure: Rprec. The first row, labeled as 0, is an average over all queries. Column labeled as diff1 is equal to Trec median minus simple letor. The following columns labeled as diff represent the difference between Trec median and the following runs.</figDesc><graphic coords="13,134.77,174.48,423.00,374.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="14,134.77,572.50,345.82,7.89;14,134.77,583.49,345.83,7.86;14,134.77,594.45,345.83,7.86;14,134.77,605.41,305.69,7.86;14,134.77,165.48,384.00,392.25"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Query specific results for Scientific Abstracts Trials. Evaluation measure: infNDCG. The first row, labeled as 0, is an average over all queries. Column labeled as diff1 is equal to Trec median minus SA LGD letor. The following columns labeled as diff represent the difference between Trec median and the following runs.</figDesc><graphic coords="14,134.77,165.48,384.00,392.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="15,134.77,568.75,345.83,7.89;15,134.77,579.74,345.82,7.86;15,134.77,590.70,345.83,7.86;15,134.77,601.66,238.46,7.86;15,134.77,169.23,384.00,384.75"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Query specific results for Scientific Abstracts Trials. Evaluation measure: P@10. The first row, labeled as 0, is an average over all queries. Column labeled as diff1 is equal to Trec median minus SA LGD letor. The following columns labeled as diff represent the difference between Trec median and the following runs.</figDesc><graphic coords="15,134.77,169.23,384.00,384.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="16,134.77,568.75,345.82,7.89;16,134.77,579.74,345.82,7.86;16,134.77,590.70,345.83,7.86;16,134.77,601.66,238.46,7.86;16,134.77,169.23,384.00,384.75"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Query specific results for Scientific Abstracts Trials. Evaluation measure: Rprec. The first row, labeled as 0, is an average over all queries. Column labeled as diff1 is equal to Trec median minus SA LGD letor. The following columns labeled as diff represent the difference between Trec median and the following runs.</figDesc><graphic coords="16,134.77,169.23,384.00,384.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,118.99,345.83,142.33"><head>Table 1</head><label>1</label><figDesc>consists of several examples of this issue. After a qualitative analysis we decided to use only the original version of queries in the Scientific Abstracts task. However, using this technique, we expanded the queries for the Clinical Trials task.</figDesc><table coords="4,134.77,185.77,317.32,75.56"><row><cell>Query Term</cell><cell>MS</cell><cell>2nd MS</cell><cell>3rd MS</cell></row><row><cell cols="4">meningioma ependymoma (0.867) astrocytoma (0.858) chordoma (0.81)</cell></row><row><cell>KRAS</cell><cell>BRAF(0.881)</cell><cell>PIK3CA(0.874)</cell><cell>TP53(0.847)</cell></row><row><cell>V600E</cell><cell>BRAFV600E(0.830)</cell><cell>V600(0.790)</cell><cell>p.V600(0.789)</cell></row><row><cell>melanoma</cell><cell>SCCHN(0.719)</cell><cell>tumor(0.717)</cell><cell>cSCC(0.707)</cell></row><row><cell>E545K</cell><cell>H1047R(0.861)</cell><cell>G12V(0.788)</cell><cell>PI3KCA(0.754)</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,203.20,117.78,208.97,145.70"><head>Table 2 .</head><label>2</label><figDesc>Aggregated results for all submitted runs.</figDesc><table coords="9,228.98,117.78,157.40,134.79"><row><cell>Run name</cell><cell cols="2">infNDCG P@10 Rprec</cell></row><row><cell>SA DPH letor</cell><cell>0,45</cell><cell>0,50 0,28</cell></row><row><cell>SA LGD letor</cell><cell>0,45</cell><cell>0,51 0,27</cell></row><row><cell>SA bc</cell><cell>0,47</cell><cell>0,52 0,31</cell></row><row><cell>SA simple LGD</cell><cell>0,48</cell><cell>0,54 0,31</cell></row><row><cell>Trec Median</cell><cell>0,46</cell><cell>0,55 0,28</cell></row><row><cell>bc</cell><cell>0,47</cell><cell>0,44 0,34</cell></row><row><cell>simple</cell><cell>0,47</cell><cell>0,44 0,33</cell></row><row><cell>simple letor</cell><cell>0,48</cell><cell>0,44 0,35</cell></row><row><cell>w2v letor</cell><cell>0,48</cell><cell>0,42 0,35</cell></row><row><cell>w2v</cell><cell>0,47</cell><cell>0,44 0,33</cell></row><row><cell>Trec Median</cell><cell>0,51</cell><cell>0,47 0,35</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,144.73,656.80,326.42,7.86"><p>Supported by the PUT DS grant no 04/45/DSPB/0197 and 04/45/DSMK/0200.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,144.73,645.84,158.00,7.86"><p>https://github.com/dudenzz/myindex/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,144.73,656.80,40.54,7.86"><p>terrier.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,140.32,342.24,10.13;10,146.91,153.55,333.68,7.86;10,146.91,164.51,333.68,7.86;10,146.91,175.46,333.68,7.86;10,146.91,186.42,166.72,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,276.58,142.59,181.40,7.86">Information-based models for ad hoc IR</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<idno type="DOI">10.1145/1835449.1835490</idno>
		<ptr target="https://doi.org/10.1145/1835449.1835490" />
	</analytic>
	<monogr>
		<title level="m" coord="10,146.91,153.55,333.68,7.86;10,146.91,164.51,236.77,7.86">Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010</title>
		<meeting>eeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">July 19-23, 2010. 2010</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,197.38,342.24,7.86;10,146.91,208.34,191.54,7.86;10,356.37,208.34,124.22,7.86;10,146.91,219.30,64.75,7.86" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-39940-9486</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-39940-9" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page">486</biblScope>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,230.26,342.24,7.86;10,146.91,241.19,333.67,7.89;10,146.91,252.18,231.87,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,187.17,230.26,114.09,7.86">Rank aggregation methods</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1002/wics.111</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.111" />
	</analytic>
	<monogr>
		<title level="j" coord="10,312.85,230.26,167.74,7.86;10,146.91,241.22,86.36,7.86">Wiley Interdisciplinary Reviews: Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,263.14,342.24,7.86;10,146.91,274.09,333.68,7.86;10,146.91,285.05,333.68,7.86;10,146.91,296.01,278.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,331.23,263.14,149.36,7.86;10,146.91,274.09,86.75,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="m" coord="10,367.75,274.09,112.84,7.86;10,146.91,285.05,161.38,7.86;10,146.91,296.01,117.28,7.86">1st International Conference on Learning Representations, ICLR 2013</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">May 2-4, 2013. 2013</date>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct coords="10,138.35,306.97,342.24,7.86;10,146.91,317.93,333.68,7.86;10,146.91,328.89,333.67,7.86;10,146.91,339.85,333.68,7.86;10,146.91,350.81,333.68,7.86;10,146.91,361.77,241.57,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,169.06,317.93,212.37,7.86">Overview of the TREC 2018 precision medicine track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Lazar</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec27/papers/Overview-PM.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,198.42,328.89,248.83,7.86;10,422.89,339.85,57.71,7.86;10,146.91,350.81,58.27,7.86">Proceedings of the Twenty-Seventh Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Seventh Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11-14">2018. November 14-16, 2018. 2018</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 500-331</note>
</biblStruct>

<biblStruct coords="10,138.35,372.72,342.24,7.86;10,146.91,383.68,333.68,7.86;10,146.91,394.64,333.68,7.86;10,146.91,405.60,333.68,7.86;10,146.91,416.56,333.68,7.86;10,146.91,427.52,241.57,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,207.17,383.68,213.79,7.86">Overview of the TREC 2017 precision medicine track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pant</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec26/papers/Overview-PM.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,233.29,394.64,243.04,7.86;10,452.18,405.60,28.41,7.86;10,146.91,416.56,77.50,7.86">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>The Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-15">2017. November 15-17, 2017. 2017</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 500-324</note>
</biblStruct>

<biblStruct coords="10,138.35,438.48,342.24,7.86;10,146.91,449.44,333.68,7.86;10,146.91,460.40,333.68,7.86;10,146.91,471.35,333.68,7.86;10,146.91,482.31,333.68,7.86;10,146.91,493.27,333.67,7.86;10,146.91,504.23,166.72,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,345.10,438.48,135.49,7.86;10,146.91,449.44,211.88,7.86">A simple and efficient sampling method for estimating AP and NDCG</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Myaeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390334.1390437</idno>
		<ptr target="https://doi.org/10.1145/1390334.1390437" />
	</analytic>
	<monogr>
		<title level="m" coord="10,394.18,460.40,86.41,7.86;10,146.91,471.35,333.68,7.86;10,146.91,482.31,220.39,7.86">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2008</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Leong</surname></persName>
		</editor>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2008<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">July 20-24, 2008. 2008</date>
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
