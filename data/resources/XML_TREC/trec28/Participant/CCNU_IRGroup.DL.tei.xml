<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.18,82.39,327.02,13.64">CCNU_IRGroup @ TREC 2019 Deep Learning Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,182.93,115.01,34.59,10.04"><forename type="first">Hao</forename><surname>Hu</surname></persName>
							<email>hu_ha0@qq.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.37,115.01,59.87,10.04"><forename type="first">Junmei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.15,115.01,45.09,10.04"><forename type="first">Xinhui</forename><surname>Tu</surname></persName>
							<email>tuxinhui@mail.ccnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.87,115.01,53.11,10.04"><forename type="first">Tingting</forename><surname>He</surname></persName>
							<email>tthe@mail.ccnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.18,82.39,327.02,13.64">CCNU_IRGroup @ TREC 2019 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D27F5452436857B7ECC91D75F46BE702</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The deep learning track consists of two tasks: passage ranking and document ranking. The former focuses on long text retrieval, while the latter focuses on short text retrieval. Both tasks use a large human-labeled set, which is from the MS-MARCO dataset. For different emphases of the two tasks, we adopt two different BERT-based retrieval models. In Section 2 and 3, we will introduce our methods in details. In Section 4 and 5, we will discuss the experiments setting and results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PASSAGE RANKING TASK</head><p>Given BERT's excellent performance in a broad range of NLP tasks, we wondered whether we could take the context-dependent token representation learned by BERT to improve the performance of an neural information retrieval model. Recently, many researches have proposed to apply BERT to down-stream tasks through a feature-based method and have shown good performance, such as SDNet <ref type="bibr" coords="1,156.26,388.25,11.19,9.60" target="#b7">[8]</ref>. In this task, we only modified the output layer of SDNet to accommodate the retrieval task. Next, we will introduce the output layer of our model. As for the other layers, please refer to SDNet <ref type="bibr" coords="1,154.58,419.45,11.18,9.60" target="#b7">[8]</ref>.</p><p>In the output layer, we first calculate the cosine similarity of a query representation and each passage token representation generated by SDNet based on the features extracted by BERT. Secondly, we select top-k signals and project them into a multi-layer perceptron to get the final decision score <ref type="bibr" coords="1,152.54,481.87,11.18,9.60" target="#b1">[2]</ref>.</p><formula xml:id="formula_0" coords="1,218.69,500.47,158.24,54.36">ğ‘¢ ğ‘ = âˆ‘ ğ›½ ğ‘– ğ‘– ğ‘¢ ğ‘– ğ‘ , ğ›½ ğ‘– âˆ exp (ğ‘¤ ğ‘¢ ğ‘¢ ğ‘– ğ‘ ) s = softmax((ğ‘¢ ğ‘ ) ğ‘‡ ğ‘Š ğ‘  V) ğ‘† ğµğ¸ğ‘…ğ‘‡-ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ = MLP(topk(s))</formula><p>where ğ‘¢ ğ‘– ğ‘ denotes the i-th query token representation. ğ‘¤ ğ‘¢ and ğ‘Š ğ‘  are parameters to be learned.</p><p>V is a matrix whose column vector is a passage token representation. BERT's pre-training on surrounding contexts favors text sequence pairs that are closer in their semantic meanings <ref type="bibr" coords="1,172.13,606.70,11.52,9.60" target="#b0">[1,</ref><ref type="bibr" coords="1,183.65,606.70,7.68,9.60" target="#b3">4,</ref><ref type="bibr" coords="1,191.33,606.70,7.68,9.60" target="#b5">6]</ref>. Guo et al. <ref type="bibr" coords="1,247.96,606.70,12.28,9.60" target="#b2">[3]</ref> discussed the differences between relevance matching and semantic matching. They argue that the ad-hoc retrieval task is mainly about relevance matching, such as exact matching signals, query term importance, and diverse matching requirement. In order to capture relevance matching signals, we combine the BM25 model with our neural IR model. The final relevance score can be calculated as follows:</p><formula xml:id="formula_1" coords="1,202.37,683.20,190.24,12.05">Score = Î± â€¢ ğ‘† ğµğ‘€25 + (1 -Î±) â€¢ ğ‘† ğµğ¸ğ‘…ğ‘‡-ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’</formula><p>where ğ‘† ğµğ‘€25 is the original BM25 score and ğ‘† ğµğ¸ğ‘…ğ‘‡ is the feature-based BERT ranking score. The hyperparameter Î± can be tuned via cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Training</head><p>We use a hinge loss to train our model and the loss function is defined as: L(ğ‘, ğ‘ -, ğ‘ + , ğœƒ) = max (0, 1 -ğ‘† ğµğ¸ğ‘…ğ‘‡-ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ (q, ğ‘ + ) + ğ‘† ğµğ¸ğ‘…ğ‘‡-ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ (q, ğ‘ -))</p><p>where ğ‘† ğµğ¸ğ‘…ğ‘‡-ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ (q, ğ‘ + ) denotes the relevance score for a query and a relevant passage and ğ‘† ğµğ¸ğ‘…ğ‘‡-ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ (q, ğ‘ -) is the score for a query and a irrelevant passage. Î¸ includes the parameters in this neural model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DOCUMENT RANKING TASK</head><p>Yang et al. <ref type="bibr" coords="2,150.95,185.42,12.28,9.60" target="#b6">[7]</ref> provide a solution for long document retrieval. Based on the hypothesis that a document is related to a query if some sentences in the document are related, Yang et al. <ref type="bibr" coords="2,472.98,201.02,12.28,9.60" target="#b6">[7]</ref> first splits the document into several sentences and calculates the similarity between each sentence and a query, and then selects the top-k scoring sentences.</p><formula xml:id="formula_2" coords="2,215.21,249.14,164.31,36.75">Score ğ‘‘ = Î² â€¢ ğ‘† ğ‘‘ğ‘œğ‘ + (1 -Î²) â€¢ âˆ‘ ğ‘¤ ğ‘– ğ‘› ğ‘– ğ‘† ğ‘–</formula><p>where ğ‘† ğ‘‘ğ‘œğ‘ is the matching score calculated by the traditional retrieval model and ğ‘† ğ‘– is the i-th top sentence score according to BERT fine-tuned on sentence-level dataset. The hyperparameter Î² and ğ‘¤ ğ‘– can be tuned via cross-validation. In this task, We reproduce this model and choose BM25 as the traditional retrieval model to calculate ğ‘† ğ‘‘ğ‘œğ‘ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Passage ranking datasets.</head><p>In order to calculate the BM25 score, we build the index over the entire passage collection file which includes 8.8 million passages. Both the indexing and the BM25 scoring process are accomplished on the Parrot, which is a Python-based Interactive Platform for Information Retrieval <ref type="bibr" coords="2,90.02,481.87,11.16,9.60" target="#b4">[5]</ref>. We randomly extract 10% of the data, about 9.7 million passages, from triples.train.small.tsv to build the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Document ranking datasets.</head><p>Multi-genre Natural Language Inference (MNLI) is used as the fine-tuning corpus. MNLI is a large-scale, crowdsourced, implicit classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>We use the BERT-Large <ref type="bibr" coords="2,206.09,606.70,12.28,9.60" target="#b0">[1]</ref> in both subtasks. We set the parameters b=0.4, k1=0.9, k2=8 in the BM25 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Passage ranking task</head><p>We use Adam with learning rate of 0.001, ğ›½ 1 = 0.9, ğ›½ 2 = 0.999 and use a dropout probability of 0.4 on all layers to train our model, but freeze all parameters of BERT when train the model and we change the maximum length of a sentence in BERT to 256. We select top 10 matching signals in the output layer. The linear interpolation weight Î± = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Document ranking task</head><p>We use a sliding window of length 100 to split the document into sentences and select the top 3 score sentences to calculate the final score. The linear interpolation weight Î² = 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Our methods' performance can be observed in tables 1 and 2. We can see that we have not achieved good results on both tasks. After analysis, we argue that there are two reasons for the failure in the passage ranking task. The first one is that we only use 10% of the data to train the model. The second reason is that we only consider the matching of tokens, and ignore the matching of sentences. However, as we know that BERT can capture the matching features between two sentences very well, because BERT is trained to predict whether the next sentence is true or not in the pre-training process. And the reason for failure in the document ranking task is that we do not fine tune the BERT on MSMARCO datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,100.82,162.02,393.97,187.71"><head>Table 1 :</head><label>1</label><figDesc>BM25+SDNet+feature-based BERT results in Passage Ranking Task, compared to summary statistics across the 37 submitted runs</figDesc><table coords="3,100.82,194.66,393.97,155.07"><row><cell>Run</cell><cell>MAP</cell><cell>nDCG</cell><cell>P@10</cell></row><row><cell>Runid2</cell><cell>0.2781</cell><cell>0.5492</cell><cell>0.6163</cell></row><row><cell>TREC Median</cell><cell>0.3864</cell><cell>0.6457</cell><cell>0.5651</cell></row><row><cell cols="4">Table 2: BM25+ fine-tuning BERT results in Document Ranking Task, compared to summary</cell></row><row><cell></cell><cell cols="2">statistics across the 38 submitted runs</cell><cell></cell></row><row><cell>Run</cell><cell>MAP</cell><cell>nDCG</cell><cell>P@10</cell></row><row><cell>Runid1</cell><cell>0.2366</cell><cell>0.4299</cell><cell>0.5977</cell></row><row><cell>TREC Median</cell><cell>0.2989</cell><cell>0.5393</cell><cell>0.6906</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,105.26,505.99,348.90,8.18;3,473.02,505.99,32.42,8.18;3,101.42,521.59,208.90,8.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="3,267.46,505.99,186.70,8.18;3,473.02,505.99,32.42,8.18;3,101.42,521.59,48.03,8.18">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="3,105.98,537.19,399.38,8.18;3,101.42,552.79,370.36,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,206.19,537.19,204.14,8.18">Modeling diverse relevance patterns in ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,428.25,537.19,77.11,8.18;3,101.42,552.79,284.67,8.18">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.98,568.39,399.56,8.18;3,101.42,583.99,363.65,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,202.58,568.39,201.62,8.18">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename><forename type="middle">Y</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,422.39,568.39,83.15,8.18;3,101.42,583.99,285.85,8.18">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.98,599.59,399.26,8.18;3,101.42,615.22,90.48,8.18" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="3,236.77,599.59,194.09,8.18">Understanding the Behaviors of BERT in Ranking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07531</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="3,105.98,630.82,399.35,8.18;3,101.42,646.42,403.69,8.18;3,101.42,662.02,171.34,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,232.45,630.82,272.88,8.18;3,101.42,646.42,31.31,8.18">Parrot: A Python-based Interactive Platform for Information Retrieval Research</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,152.30,646.42,352.81,8.18;3,101.42,662.02,76.91,8.18">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1289" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.98,677.62,399.26,8.18;3,101.42,693.22,20.35,8.18" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="3,213.52,677.62,152.02,8.18">Bert rediscovers the classical nlp pipeline</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05950</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="3,105.98,708.82,399.25,8.18;3,101.42,724.42,90.48,8.18" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="3,211.37,708.82,222.94,8.18">Simple applications of bert for ad hoc document retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="3,105.98,740.02,399.46,8.18;3,101.42,755.62,194.86,8.18" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="3,209.99,740.02,295.45,8.18;3,101.42,755.62,33.76,8.18">Sdnet: Contextualized attention-based deep network for conversational question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03593</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
