<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,176.01,173.06,258.23,14.67">Radboud University at TREC 2019</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02">February 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.77,206.86,83.30,10.19"><forename type="first">Chris</forename><surname>Kamphuis</surname></persName>
							<email>c.kamphuis@cs.ru.nl</email>
						</author>
						<author>
							<persName coords="1,278.98,206.86,62.85,10.19;1,341.83,204.51,2.27,7.64"><forename type="first">Tanja</forename><surname>Crijns</surname></persName>
							<email>tanjacrijns@msn.com</email>
						</author>
						<title level="a" type="main" coord="1,176.01,173.06,258.23,14.67">Radboud University at TREC 2019</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02">February 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">9277FC4CBA767F658C358CADBB88B49F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Radboud University Information Retrieval (RU/IR) research group has a research interest in graph based approaches to IR, where we aim to exploit the flexibility of a graph representation of documents and other types of information (such as entities) to achieve increased retrieval effectiveness, e.g. by integrating extra knowledge about a domain. The main focus of our participation in TREC 2019 has been the News Track, where we see a large potential to improve search using graph based representations. We have also participated in the new Conversational Assistance Track, where we have explored how to make use of the conversational context to improve ranking answer passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">News Track 2.1 Background linking</head><p>For the background linking task, we first reviewed the submissions to the 2018 edition of the Track <ref type="bibr" coords="1,254.68,583.69,17.73,9.29" target="#b11">[12]</ref>. One of the best performing runs in 2018 uses BM25 to produce an initial ranking and subsequently re-ranks those results using a relevance model, while filtering out the articles that were published later than the topic article <ref type="bibr" coords="1,302.97,624.34,11.86,9.29" target="#b6">[7]</ref>. We re-implemented this approach as a baseline, for comparison to a variety of ways to include entity information in the search process.</p><p>ru_bm25_rm3_fil We started with a run similar to the best performing run of last year, using the Anserini (v0.6.0) <ref type="bibr" coords="2,349.74,176.85,19.48,9.29" target="#b12">[13]</ref> system. Firstly, documents were retrieved using BM25. From the source article a query was constructed, the 100 terms with the highest TF • IDF per topic document were used. Using 100 terms worked well when evaluating on the topics of last year. From the top 10 retrieved documents, an RM3 query was constructed; in the remainder, we refer to this method as RM3 BM25 . 1 Using the newly formulated query, again the whole collection was ranked. Articles published later than the source article were filtered out of the resulting ranked list.</p><p>ru_bm25_rm3 A run without the date filter, to confirm or disprove its effectiveness.</p><p>ru_sdm_rm3_fil Because the Sequential Dependency Model (SDM) has shown good baseline performance in many entity retrieval benchmarks, we include a run where we replace the BM25 initial ranking by one produced by SDM, again interpolated with that run re-ranked using a relevance model; denoted by RM3 SDM . We use the settings suggested by the Anserini group in 2018. 2 Finally, we apply a date filter to the result list.</p><p>ru-ent-90-10-df The next method re-ranks the baseline using entity information. We tagged the articles in the collection using TagMe <ref type="bibr" coords="2,419.20,428.80,11.86,9.29" target="#b3">[4]</ref>. Using the linked entities, we can compute the ELR score introduced by Hasibi et al. <ref type="bibr" coords="2,125.80,455.90,13.13,9.29" target="#b4">[5]</ref> for each pair of articles (query document and candidate news article). Eq. ( <ref type="formula" coords="2,147.17,469.44,4.43,9.29" target="#formula_0">1</ref>) defines the ELR score:</p><formula xml:id="formula_0" coords="2,192.66,492.66,291.79,30.29">f E (e, D ) = log f ∈F w E f (1 -) tf {0,1}(e, Df ) + d f e,f d f f<label>(1)</label></formula><p>As we only used article content to build the index, we do not sum over fields (there is only a single content field, w E f = 1). The value of for Jelinek-Mercer smoothing equals 0.1. tf {0,1}(e, Df ) takes a value of 1 if entity e is present in document Df , 0 otherwise. d f e,f is the number of documents 1 Technically, RM3 refers to a linear combination of the original query's results using a language modelling approach to IR with those of a query produced by Lavrenko's Relevance Model 1 <ref type="bibr" coords="2,185.65,618.61,9.74,7.64" target="#b7">[8]</ref>, derived from the top documents <ref type="bibr" coords="2,331.01,618.61,9.74,7.64" target="#b5">[6]</ref>. In our case however, it is the BM25 retrieval score that is interpolated with the results of the RM1 relevance model, denoted as RM3 BM25 .</p><p>2 https://github.com/castorini/anserini/blob/master/docs/ runbook-trec2018-anserini.md, Last Accessed: October 28 th , 2019</p><p>where entity e is present, which is normalized by the total number of documents d f f (as there is only one field). The ELR score is then combined with the RM3 BM25 score, following Eq. ( <ref type="formula" coords="3,289.43,160.61,3.99,9.29" target="#formula_1">2</ref>).</p><formula xml:id="formula_1" coords="3,198.30,187.62,286.15,23.84">rsv = T q i ∈Q f T (q i , D ) + E e∈E(Q) s (e) f E (e, D )<label>(2)</label></formula><p>Here, f T (q i , D ) equals the RM3 BM25 score, and s (e) corresponds to the linking confidence score (provided by TagMe) for entity e in the source article. Both partial scores are normalized using min-max normalization (prior to their combination), and weighted by parameters T and E . For this run T and E are set to respectively 0.9 and 0.1. Finally, articles published after the topic article date are filtered out.</p><p>ru-ent-95-05-df The approach as above with T = 0.95 and E = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entity Ranking</head><p>For the entity ranking task (that should perhaps have been called an entity salience task), we looked into finding simple yet effective heuristics. Only a limited number of entities has to be ranked in order of importance, and the entities and their mentions in the source articles were provided by the task. Based on pilot experiments using the 2018 data, we found that simple heuristics are highly effective. Our runs for this task use just these heuristics, with the aim to provide a hard-to-beat baseline for more advanced methods to compare to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ru-t-order</head><p>The first thing we tried (evaluated on last year's test collection) was to create a run where the entities are simply ranked in the order as they are presented in the topic file; making the implicit assumption that the topic author lists important entities first. This approach already yields a high NDCG score; perhaps just because the number of entities to rank is low and the ratio of relevant entities among those to be ranked is high.</p><p>ru-m-order Salient entities tend to appear at the start of a news article, a phenomenon that is well known and provides a strong baseline <ref type="bibr" coords="3,445.69,564.13,11.86,9.29" target="#b2">[3]</ref>. Our second run therefore ranks the entities in their order of mention in the article, another competitive baseline that yields a higher NDCG than ru-torder on the 2018 test collection.</p><p>ru-tf-m-ord Another assumption we made was that entities that are important for a story, tend to be mentioned more often than entities that are less important. This run ranks entities according to their mention frequency in the article. If entities are mentioned an equal number of times, they were  ranked according to their position, preferring the entity that is mentioned first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ru-invwiki</head><p>The track only considers entities with a Wikipedia page. We decided to use the number of tokens on the Wikipedia page of the entity as an indication of salience, based on the observation that entities with less associated information in the knowledge graph are more discriminative, and therefore a priori more likely to be relevant. Figure <ref type="figure" coords="4,397.91,394.33,12.58,9.29" target="#fig_1">1a</ref> shows the correlation between relevance and Wikipedia page length in the 2018 test collection, supporting the potential of this characteristic. The run ranks the entities based on the number of tokens in their Wikipedia page, where the top ranked entity has the lowest number of tokens.</p><p>ru-tf-invwiki Instead of only ranking entities on the number of tokens in their Wikipedia page, we first rank the entities on the number of mentions in the topic article. If the number of mentions is equal between two entities, the entity with the longer Wikipedia page is ranked higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>Tables <ref type="table" coords="4,159.47,556.17,6.35,9.29" target="#tab_0">1</ref> and<ref type="table" coords="4,188.79,556.17,6.35,9.29" target="#tab_1">2</ref> summarize the results for the background linking and entity ranking tasks, respectively. We discuss each of the tasks below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Background linking</head><p>As shown by the difference in effectiveness scores between ru_bm25_rm3 and ru_bm25_rm3_fil, the date filter did not improve results on the 2019 test collection, as opposed to last year (date was one of the strongest features for background reading in <ref type="bibr" coords="4,283.30,660.42,11.52,9.29" target="#b6">[7]</ref>). Initially, when this track was started, it was determined that articles would not be relevant when they were published after the topic article. The idea behind this was that when an article is published, only articles that are published earlier can be recommend as background reading. However, the assumed user scenario was changed later on: when reading an article from the past, articles that were published later might actually be more suited as background reading, as they can provide a more information (e.g. by providing information on how the story developed). So, the context was swapped from reading the topic article at the moment it is published, to reading it some point after it was published; making newer articles potentially suitable for background reading. We attribute the difference in effectiveness of applying a date filter to this in user scenario; maybe, a large proportion of the runs submitted in 2018 did apply a filter on publication date, explicitly or implicitly, and the pool simply did not include the relevant articles published after the topic article.</p><p>The run using RM3 SDM did not perform as well as its corresponding RM3 BM25 run. We have not analyzed the root of this difference in detail, but a possible explanation lies in the simple approach to produce the query; the bi-grams and skip-grams considered are those that include any of the top 100 terms that were selected using TF•IDF weighting, an approach that might not identify the most informative phrases.</p><p>Adding entity information also did not help improve effectiveness. The ru-ent-90-10-df run achieves higher effectiveness than ru-ent-95-05df, so we plan to explore further by increasing the weight on the entityrelated part of the model; properly tuning the parameter might result in a positive effect. The method we used to re-rank the results using entity information was initially proposed for an entity retrieval task, and the TagMe entity linker <ref type="bibr" coords="5,188.78,625.27,13.13,9.29" target="#b3">[4]</ref> was created with the goal of linking short fragments of text to the knowledge base. As we tagged full news articles that are likely written differently from short pieces of text, the linking quality might have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Entity ranking</head><p>The first interesting thing we observe in the entity ranking results is the effectiveness score of the trivial baseline that ranks the entities according to topic order ru-t-order, with an NDCG@5 score of approximately 0.4. We should take into account that the task is fundamentally different from most IR tasks, because the fraction of relevant "documents" (entities) is much higher for this problem than in the usual case of ad-hoc retrieval; even a random permutation will do well, seemingly. Let us keep in mind that high scores do not necessarily indicate that we understand the task in depth.</p><p>Ordering the entities by their appearance in the topic article gives an NDCG@5 of approximately 0.5, an increase of 10% absolute. This result confirms the intuition that news articles mention important entities in the first few sentences. Taking the mention occurrence frequency into account by ranking on the within-document frequency and breaking ties by the mention's location increases the NDCG@5 to 0.538. Frequently mentioned entities automatically have a higher likelihood to appear earlier in the article, so the two approaches are clearly dependent. The observed performance increase matches our intuition that entities mentioned multiple times in the topic article are more relevant than those mentioned only once.</p><p>The best performing run is ru-invwiki, which achieves a NDCG@5 of 0.622, almost another 10% absolute more effective than the previously best approach. This run ranks the entities according to the length of their Wikipedia pages, where entities with shorter Wikipedia pages have been ranked higher. The idea behind this heuristic is that it serves a similar purpose as IDF in regular retrieval tasks. Solely using Wikipedia length as a metric gives a better effectiveness than doing this only to break ties for the entities ranked on within-article frequency (ru-tf-invwiki). Interestingly, when evaluating these approaches on last year's topics set, we found the opposite to be true. Finding a way to combine these metrics into one would be a nice follow up study, for which a variety of TF • IDF weighting schemes may be considered.</p><p>Given that these very simple heuristics already achieve high effectiveness scores, we agree with the track organizers to increase the difficulty of the problem investigated. The entities to be ranked should probably not be pre-selected and provided in the topic files, shifting the focus of the problem to the complete entity linking pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conversational Assistance Track</head><p>The Radboud University IR (RU/IR) research group also participated in the Conversational Assistance Track (CAsT), with the goal to explore the usefulness of conversation context for ranking the passages.</p><p>We consider a two-stage retrieval system that uses Google's BERT language model <ref type="bibr" coords="7,190.65,369.16,13.13,9.29" target="#b1">[2]</ref> to re-rank candidate answer passages retrieved by a standard BM25 retrieval system, inspired by two recent papers that show significant improvements for ranking answer passages over a BM25 baseline <ref type="bibr" coords="7,125.80,409.81,18.76,9.29" target="#b9">[10,</ref><ref type="bibr" coords="7,147.18,409.81,14.07,9.29" target="#b10">11]</ref>.</p><p>The basic approach is to calculate BERT activations for query and answer pairs, and re-rank the candidate answer passages identified by BM25 accordingly. Preliminary experiments using the MS-MARCO collection <ref type="bibr" coords="7,471.32,450.46,13.13,9.29" target="#b8">[9]</ref> confirmed the reported effectiveness of such an approach, where for many queries their known relevant answer passage is indeed ranked (sometimes much) higher with BERT re-ranking.</p><p>A challenge for re-ranking with BERT however is that the query length is inherently limited by the size of the model that we can use. Google shares pre-trained models with a context of 512 tokens, which limits the amount of conversation context that can be taken into account. In our experiments, we consider two approaches to deal with this limitation: simply clipping the conversation context that is considered to that which fits the input size of the model, or, alternatively, applying a fusion method to combine the rankings for multiple turns in the conversation.</p><p>Initial work to explore this direction has been described in the Master's thesis by one of the authors of this paper, and we hope to validate the results of her experiments using the CAsT test collection <ref type="bibr" coords="7,408.62,640.14,11.86,9.29" target="#b0">[1]</ref>. The results of <ref type="bibr" coords="7,137.66,653.69,13.13,9.29" target="#b0">[1]</ref> indicated that using conversational context was only beneficial in the BM25 retrieval step. Adding conversational context when re-ranking with BERT decreased performance in comparison to re-ranking with BERT using only the relevant query. However, the max fusion approach showed the most promising results, and this approach has been included in the runs.</p><p>We have two runs for evaluation: bm25_bert_fc, in the BM25 candidate retrieval step, we use all conversational context and the last query as input. For the BERT re-ranking step, we only use the last query.</p><p>bm25_bert_rankf, in the BM25 candidate retrieval step, we use all conversational context and the last query as input. For the BERT re-ranking step, we consider the three last turns in the conversation as context. We re-rank each turn with all candidate passages and apply MAX score fusion to the three intermediate results.</p><p>NDCG@5 MAP bm25_bert_fc 0.347 0.159 bm25_bert_rankf 0.350 0.159 Median 0.296 0.174</p><p>We cannot immediately confirm that BERT re-ranking is effective, because we did not submit a plain BM25 run. We see that the mean average precision of our complete result list is lower than that of the median system, but the early precision measured by NDCG@5 is higher. We cannot conclude however that taking more context into account leads to improved effectiveness, as the difference in NDCSG@5 is negligible. More detailed analysis of the results will be included in the final TREC proceedings paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,168.99,218.53,2.24,6.67;4,192.62,218.53,2.24,6.67;4,216.24,218.53,2.24,6.67;4,239.87,218.53,2.24,6.67;4,263.50,218.53,2.24,6.67;4,200.54,222.83,33.64,8.01;4,153.59,211.35,2.24,6.67;4,153.59,201.00,2.24,6.67;4,153.59,190.65,2.24,6.67;4,153.59,180.30,2.24,6.67;4,153.59,169.95,2.24,6.67;4,151.34,159.60,4.49,6.67;4,151.34,149.25,4.49,6.67;4,151.34,138.90,4.49,6.67;4,142.36,181.48,8.01,26.03;4,142.36,174.86,8.01,5.27;4,142.36,165.24,8.01,8.27;4,142.36,153.37,8.01,7.84;4,169.58,239.14,91.76,8.49;4,350.30,218.53,2.24,6.67;4,373.93,218.53,2.24,6.67;4,397.56,218.53,2.24,6.67;4,421.18,218.53,2.24,6.67;4,444.81,218.53,2.24,6.67;4,381.86,222.83,33.64,8.01;4,334.90,211.28,2.24,6.67;4,334.90,202.15,2.24,6.67;4,334.90,193.03,2.24,6.67;4,334.90,183.90,2.24,6.67;4,334.90,174.78,2.24,6.67;4,332.66,165.65,4.49,6.67;4,332.66,156.53,4.49,6.67;4,332.66,147.40,4.49,6.67;4,332.66,138.28,4.49,6.67;4,323.67,181.48,8.01,26.03;4,323.67,174.86,8.01,5.27;4,323.67,165.24,8.01,8.27;4,323.67,153.37,8.01,7.84;4,351.05,239.14,91.45,8.49"><head></head><label></label><figDesc>wiki 10k (b) TREC News 2019.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,125.80,261.55,358.66,9.29;4,125.80,275.10,281.70,9.29"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Relation between relevance grade and Wikipedia page length for entities in the 2018 (left) and 2019 (right) test collections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,219.18,132.04,171.89,101.35"><head>Table 1 :</head><label>1</label><figDesc>Results background linking</figDesc><table coords="5,234.25,155.95,141.76,77.44"><row><cell>Method</cell><cell>NDCG@5</cell></row><row><cell>ru_bm25_rm3_fil</cell><cell>0.513</cell></row><row><cell>ru_bm25_rm3</cell><cell>0.527</cell></row><row><cell>ru_sdm_rm3_fil</cell><cell>0.495</cell></row><row><cell>ru-ent-90-10-df</cell><cell>0.503</cell></row><row><cell>ru-ent-95-05-df</cell><cell>0.502</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,125.80,132.04,252.90,136.70"><head>Table 2 :</head><label>2</label><figDesc>Results entity ranking</figDesc><table coords="6,125.80,155.95,244.43,112.79"><row><cell>Method</cell><cell>NDCG@5</cell></row><row><cell>ru-t-order</cell><cell>0.397</cell></row><row><cell>ru-m-order</cell><cell>0.500</cell></row><row><cell>ru-tf-m-order</cell><cell>0.538</cell></row><row><cell>ru-invwiki</cell><cell>0.622</cell></row><row><cell>ru-tf-invwiki</cell><cell>0.585</cell></row><row><cell>suffered.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,141.94,645.70,190.74,7.64;1,137.61,654.88,230.30,9.97"><p>Radboud University, Nijmegen, The Netherlands † Alumna, Radboud University, Nijmegen, The Netherlands</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,150.74,536.53,51.40,9.29;8,217.30,536.53,267.15,9.29;8,150.74,550.08,333.72,9.29;8,150.74,563.63,333.72,9.29;8,150.74,577.85,196.58,8.41" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,217.30,536.53,267.15,9.29;8,150.74,550.08,222.23,9.29">Have a chat with BERT; passage re-ranking using conversational context. Master&apos;s thesis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Crijns</surname></persName>
		</author>
		<ptr target="https://www.ru.nl/publish/pages/769526/tanja_crijns_msc_thesis_ds_22_8_2019.pdf" />
		<imprint>
			<date type="published" when="2019-08">Aug 2019</date>
		</imprint>
		<respStmt>
			<orgName>Radboud University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.74,599.70,333.72,9.29;8,150.74,613.25,333.72,9.29;8,150.74,626.79,333.72,9.29;8,150.74,640.34,333.72,9.29;8,150.74,653.89,333.72,9.29;9,150.74,133.51,333.72,9.29;9,150.74,147.06,333.72,9.29;9,150.74,160.61,280.97,9.29" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,359.54,599.70,124.92,9.29;8,150.74,613.25,266.18,9.29">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,302.43,626.79,182.03,9.29;8,150.74,640.34,333.72,9.29;8,150.74,653.89,328.11,9.29">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s" coord="9,408.57,133.51,75.88,9.29;9,150.74,147.06,29.43,9.29">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.74,183.12,333.72,9.29;9,150.74,196.67,333.72,9.29;9,150.74,210.22,333.72,9.29;9,150.74,223.77,333.72,9.29;9,150.74,237.32,333.72,9.29;9,150.74,250.87,280.97,9.29" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,268.38,183.12,216.08,9.29;9,150.74,196.67,85.81,9.29">A new entity salience task with millions of training examples</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/E14-4040/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,424.67,196.67,59.78,9.29;9,150.74,210.22,240.63,9.29">Proceedings of the 14th Conference of the European Chapter</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bouma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Parmentier</surname></persName>
		</editor>
		<meeting>the 14th Conference of the European Chapter<address><addrLine>Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2014-04-26">2014. April 26-30, 2014. 2014</date>
			<biblScope unit="page" from="205" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.74,273.39,333.72,9.29;9,150.74,286.93,333.72,9.29;9,150.74,300.48,333.72,9.29;9,150.74,314.03,333.72,9.29;9,150.74,327.58,333.72,9.29;9,150.74,341.80,132.98,8.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,287.63,273.39,196.82,9.29;9,150.74,286.93,185.21,9.29">Tagme: On-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Scaiella</surname></persName>
		</author>
		<idno type="DOI">10.1145/1871437.1871689</idno>
		<ptr target="http://doi.acm.org/10.1145/1871437.1871689" />
	</analytic>
	<monogr>
		<title level="m" coord="9,362.92,286.93,121.54,9.29;9,150.74,300.48,333.72,9.29;9,150.74,314.03,40.35,9.29;9,279.35,314.03,41.39,9.29">Proceedings of the 19th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
	<note>CIKM &apos;10</note>
</biblStruct>

<biblStruct coords="9,150.74,363.65,333.72,9.29;9,150.74,377.20,333.72,9.29;9,150.74,390.75,333.72,9.29;9,150.74,404.29,32.35,9.29" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,321.87,363.65,162.59,9.29;9,150.74,377.20,89.75,9.29">Exploiting entity linking in queries for entity retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Bratsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,267.19,377.20,217.27,9.29;9,150.74,390.75,239.87,9.29">Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval</title>
		<meeting>the 2016 ACM International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.74,426.81,333.72,9.29;9,150.74,440.36,333.72,9.29;9,150.74,453.91,333.72,9.29;9,150.74,467.46,333.72,9.29;9,150.74,481.01,333.72,9.29;9,150.74,494.56,333.72,9.29;9,150.74,508.77,208.14,8.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,223.76,440.36,190.14,9.29">Umass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec13/papers/umass.novelty.hard.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,272.56,453.91,211.90,9.29;9,150.74,467.46,53.41,9.29;9,218.21,481.01,133.25,9.29">Proceedings of the Thirteenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Thirteenth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11-16">2004. November 16-19, 2004. 2004</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 500-261</note>
</biblStruct>

<biblStruct coords="9,150.74,530.62,333.72,9.29;9,150.74,544.17,319.27,9.29" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Foley</surname></persName>
		</author>
		<ptr target="https://jjfoley.me/2019/07/24/trec-news-bm25.html" />
		<title level="m" coord="9,212.46,530.62,256.45,9.29">Trec News Background-Linking 2018: Filter By Time</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.74,566.69,333.72,9.29;9,150.74,580.23,333.72,9.29;9,150.74,593.78,333.72,9.29;9,150.74,607.33,333.72,9.29;9,150.74,620.88,333.72,9.29;9,150.74,634.43,333.72,9.29;9,150.74,648.65,179.24,8.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,288.23,566.69,173.59,9.29">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.383972</idno>
		<ptr target="https://doi.org/10.1145/383952.383972" />
	</analytic>
	<monogr>
		<title level="m" coord="9,425.82,580.23,58.64,9.29;9,150.74,593.78,333.72,9.29;9,150.74,607.33,329.13,9.29">SIGIR 2001: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">September 9-13, 2001. 2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,150.74,133.51,333.72,9.29;10,150.74,147.06,333.72,9.29;10,150.74,160.61,333.72,9.29;10,150.74,174.16,333.72,9.29;10,150.74,187.71,333.72,9.29;10,150.74,201.26,333.72,9.29;10,150.74,214.80,333.72,9.29;10,150.74,228.35,333.72,9.29;10,150.74,242.57,272.74,8.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,212.69,147.06,271.77,9.29;10,150.74,160.61,92.86,9.29">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno>CEUR-WS.org</idno>
		<ptr target="http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,237.79,174.16,246.67,9.29;10,150.74,187.71,333.72,9.29;10,150.74,201.26,333.72,9.29;10,150.74,214.80,141.17,9.29;10,150.74,228.35,138.87,9.29">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Besold</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Garcez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</editor>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-09">December 9. 2016. 2016</date>
			<biblScope unit="volume">1773</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct coords="10,150.74,264.42,333.72,9.29;10,150.74,277.97,304.08,9.29" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,286.54,264.42,158.35,9.29">Passage re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR abs/1901.04085</idno>
		<ptr target="http://arxiv.org/abs/1901.04085" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,150.74,300.48,333.72,9.29;10,150.74,314.03,333.72,9.29;10,150.74,327.58,216.87,9.29" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,345.68,300.48,138.77,9.29;10,150.74,314.03,212.19,9.29">Investigating the successes and failures of BERT for passage re-ranking</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Padigela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno>CoRR abs/1905.01758</idno>
		<ptr target="http://arxiv.org/abs/1905.01758" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,150.74,350.10,333.72,9.29;10,150.74,363.65,333.72,9.29;10,150.74,377.20,201.68,9.29" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,319.60,350.10,146.04,9.29">Trec 2018 news track overview</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.74,363.65,333.72,9.29;10,150.74,377.20,18.38,9.29">The Twenty-Seventh Text REtrieval Conference (TREC 2018) Proceedings</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,150.74,399.71,333.72,9.29;10,150.74,413.26,333.72,9.29" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,269.38,399.71,215.07,9.29;10,150.74,413.26,49.78,9.29">Anserini: Reproducible ranking baselines using Lucene</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,208.18,413.26,192.91,9.29">Journal of Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
