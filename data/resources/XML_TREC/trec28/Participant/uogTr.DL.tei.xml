<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.42,84.23,327.16,15.44;1,179.52,104.15,252.95,15.44">University of Glasgow Terrier Team at the TREC 2019 Deep Learning Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,123.19,134.31,37.97,10.59"><forename type="first">Ting</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,284.96,134.31,42.69,10.59"><forename type="first">Xi</forename><surname>Wang</surname></persName>
							<email>x.wang.6@research.gla.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.20,134.31,80.66,10.59"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,488.49,134.31,53.61,10.59"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.42,84.23,327.16,15.44;1,179.52,104.15,252.95,15.44">University of Glasgow Terrier Team at the TREC 2019 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1DC0DD268114B2A1A3364BF2926C6E5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For TREC 2019, we focus on combining deep learning methods with traditional information retrieval methods, by using deep learning scores as an extra feature in the re-ranking process. In particular, we explore the effectiveness of using deep learning techniques based on the state-of-the-art BERT contextual language models, as well as taking into account alternative query reformulations in the re-ranking process. We submitted three official runs to the document ranking task: uogTrDNN6LM, uogTrDSS6pLM, and uogTrDSSQE5LM, where all three runs deploy a deep learning method and the LambdaMART learning-to-rank method. Our results show that uogTrDNN6LM is competitive, performing above the TREC median in terms of MAP and NDCG, yet a simple untrained DFR query expansion run was more effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The University of Glasgow Terrier team participated in the TREC 2019 Deep Learning track, in order to improve the integration between our Terrier.org Information Retrieval (IR) platform <ref type="bibr" coords="1,264.69,394.91,14.59,7.94" target="#b14">[15]</ref> and recent deep learning techniques, and to improve the effectiveness of Terrier on a large Web corpora and an adhoc ranking task.</p><p>In particular, we participated in the document ranking task of the Deep Learning track, but without using the provided initial rankings. Instead, we indexed the MSMARCO corpus using Terrier v5.2, and then used Terrier to perform the initial first-phase retrieval. We also used Terrier to re-rank the candidate results identified in the first-phase, using tools such as CEDR <ref type="bibr" coords="1,210.27,482.58,14.85,7.94" target="#b12">[13]</ref> for deep semantic matching and other query expansion techniques (e.g. collection enrichment and axiomatic query expansion <ref type="bibr" coords="1,210.59,504.50,8.99,7.94" target="#b8">[9]</ref>). These results were combined using a LambdaMART <ref type="bibr" coords="1,179.13,515.45,10.68,7.94" target="#b4">[5]</ref> learning-to-rank technique. In this way, we leveraged our existing Terrier infrastructure, while enhancing it with the integration of new deep learning techniques.</p><p>The structure of the remainder of this paper is structured as follows: Section 2 discusses our indexing setup; Section 3 describes our first-phase ranking setup; Section 4 describes the features used in the second (re-ranking) phase, including deep learning; Section 5 describes our submitted runs; Section 6 highlights our results; Concluding remarks follow in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INDEXING</head><p>We indexed the corpus using Terrier v5.2. Firstly, we chose not to use the TREC-formatted version of the MSMARCO corpus, but instead, reformatted the CSV files into TREC files such that the URL and title are clearly delineated. Next, we used several indexing configurations:</p><p>• Positions: We recorded positional information.</p><p>• Fields: We separately recorded the frequencies of terms occurring in different parts of the document. In particular, we recorded the 'TITLE', 'BODY' and 'URL' fields, following our past participations in the TREC Web track <ref type="bibr" coords="1,498.94,213.03,13.36,7.94" target="#b17">[18]</ref>. In all cases, we created the standard Terrier indexing configuration to create an inverted index, and a direct index to support query expansion and other retrieval techniques, as well as recording the raw text of the URLs, titles and contents of the documents as metadata, to allow deep learning upon these textual representations, as discussed further in Section 4 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FIRST-PHASE RETRIEVAL</head><p>After indexing the corpus with Terrier and the corresponding indexing configurations, we conduct the first-phase retrieval from the document collection.</p><p>First, we apply batch retrieval using Terrier on the queries with specific configurations matching indexed corpus with the same retrieval configuration setup. For example, to retrieve documents with the 'Stemming &amp; Stopwords' configured indexing of the given queries, we need to remove stopwords and apply stemming to the terms in the queries.</p><p>Next, we use three weighting models to calculate the scores of retrieved documents, rank documents with the obtained scores and then provide the candidate document set for the second-phase retrieval. We describe these three weighting models as follows:</p><p>• DPH is a divergence from randomness (DFR)-based <ref type="bibr" coords="1,543.35,510.92,14.85,7.94" target="#b21">[22]</ref> hyper-geometric weighting model. The 'P' in the DPH model indicates the Popper's normalization <ref type="bibr" coords="1,474.84,532.84,9.27,7.94" target="#b1">[2]</ref>. Moreover, DPH is a parameter-free model. This means that DPH can provide document scores without fine-tuning parameters. In particular, DPH is the default weighting model of Terrier v5.2. • BM25 (i.e. Okapi BM25) is a popular weighting model and has been adopted in many studies <ref type="bibr" coords="1,455.69,587.64,13.60,7.94" target="#b11">[12,</ref><ref type="bibr" coords="1,471.53,587.64,11.59,7.94" target="#b16">17]</ref> to address retrievalbased tasks. Compared to DPH, we note that BM25 can be trained with fine-tuned parameters. Hence, it provides corpus-fitted models. • PL2 is another DFR-based weighting model. According to Amati <ref type="bibr" coords="1,367.72,642.43,9.52,7.94" target="#b2">[3]</ref>, the PL2 model can provide good performances for tasks requiring high early precision.</p><p>We also tested the use of query expansion. For each query, we select the most informative terms from the top-returned documents to expand the query. In particular, we use three top-returned documents in the experimental setup. Then, we assign each term in these documents with a weighting score according to the Bo1 term weighting model <ref type="bibr" coords="2,117.99,98.75,9.45,7.94" target="#b0">[1]</ref>. After that, we use the top 10 terms with the highest weighting score to expand the query. With these expanded queries, we also rank the retrieved documents by using the three weighting models introduced above, thereby providing another candidate document set.</p><p>We partitioned 100 and 1000 queries from the provided 367013 MSMARCO training queries to evaluate the effectiveness of various combinations of indexing configurations, weighting models and query expansion. After evaluating the retrieved documents of different retrieval strategies in terms of MAP, P@5, P@10 and Recall@1000, we summarise the three main conclusions in our initial first-phrase retrieval experiments as follows:</p><p>• After applying the stemming and stopwords removal configuration to index documents and format queries, we can retrieve documents with a higher precision score compared to other configurations (i.e. (1) without stemming and keeping stopwords, (2) stopwords removed only).</p><p>• The DPH weighting model outperforms the other two weighting models (i.e. BM25 and PL2) by providing higher precision scores. Moreover, these three weighting models all achieve similar recall scores, around 90%. • The query expansion configuration can increase recall, to provide reliable candidate document sets for the secondphrase retrieval.</p><p>Our experiments found these conclusions to be consistent while using 100 or 1000 queries sampled from the MSMARCO training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SECOND-PHASE RETRIEVAL</head><p>In this section, we describe our attempts at improving adhoc reranking performance. In particular, we use two techniques: rewriters, which allow the calculation of additional query dependent features on the candidate document set obtained from the first set, where the query dependent features use alternative query formulations (Section 4.1); and deep learning techniques for semantic matching (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Rewriters</head><p>In the past, Terrier has been flexible to allow arbitrary weighting models to be expressed as separate features. This is implemented using the Fat framework <ref type="bibr" coords="2,143.77,559.03,13.29,7.94" target="#b15">[16]</ref>, which caches the postings matching the original query terms for documents entering the top K candidate set. In doing so, it is said to fatten the result set with posting information. This means that more query dependent features can be efficiently calculated for these documents without re-traversing the inverted index. Figure <ref type="figure" coords="2,146.59,613.82,4.09,7.94">1</ref> provides examples of feature definitions supported by Terrier since version 4.0.</p><p>However, a disadvantage of the Fat framework is that the additional query dependent features can only be calculated for the original query terms. Asadi &amp; Lin <ref type="bibr" coords="2,173.27,657.66,10.43,7.94" target="#b3">[4]</ref> described an alternative framework, which they called doc vectors, whereby the direct (a.k.a. forward) index data structure is used in the second retrieval phase. This has the advantage that query dependent features can use query terms not present in the original query. SAMPLE #the first pass retrieval score WMODEL:BM25 #BM25 on the whole document WMODEL:SingleFieldModel(BM25,0) #BM25 on the title WMODEL:SingleFieldModel(BM25,1) #BM25 on the body QI:StaticFeature(OIS,/path/to/inlinks.oos.gz) #inlinks We implemented the doc vectors approach using Terrier, and in particular, build on it to express different rewritten forms of the query as separate features for learning-to-rank. The actions that create different query rewrites are called rewriters -we deployed several rewriters:</p><p>• Bo1 QE: Divergence from Randomness query expansion using the Bo1 term weighting model <ref type="bibr" coords="2,476.10,432.08,10.68,7.94" target="#b0">[1]</ref> on the top-ranked documents in the first-phase candidate set. • Collection enrichment (CE): DFR Bo1 Divergence from Randomness query expansion on the top-ranked documents from Wikipedia <ref type="bibr" coords="2,402.32,475.92,13.36,7.94" target="#b13">[14]</ref>. • Axiomatic query expansion (axqe) <ref type="bibr" coords="2,470.05,486.88,9.33,7.94" target="#b8">[9,</ref><ref type="bibr" coords="2,481.63,486.88,10.13,7.94" target="#b22">23]</ref>.</p><p>• Markov Random Fields proximity <ref type="bibr" coords="2,467.50,497.84,13.50,7.94" target="#b18">[19,</ref><ref type="bibr" coords="2,483.24,497.84,10.13,7.94" target="#b20">21]</ref>. In deploying these rewriters, we are able to re-score the documents in the first-phase candidate set using different forms of expanded queries. Each rewriter is a different feature. Different weighting models can be expressed as features on the query terms obtained for each rewriter. Figure <ref type="figure" coords="2,447.50,554.55,4.25,7.94">2</ref> provides examples of query dependent features defined on groups of query terms obtained from different rewriters -the $ symbol denotes the tag of the query terms forming a given group of terms obtained from a given rewriter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Learning</head><p>Our use of deep learning techniques initially focused on using MatchZoo <ref type="bibr" coords="2,359.39,633.14,14.85,7.94" target="#b10">[11]</ref> <ref type="foot" coords="2,374.23,630.99,3.38,6.44" target="#foot_0">1</ref> , including adapting it to use different negative sampling strategies.</p><p>However, for various reasons, including both overall effectiveness and support for the state-of-the-art BERT language models <ref type="bibr" coords="2,546.83,666.01,9.27,7.94" target="#b7">[8]</ref>, we changed strategy and opted for using the CEDR toolkit <ref type="bibr" coords="2,539.47,676.97,14.85,7.94" target="#b12">[13]</ref> <ref type="foot" coords="2,554.32,674.82,3.38,6.44" target="#foot_1">2</ref>  We implemented a DocumentScoreModifier in Terrier that allows to obtain the scores of the top-ranked documents from CEDR. In particular, it collects the contents of the documents from the Terrier index, sends these and the query over a HTTP REST-ful connection to a Python-based server serving the CEDR models, which then returns the computed scores of the documents to Terrier. Our extension of Terrier is avaiable from Github 3 . CEDR provides two ranking models: a BERT model, and a PACCR model based on the BERT model. In order to train the CEDR model, we first fine-tune the pre-trained BERT model, which is then used within the CEDR model. For both BERT and PACCR models, we introduced early stopping, which terminates training if there was no validation improvement for 20 iterations.</p><p>Additionally, to enhance the effectiveness, and inspired by the recent work of Dai &amp; Callan <ref type="bibr" coords="3,155.95,400.82,9.27,7.94" target="#b6">[7]</ref>, we adapt CEDR to apply passaging to long documents. In particular, by breaking up long documents into shorter passages, effective models are more easily learned. Following Dai &amp; Callan, we break documents into passages of 150 tokens, with a stride of 75 between passages, which are prepended with the title of the document. To obtain the final score of a document, we take the max of the scores of the passages within the document. Our adaptation of CEDR is available from Github 4 .</p><p>In terms of setup, we trained all CEDR models using 1000 queries (from the MSMARCO training set), ranked to depth 1000, and for validation, we used 200 queries ranked to depth 50. In particular, reducing the rank depth of the validation set significantly reduced the speed of training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning-to-Rank: Combination of Features</head><p>We used the LambdaMART <ref type="bibr" coords="3,154.93,581.86,10.48,7.94" target="#b4">[5]</ref> learning-to-rank technique to combine the features described above. We use the Jforests LambdaMART implementation <ref type="bibr" coords="3,115.66,603.78,14.85,7.94" target="#b9">[10]</ref> 5 . We also tried the Tensorflow TF Ranking package, but at the time of our TREC campaign, the released version did not allow for the prediction of scores using a learned model 6 . Thus, we report unsubmitted runs using TF Ranking in the next sections. Furthermore, we also report effectiveness using a simple linear combination of feature values, with weights optimised 3 https://github.com/terrierteam/terrier-nrr 4 https://github.com/cmacdonald/cedr 5 https://github.com/yasserg/jforests. 6 This has now been fixed -see https://github.com/tensorflow/ranking/issues/104.  <ref type="bibr" coords="3,405.33,454.04,13.36,7.94" target="#b19">[20]</ref>.</p><p>All our learning-to-rank runs were trained on 1000 queries drawn from the MSMARCO training set, with learning validation (e.g. setting number of iterations) using a further 200 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SUBMITTED AND UNSUBMITTED RUNS</head><p>Table <ref type="table" coords="3,339.49,521.94,4.12,7.94" target="#tab_1">1</ref> lists the names and descriptions of all the features we used in our experiments, for the re-ranking process. Table <ref type="table" coords="3,521.48,532.90,4.25,7.94" target="#tab_2">2</ref> lists the detailed features we used in each of our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submitted Runs</head><p>We submitted the following runs to the document ranking task of the TREC 2019 Deep Learning track:</p><p>• uogTrDNN6LM: In this run, we use the index without prior stemming and stopword removal, denoted NN. We use the DPH model to rank documents, and deploy CEDR-PACRR on the document level to obtain a document's deep learning representation score. We use 8 features in our re-ranking process, as shown in Table <ref type="table" coords="3,448.47,657.66,3.13,7.94" target="#tab_2">2</ref>. The final ranking of documents used a LambdaMART learned model. This method is submitted as our main test run. • uogTrDSS6pLM: In this run, we use the stemmed and stopwords removed index (SS). We deploy DPH in the first pass of ranking. In addition to the document level CEDR-PACRR score, we further obtain the maximum CEDR-PACRR scores for each paragraph in the document, as one of the deep learning representation for each document. We use 6 features in our reranking process, as shown in Table <ref type="table" coords="4,224.71,131.63,3.01,7.94" target="#tab_2">2</ref>. The final ranking of documents also used a LambdaMART learned model. • uogTrDSSQE5LM: The difference between this run and the uogTrDSS6pLM run, is that we additionally deploy QE during the first pass ranking. Furthermore, similarly to the uogTrDNN6LM run, we use the document-level CEDR-PACRR scores only for each document, as the deep learning representation for each document. Similar to the previous two runs, the final ranking of documents also uses a LambdaMART learned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unsubmitted Runs</head><p>For each of the aforementioned submitted runs, we report the effectiveness of the first phase retrieval approach for each of the submitted runs (i.e. NN, SS and SSQE) as unsubmitted runs. We also report performances using two different learning-to-rank techniques, namely, AFS and Tensorflow Ranking (TF Ranking), using the same feature sets as the submitted runs. Finally, later in Section 6, we also report the effectiveness of the individual feature sets of the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS &amp; ANALYSIS</head><p>Table <ref type="table" coords="4,74.82,377.46,4.09,7.94" target="#tab_3">3</ref> lists the obtained effectiveness results for our submitted and unsubmitted runs, as well as the TREC per-topic best and median scores across all participating systems, in terms of MAP, P@10, NDCG@10 and NDCG@1000 <ref type="foot" coords="4,161.01,408.19,3.38,6.44" target="#foot_2">7</ref> . We also report the effectiveness of the top 100 results obtained from an Indri QueryLikelihood run, as provided by the track organisers for the re-ranking task (denoted TREC provided top 100). All evaluation metrics are calculated using the official qrels, as judged by NIST assessors. Firstly, on analysing the table, we note that one of our submitted runs, uogTrDNN6LM exceeds the median performance for MAP and NDCG; It also outperforms the TREC provided top 100 for all four measures; Our other two runs are below the median for the relevant measures (P@10, MAP &amp; NDCG@1000). Next, we analyse the performance of our unsubmitted runs. First, it is apparent that a simple application of Terrier's standard query expansion, i.e. uogTrDSSQE, and without any learning (deep or otherwise), would have outperformed all of our submitted runs for MAP &amp; P@10. Moreover, on inspection of the track overview paper <ref type="bibr" coords="4,77.71,574.72,9.52,7.94" target="#b5">[6]</ref>, we note that it would have outperformed the highest 'traditional' submitted run among all participants (srchvrs_run1) by 7% for NDCG@10 (0.5610 → 0.6007).</p><p>Next, we note that while the alternative learning-to-rank techniques, TF Ranking and AFS, could both marginally enhance P@10 on the 50 topics, they could not enhance the high MAP of uogTrDNN-6LM. In contrast, it is clear that the low effectiveness of uogTrDSS6pLM and uogTrDSSQE5LM is unexpected, as the same features re-ranked using AFS or TF Ranking would have resulted in dramatically increased effectiveness.</p><p>To analyse further the effectiveness of the various features, Table 4 reports the effectiveness of the various features evaluated using the TREC official qrels. On analysing the table, we note the following observations: Firstly, the initial rankings obtained from DPH are more effective than the other features, across all three settings (6 out of 9 measurements in Table <ref type="table" coords="4,442.34,304.13,2.90,7.94" target="#tab_4">4</ref>). However, we do observe that P@10 can be enhanced in two cases by the BERT-based CEDR models. Between the two CEDR models, we lack a fair comparison, but we believe that the passage-based CEDR model (feature #9) appears to be more effective than the document-level model (feature #8) in comparison to the various first-pass retrieval settings (feature #1). Finally, we compare and contrast the results in Table <ref type="table" coords="4,533.78,369.88,4.25,7.94" target="#tab_4">4</ref> with those results that we obtained before the runs were submitted. In particular, using an internal "test" set of 200 topics drawn from the MSMARCO training query set (separated from our internal training and validation sets), we compare and contrast the effectiveness of the features in Table <ref type="table" coords="4,393.96,424.68,4.11,7.94" target="#tab_4">4</ref> (evaluated using the final official evaluation qrels) with that obtained on the internal test set. The resulting scatterplots, for MAP and NDCG@10, are shown in Figure <ref type="figure" coords="4,541.63,446.60,3.13,7.94" target="#fig_2">3</ref>. In particular, for MAP, Figure <ref type="figure" coords="4,423.58,457.56,8.43,7.94" target="#fig_2">3a</ref> shows that a fairly weak overall correlation can be observed between effectiveness on our internal test set (sampled from the MSMARCO training queries) and on the final official evaluation qrels. For example, the correlation for the DNN6 setting is only Spearman's ρ = 0.18, although other settings exhibit higher correlations. On the other hand, the stronger correlation observed for NDCG@10 (Figure <ref type="figure" coords="4,481.86,523.31,7.62,7.94" target="#fig_2">3b</ref>) suggests that the CEDR PACRR features are clearly the most effective features for NDCG@10, on both the training and official qrels. Overall, we conclude that there are significant differences between the labels for the MSMARCO training queries and the final official TREC qrels, which would result in differing conclusions for feature selection and learning-to-rank for MAP. Indeed, on inspection of the various relevance assessments, we found that the training queries had very few relevant documents (1.04 on average) compared to the average of 153.4 for the official TREC qrels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Overall, our participation in the TREC Deep Learning track was a useful activity to increase our understanding about the effective integration of deep learning techniques into Terrier, as well as how different query formulations (obtained from rewriters) can be integrated within a learning-to-rank setting. In terms of effectiveness,    <ref type="table" coords="5,112.81,544.19,3.40,7.70" target="#tab_4">4</ref>, as obtained on from the MSMARCO training queries (x-axis), and the official TREC qrels (y-axis).</p><p>our most effective run uogTrDNN6LM outperformed the TREC median, but we observed that a simple DFR-based query expansion run could be more effective. Moreover, some of our learning-to-rank runs significantly diverged from their expected performances. This emphasises the difficulty in learning effective models for adhoc retrieval tasks using training datasets with very few judgements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,317.96,153.03,240.25,7.70;2,317.96,173.76,174.84,7.20;2,335.89,184.72,130.01,7.20;2,317.73,195.68,174.84,7.20;2,335.89,206.64,125.53,7.20;2,317.73,217.60,71.73,7.20;2,335.89,228.56,201.74,7.20;2,317.73,239.51,58.28,7.20;2,329.90,250.47,234.60,7.20;2,317.73,261.43,58.28,7.20;2,335.89,272.39,116.56,7.20;2,317.73,283.35,246.58,7.20;2,335.89,294.31,125.53,7.20"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Examples of standard feature definitions in Terrier. WMODEL$original:SingleFieldModel(DPH,0) #DPH on title, original query WMODEL$original:SingleFieldModel(DPH,1) #DPH on body, original query WMODEL$qeBo1:DPH #DPH on Bo1 expanded query from top documents WMODEL$ce:DPH #DPH on Bo1 expanded query from top Wikipedia documents WMODEL$ax:DPH #DPH on axiomatic QE terms WMODEL$prox:org.terrier.matching.models.dependence.pBiL #pBiL on MRF proximity terms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,132.31,359.93,82.98,6.85;5,120.29,511.24,104.77,6.85"><head>( a )</head><label>a</label><figDesc>Scatterplot for MAP (b) Scatterplot for NDCG@10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,53.80,533.23,241.85,7.70;5,53.80,544.19,241.85,7.70;5,53.80,555.15,225.56,7.70;5,71.92,368.92,201.76,134.37"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatterplot showing the performances of the features in Table4, as obtained on from the MSMARCO training queries (x-axis), and the official TREC qrels (y-axis).</figDesc><graphic coords="5,71.92,368.92,201.76,134.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.80,85.73,492.54,158.65"><head>Table 1 :</head><label>1</label><figDesc>Feature used in different runs. Note that feature #1 also scores the expanded terms in the case of the SSQE run.</figDesc><table coords="3,53.80,108.81,432.80,135.57"><row><cell cols="2"># Feature Set Features</cell><cell>Feature description</cell></row><row><cell>1 SAMPLE</cell><cell>DPH</cell><cell>DPH score between entire document and query</cell></row><row><cell cols="2">2 WMODEL SingleFieldModel(DPH,0)</cell><cell>DPH score between doc title and query</cell></row><row><cell cols="2">3 WMODEL SingleFieldModel(DPH,1)</cell><cell>DPH score between doc URL and query</cell></row><row><cell cols="2">4 WMODEL $qeBo1:DPH</cell><cell>Bo1 Query Expansion: DPH score</cell></row><row><cell cols="2">5 WMODEL $prox:pBiL</cell><cell>DFR proximity</cell></row><row><cell cols="2">6 WMODEL $ax:DPH</cell><cell>Axiomatic QE: DPH score</cell></row><row><cell cols="2">7 WMODEL $ce:DPH</cell><cell>Collection Enrichment QE: DPH score</cell></row><row><cell>8 DSM</cell><cell>CEDR-PACRR</cell><cell>CEDR-PACRR on the document level</cell></row><row><cell>9 DSM</cell><cell cols="2">passage-level CEDR-PACRR maximum CEDR-PACRR score across all passages</cell></row><row><cell cols="2">instead. We extended and integrated CEDR into Terrier as follows:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,317.96,234.37,240.25,227.61"><head>Table 2 :</head><label>2</label><figDesc>Feature used for each run</figDesc><table coords="3,317.96,257.46,240.25,204.52"><row><cell>Run</cell><cell cols="2">Features used Re-ranking method</cell></row><row><cell></cell><cell cols="2">Submitted runs</cell></row><row><cell>uogTrDNN6LM</cell><cell>1,2,5,6,7,8</cell><cell>LambdaMART</cell></row><row><cell>uogTrDSS6pLM</cell><cell>1,2,3,5,8</cell><cell>LambdaMART</cell></row><row><cell>uogTrDSSQE5LM</cell><cell>1,2,3,5,8</cell><cell>LambdaMART</cell></row><row><cell></cell><cell cols="2">Unsubmitted runs</cell></row><row><cell>uogTrDNN</cell><cell>-</cell><cell>-</cell></row><row><cell>uogTrDSS</cell><cell>-</cell><cell>-</cell></row><row><cell>uogTrDSSQE</cell><cell>-</cell><cell>-</cell></row><row><cell>uogTrDNN6AFS</cell><cell>1,2,5,6,7,8</cell><cell>Automatic feature selection</cell></row><row><cell>uogTrDSS6pAFS</cell><cell>1,2,3,5,8,9</cell><cell>Automatic feature selection</cell></row><row><cell cols="2">uogTrDSSQE5AFS 1,2,3,5,8</cell><cell>Automatic feature selection</cell></row><row><cell>uogTrDNN6TFR</cell><cell>1,2,5,6,7,8</cell><cell>Tensorflow Ranking</cell></row><row><cell>uogTrDSS6pTFR</cell><cell>1,2,3,5,8,9</cell><cell>Tensorflow Ranking</cell></row><row><cell cols="2">uogTrDSSQE5TFR 1,2,3,5,8</cell><cell>Tensorflow Ranking</cell></row><row><cell cols="3">to maximise MAP using Simulated Annealing, called Automatic</cell></row><row><cell cols="2">Feature Selection (AFS)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,327.27,85.73,221.33,161.92"><head>Table 3 :</head><label>3</label><figDesc>Performance of submitted and unofficial runs.</figDesc><table coords="4,329.64,107.63,216.87,140.02"><row><cell></cell><cell>MAP</cell><cell>P@10</cell><cell cols="2">NDCG@10 NDCG@1000</cell></row><row><cell>TREC Best</cell><cell>0.5150</cell><cell>0.8800</cell><cell>-</cell><cell>0.7428</cell></row><row><cell>TREC Median</cell><cell>0.2989</cell><cell>0.6907</cell><cell>-</cell><cell>0.5394</cell></row><row><cell>TREC provided top 100</cell><cell>0.2367</cell><cell>0.5977</cell><cell>0.5147</cell><cell>0.4303</cell></row><row><cell>uogTrDNN6LM</cell><cell>0.3158</cell><cell>0.6744</cell><cell>0.6060</cell><cell>0.5771</cell></row><row><cell>uogTrDSS6pLM</cell><cell>0.1250</cell><cell>0.5372</cell><cell>0.6333</cell><cell>0.3918</cell></row><row><cell>uogTrDSSQE5LM</cell><cell>0.1240</cell><cell>0.5442</cell><cell>0.6381</cell><cell>0.4207</cell></row><row><cell></cell><cell cols="2">Unsubmitted runs</cell><cell></cell><cell></cell></row><row><cell>uogTrDNN</cell><cell>0.2943</cell><cell>0.5837</cell><cell>0.5059</cell><cell>0.5482</cell></row><row><cell>uogTrDSS</cell><cell>0.2875</cell><cell>0.6116</cell><cell>0.5462</cell><cell>0.5014</cell></row><row><cell>uogTrDSSQE</cell><cell>0.3434</cell><cell>0.6930</cell><cell>0.6007</cell><cell>0.5549</cell></row><row><cell>uogTrDNN6AFS</cell><cell>0.2649</cell><cell>0.6512</cell><cell>0.5689</cell><cell>0.5544</cell></row><row><cell>uogTrDSS6pAFS</cell><cell>0.2976</cell><cell>0.6953</cell><cell>0.6290</cell><cell>0.5176</cell></row><row><cell>uogTrDSSQE5AFS</cell><cell>0.2943</cell><cell>0.5837</cell><cell>0.5058</cell><cell>0.5481</cell></row><row><cell>uogTrDNN6TFR</cell><cell>0.2836</cell><cell>0.6605</cell><cell>0.5979</cell><cell>0.5627</cell></row><row><cell>uogTrDSS6pTFR</cell><cell>0.2823</cell><cell>0.6953</cell><cell>0.6246</cell><cell>0.5103</cell></row><row><cell>uogTrDSSQE5TFR</cell><cell>0.2365</cell><cell>0.6535</cell><cell>0.5934</cell><cell>0.5393</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,130.05,85.73,350.15,116.55"><head>Table 4 :</head><label>4</label><figDesc>Performance of features within each first phase &amp; feature set setting.</figDesc><table coords="5,130.05,107.63,350.15,94.65"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">First Phase &amp; Feature set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature ↓</cell><cell></cell><cell>NN6</cell><cell></cell><cell></cell><cell>SS6p</cell><cell></cell><cell></cell><cell>SSQE5</cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>P@10</cell><cell>NDCG@1000</cell><cell>MAP</cell><cell>P@10</cell><cell>NDCG@1000</cell><cell>MAP</cell><cell>P@10</cell><cell>NDCG@1000</cell></row><row><cell>1</cell><cell>0.2943</cell><cell>0.5837</cell><cell>0.5482</cell><cell>0.2857</cell><cell>0.6116</cell><cell>0.5014</cell><cell cols="2">0.3434 0.6930</cell><cell>0.5549</cell></row><row><cell>2</cell><cell>0.1915</cell><cell>0.5744</cell><cell>0.4939</cell><cell>0.1944</cell><cell>0.4233</cell><cell>0.4037</cell><cell>0.2076</cell><cell>0.4349</cell><cell>0.4347</cell></row><row><cell>3</cell><cell>0.2716</cell><cell>0.5837</cell><cell>0.5358</cell><cell>0.2003</cell><cell>0.4356</cell><cell>0.4239</cell><cell>0.2279</cell><cell>0.4698</cell><cell>0.4639</cell></row><row><cell>4</cell><cell>0.2935</cell><cell>0.5837</cell><cell>0.5475</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>5</cell><cell>0.2936</cell><cell>0.5860</cell><cell>0.5474</cell><cell>0.2246</cell><cell>0.4977</cell><cell>0.4390</cell><cell>0.2624</cell><cell>0.5791</cell><cell>0.4975</cell></row><row><cell>6</cell><cell>0.2739</cell><cell>0.5442</cell><cell>0.5319</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>7</cell><cell>0.2935</cell><cell>0.5837</cell><cell>0.5475</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>8</cell><cell>0.2478</cell><cell>0.6302</cell><cell>0.5432</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.3114</cell><cell>0.6814</cell><cell>0.5434</cell></row><row><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.2784</cell><cell>0.6605</cell><cell>0.5022</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,324.49,693.39,145.86,6.18"><p>https://github.com/NTMC-Community/MatchZoo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,324.49,702.79,126.80,6.18"><p>https://github.com/Georgetown-IR-Lab/cedr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="4,60.33,713.75,184.32,6.18"><p>No TREC Best and Median results were provided for NDCG@10.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The co-authors acknowledge the assistance of <rs type="person">Zaiqiao Meng</rs> in deep learning, as well as insights from <rs type="person">Richard McCreadie</rs> and <rs type="person">Jeff Dalton</rs> in supporting our participation.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,333.39,234.45,225.51,6.23;5,333.39,242.42,225.57,6.23;5,333.39,250.44,64.78,6.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,391.38,234.45,167.52,6.23;5,333.39,242.42,64.48,6.23">Probabilistic Models for Information Retrieval based on Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation.</note>
</biblStruct>

<biblStruct coords="5,333.39,258.41,224.81,6.18;5,333.39,266.38,224.81,6.18;5,333.18,274.30,127.63,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,377.63,266.38,180.57,6.18;5,333.18,274.35,13.88,6.18">FUB, IASI-CNR and University of Tor Vergata at TREC 2008 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giuseppe</forename><surname>Amodeo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlo</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgio</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,361.95,274.30,69.52,6.23">Proceedings of TREC 2008</title>
		<meeting>TREC 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="248" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,282.32,224.81,6.18;5,333.39,290.24,224.81,6.23;5,333.39,298.21,146.67,6.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,458.85,282.32,99.35,6.18;5,333.39,290.29,170.11,6.18">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,508.54,290.24,49.66,6.23;5,333.39,298.21,84.81,6.23">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,306.23,224.81,6.18;5,333.39,314.15,213.58,6.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,433.95,306.23,124.25,6.18;5,333.39,314.20,122.94,6.18">Document vector representations for feature extraction in multi-stage document ranking</title>
		<author>
			<persName coords=""><forename type="first">Nima</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,461.82,314.15,23.44,6.23">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="747" to="768" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,322.12,225.27,6.23;5,333.39,330.09,139.55,6.23" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,421.45,322.12,137.21,6.23;5,333.39,330.09,34.24,6.23">From RankNet to LambdaRank to LambdaMART: An Overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2010-82</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="5,333.39,338.11,225.07,6.18;5,333.39,346.03,194.67,6.23" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,531.19,338.11,27.27,6.18;5,333.39,346.08,110.05,6.18">Overview of the TREC 2019 Deep Learning Track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,455.76,346.03,69.52,6.23">Proceedings of TREC 2019</title>
		<meeting>TREC 2019</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,354.05,224.81,6.18;5,333.39,361.97,206.21,6.23" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,443.03,354.05,115.17,6.18;5,333.39,362.02,109.02,6.18">Deeper Text Understanding for IR with Contextual Neural Language Modeling</title>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,454.92,361.97,55.27,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,369.99,225.63,6.18;5,333.39,377.96,225.89,6.18;5,333.39,385.88,108.33,6.23" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="5,541.50,369.99,17.53,6.18;5,333.39,377.96,222.70,6.18">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,333.39,393.90,224.81,6.18;5,333.15,401.82,214.19,6.23" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,446.11,393.90,112.09,6.18;5,333.15,401.87,103.35,6.18">Semantic Term Matching in Axiomatic Approaches to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,448.54,401.82,69.46,6.23">Proceedings of SIGIR 2006</title>
		<meeting>SIGIR 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,409.84,225.99,6.18;5,333.39,417.76,224.81,6.23;5,333.39,425.73,44.23,6.23" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,505.82,409.84,53.56,6.18;5,333.39,417.81,180.58,6.18">Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models</title>
		<author>
			<persName coords=""><forename type="first">Yasser</forename><surname>Ganjisaffar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,526.37,417.76,31.83,6.23;5,333.39,425.73,21.31,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,433.75,225.58,6.18;5,333.39,441.67,224.81,6.23;5,333.39,449.64,50.15,6.23" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,494.40,433.75,64.57,6.18;5,333.39,441.72,172.71,6.18">MatchZoo: A Learning, Practicing, and Developing System for Neural Text Matching</title>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,518.68,441.67,39.52,6.23;5,333.39,449.64,14.26,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1297" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,457.66,224.81,6.18;5,333.39,465.58,140.39,6.23" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,420.96,457.66,137.24,6.18;5,333.39,465.63,45.04,6.18">Term frequency normalisation tuning for BM25 and DFR models</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,390.89,465.58,53.27,6.23">Proceedings of ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="200" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,473.60,225.64,6.18;5,333.39,481.52,215.74,6.23" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="5,540.39,473.60,18.64,6.18;5,333.39,481.57,145.12,6.18">CEDR: Contextualized Embeddings for Document Ranking</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,491.01,481.52,55.27,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,489.54,224.99,6.18;5,333.39,497.51,224.81,6.18;5,333.18,505.43,129.39,6.23" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="5,408.87,489.54,98.39,6.18;5,528.90,489.54,29.48,6.18;5,333.39,497.51,224.81,6.18;5,333.18,505.48,18.54,6.18">University of Glasgow at TREC 2005: Experiments in Terabyte and Enterprise tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,363.71,505.43,69.52,6.23">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="193" />
		</imprint>
	</monogr>
	<note>Vassilis Plachouras, and Iadh Ounis</note>
</biblStruct>

<biblStruct coords="5,333.39,513.45,225.88,6.18;5,333.39,521.38,224.81,6.23;5,333.39,529.35,161.95,6.23" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="5,333.39,521.42,172.45,6.18">From Puppy to Maturity: Experiences in Developing Terrier</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrygo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,518.10,521.38,40.10,6.23;5,333.39,529.35,159.35,6.23">Proceedings of the SIGIR Workshop on Open Source Information Retrieval</title>
		<meeting>the SIGIR Workshop on Open Source Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,537.36,224.81,6.18;5,333.39,545.29,224.81,6.23;5,333.39,553.26,42.56,6.23" xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrygo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,514.39,537.36,43.81,6.18;5,333.39,545.29,224.81,6.23;5,333.39,553.26,22.00,6.23">About Learning Models with Multiple Query Dependent Features. Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,561.27,224.95,6.18;5,333.39,569.20,140.61,6.23" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="5,465.37,561.27,92.97,6.18;5,333.39,569.24,43.01,6.18">Upper Bound Approximation for BlockMaxWand</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,389.32,569.20,55.27,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,577.21,225.88,6.18;5,333.39,585.18,224.81,6.18;5,333.39,593.11,98.86,6.23" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="5,372.01,585.18,176.07,6.18">University of Glasgow at TREC 2009: Experiments with Terrier</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,333.39,593.11,69.52,6.23">Proceedings of TREC 2009</title>
		<meeting>TREC 2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,601.13,224.94,6.18;5,333.39,609.05,149.06,6.23" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="5,459.15,601.13,99.18,6.18;5,333.39,609.10,51.98,6.18">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,397.78,609.05,55.27,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,617.07,224.81,6.18;5,333.39,624.99,184.54,6.23" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="5,403.68,617.07,154.52,6.18;5,333.39,625.04,87.98,6.18">Automatic feature selection in the markov random field model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><forename type="middle">A</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,433.26,624.99,55.27,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,633.01,225.88,6.18;5,333.39,640.93,225.88,6.23;5,333.39,648.95,24.81,6.18" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="5,438.13,633.01,101.42,6.18;5,333.39,640.98,154.81,6.18">Incorporating term dependency in the DFR framework</title>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,500.88,640.93,55.53,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="843" to="844" />
		</imprint>
	</monogr>
	<note>Vassilis Plachouras, and Iadh Ounis</note>
</biblStruct>

<biblStruct coords="5,333.39,656.92,225.99,6.18;5,333.39,664.84,224.81,6.23;5,333.39,672.81,50.71,6.23" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="5,463.57,656.92,95.81,6.18;5,333.39,664.89,180.66,6.18">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,525.95,664.84,32.25,6.23;5,333.39,672.81,21.31,6.23">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,680.83,224.81,6.18;5,333.39,688.75,222.65,6.23" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="5,428.53,680.83,129.67,6.18;5,333.39,688.80,127.72,6.18">Reproducing and Generalizing Semantic Term Matching in Axiomatic Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,473.15,688.75,53.27,6.23">Proceedings of ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="369" to="381" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
