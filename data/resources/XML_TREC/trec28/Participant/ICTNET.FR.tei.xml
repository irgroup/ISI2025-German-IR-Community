<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.97,95.34,275.60,14.93">ICT at TREC 2019: Fair Ranking Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.19,129.07,58.47,10.37"><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<email>wangmeng18g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Laboratory of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.10,129.07,76.69,10.37"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
							<email>zhanghaopeng18g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Laboratory of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.24,129.07,64.09,10.37"><forename type="first">Fuhuai</forename><surname>Liang</surname></persName>
							<email>liangfuhuai18g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Laboratory of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.76,129.07,44.17,10.37"><forename type="first">Bin</forename><surname>Feng</surname></persName>
							<email>fengbin18s@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Laboratory of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.38,129.07,39.51,10.37"><forename type="first">Di</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Laboratory of Network Data Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.97,95.34,275.60,14.93">ICT at TREC 2019: Fair Ranking Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9ABE9484D19CD7EC1FEC149FCF346E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we will introduce our work in the 2019 TREC fair ranking task. In temporal academic search, more and more people choose to pay attention to the fairness constraints of ranking. The purpose of this task is to provide fairness exposure to different groups of authors, where the definition of group is diverse, such as based on demographics, height, topics, etc. For this reason, the model we design should consider the fairness of the ranking results while ensuring the relevance of the ranking results. We will show how to use our model to achieve the relevance of query results, then adjust the overall ranking results according to the fairness of documents, and finally achieve the relevance and fairness of document ordering. This paper will introduce the framework and methods of the fair ranking system, as well as the experimental results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the Internet age, people turn from the traditional library search to Internet information search. Rankings have become one of the dominant forms with which online systems present results to the user <ref type="bibr" coords="1,154.10,606.04,12.61,9.46" target="#b0">[1]</ref>. Traditional sorting is to rank items in descending order of relevance probability, because this is a common metric that is widely used in information retrieval. But when we rank authors, documents, and opinions, this userfocused thinking is no longer comprehensive <ref type="bibr" coords="1,271.20,673.79,14.30,9.46" target="#b1">[2]</ref>. In today's ranking system, you need to consider not only the relevance of the results, but also the fairness of the results.</p><p>To balance this conflicting metric, we try to use a variety of algorithms to rank documents so as to satisfy certain fairness concepts while en-suring relevance. We evaluated the impact of these two constraints against the evaluation criteria provided <ref type="bibr" coords="1,367.35,256.11,13.52,9.46" target="#b2">[3]</ref>. In this article, we will show how to use our framework to achieve relevance and fairness in document ordering.</p><p>This article is organized as follows: the first part is the data preprocessing, the second part is the document relevance ranking, then the ranking results are adjusted according to fairness, and the third part is the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data processing</head><p>The given data set is a Semantic(S2) open corpus from the Allen Institute of Artificial Intelligence(http://api.semanticscholar.org/corpus/), containing 47 1GB data files. Most of the data contains doc id, DOI, title, Abstract, Authors, and the data fields of a small part may be empty. There is also a query-sequence-generator.py file for generating more than 600 queries. Since the data set is too large and the data is in json format, it is in line with the characteristics of mongodb, so we consider importing the data into the mongodb database to enhance the query processing capability of the data. There are approximately 46,747,044 data after importing into the database. Finally, the data is indexed by doc id, because it is often necessary to query the data according to doc id. We paired each query with the corresponding document and found that there were only 4641 pairs of (query, document, relevance) data. The data format of each training set is as Figure <ref type="figure" coords="1,375.15,681.31,4.09,9.46" target="#fig_0">1</ref>. This is because the number of queries is rel-atively small, so there are only limited training samples. Despite this, there are still more than 1000 pieces of data in the 4641 training data, doc content, authors or title is empty. As a result, the amount of data is less and the training model will be less effective. The official has also considered this problem, given a script file that samples data by distribution, where we sample 100,000 pieces of data from a limited sample for training according to a given script file. In addition, the most important thing we need to consider is the author's Fairness and Relevance. The official also gives the correspondence between author and group, and as illustrated in Figure <ref type="figure" coords="2,221.99,242.81,4.09,9.46" target="#fig_1">2</ref>. Because in the end we evaluate, we need to calculate the Fairness of each author, so we need to add the author id and gid pair to the evaluation data. The data format of each evaluation data set is as Figure <ref type="figure" coords="2,125.03,472.45,4.09,9.46" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>To begin with, we use the existing model Conv-KNRM <ref type="bibr" coords="2,97.97,728.77,19.48,9.46" target="#b3">[4]</ref> to sort documents based on their similarity to the query and documents. The brief principle of the model is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolutional Kernel-based Neural Ranking Model</head><p>Given input query and document, the embedding layer maps their words into distributed representations pre-trained by BERT <ref type="bibr" coords="2,425.47,128.62,16.00,9.46" target="#b4">[5]</ref>, the convolutional layer generates n-gram embeddings; the crossmatch layer matches the query n-grams and document n-grams of different lengths, and forms the translation matrices; the kernel pooling layer generates soft-TF features and the learning-to-rank (LeToR) layer combines them to the ranking score.</p><p>Conv-KNRM can be learned end-to-end. Conv-KNRM, compared with KNRM, adds ngram convolution and increases the level of the original model. Conv-KNRM can capture more subtle semantic entities and cross-granularity is more fine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">N-gram Composing and Crossmatching</head><p>Given a query q and a document d, Conv-KNRM embeds their words by a word embedding layer, composes n-grams with a CNN layer, and crossmatches query n-grams and document n-grams of variant lengths to the translation matrices. Word Embedding Layer: First, the word embedding layer maps each word to an Ldimensional continuous vector (embedding). A text sequence of m words t 1 , ..., t m , and is modeled as an m * L matrix.</p><p>Convolutional Layer: Second, the convolutional layer applies convolution filters to compose n-grams from the text. For each window of h words, the filter sums up all elements in the h words'embeddings T (i:i+h) , weighted by the filter weights w ∈ R h L ,and produces a continuous score.</p><p>We use F different filters and get F scores. Then we add a bias and apply a nonlinear activation function, and obtain an F-dimensional embedding for the h-gram. When a convolution filter slides across the boundary of the text, we use h -1 &lt; P AD &gt; symbols for padding.</p><formula xml:id="formula_0" coords="2,315.12,674.24,210.43,14.19">g h i = relu W h • T i:i+h + b h , i = 1 . . . m (1)</formula><p>Cross-match Layer: Translation matrix's elements are the cosine similarity scores between the corresponding query-document n-gram pairs.</p><formula xml:id="formula_1" coords="2,360.36,749.99,165.19,16.16">M hq,h d i,j = cos g hq i , g h d j (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Ranking with N-gram Translations</head><p>Conv-KNRM uses the kernel-pooling technique and a learning-to-rank layer to calculate the ranking score using the n-gram translations M.</p><p>Kernel Pooling: Kernel-pooling uses K Gaussian kernels to count the soft matches of word or n-gram pairs at K different strength levels. Each kernel summarizes the translation scores as soft-TF counts in the region dened by its mean K k and width δ k . As a result, a translation matrix M is pooled to a K-dimensional soft-TF feature vector.</p><p>Learning to Rank: The learning-to-rank layer uses linear function to combine the soft-TF ranking features into a ranking score.</p><p>Loss Function: We use standard pairwise learning-to-rank loss function to train the model.</p><formula xml:id="formula_2" coords="3,86.88,320.36,203.39,44.26">l = q d + ,d -∈D +- q max(0, 1 -f (q, d + ) + f (q, d -)) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adjust for exposure</head><p>After sorting D q according to correlation with KNRM model, the sorted document π r is obtained. The ranking results at this time only take into account the relevance of query to document d. Suppose the result of KNRM model ranking is the optimal correlation ranking result we can get.</p><p>Next, consider the fairness of document sorting.</p><p>For the π q and π w in the sort result, swapping their positions at this point causes the overall sort result to be less relevant. At the same time, the exposure fairness for authors included in documents q and w may rise or fall.</p><p>The result of swapping document q, w is denoted as π s . The correlation change of the overall ranking result is denoted as ∆u π q,w .Document q precedes document w.</p><formula xml:id="formula_3" coords="3,79.17,639.37,206.86,85.31">∆u π q,w = n i=q [γ i-1 i-1 j=1 (1 -p(s|π s j ))]p(s|π s i )- n i=q [γ i-1 i-1 j=1 (1 -p(s|π j ))]p(s|π i )<label>(4</label></formula><p>) Swapping documents q and w does not affect author relevance. But it changes the exposure of the author.</p><p>If the author a ∈ q , then switch the document q and w positions, and the exposure change for author a is denoted as ∆e π a .</p><p>∆e π a =γ w-1 w-1 j=1</p><p>(1 -p(s|π s j ))-</p><formula xml:id="formula_4" coords="3,376.23,161.52,149.31,41.15">γ q-1 q-1 j=1 (1 -p(s|π j ))<label>(5)</label></formula><p>Based on this, it is possible to calculate the changes in exposure of all authors after swapping documents q and w.</p><p>Each author belongs to one of |G| groups. After calculating the exposure changes of each author, the exposure changes of group g after exchanging documents a and b can be obtained. The change in group g's exposure is denoted as ∆ε g .</p><formula xml:id="formula_5" coords="3,378.73,340.26,146.81,22.26">∆ε g = a∈Ag ∆e a<label>(6)</label></formula><p>The final fair exposure change can be calculated based on the change in the exposure of each group g. In order not to conflict with the symbol of the guideline, the final fair exposure change is denoted as ∆f .</p><formula xml:id="formula_6" coords="3,351.80,457.50,173.75,23.41">∆f = g∈G ∆ε 2 -2∆ε g R g (7)</formula><p>Adjust the parameter so that ∆u π q,w and ∆f are in the same range.</p><p>The next step is to determine whether document q and document w should be swapped based on the correlation change ∆u π q,w and fair exposure changes ∆f . If ∆f is greater than ∆u π q,w , then document q and document w are exchanged. Adjust all the sorting results in this way.</p><p>For the sake of time complexity, when deciding whether document q and document w should be switched, only adjacent documents should be taken, and the whole sorting result, π , should only be iterated once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation result</head><p>We submitted a ranking result, and the evaluation results are shown in table <ref type="table" coords="3,419.37,728.77,5.45,9.46" target="#tab_0">1</ref>  As can be seen, after our strategy ranking, the fairness of the overall results is in the middle level among all submitted results. In the h-index evaluation results, even close to the best results.</p><p>However, the utility is poor, which is caused by the fact that the ranking based on fairness destroys the relevance of the ranking results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The fairness of sorting in retrieval system is very important. It is very difficult to consider both relevance and fair exposure in the ranking results.</p><p>In this Track, our strategy is to first consider the relevance of documents and queries, and then adjust the results according to fair exposure. However, due to time complexity, we did not iterate the adjust process many times. This approach has a high time complexity and is difficult to practice in systems with short response times. We think it is a good method to consider the fairness and relevance of the ranking results in the same stage of ranking, and it is also a direction that our method can be improved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,315.32,731.54,202.18,9.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The data format of each training set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,72.00,377.58,218.27,9.46;2,72.00,391.13,25.45,9.46;2,94.73,265.84,170.08,95.33"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The correspondence between author and group</figDesc><graphic coords="2,94.73,265.84,170.08,95.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,72.00,542.69,218.27,9.46;2,72.00,556.24,12.12,9.46"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The data format of each evaluation data set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,307.28,728.77,218.27,36.56"><head>Table 1 :</head><label>1</label><figDesc>and table 2. Validation sets are used to group authors based on h-index and level respectively. Evaluation on Groups Based on H-index Scores</figDesc><table coords="4,97.03,100.77,168.21,230.99"><row><cell>SeqID</cell><cell>util</cell><cell></cell><cell cols="2">unfairness</cell></row><row><cell></cell><cell>mean</cell><cell>run</cell><cell>mean</cell><cell>run</cell></row><row><cell>0</cell><cell>0.608679</cell><cell>0.549371</cell><cell>0.074779</cell><cell>0.044143</cell></row><row><cell>1</cell><cell>0.609166</cell><cell>0.553952</cell><cell>0.076113</cell><cell>0.045979</cell></row><row><cell>2</cell><cell>0.609620</cell><cell>0.550504</cell><cell>0.074923</cell><cell>0.046058</cell></row><row><cell>3</cell><cell>0.609211</cell><cell>0.547126</cell><cell>0.075145</cell><cell>0.046286</cell></row><row><cell>4</cell><cell>0.611033</cell><cell>0.552450</cell><cell>0.075360</cell><cell>0.045370</cell></row><row><cell cols="5">Table 2: Evaluation on Groups Based</cell></row><row><cell cols="3">on Level Scores</cell><cell></cell><cell></cell></row><row><cell>SeqID</cell><cell>util</cell><cell></cell><cell cols="2">unfairness</cell></row><row><cell></cell><cell>mean</cell><cell>run</cell><cell>mean</cell><cell>run</cell></row><row><cell>0</cell><cell>0.608679</cell><cell>0.549371</cell><cell>0.041605</cell><cell>0.039809</cell></row><row><cell>1</cell><cell>0.609166</cell><cell>0.553952</cell><cell>0.043898</cell><cell>0.051944</cell></row><row><cell>2</cell><cell>0.609620</cell><cell>0.550504</cell><cell>0.044432</cell><cell>0.042368</cell></row><row><cell>3</cell><cell>0.609211</cell><cell>0.547126</cell><cell>0.042603</cell><cell>0.042178</cell></row><row><cell>4</cell><cell>0.611033</cell><cell>0.552450</cell><cell>0.044838</cell><cell>0.037862</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,90.17,742.32,200.09,9.46;4,90.17,755.68,200.09,9.64;4,325.45,66.48,200.09,9.39;4,325.45,80.03,200.09,9.39;4,325.45,93.58,155.92,9.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,269.44,742.32,20.83,9.46;4,90.17,755.86,124.03,9.46">Fairness of exposure in rankings</title>
		<author>
			<persName coords=""><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,236.23,755.68,54.04,9.39;4,325.45,66.48,200.09,9.39;4,325.45,80.03,200.09,9.39;4,325.45,93.58,12.50,9.39">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,325.45,116.28,200.09,9.46;4,325.45,129.83,200.09,9.46;4,325.45,143.19,200.09,9.64;4,325.45,156.93,24.55,9.46" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Elisa</forename><surname>Celis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damian</forename><surname>Straszak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nisheeth</forename><forename type="middle">K</forename><surname>Vishnoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06840</idno>
		<title level="m" coord="4,426.63,129.83,98.91,9.46;4,325.45,143.38,45.83,9.46">Ranking with fairness constraints</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,325.45,179.44,200.10,9.46;4,325.45,192.81,200.09,9.64;4,325.45,206.36,200.09,9.39;4,325.45,219.90,200.09,9.39;4,325.45,233.64,94.10,9.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,478.89,179.44,46.66,9.46;4,325.45,192.99,116.97,9.46">Measuring fairness in ranked outputs</title>
		<author>
			<persName coords=""><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Stoyanovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,471.50,192.81,54.04,9.39;4,325.45,206.36,200.09,9.39;4,325.45,219.90,194.69,9.39">Proceedings of the 29th International Conference on Scientific and Statistical Database Management</title>
		<meeting>the 29th International Conference on Scientific and Statistical Database Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,325.45,256.16,200.09,9.46;4,325.45,269.70,200.09,9.46;4,325.45,283.25,200.09,9.46;4,325.45,296.62,200.09,9.64;4,325.45,310.17,200.09,9.39;4,325.45,323.72,183.50,9.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,411.51,269.70,114.03,9.46;4,325.45,283.25,200.09,9.46;4,325.45,296.80,26.22,9.46">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,375.93,296.62,149.61,9.39;4,325.45,310.17,200.09,9.39;4,325.45,323.72,50.44,9.39">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,325.45,346.42,200.09,9.46;4,325.45,359.97,200.09,9.46;4,325.45,373.52,200.09,9.46;4,325.45,386.88,200.10,9.64;4,325.45,400.43,109.69,9.64" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="4,507.37,359.97,18.17,9.46;4,325.45,373.52,200.09,9.46;4,325.45,387.06,121.27,9.46">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
