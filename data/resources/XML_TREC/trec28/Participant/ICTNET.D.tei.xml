<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,184.20,71.79,229.13,12.90">ICTNET at Trec 2019 Decision Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.48,111.83,67.99,10.75"><forename type="first">Wanqing</forename><surname>Cui</surname></persName>
							<email>cuiwanqing18z@ict.ac.cn</email>
						</author>
						<author>
							<persName coords="1,217.65,111.83,51.13,10.75"><forename type="first">Yan</forename><surname>Jiang</surname></persName>
							<email>jiangyan18s@ict.ac.cn</email>
						</author>
						<author>
							<persName coords="1,287.96,111.83,72.32,10.75"><forename type="first">Shuchang</forename><surname>Tao</surname></persName>
							<email>taoshuchang18z@ict.ac.cn</email>
						</author>
						<author>
							<persName coords="1,379.45,111.83,77.40,10.75"><forename type="first">Hanzhang</forename><surname>Guo</surname></persName>
							<email>guohanzhang18s@ict.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,184.20,71.79,229.13,12.90">ICTNET at Trec 2019 Decision Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">573849C1F15C141B85DB02290BD1B958</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we report on our participation in the Trec 2019 Decision Track[1] which aims to provide a venue for research on retrieval methods that promote better decision making with search engines and develop new online and offline evaluation methods to predict the decision making quality induced by search results. We convert this task into a standard information retrieval task and use traditional IR model. Finally we give a summary for the solution of our work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Search engine results underpin many consequential decision making tasks. Such as seeking health advice online, deciding the best treatment/diagnosis/test for a patient. However, when the search occurs within uncontrolled data collections, such as the web, where information can be unreliable, generally misleading, too technical, and can lead to unfounded escalations. Information from search engine results can significantly influence decisions, and research shows that increasing the amount of incorrect information about a topic presented in a SERP can impel users to take incorrect decisions.</p><p>So this track focus on encouraging participants to design search techniques that promote correct information over incorrect information. Given a data collection and a set of topics, participants should return relevant and credible information that will help searchers make correct decisions. Besides, more than simply define what is relevant, there are 3 types of results: correct and relevant, incorrect, and non-relevant. And the goal is to return relevant and correct information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>This track is the retrieval task. Given the topic and narrative as query, we are supposed to return the most relevant and credible document. This is essentially a text matching problem. The higher the matching degree between the query and the document, the higher its ranking should be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Terrier</head><p>Terrier is a highly flexible, efficient, and effective open source search engine, readily deployable on large-scale collections of documents. Terrier implements state-of-the-art indexing and retrieval functionalities, and provides an ideal platform for the rapid development and evaluation of large-scale retrieval applications.</p><p>With the Terrier tool, we can easily create an index. The first step is to collect documents from the entire document set, and get a list. We can enter the command under the project directory:</p><p>./bin/trec_setup.sh "path/to/docset" The second step is index creation.</p><p>Enter "./bin/trec terrier.sh -i", we will get the index of the document set, but this need a long time. Finally, we search for the result of topics. Enter "./bin/trec terrier.sh -r", and then we will get the results under the directory "./var/results".</p><p>In the meantime, before the second step, we can configure the index, such as the selected retrieval method, which tags in the document we indexed, and so on. These are achieved by configuring the file "terrier.properties", which can be referred to in its official documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BM25</head><p>Our retrieval method uses BM25, which is an algorithm used to evaluate the correlation between search terms and documents. It is based on the probability retrieval model. BM25 algorithm expands the scoring function of binary independent model by adding document weights and query weights. This extension is based on probability theory and experimental verification, and is not a formal model. On the basis of binary independent model, BM25 model takes into account the weight of words in query and the weight of words in documents, fits the formula of synthesizing the above considerations, and introduces empirical parameters through experiments.</p><p>Compared with tf-idf, BM25 makes better use of the fact that the relationship between word frequency and correlation is non-linear. Its general formula is as follows:</p><formula xml:id="formula_0" coords="2,110.96,222.41,179.30,33.17">Score (Q, d) = i n W i R (q i , d)<label>(1)</label></formula><p>Among them, Q is the query, d is the document, q i is the word in the query, and W i is the word weight. Word weights can actually be calculated using idf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DSSM</head><p>Deep Structured Semantic Models (DSSM) <ref type="bibr" coords="2,273.70,348.24,16.56,9.46" target="#b1">[2]</ref> uses a deep neural network to rank a set of documents for a given query as follows. First, a non-linear projection is performed to map the query and the documents to a common semantic space. Then, the relevance of each document given the query is calculated as the cosine similarity between their vectors in that semantic space. The neural network models are discriminatively trained using the clickthrough data such that the conditional likelihood of the clicked document given the query is maximized. Furthermore, to deal with large vocabularies, a word hashing method is used, through which the highdimensional term vectors of queries or documents are projected to low-dimensional letter based ngram vectors with little information loss.</p><p>The semantic relevance score between a query Q and a document D is then measured as:</p><formula xml:id="formula_1" coords="2,79.62,613.44,210.64,29.16">R(Q, D) = cosine(y Q , y D ) = y T Q y D ||y Q ||||y D ||<label>(2)</label></formula><p>Where y Q , y D are the concept vectors of the query and the document, respectively. They are the output of DNN.</p><p>So the posterior probability of a document given a query from the semantic relevance score between them is:</p><formula xml:id="formula_2" coords="2,109.37,741.45,180.89,26.38">p(D + |Q) = exp(γR(Q, D + )) exp(γR(Q, D))<label>(3)</label></formula><p>2.4 CDSSM CDSSM <ref type="bibr" coords="2,346.66,84.14,12.72,9.46" target="#b2">[3]</ref> is an improvement over DSSM. Compared to DSSM, it uses CNN to learn the lowerdimensional semantic vectors of queries and documents. By using the convolution-max pooling operation, local contextual information at the word n-gram level is modeled first. Then, salient local features in a word sequence are combined to form a global feature vector. Finally, the highlevel semantic information of the word sequence is extracted to form a global vector representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">DRMM</head><p>Deep relevance matching model (DRMM) <ref type="bibr" coords="2,493.42,245.51,12.72,9.46" target="#b3">[4]</ref> employs a joint deep architecture at the query term level for relevance matching. It first builds local interactions between each pair of terms from a query and a document based on term embeddings.</p><p>For each query term, DRMM maps the variablelength local interactions into a fixed-length matching histogram. Based on this fixed-length matching histogram, then a feed forward matching network is employed to learn hierarchical matching patterns and produce a matching score. Finally, the overall matching score is generated by aggregating the scores from each query term with a term gating network computing the aggregation weights. We show how our major model designs, including matching histogram mapping, a feed forward matching network, and a term gating network, address the three key factors in relevance matching for ad-hoc retrieval. Suppose both query and document are represented as a set of term vectors denoted by q = {w (q) 1 , ..., w (q) M } and d = {w </p><formula xml:id="formula_3" coords="2,374.18,576.46,151.36,16.00">z (0) i = h(w (q) i ⊗ d)<label>(4)</label></formula><formula xml:id="formula_4" coords="2,346.75,596.23,178.79,16.00">z (l) i = tanh(W (l) z (l-1) i + b (l) )<label>(5)</label></formula><formula xml:id="formula_5" coords="2,384.30,616.57,141.24,33.71">s = M i=1 g i z (L) i<label>(6)</label></formula><p>Where h denotes the mapping function from local interactions to matching histogram, z (l) i , l = 0, ..., L denotes the intermediate hidden layers for the i-th query term, and g i , i = 1, ..., M denotes the aggregation w eight produced by the term gating network. W (l) denotes the l-th weight matrix and b (l) denotes the l-th bias term, which are shared across different query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">MatchPyramid</head><p>MatchPyramid <ref type="bibr" coords="3,141.84,84.30,12.72,9.46" target="#b4">[5]</ref> models text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layerby-layer way.</p><p>For two texts W and V , first represent the input of text matching as a matching matrix M , with each element M ij standing for the basic interaction, i.e. similarity between word w i and v j :</p><formula xml:id="formula_6" coords="3,147.84,246.95,142.43,10.63">M ij = w i ⊗ v j<label>(7)</label></formula><p>In this way, we can view the matching matrix M as an image, where each entry (i.e. the similarity between two words) stands for the corresponding pixel value. And different kinds of ⊗ can be adopted to model the interactions between two words, leading to different kinds of raw images.</p><p>Then M can be input to CNN to extract different levels of feature maps z from different layers. And a MLP (Multi-Layer Perception) is used to produce the final matching score:</p><formula xml:id="formula_7" coords="3,104.85,427.84,185.42,13.13">(s0, s1) T = W 2 σ(W 1 z + b1) + b 2 (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we first describe the datasets and then the methods we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>ClueWeb12-B13 was utilised for the decision track. It was created to support research on information retrieval and related human language technologies. The dataset consists of 733,019,372 English web pages, collected between February 10, 2012 and May 10, 2012. ClueWeb12 is a companion or successor to the ClueWeb09 web dataset. Unlike previous tracks, the assessors will be provided the topic query and narrative. And the topics will be provided as XML files.</p><p>The dataset are shown as the following examples: </p><formula xml:id="formula_8" coords="3,72.00,702.69,45.82,7.68">WARC/1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Some attempts</head><p>First, we read data with warcat directly. However, the web page enters the folder named after the URL, divided by / after decompression.</p><p>It is hard to index them with "ClueWeb12 B13 DocID To URL.txt.bz2". Because the file names are often modified when unzipping. For example, the "//" is replaced by"/ index da39a3/ index da39a3", "/?" is replaced by "/ index da39a3 ", "=http://" is replaced by "=http ", and so on. What's worse, the modification doesn't happen all the time. Without the knowledge of decompression coding, we can only change the name by trial and error.</p><p>The size of ClueWeb12-B13 is 382GB after compressed. After uncompressed the size of it is 1.95TB, with a subset about 50 million pages. Since the data is extremely large, we try to use other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Decompressing</head><p>We use gunzip <ref type="bibr" coords="4,148.64,170.81,14.14,9.46" target="#b5">[6]</ref> command together with warcio.archiveiterator <ref type="bibr" coords="4,167.21,184.36,12.98,9.46" target="#b6">[7]</ref> toolbox instead .</p><p>We firstly extract <ref type="bibr" coords="4,159.67,197.91,12.67,9.46" target="#b7">[8]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Convert</head><p>We need to convert the HTML document we extracted to XML. Here we use Python to write scripts for processing. HTML documents can be read by calling the library BeautifulSoup, but the data format is confusing and not the text of the web page. So we call html2text to extract the text from the previous cluttered string, and the data is in markdown format. In order to reduce the impact of hyperlinks on the retrieval results, we use the following regular expressions to remove the hyperlinks: text = re.sub(r'\(http[\s\S] * ?\)', '', text)</p><p>Ultimately, our XML document retains the head and body content of the original html, which is implemented by calling minidom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Processing</head><p>In this part, we use Terrier as a search tool. Firstly, the document is preprocessed, the HTML document is converted into the required XML format, and then the index is established. Finally, we get the results about the given topics. BM25 is used as our retrieval method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We also tried other models, such as DRMM, Match Pyramid. We didn't adopt them in the end due to the difficulty of data processing and the large amount of data. Because of the time to build the index and process the data, we finally submitted the retrieval results only on some of the datasets. In the future, we will try the PageRank method to take advantage of a large number of web links in the data. We will also improve the performance of the model to better adapt to large-scale data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,429.91,527.49,10.94,6.99;2,429.62,536.37,4.23,6.99;2,441.35,531.23,26.59,9.57;2,468.24,527.49,10.94,6.99;2,467.95,536.68,6.72,6.99;2,479.69,531.23,45.86,9.81;2,307.28,544.78,87.87,9.81"><head></head><label></label><figDesc>the final relevance score s is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,197.91,222.55,150.54"><head></head><label></label><figDesc>the xxxx.warc.gz file to a xxxx.warc file. Then we use the following python code to extract the warc file into an .html file.</figDesc><table coords="4,72.00,245.98,222.55,102.47"><row><cell>from warcio.archiveiterator import</cell></row><row><cell>ArchiveIterator</cell></row><row><cell>for record in ArchiveIterator</cell></row><row><cell>(stream):</cell></row><row><cell>filename =record.rec_headers.</cell></row><row><cell>get_header('WARC-TREC-ID')</cell></row><row><cell>tr = record.content_stream()</cell></row><row><cell>text = str(tr.read())</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,318.89,229.82,206.65,8.64;4,318.19,241.72,158.41,7.01" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,378.74,229.82,75.99,8.64">Trec decision track</title>
		<ptr target="https://trec-decision.github.io/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>TREC group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,318.89,260.71,206.66,8.64;4,318.19,271.66,207.37,8.64;4,318.19,282.62,207.37,8.64;4,318.19,293.40,207.36,8.82;4,318.19,304.36,207.37,8.59;4,318.19,315.32,207.37,8.82;4,318.19,326.46,22.42,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,489.59,271.66,35.97,8.64;4,318.19,282.62,207.37,8.64;4,318.19,293.58,85.33,8.64">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,425.89,293.40,99.66,8.59;4,318.19,304.36,207.37,8.59;4,318.19,315.32,97.43,8.59">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,318.89,346.38,206.66,8.64;4,318.19,357.34,207.36,8.64;4,318.19,368.30,207.37,8.64;4,318.19,379.08,207.36,8.82;4,318.19,390.04,207.36,8.82;4,318.19,401.18,49.69,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,410.62,357.34,114.92,8.64;4,318.19,368.30,207.37,8.64;4,318.19,379.26,23.95,8.64">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName coords=""><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,368.06,379.08,157.48,8.59;4,318.19,390.04,132.60,8.59">Proceedings of the 23rd International Conference on World Wide Web</title>
		<meeting>the 23rd International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,318.89,421.10,206.66,8.64;4,318.19,432.06,207.37,8.64;4,318.19,442.84,207.36,8.82;4,318.19,453.80,207.37,8.59;4,318.19,464.76,182.76,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,345.61,432.06,179.94,8.64;4,318.19,443.02,31.67,8.64">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,374.66,442.84,150.89,8.59;4,318.19,453.80,207.37,8.59;4,318.19,464.76,70.57,8.59">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,318.89,484.87,206.66,8.64;4,318.19,495.82,207.37,8.64;4,318.19,506.60,207.36,8.82;4,318.19,517.56,124.71,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,467.73,495.82,57.83,8.64;4,318.19,506.78,81.63,8.64">Text matching as image recognition</title>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,418.43,506.60,107.12,8.59;4,318.19,517.56,96.00,8.59">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,318.89,537.67,116.42,8.64;4,455.12,537.67,70.44,8.64;4,318.19,548.63,72.02,8.64;4,405.99,549.56,119.56,7.01;4,318.19,560.52,181.82,7.01" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,455.12,537.67,70.44,8.64;4,318.19,548.63,38.11,8.64">Gnu free codumentation</title>
		<ptr target="https://www.gnu.org/software/gzip/manual/gzip.html" />
	</analytic>
	<monogr>
		<title level="m" coord="4,318.89,537.67,112.08,8.64">Free Software Foundation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,318.89,579.51,206.66,8.64;4,318.19,590.47,77.48,8.64;4,411.97,591.41,113.58,7.01;4,318.19,602.36,110.09,7.01" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="4,385.08,579.51,140.48,8.64;4,318.19,590.47,44.05,8.64">Warcio: Warc (and arc) streaming library</title>
		<author>
			<persName coords=""><surname>Webrecorder</surname></persName>
		</author>
		<ptr target="https://github.com/webrecorder/warcio" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,318.89,621.35,126.39,8.64;4,464.20,621.35,61.35,8.64;4,318.19,633.25,193.78,7.01" xml:id="b7">
	<analytic>
		<title/>
		<ptr target="https://pypi.org/project/Warcat/" />
	</analytic>
	<monogr>
		<title level="j" coord="4,318.89,621.35,126.39,8.64;4,464.20,621.35,26.11,8.64">Python Software Foundation. Warcat</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
