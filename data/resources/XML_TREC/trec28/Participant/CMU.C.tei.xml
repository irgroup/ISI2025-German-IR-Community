<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,53.92,83.74,503.57,15.10">A Step towards Context Identification for Conversational Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.39,113.49,75.82,10.59"><forename type="first">Vaibhav</forename><surname>Kumar</surname></persName>
							<email>vaibhav2@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.63,113.49,61.77,10.59"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,53.92,83.74,503.57,15.10">A Step towards Context Identification for Conversational Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8586AE91D5E3E39D127649A73E8E2D76</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>KL-Divergence</term>
					<term>ery Expansion</term>
					<term>Indri</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>e system comprises of three di erent components.</p><p>e rst component makes a decision whether to incorporate contextual information for the current query in ongoing conversation. e decision is based on the KL-divergence between the retrieved documents for the original query and whether the query consists of pronouns. e second component identi es the contextual information (if required) for the answering the current query. is identi cation is performed using an SVM classi er which uses BERT a ention weights along with other linguistic features. Finally, the third component utilises Indri for document retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>e TREC 2019 Conversational Assistance Track (2019) introduces a dataset for evaluation of Conversational Information Seeking (CIS) Systems. e task, as de ned by the track, is to read the dialogue context for each conversation and extract vital information which is necessary for retrieving documents that can answer the current query (dialogue) under consideration. is creates challenges for the current search systems which are highly tuned to retrieve documents for queries which are self-contained i.e no additional information is required to answer them.</p><p>More formally, for a conversation C, which contains queries q ∈ (q 1 , q 2 ..q n ), the system should be able to generate candidate documents for each q i by conditioning on q i and information extracted from all preceding queries q 1 ..q i-1 .</p><p>e retrieved documents should be capable of answering the current query. Here, all the queries q are posed as natural language sentences.</p><p>Based on the training dataset, the queries can majorly be grouped into two categories. e rst category (Cat1) contains self-contained queries where no additional information may be required for answering it. On the other hand, the second category (Cat2) contains queries where contextual information is absolutely necessary for answering it. It is possible that a particular conversation can have queries belonging to both these categories or may simply contain queries from one of the two categories. Moreover, the second category of queries can be further divided into two sub-categories, namely, Cat2-exp and Cat2-imp. Cat2-exp consists of queries which have explicit contextual markers like pronouns that needs to be resolved to form a self-contained query. Cat2-imp consists of queries which does not have explicit but rather implicit (zero pronoun) markers. Examples of all the types can be seen from Table <ref type="table" coords="1,268.52,657.63,3.07,7.94">1</ref>.</p><p>e types of the aforementioned query classes motivates the structure of the implemented system.</p><p>e entire system can be sub-divided into three components. To summarise, the pipeline of the implemented system is as follows:</p><p>• If the query belongs to Cat2-exp i.e., it consists of an explicit marker (pronoun), then the pronoun is resolved using a classi er. e resolution of pronouns is done using an SVM which takes the BERT <ref type="bibr" coords="1,446.57,191.44,13.48,7.94" target="#b0">[1]</ref> a ention values and other linguistic features as input. If the subsequent queries have explicit markers, then the identi ed context is carried over.</p><p>• If the query does not belong to Cat2-exp, then the KLdivergence of the top retrieved documents (based on the unmodi ed query) is used to judge whether it belongs to Cat1 or Cat2-imp. If the current query belongs to Cat2-imp, then the context identi ed using the classi er is appended to it. However, if it belongs to Cat1, then no additional context is appended. It is also assumed that this marks a contextual shi and that the previously identi ed context cannot be carried over i.e a new context needs to be generated for subsequent queries belonging to Cat2-exp and Cat2-imp. • Finally, retrieval is performed using Indri. Two out of the four runs utilise the pipeline mentioned above. Out of these two, one (coref chis qe) utilises query expansion and the other (coref cshi ) does not. e third run (ensemble) is an ensemble of four di erent retrieval systems. Two of the four systems comprise of coref chis qe and coref chis . e third system uses heuristics to identify the major topic of the conversation based on its rst query and appends it to all subsequent queries.</p><p>e fourth system discards the assumption of contextual shi and appends all possible contexts to the query which have been identied upto that point. e fourth run (manual indri)simply utilises manually rewri en queries without any preprocessing and retreives using Indri.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEM ARCHITECTURE</head><p>is section describes the three components of the system. e rst component is responsible for context identi cation, the second is responsible for identifying contextual shi s, and the third is responsible for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context Identi cation</head><p>Context identi cation is performed using an SVM classi er with a variety of features. e dataset for training the classi er is based on the training topics provided by the track. Both implicit and explicit markers were manually resolved in order to construct the training dataset. For example: In Table <ref type="table" coords="1,430.67,624.75,3.09,7.94">1</ref>, for Cat2-imp, "about" in 1 5 was resolved to "average starting salary" in 1 4. An 80 -20 split was used for training and evaluation.</p><p>At the time of testing, each pronoun token in the current query is matched against all the tokens in the previous query. e classi er merely states whether to include the previous token <ref type="bibr" coords="1,521.68,679.54,9.70,7.94" target="#b0">(1)</ref> or not (0). us, the problem of selecting context is posed as a binary classi cation problem. However, a slight modi cation is made to In 15 2, "about" implicitly refers to "average starting salary". Here "about" is not a pronoun in a conventional sense. Table <ref type="table" coords="2,238.26,218.98,3.45,7.94">1</ref>: Example of various query categories.  the nal result. It might be possible that the classi er selects a discontinuous set of tokens from the previous query. For example: In Table <ref type="table" coords="2,352.20,281.07,3.13,7.94">1</ref>, for Cat2-imp, only "average" and "salary" might be selected by the classi er, while leaving the term "starting" out. In order to overcome such a phenomena, the tokens le out in between the selected tokens are added as well.</p><p>As mentioned earlier a variety of features are used by the classi er. An investigation of the BERT a ention weights<ref type="foot" coords="2,520.72,333.71,3.38,6.44" target="#foot_0">1</ref> revealed that it might be helpful in identifying the important context (resolving pronouns in particular). For example: In Figure <ref type="figure" coords="2,519.16,357.78,3.03,7.94" target="#fig_1">1</ref>, it can be observed that "it" has highest a ention weights over "the neolithic revolution" and in Figure <ref type="figure" coords="2,413.34,379.70,3.11,7.94" target="#fig_2">2</ref>, "it" has highest a ention values over "olympiad". is suggests that utilising the BERT a ention values might help in be er classi cation. All the experiments utilise the BERT-base-uncased models and the corresponding a ention values of the 9th and the 11th Layer.</p><p>Apart from the BERT a ention values, various linguistic features are used as well. ese features are: POS tags of the current and the previous query tokens, binary indicator for plurality of the current and the previous query tokens, binary indicator for stopwords, binary indicator suggesting whether the two tokens appeared together in some other previous query, rank of the a ention values, number of token matches between the two queries, POS tags of the neighbouring tokens.</p><p>An SVM utilsing the above mentioned features achieves a training accuracy of 89.97% and a testing accuracy of 91.10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Identi cation of Contextual Shi</head><p>If a query consists of a pronoun, then it is resolved using the method described in Section 2.1. However, if this is not the case, then there are two possibilities: either the query is of type Cat1 or of type Cat2imp. If the query belongs to Cat1, then it is a self-contained query and demarcates a contextual shi . If the query belongs to Cat2imp, then the action to be performed is to carry over the previous context (if available) or extract a new context. e new context is extracted based on a similar procedure as mentioned earlier with a slight modi cation. Since this current query has no pronouns, all its tokens are matched against the tokens of the previous query.</p><p>Overall, the entire problem of identi cation of contextual shi can be posed as a binary classi cation problem once again.</p><p>It can be well observed that Cat1 queries contain important tokens, which if used as is can retrieve a good set of documents capable of providing an answer to it. For example: A query of category Cat1 15 5: "Tell me about the linguistics as a eld", if used as is for retrieval should retrieve a consistent set of documents which contains terms related to the particular query. One can expect that the top documents, must be somehow related to the topic "history" and "linguistics" even though such documents might not provide a relevant answer. On the contrary, one can expect that a query of type Cat2-imp, will not be able to retrieve documents which are consistent in the terms of the topics they contain. For example: 1 5: "What about in the US?", the retrieved documents for this query might contain the term "US", but all the documents might not talk about one particular aspect related to "US". Some documents might talk about "politics in US", some would mention "tourism in US". us, there is a variety of unique terms that the top retrieved documents for a such query would contain signifying that this particular query needs some salient piece of information for be er on-topic retrieval.</p><p>Based on the argument presented above, contextual shi can be identi ed based on the KL-divergence of the top retrieved documents. A low KL-divergence would signify that the retrieved documents correspond to similar topics and thus would denote a contextual shi . On the other hand, a higher KL-divergence would mean that the retrieved document pool consists of a variety of topics. is would denote that there is an implied context that needs to be resolved.</p><p>e average of the KL-divergence between pairs of the top 20 documents is computed. A threshold is set below which a contextual shi is identi ed, thus requiring no additional terms. It is to note that the KL-divergence is not symmetric and hence pairs are formed in a rank-wise sorted order i,e (d i , d i+1 ) is a valid pair whereas d i+1 , d i is not. Here d i represents the document at rank i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval Using Indri</head><p>Finally, retrieval is performed using Indri in a completely unsupervised fashion. e stop words from the original query are not removed. However, if a context has been appended to the query, then stopwords from the appended context is removed before retrieval.</p><p>Run1 (coref chis qe) of the system comprises of all the components as mentioned above. Adding to it, it also uses query expansion. Run2 (coref chis ) of the system is the same as Run1 except for the query expansion part.</p><p>Run3 (ensemble) is an ensemble of four di erent systems. e rst two systems are the results of Run1 and Run2.</p><p>e third system utilises a simple heuristic to identify the main topic of the conversation and then appends it to the subsequent queries.</p><p>e heuristic is to select the longest noun phrase and adjacent adjectives to this phrase. is serves as the context for the entire conversation. e fourth system discards the component which identi es contextual shi and instead appends all possible contexts that has been identi ed from the beginning. e results of the four Rund Id P@5 P@10 P@20 P@100 coref cshi 0.3977 0. Run4 (manual indri) simply uses the manually rewri en queries and performs retreival using Indri.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head><p>e results can be seen in Tables 2, 3 and 4. Table <ref type="table" coords="3,506.06,456.04,4.25,7.94" target="#tab_0">2</ref> presents the Precision scores at di erent levels for the various runs. Table <ref type="table" coords="3,538.72,466.99,4.09,7.94" target="#tab_1">3</ref> and 4 presents the Recall and NDCG values for the di erent runs.</p><p>It is quite evident that manual indri outperforms the other systems by a signi cant margin. is points towards the low e cacy of the classi er responsible for selecting contextual terms. Perhaps, a good classi er is essential for a be er performance on the task. Apart from this, query expansion seems to have helped. As it can be seen, the results of coref cshi qe is be er than that of coref cshi .</p><p>Amongst the non-manual runs, ensemble seems to have performed best. is gain could be a ributed to the heuristic which might have selected contextual terms if the classi er missed out any.</p><p>In future, we would like to explore more sophisticated methods for context identi cation which are end-to-end trainable and work well even in dearth of training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,59.60,67.89,7.97;2,435.63,59.60,122.57,7.97;2,154.15,84.89,37.05,7.94;2,235.07,84.89,63.64,7.94;2,376.69,84.89,35.16,7.94;2,164.22,112.81,16.89,7.94;2,202.22,96.26,117.42,7.94;2,244.63,107.22,32.60,7.94;2,201.16,118.18,119.56,7.94;2,224.06,129.14,73.75,7.94;2,347.56,107.22,93.42,7.94;2,352.92,118.18,82.70,7.94;2,156.21,151.57,32.92,7.94;2,202.38,145.98,117.12,7.94;2,208.73,156.93,104.41,7.94;2,344.08,140.50,100.38,7.94;2,339.81,151.45,108.91,7.94;2,353.12,162.41,82.30,7.94;2,155.62,190.32,34.09,7.94;2,203.42,179.25,115.04,7.94;2,229.25,190.21,63.36,7.94;2,212.54,201.17,96.78,7.94"><head></head><label></label><figDesc>What kind of problems can I expect? 15 5:Tell me about the history of linguistics as a eld. 15 5 does not require any additional information Cat2-exp 25 1:Tell me about Orca whales. 25 2:Are they really whales? " ey" in 25 2 is a pronoun (explicit marker )which needs to be resolved to Orca. Cat2-imp 1 4:What's the average starting salary in the UK? 1 5:What about in the US?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,53.80,403.05,240.25,7.94;2,53.80,414.01,241.78,7.94;2,53.80,424.97,240.24,7.96;2,53.80,435.93,241.85,7.96;2,53.80,446.89,165.01,7.94;2,53.80,255.04,252.20,134.26"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attention weights of the 9th Layer of BERT. e di erent color boxes represent the various attention heads.On the le side query 4 2 is presented and on the right side query 4 1 is presented. It can be clearly seen that the pronoun "it" maps to "neolithic revolution".</figDesc><graphic coords="2,53.80,255.04,252.20,134.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,53.80,647.16,240.25,7.94;2,53.80,658.12,240.25,7.94;2,53.80,669.08,240.25,7.96;2,53.80,680.04,241.85,7.96;2,53.80,691.00,122.20,7.94;2,53.80,464.02,252.20,169.39"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention weights of the 9th Layer of BERT. e di erent color boxes represent the various attention heads On the le side query 15 2 is presented and on the right side query 15 1 is presented. It can be clearly seen that the pronoun "it" maps to "olympiad".</figDesc><graphic coords="2,53.80,464.02,252.20,169.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,317.66,96.26,240.54,146.03"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Precision at di erent levels for the various runs.</figDesc><table coords="3,449.39,96.26,84.61,7.94"><row><cell>3890 0.3460 0.1746</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,317.66,246.68,242.15,99.62"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Recall at di erent levels for the various runs.</figDesc><table coords="3,325.97,292.91,224.21,53.39"><row><cell>Rund Id</cell><cell cols="4">ND@5 ND@10 ND@20 ND@100</cell></row><row><cell>coref cshi</cell><cell>0.2618</cell><cell>0.2695</cell><cell>0.2684</cell><cell>0.3034</cell></row><row><cell cols="2">coref cshi qe 0.2816</cell><cell>0.2862</cell><cell>0.2841</cell><cell>0.3092</cell></row><row><cell>ensemble</cell><cell>0.3010</cell><cell>0.3079</cell><cell>0.3162</cell><cell>0.3583</cell></row><row><cell>manual indri</cell><cell>0.3691</cell><cell>0.3784</cell><cell>0.3749</cell><cell>0.4237</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,317.66,350.69,240.54,56.50"><head>Table 4 :</head><label>4</label><figDesc>Comparison of NDCG at di erent cut points for the various runs.systems are then merged. e scores of the individual systems are normalized before merging.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,320.88,702.76,99.01,6.18"><p>h ps://github.com/jessevig/bertviz</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,334.39,653.60,224.64,6.18;3,334.39,661.57,224.89,6.18;3,334.39,669.54,108.32,6.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="3,545.56,653.60,13.47,6.18;3,334.39,661.57,221.77,6.18">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
