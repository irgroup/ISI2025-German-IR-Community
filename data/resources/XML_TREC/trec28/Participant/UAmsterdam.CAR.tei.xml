<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.71,164.85,307.87,15.12;1,191.31,186.77,228.62,15.12">University of Amsterdam at the TREC 2019 Complex Answer Retrieval Track</title>
				<funder ref="#_gpS5fBh">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.77,219.25,115.57,10.48"><forename type="first">Mahsa</forename><forename type="middle">S</forename><surname>Shahshahani</surname></persName>
							<email>m.shahshahani@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.20,219.25,63.95,10.48"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.02,219.25,74.46,10.48"><forename type="first">Maarten</forename><surname>Marx</surname></persName>
							<email>maartenmarx@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.71,164.85,307.87,15.12;1,191.31,186.77,228.62,15.12">University of Amsterdam at the TREC 2019 Complex Answer Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C60494550E591D8256AA3620CD42C53</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper documents the University of Amsterdam's participation in the TREC 2019 Complex Answer Retrieval Track. This is the first year we actively participate in TREC CAR, attracted by the introduction to the limited "budget" of 20 passages per heading in the outline. We conducted initial exploratory experiments on making each heading contain a unique set of passages within the outline, and even do this hierarchical for each subtree and main title/article level, hence remove any redundancy between passages for different "queries" within the same title. We experimented with top-down and bottom-up filtering approaches. At the time of writing we are still in the process of analyzing the results. Some initial observations are the following. First, the restriction makes the task very challenging, as assigning any passage to the right heading in the outline is highly non-trivial. Qualitative analysis shows that our simple heuristics often make a different decision than the editorial judges on the heading under which a passage relevant to the title's topic is assigned. Second, the fraction of judged and relevant passages per individual query or leave node is very small, making it hard to draw any definite conclusions on our experiments, and also resulting in a too small recall base to evaluate our non-pooled runs in a meaningful way. Third, when aggregating all qrels and runs to the title level, there is reasonable effectiveness of the underlying BM25 rankings, showing that the underlying passage ranking is not unreasonable, and that the hard and interesting problem is in the exact assignment of passages to the "right" headings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We have followed the CAR Track with great interest since it's proposal, and the introduction of the fixed "budget" of 20 passages per leave node was the reason for us to participate actively in TREC 2019 edition of the track.</p><p>TREC CAR offers a potential solution to some of the major challenges we have been working on in a line of projects using political data <ref type="bibr" coords="1,402.39,665.94,9.96,8.74" target="#b4">[5]</ref>. This political data consists of billions of speeches by individual MP's, making it notoriously hard to extract a meaning overview of the different views of MP's, political parties, or government for a general topic <ref type="bibr" coords="2,319.27,151.87,9.96,8.74" target="#b1">[2]</ref>. We have effectively applied passage retrieval and exploratory search approaches, but these still require reading many individual passages or speeches. One of the main distinctions between the Wikipedia setting and our political data, is that our semantic assignments to speaker (and based on the speaker to the party, and status in terms of member of government or opposition, etc) are strict, whereas a single Wikipedia passage at TREC CAR maybe have relevance to multiple headings. The introduction of the top 20 budget in CAR at TREC 2019, is a large step towards bridging this divide -as with the limited budget redundantly repeating the same passage at different places of the outline comes with a considerable cost, and naturally encourages a strict and non-redundant assignment. It is exactly with this question in mind, that we conducted our initial experiments in allocating a high ranked passage to the "right" node of the outline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Complex Answer Retrieval</head><p>For detailed information about the CAR track's experimental setup, we refer to the overview paper (this volume) and to the track homepage <ref type="bibr" coords="2,402.14,362.05,9.96,8.74" target="#b5">[6]</ref>. We provide a high-level summary to makes this paper self-contained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task definition</head><p>Traditional retrieval systems could successfully satisfy current simple and entitycentric information needs. TREC-CAR track is designed to turn researchers' attention to more complex information needs which require longer and more structured answers which may need using many pieces of information on separate documents.</p><p>In the first two years of running this track (2017 and 2018) the task was focused on retrieving and ranking passages according to a complex query with different facets. In the third year (2019) the main task has been changed to order retrieved passages in order to create a fully structured document in response to a multi-facet query.</p><p>Although the task has been changed in this year, the evaluation metrics seemed to remain the same as last year means they cannot evaluate the new requirements of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paragraph corpus</head><p>The paragraph corpus which is considered as the passagepool consists of 20 million paragraphs obtained from Wikipedia pages from a snapshot of 2016.</p><p>Outlines In this year outlines are extracted from TQA dataset. TQA queries seem to be slightly easier than hierarchical Wikipedia headlines because they contain fewer sections and the depth of queries is limited to 2 or 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TREC CAR Experiments</head><p>The main task for this year's competition is to select top K (K = 20) passages for each question that can make a good answer to the question. To this end we break the goal to two steps:</p><p>• First, we should rank passages in response to each question (essentially passage retrieval taking each query independently).</p><p>• Second, we should select the best passages regarding other considerations such as diversity rather than just relevancy (essentially attempting to assign each passage to the "right" part of the outline).</p><p>In order to maximize diversity, we are most interested in an extreme version of this approach where we have no redundancy within an outline, and uniquely assign each passage to the 'best' node of the outline.</p><p>As it turned out, also the editorial judges have done the same, and have not assigned the same paragraph to multiple headings of the same outline. This makes our own experiments center on the question on to what degree our simple heuristics mimic the choice for the assignment of assessors -a very challenging task indeed!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Passage Ranking Approach</head><p>This part of the solution is like the main TREC CAR task in Y1 and Y2, essentially asking to rank passages for each heading in the outline independently.</p><p>We tested both ranking with BM25 and re-ranking with BERT (after retrieving top passages using BM25) models on Y2 data, to get an insight of which one can be more useful. Although using BERT had shown promising results on Y1 <ref type="bibr" coords="3,149.38,510.53,9.96,8.74" target="#b3">[4]</ref>, it did not result in better results in comparison with BM25 on Y2 data. This is aligned with what results from TREC-CAR 2018 had showed. According to Dietz et al. <ref type="bibr" coords="3,217.25,534.44,10.52,8.74" target="#b0">[1]</ref> neural network models did not work as well as learning to rank models on Y2. Our training experiments reranking passages based on using pre-trained BERT, shown in Table <ref type="table" coords="3,316.83,558.35,3.87,8.74" target="#tab_0">1</ref>, are confirming these observations. Qualitative analysis shows both the clear value of BERT to uncover meaningful passages not literally matching the query, but these often are unjudged and appear lower in the rankings. Qualitative analysis also shows a clear loss of precision against traditional text and word-based expansion approaches, in line with the observed higher scores of those approaches in Y2. As Y3 is very much about high precision, given the limited budget of 20 results, none of our official submissions was based on the BERT rankings.</p><p>BM25 is the traditional unsupervised ranking model which scores the relevancy of documents (here passages) in regards to queries based on the frequency of common terms between the query and passage. We used the Lucene framework <ref type="foot" coords="4,154.83,239.10,3.97,6.12" target="#foot_0">1</ref> to run the BM25 model for passage ranking, in a set-up following the infrastructure already provided by the track organizers. Rather than use our own index and tuned BM25 runs, we rerun our experiments based on the rankings provided by the organizers, in order to start from the same benchmark BM25 runs as other teams and allow for clearer comparative evaluation between teams and submissions. Several pseudo relevance feedback models have been introduced by IR researchers to expand a query with words that occur in top-retrieved documents in response to a query (considering high ranking as an approximation of relevance). In Nanni et al. <ref type="bibr" coords="4,240.44,348.28,10.52,8.74" target="#b2">[3]</ref> the usage of feedback models has been studied for TREC CAR. We used RM3 feedback model for our submission. RM3 is based on interpolation between the language model created by the query itself and language model created by the expanded query. We use a Dirichlet smoothed language model for the feedback run. Each query is expanded with top 10 terms extracted from the top 10 feedback paragraphs.</p><p>Again, in order to allow for clearer comparative evaluation, we reran our experiments and based our submissions on the rankings provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Ordering Approach</head><p>The main task for this year's competition is not about passage ranking, but it is about ordering top-ranked passages in response to the multi-facet query in a way that all selected passages have:</p><p>• Highest relevance of all passages.</p><p>• Balanced coverage of all query facets as defined through headings in the outline</p><p>• Maximizing topical coherence, minimizing topic switches, i.e., first all passages about one topic, then all passages of the next topic while avoiding to interleave multiple topics.</p><p>By way of example, assume we have two queries such as "radio waves/television" and "radio waves/AM and FM radio." The general topic in these examples is "radio waves" and the facets are "television" and "AM and FM radio." Here we want to rank and order passages from Paragraph Corpus in response to both queries in a way that the response include both general information about the general topic ("radio waves") and more detailed information about the facet ("television").</p><p>We applied two methods to select the passages among top-ranked documents:</p><p>Top-Down The intuition behind this approach is "safety first" -earlier years showed that finding a set of relevant passages for the title as a whole is quite effective. So let's first lock in the "best" ranked 20 passages for the top level, and then move step by step through filtering out any passage that was already select before at a higher or earlier node of the outline. In terms of our example, we would do the most general topic of "radio waves" first (including TV, radio and other facets combined), and only then process each facet in turn (here, "television" and "AM and FM radio") and leave out any passage that already was selected before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-Up</head><p>The intuition behind this approach is assign passage to the most specific node of the outline, hence only general passages covering the whole topic should be assigned to the top level, and any passage exclusively about a particular facet should be assign to that facet. There are many reasons why this this way is preferred and it is used as a fundamental principle of information organization in library and information science (known as "Cutter's rule") for far more than a century. In terms of our example, we would start at the leave nodes, and select the highest ranked passages for a particular facet, TV passages assigned to "television" and radio passages to "AM and FM radio", and step-by-step work our way up the tree, excluding any passage that already was selected at an earlier, more specific level of the outline.</p><p>Although more risky, if successful the bottom-up approach is intuitively the preferred approach, and all our official submissions were based on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section presents an analysis of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relevance Judgments</head><p>Table <ref type="table" coords="5,161.65,603.59,4.98,8.74" target="#tab_1">2</ref> shows the number of (judged) topics and relevant passages, and their distribution over relevance levels. With 2,790 relevant passages for 303 headings, the recall base is small with less than 10 relevant passages per heading. Looking deeper into the qrels, Table <ref type="table" coords="5,256.67,639.45,4.98,8.74" target="#tab_2">3</ref>  recall base is small: for some topics there is only 1 relevant passage, and the distribution is skewed to the lower end with a median of 7 per query. Looking at our non-official runs, which did not contribute to the pooling, we see very few judged and relevant results. With less than 5% of the top 20 results of a non-official run in the qrels, it is not meaningful to include results for the non-official runs at the time of writing. Note that the qrels do not contain information on judged but non-relevant passages, which would allow the use of specific measures (such as bpref or InfAP) dealing with incompleteness, although evaluating runs with so few judgments can only be done with extreme caution when interpreting the results.</p><p>Table <ref type="table" coords="6,177.03,417.16,4.98,8.74" target="#tab_2">3</ref> also provides the same distribution after mapping all passages to the title level, lumping together all facets into one bag of passages. We see that there are judgments for 50 titles, hence there are on average about 6 facets judged per title or outline as a whole combined, and recall base per title is less small with a minimum of 12 relevant passages, and around 50 relevant passages per title on average.</p><p>In the rest of this paper, we will restrict our analysis to our official submissions only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TREC 2019 CAR Submissions</head><p>Although, the CAR track generously allowed up to 10 submission, the "budget" was restricted to a single one with high judging precedence (presumed to be assessed), and another two with medium precedence (possibly, but not necessarily, judged). As we did not replicate all our 2018 variations on the 2019 data, we refrained from submitting any of the remaining low judging precedence runs (i.e., additional submissions not meant to be assessed).</p><p>We submitted the following three submissions:</p><p>BM25+RM3 The first submission, labeled UvABM25RM3 and medium judging precedence, is based on a word-based BM25 ranking, using the section-path queries and RM3 blind feedback. This run was essentially derived directly by restricting the provided passage rankings to the restricted budget, just based on the retrieval status value and ignored potential overlap or dependencies between the different heading within the same outline. While our aim was to remove all redundancies and assign passages to a unique heading within the same title, this run serves as a baseline and proper comparative evaluation would profit from this baseline also contribution to the pool of passages to be judged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BottomUp-Score</head><p>The second submission, labeled UvABottomUp2 and medium judging precedence, is based on BM25+RM3 and applying the Bottom Up approach to select paragraph to assign to each heading, allowing no redundancy. This is a straightforward heuristic where we process the source ranking, and all remaining paragraph at each heading remain ordered according to their original RSV value in the source run.</p><p>BottomUp-SumScore The third submission, labeled UvABottomUpChangeOrder and high judging precedence, is again based on the BM25+RM3 and applying again the Bottom Up approach to select to which node a paragraph should be assigned, allowing for no redundancy, but uses possible other occurrences of this passage in other headings to aggregate their scores, and hence rerank the passages per heading according classic RSV combination or score fusion approaches. This will change the ordering relative to the BottomUp-Score submission above. <ref type="foot" coords="7,309.73,504.24,3.97,6.12" target="#foot_1">2</ref>The preliminary results are shown in Table <ref type="table" coords="7,338.03,524.81,3.87,8.74" target="#tab_3">4</ref>. As we retrieve only 20 results per topic or heading, we focus on early precision measures. A few observations present themselves. First, all scores are relatively low, clearly demonstrating how hard the task of accurate assigning passages to the right heading of the outline is, relatively to more standard document and passage retrieval tasks. Second, even the source run, BM25+RM3, scores quite low indicating that many paragraphs in that run have not been regarded as relevant by the respective assessors. Not surprisingly, the two runs de-duplicating the submissions by uniquely assigning paragraphs to unique parts of the outline score even lower 1 2 5 10 20 BM25+RM3 10.23% 10.07% 8.84% 7.79% 6.49% BottomUp-Score 6.60% 6.77% 6.07% 5.74% 4.42% BottomUp-SumScore 6.93% 6.27% 5.74% 5.97% 4.42% than the baseline run. This is indeed a hard and risky additional constraint, and qualitative analysis comparing difference in the runs presents cases were did retrieve the 'relevant' passage, but our heuristic approach assigned it to a different heading than the ground truth assessment did. Third, although scoring low, we can do a comparative analysis of the two variant approaches. There is no clear winner, with precision at 5 favoring the original score based approach, and the others slightly favoring the sum of scores approach.</p><p>We look a little deeper, and looked at the distribution of judged and judged relevant pages. The fraction of judged relevant over ranks are shown in Table <ref type="table" coords="8,472.49,453.02,4.98,8.74" target="#tab_4">5</ref> which reveal that only a small fraction of the results in our runs have a relevance label in the qrels. Recall from before that the qrels do not contain judged but not relevant labels, hence we cannot investigate directly whether this is due to these results not being assessed (hence the runs not contributing to the pooling), or whether they were all assessed up to a given rank but deemed non-relevant by the judges. Qualitative inspection of some cases didn't reveal an immediate explanation, none of the retrieved passages looked clearly off-topic. We will investigate this in further analysis.</p><p>Although we cannot exclude various system or submission format conversion errors, the runs were partly based on the independent implementation from the organizers, and those rankings looks reasonable in superficial inspection. A plausible explanation could be in the small recall base, low pooling depth, and great diversity over different submissions. To quantify this, we create synthetic versions of the qrels and our submissions, in which we assign all retrieved passages to the title as a single topic (hence retrieving more than the top 20 results). We also aggregate all the relevant pages for a particular subheading to the main title in the qrels (as previously shown in Table <ref type="table" coords="8,341.47,656.26,3.87,8.74" target="#tab_2">3</ref>). In this way we create fewer topics with a larger recall base that can indicate the underlying ranking quality of the used approach. These additional results for analytical purposes are shown in Table <ref type="table" coords="9,203.83,151.87,3.87,8.74" target="#tab_5">6</ref>. We make two observations. First, we see that the precision scores are in the range of what's expected for a passage retrieval or document retrieval task, and not unusually low. This is suggesting that the rankings we start out with are of reasonable quality. Second, we see again a drop in effectiveness for the additional non-redundancy processing, proving that this is the hard task even when we ignore the case where we assign a "relevant" passages to the wrong heading. Note that in this setting, we also "remove" potential assessor disagreement on to what heading to assign a given "relevant" passage to.</p><p>Our analysis present clear evidence that one of the next steps in CAR, where we enforce systems to be selective, or even require non-redundancy over all retrieved passages within the same outline, is indeed challenging. As this is a necessary step to go from passage retrieval to generating comprehensive complex answers, this is perhaps only making this task more interesting and more important to look at.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper documents our first participation in the TREC 2019 CAR Track.</p><p>Our experimental results lead to the following observations. First, the restriction to 20 passages makes the task very challenging, as assigning any passage to the right heading is highly non-trivial. Qualitative analysis shows that our simple heuristics often make a different decision than the editorial judges on the heading to which a passage relevant to the title's topic is assigned. Second, the fraction of judged and relevant passages per individual query or leave node is very small, making it hard to draw any definite conclusions on our experiments, and also resulting in a too small recall base to evaluate our non-pooled runs in a meaningful way. Third, when aggregating all qrels and runs to the title level, there is reasonable effectiveness of the underlying BM25 rankings, showing that the underlying passage ranking is not unreasonable, and that the hard and interesting problem is in the exact assignment of passages to the "right" headings.</p><p>We hope and expect that the valuable bench-marking data created at TREC CAR will be of great value to motivate, and greatly facilitate, further research into this important and hard problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,203.08,129.95,205.09,74.04"><head>Table 1 :</head><label>1</label><figDesc>Training results on TREC-CAR 2018.</figDesc><table coords="4,239.39,129.95,132.46,49.60"><row><cell>Run</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>BM25</cell><cell cols="2">0.2895 0.6223</cell></row><row><cell cols="3">BM25+RM3 0.3049 0.6549</cell></row><row><cell>BERT</cell><cell cols="2">0.1870 0.5118</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,133.77,639.45,343.71,20.69"><head>Table 2 :</head><label>2</label><figDesc>provides the distribution of relevant passages (at any level of relevance) over the queries or headings. We see that, indeed the Various statistics (#) on the Y3 topics and judgments.</figDesc><table coords="6,151.45,129.95,308.35,106.53"><row><cell cols="8">Topics Judged topics Rel. passages Rel-0 Rel-1 Rel-2 Rel-3</cell></row><row><cell>722</cell><cell>303</cell><cell></cell><cell>2,790</cell><cell>-</cell><cell>693</cell><cell>1,577</cell><cell>520</cell></row><row><cell>Level</cell><cell>#</cell><cell cols="5">Min Max Mean Median St.dev.</cell></row><row><cell cols="2">Heading 303</cell><cell>1</cell><cell>79</cell><cell>9.2</cell><cell>7</cell><cell>9.4</cell></row><row><cell>Title</cell><cell>50</cell><cell>12</cell><cell>107</cell><cell>50.7</cell><cell>47</cell><cell>22.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,133.77,252.18,343.71,20.69"><head>Table 3 :</head><label>3</label><figDesc>Various statistics on the Y3 topics and judgments per heading and aggregated per title.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,133.77,129.95,346.05,97.95"><head>Table 4 :</head><label>4</label><figDesc>Preliminary results on the TREC 2019 CAR Track's Passage Ordering Task.</figDesc><table coords="7,133.77,129.95,346.05,61.56"><row><cell>Submission</cell><cell>MRR</cell><cell>Precision</cell><cell></cell><cell></cell><cell>NDCG</cell><cell>MAP</cell></row><row><cell></cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>BM25+RM3</cell><cell cols="6">0.1877 0.0884 0.0779 0.0649 0.0891 0.0891 0.0891 0.0442</cell></row><row><cell>BottomUp-Score</cell><cell cols="6">0.1126 0.0607 0.0574 0.0442 0.0561 0.0561 0.0561 0.0252</cell></row><row><cell cols="7">BottomUp-SumScore 0.1142 0.0574 0.0597 0.0442 0.0592 0.0592 0.0592 0.0260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,133.77,195.25,346.05,101.01"><head>Table 5 :</head><label>5</label><figDesc>Percentage of results with a known relevance label (only judged and relevant) per rank.</figDesc><table coords="8,133.77,234.70,346.05,61.56"><row><cell>Submission</cell><cell>MRR</cell><cell>Precision</cell><cell></cell><cell></cell><cell>NDCG</cell><cell>MAP</cell></row><row><cell></cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>BM25+RM3</cell><cell cols="6">0.7064 0.4873 0.4291 0.3573 0.3830 0.3830 0.3830 0.1014</cell></row><row><cell>BottomUp-Score</cell><cell cols="6">0.4798 0.3345 0.3164 0.2436 0.2659 0.2659 0.2659 0.0610</cell></row><row><cell cols="7">BottomUp-SumScore 0.4878 0.3164 0.3291 0.2436 0.2706 0.2706 0.2706 0.0592</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,133.77,311.95,343.71,20.69"><head>Table 6 :</head><label>6</label><figDesc>Preliminary results on aggregated topics with a bag of relevant passages per title.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,149.01,665.09,94.62,6.99"><p>https://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,149.01,638.91,328.47,6.99;7,133.77,648.37,343.71,6.99;7,133.77,657.84,343.71,6.99;7,133.77,667.30,309.74,6.99"><p>Although this was our high judging precedence run, and it was validated by the eligibility script before submission, there remained an accidental duplicate passage id in the run, we report here the evaluation after removing duplicates. This may also have prevented this run from being parsed properly and contributing to the pool of passages to be assessed.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>A special word of thanks to the <rs type="institution">TREC CAR Track organizers</rs> for their amazing support during the track! This research was supported in part by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grant # <rs type="grantNumber">CISC.CC.016</rs>, <rs type="projectName">AC-CESS</rs> project). Views expressed in this paper are not necessarily shared or endorsed by those funding the research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gpS5fBh">
					<idno type="grant-number">CISC.CC.016</idno>
					<orgName type="project" subtype="full">AC-CESS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,149.27,149.78,328.21,8.74;10,149.27,161.74,153.44,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,375.46,149.78,102.01,8.74;10,149.27,161.74,75.01,8.74">TREC complex answer retrieval overview</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gamari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,245.64,161.74,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,181.66,328.22,8.74;10,149.27,193.62,328.22,8.74;10,149.27,205.57,328.21,8.74;10,149.27,217.53,328.21,8.74;10,149.27,229.48,240.85,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,317.99,181.66,159.49,8.74;10,149.27,193.62,103.11,8.74">Who said what to whom? capturing the structure of debates</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,248.86,205.57,228.62,8.74;10,149.27,217.53,324.07,8.74">Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="831" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,249.41,328.22,8.74;10,149.27,261.36,328.21,8.74;10,149.27,273.32,273.18,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,373.43,249.41,104.06,8.74;10,149.27,261.36,67.15,8.74">Benchmark for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.72,261.36,241.75,8.74;10,149.27,273.32,145.58,8.74">Proceedings of the ACM SIGIR international conference on theory of information retrieval</title>
		<meeting>the ACM SIGIR international conference on theory of information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,293.24,328.21,8.74;10,149.27,305.20,105.05,8.74" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="10,267.89,293.24,133.95,8.74">Passage re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,149.27,325.12,328.21,9.02;10,149.27,337.08,159.54,9.02" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Political</forename><surname>Mashup</surname></persName>
		</author>
		<ptr target="https://www.politicalmashup.nl/" />
		<title level="m" coord="10,228.60,325.12,212.49,8.74">Who said what? to whom? and why? and when?</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,357.00,328.21,9.02;10,149.27,368.96,22.69,8.74" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><surname>Trec Car</surname></persName>
		</author>
		<ptr target="http://trec-car.cs.unh.edu/" />
		<title level="m" coord="10,212.49,357.00,110.58,8.74">Complex answer retrieval</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
