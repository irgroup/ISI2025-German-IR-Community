<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,79.26,84.23,13.15,15.44;1,92.41,90.04,5.79,11.03;1,98.70,84.23,434.03,15.44;1,161.53,104.15,289.44,15.44">H 2 oloo at TREC 2019: Combining Sentence and Document Evidence in the Deep Learning Track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.98,134.27,132.60,10.59"><forename type="first">Zeynep</forename><forename type="middle">Akkalyoncu</forename><surname>Yilmaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.69,134.27,70.95,10.59"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.86,134.27,51.16,10.59"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.40,150.37,73.47,8.83"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,79.26,84.23,13.15,15.44;1,92.41,90.04,5.79,11.03;1,98.70,84.23,434.03,15.44;1,161.53,104.15,289.44,15.44">H 2 oloo at TREC 2019: Combining Sentence and Document Evidence in the Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">84F1B613B3C38F85D960230F3AEE41E6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The h 2 oloo team at the University of Waterloo participated in the TREC 2019 Deep Learning Track for both the document and passage (full) ranking tasks. Our primary goal was to validate BERT-based retrieval techniques that we have been working on <ref type="bibr" coords="1,233.90,229.48,9.23,7.94" target="#b1">[2,</ref><ref type="bibr" coords="1,244.74,229.48,11.47,7.94" target="#b11">12]</ref> in the context of a two-stage retrieval architecture <ref type="bibr" coords="1,198.91,240.44,10.43,7.94" target="#b2">[3]</ref> comprising a candidate document generation stage (driven by "bag of words" techniques) followed by a reranking stage built around BERT <ref type="bibr" coords="1,236.65,262.36,9.45,7.94" target="#b3">[4]</ref>, the popular deep transformer model that takes advantage of massive pretraining using language modeling tasks.</p><p>All of our experiments used the open-source Anserini IR toolkit <ref type="bibr" coords="1,287.72,295.24,9.23,7.94" target="#b8">[9,</ref><ref type="bibr" coords="1,53.59,306.19,10.21,7.94" target="#b9">10]</ref>, which is based on the popular open-source Lucene search library, for initial retrieval that feeds the BERT-based rerankers. We used a pre-0.6.0 release of Anserini based on Lucene version 8.0. For passage reranking and document reranking of the candidate lists retrieved with Anserini, we used different codebases, which will be described in detailed below.</p><p>In addition to our own submissions, at the invitation of the track coordinators, we also prepared a number of baseline runs (to be more specific, runs that did not take advantage of deep learning) to enrich the judgment pool. These runs used a variety of query expansion techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PASSAGE RANKING 2.1 Baselines</head><p>Our first four runs used BM25 with Anserini's default parameters, alone and in conjunction with three different query expansion approaches based on pseudo-relevance feedback:</p><p>‚Ä¢ baseline/bm25base_p: BM25 baseline using Anserini's default parameters (ùëò 1 = 0.9, ùëè = 0.4). ‚Ä¢ baseline/bm25base_ax_p: BM25 baseline using Anserini's default parameters with axiomatic semantic term matching <ref type="bibr" coords="1,272.58,534.79,13.36,7.94" target="#b10">[11]</ref>. ‚Ä¢ baseline/bm25base_prf_p: BM25 baseline using Anserini's default parameters with probabilistic relevance feedback <ref type="bibr" coords="1,262.71,556.70,13.36,7.94" target="#b12">[13]</ref>. ‚Ä¢ baseline/bm25base_rm3_p: BM25 baseline using Anserini's default parameters with RM3 query expansion <ref type="bibr" coords="1,225.84,578.62,9.39,7.94" target="#b4">[5]</ref>. For all query expansion approaches, we used Anserini's default parameters; these exact values are specified as constants in the class io.anserini.search.SearchArgs.</p><p>The next set of four runs were based on BM25 using parameters tuned with the MS MARCO passage dataset. Since we did not have sufficient time or resources to tune on the entire dev set, we performed parameter tuning on sampled subsets. Specifically, tuning was based on five different sets of 10k samples (extracted using the Linux shuf command). We tuned on each individual set (grid search on ùëò 1 and ùëè in tenth increments) and then averaged the optimal parameter values across all five sets, which has the effect of regularization. We optimized recall@1000 since Anserini output serves as input to later stage rerankers, and we wanted to maximize the number of relevant documents the rerankers have to work with. The tuned parameters using this method are ùëò 1 = 0.82, ùëè = 0.68. This led to the following four runs:</p><p>‚Ä¢ baseline/bm25tuned_p: BM25 baseline using tuned parameters (ùëò 1 = 0.82, ùëè = 0.68). ‚Ä¢ baseline/bm25tuned_ax_p: BM25 baseline using tuned parameters with axiomatic semantic term matching. ‚Ä¢ baseline/bm25tuned_prf_p: BM25 baseline using tuned parameters with probabilistic relevance feedback. ‚Ä¢ baseline/bm25tuned_rm3_p: BM25 baseline using tuned parameters with RM3 query expansion.</p><p>For all query expansion approaches, we also used Anserini's default parameters. Note this likely led to sub-optimal effectiveness, since it is likely that BM25 parameters need to be tuned in conjunction with query expansion parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BERT Runs</head><p>The starting point of our deep learning work is the BERT-based passage reranking model of Nogueira and Cho <ref type="bibr" coords="1,483.06,421.20,9.27,7.94" target="#b6">[7]</ref>, which has served as a competitive baseline for the MS MARCO passage ranking leaderboard since early 2019. In this approach, BERT is used as a relevance classifier trained on query-passage pairs. To this, we added doc2query <ref type="bibr" coords="1,383.24,465.04,9.45,7.94" target="#b7">[8]</ref>, a document expansion technique that takes advantage of a neural sequence-to-sequence model (also trained on the query-passage pairs). These two techniques were combined into the following submitted runs:</p><p>‚Ä¢ h2oloo/p_bert: We retrieved the top 1000 passages for each query with tuned BM25 and then reranked the passages with the BERT-based relevance classifier <ref type="bibr" coords="1,444.06,537.08,9.39,7.94" target="#b6">[7]</ref>. ‚Ä¢ h2oloo/p_exp_bert: Prior to indexing, we expanded each passage using doc2query <ref type="bibr" coords="1,410.03,559.00,9.52,7.94" target="#b7">[8]</ref>. To generate diverse expansions, we used two different models: the first is a from-scratch trained model on MS MARCO passage data and the second is the TREC CAR model made available by Nogueira et al., which is exactly the same model used in their paper. We used top 10 sampling for decoding (as recommended by the authors) and simply took the union of the expansions generated by both models. The expansions were appended to the original passages to form an expanded collection. This expanded collection was then indexed with Anserini, which we queried using tuned BM25, followed by reranking with the BERT-based classifier <ref type="bibr" coords="1,478.08,668.59,9.39,7.94" target="#b6">[7]</ref>. the results of which are then reranked with the BERT-based relevance classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>Results of our passage ranking runs are shown in Table <ref type="table" coords="2,252.46,336.96,3.01,7.94" target="#tab_0">1</ref>. Note that NIST judgments were provided on a four-point scale: (3) perfectly relevant, (2) highly relevant, (1) related, and (0) irrelevant. For the purposes of computing nDCG, all grades were used, but for computing AP, grade (1) related judgments were not considered relevant. 1 For reference, we show results from the best submitted run in terms of nDCG@10 (idst_bert_p1). On a per-team basis in terms of nDCG@10, we ranked number two among all participants, based on our best run (p_exp_rm3_bert). However, in terms of AP, p_exp_rm3_bert was the best submitted run in the evaluation. Interestingly, we note that the effectiveness of our tuned runs is not much different (and in some cases lower) than the corresponding runs with default BM25 parameters. The reason appears to be that we performed parameter tuning using the sparse MS MARCO judgments, whereas the official evaluation used the dense judgments from NIST assessors. This suggests that parameters may not generalize across different "styles" of relevance judgments.</p><p>Our main runs nicely show an increase in effectiveness with growing sophistication of the applied technique. Starting from BERT-based reranking, adding document expansion improves AP (but not nDCG@10), and further introducing query expansion yields even more improvements (in both metrics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DOCUMENT RANKING 3.1 Baselines</head><p>Our first four runs used BM25 with default parameters (ùëò 1 = 0.9, ùëè = 0.4), alone and in conjunction with three different query expansion approaches based on pseudo-relevance feedback (all using Anserini default parameters):</p><p>‚Ä¢ baseline/bm25base: BM25 baseline using default parameters (ùëò 1 = 0.9, ùëè = 0.4). 1 In trec_eval, this is specified using the -l 2 option.</p><p>‚Ä¢ baseline/bm25base_ax: BM25 baseline using default parameters with axiomatic semantic term matching. ‚Ä¢ baseline/bm25base_prf: BM25 baseline using default parameters with probabilistic relevance feedback. ‚Ä¢ baseline/bm25base_rm3: BM25 baseline using default parameters with RM3 query expansion.</p><p>These four runs mirror the four baselines in the passage retrieval condition described in the previous section.</p><p>In addition, we submitted four runs using BM25 parameters tuned with the MS MARCO document data. Similar to the passage condition, tuning was performed on five different sets of 10k samples from the training queries (grid search on parameters in tenth increments). The final setting was the average of the optimal parameters across all five sets. We optimized for average precision.</p><p>‚Ä¢ baseline/bm25tuned: BM25 baseline using tuned parameters (ùëò 1 = 3.44, ùëè = 0.87). ‚Ä¢ baseline/bm25tuned_ax: BM25 baseline using tuned parameters with axiomatic semantic term matching. ‚Ä¢ baseline/bm25tuned_prf: BM25 baseline using tuned parameters with probabilistic relevance feedback. ‚Ä¢ baseline/bm25tuned_rm3: BM25 baseline using tuned parameters with RM3 query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BERT Runs</head><p>Our work on BERT for ranking documents <ref type="bibr" coords="2,478.64,368.02,9.41,7.94" target="#b1">[2,</ref><ref type="bibr" coords="2,490.28,368.02,11.58,7.94" target="#b11">12]</ref> was motivated by the inability of the relevance classifier of Nogueira and Cho <ref type="bibr" coords="2,547.70,378.98,10.50,7.94" target="#b6">[7]</ref> to handle long spans of text, as BERT has a 512 token limit. We have discovered a surprisingly simple solution <ref type="bibr" coords="2,487.61,400.90,9.30,7.94" target="#b1">[2]</ref>. Given an initial ranked list of documents, we segment each into sentences, and then apply inference (with the relevance classifier) over each sentence separately, after which sentence-level scores are aggregated to yield a final score for ranking documents. Specifically, we combine the top ùëõ sentence scores with the original document score as follows:</p><formula xml:id="formula_0" coords="2,375.80,487.89,182.40,24.75">ùëÜ ùëì = ùõº ‚Ä¢ ùëÜ doc + (1 -ùõº) ‚Ä¢ ùëõ ùëñ=1 ùë§ ùëñ ‚Ä¢ ùëÜ ùëñ (1)</formula><p>where ùëÜ doc is the original document score and ùëÜ ùëñ is the ùëñ-th top scoring sentence according to BERT. In other words, the relevance score of a document comes from the combination of a documentlevel term-matching score and evidence contributions from the top sentences in the document as determined by the BERT model. Typically, we find that ùëõ = 3 achieves optimal effectiveness (and in some cases, ùëõ = 1). The parameters ùõº and ùë§ ùëñ 's are learned. The sentence-level relevance classifier can be trained with the MS MARCO passage data, although we have found that BERT is able to transfer models of relevance across domains <ref type="bibr" coords="2,496.12,624.75,9.52,7.94" target="#b1">[2]</ref>. For example, data from the TREC Microblog Tracks <ref type="bibr" coords="2,463.42,635.71,10.68,7.94" target="#b5">[6]</ref> are useful for ranking newswire documents (a completely different domain). Since those data comprise (query, tweet, relevance judgment) triples and tweets are quite short, they can be used to directly fine-tune BERT as well. In more detail, we began with a BERT model that has already been fine-tuned on the MS MARCO passage data, and then further fine-tune with the microblog data. This technique is implemented in our new open-source retrieval toolkit called Birch <ref type="bibr" coords="3,124.42,295.61,9.28,7.94" target="#b0">[1]</ref>. With Birch and Anserini, we submitted the following runs: ‚Ä¢ h2oloo/bm25_marcomb: We first retrieved the top 1000 documents using tuned BM25 with RM3 query expansion. These are then reranked using the approach described above, with the relevance classifier fine-tuned on both MS MARCO passage data and microblog data. Weights for the top three sentences (ùëõ = 3, selected based on our previous experiences <ref type="bibr" coords="3,219.13,374.66,9.78,7.94" target="#b1">[2]</ref>) are learned from the dev set of the MS MARCO document data. ‚Ä¢ h2oloo/bm25exp_marcomb: Prior to indexing, we expanded each document using doc2query <ref type="bibr" coords="3,165.89,407.54,9.52,7.94" target="#b7">[8]</ref>. To generate diverse expansion terms, used exactly the same approach as in the passage task (i.e., union of two different models). The expansion of a document was defined as the union of the expansion of each passage in the document. We retrieved and reranked documents from the index with the expanded documents in the same way as h2oloo/bm25_marcomb. ‚Ä¢ h2oloo/bm25exp_marco: This condition differs from the previous condition only in the relevance classifier used to rerank the documents: instead of a model fine-tuned on both MS MARCO and microblog data, we only fine-tuned on MS MARCO data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Document ranking results are shown in Table <ref type="table" coords="3,228.57,552.44,3.13,7.94" target="#tab_1">2</ref>. Note that NIST judgments were provided on a four-point scale: (3) perfectly relevant, (2) highly relevant, (1) relevant, and (0) irrelevant. The scale was defined in a slightly different way from the passage ranking task, and thus the metrics were computed differently as well. For the purposes of computing nDCG, all grades were used, and for computing AP, grade (1) relevant judgments were also considered relevant (unlike in the passage case). However, for fair comparison between the "full ranking" and "reranking" conditions, all submitted runs were truncated to 100 hits per query. 2 For reference, we show results from the best submitted run in terms of nDCG@10 (idst_bert_v3). On a per-team basis in terms of nDCG@10, we ranked number two among all participants. There appears to be a 2 In trec_eval, this is specified using the -M 100 option.</p><p>big gap between runs from the IDST group and our group. However, in terms of AP, bm25_marcomb was the best submitted run in the evaluation. Since nDCG@10 is an early precision metric, this rank swap is perhaps not surprising.</p><p>Looking at the baselines: as with the passage retrieval runs, tuned BM25 performed worse than BM25 with default parameters. However, with query expansion, tuning generally improved both metrics, although AP for RM3 appears to be an outlier.</p><p>Our BERT-based runs reveal two interesting findings: First, runs bm25_marcomb and bm25exp_marcomb form a contrastive pair, and shows that document expansion (at least with our implementation) does not appear to improve results (in fact, AP degrades slightly). Second, runs bm25exp_marco and bm25exp_marcomb form another contrastive pair, and shows that (additionally) fine-tuning the BERT relevance classifier on microblog data improves effectiveness. This confirms our previous finding <ref type="bibr" coords="3,431.17,252.15,10.66,7.94" target="#b1">[2]</ref> that BERT is able to effectively perform cross-domain relevance transfer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,317.96,679.09,241.77,30.32"><head>Table 1 :</head><label>1</label><figDesc>Passage ranking results.</figDesc><table coords="2,77.91,88.72,189.78,156.24"><row><cell>Run</cell><cell cols="2">AP nDCG@10</cell></row><row><cell>baseline/bm25base_p</cell><cell>0.3013</cell><cell>0.5058</cell></row><row><cell>baseline/bm25base_ax_p</cell><cell>0.3745</cell><cell>0.5511</cell></row><row><cell>baseline/bm25base_prf_p</cell><cell>0.3561</cell><cell>0.5372</cell></row><row><cell>baseline/bm25base_rm3_p</cell><cell>0.3390</cell><cell>0.5180</cell></row><row><cell>baseline/bm25tuned_p</cell><cell>0.2903</cell><cell>0.4973</cell></row><row><cell>baseline/bm25tuned_ax_p</cell><cell>0.3632</cell><cell>0.5461</cell></row><row><cell cols="2">baseline/bm25tuned_prf_p 0.3684</cell><cell>0.5536</cell></row><row><cell cols="2">baseline/bm25tuned_rm3_p 0.3377</cell><cell>0.5231</cell></row><row><cell>h2oloo/p_bert</cell><cell>0.4677</cell><cell>0.7380</cell></row><row><cell>h2oloo/p_exp_bert</cell><cell>0.4749</cell><cell>0.7336</cell></row><row><cell>h2oloo/p_exp_rm3_bert</cell><cell>0.5049</cell><cell>0.7422</cell></row><row><cell>IDST/idst_bert_p1</cell><cell>0.5030</cell><cell>0.7645</cell></row></table><note coords="1,317.96,679.09,241.77,8.40;1,326.40,690.51,231.80,7.94;1,326.40,701.47,232.79,7.94"><p>‚Ä¢ h2oloo/p_exp_rm3_bert: After expanding and indexing the collection as described for h2oloo/p_exp_bert, we retrieved the top 1000 passages with tuned BM25 and RM3 query expansion,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,82.39,88.72,180.82,176.16"><head>Table 2 :</head><label>2</label><figDesc>Document ranking results.</figDesc><table coords="3,82.39,88.72,180.82,156.24"><row><cell></cell><cell cols="2">AP nDCG@10</cell></row><row><cell>baseline/bm25base</cell><cell>0.2443</cell><cell>0.5190</cell></row><row><cell>baseline/bm25base_ax</cell><cell>0.2452</cell><cell>0.4730</cell></row><row><cell>baseline/bm25base_prf</cell><cell>0.2542</cell><cell>0.5106</cell></row><row><cell>baseline/bm25base_rm3</cell><cell>0.2772</cell><cell>0.5169</cell></row><row><cell>baseline/bm25tuned</cell><cell>0.2318</cell><cell>0.5140</cell></row><row><cell>baseline/bm25tuned_ax</cell><cell>0.2816</cell><cell>0.5245</cell></row><row><cell cols="2">baseline/bm25tuned_prf 0.2759</cell><cell>0.5281</cell></row><row><cell cols="2">baseline/bm25tuned_rm3 0.2700</cell><cell>0.5485</cell></row><row><cell>h2oloo/bm25_marcomb</cell><cell>0.3229</cell><cell>0.6403</cell></row><row><cell>h2oloo/bm25exp_marco</cell><cell>0.3030</cell><cell>0.6399</cell></row><row><cell cols="2">h2oloo/bm25exp_marcomb 0.3190</cell><cell>0.6456</cell></row><row><cell>IDST/idst_bert_v3</cell><cell>0.3137</cell><cell>0.7257</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was supported by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC)</rs> of <rs type="funder">Canada</rs>, with computational resources provided by <rs type="person">Compute Ontario</rs> and <rs type="funder">Compute Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,333.39,353.08,224.81,6.18;3,333.28,361.03,224.92,6.23;3,333.39,369.00,224.81,6.23;3,333.39,376.97,225.51,6.23;3,333.39,384.94,168.96,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,380.39,361.05,135.39,6.18">Applying BERT to Document Retrieval with Birch</title>
		<author>
			<persName coords=""><forename type="first">Akkalyoncu</forename><surname>Zeynep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengjin</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,526.63,361.03,31.57,6.23;3,333.39,369.00,224.81,6.23;3,333.39,376.97,225.51,6.23;3,333.39,384.94,89.53,6.23">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,392.93,225.88,6.18;3,333.39,400.90,224.81,6.18;3,333.39,408.85,225.50,6.23;3,333.39,416.82,224.81,6.23;3,332.97,424.79,139.48,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,333.39,400.90,214.63,6.18">Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Akkalyoncu</forename><surname>Zeynep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,333.39,408.85,225.50,6.23;3,333.39,416.82,224.81,6.23">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP). Hong Kong</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3490" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,432.79,224.81,6.18;3,333.39,440.73,224.81,6.23;3,333.39,448.70,224.81,6.23;3,333.39,456.67,172.51,6.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,425.75,432.79,132.45,6.18;3,333.39,440.76,145.38,6.18">Effectiveness/Efficiency Tradeoffs for Candidate Generation in Multi-Stage Retrieval Architectures</title>
		<author>
			<persName coords=""><forename type="first">Nima</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,492.20,440.73,66.00,6.23;3,333.39,448.70,224.81,6.23;3,333.39,456.67,59.26,6.23">Proceedings of the 36th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 36th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="997" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,464.67,225.63,6.18;3,333.39,472.64,224.81,6.18;3,333.39,480.58,224.81,6.23;3,333.39,488.55,224.81,6.23;3,333.39,496.52,142.95,6.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,541.50,464.67,17.53,6.18;3,333.39,472.64,214.32,6.18">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,333.39,480.58,176.41,6.23">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s" coord="3,533.99,488.55,24.21,6.23;3,333.39,496.52,32.62,6.23">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,504.52,225.89,6.18;3,333.39,512.46,90.45,6.23" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,384.32,504.52,172.10,6.18">The Neural Hype and Comparisons Against Weak Baselines</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,333.39,512.46,35.20,6.23">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,520.46,224.81,6.18;3,333.18,528.40,225.02,6.23;3,333.39,536.37,139.40,6.23" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,513.36,520.46,44.84,6.18;3,333.18,528.43,80.95,6.18">Overview of the TREC-2014 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,426.63,528.40,131.57,6.23;3,333.39,536.37,30.36,6.23">Proceedings of the Twenty-Third Text REtrieval Conference</title>
		<meeting>the Twenty-Third Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,544.37,224.81,6.18;3,333.39,552.31,47.93,6.23" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="3,462.54,544.37,84.76,6.18">Passage Re-ranking with BERT</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,560.31,224.81,6.18;3,333.39,568.25,148.13,6.23" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="3,528.69,560.31,29.51,6.18;3,333.39,568.28,88.06,6.18">Document Expansion by Query Prediction</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,576.25,224.81,6.18;3,333.39,584.19,224.81,6.23;3,333.39,592.16,224.81,6.23;3,333.39,600.13,135.21,6.23" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,470.21,576.25,87.99,6.18;3,333.39,584.22,123.07,6.18">Anserini: Enabling the Use of Lucene for Information Retrieval Research</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,469.73,584.19,88.47,6.23;3,333.39,592.16,224.81,6.23;3,333.39,600.13,24.51,6.23">Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,608.13,224.81,6.18;3,333.39,616.07,225.58,6.23;3,333.15,624.07,29.24,6.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="3,465.07,608.13,93.13,6.18;3,333.39,616.10,66.78,6.18">Anserini: Reproducible Ranking Baselines Using Lucene</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,406.45,616.07,115.97,6.23">Journal of Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,632.04,224.81,6.18;3,333.39,639.98,224.81,6.23;3,333.39,647.95,202.78,6.23" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="3,428.53,632.04,129.67,6.18;3,333.39,640.01,124.59,6.18">Reproducing and Generalizing Semantic Term Matching in Axiomatic Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,469.49,639.98,88.72,6.23;3,333.39,647.95,118.09,6.23">Proceedings of the 41th European Conference on Information Retrieval, Part I</title>
		<meeting>the 41th European Conference on Information Retrieval, Part I<address><addrLine>Cologne, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="369" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,655.95,225.02,6.18;3,333.39,663.89,149.19,6.23" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="3,474.43,655.95,83.99,6.18;3,333.39,663.92,89.22,6.18">Simple Applications of BERT for Ad Hoc Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,671.89,224.81,6.18;3,333.15,679.83,225.05,6.23;3,333.39,687.80,225.58,6.23;3,333.39,695.80,18.33,6.18" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="3,445.48,671.89,112.72,6.18;3,333.15,679.86,84.42,6.18">BM25 Pseudo Relevance Feedback Using Anserini at Waseda University</title>
		<author>
			<persName coords=""><forename type="first">Zhaohao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,429.63,679.83,128.57,6.23;3,333.39,687.80,28.10,6.23;3,408.61,687.80,81.69,6.23">Proceedings of the Open-Source IR Replicability Challenge</title>
		<meeting>the Open-Source IR Replicability Challenge<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>OSIRRC</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="62" to="63" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
