<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,77.21,84.23,458.29,15.44">CAsT 2019: The Conversational Assistance Track Overview</title>
				<funder>
					<orgName type="full">Indri</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.11,112.78,68.19,5.45"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
							<email>jeff.dalton@glasgow.ac.uk</email>
						</author>
						<author>
							<persName coords="1,258.49,112.78,76.68,5.45"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<email>chenyan.xiong@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,366.35,112.78,61.77,5.45"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow 1</orgName>
								<address>
									<addrLine>Microsoft Research 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,77.21,84.23,458.29,15.44">CAsT 2019: The Conversational Assistance Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">384F14A7AA62EB0D4C8E99C446D8EA83</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The importance of conversation and conversational models for complex information seeking tasks is well-established within information retrieval, initially to understand user behavior during interactive search <ref type="bibr" coords="1,121.07,204.61,9.35,4.09" target="#b3">[4,</ref><ref type="bibr" coords="1,132.66,204.61,7.37,4.09" target="#b7">8]</ref> and later to improve search accuracy during search sessions <ref type="bibr" coords="1,125.21,215.57,9.36,4.09" target="#b0">[1]</ref>. The rapid adoption of a new generation of conversational assistants such as Alexa, Siri, Cortana, Bixby, and Google Assistant increase the scope and importance of conversational approaches to information seeking and also introduce a broad range of new research problems <ref type="bibr" coords="1,173.20,259.41,9.39,4.09" target="#b1">[2]</ref>.</p><p>The TREC Conversational Assistance Track (CAsT) is a new initiative to facilitate Conversational Information Seeking (CIS) research and to create a large-scale reusable test collection for conversational search systems. We define it as a task in which effective response selection requires understanding a question's context (the dialogue history). It focuses attention on user modeling, analysis of prior retrieval results, transformation of questions into effective queries, and other topics that have been difficult to study with previous datasets.</p><p>To make this tractable and reusable for the first year of CAsT, we begin with pre-determined conversation trajectories and passage responses. Our target conversations include several rounds of utterances that are coherent in topic and explore relevant information. The primary initial focus is on system understanding of information needs in a conversational format and finding relevant passages leveraging conversational context.</p><p>The long-term vision of CAsT is to allow natural conversions with mixed-initiative, where the system performs a variety of information actions <ref type="bibr" coords="1,110.87,467.63,9.47,4.09" target="#b6">[7]</ref>, e.g., providing information (INFORM), asking clarifying questions (CLARIFY), leading conversations with more interactions (SUGGEST), and others. For the first year we focus on context understanding and use simple INFORM actions, where systems return text passages to the user. In the future, we plan to explore richer sets of information actions, richer response formats, and more interactions between users and conversational agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK DESCRIPTION</head><p>CAsT defines conversational search as an information retrieval task in the conversational context. The goal of the task is to satisfy a user's information need, which is expressed or formalized through turns in a conversation. The response from the retrieval system is not a list of relevant documents. Instead the response is limited to brief text passages (approximately 1-3 sentences in length) suitable for presentation in a voice-interface or a mobile screen.</p><p>Task Definition. The task in Year 1 focuses on candidate response retrieval for information seeking conversations. Our goal is What makes it so unusual? 3</p><p>Tell me about its orbit. <ref type="bibr" coords="1,333.36,245.61,4.17,4.09" target="#b3">4</ref> Why is it tilted? 5</p><p>How is its rotation different from other planets? 6</p><p>What is peculiar about its seasons? 7</p><p>Are there any other planets similar to it? 8</p><p>Describe the characteristics of Neptune. 9</p><p>Why is it important to our solar system? 10</p><p>How are these two planets similar to each other? 11</p><p>Can life exist on either of them?</p><p>to create a low barrier to entry as well as keep the task simple for the purpose of creating a reusable collection. Specifically, given a series of natural conversational turns for a topic, ùëá , with utterances (ùë¢) for each turn ùëá = {ùë¢ 1 , ...ùë¢ ùëñ ...ùë¢ ùëõ }, the task is to identify relevant passages ùëÉ ùëñ for each turn (user utterance) ùë¢ ùëñ to satisfy the information needs in round ùëñ with the context in round ùë¢ &lt;ùëñ = ùë¢ 1 : ùë¢ ùëñ-1 .</p><p>To construct this task, we start with a selection of open-domain exploratory information needs and create conversational topics ùëá . Then we use the passage collection ùê∂ to provide candidate response passages for those topics.</p><p>Information Needs. We semi-manually constructed exploratory information needs (topics) from the combination of previous TREC topics (Common Core, Session Track, etc.), MS MARCO Conversational Sessions (described in Sec 3), and our own interests and experience. The information needs were selected to ensure complexity (requiring multiple rounds of elaboration), diversity (across different information categories), open-domain (not requiring expert domain knowledge to access), and answerable (sufficient coverage in the passage collections).</p><p>Conversational Sequences. We manually created the sequences of conversation utterances for each turn in a topic. In general, we started with a general introduction of the topic and then manually formulated exploratory information seeking trajectories. For the re-usability of the topics for year one we ensured that later turns only depended on the previous utterances, not on system responses (an area for future work).</p><p>When curating the conversational trajectories multiple sources of information are used. The MS MARCO search session data is one input. Query suggestions from commercial search engines (Google and Bing) and specifically the natural language questions from the "People Also Ask" feature in Google and Bing are used. These questions are similar to the questions released in the Google Natural Language Questions dataset <ref type="bibr" coords="1,422.81,703.09,9.39,4.09" target="#b5">[6]</ref>.</p><p>The conversational sequences are written to mimic "real" dialogues. Namely, we ensure that there are coherent transitions between turns. We also introduce common conversation phenomena including coreference and omission. Comparisons between various subtopics are also introduced where relevant. To focus on long-form dialogue most topic turns require more than a short answer response (i.e., a simple factoid response is insufficient).</p><p>An example topic from the released training set is shown in Table <ref type="table" coords="2,76.31,177.25,3.13,4.09" target="#tab_0">1</ref>. For the first year of the track, we developed 30 training and 50 evaluation topics, each with about ten turns. The evaluation conversations cover a diverse range of open-domain topics.</p><p>Passage Collection. The corpora used are passages from MS MARCO 1 , the TREC Complex Answer Retrieval Paragraph Collection <ref type="bibr" coords="2,70.68,232.04,9.39,4.09" target="#b2">[3]</ref>, and the Washington Post 2 collections.</p><p>The TREC CAR (Wikipedia) paragraphCorpus V2.0 3 is used, which consists of all paragraphs from Wikipedia '16. Note that this corpus has been deduplicated. It contains approximately 30 million unique paragraphs. Refer to the TREC CAR Overview <ref type="bibr" coords="2,246.21,275.88,10.43,4.09" target="#b2">[3]</ref> for details on how this corpus was created.</p><p>MS MARCO also has 1M real search queries each with 10 passages from top ranked results, resulting in a pool of approximately 8 million passages. A passage's metadata includes the source URL, the MARCO queries associated with it, and relevance labels for adhoc passage retrieval. The MARCO collection does contain near duplicates.</p><p>The Washington Post Collection (WaPo) collection was also initially used and was included in the submitted runs. When deduplicating the WaPo corpus an error in the process led to ambiguous document ids. As a result, the final assessments are restricted to MS MARCO and CAR passages. The MS MARCO and CAR passages combined provided a sizable and proven collection of passages. The WaPo documents are filtered from all the runs for pool creation and evaluation (this removed less than 5% of results returned by systems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESOURCES</head><p>Beyond the conversational topics and passage collections, the organizers also provided additional resources to track participants and release those feasible ones to the public for future CIS research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data and Manual Annotations.</head><p>We curated and provided three resources for model training: training topics with incomplete manual judgments, MS MARCO conversational search sessions, and manually rewritten topics. We also provided near-duplicate files of the passage collections.</p><p>Training data. The organizers created thirty training topics. Five of these topics have manually created relevance assessments developed by a CMU PhD student (approximately 50 turns). Relevance assessment for these was performed on a different (compressed) three-point relevance scale.</p><p>External Data. Building on MARCO and TREC CAR collections allowed this track to share previous existing relevance labels 1 http://www.msmarco.org/ 2 https://trec.nist.gov/data/wapost/ 3 http://trec-car.cs.unh.edu/datareleases/ for the non-conversational topics. These labels could be used by participants to train single-shot relevance.</p><p>For CAsT the organizers also created an extension of the MS MARCO collection, the MS MARCO Conversational Search Session dataset <ref type="foot" coords="2,344.38,130.93,3.38,3.32" target="#foot_0">4</ref> . The Conversation Search Sessions are constructed by aligning the one million released MS MARCO queries to Bing search sessions, to simulate actual sessions in search logs. The alignment was conducted using the Generic Intent Encoder that maps queries with similar search intents together <ref type="bibr" coords="2,453.46,177.25,9.52,4.09" target="#b8">[9]</ref>. We first obtained all the encodings of MS MARCO queries and built an approximate nearest neighbor index (ANN) <ref type="foot" coords="2,398.31,196.68,3.38,3.32" target="#foot_1">5</ref> . Then for each Bing search session, we ran its queries on the ANN index and replaced the Bing query with the most similar MS MARCO query if their cosine similar was greater than 0.85 (considered to be paraphrases). If such a MS MARCO query did not exist, the query was discarded from the session. We kept those sessions longer than three queries that are also topically coherent following gen encoder's definition <ref type="bibr" coords="2,487.37,264.92,9.52,4.09" target="#b8">[9]</ref>. The result is a publicly available dataset of realistic information seeking sessions.</p><p>Manual Conversation Rewrites. As shown in Table <ref type="table" coords="2,536.76,286.84,3.13,4.09" target="#tab_0">1</ref>, the topic utterances include natural phenomena including coreference and omission. To facilitate assessment and research, we manually created an annotated dataset for the training and evaluation topics to resolve ambiguity and implicit conversational context. The manually rewritten utterances ("resolved") contain all of the information required to represent the single turn of the underlying information need.</p><p>Each utterance was rewritten by two organizers. The results were compared and the disagreements adjudicated to a canonical form. On average it took approximately 5-10 minutes to rewrite a topic (ten turns on average), indicating this is non-trivial even for those familiar with the topics.</p><p>Passage Collection Deduplication. Early results found that both the MARCO and WaPo corpora contain a significant number of near-duplicate paragraphs. The organizers ran near-duplicate detection to cluster results; only one result per duplicate cluster was evaluated. The organizers recommended to participants that they remove duplicates (keeping the canonical document) from their indices.</p><p>The MARCO near-duplication algorithm grouped passages based on their URLs. Within each URL group, passages were first sorted by their length in descending order. Pairwise matches between the passages in the group were calculated. A pairwise match was defined as the total percentage of matching words in the smaller passage with respect to the longer passage in the input pair. If this percentage match was greater than 95%, then the ID of the smaller passage was added to the near-duplicate dictionary. The IDs in the final duplicate dictionary were then mapped back to the MS-MARCO ranking corpus IDs (based on a prior alignment of IDs between the two corpora).</p><p>A similar procedure was used for WaPo, but the organizers discovered that the near-duplicate algorithm was run on an outdated version of the WaPo collection and could not be corrected easily. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Software Tools</head><p>The track provided various software tools to support the development of CIS systems.</p><p>CAsT Topic Tools. <ref type="foot" coords="3,139.36,258.11,3.38,3.32" target="#foot_2">6</ref> These tools are publicly available. The tools include sample code to load the conversation topics in both Python and Java. The topic files are available in multiple formats including JSON, text, and Google Protocol Buffers. The protocol buffer format is the canonical representation.</p><p>Indri Baseline Retrieval System. The organizers provided web access to an Indri search engine for the CAsT corpus. <ref type="foot" coords="3,271.48,323.86,3.38,3.32" target="#foot_3">7</ref> During indexing, Krovetz stemming <ref type="bibr" coords="3,177.73,337.31,10.68,4.09" target="#b4">[5]</ref> was applied and stopwords were retained. CAR passages were indexed with title and body fields. MARCO and WAPO passages were indexed with only body fields. Near-duplicates were discarded from the MARCO and WAPO collections, as described above. Interactive and batch search were supported, with up to 1,000 results returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION METHODOLOGIES</head><p>We describe the judgment criteria, the labeling process, and the evaluation metrics in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Assessment Guidelines.</head><p>The evaluation of the returned passages is similar to relevance assessment in other TREC settings. However, the conversational setting introduces several unique challenges.</p><p>(1) Contextualized: The meaning of a turn and the relevance of an answer passage may depend on preceding turns in the same conversation. For example, "What is throat cancer?" followed by "Is it treatable?" Each question must be interpreted in the context established by the preceding turn. (2) Coreference and omission: As with most human conversations, many CAsT turns have some form of ellipsis, for example, pronouns and implied context that omits words that can be understood from the preceding context. To aid assessment, the track organizers provide resolved versions of each turn. For example, the resolved version of "Is it treatable?" is rewritten to "Is throat cancer treatable?". (3) Brevity and Completeness: Conversational assistants interact with users via spoken or chat interfaces that are designed for brief responses. The answer passages in the CAsT corpus tend to be short. A good system will select passages that provide a complete answer in a concise response. The relevance standard for a [turn, passage] pair is intended to represent how a person would feel if she asked the question to her favorite conversational assistant (Siri, Cortana, Alexa, Google Assistant, etc.) and it responded with the passage. A five-point relevance scale from the Google Needs Met rating scale <ref type="foot" coords="3,481.02,155.16,3.38,3.32" target="#foot_4">8</ref> was adapted for the CAsT task with the following definitions.</p><p>(1) Fully meets (4). The passage is a perfect answer for the turn.</p><p>It includes all of the information needed to fully answer the turn in the conversation context. It focuses only on the subject and contains little extra information. (2) Highly meets <ref type="bibr" coords="3,392.46,223.88,8.80,8.02" target="#b2">(3)</ref>. The passage answers the question and is focused on the turn. It would be a satisfactory answer if Google Assistant or Alexa returned this passage in response to the query. It may contain limited extraneous information. (3) Moderately meets <ref type="bibr" coords="3,405.31,267.71,8.45,8.02" target="#b1">(2)</ref>. The passage answers the turn, but is focused on other information that is unrelated to the question. The passage may contain the answer, but users will need extra effort to pick the correct portion. The passage may be relevant, but it may only partially answer the turn, missing a small aspect of the context. (4) Slightly meets <ref type="bibr" coords="3,395.89,333.46,8.80,8.02" target="#b0">(1)</ref>. The passage includes some information about the turn, but does not directly answer it. Users will find some useful information in the passage that may lead to the correct answer, perhaps after additional rounds of conversation (better than nothing). ( <ref type="formula" coords="3,332.04,390.11,3.17,4.09" target="#formula_0">5</ref>) Fails to meet (0). The passage is not relevant to the question.</p><p>The passage is unrelated to the target query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Assessment Process</head><p>The labeling approach uses the standard TREC style pooling and relevance assessments.</p><p>Pooling. We created the assessment pool using the two runs marked as highest priority from each participant. The pool depth was judged to depth 10. We also ensured that we included two manual runs from the organizers, but this only added a small number of results to the pool (about 30). The total pool size for all turns for the 20 target topics is 33,614 unique paragraphs, several topics were truncated early in the final assessment.</p><p>Relevance Assessments. The relevance assessments were done by NIST assessors during a three-week period. Six assessors each worked approximately 50 hours. The average labeling speed was about 100 minutes per turn, or about 35 second per paragraph. When labeling, the assessors were provided with both the raw utterance and also our manually re-written ("resolved") utterance. The latter was written to contain full information to define the passage relevance without depending on previous rounds. The assessors were presented with one topic at time in order of the turns. Thus the conversational context was preserved in the labeling process.</p><p>A total 20 conversational topics are labeled (almost completely), with each of them labeled to the eighth round, on average. There are total of 173 turns judged. The statistics for the distribution of the relevance labels is provided in Table <ref type="table" coords="3,470.27,677.45,3.13,4.09" target="#tab_1">2</ref>. There are on average 170 unique paragraphs per turn in the pool. Each has on average 47 paragraphs with non-zero relevance score. Turn 75_7 had no relevant paragraphs in the assessment pool. And further turn 31_3 had all assessed passages at least slightly relevant.</p><p>The track attracted twenty one participants with a diverse combination of techniques. This made the labeling budget a significant challenge for the first year. It was also unclear what the optimal labeling depth and number of turns per topic should be. The evaluation results from year one provide some observations and will help guide the design of the evaluation methodologies for year two.</p><p>Evaluation Metrics. There are two dimensions in the ranking evaluation, the ranking depth and the turn depth. The ranking depth is the same as for adhoc search, but we focus on the earlier positions <ref type="bibr" coords="4,106.25,232.04,8.99,4.09" target="#b0">(1,</ref><ref type="bibr" coords="4,118.28,232.04,6.26,4.09" target="#b2">3,</ref><ref type="bibr" coords="4,127.60,232.04,6.98,4.09" target="#b4">5)</ref> for the conversational scenario. The turn depth evaluates the system performance at the n-th conversational turn. Performing well on deeper rounds indicates better ability to understand contexts.</p><p>We use the mean NDCG@3 as the main evaluation metric, with all conversation rounds averaged using uniform weights. We also measure the turned-depth measure based NDCG@3&amp;N, with the per query NDCG@3 scores averaged at depth (N). Finally we calculate the MAP and Mean Reciprocal Rank to evaluate the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PARTICIPANTS</head><p>CAsT received 65 run submissions from 21 teams shown in Table <ref type="table" coords="4,289.41,356.04,3.01,4.09" target="#tab_3">3</ref>. This includes 2 teams and 8 submissions from the organizing institutions. When submitting, we asked the participants to provide metadata describing certain properties of their runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submitted team descriptions</head><p>Below are brief summaries of approaches from each participant. Teams are listed in alphabetical order.</p><p>‚Ä¢ ATeam We trained several sequence-to-sequence generation models to translate questions augmented with previous conversation turns into stand-alone questions that are afterwards used to retrieve relevant passages with Anserini and re-rank them using a BERT-based model. Our question rewriting approach follows the transfer learning paradigm in which we utilised a pre-trained GPT-2 Transformer model and fine-tuned on the question rewriting task using a newly developed conversational QA dataset. ‚Ä¢ ADAPT-DCU We focus finding relevant information using contextual information from the queries. We divide our investigation of finding relevant information for conversational search into two aspects: i) Effective query formulation using syntactic analysis, ii) Data Fusion results for re-ranking top candidates retrieved from three different data sources. the usefulness of conversation context for ranking. We first rank all passages based on the union of all the turns in the conversation, using BM25, to create a pool of candidate answers. Next, we rerank the pool using BERT for 1) just the final question in the conversation, or 2) taking the max score fusion results of reranking with the three last questions in the conversation. The method had been tuned on the MS MARCO passage collection using different amounts of context and varying rank and score fusion methods. ‚Ä¢ TREMA-UNH Our methods are based on entity and passage features without any dedicated question answering and dialogue tracking component. The base run works on entity relations that co-occur in top ranked passages. Subsequent runs build on that and combine both text and entity features. ‚Ä¢ udel_fang We proposed a method that consists of two key steps: query formatting and passage re-ranking. We first apply the coreference resolution model and add the topic title. We retrieve the top 100 passages with Indri and re-rank them in a second phase using a fine-tuned BERT model. ‚Ä¢ uogTr Glasgow used probabilistic retrieval based on the Sequential Dependence model. Experiments were performed varying previous turns as weighted context as well as combined with feedback models (RM3). A model was also trained Interaction matrices are fed to a CNN followed by max pooling, and then a BiGRU layer. We finally feed the learned representation to a multi-layer perceptron (MLP) network to generate the matching score. We train the model on MS MARCO session data in a pairwise setting. ‚Ä¢ USI To understand the dependency of conversation turns, we have annotated each turn of the conversation with related (relevant) turns in the conversation's context. We employed a high-dimensional language and position representationbased classifier to predict the relevant utterances against current utterance(s) and use them, along with other heuristics, to reformulate the query. The passage retrieval is then performed using classical term-matching models followed by neural re-ranking. ‚Ä¢ UvA.ILPS Submitted both unsupervised and supervised approaches. The unsupervised run is based on language modeling and expansion using relevance feedback (RM3). Our supervised runs rerank the set of passages retrieved by the unsupervised run. BERT is used to encode the sequence of queries up to the current turn and the passage to produce a matching score. The final matching score is obtained by linearly combining the BERT score and the unsupervised ranker's score. MS MARCO was used for pretraining as well as a dataset originally proposed for a different task (Question Answering in Context). ‚Ä¢ WaterlooClarke The overall approach can be explained as three steps: 1) query construction, 2) passage retrieval and ranking, and 3) passage re-ranking. Query construction used crude methods to improve retrieval performance and maintain conversational context between turns. Passage retrieval and ranking used standard BM25 ranking with pseudo-relevance feedback. Re-ranking was treated as a classification task with class probabilities used for re-ranking.</p><p>We observe diverse approaches being utilized. There are traditional retrieval based methods, feature based learning-to-rank, neural models, and knowledge enhanced methods. A common theme through many of them is the use of BERT-based reranking methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OVERALL RESULTS</head><p>In this section we present results of the submitted runs. We first present results macro-averaged at the level of every turn independently. We use three standard TREC evaluation measures, Mean-average Precision (MAP), and Normalized Discounted Cumulative Gain (NDCG), and Mean Reciprocal Rank (MRR). In particular, we use NDCG@3 as the primary measure because we focus on graded relevance of results at the top ranks.</p><p>For reporting results we make a distinction between automatic and manual runs. Automatic runs use the provided test topics. Manual runs use the test topics, but use the manually rewritten (resolved) queries where coreference and other phenomena have been replaced to create clear and unambiguous utterances. Automatic run results. The results for the 41 automatic runs are provided in Table <ref type="table" coords="6,396.50,396.19,3.02,4.09" target="#tab_4">4</ref>. The results show systems that vary widely in effectiveness. The median NDCG@3 score of the automatic runs is 0.286. The best performing run not utilizing BERT for reranking is clacBase with a NDCG@3 value of 0.360. We observe that nine of the top ten best performing runs use a form of BERT for ranking results. The top two performing teams perform contextual query rewriting and expansion.</p><p>Manual run results. The results for the 24 manual runs are provided in Table <ref type="table" coords="6,387.94,483.86,3.13,4.09" target="#tab_5">5</ref>. The median manual run has an NDCG@3 value of 0.361. We observe that the manual_indri query-likelihood run provided by the organizers is the median run. This indicates that models trained on the limited training data may not have generalized well. The best performing run is humanbert with an NDCG@3 value of 0.589. The humanbert run uses BERT-large as a reranking method on top of Anserini results. Similar to the automatic runs, the best performing runs all leverage BERT as a feature in reranking. We also observe the gap between the best manual and automatic runs is large, a 26% relative difference in median and 35% relative difference in the best runs.</p><p>Influence of WaPo posting filtering. Post-filtering the Washington Post paragraphs in the run results may benefit those runs do not include WaPo results. There are eight such runs and four are top performing ones: BM25_BERT_RANKF, BM25_BERT_FC, pgbert, and humanbert. Overall about 10% of pool candidates are from WaPo.</p><p>Pool Incompleteness. Due to assessment resource constraints, only the top two prioritized runs (out of total maximum four) from each group are pooled. The influence of this incomplete pooling appears to be small for submitted systems. There is about a 10% absolute NDCG score difference between pooled and unpooled runs; however, it is not clear how much of this gap originates from the preference of the teams-they likely picked their best runs to be in the pool. There are on average 0.6 passages unjudged from top runs (in the top 10). The average number of unjudged documents in the top 10 results is 1.67 per turn for all unpooled runs. Approximately half of the unjudged documents are from three groups with the lowest effectiveness overall. Further, the top performing run is not included in the pool. Although more work could be done, we are optimistic about the reusability of the produced benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results by turn depth</head><p>We plot the average NDCG@3 at each turn depth. The results are shown in Figure <ref type="figure" coords="7,115.69,448.81,3.09,4.09" target="#fig_0">1</ref>. Turns beyond eight are truncated due to small sample size. We measure statistical significance with paired t-test at greater than 95% confidence interval (with Bonferroni correction where needed).</p><p>For automatic runs the table shows the best depths are early at 1 and 3. For an unknown reason there is a significant dip for automatic runs at the second turn. There is a downward trend from an average of approximately 0.3 at the first turn to an average of 0.23 by turn eight. This represents a statistically significant decrease of 23% from the start turn to the end. However, the variation in the decline varies by depth. For example, depth 7 is statistically equivalent to the effectiveness of the first turn. The reason for this behavior bears further investigation.</p><p>The results for manual runs show a different pattern. Effectiveness dips very slightly (insignificantly) at turn 2, but increases steadily until turn 4. It drops at turn 5, possibly due to shifting subtopics. Unlike for the automatic runs the result at the end of the conversation is statistically equivalent to the start of the conversation. The manually resolved queries do not face the challenge of conversational query rewriting. When compared with the automatic runs the results show an increasing gap in system effectiveness over time (except at 5 where both perform poorly). Comparing the start and the end there is a more than 100% relative increase in the effectiveness gap between manual and automatic runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS ANALYSES</head><p>We encouraged the groups to submit metadata along with their runs. We designed a questionnaire with Yes/No questions in three categories: Query Understanding, Training/Retrieval Models, and the Utilization of Context information. Each question asks the teams to provide whether their runs used a specific type of resource or technique. This section discusses the impact of the varying approaches and resource usage on system effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Query Understanding</head><p>The query understanding questions show the techniques used in understanding the conversational queries, including but not limited to query rewriting, term re-weighting, query expansion, coreference resolution, and others. In total the following eleven binary questions are asked.</p><p>(1) Entity Linking. Whether the method uses entity linking techniques on the query.  For our analysis we focus on automatic runs because manual runs had key understanding issues removed. Figure <ref type="figure" coords="8,227.19,312.51,4.21,4.09" target="#fig_1">2</ref> plots he fraction of the automatic runs using each feature (answering Yes to the question) as well as the relative NDCG@3 performances of those using the feature over those that do not.</p><p>The most popular technique in query understanding is deep learning, with 57% of runs using it, but the influence is slightly negative, performing 8% worse on average than those runs not using it. NLP toolkits are used by half of runs but there is no relative gain by using it. There are only two runs labeled as "None" in the query understanding category, so its relative gain might not be reliable due to small sample size (one run was the best performing CFDA_CLIP_RUN7 run.</p><p>Overall the results reveal the challenge of query understanding in CIS. Many widely utilized techniques from adhoc retrieval led to negative average contribution to conversational search accuracy. The only significant gains are from manually designed rules that used query term reweighting and conversational stopword removal. The most effective method was a form of query expansion leveraging results from previous turns.</p><p>To characterize the nature of the query understanding challenges the organizers manually analyzed the topics. We highlight four primary types of language coreference phenomena observed. Examples of the four phenomena and their statistics in the CAsT Y1 train and test conversations are listed in Table <ref type="table" coords="8,217.94,564.57,4.09,4.09" target="#tab_6">6</ref> and Table <ref type="table" coords="8,259.94,564.57,3.01,4.09" target="#tab_7">7</ref>. How to handle these types of conversational contextualization effectively is one of the main challenges for CIS identified in CAsT Y1. In particular, the use of zero anaphora (implied mention) and group coreference differs from typical coreference in other text genres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Retrieval and Ranking</head><p>Eight questions were asked on the data and techniques used in the retrieval and ranking model components. ( The fraction of each technique/data being used and their influence for automatic runs is shown in Figure <ref type="figure" coords="8,458.07,559.87,3.07,4.09">3</ref>.</p><p>Utilizing any types of training data leads to improvements in effectiveness. The unsupervised runs, consisting of 43% of all runs, performed 8% worse on average. The MS MARCO training data is the most frequently used supervision source. It was also the recommended source for single turn relevance training data. Using it leads to a greater than 20% improvement on average. This is not surprising given the large fraction of MARCO passages in our corpus and the similarity between the single turn passage ranking task. The other most effective method is to leverage 'other' additional training data, although the sample size is small; it was only used by four runs from two teams. In particular, this reflects the three strongly performing ATeam runs that used Google Natural Questions as a training source. The most used ranking method is neural ranking methods, half of the CAsT Y1 runs leveraged deep learning techniques. The effectiveness of using them, however, is mixed. On average, using deep learning does not necessary lead to better ranking effectiveness. There is no difference over teams not using them on average. However, at the same time, nine out of top ten best performing runs used neural approaches. This shows the potential of these methods as well as the challenge in using them with consistent effectiveness.</p><p>Figure <ref type="figure" coords="9,90.25,465.90,4.25,4.09">3</ref> only reports data for automatic runs, but the results across all runs are similar. The biggest difference between automatic and manual runs is the effectiveness of neural methods. For manual runs they show a 19% relative improvement versus 1% for automatic runs. The current neural methods appear to be more effective on reranking for the manually resolved queries, which bears further investigation. It could be due to improved result recall, the resolved queries themselves, or combination of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Conversational Context</head><p>Another key challenge in conversational search is to accurately infer information needs through the conversation history in the multi-round interactions. The inferred user's information needs can be used as part of the query understanding in query expansion/rewriting, or in the ranking models.</p><p>There are a variety of contextual information a CIS system could leverage. In CAsT Y1, we asked about five types of context.  Figure <ref type="figure" coords="9,342.72,117.55,4.09,4.09" target="#fig_4">4</ref> shows the use of the varying context information and their influence on automatic runs.</p><p>The majority of runs (86%) utilized previous turns in the multiround conversation; these are crucial to resolve the contextual dependence of the current turn using previous information. The title of the conversation topic is the most effective context; it is manually written by the organizers and its keyword-like style can be effectively handled by adhoc retrieval systems.</p><p>A few runs used the description or all turns of the conversation. These two resources are challenging to utilize. For all turns, we also think this metadata is be noisy and some teams may have interpreted all turns as previous turns. For the description, a potential reason could be that the long description is difficult to use effectively.</p><p>For the manual runs, a much smaller fraction used previous turn context, only 20%. This is because ambiguous context was manually rewritten.</p><p>The descriptions, latter turns, and conversation titles are not actual interactions between the user and the CIS system. These are unlikely to be available in actual conversational search systems and future iterations may not allow their use by automatic systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In the first year of TREC CAsT we learned a lot about the structure of the problem of conversational search.</p><p>‚Ä¢ Conversational Language Understanding. Existing offthe-shelf coreference models struggled with TREC CAsT topics more than expected. The results using the manually resolved queries demonstrates a gap of approximately 35% over the best automatic system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Conversational Context The results on the manual runs</head><p>show that clean context has potential to maintain or even improve effectiveness over the course of the conversation as an information need unfolds. In contrast, automatic runs show a decline in effectiveness as turn depth increases. ‚Ä¢ Ranking. BERT-based neural models are the current leading method for response ranking across both manual and automatic methods. However, its application has mixed results. Many BERT-based runs are outperformed by simpler traditional ranking approaches. The neural reranking approaches show a larger relative gain on cleaner manually resolved queries, indicating that effective query formulation is an important factor.</p><p>After the success of the first year we look forward to year two.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,62.38,250.53,223.09,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NDCG@3 at varying conversation turn depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,317.96,307.39,241.85,7.70;7,317.96,318.35,113.35,7.70"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Query understanding influences on system effectiveness for automatic runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,328.87,547.32,193.86,8.02;7,328.87,558.28,229.57,8.02;7,342.36,571.09,93.36,4.09;7,328.87,580.20,218.13,8.02;7,328.87,591.16,229.34,8.02;7,342.36,603.97,27.88,4.09;7,328.87,613.08,230.85,8.02;7,342.36,625.88,16.61,4.09;7,328.87,635.00,229.34,8.02;7,342.36,647.80,162.58,4.09;7,328.87,656.91,229.34,8.02;7,342.36,669.72,181.25,4.09;7,328.87,678.83,230.71,8.02;7,324.70,689.79,191.43,8.02;7,324.70,700.75,189.10,8.02"><head>( 2 )</head><label>2</label><figDesc>External. Whether the method uses external data. (3) Unsupervised. Whether the method uses unsupervised query understanding technique. (4) Deep Learning. Whether the method uses deep learning. (5) Y1 Training. Whether the method uses CAsT Y1 training dataset. (6) Coreference. Whether the method uses coreference resolution. (7) MS MARCO Conv. Whether the method uses data from the MS MARCO Conversational Session dataset. (8) Y1 Manual Testing Query Annotation. Whether the method uses CAsT Y1 annotated (resolved) query dataset. (9) NLP Toolkit. Whether the method uses a standard NLP toolkit. (10) Rules. Whether the method uses heuristic rules. (11) None. No query understanding method is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,317.96,420.20,241.85,7.70;8,317.96,431.15,125.39,7.70"><head>( 1 )Figure 3 :</head><label>13</label><figDesc>Figure 3: Training retrieval model influences on system effectiveness for automatic runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,53.80,332.09,241.85,7.70;9,53.80,343.05,98.20,7.70"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Retrieval context influences on system effectiveness for automatic runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,64.71,656.92,179.65,8.02;9,64.71,667.88,229.34,8.02;9,78.21,680.68,121.17,4.09;9,64.71,689.79,212.72,8.02;9,64.71,700.75,230.72,8.02"><head>( 1 )</head><label>1</label><figDesc>Description. The long description of the topic. (2) All Turns. The utterances in the conversation topic, both before and after the current turn. (3) Previous Turns. The utterances before the current turn. (4) Other. External information outside those provided by CAsT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,328.87,87.73,166.26,8.02;9,328.87,98.69,224.74,8.02"><head>( 5 )</head><label>5</label><figDesc>Title. The short keyword title of the topic. (6) No. No context information is used, only the current turn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,333.36,154.18,198.65,73.59"><head>Table 1 :</head><label>1</label><figDesc>CAsT Training Topic 18.</figDesc><table coords="1,333.36,177.50,198.65,50.28"><row><cell cols="2">Title: Uranus and Neptune</cell></row><row><cell cols="2">Description: Information about Uranus and Neptune.</cell></row><row><cell cols="2">Turn Conversation Utterances</cell></row><row><cell>1</cell><cell>Describe Uranus.</cell></row><row><cell>2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,118.33,85.73,110.89,108.06"><head>Table 2 :</head><label>2</label><figDesc>Judgment statistics</figDesc><table coords="3,118.83,112.59,110.18,81.20"><row><cell>Topics</cell><cell>20</cell></row><row><cell>Turns</cell><cell>173</cell></row><row><cell>Assessments</cell><cell>29,571</cell></row><row><cell>Fails to meet (0)</cell><cell>21,451</cell></row><row><cell>Slightly meets (1)</cell><cell>2,889</cell></row><row><cell cols="2">Moderately meets (2) 2,157</cell></row><row><cell>Highly meets (3)</cell><cell>1,456</cell></row><row><cell>Fully meets (4)</cell><cell>1,618</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,69.77,89.58,489.96,617.06"><head></head><label></label><figDesc>‚Ä¢ CFDA_CLIP h2oloo The core methods uses BM25 retrieval, doc2query to expand the MSMARCO paragraphs, and reranking use a BERT Model trained on MS MARCO. We propose two ad-hoc and intuitive approaches: Historical Query Expansion and Historical Answer Expansion, to improve the performance of the conversational IR system with limited training data. ‚Ä¢ CMU CMU used BERT attention features for coreference resolution, and identifies context shift using KL Divergence between top retrieved documents for each turn in the conversation. Retrieval is done using Indri with (and without) query expansion. ‚Ä¢ ECNU-ICA Developed a retrieval-based conversational systems named linber. linber features include five modules: coreference resolution, keywords extraction, entity linking, retrieval using Elastic Search, and BERT re-ranking. ‚Ä¢ mpi-inf-d5 We propose an unsupervised method, termed CROWN: Conversational passage ranking by Reasoning Over Word Networks. CROWN works by formulating the passage score for a query as a combination of similarity and coherence, where this score is the objective to be maximized. CROWN builds a word-proximity network (WPN) from a large corpus, where words are nodes and there is an edge between two nodes if they co-occur in the same passage in a statistically significant way. Finally, passages are ranked using a weighted combination of the Indri retrieval score, a node score, and an edge score. ‚Ä¢ mpii Our approach consists of an initial stage ranker followed by a BERT-based neural document re-ranking model. BM25 with query expansion based on external knowledge (i.e., Wikipedia and ConceptNet) serves as the first stage ranking method, while the neural model uses BERT embeddings and a kernel-based ranking module (K-NRM) to predict a document-query relevance score. For training we repurpose and rewrite subtopics from the TREC Web Track's diversity task in such a way that the diversity task's existing relevance judgments may be used. ‚Ä¢ RUCIR Methods vary widely across runs and use a variety of text matching and learning to rank frameworks. AllenNLP is used for coreference resolution as well as key-value memory networks. Ranking is performed with a KNRM model as well as a MLP. A final query is generated and ranked using Indri. ‚Ä¢ RUIR The Radboud University IR team (RUIR) investigated</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,69.77,85.73,444.16,429.56"><head>Table 3 :</head><label>3</label><figDesc>Participants and their runs.</figDesc><table coords="5,78.21,108.97,435.72,373.44"><row><cell>Group</cell><cell>Run ID</cell><cell cols="3">Pooled Run Type Group</cell><cell>Run ID</cell><cell cols="2">Pooled Run Type</cell></row><row><cell cols="2">ADAPT-DCU combination</cell><cell></cell><cell>manual</cell><cell>UAmsterdam</cell><cell>ilps-bert-feat1</cell><cell>Y</cell><cell>automatic</cell></row><row><cell cols="2">ADAPT-DCU datasetreorder</cell><cell></cell><cell>manual</cell><cell>UAmsterdam</cell><cell>ilps-bert-feat2</cell><cell></cell><cell>automatic</cell></row><row><cell cols="2">ADAPT-DCU rerankingorder</cell><cell>Y</cell><cell>manual</cell><cell>UAmsterdam</cell><cell>ilps-bert-featq</cell><cell>Y</cell><cell>automatic</cell></row><row><cell cols="2">ADAPT-DCU topicturnsort</cell><cell>Y</cell><cell>manual</cell><cell>UMass</cell><cell>UMASS_DMN_V1</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>ATeam</cell><cell>humanbert</cell><cell>Y</cell><cell>manual</cell><cell>UMass</cell><cell>UMASS_DMN_V2</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>ATeam</cell><cell>pg2bert</cell><cell></cell><cell>automatic</cell><cell>USI</cell><cell>bertrr_rel_1st</cell><cell></cell><cell>automatic</cell></row><row><cell>ATeam</cell><cell>pgbert</cell><cell>Y</cell><cell>automatic</cell><cell>USI</cell><cell>bertrr_rel_q</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_RUN1</cell><cell></cell><cell>automatic</cell><cell>USI</cell><cell>galago_rel_1st</cell><cell></cell><cell>automatic</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_RUN6</cell><cell>Y</cell><cell>manual</cell><cell>USI</cell><cell>galago_rel_q</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_RUN7</cell><cell></cell><cell>manual</cell><cell>UvA.ILPS</cell><cell>ilps-lm-rm3-dt</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_RUN8</cell><cell>Y</cell><cell>manual</cell><cell>VES</cell><cell>VESBERT</cell><cell>Y</cell><cell>manual</cell></row><row><cell>CMU</cell><cell>coref_cshift</cell><cell></cell><cell>automatic</cell><cell>VES</cell><cell>VESBERT1000</cell><cell>Y</cell><cell>manual</cell></row><row><cell>CMU</cell><cell>coref_shift_qe</cell><cell>Y</cell><cell>automatic</cell><cell cols="2">WaterlooClarke clacBase</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>CMU</cell><cell>ensemble</cell><cell>Y</cell><cell>automatic</cell><cell cols="2">WaterlooClarke clacBaseRerank</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>CMU</cell><cell>manual_indri</cell><cell>Y</cell><cell>manual</cell><cell cols="2">WaterlooClarke clacMagic</cell><cell></cell><cell>automatic</cell></row><row><cell>ECNU-ICA</cell><cell>ECNUICA_BERT</cell><cell></cell><cell>automatic</cell><cell cols="2">WaterlooClarke clacMagicRerank</cell><cell></cell><cell>automatic</cell></row><row><cell>ECNU-ICA</cell><cell>ECNUICA_MIX</cell><cell>Y</cell><cell>automatic</cell><cell>h2oloo</cell><cell>h2oloo_RUN2</cell><cell></cell><cell>automatic</cell></row><row><cell>ECNU-ICA</cell><cell>ECNUICA_ORI</cell><cell>Y</cell><cell>automatic</cell><cell>h2oloo</cell><cell>h2oloo_RUN3</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>ICTNET</cell><cell>ict_wrfml</cell><cell>Y</cell><cell>automatic</cell><cell>h2oloo</cell><cell>h2oloo_RUN4</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>RALI</cell><cell>MPgate</cell><cell>Y</cell><cell>automatic</cell><cell>h2oloo</cell><cell>h2oloo_RUN5</cell><cell></cell><cell>automatic</cell></row><row><cell>RALI</cell><cell>MPmlp</cell><cell>Y</cell><cell>automatic</cell><cell>mpi-inf-d5</cell><cell>mpi-d5_cqw</cell><cell></cell><cell>automatic</cell></row><row><cell>RALI</cell><cell>SMNgate</cell><cell></cell><cell>automatic</cell><cell>mpi-inf-d5</cell><cell>mpi-d5_igraph</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>RALI</cell><cell>SMNmlp</cell><cell></cell><cell>automatic</cell><cell>mpi-inf-d5</cell><cell>mpi-d5_intu</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>RUCIR</cell><cell>RUCIR-run1</cell><cell>Y</cell><cell>automatic</cell><cell>mpi-inf-d5</cell><cell>mpi-d5_union</cell><cell></cell><cell>automatic</cell></row><row><cell>RUCIR</cell><cell>RUCIR-run2</cell><cell>Y</cell><cell>automatic</cell><cell>mpii</cell><cell>mpi_base</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>RUCIR</cell><cell>RUCIR-run3</cell><cell></cell><cell>automatic</cell><cell>mpii</cell><cell>mpi_bert</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>RUCIR</cell><cell>RUCIR-run4</cell><cell></cell><cell>automatic</cell><cell>udel_fang</cell><cell>UDInfoC_BL</cell><cell></cell><cell>automatic</cell></row><row><cell>RUIR</cell><cell>BM25_BERT_FC</cell><cell>Y</cell><cell>automatic</cell><cell>udel_fang</cell><cell>UDInfoC_TS</cell><cell>Y</cell><cell>automatic</cell></row><row><cell>RUIR</cell><cell>BM25_BERT_RANKF</cell><cell>Y</cell><cell>automatic</cell><cell>udel_fang</cell><cell>UDInfoC_TS_2</cell><cell>Y</cell><cell>automatic</cell></row><row><cell cols="2">TREMA-UNH UNH-trema-ecn</cell><cell>Y</cell><cell>automatic</cell><cell>uogTr</cell><cell>ug_1stprev3_sdm</cell><cell></cell><cell>automatic</cell></row><row><cell cols="2">TREMA-UNH UNH-trema-ent</cell><cell>Y</cell><cell>automatic</cell><cell>uogTr</cell><cell>ug_cedr_rerank</cell><cell>Y</cell><cell>automatic</cell></row><row><cell cols="2">TREMA-UNH unh-trema-relco</cell><cell></cell><cell>automatic</cell><cell>uogTr</cell><cell>ug_cont_lin</cell><cell>Y</cell><cell>automatic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>uogTr</cell><cell>ug_cur_sdm</cell><cell>Y</cell><cell>manual</cell></row><row><cell cols="4">that used the CEDR deep learning model trained on MS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MARCO for reranking passages.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="5,69.77,487.01,225.80,8.41;5,78.21,500.24,215.84,4.09;5,78.21,511.20,217.22,4.09"><p>‚Ä¢ UMass Our re-ranking model is based on convolutional neural networks. It takes into account the context and benefits from bag-of-words pre-trained embeddings (i.e., word2vec).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,58.28,85.73,231.28,452.57"><head>Table 4 :</head><label>4</label><figDesc>Automatic response retrieval results.</figDesc><table coords="6,58.28,108.80,231.28,429.50"><row><cell>Run</cell><cell>Group</cell><cell cols="2">MAP MRR NDCG@3</cell></row><row><cell>SMNgate</cell><cell>RALI</cell><cell>0.030 0.072</cell><cell>0.008</cell></row><row><cell>ECNUICA_BERT</cell><cell>ECNU-ICA</cell><cell>0.008 0.106</cell><cell>0.021</cell></row><row><cell>mpi-d5_union</cell><cell>mpi-inf-d5</cell><cell>0.098 0.274</cell><cell>0.078</cell></row><row><cell>MPmlp</cell><cell>RALI</cell><cell>0.054 0.285</cell><cell>0.090</cell></row><row><cell>SMNmlp</cell><cell>RALI</cell><cell>0.060 0.244</cell><cell>0.090</cell></row><row><cell>UMASS_DMN_V1</cell><cell>UMass</cell><cell>0.077 0.298</cell><cell>0.108</cell></row><row><cell>MPgate</cell><cell>RALI</cell><cell>0.053 0.282</cell><cell>0.108</cell></row><row><cell>indri_ql_baseline</cell><cell>-</cell><cell>0.139 0.328</cell><cell>0.152</cell></row><row><cell>galago_rel_q</cell><cell>USI</cell><cell>0.105 0.394</cell><cell>0.181</cell></row><row><cell>galago_rel_1st</cell><cell>USI</cell><cell>0.112 0.426</cell><cell>0.197</cell></row><row><cell>ECNUICA_MIX</cell><cell>ECNU-ICA</cell><cell>0.171 0.522</cell><cell>0.231</cell></row><row><cell>mpi_base</cell><cell>mpii</cell><cell>0.173 0.508</cell><cell>0.234</cell></row><row><cell>ECNUICA_ORI</cell><cell>ECNU-ICA</cell><cell>0.190 0.519</cell><cell>0.242</cell></row><row><cell>RUCIR-run2</cell><cell>RUCIR</cell><cell>0.092 0.494</cell><cell>0.253</cell></row><row><cell>UDInfoC_TS_2</cell><cell>udel_fang</cell><cell>0.061 0.541</cell><cell>0.253</cell></row><row><cell>coref_cshift</cell><cell>CMU</cell><cell>0.213 0.505</cell><cell>0.253</cell></row><row><cell>RUCIR-run3</cell><cell>RUCIR</cell><cell>0.093 0.502</cell><cell>0.255</cell></row><row><cell>ilps-lm-rm3-dt</cell><cell>UvA.ILPS</cell><cell>0.229 0.528</cell><cell>0.267</cell></row><row><cell>coref_shift_qe</cell><cell>CMU</cell><cell>0.224 0.509</cell><cell>0.272</cell></row><row><cell>RUCIR-run4</cell><cell>RUCIR</cell><cell>0.105 0.527</cell><cell>0.273</cell></row><row><cell>UDInfoC_TS</cell><cell>udel_fang</cell><cell>0.067 0.567</cell><cell>0.278</cell></row><row><cell>mpi-d5_cqw</cell><cell>mpi-inf-d5</cell><cell>0.185 0.591</cell><cell>0.286</cell></row><row><cell>mpi-d5_igraph</cell><cell>mpi-inf-d5</cell><cell>0.187 0.597</cell><cell>0.287</cell></row><row><cell>mpi-d5_intu</cell><cell>mpi-inf-d5</cell><cell>0.240 0.596</cell><cell>0.289</cell></row><row><cell>ensemble</cell><cell>CMU</cell><cell>0.258 0.587</cell><cell>0.294</cell></row><row><cell>bertrr_rel_q</cell><cell>USI</cell><cell>0.141 0.516</cell><cell>0.298</cell></row><row><cell>bertrr_rel_1st</cell><cell>USI</cell><cell>0.146 0.539</cell><cell>0.308</cell></row><row><cell>UDInfoC_BL</cell><cell>udel_fang</cell><cell>0.075 0.596</cell><cell>0.316</cell></row><row><cell>mpi_bert</cell><cell>mpii</cell><cell>0.166 0.597</cell><cell>0.319</cell></row><row><cell>ug_cont_lin</cell><cell>uogTr</cell><cell>0.275 0.584</cell><cell>0.325</cell></row><row><cell>ug_1stprev3_sdm</cell><cell>uogTr</cell><cell>0.253 0.585</cell><cell>0.328</cell></row><row><cell>clacBaseRerank</cell><cell cols="2">WaterlooClarke 0.244 0.629</cell><cell>0.343</cell></row><row><cell cols="2">BM25_BERT_RANKF RUIR</cell><cell>0.158 0.597</cell><cell>0.350</cell></row><row><cell>ilps-bert-feat2</cell><cell>UAmsterdam</cell><cell>0.256 0.603</cell><cell>0.352</cell></row><row><cell>BM25_BERT_FC</cell><cell>RUIR</cell><cell>0.158 0.601</cell><cell>0.354</cell></row><row><cell>ug_cedr_rerank</cell><cell>uogTr</cell><cell>0.216 0.643</cell><cell>0.356</cell></row><row><cell>clacBase</cell><cell cols="2">WaterlooClarke 0.246 0.640</cell><cell>0.360</cell></row><row><cell>ilps-bert-featq</cell><cell>UAmsterdam</cell><cell>0.262 0.653</cell><cell>0.365</cell></row><row><cell>ilps-bert-feat1</cell><cell>UAmsterdam</cell><cell>0.260 0.614</cell><cell>0.377</cell></row><row><cell>pg2bert</cell><cell>ATeam</cell><cell>0.258 0.641</cell><cell>0.389</cell></row><row><cell>pgbert</cell><cell>ATeam</cell><cell>0.269 0.665</cell><cell>0.413</cell></row><row><cell>h2oloo_RUN2</cell><cell>h2oloo</cell><cell>0.273 0.714</cell><cell>0.434</cell></row><row><cell>CFDA_CLIP_RUN7</cell><cell>CFDA_CLIP</cell><cell>0.267 0.715</cell><cell>0.436</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,317.66,85.73,240.54,273.33"><head>Table 5 :</head><label>5</label><figDesc>Manual response retrieval results. These runs used the manually resolved queries.</figDesc><table coords="6,322.57,119.82,231.02,239.24"><row><cell>Run</cell><cell>Group</cell><cell cols="2">MAP MRR NDCG@3</cell></row><row><cell>UMASS_DMN_V2</cell><cell>UMass</cell><cell>0.082 0.300</cell><cell>0.100</cell></row><row><cell>ict_wrfml</cell><cell>ICTNET</cell><cell>0.105 0.373</cell><cell>0.165</cell></row><row><cell>UNH-trema-ecn</cell><cell>TREMA-UNH</cell><cell>0.073 0.505</cell><cell>0.222</cell></row><row><cell>unh-trema-relco</cell><cell>TREMA-UNH</cell><cell>0.077 0.533</cell><cell>0.239</cell></row><row><cell>UNH-trema-ent</cell><cell>TREMA-UNH</cell><cell>0.076 0.534</cell><cell>0.242</cell></row><row><cell>topicturnsort</cell><cell>ADAPT-DCU</cell><cell>0.136 0.555</cell><cell>0.259</cell></row><row><cell>rerankingorder</cell><cell>ADAPT-DCU</cell><cell>0.137 0.564</cell><cell>0.259</cell></row><row><cell>combination</cell><cell>ADAPT-DCU</cell><cell>0.130 0.539</cell><cell>0.259</cell></row><row><cell>datasetreorder</cell><cell>ADAPT-DCU</cell><cell>0.135 0.550</cell><cell>0.260</cell></row><row><cell>VESBERT</cell><cell>VES</cell><cell>0.124 0.541</cell><cell>0.291</cell></row><row><cell>VESBERT1000</cell><cell>VES</cell><cell>0.204 0.555</cell><cell>0.304</cell></row><row><cell>manual_indri_ql</cell><cell>-</cell><cell>0.309 0.660</cell><cell>0.361</cell></row><row><cell>clacMagic</cell><cell cols="2">WaterlooClarke 0.302 0.687</cell><cell>0.411</cell></row><row><cell>clacMagicRerank</cell><cell cols="2">WaterlooClarke 0.301 0.732</cell><cell>0.411</cell></row><row><cell>RUCIR-run1</cell><cell>RUCIR</cell><cell>0.163 0.725</cell><cell>0.415</cell></row><row><cell>ug_cur_sdm</cell><cell>uogTr</cell><cell>0.334 0.715</cell><cell>0.421</cell></row><row><cell cols="2">CFDA_CLIP_RUN1 CFDA_CLIP</cell><cell>0.224 0.772</cell><cell>0.460</cell></row><row><cell>h2oloo_RUN4</cell><cell>h2oloo</cell><cell>0.319 0.811</cell><cell>0.529</cell></row><row><cell>h2oloo_RUN3</cell><cell>h2oloo</cell><cell>0.322 0.810</cell><cell>0.531</cell></row><row><cell cols="2">CFDA_CLIP_RUN8 CFDA_CLIP</cell><cell>0.361 0.854</cell><cell>0.560</cell></row><row><cell>h2oloo_RUN5</cell><cell>h2oloo</cell><cell>0.352 0.864</cell><cell>0.561</cell></row><row><cell cols="2">CFDA_CLIP_RUN6 CFDA_CLIP</cell><cell>0.392 0.861</cell><cell>0.572</cell></row><row><cell>humanbert</cell><cell>ATeam</cell><cell>0.405 0.879</cell><cell>0.589</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,136.07,85.73,339.87,75.19"><head>Table 6 :</head><label>6</label><figDesc>Query understanding examples from four reference types.</figDesc><table coords="8,136.07,111.03,339.87,49.88"><row><cell>Type</cell><cell>Utterance</cell><cell>Understanding</cell></row><row><cell>Pronominal</cell><cell cols="2">How to they celebrate Three Kings Day? they ‚Üí Spanish People</cell></row><row><cell>Zero</cell><cell>What cakes are traditional?</cell><cell>Null‚ÜíSpanish, Three Kings Day</cell></row><row><cell>Groups</cell><cell>Which team came first?</cell><cell>team‚ÜíAvengers, Justice League</cell></row><row><cell cols="2">Abbreviations What are the main types of VMs?</cell><cell>VMs‚ÜíVirtual Machines</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,53.50,183.56,240.77,86.15"><head>Table 7 :</head><label>7</label><figDesc>Manual counts of four reference types in CAsT Y1 (2019) conversational topics.</figDesc><table coords="8,118.68,219.82,110.49,49.88"><row><cell>Type</cell><cell cols="2">Train Test</cell></row><row><cell>Pronominal</cell><cell>102</cell><cell>128</cell></row><row><cell>Zero</cell><cell>82</cell><cell>111</cell></row><row><cell>Groups</cell><cell>6</cell><cell>4</cell></row><row><cell>Abbreviations</cell><cell>29</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,328.87,466.61,229.54,71.69"><head></head><label></label><figDesc>) KG. Whether the method uses a knowledge graph.</figDesc><table coords="8,328.87,477.57,229.54,60.73"><row><cell>(6) Y1 Training. Whether the method is trained with CAsT Y1</cell></row><row><cell>training data.</cell></row><row><cell>(7) MS MARCO Training. Whether the method is trained with</cell></row><row><cell>MS MARCO dataset.</cell></row><row><cell>(8) Other Training. Whether the method is trained with other</cell></row><row><cell>datasets.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,320.88,693.16,184.41,3.18"><p>https://github.com/microsoft/MSMARCO-Conversational-Search</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,321.00,701.57,94.59,3.18"><p>https://github.com/spotify/annoy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="3,56.84,686.96,120.05,3.18"><p>https://github.com/grill-lab/trec-cast-tools</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="3,56.72,695.37,237.78,3.18;3,53.80,703.59,75.31,3.18"><p>http://boston.lti.cs.cmu.edu/Services/treccast19/ and http://boston.lti.cs.cmu.edu/ Services/treccast19_batch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="3,321.00,695.75,209.49,3.18;3,317.73,703.97,107.15,3.18"><p>https://static.googleusercontent.com/media/guidelines.raterhub.com/en/ /searchqualityevaluatorguidelines.pdf</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Vaibhav Kumar</rs> for his work in developing topics, training relevance assessments, and the <rs type="funder">Indri</rs> search engine provided to CAsT participants. We thank the Deep Learning track organizers for helping align the two tracks. We also are deeply thankful for Ellen Voorhees' experience, patience, and persistence in running the assessment process. Finally we thank all our participants.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,65.99,101.19,228.05,3.18;10,65.99,109.16,229.13,3.18;10,65.99,117.13,112.73,3.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,200.55,101.19,93.49,3.18;10,65.99,109.16,13.27,3.18">Overview of the trec 2014 session track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,92.85,109.16,199.53,3.18">The Twenty-Third Text REtrieval Conference Proceedings (TREC 2014)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="500" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,65.99,125.10,228.87,3.18;10,65.99,133.07,228.23,3.18;10,65.99,139.83,110.42,5.99" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,173.05,125.10,121.82,3.18;10,65.99,133.07,228.23,3.18;10,65.99,141.04,13.82,3.18">Research frontiers in information retrieval: Report from the third strategic workshop on information retrieval in Lorne (SWIRL 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="34" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,65.99,149.01,228.05,3.18;10,65.99,156.98,228.05,3.18;10,65.99,164.95,131.06,3.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,203.38,149.01,90.67,3.18;10,65.99,156.98,24.03,3.18">Trec complex answer retrieval overview</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gamari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,103.92,156.98,190.13,3.18;10,65.99,164.95,13.82,3.18">The Twenty-Seventh Text REtrieval Conference Proceedings (TREC 2018)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="500" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,65.99,172.92,228.05,3.18;10,65.99,180.89,228.05,3.18;10,65.99,187.65,134.04,5.99" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,181.95,172.92,112.09,3.18;10,65.99,180.89,203.29,3.18">Cases, scripts, and information-seeking strategies: On the design of interactive information retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Thiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,274.78,180.89,19.26,3.18;10,65.99,188.86,75.88,3.18">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="395" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,65.99,196.83,228.05,3.18;10,65.83,204.80,228.21,3.18;10,65.99,212.77,141.20,3.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,100.43,196.83,126.91,3.18">Viewing morphology as an inference process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,241.18,196.83,52.87,3.18;10,65.83,204.80,228.21,3.18;10,65.99,212.77,66.08,3.18">Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 16th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,330.15,90.48,228.82,3.18;10,330.15,98.45,228.82,3.18;10,330.15,106.42,228.05,3.18;10,330.15,114.39,228.05,3.18;10,330.15,122.36,229.12,3.18;10,330.15,130.33,168.61,3.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,498.63,106.42,59.57,3.18;10,330.15,114.39,129.17,3.18">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<ptr target="https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="10,464.65,114.39,93.55,3.18;10,330.15,122.36,83.02,3.18">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,330.15,138.30,228.88,3.18;10,330.15,146.27,228.05,3.18;10,330.15,154.24,111.82,3.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,405.55,138.30,141.47,3.18">A theoretical framework for conversational search</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,330.15,146.27,228.05,3.18;10,330.15,154.24,36.54,3.18">Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval</title>
		<meeting>the 2017 Conference on Conference Human Information Interaction and Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,330.15,162.21,228.05,3.18;10,330.15,168.97,210.32,5.99" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,366.06,162.21,192.14,3.18;10,330.15,170.18,29.60,3.18">Conversation in information-seeking contexts: A test of an analytical framework</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,364.78,170.18,113.95,3.18">Library &amp; Information Science Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="248" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,330.15,178.15,228.87,3.18;10,330.15,186.12,229.23,3.18;10,330.15,194.09,228.05,3.18;10,330.15,202.06,93.09,3.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,330.15,186.12,126.61,3.18">Generic intent representation in web search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,471.04,186.12,88.34,3.18;10,330.15,194.09,228.05,3.18;10,330.15,202.06,24.30,3.18">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
