<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.01,112.05,303.98,15.12">Overview of the TREC 2019 Decision Track</title>
				<funder>
					<orgName type="full">AMAOS (Advanced Machine Learning for Automatic Omni-Channel Support)</orgName>
				</funder>
				<funder>
					<orgName type="full">Google Faculty Award</orgName>
				</funder>
				<funder>
					<orgName type="full">Innovationsfonden, Denmark</orgName>
				</funder>
				<funder ref="#_e3vR59c">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
				<funder ref="#_XmdaqxT">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,81.61,144.53,99.95,10.48"><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.43,144.53,83.48,10.48"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.79,144.53,73.55,10.48"><forename type="first">Maria</forename><surname>Maistro</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.22,144.53,90.23,10.48"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,499.10,144.53,31.30,10.48;1,285.42,158.48,36.42,10.48"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Queensland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.01,112.05,303.98,15.12">Overview of the TREC 2019 Decision Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DEDCDF3EDD7988EDB8A2225A3841220F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Search engine results underpin many consequential decision making tasks. Examples include people using search technologies to seek health advice online <ref type="bibr" coords="1,278.38,311.92,15.50,8.74">[10,</ref><ref type="bibr" coords="1,296.68,311.92,11.62,8.74" target="#b17">18]</ref>, or time-pressured clinicians relying on search results to decide upon the best treatment/diagnosis/test for a patient <ref type="bibr" coords="1,347.63,323.88,15.50,8.74" target="#b21">[22,</ref><ref type="bibr" coords="1,366.45,323.88,11.62,8.74" target="#b19">20]</ref>.</p><p>A key problem when using search engines in order to complete such decision making tasks, is whether users are able to discern authoritative from unreliable information and correct from incorrect information. This problem is further exacerbated when the search occurs within uncontrolled data collections, such as the web, where information can be unreliable, generally misleading, too technical, and can lead to unfounded escalations <ref type="bibr" coords="1,122.62,383.65,14.61,8.74" target="#b23">[24]</ref>. Information from search engine results can significantly influence decisions, and research shows that increasing the amount of incorrect information about a topic presented in a Search Engine Result Page (SERP) can impel users to take incorrect decisions <ref type="bibr" coords="1,317.50,407.56,14.61,8.74" target="#b15">[16]</ref>. As noted in the SWIRL III report <ref type="bibr" coords="1,489.37,407.56,9.96,8.74" target="#b6">[7]</ref>, decision making with search engines is poorly understood, and likewise, evaluation measures for these search tasks need to be developed and improved.</p><p>In this context, the TREC 2019 Decision track aims to (1) foster research on retrieval methods that promote better decision making with search engines, and (2) develop new online and offline evaluation methods to predict the decision making quality induced by search results.</p><p>This overview paper is organized as follows: Section 2 describes the track setup, the collection and the official evaluation measures, Section 3 reports and discusses the evaluation results for the submitted runs, and finally Section 4 outlines future directions for the next edition of the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Decision Track Setup</head><p>The track is planned over multiple years, with data and resources created in one year flowing into the next year. We plan for the track to run for at least 3 years, with 2019 being the first year.</p><p>In this year's edition, we proposed only Task 1, where we asked participants to devise search technologies that promote correct information over incorrect information, with the assumption that correct information can better lead people to make correct decisions. Note that this task is more than simply a new definition of what is relevant. Because incorrect information can have a negative effect on decisions <ref type="bibr" coords="1,453.88,617.55,14.61,8.74" target="#b15">[16]</ref>, there are three types of results: correct and relevant, incorrect, and non-relevant. It is important that search results avoid incorrect results, and ranking non-relevant results above incorrect is preferred.</p><p>For the next year's edition of the track (2020), we are renaming the track to be the Health Misinformation Track. The goals of the track remain the same. The new name makes clear that the key problem we are focused on in the near term is that of how health misinformation in search can negatively affect searcher decisions. For the TREC 2020 Health Misinformation track, we will add two tasks in addition to the current retrieval task. As planned, a new task will be to predict user decisions after the search in an offline context, i.e. to develop new evaluation measures for the track. The existing and continuing retrieval task (Task 1) has a goal of finding relevant, credible, and correct information, and to complement this task, we will add a task of finding all of the documents containing misinformation for a given search topic. Further details about the next year edition can be found in Section 4.</p><p>In the following sections, we describe the corpus and topics used in Task 1, and the assessment phase performed by National Institute of Standards and Technology (NIST).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus and Topics</head><p>Corpus The track used ClueWeb12-B13<ref type="foot" coords="2,258.66,167.68,3.97,6.12" target="#foot_0">1</ref> as the corpus, since this web collection provides a readily available source of documents that contain both correct and incorrect information and documents of varying credibility and quality. The full dataset consists of 52, 343, 021 English Web pages, collected between February 10, 2012 and May 10, 2012, and is a representative 7% sample of the whole ClueWeb12 corpus.</p><p>Topics The track focused on topics within the consumer health search domain (people seeking health advice online) to form user stories (search topics). Consumer health search represents an ideal prototypical example of the consequential decisions that we want search engines to correctly support. This domain also allows us to use evidence from systematic reviews<ref type="foot" coords="2,292.54,265.31,3.97,6.12" target="#foot_1">2</ref>  <ref type="bibr" coords="2,300.41,266.88,10.52,8.74" target="#b7">[8]</ref> to inform what a correct decision may be. Previous information retrieval evaluation challenges have tackled problems related to consumer health search, e.g., the CLEF eHealth CHS tasks <ref type="bibr" coords="2,209.08,290.79,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="2,228.76,290.79,12.73,8.74" target="#b11">12]</ref> and the FIRE 2016 CHIS task <ref type="bibr" coords="2,386.59,290.79,14.61,8.74" target="#b16">[17]</ref>. However the TREC Decision Track is novel in that it goes beyond the retrieval and ranking of search results, but it also consider the consequent decisions people make based on this information. This TREC track also considers multiple aspects of relevance along with topicality, and specifically correctness and credibility -in this it is similar to the CLEF eHealth CHS track, that considered trustworthiness (which partially resembles the notion of credibility we consider here) and understandability. Mark Smucker's research group at the University of Waterloo developed the set of topics for the track. Each topic consisted of a health treatment and a health issue (e.g. acupuncture for insomnia). Some of the topics used in the track are the same as used by White and Hassan <ref type="bibr" coords="2,366.79,386.44,14.61,8.74" target="#b22">[23]</ref>. White and Hassan <ref type="bibr" coords="2,472.61,386.44,15.50,8.74" target="#b22">[23]</ref> categorized the topics into three categories depending on the effectiveness of the treatment towards the health issue. The three categories are summarized as follows:</p><p>• Helpful : The health treatment is helpful towards the health issue.</p><p>• Inconclusive: It is still unclear by medical professionals whether or not the treatment is effective towards the health issue.</p><p>• Not helpful : The treatment is not helpful towards the health issue.</p><p>White and Hassan <ref type="bibr" coords="2,171.09,502.00,15.50,8.74" target="#b22">[23]</ref> assessed the effectiveness of the health treatments of their topics by reading the corresponding Cochrane Review<ref type="foot" coords="2,212.66,512.38,3.97,6.12" target="#foot_2">3</ref> . Cochrane Review provides systematic reviews of health-care interventions made by medical professionals for a broad audience <ref type="bibr" coords="2,293.65,525.91,9.96,8.74" target="#b2">[3]</ref>. Other topics were selected from the Cochrane Review library, and were assessed in a similar way as in White and Hassan <ref type="bibr" coords="2,370.62,537.87,14.61,8.74" target="#b22">[23]</ref>. In total, we created 51 topics (17 for each of the three categories above). Examples topics are reported in Figure <ref type="figure" coords="2,419.67,549.82,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevance, Efficacy and Credibility Assessment</head><p>The documents were judged by NIST with respect to three aspects: topical relevance, efficacy and credibility. A total of 22 859 assessments were collected. Table <ref type="table" coords="2,299.63,608.06,4.98,8.74">1</ref> reports the labels that were collected for each aspect with the corresponding value in the qrels file and the percentage of documents for each label.</p><p>Assessors judged a document for efficacy and credibility only if that document has been judged as Highly Relevant or Relevant.</p><p>The format adopted for the qrels file is as follows: where the columns are space separated, and the last three columns report the relevance, efficacy, and credibility labels respectively. Figure <ref type="figure" coords="3,214.83,500.34,9.96,8.74" target="#fig_3">2a</ref> shows a sample of the original qrels produced by NIST. Note that relevance assessments were not collected for topic 14 due to time limitations from NIST. Topic 14 has therefore been excluded from the qrels and the submitted runs, and a total of 50 topics (rather than the original 51 provided to participants) is used for evaluation in the TREC 2019 Decision track.</p><p>Topical Relevance Assessment Topical Relevance was judged similar to previous tracks at TREC. However, unlike previous tracks, the assessors did not create their own topic statements; instead, they were provided the topic query and narrative as shown in Figure <ref type="figure" coords="3,330.31,585.99,3.87,8.74" target="#fig_0">1</ref>.</p><p>Based on <ref type="bibr" coords="3,130.04,597.94,14.61,8.74" target="#b20">[21]</ref>, an assessor could judge the topical relevance of documents on a three points scale:</p><p>• Highly Relevant: if the document directly addresses the core issue of the topic;</p><p>• Relevant: if the document contains information that the user would find helpful in meeting their information need;</p><p>• Not Relevant: if the document does not contain helpful information, written in a foreign language (not in English), contains adult material, is unreadable or broken.</p><p>Note that the assessors were asked to judge the documents considering solely topical relevance, without considering whether the information provided by a document was incorrect and could harm the user. Table <ref type="table" coords="4,99.79,73.56,3.87,8.74">1</ref>: Labels collected for each aspect together with the corresponding value in the NIST qrels file and the percentage of documents with that value in the qrels. For "Efficacy -Not Judged" and "Credibility -Not Judged", the difference between -1 and -2 is as follows: -1 is assigned whenever the document was not relevant, while -2 is assigned whenever the document was relevant, but it was erroneously missed in the judged process by NIST. For example, if a document mentions that a medical intervention helps, but the assessor knows that it does not -based on previous knowledge or acquired knowledge from assessing other documents -the document's medical intervention efficacy is still judged as Effective. Assessors were not required to have any medical knowledge to assess efficacy because their task does not include testing the correctness of the information presented. Treatment efficacy was only judged when the topical relevance of the document was Highly Relevant or Relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head><p>Efficacy was labelled as one of the following:</p><p>• Effective: If the document states that the medical intervention is or can be an effective option to the health issue. If the document contains evidences for both the ineffective and effective directions, but it clearly supports the effective option over the ineffective one.</p><p>• Inconclusive: If the document contains evidence for both the ineffective and effective directions, but it does not clearly support one over the other. Or, it states that it is unknown whether or not the medical intervention helps. Or, it explicitly mentions the medical intervention, but does not provide any information on its efficacy, benefits, or disadvantages.</p><p>• Ineffective: If the document states that the medical intervention is ineffective or harmful. If the document contains evidence for both the ineffective and effective directions, but it clearly supports the ineffective option over the effective one;</p><p>• No Information: If the document does not state the health issue, but the assessor considered it relevant.</p><p>Credibility Understanding the purpose of a document should be the first step to judge credibility, thus the assessor's opinion of the purpose of the document and the credibility of information are fundamental in judging credibility. The idea of understanding the purpose of a website before judging its quality, determining the amount of Expertise, Authoritativeness, and Trustworthiness (E-A-T), is based on Google Search Quality Evaluator Guidelines<ref type="foot" coords="4,166.66,660.56,3.97,6.12" target="#foot_3">4</ref> .</p><p>Credibility is only judged if the topical relevance of the document is Highly Relevant or Relevant. Furthermore, credibility should not be confused with topical relevance or efficacy and was judged independently of them, since a Not Credible document may be equally useful or helpful for a person making his/her decision.</p><p>Credibility was labelled as one of:</p><p>• Credible: Some criteria used to consider a document credible are: 1) if the document has a high level of E-A-T, 2) includes an author or a publishing institute expert in the field, 3) includes citations or references to credible sources such as universities, research/clinics, government websites, medical publications, and lab studies, 4) is hosted in a hospital/clinic or government website, or online newspaper with wide circulation, is well written, motivated and organized.</p><p>• Not credible: If the document is a mask for advertising or marketing purposes, is from a personal blog or a forum, or written by a non-expert person. If the document itself, or the hosting website, provides or claims that go against well-known medical consensus (e.g., smoking cigarettes does not cause cancer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correctness Mapping</head><p>The correctness labels are computed based on a document's assessed efficacy by comparing the topic efficacy with the document efficacy. We consider a document Correct if the document label matches the document topic label. For example, consider Topic 8 in Figure <ref type="figure" coords="5,418.14,281.72,3.87,8.74" target="#fig_0">1</ref>, suggesting that melatonin can be used to reduce jet lag. Our truth is our interpretation of the systematic review done by the medical experts from Cochrane Review, which, in our opinion, concluded that melatonin is helpful. If a document for this topic was judged as Effective, i.e. the document claims that melatonin is helpful to reduce jet lag, then the document is considered correct, whereas a document judged as Ineffective or Inconclusive is considered not correct. Table <ref type="table" coords="5,155.53,341.49,4.98,8.74">2</ref> reports the mapping used to obtain the correctness labels on documents. Correctness labels for unjudged or non-relevant documents are not computed. Unjudged documents are assumed to be non-relevant, and non-relevant documents were not judged for efficacy or credibility.</p><p>Table <ref type="table" coords="5,99.52,386.89,3.88,8.74">2</ref>: Mapping of the correctness values from the topic and document labels, 1 stands for correct, while 0 stands for incorrect. Documents that do not provide information on the efficacy of the medical intervention (i.e. judged as "No Information" by the assessor) are considered to be "Not Correct" since they do not provide any evidence to support or reject the claim included in the topic narrative. As shown in Table <ref type="table" coords="5,406.76,528.39,3.87,8.74">1</ref>, the percentage of documents judged as No Information is particularly low, 0.38%, thus we do not expect them to affect the correctness labels significantly.  Finally, Figure <ref type="figure" coords="5,156.30,677.33,4.98,8.74" target="#fig_3">2</ref> reports some sample documents from the qrels file before and after the mapping to the correctness labels. The new qrels are formatted as the original qrels from NIST, except for the efficacy value -second to last column -which is replaced by the correctness value. For example, document clueweb12-0000wb-03-01030 was assessed as Inconclusive, but topic 1 was assessed as Helpful, therefore the document is Not Correct. Similarly, document clueweb12-0000wb-47-24784 was assessed as effective for topic 1, thus it is considered Not Correct. The documents clueweb12-0000wb-54-11923 and clueweb12-1902wb-14-21300 were not judged for efficacy, therefore they result as not judged also for correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics Documents Effective Inconclusive Ineffective No Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measures</head><p>We evaluated Task 1 with two different approaches: firstly as a standard ad-hoc retrieval task, i.e. we considered just topical relevance, and secondly we considered all the aspects: relevance, correctness, and credibility. The purpose of using the two approaches to evaluation was to investigate the extent to which merely evaluating retrieval quality by relevance can fail to measure other important aspects of retrieval quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Ad-hoc Retrieval Evaluation</head><p>For the standard ad-hoc retrieval evaluation, we used Average Precision (AP) <ref type="bibr" coords="6,407.56,249.41,10.52,8.74" target="#b4">[5]</ref> and Normalized Discounted Cumulated Gain (nDCG) <ref type="bibr" coords="6,186.85,261.36,10.52,8.74" target="#b8">[9]</ref> with a cut-off at 10. We used trec eval <ref type="foot" coords="6,377.80,259.79,3.97,6.12" target="#foot_4">5</ref> to compute AP and nDCG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Multi-aspect Evaluation</head><p>To evaluate the runs with respect to the three aspects, we selected two measures from Lioma et al. <ref type="bibr" coords="6,521.73,305.65,14.61,8.74" target="#b12">[13]</ref>, namely Convex Aggregating Measure (CAM) and Normalized Local Rank Error (NLRE). Note that the efficacy labels for documents and topics were not used for the evaluation, but we considered correctness instead. Both CAM and NLRE are originally defined for two aspects, topical relevance and credibility, therefore we extended the definition of those measures to deal with correctness as well. Note that, unjudged documents are considered to be Non-relevant, Not Correct, and Not Credible by both these measures.</p><p>Convex Aggregating Measure (CAM) Let r be a ranked list of documents with multi-aspect labels, Convex Aggregating Measure (CAM) is defined as the convex sum of the M scores computed with respect to each aspect individually:</p><formula xml:id="formula_0" coords="6,187.21,437.15,352.79,9.65">CAM(r) = λ rel M rel (r) + λ corr M cor (r) + λ cre M cre (r)<label>(1)</label></formula><p>where M rel , M cor , and M cre denote respectively any valid relevance, correctness, and credibility evaluation measure, and λ rel + λ corr + λ cre = 1 are non negative parameters controlling the impact of the individual relevance, correctness and credibility measures in the overall computation. We instantiated CAM with λ # = 1/3, thus assigning the same weight to each aspect. As evaluation measure we used nDCG for each individual aspect, i.e. M rel is the standard nDCG computed with respect to relevance, M cor is nDCG computed with respect to the correctness labels, while M cre is nDCG computed with respect to the credibility labels. We did not use F-1 or G-measure for credibility, as proposed by Lioma et al <ref type="bibr" coords="6,94.70,542.75,14.61,8.74" target="#b12">[13]</ref>, since we wanted to account for the rank position of credible and not credible documents as well.</p><p>Normalized Local Rank Error (NLRE) Normalized Local Rank Error (NLRE) <ref type="bibr" coords="6,450.27,568.65,15.50,8.74" target="#b12">[13]</ref> accounts for the error computed with respect to three additional ideal re-rankings independent of each other: one by relevance only, one by correctness only, and one by credibility only. Given an input ranked list r, Normalized Local Rank Error (NLRE) takes adjacent pairs of documents in r and checks for errors in r compared to the three ideal re-rankings. Let d i be the document at rank position i in the ranked list r, then let r rel [d i ] be the rank position of d i in the ideal ranked list computed with respect to relevance only, similarly r cor [d i ] is the rank position of d i in the ideal ranked list by correctness and r cre [d i ] is the rank position of d i in the ideal ranked list by credibility.</p><p>The relevance, correctness and credibility errors for d i , namely rel [d i ], cor [d i ], and cre [d i ] are defined as follows:</p><formula xml:id="formula_1" coords="7,227.88,106.44,312.12,39.54">rel [d i ] = max{0, r rel [d i ] -r rel [d i+1 ]} cor [d i ] = max{0, r cor [d i ] -r cor [d i+1 ]} cre [d i ] = max{0, r cre [d i ] -r cre [d i+1 ]} (2)</formula><p>For example, given two documents d i and d i+1 in r, the relevance error is greater than zero if the document d i is ranked after d i+1 in the ideal re-ranked list computed by relevance only.</p><p>Let n be the total number of documents in the ranked list. We define the Local Rank Error (LRE) evaluation measure as LRE = 0 if n = 1 and otherwise:</p><formula xml:id="formula_2" coords="7,154.14,212.15,385.86,30.32">LRE = n-1 i=1 1 log 2 (1 + i) (µ + rel [d i ])(ν + cor [d i ])(ξ + cre [d i ]) -µνξ<label>(3)</label></formula><p>where # [d i ] are the errors as defined in Equation <ref type="formula" coords="7,294.40,253.74,3.87,8.74">2</ref>, and µ, ν, ξ are non negative real numbers controlling how much each aspect should be penalised. As for CAM, we assigned the same weight to each aspect, thus setting µ = ν = ξ = 1/3. Finally, because Equation 3 is large for bad rankings and small for good rankings, we invert and normalize it as follows:</p><formula xml:id="formula_3" coords="7,261.77,309.37,278.23,23.22">NLRE = 1 - NLRE C LRE<label>(4)</label></formula><p>where C LRE is the normalization constant, defined as:</p><formula xml:id="formula_4" coords="7,187.02,361.41,352.98,34.24">C LRE = n 2 -1 j=0 (n -2j -1) 3 + (µ + ν + ξ)(n -2j -1) 1 + log 2 (1 + j)<label>(5)</label></formula><p>Equation 5 ensures that NLRE/C LRE ≤ 1. C LRE computes the maximum possible error attainable, i.e. rankings that produce the largest possible relevance, correctness and credibility errors. NLRE is 1 if no errors of any kind occur, since in this case LRE is 0.</p><p>MM framework Along with CAM and NLRE, we also computed nDCG@10 and AP using the MM framework for multidimensional relevance evaluation <ref type="bibr" coords="7,282.89,470.04,14.61,8.74" target="#b14">[15]</ref>, as an alternative approach to incorporate correctness and credibility in the evaluation alongside topical relevance. In the MM framework, the evaluation scores for each aspect of relevance (topical relevance, correctness, credibility) is calculated separately according to a selected evaluation measure (e.g., nDCG@10, AP), and then these are combined into a unique effectiveness measure using the weighted harmonic mean. The weighted harmonic mean is particularly sensitive to a single lower-than-average value, thus rewarding systems that are consistently more effective across all aspects of relevance. (The same intuition is used to combine recall and precision in the widely used F -measure.) Given an evaluation measure M, we apply the measure to evaluate a ranked list of documents r δ which have been labeled with respect to aspect δ (i.e., we compute M(r δ )). Then, to compute M M M , all M(r δ ) are combined for each aspect using the harmonic mean, where each aspect is weighted according to a preferential weight w δ assigned to each aspect, as it was for λ # for CAM. Formally:</p><formula xml:id="formula_5" coords="7,203.71,609.08,336.29,52.35">M M M =     n δ=1 w δ • M(r δ ) -1 n δ=1 w δ     -1 = n δ=1 w δ n δ=1 w δ M(r δ )<label>(6)</label></formula><p>We set w δ to 1/3 across all aspects, and compute the MM variants for AP (MM(AP)) and nDCG@10 (MM(nDCG@10)). Note that MM-based evaluation results were not distributed to participants at the time of writing their notebook papers.</p><p>We received 32 runs from 4 groups: University of Waterloo (UWaterlooMDS), University of Queensland (UE IELab), Chinese Academy of Sciences (ICTNET), and Bauhaus-Universität Weimar (Webis). Table <ref type="table" coords="8,501.75,108.93,4.98,8.74" target="#tab_1">3</ref> reports the runs submitted by each group. UWaterlooMDS was the only group that generated both automatic and manual runs -all other groups generated automatic runs. IElab was the only group that considered both information in the query field and in the other portion of the topic to generate runs -all other groups only used the query field. A brief summary of each group's submissions:</p><p>Webis The Webis group <ref type="bibr" coords="8,192.37,182.65,10.52,8.74" target="#b3">[4]</ref> manipulated Elasticsearch's BM25F initial ranking based on the credibility of documents' web hostnames' domains. The result is then re-ranked using an axiomatic approach that captures argumentativeness and information credibility.</p><p>ICTNET The ICTNET group <ref type="bibr" coords="8,222.95,232.46,10.52,8.74" target="#b5">[6]</ref> used Terrier's BM25 as their method of retrieval. The group also considered other retrieval methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UQ IElab</head><p>The UQ IElab group <ref type="bibr" coords="8,232.68,270.31,15.50,8.74" target="#b10">[11]</ref> employed query expansion methods using knowledge-bases (e.g. Wikipedia) to capture medical vocabulary from the topic fields. The underlying retrieval method used is Elasticsearch's BM25F.</p><p>UWaterlooMDS The UWaterlooMDS group <ref type="bibr" coords="8,288.46,320.11,10.52,8.74" target="#b1">[2]</ref> submitted manual and automatic runs. For manual runs, the group used HiCAL <ref type="bibr" coords="8,198.75,332.07,9.96,8.74" target="#b0">[1]</ref>, an open-source high-recall retrieval system, to retrieve and manually judge documents. The manually judged documents are then used to re-rank documents from a baseline BM25 ranking obtained using Ansereni<ref type="foot" coords="8,214.21,354.41,3.97,6.12" target="#foot_5">6</ref> . For automatic runs, the team built a credibility classifier trained on an annotated corpus prepared using HiCAL for finding non-credible documents. The automatic runs combined spam and credibility classifier scores to modify a BM25 baseline run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Ad-hoc Retrieval Evaluation Table <ref type="table" coords="8,249.81,426.17,4.98,8.74" target="#tab_2">4</ref> reports the AP and nDCG scores of the submitted runs when only topical relevance is considered. Measures are averaged across topics. Tables <ref type="table" coords="8,412.04,438.12,4.98,8.74" target="#tab_4">6</ref> and<ref type="table" coords="8,440.68,438.12,4.98,8.74" target="#tab_5">7</ref> report the statistical analysis of differences in effectiveness scores among the top runs from each group. Furthermore, Figure <ref type="figure" coords="8,535.02,450.08,4.98,8.74">3</ref> and Figure <ref type="figure" coords="8,122.81,462.03,4.98,8.74" target="#fig_5">4</ref> provide a per-topic analysis for AP and nDCG@10.</p><p>When AP is considered, runs from UWaterlooMDS (University of Waterloo) are consistently better than those from other groups, as shown in Figure <ref type="figure" coords="8,266.44,485.94,3.87,8.74">3</ref>. Indeed, the first, second and third best runs are respectively UWatMDSBM25 HC3, UWatMDSBM25 HC1, UWatMDSBM25 HC2. In addition, the best run from UWaterlooMDS with respect to AP is statistically significantly different from the best run of the runner up team, IElab (paired two tails t-test with Bonferroni correction, see Table <ref type="table" coords="8,308.86,521.81,3.87,8.74" target="#tab_4">6</ref>). Furthermore, the best run from UWaterlooMDS has a relative improvement of 0.2322 over the best run from IElab (UQ), 1.7314 over the best run from Webis (Bauhaus-Universität Weimar), and 110.0270 over the best run from ICTNET (Chinese Academy of Sciences).</p><p>Similar findings are obtained when nDCG@10 is considered, with UWaterlooMDS providing the best two runs for this measure, although the actual runs that achieve the highest nDCG@10 values (UWatMDS BMF S30 and UWaterMDS BM25) are different from those that obtain the highest AP values. Unlike AP, the best run from UWaterlooMDS with respect to nDCG@10 is not statistically significantly different from the best run of the runner up team, IElab (see Table <ref type="table" coords="8,248.15,617.45,3.87,8.74" target="#tab_5">7</ref>). The relative improvement of the best run from UWaterlooMDS over the best run from IElab is 0.0356, while it is 0.9516 for Webis, and 13.7493 for ICTNET.</p><p>Note that the improvement of UWaterlooMDS over the other teams is less pronounced when considering nDCG@10 compared to AP. This suggests that UWaterlooMDS best runs are better than the others at ranking relevant documents earlier across the entirety of the ranking; while when just the first 10 rank positions are considered, runs from UWaterlooMDS and IElab have comparable effectiveness. It is interesting to note that the UWaterlooMDS runs that are best for AP are not so when considering the number of relevant Multi-aspect Evaluation Table <ref type="table" coords="9,231.42,623.30,4.98,8.74" target="#tab_3">5</ref> reports the evaluation results when all aspects are considered (relevance, correctness, credibility) and CAM and NLRE are used. The measures are averaged across topics. Tables <ref type="table" coords="9,103.58,647.21,4.98,8.74" target="#tab_6">8</ref> and<ref type="table" coords="9,131.75,647.21,4.98,8.74" target="#tab_7">9</ref> report the statistical analysis of differences in effectiveness scores among the top runs from each group. Furthermore, Figure <ref type="figure" coords="9,218.53,659.17,4.98,8.74" target="#fig_6">5</ref> provides a per-topic analysis for CAM.</p><p>The findings for the multi-aspect evaluation are similar to those when only topical relevance is considered (ad-hoc evaluation). When CAM is considered, in fact, the most effective runs remain those from UWater-looMDS (University of Waterloo): the first, second and third best runs are respectively UWaterMDS BM25, UWatMDS BM25 Z, UWatMDSBM25 HC3. The best run from UWaterlooMDS has a relative improvement of 0.0517   over the best run from IElab, and 0.4516 and 63.4353 over the best run from Webis and ICTNET, respectively. Note that CAM is defined as the average of nDCG, computed separately with respect to each different aspect. If nDCG was to be computed on the whole ranking with respect to topical relevance only, the results obtained using CAM will be somehow aligned with those obtained using nDCG. Indeed the first and second best runs with respect to nDCG@1000 coincide with the first and second run with respect to CAM.</p><p>Furthermore, the scores computed with CAM are generally lower in terms of absolute values than the scores computed with nDCG@1000. This shows that considering both correctness and credibility affects  the evaluation, sometimes also by changing the global ranking of systems, as for example UWatMDSBM25 HC3 which is the third best run for CAM, while it is ranked after IELAB01 ori q for nDCG@1000. Table <ref type="table" coords="12,115.05,442.43,4.98,8.74" target="#tab_3">5</ref> reports NLRE average scores and Figure <ref type="figure" coords="12,309.06,442.43,4.98,8.74" target="#fig_7">6</ref> shows the box-plot with the per-topic analysis for NLRE. As shown in Figure <ref type="figure" coords="12,194.55,454.38,3.87,8.74" target="#fig_7">6</ref>, NLRE scores are surprisingly close to 1, the reason is detailed in Section 3.2.</p><p>With respect to NLRE, the best group is ICTNET, followed by UWaterlooMDS. Note that for NLRE the ranking of groups is extremely different from the ranking obtained with all the other measures. This is due to the definition of NLRE, which exploits the idea of computing the error between the ranking and the ideal re-ranking, which is different from both AP and nDCG.</p><p>Finally, the evaluation results obtained using the MM framework is reported in Figures <ref type="figure" coords="12,459.75,514.16,4.98,8.74" target="#fig_8">7</ref> and<ref type="figure" coords="12,485.58,514.16,4.98,8.74">8</ref> for AP and nDCG@10, respectively. The trends observed for MM(nDCG@10) are similar to those obtained with CAM: this is not surprising, as both measures are based on the interpolation of nDCG@10 computed separately for each aspect. However, we observe that the use of the harmonic mean in MM, rather than the arithmetic mean as in CAM does provide some key differences. For example, run IELAB01 ori q (the best from the IElab group) ranks 6th according to CAM, but it only ranks 11th according to MM(nDCG@10) (although  it still is the best run for that group). This difference is due to the harmonic mean punishing the fact that the run perform particularly worse than the other tuns from UWaterlooMDS in a specific relevance aspect, while it does perform better in the other two aspects -while the runs from UWaterlooMDS perform more consistently across all aspects. We also observe that when using MM(nDCG@10) for evaluation, systems are mostly indistinguishable (note the whiskers in Figure <ref type="figure" coords="13,308.09,466.34,3.87,8.74">8</ref>), except for the runs from ICTNET and two of the UWaterlooMDS runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion on Multi-aspect Evaluation Measures</head><p>As mentioned in Section 1, one of the goal of the Decision Track is to investigate possible strategies to develop new offline evaluation methods able to account for multiple aspects simultaneously. To this end, we can draw two main conclusions from Table <ref type="table" coords="13,266.61,548.48,3.87,8.74" target="#tab_3">5</ref>: (1) CAM is more reliable than NLRE to some extent, but it is closely bound to nDCG due to its definition; (2) NLRE has some limitations that prevent a proper understanding of the effectiveness performance. Table <ref type="table" coords="13,115.04,584.35,4.98,8.74" target="#tab_3">5</ref> and Figure <ref type="figure" coords="13,176.25,584.35,4.98,8.74" target="#fig_7">6</ref> clearly show two main limitations of NLRE: all the scores are close or equal to the perfect score, suggesting that the runs perform well even if the other measures show that this is not the case. That is, the measure cannot discriminate among runs: for example, all the runs from the Webis group have the same NLRE score. We identified the reasons behind these limitations in two main issues: NLRE considers the ideal ranking as a re-ranking of the input ranking; the normalization constant assumes very large values for three aspects and when the whole ranking is considered.</p><p>First, when NLRE computes the error with respect to each aspect, it does not consider an ideal ranking on the whole collection, but simply a re-ranking of the input ranking. Theoretically, the measure would work if every run sorts the whole collection or if every run retrieves all the relevant, correct, and credible documents. However, both the assumptions are often not satisfied in a real world scenario, as it is the case with this TREC task where just the first 1000 documents are considered. In other words, this means that NLRE does not make use of the recall base.</p><formula xml:id="formula_6" coords="14,169.68,285.77,187.54,67.03">ICTNETv1BM25 ICTNETv2BM25 UWatMDS_BMF_C90 UWatMDS_BMF_C95 UWatMDSBM25_HC3 UWatMDS_BMF_S30 UWatMDSBM25_HC2 UWatMDSBM25_HC1 UWatMDS_BMZBS10 UWatMDS_BM25_ZS UWatMDS_BM25_Z IELAB03_umls_d IELAB08_xWiki_d IELAB09_xCW_q IELAB05_xChv_q IELAB07_xWiki_q IELAB10_xCW_d IELAB01_ori_q IELAB06_xChv_d IELAB02_ori_d UWaterMDS_BM25</formula><p>The practical consequence of the definition of NLRE upon the ideal re-rankings, is that if a ranked list does not retrieve any relevant, correct or credible document, then LRE is equal to 0 because there are no errors in the input ranking, being all the documents correctly sorted. This effect is particularly evident from the first two lines of Table <ref type="table" coords="14,195.96,466.34,4.98,8.74" target="#tab_3">5</ref> representing the first two runs in Figure <ref type="figure" coords="14,390.27,466.34,3.87,8.74" target="#fig_7">6</ref>, where both ICTNETv1BM25 and ICTNETv2BM25 have NLRE average score equal to 1, even if CAM score is close to 0, meaning that just few relevant documents are retrieved.</p><p>Second, NLRE was originally tested on a sample of runs with two aspects, where just the first 5 documents where considered, i.e. only NLRE@5 was computed <ref type="bibr" coords="14,295.44,514.16,14.61,8.74" target="#b12">[13]</ref>. In that case the value of the normalization constant is C LRE = 20. The results reported in Table <ref type="table" coords="14,278.96,526.12,4.98,8.74" target="#tab_3">5</ref> consider the whole ranked list instead. In this case, the normalization constant, which computes the maximum possible error, is C LRE ∼ 20 * 10 9 . Therefore, LRE scores are divided by a large constant, which means that LRE/C LRE → 0 and consequently NLRE → 1. This explains why all the scores in Table <ref type="table" coords="14,260.30,561.98,4.98,8.74" target="#tab_3">5</ref> and in Figure <ref type="figure" coords="14,334.56,561.98,4.98,8.74" target="#fig_7">6</ref> are particularly close to 1 and also why the measure does not effectively discriminate among different runs, as is the case for example with the runs by Webis.</p><p>While CAM is more reliable than NLRE in evaluating the true performance of runs, it has some potential issues. Since one of the track's goals is to devise search technologies that promote correct information over incorrect information, it is reasonable to claim that a non-relevant document is preferable to a relevant document with incorrect and potentially harmful information. As previous work has shown, such documents can influence users to make incorrect or harmful decisions, and search engines should avoid presenting incorrect documents to users <ref type="bibr" coords="14,206.26,657.62,14.61,8.74" target="#b15">[16]</ref>. Unfortunately, the definition of CAM does not take such cases into consideration.</p><p>The aggregation of the same measure using different aspects (e.g. relevance, correctness, and credibility) in CAM also means that returning a non-relevant document would penalize the performance of a run in terms of both correctness and credibility, when clearly it should penalize for relevance only. Another observation of the pitfalls of CAM can be inferred from Figure <ref type="figure" coords="15,389.28,418.52,3.87,8.74" target="#fig_6">5</ref>. CAM scores UWaterlooMDS BM25 run as the best performing run, while the analysis from the UWaterlooMDS team in Abualsaud et al. <ref type="bibr" coords="15,529.48,430.47,10.52,8.74" target="#b1">[2]</ref> shows that some of their retrieval methods were able to push credible or correct documents to the top of the ranking relative to their original position in their baseline run UWaterlooMDS BM25, which may indicate that CAM may not capture performance as intended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Incomplete assessments: pool coverage</head><p>As it is common in information retrieval evaluation, not all documents retrieved by the submitted systems could be assessed by the NIST judges, due to budget constraints. The depth@k pooling method was used to select documents from the submitted retrieval runs to assess. All the submitted systems contributed to the pool. The depth k was initially set at 75, but due to time limitations this was reduced to 60 for some topics. Table <ref type="table" coords="15,133.62,560.44,9.96,8.74">10</ref> reports the average coverage of the relevance assessments for each of the submitted runs, i.e. the percentage of documents in the runs for which a relevance assessment has been recorded.</p><p>We furthermore study whether relevance assessment coverage and ad-hoc evaluation performance are correlated: for example, one may hypothesise that the top performing runs do so because most of their retrieved documents have been assessed, while the lower performing runs present more missing judgements. We found this not to be the case; specifically, both AP and nDCG@10 present no correlation with the coverage of the assessments (Pearson's correlation 0.1748 and 0.0283, respectively, and τ AP correlation <ref type="bibr" coords="15,524.51,632.17,15.50,8.74" target="#b24">[25]</ref> 0.0225 and -0.0547, respectively).</p><p>NIST ran out of assessing budget before Topic 14, and thus Topic 14 is excluded.</p><p>The TREC 2019 Decision Track received 32 submissions from 4 groups: ICTNET, IELab, UWaterlooMDS and Webis. The empirical evaluation of the submitted runs that relies on relevance based measures, i.e. AP and nDCG@10, shows that the best performing group is UWaterlooMDS, followed by IElab and Webis.</p><p>The evaluation considering all the aspects -(topical) relevance, correctness and credibility -is performed with respect to CAM and NLRE <ref type="bibr" coords="17,220.25,144.80,14.61,8.74" target="#b12">[13]</ref>. CAM is consistent with the results reported by the relevance based evaluation, with UWaterlooMDS being the leading group followed by IElab and Webis. NLRE results show a different evaluation perspective; however these results are affected by some biases due to the definition of NLRE. This further shows the necessity of well defined measures, able to account for multiple aspects simultaneously and without bias. Therefore, one of the goals of the track for next year will be to design new measures able to overcome the pitfalls shown by NLRE.</p><p>In 2020, the track will change its name to the TREC Health Misinformation Track. The organizers decided to change the name of the track to reflect its focus on health search and the effect of incorrect information on searcher decisions. The goals of the track remain the same, but we hope the new name will better communicate the track's tasks. In addition to changing the name of the track, the track organizers will change with Christina Lioma stepping down and Charles Clarke (University of Waterloo) joining.</p><p>The TREC Health Misinformation Track will have three tasks in 2020. The first task will be a repeat of the 2019 retrieval task but with a new set of search topics. The second task will be new and will be a recall task to find all health misinformation for each of the search topics. The third task, also new, will be to design a new offline measure to predict the decision making performance of users.</p><p>To support the offline measure task, following this year's assessment, we will recruit test subjects to perform a decision making task using a selection of this year runs. We anticipate that our methods will be similar to those used by Pogacar et al. <ref type="bibr" coords="17,259.55,348.04,15.50,8.74" target="#b15">[16]</ref> and <ref type="bibr" coords="17,298.37,348.04,73.83,8.74">Jimmy et al. [10]</ref>, where test subjects are given a fixed results list and must use it to help them make a decision. The fixed results list will be defined from the runs submitted to Task 1 (retrieval task) this year. Specifically, the task will be to predict the decision the user will make at the end of the search process given a query, document ranking (results list), and relevance judgments. The user will need to decide whether a treatment is helpful for a given health issue. We will evaluate groups' performance on this task based on their prediction quality. In effect, we will perform a meta-evaluation (an evaluation of evaluation methods). We anticipate that evaluation methods may use ratios of correct vs incorrect decisions made having examined the systems' result lists, the effort (amount of interaction) required by users to reach the decisions, and other inputs to make their predictions. These avenues of evaluation are currently being explored, e.g., see van der Vegt et al. <ref type="bibr" coords="17,418.45,455.63,14.61,8.74" target="#b18">[19]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,249.37,435.19,113.27,8.74;3,95.40,72.00,421.20,351.67"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example topics.</figDesc><graphic coords="3,95.40,72.00,421.20,351.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,72.00,640.59,468.00,8.74;5,72.00,652.54,101.78,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample of the original qrels as they were assessed by NIST (2a), and after the mapping to the correctness values (2b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,72.00,477.84,468.00,8.74;11,72.00,489.79,110.58,8.74"><head>8 APFigure 3 :</head><label>83</label><figDesc>Figure 3: Relevance based evaluation: box-plot of AP scores over the 50 topics. The runs are sorted by descending average score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,72.00,373.09,468.00,8.74;12,72.00,385.04,124.42,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relevance based evaluation: box-plot of nDCG@10 scores over the 50 topics. The runs are sorted by descending average score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="13,72.00,373.09,468.00,8.74;13,72.00,385.04,110.58,8.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Multi-aspect evaluation: box-plot of CAM scores over the 50 topics. The runs are sorted by descending average score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="14,72.00,373.09,468.00,8.74;14,72.00,385.04,110.58,8.74"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Multi-aspect evaluation: box-plot of NLRE scores over the 50 topics. The runs are sorted by descending average score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="15,72.00,373.09,468.00,8.74;15,72.00,385.04,124.42,8.74"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Multi-aspect evaluation: box-plot of MM(AP@1000) scores over the 50 topics. The runs are sorted by descending average score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,146.89,468.00,203.19"><head></head><label></label><figDesc>Medical Intervention Efficacy AssessmentThe assessors were asked to judge the medical intervention efficacy according to the content of the web document, and not what they believed is the correct information.</figDesc><table coords="4,161.74,146.89,288.53,155.25"><row><cell></cell><cell></cell><cell cols="2">qrels value % Judgments</cell></row><row><cell></cell><cell>Highly Relevant</cell><cell>2</cell><cell>4.50 %</cell></row><row><cell>Topical Relevance</cell><cell>Relevant</cell><cell>1</cell><cell>13.72 %</cell></row><row><cell></cell><cell>Not relevant</cell><cell>0</cell><cell>81.78 %</cell></row><row><cell></cell><cell>Effective</cell><cell>3</cell><cell>13.23 %</cell></row><row><cell></cell><cell>Inconclusive</cell><cell>2</cell><cell>3.85 %</cell></row><row><cell>Efficacy</cell><cell>Ineffective</cell><cell>1</cell><cell>0.70 %</cell></row><row><cell></cell><cell>No Information</cell><cell>0</cell><cell>0.38 %</cell></row><row><cell></cell><cell>Not Judged</cell><cell>-1 or -2</cell><cell>81.84 %</cell></row><row><cell></cell><cell>Credible</cell><cell>1</cell><cell>9.75 %</cell></row><row><cell>Credibility</cell><cell>Not Credible</cell><cell>0</cell><cell>8.44 %</cell></row><row><cell></cell><cell>Not Judged</cell><cell>-1 or -2</cell><cell>81.81 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,72.00,73.56,468.00,532.58"><head>Table 3 :</head><label>3</label><figDesc>Groups participating in TREC 2019 Decision Track and submitted runs. Runs are classified as either automatic (auto) or manual (man), and ad either based on the query field only (query), or also on other fields in the topic (other).</figDesc><table coords="9,72.00,122.98,468.00,483.16"><row><cell>Group</cell><cell>Affiliation</cell><cell cols="2"># Submissions Runs</cell><cell cols="2">Type Field Used</cell></row><row><cell>ICTNET</cell><cell>Chinese Academy of Sciences</cell><cell>2</cell><cell>ICTNETv1BM25 ICTNETv2BM25</cell><cell>auto auto</cell><cell>query query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB01 ori q auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB02 ori d auto</cell><cell>other</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB03 umls d auto</cell><cell>other</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB04 umls n auto</cell><cell>other</cell></row><row><cell>IElab</cell><cell>University of Queensland</cell><cell>10</cell><cell cols="2">IELAB05 xChv q auto IELAB06 xChv d auto</cell><cell>query other</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB07 xWiki q auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB08 xWiki d auto</cell><cell>other</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB09 xCW q auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">IELAB10 xCW d auto</cell><cell>other</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWatMDSBM25 HC1 man</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWatMDSBM25 HC2 man</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWatMDSBM25 HC3 man</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWatMDS BM25 Z auto</cell><cell>query</cell></row><row><cell cols="2">UWaterlooMDS University of Waterloo</cell><cell>10</cell><cell cols="2">UWatMDS BM25 ZS auto UWatMDS BMF C90 auto</cell><cell>query query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWatMDS BMF C95 auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWatMDS BMF S30 auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWatMDS BMZBS10 auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UWaterMDS BM25 auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell>webisMAll1</cell><cell>auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">webisMMajority1 auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell>webisMSame1</cell><cell>auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell>webisMSame2</cell><cell>auto</cell><cell>query</cell></row><row><cell>Webis</cell><cell>Bauhaus-Universität Weimar</cell><cell>10</cell><cell>webisMSame3 webisMSame4</cell><cell>auto auto</cell><cell>query query</cell></row><row><cell></cell><cell></cell><cell></cell><cell>webisMSame5</cell><cell>auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell>webisTAll1</cell><cell>auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">webisTMajority1 auto</cell><cell>query</cell></row><row><cell></cell><cell></cell><cell></cell><cell>webisTSame1</cell><cell>auto</cell><cell>query</cell></row><row><cell cols="6">documents retrieved -in fact IELAB01 ori q has a better recall, retrieving 70.66 relevant documents per</cell></row><row><cell cols="6">queries on average, compared to the 66.76 of UWaterMDSBM25 HC3. Nevertheless, the best two runs in terms of</cell></row><row><cell cols="6">number of relevant documents retrieved are from UWaterlooMDS (UWaterMDS BM25 and UWaterMDS BM25 Z</cell></row><row><cell cols="3">retrieve 72.86 and 71.92 relevant documents per query, on average).</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,72.00,87.78,210.60,484.91"><head>Table 4 :</head><label>4</label><figDesc>Evaluation results where only relevance is considered. The average score over the topics is reported. The best scores for each team are in bold. The overall first, second and third runs are denoted by , , and respectively.</figDesc><table coords="10,81.11,161.11,192.39,411.59"><row><cell>Run Name</cell><cell>AP</cell><cell>nDCG@10</cell></row><row><cell>ICTNETv1BM25</cell><cell>0.0000</cell><cell>0.0007</cell></row><row><cell>ICTNETv2BM25</cell><cell>0.0037</cell><cell>0.0339</cell></row><row><cell>IELAB01 ori q</cell><cell>0.3334</cell><cell>0.4828</cell></row><row><cell>IELAB02 ori d</cell><cell>0.2082</cell><cell>0.3525</cell></row><row><cell>IELAB03 umls d</cell><cell>0.2723</cell><cell>0.3948</cell></row><row><cell>IELAB04 umls n</cell><cell>0.2387</cell><cell>0.3565</cell></row><row><cell>IELAB05 xChv q</cell><cell>0.2642</cell><cell>0.4125</cell></row><row><cell>IELAB06 xChv d</cell><cell>0.2613</cell><cell>0.3906</cell></row><row><cell cols="2">IELAB07 xWiki q 0.3129</cell><cell>0.4651</cell></row><row><cell cols="2">IELAB08 xWiki d 0.2718</cell><cell>0.3936</cell></row><row><cell>IELAB09 xCW q</cell><cell>0.2568</cell><cell>0.4065</cell></row><row><cell>IELAB10 xCW d</cell><cell>0.2611</cell><cell>0.3898</cell></row><row><cell cols="2">UWatMDSBM25 HC1 0.4027</cell><cell>0.4504</cell></row><row><cell cols="2">UWatMDSBM25 HC2 0.3911</cell><cell>0.4504</cell></row><row><cell cols="2">UWatMDSBM25 HC3 0.4108</cell><cell>0.4504</cell></row><row><cell>UWatMDS BM25 Z</cell><cell>0.3448</cell><cell>0.4430</cell></row><row><cell cols="2">UWatMDS BM25 ZS 0.3105</cell><cell>0.4302</cell></row><row><cell cols="2">UWatMDS BMF C90 0.1562</cell><cell>0.4249</cell></row><row><cell cols="2">UWatMDS BMF C95 0.1699</cell><cell>0.4450</cell></row><row><cell cols="2">UWatMDS BMF S30 0.2855</cell><cell>0.5000</cell></row><row><cell cols="2">UWatMDS BMZBS10 0.2827</cell><cell>0.3921</cell></row><row><cell>UWaterMDS BM25</cell><cell>0.3764</cell><cell>0.4986</cell></row><row><cell>webisMAll1</cell><cell>0.1504</cell><cell>0.2562</cell></row><row><cell cols="2">webisMMajority1 0.1475</cell><cell>0.2395</cell></row><row><cell>webisMSame1</cell><cell>0.1402</cell><cell>0.1641</cell></row><row><cell>webisMSame2</cell><cell>0.1382</cell><cell>0.1510</cell></row><row><cell>webisMSame3</cell><cell>0.1417</cell><cell>0.1481</cell></row><row><cell>webisMSame4</cell><cell>0.1354</cell><cell>0.1149</cell></row><row><cell>webisMSame5</cell><cell>0.1349</cell><cell>0.1162</cell></row><row><cell>webisTAll1</cell><cell>0.1495</cell><cell>0.2512</cell></row><row><cell cols="2">webisTMajority1 0.1491</cell><cell>0.2498</cell></row><row><cell>webisTSame1</cell><cell>0.1373</cell><cell>0.1447</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,329.40,87.78,210.60,484.91"><head>Table 5 :</head><label>5</label><figDesc>Evaluation results where all the aspects are considered. The average score over the topics is reported. The best scores for each team are in bold. The overall first, second and third runs are denoted by , , and respectively.</figDesc><table coords="10,339.36,161.11,177.93,411.59"><row><cell>Run Name</cell><cell>CAM</cell><cell>NLRE</cell></row><row><cell>ICTNETv1BM25</cell><cell>0.0001</cell><cell>1.0000</cell></row><row><cell>ICTNETv2BM25</cell><cell>0.0085</cell><cell>1.0000</cell></row><row><cell>IELAB01 ori q</cell><cell>0.5208</cell><cell>0.9960</cell></row><row><cell>IELAB02 ori d</cell><cell>0.3872</cell><cell>0.9959</cell></row><row><cell>IELAB03 umls d</cell><cell>0.4547</cell><cell>0.9966</cell></row><row><cell>IELAB04 umls n</cell><cell>0.4234</cell><cell>0.9957</cell></row><row><cell>IELAB05 xChv q</cell><cell>0.4665</cell><cell>0.9963</cell></row><row><cell>IELAB06 xChv d</cell><cell>0.4493</cell><cell>0.9959</cell></row><row><cell cols="2">IELAB07 xWiki q 0.5084</cell><cell>0.9962</cell></row><row><cell cols="2">IELAB08 xWiki d 0.4547</cell><cell>0.9963</cell></row><row><cell>IELAB09 xCW q</cell><cell>0.4621</cell><cell>0.9963</cell></row><row><cell>IELAB10 xCW d</cell><cell>0.4491</cell><cell>0.9961</cell></row><row><cell cols="2">UWatMDSBM25 HC1 0.5360</cell><cell>0.9972</cell></row><row><cell cols="2">UWatMDSBM25 HC2 0.5336</cell><cell>0.9976</cell></row><row><cell cols="2">UWatMDSBM25 HC3 0.5386</cell><cell>0.9983</cell></row><row><cell>UWatMDS BM25 Z</cell><cell>0.5467</cell><cell>0.9968</cell></row><row><cell cols="2">UWatMDS BM25 ZS 0.5096</cell><cell>0.9969</cell></row><row><cell cols="2">UWatMDS BMF C90 0.3089</cell><cell>0.9991</cell></row><row><cell cols="2">UWatMDS BMF C95 0.3339</cell><cell>0.9991</cell></row><row><cell cols="2">UWatMDS BMF S30 0.4560</cell><cell>0.9978</cell></row><row><cell cols="2">UWatMDS BMZBS10 0.4918</cell><cell>0.9971</cell></row><row><cell>UWaterMDS BM25</cell><cell>0.5477</cell><cell>0.9958</cell></row><row><cell>webisMAll1</cell><cell>0.3773</cell><cell>0.9940</cell></row><row><cell cols="2">webisMMajority1 0.3711</cell><cell>0.9940</cell></row><row><cell>webisMSame1</cell><cell>0.3479</cell><cell>0.9940</cell></row><row><cell>webisMSame2</cell><cell>0.3468</cell><cell>0.9940</cell></row><row><cell>webisMSame3</cell><cell>0.3518</cell><cell>0.9940</cell></row><row><cell>webisMSame4</cell><cell>0.3419</cell><cell>0.9940</cell></row><row><cell>webisMSame5</cell><cell>0.3436</cell><cell>0.9940</cell></row><row><cell>webisTAll1</cell><cell>0.3758</cell><cell>0.9940</cell></row><row><cell cols="2">webisTMajority1 0.3727</cell><cell>0.9940</cell></row><row><cell>webisTSame1</cell><cell>0.3462</cell><cell>0.9940</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,72.00,614.55,467.99,87.06"><head>Table 6 :</head><label>6</label><figDesc>Statistical significance analysis (p-values obtained with paired two tails t-test, with Bonferroni correction) on AP scores for the best runs submitted by each team.</figDesc><table coords="10,118.50,652.01,371.67,49.61"><row><cell></cell><cell cols="3">ICTNETv2BM25 IELAB01 ori q UWatMDSBM25 HC3</cell></row><row><cell>IELAB01 ori q</cell><cell>&lt;2e-16</cell><cell>-</cell><cell>-</cell></row><row><cell>UWatMDSBM25 HC3</cell><cell>&lt;2e-16</cell><cell>0.0012</cell><cell>-</cell></row><row><cell>webisMAll1</cell><cell>5.6e-11</cell><cell>3.2e-15</cell><cell>&lt;2e-16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,72.00,73.56,468.00,383.99"><head>Table 7 :</head><label>7</label><figDesc>Statistical significance analysis (p-values obtained with paired two tailed t-test, with Bonferroni correction) on nDCG@10 scores for the best runs submitted by each team.</figDesc><table coords="11,122.53,111.02,363.63,49.60"><row><cell></cell><cell cols="3">ICTNETv2BM25 IELAB01 ori q UWatMDS BMF S30</cell></row><row><cell>IELAB01 ori q</cell><cell>3.0e-15</cell><cell>-</cell><cell>-</cell></row><row><cell>UWatMDS BMF S30</cell><cell>&lt; 2e-16</cell><cell>1</cell><cell>-</cell></row><row><cell>webisMAll1</cell><cell>1.0e-07</cell><cell>1.9e-07</cell><cell>5.4e-09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,72.00,625.23,468.00,87.06"><head>Table 8 :</head><label>8</label><figDesc>Statistical significance analysis (p-values obtained with paired two tailed t-test, with Bonferroni correction) on CAM scores for the best runs submitted by each team.</figDesc><table coords="11,131.48,662.69,349.04,49.61"><row><cell></cell><cell cols="3">ICTNETv2BM25 IELAB01 ori q UWaterMDS BM25</cell></row><row><cell>IELAB01 ori q</cell><cell>&lt; 2e-16</cell><cell>-</cell><cell>-</cell></row><row><cell>UWaterMDS BM25</cell><cell>&lt; 2e-16</cell><cell>0.082</cell><cell>-</cell></row><row><cell>webisMAll1</cell><cell>&lt; 2e-16</cell><cell>3.9e-10</cell><cell>5.1e-11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,72.00,604.72,489.08,110.97"><head>Table 9 :</head><label>9</label><figDesc>Statistical significance analysis (p-values obtained with paired two tailed t-test, with Bonferroni correction) on NLRE scores for the best runs submitted by each team. Note, we ignored run ICTNETv2BM25 as it was equivalent to run ICTNETv1BM25, reported here.</figDesc><table coords="12,77.98,654.13,483.10,61.56"><row><cell></cell><cell cols="4">ICTNETv1BM25 IELAB03 umls d UWatMDS BMF C90 UWatMDS BMF C95</cell></row><row><cell>IELAB03 umls d</cell><cell>0.0076</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UWatMDS BMF C90</cell><cell>0.1589</cell><cell>0.0110</cell><cell>-</cell><cell>-</cell></row><row><cell>UWatMDS BMF C95</cell><cell>0.0846</cell><cell>0.0182</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>webisMAll1</cell><cell>0.0055</cell><cell>0.2437</cell><cell>0.0108</cell><cell>0.0101</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,87.24,674.81,136.97,6.99"><p>https://lemurproject.org/clueweb12/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,87.24,684.31,452.76,6.99;2,72.00,693.77,86.15,6.99"><p>Systematic reviews summarise the available scientific evidence towards an hypothesis, e.g. the effectiveness of a treatment for a specific condition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,87.24,703.28,128.74,6.99"><p>https://www.cochranelibrary.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,87.24,679.13,452.75,6.99;4,72.00,688.59,51.99,6.99"><p>https://static.googleusercontent.com/media/guidelines.raterhub.com/en/searchqualityevaluatorguidelines.pdf Last visited: October 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,87.24,671.26,115.13,6.99"><p>https://trec.nist.gov/trec eval/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,87.24,708.11,139.88,6.99"><p>https://github.com/castorini/anserini</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>Thanks to <rs type="person">Fuat Beylunioglu</rs> and <rs type="person">Lucas Chaves Lima</rs> for their help. This work was supported in part by <rs type="funder">AMAOS (Advanced Machine Learning for Automatic Omni-Channel Support)</rs>, funded by <rs type="funder">Innovationsfonden, Denmark</rs>, and in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs> (Grant <rs type="grantNumber">GPIN-2014-03642</rs>). <rs type="person">Guido Zuccon</rs> is the recipient of an <rs type="funder">Australian Research Council</rs> <rs type="grantName">DECRA Research Fellowship</rs> (<rs type="grantNumber">DE180101579</rs>) and a <rs type="funder">Google Faculty Award</rs>; both partially supported this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_e3vR59c">
					<idno type="grant-number">GPIN-2014-03642</idno>
				</org>
				<org type="funding" xml:id="_XmdaqxT">
					<idno type="grant-number">DE180101579</idno>
					<orgName type="grant-name">DECRA Research Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="17,92.48,567.69,447.52,8.74;17,92.48,579.64,447.52,8.74;17,92.48,591.60,447.52,8.74;17,92.48,603.55,447.52,8.74;17,92.48,616.22,206.75,8.30" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,532.53,567.69,7.47,8.74;17,92.48,579.64,166.65,8.74">A system for efficient high-recall retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210176</idno>
		<ptr target="https://doi.org/10.1145/3209978.3210176" />
	</analytic>
	<monogr>
		<title level="m" coord="17,278.63,579.64,261.37,8.74;17,92.48,591.60,234.87,8.74">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1317" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,92.48,635.43,447.52,8.74;17,92.48,647.39,164.37,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,410.20,635.43,129.80,8.74;17,92.48,647.39,85.87,8.74">UWaterlooMDS at the TREC 2019 Decision Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Beylunioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Duimering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,199.79,647.39,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,92.48,667.31,447.52,8.74;17,92.48,679.27,447.52,8.74;17,92.48,691.22,439.51,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,198.77,667.31,341.23,8.74;17,92.48,679.27,192.13,8.74">The Cochrane Collaboration: Preparing, Maintaining, and Disseminating Systematic Reviews of the Effects of Health Care</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rennie</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.1995.03530240045039</idno>
		<ptr target="https://doi.org/10.1001/jama.1995.03530240045039" />
	</analytic>
	<monogr>
		<title level="j" coord="17,292.31,679.27,25.39,8.74">JAMA</title>
		<idno type="ISSN">0098-7484</idno>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="1935" to="1938" />
			<date type="published" when="1995">12 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,75.16,447.52,8.74;18,92.48,87.11,141.12,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,441.42,75.16,98.58,8.74;18,92.48,87.11,62.62,8.74">Webis at TREC 2019: Decision Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kasturia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,176.54,87.11,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,107.04,447.52,8.74;18,92.48,118.99,268.21,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,257.81,107.04,124.01,8.74">Retrieval System Evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,406.60,107.04,133.40,8.74;18,92.48,118.99,92.98,8.74">Experiment and Evaluation in Information Retrieval</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="53" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,138.92,440.64,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,291.37,138.92,163.25,8.74">ICTNET at Trec 2019 Decision Track</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hanzhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,476.06,138.92,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,158.84,447.52,8.74;18,92.48,170.80,447.52,8.74;18,92.48,182.75,447.52,9.02;18,92.48,195.43,81.22,8.30" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3274784.3274788</idno>
		<ptr target="http://doi.acm.org/10.1145/3274784.3274788" />
		<title level="m" coord="18,292.26,158.84,247.73,8.74;18,92.48,170.80,281.60,8.74;18,448.24,170.80,57.83,8.74">Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="34" to="90" />
		</imprint>
	</monogr>
	<note>SIGIR Forum</note>
</biblStruct>

<biblStruct coords="18,92.48,214.64,447.52,8.74;18,92.48,226.59,113.10,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="18,229.46,214.64,260.75,8.74">Cochrane Handbook for Systematic Reviews of Interventions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P T</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,246.52,447.52,8.74;18,92.48,258.47,447.52,8.74;18,92.48,271.14,211.98,8.30" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,227.32,246.52,222.61,8.74">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582418</idno>
		<ptr target="http://doi.acm.org/10.1145/582415.582418" />
	</analytic>
	<monogr>
		<title level="j" coord="18,458.08,246.52,81.92,8.74;18,92.48,258.47,104.74,8.74">ACM Transactions on Information Systems</title>
		<idno type="ISSN">1046-8188</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,99.47,290.35,440.53,8.74;18,92.48,302.31,447.52,8.74;18,92.48,314.26,189.14,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,341.05,290.35,178.48,8.74">Health cards for consumer health search</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Demartini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,92.48,302.31,447.52,8.74;18,92.48,314.26,71.19,8.74">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,334.19,358.15,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,196.72,334.19,175.40,8.74">UQ IElab at TREC 2019 Decision Track</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,393.56,334.19,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,354.11,447.52,8.74;18,92.48,366.07,153.79,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="18,381.86,354.11,158.15,8.74;18,92.48,366.07,77.34,8.74">Overview of the clef 2018 consumer health search task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R M</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,190.85,366.07,23.53,8.74">CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,385.99,447.52,8.74;18,92.48,397.95,447.52,8.74;18,92.48,409.90,447.52,8.74;18,92.48,421.86,364.12,9.02" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,297.22,385.99,242.78,8.74;18,92.48,397.95,56.18,8.74">Evaluation Measures for Relevance and Credibility in Ranked Lists</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3121050.3121072</idno>
		<ptr target="http://doi.acm.org/10.1145/3121050.3121072" />
	</analytic>
	<monogr>
		<title level="m" coord="18,171.37,397.95,368.63,8.74;18,92.48,409.90,37.33,8.74">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,441.78,447.52,8.74;18,92.48,453.74,447.52,8.74;18,92.48,465.69,334.03,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="18,477.04,441.78,62.95,8.74;18,92.48,453.74,222.18,8.74">Clef 2017 task overview: the ir task at the ehealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">P</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,337.62,453.74,202.38,8.74;18,92.48,465.69,250.46,8.74">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,485.62,447.52,8.74;18,92.48,497.57,447.52,8.74;18,92.48,509.53,395.83,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="18,282.37,485.62,257.63,8.74;18,92.48,497.57,78.50,8.74">MM: A new Framework for Multidimensional Evaluation of Search Engines</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,198.74,497.57,341.26,8.74;18,92.48,509.53,160.69,8.74">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1699" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,529.45,447.52,8.74;18,92.48,541.41,447.52,8.74;18,92.48,553.36,447.53,8.74;18,92.48,565.32,447.52,8.74;18,92.48,577.99,222.44,8.30" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="18,381.00,529.45,159.00,8.74;18,92.48,541.41,361.98,8.74">The Positive and Negative Influence of Search Results on People&apos;s Decisions About the Efficacy of Medical Treatments</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Pogacar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghenai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3121050.3121074</idno>
		<ptr target="http://doi.acm.org/10.1145/3121050.3121074" />
	</analytic>
	<monogr>
		<title level="m" coord="18,478.15,541.41,61.85,8.74;18,92.48,553.36,392.92,8.74">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR &apos;17</title>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,597.20,447.52,8.74;18,92.48,609.16,298.65,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="18,275.16,597.20,264.84,8.74;18,92.48,609.16,79.44,8.74">Chis@ fire: Overview of the shared task on consumer health information search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mannarswamy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,193.04,609.16,99.47,8.74">FIRE (Working Notes)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,629.08,447.52,8.74;18,92.48,641.04,372.12,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="18,378.41,629.08,161.59,8.74;18,92.48,641.04,129.33,8.74">Enhancing web search in the medical domain via query clarification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,230.06,641.04,129.32,8.74">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="149" to="173" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,92.48,660.96,447.52,8.74;18,92.48,672.92,447.52,8.74;18,92.48,684.87,351.42,9.02" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,380.70,660.96,159.30,8.74;18,92.48,672.92,178.61,8.74">A Task Completion Framework to Support Single-Interaction IR Research</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Der Vegt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bruza</surname></persName>
		</author>
		<idno type="DOI">10.1108/JD-09-2017-0128</idno>
		<ptr target="https://doi.org/10.1108/JD-09-2017-0128" />
	</analytic>
	<monogr>
		<title level="j" coord="18,288.80,672.92,117.44,8.74">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="308" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,92.48,75.16,447.52,8.74;19,92.48,87.11,447.52,8.74;19,92.48,99.07,78.59,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="19,372.17,75.16,167.83,8.74;19,92.48,87.11,331.52,8.74">Impact of a search engine on clinical decisions under time and system effectiveness constraints: Research protocol</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Der Vegt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Deacon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,432.56,87.11,103.41,8.74">JMIR research protocols</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">12803</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,92.48,118.99,447.53,8.74;19,92.48,130.95,447.52,8.74;19,92.48,142.90,447.52,8.74;19,92.48,155.58,211.98,8.30" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,167.99,118.99,184.42,8.74">Evaluation by Highly Relevant Documents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.383963</idno>
		<ptr target="http://doi.acm.org/10.1145/383952.383963" />
	</analytic>
	<monogr>
		<title level="m" coord="19,375.32,118.99,164.68,8.74;19,92.48,130.95,443.65,8.74">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,92.48,174.78,447.52,8.74;19,92.48,186.74,447.52,8.74;19,92.48,198.69,89.11,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="19,318.01,174.78,221.99,8.74;19,92.48,186.74,184.63,8.74">Do online information retrieval systems help experienced clinicians answer clinical questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">I</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Coiera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Gosling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,287.09,186.74,248.52,8.74">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,92.48,218.62,447.52,8.74;19,92.48,230.58,431.57,9.02" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="19,223.49,218.62,155.67,8.74">Content bias in online health search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2663355</idno>
		<ptr target="http://doi.acm.org/10.1145/2663355" />
	</analytic>
	<monogr>
		<title level="j" coord="19,387.88,218.62,74.67,8.74">ACM Trans. Web</title>
		<idno type="ISSN">1559-1131</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,92.48,250.50,447.52,8.74;19,92.48,262.46,447.52,8.74;19,92.48,274.41,384.60,9.02" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="19,228.76,250.50,311.24,8.74;19,92.48,262.46,26.83,8.74">Cyberchondria: Studies of the Escalation of Medical Concerns in Web Search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/1629096.1629101</idno>
		<ptr target="http://doi.acm.org/10.1145/1629096.1629101" />
	</analytic>
	<monogr>
		<title level="j" coord="19,129.03,262.46,225.46,8.74">ACM Transactions on Information Systems (TOIS)</title>
		<idno type="ISSN">1046-8188</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,92.48,294.34,447.52,8.74;19,92.48,306.29,447.52,8.74;19,92.48,318.25,229.26,8.74" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="19,282.44,294.34,253.71,8.74">A new rank correlation coefficient for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,105.30,306.29,434.70,8.74;19,92.48,318.25,101.66,8.74">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="587" to="594" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
