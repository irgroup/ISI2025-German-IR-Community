<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,61.53,84.23,488.42,15.44;1,147.55,104.15,316.19,15.44">TREC CAsT 2022: Going Beyond User Ask and System Retrieve with Initiative and Response Generation</title>
				<funder ref="#_bEu3GTZ">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,73.76,133.09,68.61,11.96"><forename type="first">Paul</forename><surname>Owoicho</surname></persName>
							<email>p.owoicho.1@research.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,152.56,133.09,68.19,11.96"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
							<email>jeff.dalton@glasgow.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.95,133.09,117.82,11.96"><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
							<email>m.aliannejadi@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.96,133.09,71.37,11.96"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
							<email>leif.azzopardi@strath.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Strathclyde</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,440.53,133.09,92.82,11.96"><forename type="first">Johanne</forename><forename type="middle">R</forename><surname>Trippas</surname></persName>
							<email>j.trippas@rmit.edu.au</email>
							<affiliation key="aff3">
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>5 Amazon</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.99,147.03,94.45,11.96"><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
						</author>
						<title level="a" type="main" coord="1,61.53,84.23,488.42,15.44;1,147.55,104.15,316.19,15.44">TREC CAsT 2022: Going Beyond User Ask and System Retrieve with Initiative and Response Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B53EA0B1197D731E99BAD95F206A113</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The fourth year of the TREC Conversational Assistance Track (CAsT) continues to focus on evaluating Conversational Passage Ranking (ConvPR) for information seeking but with several new additions to improve the realism of the task and to improve our understanding of conversational search.</p><p>This year the topics were more realistic and dynamic involving branching that created different conversational paths and trajectories through the topic space. These conversational paths motivated introducing and piloting of new evaluation metrics that go beyond independently evaluated turn-based measures to metrics that consider the flow of conversation over the sequences of turns.</p><p>Next, the track introduced a response generation sub-task, with retrieved passages used as provenance, to explore the use of generating responses from one or more passages, including both extractive and generative approaches. The response evaluation includes elements of naturalness, conciseness, as well as relevance. The track also included a mixed initiative sub-task, where given the prior conversational context, the task was to generate clarifying or follow-up questions to direct the conversation in a relevant direction. And, for each conversation turn, the system may return a response or ask a question. The system may select one or more questions to ask the user for each turn in a conversation. This also motivated new evaluation measures to be developed.</p><p>In sum, the tasks and topics for CAsT were designed to be more challenging and a step closer to a fully conversational systemin line with the developments in core techniques such as conversational query rewriting (CQR), conversational query expansion (CQE), and the continued shift towards the use of dense retrieval and learned sparse representations in combination with hybrid approaches and multi-stage rankers leveraging large pre-trained language models.</p><p>The core task is for the system to return a response after every user utterance. Each system response may be a simple passage, but it may also be an extracted or generated summary from one or more passage results. All responses must have at least one passage called 'provenance' from the collection because the primary task evaluation remains passage/provenance ranking. Similar to previous years, the system may use all previous turns in the conversation as context; this is all parents in the conversational topic tree.</p><p>To support richer and more realistic conversations, the topic structure changed to be a tree that consists of multiple overlapping No, what are some popular dishes? --conversations on the same topic rather than a simple (or linear) series of user-system turns (in past years). An example topic with a tree structure with parent utterance links is shown in Table <ref type="table" coords="1,553.32,513.72,3.13,8.97" target="#tab_0">1</ref>.</p><p>The topic tree structure was introduced as different paths evolve based on the results and mixed-initiative that is experienced -and so they provide a greater breadth/coverage of the topic as a result.</p><p>Another change from previous CAsT editions is that mixedinitiative responses are included in trajectories. These turns provide the system with a chance to ask the user a question to 1) clarify the information need, 2) ask for feedback, or 3) elicit the task. This new addition aims to make the track more interactive and realistic. This led to the MI sub-task. Participants had the option to submit mixed-initiative (MI) utterances at every point of the conversation and receive a user response. This was organized as a separate phase of the track, but the outcome of this sub-task could be used in the main phase. This represents a first step for the track beyond "user ask, system reply", albeit on predefined fixed trajectories.</p><p>Mixed initiative is incorporated into the canonical system responses. As a result, some canonical turns have the system ask a question where the user (topic creator) responds appropriately in the next turn. This results in groups of related turns that share the same information need. It also provides flexibility to allow some user utterances to be vague, under-specified, etc... For example, starting the conversation with a vaguely defined information need. We provide annotations on these relationships. We only create judgments for and evaluate system effectiveness on the subset of turns that have been clarified to be unambiguous and correctly specified. These innovations allow us to create more natural conversations with richer discourse structure while also retaining reusability.</p><p>The use of the new sub-tree structure allows flexibility and more realistic conversations by supporting multiple paths on the same topic. For example, on a trajectory with a relevant result and one with an irrelevant or partially relevant result. It allows conversations of a greater variety of depth and breadth.</p><p>Similar to previous years, most 2022 topics are based on real user needs from information-seeking sessions in Bing sessions <ref type="bibr" coords="2,282.97,251.31,9.34,8.97" target="#b5">[6]</ref>. The organizers also added a few more diverse topics to expand the themes. The organizers manually reviewed and filtered sessions to ensure they had meaningful trajectories that are manually rewritten to make them conversational. The topics reflect diverse types of exploratory information needs while also being grounded in real information needs with content in the target collection. We detail topic construction in Section 2.1.</p><p>Year four continued to have strong participation from more than a dozen teams worldwide. There remains a gap in effectiveness between manual and automatic systems, although this is shrinking, particularly in areas of recall.</p><p>We see CAsT continue to evolve as systems become more capable. This year presented a shift towards interaction with user responses such as clarification and feedback being used for the first time. It also presented the first opportunity to test response generation approaches that leverage retrieved passage sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK, DATA, AND RESOURCES</head><p>In this section, we describe data and resources used in both the main task as well as the mixed-initiative sub-task. The MI sub-task builds on the main task using the same collection and topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Main Task</head><p>A key change to the main task involved how systems returned responses (rather than just returning passage ids). A response is a text suitable for showing to a user. It should be fluent, satisfy their information need, and not contain extraneous or redundant information. A response is limited to a maximum of 250 words (as measured by spaCy v3.3), but should vary depending on an appropriate queryresponse. We evaluated the quality of the responses using human judgments (described later in this section). The system responses must be grounded in canonical passages from a document corpus consisting of MS MARCO V2 <ref type="bibr" coords="2,162.58,622.52,9.39,8.97" target="#b2">[3]</ref>, Wikipedia -the KILT dump <ref type="bibr" coords="2,282.50,622.52,9.39,8.97" target="#b4">[5]</ref>, and news from the Washington Post V4 collection.</p><p>For compatibility, the main task remains provenance passage ranking, ConvPR. The final provenance "run" takes the provenance passages for all responses in response order. For compatibility with previous years, the first 1000 provenances for each turn are used. Because a response may have multiple source passages, the score of passages in the provenance list for a response is used to order passages in descending order. If a source passage occurs in multiple responses, it will be ranked by its first response.</p><p>CAsT 2022 has 18 information needs (topics) with an average length of 11.39 user utterances with an average of 2.7 sub-topics. There are a total of 205 user utterances, including vague, ambiguous, or user responses to system questions. In comparison, the CAsT 2021 topics are slightly shorter, with an average of 9.2 utterances per topic. A major difference from previous years is that topics in CAsT 2022 follow a "tree" structure with distinct conversational paths. Each topic starts off with a common query, (i.e. "I remember Glasgow hosting COP26 last year, but unfortunately, I was out of the loop. What was it about?") but branches off at various points in the conversation as the topic unfolds. Topics have a maximum of nine distinct conversational paths and a minimum of one. A subset of Topic 140 is shown in Table <ref type="table" coords="2,416.87,240.35,3.01,8.97" target="#tab_0">1</ref>. Note that User and System utterances are decoupled to support branches with multiple different system responses to a single user utterance. The turn structure is encoded by the parent relationship.</p><p>Topic Creation. The high-level method for constructing and filtering topics remains the same as in previous years. Information needs are based on long sessions from a commercial search engine. Once sessions are filtered, the organizer interacts with the iCAsT system described in <ref type="bibr" coords="2,392.87,328.02,9.45,8.97" target="#b3">[4]</ref>, to select a set of passages to synthesize a natural language system response. The system response is a humanwritten summary of one or more passages. This response takes the place of the canonical passage response from Year 3. The topics include both the human summary as well as passage provenance links that ground the response.</p><p>Collection. The current iteration updates the document collection to include MS MARCO V2 documents, keeping the KILT-based Wikipedia dump and Washington Post V4 collections from year 3. This update brings the total number of documents to about 17 million. Due to the size and nature of the collection, we observed that several documents (within and across each collection) contained the same or similar web page hosted on different URLs. We de-duplicated the entire collection by grouping documents from the same website with the same title and cosine similarity greater than 90%. The longest document in each group was treated as the original and included in the final collection. This excluded roughly 1 million documents.</p><p>As in year three, we split each document from the de-duplicated collection into canonical passages of at most 250 words using version 3.3.0 of the spaCy toolkit and the en_core_web_sm-3.3.0 model. We provided these canonical passage splits to participants together with python based scripts to allow participants to process the collection themselves as well as MD5 hashes to verify chunking correctness.</p><p>Generated Baseline Runs. We generated two baseline runs for the main task this year. For NLU the automatic run -BM25_-T5_BART_automatic uses a query rewriter trained on CANARD with k-context of both query and results, see the 2021 overview for details. The manual run BM25_T5_BART_manual uses the manual rewrites provided in the topics file. For both the rest of the processing is fixed. It is a multi-stage pipeline with (1) document retrieval, (2) passage segmentation, and (3) passage reranking with a fine-tuned language model. For document retrieval, we use a BM25 (K1=4.46, b=0.82) from the Pyserini toolkit. The segmenter used the standard spaCy segmentation described above. The first 1000 passages (in document rank order) were re-ranked using a T5 ranker trained on the MS MARCO dataset, available on Hugging-Face<ref type="foot" coords="3,71.90,117.82,3.38,7.27" target="#foot_1">1</ref> . The top 1000 ranked passages form the provenance run. For response generation, the baseline generated one response using the top three ranked passages and a standard off-the-shelf abstractive summarizer based on BART <ref type="foot" coords="3,156.08,150.69,3.38,7.27" target="#foot_2">2</ref> .</p><p>Relevance Assessments The main provenance judgment task was performed by NIST assessors following the scale and methodology of previous years. They only assessed the source passages, not the generated responses. The total pool up to depth 20 contained 49,878 passages, of which 43,027 passages were judged. Only 17 of the 18 topics are judged; topic 134 is not assessed. Additionally, two turns were filtered out (139_2-5 140_4-4) because they did not contain sufficient relevant results (less than 3 results with the relevance of at least 2). Of the total of 205 user turns, there were judgments for 163 user turns. Utterances that were user responses to system questions were not judged. Additionally, vague and unspecified turns were not included, only the clarified versions. This supports turn-level relevance assessment and focuses provenance assessment resources on well-specified turns.</p><p>There was an average of 258 judged results per turn, with an average of 76 at least partially relevant. There was a total of 12,318 at least partially relevant results. There are 5053 1s, 3297 2s, 2129 3s, and 1839 4s. Note that, like in previous years, we use a threshold of 2 for binary relevance, with 1 being quite marginal. The relevant distribution by collection is 10,775 MARCO V2, 999 KILT, and 544 WAPO.</p><p>Response Quality Judgments. To label the quality of responses, we employed crowd workers. We sourced annotations for relevance, naturalness, and conciseness for the top response across all turns from each submission to the main task. Additionally, we released a mapping of each unique response to a Response ID to support reusable evaluation with the crowd-sourced judgments using any standard IR evaluation toolkit. The response bank contains 2314 unique responses and we released 2479 relevance judgments across all (judged) turns.</p><p>We collected these judgments from crowd workers through the Prolific platform. We asked between 5 to 10 workers to assess the responses from the response pools for all turns in one topic. For each turn-response pair, workers see the "conversation so far" as context to make their judgments. The dimensions and rubric for judgments are defined below. Note that the relevance criteria were on a simplified graded scale that differs from those used by the NIST assessors for provenance assessment.</p><p>Relevance: Does the response follow on from previous utterances?</p><p>• 0. Not Relevant -The response does not follow the previous utterances; seems to be completely random to the current conversation; seems to be a completely different conversation. • 1. Partially Relevant -The response is partially off-topic; may be vaguely related, but too divergent from the conversation.</p><p>• 2. Relevant -The response follows from previous turns, but it is not entirely clear why the response is being presented. • 3. Highly Relevant -The response directly follows and it is clear why the response is being presented.</p><p>Naturalness: Does the response sound human-like?</p><p>• 0. No -The response does not sound like something a human would say given the conversation. • 1. Somewhat -The response is a bit human-like. The response is somewhat understandable but may not be entirely fluent and natural. • 2. Yes (but not completely) -The response is almost humanlike. The response is well-formed but is not natural. • 3. Yes -The response is very human-like and fluent.</p><p>Conciseness: Does the response adequately follow the previous utterances in a concise manner?</p><p>• 0. No -The response is too wordy or too short. The response may also contain lots of irrelevant content or no relevant information at all. • 1. Somewhat -The response is a bit wordy and does not adequately address the user's utterance (i.e the response is longer than needed). • 2: Yes (but not completely) -The response is brief but not comprehensive (i.e does not adequately address the user's utterance/query or properly follow on from the conversation). • 3: Yes -The response is brief but comprehensive (the response was concise and to the point without too much/little other information).</p><p>To eliminate low-quality workers, we screened the responses for clearly low-quality responses for each topic -where the judgments should be 0 or 1 for each question. These were then used as a check to filter out workers who marked such responses differently from what was expected. Specifically, where a response should be labeled 0 or 1 for relevance, but receives a 3 from a worker then all the worker's judgments for that topic are discarded.</p><p>Based on these crowd-sourced judgments, we released files containing the response pool, response bank, and annotations for the three criteria. We determined the final judgment for each response based on a majority vote among the crowd workers. Where there is no majority, the final judgment is the average across all workers. Note that despite the quality filtering process described earlier, each topic had a minimum of three judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MI Sub-task</head><p>Teams are able to test their systems in terms of the generation or selection of MI utterances. At each point in the conversation, a system could pose a clarifying question and receive an answer from a human annotator. We provided a question bank to the teams from which they could select the MI utterance. Given the humanin-the-loop nature of this sub-task, the teams could also opt for a generative model. We collected the MI sub-task submissions a few days prior to the deadline of the main task and crowd-source the responses to the top-ranked utterance of all the submissions. Each team's responses, together with baseline responses were then communicated to the teams one week before the main deadline.</p><p>Teams could submit runs to the main task, using the MI utterances and responses.</p><p>Question Bank. Following <ref type="bibr" coords="4,166.50,108.84,9.29,8.97" target="#b0">[1,</ref><ref type="bibr" coords="4,178.03,108.84,6.20,8.97" target="#b1">2]</ref>, we collected a question bank on the topics in the collection. The idea was to have a set of humangenerated questions on each topic, covering various aspects of the topic. To this end, we set up a crowd-sourcing task where we provided the workers with the topic description and asked them to input the query into a commercial search engine of their choice. We then instructed them to scan the first two pages of the results page, as well as query auto-complete and suggestions to get an idea of the different aspects or facets of the query. Finally, we asked each worker to input 6 questions per topic, each focusing on a different aspect. The released question bank contains 4,496 questions.</p><p>Crowd-sourced MI Responses. We designed a consequent crowd-sourcing task to collect the user responses to the MI utterances submitted by the teams. In this task, we provided the crowdworkers with the conversation context up to the point where the MI utterance was posed and instructed workers to respond to the MI utterance as if they are a part of the conversation. To give the worker more information about the topic and the user's information needs, we also revealed three turns of the conversation in the future. Therefore, the worker uses some pieces of information that could have been revealed in the future to answer the current question. The intuition behind this decision is based on the fact that MI utterances can be used to clarify or elicit different aspects of a topic that could be naturally revealed as the conversation progresses in a non-mixed-initiative manner. We only revealed three future turns to mimic the situation where an information need evolves within a conversation as the user and system interact.</p><p>Baselines. We released 4 baseline systems for the MI Sub-task that used various ranking and generative methods. These are described below:</p><p>• miniLM-bert-mi: This is a two-step system that uses the distilled miniLM model to generate a candidate set of questions from the question bank (using sentence similarity), and re-ranked them with a BERT model fine-tuned for pair-wise ranking on the QULAC dataset. We performed training and inference with the SentenceTransformers library. • bm25-baseline: This system used a BM25 function to retrieve candidate questions from the question bank. • T5-*: The T5-based systems were trained on the ClariQ dataset for clarification question generation. This included T5-raw variant that used the raw utterance at each turn for generation and the T5-rewrite variant that used the automatic rewritten utterance as input.</p><p>Clarification Question Judgments. Following the same methodology for collecting response judgments, we collected clarification question judgments against the following dimensions and criteria:</p><p>Relevance: Does the question (logically) follow on from previous utterances?</p><p>• 0. Not Relevant -The question does not follow on from the previous utterances; seems to be completely random, to the current conversation; seems to be a completely different conversation. • 1. Partially Relevant -The question veers off-topic; is vaguely related, but too divergent from the conversation.</p><p>• 2. Relevant -The question follows, but it is not entirely clear why the question is being presented. • 3. Highly Relevant -The question directly follows, and it is clear why the question is being presented.</p><p>Novelty: Does the question add new information to the conversation?</p><p>• 0. No -The question restates the user query; asks a question for which the answer can already be determined from the conversation thus far; restates something already said. • 1. Somewhat, but no (non-relevant/nonsensical) -The question adds something new, but it does not make sense in the current conversation. • 2. Yes (but not useful) -The question adds something new but is not helpful or interesting to the conversation. • 3. Yes (adds to the conversation/interest) -The question adds something new to the conversation that could be interesting to follow up on, or presents paths that could be taken later in the conversation.</p><p>Diversity: Does the question provide a number of options?</p><p>• 0. No -The question provides an answer without explicitly trying to provide new avenues for the user to inquire about. • 1. Offers binary choice (did you mean. . . ) -The question presents a binary choice, i.e. yes/no or A and B • 2. Offers 3 or more -The question offers the user a number of choices on how to proceed. • 3. Open-ended -The question invites any number of responses/answers from the user.</p><p>We generated and released files containing the question pool, an updated question bank, and relevance judgments for each criterion. The updated question bank contains a 2024 set of judgments, while the question bank contains 5596 unique questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATION</head><p>This year, we explore new evaluation methodologies for the main task and the MI sub-task. These are discussed in the following section:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Task</head><p>Turn Based Evaluation: For the main task, we evaluated the runs across two dimensions given the ranking for each topic turn (i) the ranking depth, and (ii) the turn depth. For ranking depth, we focus on earlier positions <ref type="bibr" coords="4,414.15,557.59,8.97,8.97" target="#b0">(1,</ref><ref type="bibr" coords="4,425.36,557.59,6.25,8.97" target="#b2">3,</ref><ref type="bibr" coords="4,433.85,557.59,6.96,8.97" target="#b4">5)</ref> for the conversational scenario (where we assume that the top 𝑘 results will be used to formulate the response back to the user). The turn depth evaluates the run performance at the n-th conversational turn. Performing well on deeper rounds indicates a better ability to understand the preceding context. We use the mean NDCG@3 as the main evaluation metric, with all conversational turns averaged using uniform weights. We also measure the turned-depth measure based on NDCG@3&amp;n, with the per query NDCG@3 scores averaged at depth (𝑛). In addition to the NDCG, we also calculated the precision, recall, Average Precision, and Reciprocal Rank, where again we averaged over all turns.</p><p>Conversational Path Evaluation: To evaluate the quality of the overall conversation, we developed a series of new metrics that aimed to summarize the conversational utility given the flow of relevant responses. To do so, we defined three related metrics, that build upon the turn-based evaluation. Given the conversational tree for each topic, we first extract the conversational paths (and so one topic produces many paths). Each path 𝑝 consists of a series of turns (𝑡). The quality of the response (based on the ranking) given each turn 𝑡 was evaluated using the standard metrics reported earlier.</p><p>Given the scores 𝑠 𝑡 for each turn 𝑡, we then computed the overall score for the path 𝑝.</p><p>The Conversational Cumulative Gain (CCG) for a path 𝑝 is:</p><formula xml:id="formula_0" coords="5,137.20,198.95,157.39,24.00">𝐶𝐶𝐺 (𝑠) = 1 |𝑝 | ∑︁ 𝑡 ∈𝑝 𝑠 𝑡<label>(1)</label></formula><p>where we take the average of the scores 𝑠 𝑡 over the path (This is analogous to session-based cumulative gain). The CCG score for each path is then averaged over all paths to get the average CCG score for the system. Note, 𝑠 denotes the underlying metric used to compute the turn level score (i.e., NDCG@3). The CCG metric treats each turn independently, so the dependence between turns is only weakly captured by averaging over the path. To address this limitation, we developed two additional metrics that tried to encode the dependence turns and flow of the conversation into the metric score. First, we developed a Conversational Path Score (CPS) where the score was computed based on the number of consecutive sequences of responses with a score 𝑠 𝑡 greater than a threshold 𝜃 . To compute the CPS, we first split the path into a number of relevant, 𝑟 (𝑡 𝑖 , ..., 𝑡 𝑖+𝑗 ) and non-relevant sequences, 𝑛(𝑡 𝑖 ), where sequences have a length |𝑟 (.)| based on the number of consequence turns where 𝑠 𝑡 &gt; 𝜃 (i.e., they are considered reasonably relevant to enable the conversation to flow). Then for each relevant sequence, we took the length of the sequence and raised it to an exponent 𝛾, before summing over all relevant sequences. The maximum relevant sequence score would be the total path length raised to the exponent (i.e. |𝑝 | 𝛾 ). This was used to normalize the score. Thus the CPS can be formulated as follows:</p><formula xml:id="formula_1" coords="5,123.51,471.33,171.08,25.11">𝐶𝑃𝑆 (𝑠) = 1 |𝑝 | 𝛾 ∑︁ 𝑟 (.) ∈𝑝 |𝑟 (.)| 𝛾<label>(2)</label></formula><p>where 𝛾 defines how much we value the flow of conversation. If</p><p>𝛾 &gt; 1 then it implies that longer relevant sequences are considered better and users would receive more gain from the conversation "flowing". While if 𝛾 &lt; 1 then it implies that longer relevant sequences are considered worse and users would receive less gain from such "flowing" conversations despite each turn providing relevant material. If 𝛾 = 1 then the metric ignores the dependency between turns -and can be considered a binary version of CCG.</p><p>To provide some examples of how the metric is calculated, imagine we have the following conversational paths where we observe path A, 𝑝 (𝐴) = {𝑟 (𝑡 1 , 𝑡 2 ), 𝑛(𝑡 3 ), 𝑟 (𝑡 4 ), 𝑛(𝑡 5 )} and path B, 𝑝 (𝐴) = {𝑛(𝑡 1 ), 𝑟 (𝑡 2 , 𝑡 2 , 𝑡 3 ), 𝑛(𝑡 5 )}. Both paths have three turns which are considered reasonably relevant responses. However, path B has 3 relevant turns in a row, while path A has 2 relevant turns in a row, and then an isolated relevant turn. The CPS for A then is (2 𝛾 + 1 𝛾 )/5 𝛾 ), while for B it would be (3 𝛾 )/5 𝛾 . If 𝛾 = 2 then A would receive a score of 5/25 while B would receive a score of 9/25. And, so with a 𝛾 &gt; 1 we can see that path B receives the higher score.</p><p>Our next metric, Turn-Biased Conversational Cumulative Gain (TBCCG) provides a different variation on the former two approaches by encoding an explicit user "conversing" model 3 into the metric. After each turn, we assume that a user will either continue the conversation or stop. The probability of the user continuing depends upon whether the system's response was sufficiently relevant. We further assume that the probability of continuing to give a relevant response is greater than the probability of continuing to give a non-relevant response. And as above, if the score for the turn is above the threshold 𝜃 , i.e., 𝑠 𝑡 &gt; 𝜃 then we assume the response was reasonably relevant 𝑟 , else non-relevant 𝑛. Essentially, this means that only a certain proportion of users will reach subsequent turns -depending on the probabilities of continuing and the relevance of the responses.</p><p>To compute TBCCG, we first need a vector of continuation probabilities associated with each subsequent turn to calculate the metric. In the first turn, we assume all users consider the response and then depending on the response, the user continues with some probability 𝑃 (𝑐 |𝑡 1 ). The proportion of users that consider the first turn is 1.0, the proportion of users that consider the second turn is 1.0 × 𝑃 (𝑐 |𝑡 1 ), the third turn 1.0 × 𝑃 (𝑐 |𝑡 1 ) × 𝑃 (𝑐 |𝑡 2 ), and so on. For a given path 𝑝 where we have turns 𝑡 1 to 𝑡 𝑛 , we can calculate the TBCCG as follows:</p><formula xml:id="formula_2" coords="5,357.92,341.56,200.82,24.75">𝑇 𝐵𝐶𝐶𝐺 (𝑠) = 1 |𝑝 | 𝑠 𝑡 1 + 𝑛 ∑︁ 𝑖=2 𝑛 𝑖=2 𝑃 (𝑐 |𝑡 𝑖 -1 ).𝑠 𝑡 𝑖<label>(3)</label></formula><p>where 𝑠 𝑡 𝑖 is the score for the 𝑖th turn, and the probability of continuing 𝑃 (𝑐 |𝑡 𝑖 ) depends on the relevance of the response, where</p><formula xml:id="formula_3" coords="5,317.82,394.33,146.73,9.22">𝑃 (𝑐 |𝑡 𝑖 ) = 𝑃 𝑟 if 𝑠 𝑡 𝑖 &gt; 𝜃 , else 𝑃 (𝑐 |𝑡 𝑖 ) = 𝑃 𝑛 .</formula><p>We assume that 𝑃 𝑟 ≥ 𝑃 𝑛 is such that users are more likely to continue the conversation when they receive a relevant response than when they receive a non-relevant response. Note that if the probability of continuing is for 𝑃 𝑟 = 𝑃 𝑛 = 1 then the metric reverts back to CCG. If 𝑝 𝑛 is set to 0, then, as soon as a user encounters a non-relevant turn, then they would stop, and so the user would not accumulate any further gain. While if 𝑃 𝑛 is greater than zero, it suggests there is some probability that the user will persist and continue interacting.</p><formula xml:id="formula_4" coords="5,327.96,506.39,230.78,24.75">𝑊𝑇 𝐵𝐶𝐶𝐺 (𝑠) = 1 𝑖 𝑤 𝑡 𝑖 𝑤 𝑡 1 .𝑠 𝑡 1 + 𝑛 ∑︁ 𝑖=2 𝑛 𝑖=2 𝑃 (𝑐 |𝑡 𝑖 -1 ).𝑤 𝑡 𝑖 .𝑠 𝑡 𝑖<label>(4)</label></formula><p>Response Quality Evaluation: On top of the standard provenance ranking, we also evaluate the response quality of the main task runs in terms of their relevance, naturalness, and conciseness. Judgments for these dimensions are collected as described in 2.1. We use Precision @ 1 and NDCG @ 1 as a proxy for these evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MI Sub-task</head><p>We evaluate the MI submissions in two ways, namely, human evaluation and end-to-end evaluation.</p><p>Human evaluation. As described in Section 2.2, we collect human judgments on the MI submissions, concerning different aspects of the utterance, namely, relevance, novelty, and diversity. We evaluate the runs in terms of collected relevance labels and report the results in terms of NDCG@1 and P@1. We take only 3 Akin to the user browsing model when interacting with a ranked list. the top one utterance, as we assume that only one MI utterance is posed to the user. End-to-end evaluation. We give teams who participate in the MI sub-task the opportunity to leverage the MI turns in the main task, i.e., passage retrieval. Therefore, we also evaluate the MI submissions in an end-to-end manner where we examine how much it affects the final passage ranking performance. Note that this is not an accurate measure of the quality of MI submissions, since the quality of the ranker (and how it incorporates MI turns) plays a role in the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PARTICIPANTS 4.1 Main Task</head><p>The CAsT main task received 36 run submissions from 9 groups shown in Table <ref type="table" coords="6,109.73,511.58,3.01,8.97" target="#tab_1">2</ref>. The organizers provided two runs (one automatic, one manual) as baselines for comparison. Participants provided metadata and descriptions of their runs.</p><p>Similar to previous years, many teams used a multi-step pipeline consisting of: (1) conversational rewriting (most incorporating the previous canonical responses) and conversational query expansion, (2) retrieval using traditional IR or dense model, and (3) multi-stage passage re-ranking with neural language models fine-tuned for point-wise (mono) and pairwise (duo) ranking. Almost all teams leverage pre-trained Transformer-based language models for rewriting (BART, T5) and ranking (mostly T5). There continued to be a trend of using learned representations for ranking, including sparse (Splade) and dense retrieval. There's also an emerging thread on dense conversational query expansion. In practice, many of the bestperforming runs appeared to use a fusion of retrieval approaches for first-pass retrieval and combined re-ranking scores from the multiple passes of retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MI Sub-task</head><p>The sub-task received 10 runs across 5 groups. In addition, the organizers provided four additional baseline runs (described in 2.1). We saw a variety of approaches used for the task. All runs used some form of ranking, generative, and template-based approaches, with some using a combination of the three. Generative approaches mostly used the ClariQ dataset for training and almost all of these runs use a T5 model for clarification generation. The UoG_GRILL used GPT-3 with few-shot prompting from 2021 CAsT data. Most runs asked questions for all turns, and only four of the runs performed selective MI generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OVERALL RESULTS</head><p>In this section, we present the results of the submitted runs. We include the organizer baselines (Organizers Group) described above that are available in the public CAsT Github repository<ref type="foot" coords="6,519.66,493.04,3.38,7.27" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Task</head><p>For the main task, we evaluate the ranking of the provenance passages. The results are turn-level macro-averaged retrieval effectiveness. We use four standard evaluation measures: Recall, Mean Average Precision (MAP@1K), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG@1K). The primary measure continues to be NDCG@3, focusing on high-precision and quality responses in the top ranks. We use a relevance cutoff of two as positive for binary measures because the value of one is marginal accordingly to the guidelines.</p><p>We distinguish between the two broad categories of runs: automatic and manual. Automatic runs use the raw utterances or provided automatic rewrites. The Automatic MI runs also use responses from the MI sub-task. Manual runs use manually rewritten (resolved) queries. Automatic run results. Table <ref type="table" coords="7,183.04,639.85,4.25,8.97" target="#tab_2">3</ref> shows the results for the 31 automatic runs with a median NDCG@3 score of 0.348.</p><p>The best-performing run uses fusion on top of four retrieval methods that each include sparse and dense retrieval combined with mono-duo T5 reranking. It also includes sparse and dense conversational query expansion. It also incorporates output from MI interactions. The second best-performing run only uses dense retrieval.</p><p>Manual run results. Table <ref type="table" coords="7,434.88,661.76,4.25,8.97" target="#tab_3">4</ref> shows the results for the seven manual runs with a median NDCG@3 value of 0.501. Two runs outperform the organizer bm25_t5 re-ranking baseline. One of these, CNC_MD-C, uses conversational dense passage retrieval (ConvDPR) and mono reranking. The highest recall is 0.702 and uses sparse retrieval. The UWCmanual22 also achieves high recall and is based on feedback from external corpora.</p><p>Overall. It is noteworthy that two automatic runs achieve high recall (&gt; 0.7) and are more effective in recall than any of the manual runs. However, the automatic results still lag behind in terms of precision in the top ranks. Although the gap in candidate retrieval appears to have shrunk (or even closed), using manual queries in reranking still results in significant gains (over 13% relative gain in the best runs) over the best automatic method. We also observe that the best run uses mixed-initiative interaction data.</p><p>Results by Topic: Figure <ref type="figure" coords="8,161.67,207.47,4.23,8.97">1</ref> provides a per-topic analysis comparing the three classes of systems across topics. It uses data from runs above the median. The results show that the topic difficulty varies widely across topics. Interestingly, automatic-mi runs perform better than manual runs 4 / 17 topics ( 25%). There are 11 topics where manual runs still strongly outperform automatic and automatic-mi methods (over half). Topics 133, 142, and 143 have the largest gap between manual and automatic runs. The hardest topics across all types are 135 and 144 which include several feedback and comment turns.</p><p>Results by turn depth: In this section, we discuss how systems perform over the course of the conversation and as turn depth increases. Due to the small sample size, turns beyond eleven are truncated. Figure <ref type="figure" coords="8,117.04,349.94,4.09,8.97" target="#fig_0">2</ref> shows the average NDCG@3 at each turn depth for the categories of runs. The figure only shows the data for runs that perform at or above the median NDCG@3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Conversational Path Evaluation.</head><p>In this section, we go beyond turn-level metrics and focus on evaluating the conversations holistically across turns. Given our three proposed related conversational metrics, we instantiated a number of different variations, but only report a small subset. For those reported, we use the official turn level metric NDCG@3 as 𝑠 given the output of the strict evaluation (relevance of 2 or higher is relevant). We then set the threshold 𝜃 of 0.33 for all reported metrics. That is we assume a reasonably relevant response would return at least one highly relevant item, one relevant and one marginally relevant item, or three marginally relevant items. For our CPS metric, we set 𝛾 to 2 and 3, and for TBCCG we fixed 𝑃 𝑟 equal to one (always continue if relevant) and then varied 𝑃 𝑛 was set to 0.0 and 0.25. The mean of each metric is reported. Note that this is the mean over all the conversational tree paths, not the mean over turns.</p><p>Table <ref type="table" coords="8,85.82,556.33,4.17,8.97" target="#tab_4">5</ref> presents the results given our conversational path measures where systems have been ordered by CCG. It is worth noting that the correlation between our conversational path measures is very high (with Pearson's r of approx. 0.88-0.97). As a result, the systems with high CCG also obtained high scores on our conversational flow-based measures. Over the conversational path, measures present a slightly different story than turn-based ones. The top ranking system according to the CCG, CPS, and TBCCG is the HEATWAVE run combine0.5. However, when the flow is weighted more highly (i.e. CPS with 𝛾 = 3) then we see that udel_fang's run udinfo_mi_b2021 is more effective. Clearly, the more turns that are considered as reasonably relevant responses (𝑠 𝑡 ≥ 𝜃 ), the higher their conversational path metric scores are likely to be -and the higher the penalty to runs which can not consistently return longer sequences of relevant responses. In contrast with the pure turn-based evaluation, we see a difference in the order of the top runs -suggesting that if we also care about consistency of and flow of experience, simply maximizing turn-based NDCG@3 on its own is not optimal.</p><p>Table <ref type="table" coords="8,351.03,141.72,4.25,8.97" target="#tab_5">6</ref> presents the results for the manual runs for our conversational path measures. Here, we see that CFDA CLIP's CNC MD-C run consistently outperforms all the other automatic runswhich is also consistent with the turn-based NDCG@3 results. It changes the order of the second and third teams, with UWCman-ual22 outperforming the organizer baseline for all the measures. We also observe that the manual runs above the median outperform all automatic runs on CCG. It also shows that even for manual runs there is still significant headroom to focus on consistency.</p><p>5.1.2 Response Quality. In this section, we describe the response quality output using crowdsourced judgments. Given that all responses have at least one judged relevant document in their provenance, the focus is on how this is presented to the user in the response. The results tables are sorted by the mean of all three factors.</p><p>This evaluation is performed only on the subset of queries where the system returned a response with a relevant document in its top three responses. This focuses the evaluation on the quality of the response, not on core relevance, which is already reflected in the main task turn relevance evaluation. Note that the UoGTr group runs are not included in the response quality evaluation because they were added after the quality assessment was performed.</p><p>Table <ref type="table" coords="8,350.15,389.69,4.21,8.97" target="#tab_6">7</ref> shows the quality of the automatic results. We observe that the perceived response relevance is high overall, as we would expect given that at least one relevant passage is being used in the response, but there is wide variance. The best-performing conciseness is from generative models, the WaterlooClarke runs used T5 and the Organizer run used BART to generate abstractive summaries of the results. Examining the results, we observe that QA models produced very short segments, that did not score as well for relevance and naturalness. Systems that returned long passages (up to 250 words) were sometimes judged to be less natural as well as less concise. Table <ref type="table" coords="8,396.08,499.28,4.14,8.97" target="#tab_7">8</ref> shows the same response quality measures for the manual runs. The relevance is comparable to the automatic runs as well as conciseness and naturalness.</p><p>Overall, the effectiveness for manual and automatic runs are both low around turns four and eight with the high points at five and nine. Over time we see a decline in the effectiveness of automatic systems. We also studied the differences in automatic and automaticmi runs by depth and we observe that turns 5 and 7 are higher for automatic-MI runs and comparable for the other turns.</p><p>The gap between manual and automatic is smaller than in previous years, with a smaller gap between them even later in the conversation. The number of turns at depths 10 and 11 is quite limited and not enough to draw conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MI Sub-Task</head><p>In this section, we present the results of the MI sub-task both in terms of human evaluation.</p><p>Human Intrinsic Relevance Evaluation There are human annotations that include relevance as well as other MI factors. In this  section, we focus just on the relevance of the posed question to the previous response using crowdsourced judgments. Table ?? shows the human evaluation of the MI submissions. The table includes all turns as well as predicted turns, a subset where the system We evaluate the submissions in two settings, namely, on all the turns and only the utterances where the system posed MI questions. When evaluating on all the turns, we see that the uis-clearboat achieves the highest NDCG@1 and P@1. It's significantly higher than the other approaches. However, when we focus only on the subset of turns where MI questions are posed, the GPT-3 method by UofG_GRILL is the most effective, with an almost 20% absolute difference in NDCG@1 over the next best system. This approach implicitly performs classification of when to use MI by including it in the few-shot prompt. The uogTR-MI run also performs well in both and incorporates a retrieve and generate approach along with further T5 question ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>The fourth TREC CAsT edition developed resources for studying conversational information seeking and added to the community's understanding of the topic. It made significant advances over the previous edition to focus on generating responses, having multiple varied conversations on the same topic using topic trees, and becoming more realistic and interactive.</p><p>The MI sub-task provided a way for participants to gain interactive feedback to improve effectiveness. The most effective automatic team leveraged MI responses. However, the static nature of the topics for the main task limited the headroom of these methods. Future directions should incorporate these into topic development more deeply and support multiple rounds of MI as the conversations evolve.</p><p>This year also introduced grounded response generation to focus not just on retrieving content, but on synthesizing relevant, concise, and natural responses. This is an important emerging area that will continue to grow in importance as rapid advancements in generative language models become more widely used in conversational information-seeking systems.</p><p>We look forward to future interactive tracks that continue to push the boundary of systems that can meaningfully interact in order to help people accomplish increasingly complex tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,53.80,463.95,240.24,7.70;11,53.80,474.91,195.95,7.70;11,60.43,295.89,226.98,154.07"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NDCG@3 at varying conversation turn depths. We report the average across runs, median or better.</figDesc><graphic coords="11,60.43,295.89,226.98,154.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,322.94,203.31,238.13,251.20"><head>Table 1 :</head><label>1</label><figDesc>CAsT 2022 Topic 140. Title: South America Description: An exploration of different aspects of South America's culture, tourist attractions, and cuisine.</figDesc><table coords="1,322.94,266.81,238.13,187.70"><row><cell cols="3">Turn Parent Conversation Utterances</cell></row><row><cell>1-1</cell><cell>-</cell><cell>What should I know about Argentina?</cell></row><row><cell>1-3</cell><cell>1-2</cell><cell>What makes it the capital?</cell></row><row><cell>1-5</cell><cell>1-4</cell><cell>That's not what I meant. I'd like to know about</cell></row><row><cell></cell><cell></cell><cell>the culture of Buenos Aires.</cell></row><row><cell>2-2</cell><cell>2-1</cell><cell>What makes their culture distinct from the rest</cell></row><row><cell></cell><cell></cell><cell>of the country?</cell></row><row><cell>2-4</cell><cell>2-3</cell><cell>What's Merienda?</cell></row><row><cell>3-1</cell><cell>2-3</cell><cell>You've mentioned that several times now. Tell</cell></row><row><cell></cell><cell></cell><cell>me more.</cell></row><row><cell>3-3</cell><cell>3-2</cell><cell>I meant the tango.</cell></row><row><cell>3-5</cell><cell>3-4</cell><cell>Thanks, but I'm not asking about clothing. Why</cell></row><row><cell></cell><cell></cell><cell>is it important historically?</cell></row><row><cell>4-1</cell><cell>2-1</cell><cell>What are they known for?</cell></row><row><cell>5-1</cell><cell>1-2</cell><cell>Mmm, meat sweats. I've heard it's good there,</cell></row><row><cell></cell><cell></cell><cell>tell me more.</cell></row><row><cell>5-3</cell><cell>5-2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,140.85,85.73,330.30,216.87"><head>Table 2 :</head><label>2</label><figDesc>Participants and their runs.</figDesc><table coords="6,140.85,111.79,330.30,190.81"><row><cell>Group</cell><cell>Run ID</cell><cell cols="2">Run Type Group</cell><cell>Run ID</cell><cell>Run Type</cell></row><row><cell cols="2">CFDA_CLIP CNC_AD</cell><cell>automatic</cell><cell>MLIA-DAC</cell><cell cols="2">MLIA_DAC_splade automatic</cell></row><row><cell cols="2">CFDA_CLIP CNC_AD-C</cell><cell>automatic</cell><cell>MLIA-DAC</cell><cell>splade_t5mm</cell><cell>automatic</cell></row><row><cell cols="2">CFDA_CLIP CNC_AS</cell><cell>automatic</cell><cell>MLIA-DAC</cell><cell>splade_t5mm_ens</cell><cell>automatic</cell></row><row><cell cols="2">CFDA_CLIP CNC_AS-C</cell><cell>automatic</cell><cell>MLIA-DAC</cell><cell>splade_t5mse</cell><cell>automatic</cell></row><row><cell cols="2">CFDA_CLIP CNC_MD-C</cell><cell>manual</cell><cell>udel_fang</cell><cell>udinfo_best2021</cell><cell>automatic</cell></row><row><cell cols="2">CFDA_CLIP CNC_MS-C</cell><cell>manual</cell><cell>udel_fang</cell><cell>udinfo_mi_b2021</cell><cell>automatic-mi</cell></row><row><cell>CNR</cell><cell>CNR_run1</cell><cell>automatic</cell><cell>udel_fang</cell><cell>udinfo_onlyd</cell><cell>automatic</cell></row><row><cell>CNR</cell><cell>CNR_run2</cell><cell>manual</cell><cell>udel_fang</cell><cell>udinfo_onlyd_mi</cell><cell>automatic-mi</cell></row><row><cell>CNR</cell><cell>CNR_run3</cell><cell>automatic</cell><cell>UiS</cell><cell>uis_cargoboat</cell><cell>automatic</cell></row><row><cell>CNR</cell><cell>CNR_run4</cell><cell>automatic</cell><cell>UiS</cell><cell>uis_duoboat</cell><cell>automatic</cell></row><row><cell cols="2">HEATWAVE combine0.5</cell><cell>automatic</cell><cell>UiS</cell><cell>uis_mixedboat</cell><cell>automatic-mi</cell></row><row><cell cols="3">HEATWAVE duo_reranker automatic</cell><cell>UiS</cell><cell>uis_sparseboat</cell><cell>automatic</cell></row><row><cell cols="2">HEATWAVE gold</cell><cell>manual</cell><cell>uogTr-AT</cell><cell>UoGTr</cell><cell>automatic</cell></row><row><cell cols="2">HEATWAVE monot5</cell><cell>automatic</cell><cell>uogTr-MI-HB</cell><cell>UoGTr</cell><cell>automatic-mi</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run1</cell><cell>automatic</cell><cell>uogTr-MT</cell><cell>UoGTr</cell><cell>manual</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run2</cell><cell>automatic</cell><cell cols="2">WaterlooClarke UWCauto22</cell><cell>automatic</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run4</cell><cell>automatic</cell><cell cols="2">WaterlooClarke UWCcano22</cell><cell>automatic</cell></row><row><cell>iiia-unipd</cell><cell cols="2">DEI-run5.json automatic</cell><cell cols="2">WaterlooClarke UWCmanual22</cell><cell>manual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,53.50,85.73,504.70,389.18"><head>Table 3 :</head><label>3</label><figDesc>Automatic evaluation of provenance retrieval results. Evaluation at retrieval cutoff of 1000 with a binary relevance threshold of 2.</figDesc><table coords="7,129.71,122.82,352.58,352.09"><row><cell>Group</cell><cell>Run</cell><cell cols="4">Recall MAP MRR NDCG NDCG@3</cell></row><row><cell>udel_fang</cell><cell>udinfo_mi_b2021</cell><cell cols="3">0.771 0.246 0.656 0.557</cell><cell>0.452</cell></row><row><cell>udel_fang</cell><cell>udinfo_onlyd_mi</cell><cell>0.729</cell><cell>0.243 0.646</cell><cell>0.540</cell><cell>0.450</cell></row><row><cell>HEATWAVE</cell><cell>duo_reranker</cell><cell>0.453</cell><cell>0.189 0.639</cell><cell>0.392</cell><cell>0.440</cell></row><row><cell>HEATWAVE</cell><cell>combine0.5</cell><cell>0.453</cell><cell>0.184 0.641</cell><cell>0.389</cell><cell>0.439</cell></row><row><cell>HEATWAVE</cell><cell>monot5</cell><cell>0.453</cell><cell>0.178 0.619</cell><cell>0.381</cell><cell>0.426</cell></row><row><cell cols="2">WaterlooClarke UWCcano22</cell><cell>0.556</cell><cell>0.215 0.624</cell><cell>0.445</cell><cell>0.424</cell></row><row><cell>MLIA-DAC</cell><cell>splade_t5mm_ens</cell><cell>0.638</cell><cell>0.219 0.592</cell><cell>0.483</cell><cell>0.416</cell></row><row><cell>MLIA-DAC</cell><cell>splade_t5mm</cell><cell>0.638</cell><cell>0.202 0.574</cell><cell>0.470</cell><cell>0.401</cell></row><row><cell>UiS</cell><cell>uis_sparseboat</cell><cell>0.507</cell><cell>0.189 0.566</cell><cell>0.409</cell><cell>0.388</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AS</cell><cell>0.527</cell><cell>0.184 0.559</cell><cell>0.411</cell><cell>0.377</cell></row><row><cell cols="2">WaterlooClarke UWCauto22</cell><cell>0.517</cell><cell>0.206 0.566</cell><cell>0.416</cell><cell>0.377</cell></row><row><cell>UiS</cell><cell>uis_cargoboat</cell><cell>0.450</cell><cell>0.180 0.526</cell><cell>0.377</cell><cell>0.373</cell></row><row><cell>UiS</cell><cell>uis_mixedboat</cell><cell>0.445</cell><cell>0.186 0.499</cell><cell>0.374</cell><cell>0.363</cell></row><row><cell>-</cell><cell cols="2">BM25_T5_BART_automatic 0.324</cell><cell>0.150 0.527</cell><cell>0.299</cell><cell>0.362</cell></row><row><cell>MLIA-DAC</cell><cell>MLIA_DAC_splade</cell><cell>0.638</cell><cell>0.162 0.514</cell><cell>0.433</cell><cell>0.348</cell></row><row><cell>udel_fang</cell><cell>udinfo_onlyd</cell><cell>0.651</cell><cell>0.178 0.531</cell><cell>0.453</cell><cell>0.348</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AD</cell><cell>0.320</cell><cell>0.117 0.517</cell><cell>0.294</cell><cell>0.347</cell></row><row><cell>UiS</cell><cell>uis_duoboat</cell><cell>0.365</cell><cell>0.154 0.476</cell><cell>0.323</cell><cell>0.345</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AD-C</cell><cell>0.320</cell><cell>0.109 0.487</cell><cell>0.286</cell><cell>0.334</cell></row><row><cell>UoGTr</cell><cell>uogTr-MI-HB</cell><cell>0.413</cell><cell>0.152 0.503</cell><cell>0.337</cell><cell>0.331</cell></row><row><cell>udel_fang</cell><cell>udinfo_best2021</cell><cell>0.681</cell><cell>0.181 0.514</cell><cell>0.465</cell><cell>0.325</cell></row><row><cell>UoGTr</cell><cell>uogTr-AT</cell><cell>0.319</cell><cell>0.134 0.494</cell><cell>0.290</cell><cell>0.317</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run1</cell><cell>0.445</cell><cell>0.126 0.443</cell><cell>0.327</cell><cell>0.280</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run5</cell><cell>0.420</cell><cell>0.120 0.455</cell><cell>0.315</cell><cell>0.276</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run4</cell><cell>0.456</cell><cell>0.126 0.441</cell><cell>0.334</cell><cell>0.274</cell></row><row><cell>MLIA-DAC</cell><cell>splade_t5mse</cell><cell>0.638</cell><cell>0.127 0.410</cell><cell>0.393</cell><cell>0.271</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run2</cell><cell>0.458</cell><cell>0.125 0.426</cell><cell>0.333</cell><cell>0.263</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AS-C</cell><cell>0.515</cell><cell>0.141 0.366</cell><cell>0.369</cell><cell>0.235</cell></row><row><cell>CNR</cell><cell>CNR_run4</cell><cell>0.316</cell><cell>0.072 0.367</cell><cell>0.224</cell><cell>0.216</cell></row><row><cell>CNR</cell><cell>CNR_run3</cell><cell>0.332</cell><cell>0.068 0.340</cell><cell>0.231</cell><cell>0.201</cell></row><row><cell>CNR</cell><cell>CNR_run1</cell><cell>0.333</cell><cell>0.059 0.274</cell><cell>0.219</cell><cell>0.177</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,53.50,492.20,504.70,126.16"><head>Table 4 :</head><label>4</label><figDesc>Manual provenance ranking results. These runs used the manually resolved queries. Evaluation at retrieval cutoff of 1000 with a binary relevance threshold of 2.</figDesc><table coords="7,134.33,529.29,343.35,89.07"><row><cell>Group</cell><cell>Run</cell><cell cols="4">Recall MAP MRR NDCG NDCG@3</cell></row><row><cell>HEATWAVE</cell><cell>gold</cell><cell>0.557</cell><cell>0.257 0.717</cell><cell>0.485</cell><cell>0.513</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_MD-C</cell><cell>0.339</cell><cell>0.163 0.706</cell><cell>0.350</cell><cell>0.512</cell></row><row><cell>-</cell><cell cols="2">BM25_T5_BART_manual 0.465</cell><cell>0.231 0.716</cell><cell>0.423</cell><cell>0.503</cell></row><row><cell cols="2">WaterlooClarke UWCmanual22</cell><cell>0.676</cell><cell>0.294 0.711</cell><cell>0.550</cell><cell>0.501</cell></row><row><cell>UoGTr</cell><cell>uogTr-MT</cell><cell>0.553</cell><cell>0.249 0.695</cell><cell>0.477</cell><cell>0.487</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_MS-C</cell><cell cols="2">0.702 0.260 0.593</cell><cell>0.537</cell><cell>0.398</cell></row><row><cell>CNR</cell><cell>CNR_run2</cell><cell>0.382</cell><cell>0.076 0.338</cell><cell>0.255</cell><cell>0.202</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,53.50,93.12,504.70,593.31"><head>Table 5 :</head><label>5</label><figDesc>NDCG@3 aggregated for each topic across all runs. We report the average across runs, median or better for each run category. Conversational Path Evaluation on the Main Task for Automatic Runs. The turn-based metric used was NDCG@3 with a threshold 𝜃 = 0.33.</figDesc><table coords="9,311.71,240.32,11.55,4.37"><row><cell>Topic</cell></row></table><note coords="9,143.91,334.34,14.72,7.70;9,258.67,333.25,171.98,8.97;9,433.89,333.25,73.44,8.97;9,510.58,333.25,25.40,8.97"><p><p>run</p>CCP CPS(𝛾 = 2) CPS(𝛾 = 3) TBCCG(𝑃 𝑛 = 0.0) TBCCG(𝑃 𝑛 = 0.25)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,53.50,85.73,504.70,126.16"><head>Table 6 :</head><label>6</label><figDesc>Conversational Path Evaluation on Main Task over Manual Runs. These runs used manually resolved queries. The turn-based metric used was NDCG@3 with a threshold 𝜃 = 0.33.runCCG CPS(𝛾 = 2) CPS(𝛾 = 3) TBCCG(𝑃 𝑛 = 0.0) TBCCG(𝑃 𝑛 = 0.25)</figDesc><table coords="10,80.64,137.17,450.73,74.72"><row><cell>CFDA_CLIP</cell><cell>CNC_MD-C</cell><cell>0.389</cell><cell>0.264</cell><cell>0.169</cell><cell>0.154</cell><cell>0.193</cell></row><row><cell cols="2">WaterlooClarke UWCmanual22</cell><cell>0.370</cell><cell>0.222</cell><cell>0.122</cell><cell>0.131</cell><cell>0.168</cell></row><row><cell>-</cell><cell cols="2">BM25_T5_BART_manual 0.369</cell><cell>0.205</cell><cell>0.096</cell><cell>0.126</cell><cell>0.164</cell></row><row><cell>UoGTr</cell><cell>uogTr-MT</cell><cell>0.358</cell><cell>0.198</cell><cell>0.114</cell><cell>0.140</cell><cell>0.166</cell></row><row><cell>HEATWAVE</cell><cell>gold</cell><cell>0.325</cell><cell>0.186</cell><cell>0.095</cell><cell>0.105</cell><cell>0.140</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_MS-C</cell><cell>0.294</cell><cell>0.180</cell><cell>0.117</cell><cell>0.073</cell><cell>0.103</cell></row><row><cell>CNR</cell><cell>CNR_run2</cell><cell>0.159</cell><cell>0.054</cell><cell>0.017</cell><cell>0.032</cell><cell>0.047</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,105.92,225.20,399.87,356.30"><head>Table 7 :</head><label>7</label><figDesc>Response quality evaluation for automatic main task runs using crowdsourced jugdments.</figDesc><table coords="10,109.29,251.33,393.43,330.17"><row><cell></cell><cell>Run</cell><cell cols="3">Relevance @ 1 Conciseness @ 1 Naturalness @ 1</cell></row><row><cell cols="2">WaterlooClarke UWCauto22</cell><cell>0.803</cell><cell>0.667</cell><cell>0.704</cell></row><row><cell cols="2">WaterlooClarke UWCcano22</cell><cell>0.807</cell><cell>0.591</cell><cell>0.591</cell></row><row><cell>Organizers</cell><cell>BM25_T5_BART_automatic</cell><cell>0.745</cell><cell>0.617</cell><cell>0.500</cell></row><row><cell>udel_fang</cell><cell>udinfo_mi_b2021</cell><cell>0.820</cell><cell>0.470</cell><cell>0.530</cell></row><row><cell>udel_fang</cell><cell>udinfo_best2021</cell><cell>0.779</cell><cell>0.456</cell><cell>0.529</cell></row><row><cell>MLIA-DAC</cell><cell>splade_t5mse</cell><cell>0.842</cell><cell>0.439</cell><cell>0.456</cell></row><row><cell>UiS</cell><cell>uis_duoboat</cell><cell>0.809</cell><cell>0.412</cell><cell>0.515</cell></row><row><cell>udel_fang</cell><cell>udinfo_onlyd_mi</cell><cell>0.796</cell><cell>0.439</cell><cell>0.490</cell></row><row><cell>UiS</cell><cell>uis_sparseboat</cell><cell>0.803</cell><cell>0.382</cell><cell>0.434</cell></row><row><cell>UiS</cell><cell>uis_cargoboat</cell><cell>0.836</cell><cell>0.315</cell><cell>0.466</cell></row><row><cell>udel_fang</cell><cell>udinfo_onlyd</cell><cell>0.716</cell><cell>0.419</cell><cell>0.446</cell></row><row><cell>UiS</cell><cell>uis_mixedboat</cell><cell>0.761</cell><cell>0.375</cell><cell>0.421</cell></row><row><cell>HEATWAVE</cell><cell>combine0.5</cell><cell>0.634</cell><cell>0.430</cell><cell>0.484</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AD</cell><cell>0.645</cell><cell>0.355</cell><cell>0.419</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AD-C</cell><cell>0.568</cell><cell>0.432</cell><cell>0.398</cell></row><row><cell>HEATWAVE</cell><cell>duo_reranker</cell><cell>0.557</cell><cell>0.454</cell><cell>0.361</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AS</cell><cell>0.621</cell><cell>0.359</cell><cell>0.388</cell></row><row><cell>MLIA-DAC</cell><cell>splade_t5mm</cell><cell>0.700</cell><cell>0.338</cell><cell>0.325</cell></row><row><cell>HEATWAVE</cell><cell>monot5</cell><cell>0.539</cell><cell>0.393</cell><cell>0.427</cell></row><row><cell>MLIA-DAC</cell><cell>splade_t5mm_ens</cell><cell>0.713</cell><cell>0.310</cell><cell>0.322</cell></row><row><cell>CNR</cell><cell>CNR_run3</cell><cell>0.695</cell><cell>0.237</cell><cell>0.390</cell></row><row><cell>MLIA-DAC</cell><cell>MLIA_DAC_splade</cell><cell>0.722</cell><cell>0.250</cell><cell>0.306</cell></row><row><cell>CNR</cell><cell>CNR_run4</cell><cell>0.657</cell><cell>0.271</cell><cell>0.343</cell></row><row><cell>CNR</cell><cell>CNR_run1</cell><cell>0.729</cell><cell>0.188</cell><cell>0.354</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run4</cell><cell>0.491</cell><cell>0.309</cell><cell>0.400</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run5</cell><cell>0.462</cell><cell>0.269</cell><cell>0.365</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_AS-C</cell><cell>0.480</cell><cell>0.370</cell><cell>0.233</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run2</cell><cell>0.481</cell><cell>0.250</cell><cell>0.346</cell></row><row><cell>iiia-unipd</cell><cell>DEI-run1</cell><cell>0.482</cell><cell>0.259</cell><cell>0.333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,110.87,594.81,389.98,104.25"><head>Table 8 :</head><label>8</label><figDesc>Response quality evaluation for manual main task runs using crowdsourced jugdments.</figDesc><table coords="10,112.78,620.95,386.44,78.11"><row><cell></cell><cell>Run</cell><cell cols="3">Relevance @ 1 Conciseness @ 1 Naturalness @ 1</cell></row><row><cell cols="2">WaterlooClarke UWCmanual22</cell><cell>0.856</cell><cell>0.673</cell><cell>0.673</cell></row><row><cell>Organizers</cell><cell>BM25_T5_BART_manual</cell><cell>0.744</cell><cell>0.566</cell><cell>0.566</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_MD-C</cell><cell>0.623</cell><cell>0.485</cell><cell>0.492</cell></row><row><cell>HEATWAVE</cell><cell>gold</cell><cell>0.642</cell><cell>0.406</cell><cell>0.453</cell></row><row><cell>CFDA_CLIP</cell><cell>CNC_MS-C</cell><cell>0.505</cell><cell>0.367</cell><cell>0.358</cell></row><row><cell>CNR</cell><cell>CNR_run2</cell><cell>0.688</cell><cell>0.203</cell><cell>0.266</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,126.92,85.73,358.15,191.92"><head>Table 9 :</head><label>9</label><figDesc>Clarification question evaluation of MI runs using crowdsourced jugdments.</figDesc><table coords="11,126.92,111.86,358.15,165.78"><row><cell></cell><cell>Run</cell><cell cols="3">Relevance @ 1 Novelty @ 1 Diversity @ 1</cell></row><row><cell cols="2">UoG_GRILL GPT-3_full_context</cell><cell>0.852</cell><cell>0.536</cell><cell>0.607</cell></row><row><cell cols="2">UoG_GRILL GPT-3_rewrite</cell><cell>0.657</cell><cell>0.515</cell><cell>0.551</cell></row><row><cell>UoGTr</cell><cell>uogTr-MI</cell><cell>0.663</cell><cell>0.494</cell><cell>0.369</cell></row><row><cell>UiS</cell><cell>uis_clearboat</cell><cell>0.639</cell><cell>0.488</cell><cell>0.371</cell></row><row><cell cols="2">UoG_GRILL GPT-3_raw</cell><cell>0.592</cell><cell>0.362</cell><cell>0.490</cell></row><row><cell>Organizers</cell><cell>miniLM_bert_sample_mi_run</cell><cell>0.371</cell><cell>0.317</cell><cell>0.395</cell></row><row><cell>Organizers</cell><cell>bm25_baseline_mi</cell><cell>0.345</cell><cell>0.293</cell><cell>0.307</cell></row><row><cell>UiS</cell><cell>uis_vagueboat</cell><cell>0.237</cell><cell>0.322</cell><cell>0.381</cell></row><row><cell cols="2">CFDA_CLIP CNC_kwqlm2_cqg</cell><cell>0.340</cell><cell>0.283</cell><cell>0.220</cell></row><row><cell cols="2">CFDA_CLIP CNC_kwqlm_cqg</cell><cell>0.340</cell><cell>0.263</cell><cell>0.234</cell></row><row><cell>Organizers</cell><cell>T5_rewrite</cell><cell>0.320</cell><cell>0.229</cell><cell>0.210</cell></row><row><cell cols="2">CFDA_CLIP CNC_cqg</cell><cell>0.294</cell><cell>0.234</cell><cell>0.224</cell></row><row><cell>UDel</cell><cell>mi_task_0822_1</cell><cell>0.155</cell><cell>0.117</cell><cell>0.322</cell></row><row><cell>Organizers</cell><cell>T5_raw</cell><cell>0.232</cell><cell>0.166</cell><cell>0.185</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,53.80,685.71,91.85,6.25;1,53.32,693.86,113.01,6.97"><p>TREC'22, November 2022, Virtual © 2023 Copyright held by the author(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="3,56.72,692.38,158.82,6.97"><p>https://huggingface.co/castorini/monot5-base-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="3,56.84,700.80,149.60,6.97"><p>https://huggingface.co/facebook/bart-large-cnn?text</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,320.88,701.18,110.68,6.97"><p>https://github.com/daltonj/treccastweb</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Carlos Gemmell</rs>, <rs type="person">Iain Mackie</rs>, and <rs type="person">Alessandro Speggorin</rs> for their contributions to topic development and annotation. We also are deeply thankful for Ian Soboroff's experience, patience, and persistence in running the assessment process. Finally, we thank all our participants. The crowd-sourcing annotation efforts are supported by <rs type="funder">EPSRC</rs>, the grant, <rs type="projectName">Neural Conversational Information Seeking Assistant</rs>, <rs type="grantNumber">EP/V025708/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_bEu3GTZ">
					<idno type="grant-number">EP/V025708/1</idno>
					<orgName type="project" subtype="full">Neural Conversational Information Seeking Assistant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,330.15,684.33,228.05,6.97;11,330.15,692.30,228.05,6.97;11,329.94,700.27,193.36,6.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,521.01,684.33,37.19,6.97;11,330.15,692.30,191.93,6.97">Building and evaluating open-domain dialogue corpora with clarifying questions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Burtsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,535.98,692.30,22.22,6.97">EMNLP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4473" to="4484" />
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.99,88.42,228.05,6.97;12,65.99,96.39,228.05,6.97;12,65.78,104.36,17.13,6.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,216.52,88.42,77.53,6.97;12,65.99,96.39,143.35,6.97">Asking clarifying questions in open-domain information-seeking conversations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,223.13,96.39,14.78,6.97">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.99,112.33,228.05,6.97;12,65.99,120.30,229.12,6.97;12,65.78,128.27,228.71,6.97;12,65.99,136.24,138.83,6.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,242.88,112.33,51.17,6.97;12,65.99,120.30,96.92,6.97">Overview of the trec 2021 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/overview-of-the-trec-2021-deep-learning-track/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,183.84,120.30,108.02,6.97">Text REtrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.99,144.21,228.06,6.97;12,65.99,152.18,228.05,6.97;12,330.15,88.42,227.99,6.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,155.24,144.21,138.80,6.97;12,65.99,152.18,41.00,6.97">TREC CAsT 2021: The conversational assistance track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,121.11,152.18,172.93,6.97;12,330.15,88.42,11.06,6.97">The Thirtieth Text REtrieval Conference Proceedings (TREC 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.15,96.39,228.82,6.97;12,329.95,104.36,228.26,6.97;12,330.15,112.33,157.33,6.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,461.52,104.36,96.68,6.97;12,330.15,112.33,68.46,6.97">Kilt: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rocktaschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno>ArXiv abs/2009.02252</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,330.15,120.30,228.88,6.97;12,330.15,128.27,228.05,6.97;12,329.94,136.24,136.84,6.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,330.15,128.27,172.57,6.97">Leading conversational search by suggesting useful questions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,516.50,128.27,41.70,6.97;12,329.94,136.24,59.77,6.97">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1160" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
