<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.29,99.57,409.45,14.93">OVERVIEW OF THE TREC 2022 DEEP LEARNING TRACK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,83.45,162.69,57.27,8.64"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<email>nickcr@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,149.69,162.69,57.27,8.64"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
							<email>bmitra@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,215.93,162.69,57.29,8.64"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<email>emine.yilmaz@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.41,162.69,61.71,8.64"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
							<email>dcampos3@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Neural Magic Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.31,162.69,43.46,8.64"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.74,162.69,74.25,8.64"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
							<email>ellen.voorhees@nist.gov</email>
						</author>
						<author>
							<persName coords="1,515.83,162.69,12.72,8.64;1,286.42,173.60,35.17,8.64"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
						</author>
						<author>
							<persName coords="1,215.26,194.47,37.60,8.64"><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main" coord="1,101.29,99.57,409.45,14.93">OVERVIEW OF THE TREC 2022 DEEP LEARNING TRACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B216F3FA8ED032DE6E00404CD8018B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the fourth year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we also leverage both the refreshed passage and document collections that were released last year leading to a nearly 16 times increase in the size of the passage collection and nearly four times increase in the document collection size. Unlike previous years, in 2022 we mainly focused on constructing a more complete test collection for the passage retrieval task, which has been the primary focus of the track. The document ranking task was kept as a secondary task, where document-level labels were inferred from the passage-level labels. Our analysis shows that similar to previous years, deep neural ranking models that employ large scale pretraining continued to outperform traditional retrieval methods. Due to the focusing our judging resources on passage judging, we are more confident in the quality of this year's queries and judgments, with respect to our ability to distinguish between runs and reuse the dataset in future. We also see some surprises in overall outcomes. Some top-performing runs did not do dense retrieval. Runs that did single-stage dense retrieval were not as competitive this year as they were last year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At TREC 2022, we hosted the fourth TREC Deep Learning Track continuing our focus on benchmarking ad hoc retrieval methods in the large-data regime. As in previous years <ref type="bibr" coords="1,338.09,530.36,86.17,8.64" target="#b2">[Craswell et al., 2020</ref><ref type="bibr" coords="1,432.12,530.36,22.37,8.64">[Craswell et al., , 2021a</ref><ref type="bibr" coords="1,462.33,530.36,21.44,8.64">[Craswell et al., , 2022]]</ref>, we leverage the MS MARCO datasets <ref type="bibr" coords="1,180.01,541.27,76.98,8.64" target="#b0">[Bajaj et al., 2016]</ref> that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, last year we refreshed both the passage and the document collections which also led to a nearly 16 times increase in the size of the passage collection and nearly four times increase in the document collection size. In addition to evaluating ranking methods on the larger collections, the data refresh also aimed at providing additional metadata-e.g., passage-to-document mappings-that may be useful for ranking, as well as incorporating some fixes for known text encoding issues in previous versions of the datasets. This year we continue to benchmark against these larger passage and document collections. However, the significant increase in collection sizes last year led to a corresponding increase in the number of relevant results in the collection per query and the existing judgment budget was exceeded before a reasonably complete set of these relevant results could be identified by the NIST judges. This large number of relevant raised serious concerns about the test collection generated by last year's track, relating to reusability and also score saturation <ref type="bibr" coords="1,386.09,650.36,86.08,8.64" target="#b16">[Voorhees et al., 2022</ref><ref type="bibr" coords="1,472.17,650.36,67.83,8.64;1,72.00,661.27,21.44,8.64">, Craswell et al., 2022]</ref>. To address these concerns, we made three changes this year with the goal of reducing the number of relevant results per query and in general the judgment costs so that they may be reused to obtain more complete set of judgments and consequently a more reusable test collection:</p><p>[1] We used test queries that did not contribute to the MS MARCO corpus. In all previous TREC DL and MS MARCO leaderboard evaluation, ten Bing results for the test query were included in the corpus whenever available. Further, we chose test queries where one of the Bing results was annotated as positive <ref type="bibr" coords="2,495.32,75.48,44.68,8.64;2,107.87,86.39,75.65,8.64" target="#b7">[Gupta and MacAvaney, 2022]</ref> and the positive result made it into our corpus. This year's queries went through the same MS MARCO sampling and top-10 annotation, but this happened after we finalized the MS MARCO dataset. We still choose queries that got a positive qrel during annotation, but the Bing top-10 passages and associated URLs were never used during corpus construction and we don't check whether the positive qrel is in the corpus. We no longer measure reciprocal rank, since that was the evaluation that used the MS MARCO qrels. Such qrels are still used for training and dev sets.</p><p>[2] We employ NIST judges to manually evaluate the relevance of retrieved results only for the passage ranking task and propagate the same labels to the source documents for the document ranking task.</p><p>[3] And finally, we detect near-duplicate passages and only judge one representative passage from each nearduplicate cluster with respect to the target query.</p><p>This year we are more confident that our test collection is reusable and discriminative. We find results that confirm previous results, but also an overall larger gap between the best neural methods and traditional ranking methods. This could be due to the change in query sampling, but could also be due to progress in the field. We also see that there is a top run without dense retrieval, and the best run using single-stage dense retrieval is not as competitive as last year.</p><p>Please see participant papers for more insights about what we learned this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>Similar to previous years, the Deep Learning Track in 2022 has two tasks: Passage ranking and document ranking.</p><p>Participants were allowed to submit up to three official runs, and up to five additional baseline runs, for each task.</p><p>When submitting each run, participants indicated what external data, pretrained models and other resources were used, as well as information on what style of model was used.</p><p>The TREC Deep Learning Track has a focus on generating reusable test collections and analyzing reusability. Since previous analysis showed that test collections constructed as part of the track in 2021 were not as reusable as the collections from the previous years <ref type="bibr" coords="2,216.75,384.04,86.48,8.64" target="#b16">[Voorhees et al., 2022</ref><ref type="bibr" coords="2,303.22,384.04,97.08,8.64">, Craswell et al., 2021c]</ref>, in 2022 we primarily focused on improving the reusability of the test collections constructed as part of the track. Hence, we focused on the passage ranking task as the primary task (while keeping the document ranking task as the secondary task) and mainly aimed at constructing a sufficiently complete and reusable test collection for the passage ranking task. Labels inferred from passage-level labels have then been used for the document ranking task.</p><p>We changed our method for query sampling in 2022 with the intention of making the queries more difficult, to avoid the case where all runs have equally high performance and the evaluation is less discriminative. Since there was a risk the new queries would be unusable, we sampled 250 backup queries using the same method as in 2021, and 250 queries from a new method. Queries from the new method have query IDs of two million and above. Participants ran all 500 queries. Our hope was that NIST judges would not find any problems with the new method, and could judge entirely queries from that set of 250, and this was indeed the case.</p><p>Our new method uses queries from the same sampling and annotation pipeline as standard MS MARCO queries. The pipeline samples Bing queries, uses a classifier to find queries that are answerable by a short passage, and since the classifier is imperfect the annotators can also reject a query as "can't judge". For consistency with previous years, we also eliminated queries where the judge did not select a passage, see Figure1. The difference is that all our MS MARCO ranking datasets until now were based on a 2018 version of the MS MARCO data with one million queries as described in a 2018 update of the MS MARCO paper <ref type="bibr" coords="2,303.39,569.54,74.09,8.64" target="#b0">[Bajaj et al., 2016]</ref>.<ref type="foot" coords="2,381.77,567.87,3.49,6.05" target="#foot_0">1</ref> This year's annotations went through the same process, but after the one million query cutoff. This means they were not in the one million MS MARCO queries, their top-10 passages and URLs were not used to construct the MS MARCO passage and document corpora.</p><p>It also means we do not have an evaluation using the MS MARCO sparse qrels and we did not filter out test queries where the sparse qrel failed to make it into the corpus. We expect queries from the new method to be more difficult because</p><p>In the pooling and judging process, NIST chose a subset of the 250 queries for judging as described below. This led to a judged test set of 76 queries for the passage ranking task, and we evaluated the document ranking runs on the same set of 76 queries by propagating the passage labels to their source documents.</p><p>Below we provide more detailed information about the document retrieval and passage retrieval tasks, as well as the datasets provided as part of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Passage ranking task</head><p>The first task focuses on passage ranking, with two subtasks: (i) a full ranking and (ii) a top-100 reranking tasks.</p><p>In the full ranking subtask, given a query, the participants were expected to retrieve a ranked list of passages from the full collection based on the estimated likelihood of the passage containing an answer to the question. Participants could submit up to 100 passages per query for this end-to-end ranking task.</p><p>In the top-100 reranking subtask, 100 passages per query were provided to participants, which were retrieved using Pyserini <ref type="bibr" coords="3,107.39,200.59,71.03,8.64">[Lin et al., 2021b]</ref>. The reranking subtask allows all participants to start from the same starting point and to focus on learning an effective relevance estimator, without the need for implementing an end-to-end retrieval system. It also makes the reranking runs more comparable, because they all rerank the same set of 100 candidates.</p><p>This year's focus on building a more reusable test collection for the passage ranking task than the TREC 2021 collection caused changes in the assessment process at NIST. One of the biggest changes was that only passages were judged, with passage judgments subsequently propagated to documents to form the document relevance judgments. In previous years of the track, both documents and passages were judged independently, so focusing assessing resources on only passages effectively doubled the passage judgment budget.</p><p>The other major change was judging only a single element from a set of near-duplicate passages. To effect this change, the passage corpus was clustered into classes of near-duplicate documents using the process at https:// github.com/isoboroff/dedupe. Each class had a single passage designated as the canonical passage for the class and the passage id of that passage was used as the class identifier. The relevance label of the canonical passage with respect to a query was propagated to all the other passages in the same class.</p><p>The track received 100 submissions to the passage ranking task, 40 of which were baseline runs. Of the 100 submitted runs, 82 runs contributed to the initial judgment pools. The pool runs included all baseline runs, the three highestpriority submissions per team for the reranking subtask, and the three highest-priority submissions per team for the full ranking subtask.</p><p>The test set consisted of 500 queries, 250 of which (those whose query id is greater than 2,000,000) were queries that have no MSMARCO judgments. NIST used this set of 250 new queries as candidates for judging. Twenty-one of the candidates were eliminated by NIST staff before any judging took place because it was deemed unlikely to serve as a good evaluation query (e.g., what a pull and what is my network name and password); the remaining 229 candidates formed the set of queries that assessors could choose to work on.</p><p>An assessor chose a candidate query from the set and judged the first 100 passages (ordered by smallest rank at which the passage was retrieved across all pooled runs) in the depth-10 pool, or the entire depth-10 pool if the pool was smaller than 100 passages. The candidate was discarded if at least 50% of the judged passages were relevant or if no passage was relevant. Otherwise, the assessor judged the remainder of the depth-10 pool (if any) plus the depth-10 pool formed on a fraction of the collection.</p><p>To support an (eventual) investigation of using corpus subsets in test collections, we needed to obtain judgments for the passages that would arise in such a case while the assessors were present. This need motivated the use of both the full and subset corpora in the track judgment process. The passages in the "fractional" collection were selected by randomly ordering the entire (deduped) passage corpus and using the first 1/10 of the passages in that ordering as the corpus. The ordering was query independent, and the same ordering was used for all queries. Runs were then restricted to the passages appearing in the 1/10 set by dropping any passage in the ranking but not in the fractional corpus from the ranking. These are the restricted runs. The passages added to the set of passages to be judged (the judgment set) were the depth-10 pools formed from the restricted runs in the pooled set, minus any passage already judged (because it was in the depth-10 pool of the full corpus).</p><p>The judgments from the first 100 passages were used to do an initial round of CAL processing <ref type="bibr" coords="3,453.82,631.67,81.76,8.64;3,72.00,642.58,43.47,8.64" target="#b1">[Cormack and Grossman, 2015]</ref>. A CAL iteration takes all judgments made to date and ranks the remainder of the collection by likelihood of relevance. While CAL was run across the corpus containing near duplicates, subsequent selection of passages to be added to the judgment set removed near duplicates, so only the canonical passage could be judged.The first 25 passages in the deduped CAL ranking were also added to the judgment set.</p><p>A candidate query then went through a series of CAL iterations until a stopping condition was met. If the relevant density (that is, the proportion of relevant passages to judged passages) was less than 40%, at least 150 passages had been judged, and more than 3 relevant passages had been found, the candidate was accepted as a topic in the evaluation set. If more than 300 passages had been judged and the relevant density was greater than 50%, the candidate was stopped and rejected. Otherwise, a candidate continued until the assessing budget had been expended. Once the budget was spent, candidates with fewer than 150 passages judged, with fewer than four relevant passages found, or with a relevant density of at least 40% were rejected.</p><p>Because CAL results depend on the set of judged passages given to it, we had three separate threads of CAL iterations running in parallel for each query that made it to the CAL stage. One thread ran CAL using only passages in the fractional collection; the second thread ran CAL on the full corpus, but the judgments given to CAL contained only passages encountered in this thread; and the third thread used the full corpus and any available judgment. The output of the third thread is the official qrels for the track. Each CAL iteration added the next 25 passages from the ranking produced by CAL to the judgment set, except later iterations for the fractional thread which added 50 passages. The final evaluation set of topics contains the 76 topics accepted through this process.</p><p>Judgments were collected on a four-point scale:</p><p>[3] Perfectly relevant: The passage is dedicated to the query and contains the exact answer.</p><p>[2] Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information.</p><p>[1] Related: The passage seems related to the query but does not answer it.</p><p>[0] Irrelevant: The passage has nothing to do with the query.</p><p>For metrics that binarize the judgment scale, we map passage judgment levels 3,2 to relevant and map passage judgment levels 1,0 to irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document ranking task</head><p>Similar to the passage ranking task, the document ranking task focuses on two subtasks: (i) Full ranking and (ii) top-100 reranking.</p><p>The full ranking subtask models the end-to-end retrieval scenario, documents can be retrieved from the full document collection provided and the runs are expected to rank documents based on their relevance to the query. Similar to passage ranking, in the document reranking subtask, participants were provided with an initial ranking of 100 documents, giving all participants the same starting point. The 100 documents provided to the participants were generated using Pyserini <ref type="bibr" coords="4,174.25,432.33,69.78,8.64">Lin et al. [2021b]</ref>. Participants were expected to rerank the 100 documents based on their estimated likelihood of containing an answer to the query.</p><p>Instead of collecting additional judgments for the document ranking task, we used passage judgments to infer judgments for documents: For each document we first identified the passages that were judged from within that document when collecting judgments for the passage ranking task, where all duplicates of a judged passage are assumed to have the same relevance judgment as the judged passage. If a document contains multiple passages with associated relevant judgments, we use the max judgment across all the passages to infer the final relevance judgment for the document. Previous work has shown that such an approach results in reasonable quality relevance judgments <ref type="bibr" coords="4,468.99,514.17,66.72,8.64" target="#b17">[Wu et al., 2019]</ref>, and our study on the 2021 test collections further validated this <ref type="bibr" coords="4,325.50,525.08,87.02,8.64">[Craswell et al., 2022]</ref>.</p><p>Different from the passage ranking task, for document ranking metrics that use binary judgments we map document judgment levels 3,2,1 to relevant and map document judgment level 0 to irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>This year we leveraged the MS MARCO v2 dataset, which was used in both tasks. To understand how the new dataset differs from the old, we will first describe the natural language generation data and v1 ranking data.</p><p>MS MARCO natural language generation dataset. The original MS MARCO dataset was for a natural language generation task, rather than a ranking task. It processed one million queries, using a crowd task as shown in Figure <ref type="figure" coords="4,532.53,648.06,3.74,8.64">1</ref>. The crowd worker would read the query, consider up to ten passages related to the query, decide if the passages could be used to answer the query and if answerable write an answer to the query in their own words. For each answerable question the crowd workers provided a non-extractive answer and an annotation of which passages they used to generate their answer. There was substantial quality work with the crowd workers to ensure quality and the crowd workers spent an average of 2.5 minutes on each annotation. The million queries were drawn from actual user queries to Bing. The ten results were generated by a Bing passage retrieval and ranking system. The queries Figure <ref type="figure" coords="5,100.66,315.04,3.88,8.64">1</ref>: Crowd task used to generate the original MS MARCO natural language generation leaderboard. This same crowd data was later adapted to become the MS MARCO ranking tasks.</p><p>were filtered before being annotated to remove any adult or offensive queries and any non-English queries. Moreover, further filtering was performed to ensure that the queries came from the 10-20% of English queries that were detected as potentially being answerable with a short passage. Although the filter may be imperfect, the intention was to exclude navigational queries (such as [youtube]), queries that require a longer answer (such as [beef wellington recipe]) and queries that aim to complete some transaction (such as [buy xbox live]). We note that about 35% of the queries could not be answered using the ten passages, in which case the crowd worker would indicate No answer, and one part of the original MS MARCO challenge was to predict which queries were answerable.</p><p>MS MARCO ranking v1 datasets. The MS MARCO passage and document ranking v1 datasets are used in the current MS MARCO leaderboards <ref type="bibr" coords="5,212.85,462.56,62.56,8.64" target="#b12">[Lin et al., 2022</ref><ref type="bibr" coords="5,275.42,462.56,92.91,8.64">, Craswell et al., 2021b</ref><ref type="bibr" coords="5,368.32,462.56,76.25,8.64">, Lin et al., 2021a]</ref> and in TREC 2019 and TREC 2020.</p><p>To generate the v1 passage ranking data, we took the union of the top ten passage lists for the one million queries, giving us 8.8 million distinct passages. For queries that were answerable, we used the crowd judge annotation for selected passages as a positive qrel. This gives us highly incomplete qrels, as noted in the original description <ref type="bibr" coords="5,515.65,511.67,24.35,8.64;5,72.00,522.58,46.37,8.64" target="#b0">[Bajaj et al., 2016]</ref>. We should in no way expect the positive qrel to be the "best answer". We found that training and evaluating using these sparse qrels gives us results that are quite correlated with results using much more comprehensive NIST judgments <ref type="bibr" coords="5,140.55,544.40,83.38,8.64" target="#b2">[Craswell et al., 2020</ref><ref type="bibr" coords="5,230.86,544.40,25.85,8.64">[Craswell et al., , 2021a]]</ref>. Further study is needed to understand why this works, but we suspect it's important that the qrel is selected from a Bing ranking that has access to information that's unavailable to TREC participants, such as billions of past queries. This means the selected qrel is not biased towards some existing academic approach such as BM25. For each query that has a qrel, we generated a BM25 top-1000 for use in a reranking task and also allowed fullrank from the 8.8 million passages. We used the same split as in the QnA task: training (80%), dev (10%) and eval (10%).</p><p>To generate the v1 document ranking data, we collected the corresponding urls for which the passages were extracted. Using these 3.5 million URLS, we obtained the associated document title and body corresponding to the ranking qrels. It is worth noting that the original passages were extracted between January 2016 and February 2018 while the full documents were extracted in March of 2018 and as a result only 3.2 million URLs were still able to be successfully extracted. From these documents, a clean form was extracted where the body text had the HTML removed and focused on the main content of the page, removing web-page boilerplate such as navigation menus. Since we extracted the document text more than a year later than the passage data and used a completely different document parsing and processing pipeline (which unfortunately had character set processing issues) there was a chance that some pages that had a relevant passage no longer existed, no longer contained the passage, or even had the section of text with the passage accidentally removed as boilerplate. These are all realistic things to happen in a real-world application, where the document corpus is constantly changing, we do not wish to throw away our old relevance labels, and indeed we may not have budget to generate new labels. Doing a better job of generating a clean dataset using old labels is what we have now done in generating the v2 data. Qrels for the document task were assigned by assuming that a relevant passage qrel transfers to the document level as a positive document qrel. We generated top-100 document rankings using Indri, for use in a reranking task and also allowed fullrank from the 3.2 million documents.</p><p>The v1 data had several problems. The corpus was generated based on the queries, such that each passage and each document is in the corpus due to one of our million original queries. For each document in the corpus there may only be one passage in the passage dataset (and on average 2.8 passages per document), but that passage was identified by Bing in relation to one of the MS MARCO queries, possibly a test query. This is unrealistic, since a real system would be able to generate many candidate passages per document, and would not know what the test queries will be ahead of time. Therefore, we had to forbid participants from considering the passage-document mapping. The document dataset had several problems with character sets and missing whitespace.</p><p>MS MARCO ranking v2 datasets. The MS MARCO passage and document ranking v2 datasets were used for the first time in TREC 2021. The goal of the v2 dataset was to increase the scale and introduce a wider variety of documents such that not all documents were relevant to at least some query.</p><p>While the v1 data started with passages and was expanded to documents, the v2 data is document native. It begun by identifying documents based on the source urls of the v1 dataset. Of the original 3.5 million MS MARCO URLs, we were able to still find content for 2.7 million. We added an additional 9.2 million documents, selected to be the kind of documents that had useful passages of text in past Bing queries, giving a total of 11.9 million documents. For each document we ran a query-independent proprietary algorithm for identifying promising passages, and selected the best non-overlapping passages, giving on average 11.6 passages per document. This gives us our 138 million passages in the v2 passage corpus. We mapped the document qrels at the URL level, for training, dev and eval. The chance that the document is no longer relevant to the query, which also was a concern in v1 data, is now increased since the document content was extracted at a later date. We can consider how big this problem is by analyzing the disagreement rate between MS MARCO qrels and NIST qrels (in v1 and v2), and seeing whether training on MS MARCO qrels yields improved NIST NDCG on the test set. For mapping passage qrels, we required that the passage comes from the same URL as the original passage, and has sufficient text similarity to the positive passage text from v1.</p><p>It is now possible for participants to use the passage-document mapping in participation, for example by considering document information in passage ranking, passage information in document ranking, and so on. Using a larger corpus prevents participants from proposing completely unscalable ranking approaches. The new dataset has fewer character encoding and whitespace issues, and could form the basis for future tasks that include some elements of additional document processing, such as extracting even shorter (phrase) answers.</p><p>This year we had fewer participating groups (14 groups) compared to previous years (15 groups in 2019, 25 in 2020, and 19 in 2021). However, we received a larger number of runs this year (142 runs) compared to previous years (75 runs in 2019, 123 in 2020, and 129 in 2021). A larger number of baseline runs this year (59 runs) contributed towards this growth compared to previous years (16 runs in 2019, 34 in 2020, and 37 in 2021). The number of official runs this year (83 runs) was slightly lower than the previous two years (89 runs in 2020 and 92 in 2021) but higher than the inaugural year of the track (59 runs in 2019).</p><p>This year we asked participants to self-classify each of their runs under the following three categories (same taxonomy as was employed in our previous track overview papers <ref type="bibr" coords="7,294.83,157.32,83.51,8.64" target="#b2">[Craswell et al., 2020</ref><ref type="bibr" coords="7,385.31,157.32,22.37,8.64">[Craswell et al., , 2021a</ref><ref type="bibr" coords="7,414.64,157.32,20.95,8.64">[Craswell et al., , 2022]]</ref>):</p><p>• trad: No neural representation learning-e.g., classical learning to rank, PRF, and BM25</p><p>• nn: Representation learning with text as input, but not using a pre-trained model</p><p>• nnlm: Using a pre-trained model in any part of the pipeline-e.g., neural document expansion and BERT-style reranking</p><p>The largest category of runs was of type "nnlm" constituting 85% of submissions across both tasks this year. This was a significant increase over previous years-44% in 2019, 57% in 2020, and 76% in 2021-while the percentage of "trad" runs dipped this year to 15% after having remained relatively stable over the previous years-29% in 2019, 33% in 2020, and 24% in 2021. A significant shift also happened for the "nn" category over the previous years, decreasing from 27% in 2019 to 10% in 2020 and altogether disappearing as a category last year and this year. This may reflect a convergence in the neural IR community, and the IR community in general, towards large language models, although whether this homogenization of approaches is healthy or premature is yet to be seen.</p><p>Participants were also asked to categorize their runs based on subtasks:</p><p>• Rerank: Reranking the official top-100 candidates</p><p>• Fullrank: Full ranking from the collection (retrieval)</p><p>We observed an increase in the percentage of "fullrank" runs this year-90% compared to 72% in 2019, 70% in 2020, and 79% in 2021. The percentage of "fullrank" runs for the passage ranking task increased again this year-89% this year compared to 70% in 2019, 69% in 2020, and 81% in 2021-which may have been partially influenced since last year by the reduction in size of the official reranking candidate set for the passage ranking task from 1000 (as in the first two years of the track) to 100 last year and this year. The growing percentage of "fullrank" runs may also be due to increasing application of neural methods in the full ranking setting-either using dense retrieval methods <ref type="bibr" coords="7,521.75,432.12,18.25,8.64;7,72.00,443.03,47.88,8.64" target="#b9">[Lee et al., 2019]</ref> or query term independent neural ranking models <ref type="bibr" coords="7,323.01,443.03,74.17,8.64" target="#b13">[Mitra et al., 2019]</ref>. The percentage of "fullrank" runs also increased for the document ranking task this year-93% this year compared to 74% in 2019, 70% in 2020, and 77% in 2021. Coincidentally, this year, we also asked participants to tell us (i) if their runs employed dense retrieval methods, and (ii) if the retrieval was performed in a single-stage under full retrieval setting. We received 9 single-stage dense retrieval runs for the passage ranking task this year and 2 for the document ranking task.</p><p>Overall results Table ?? and Table ?? present a standard set of relevance quality metrics for document and passage ranking runs, respectively, as we have reported for the track in previous years. The reported metrics include Normalized Discounted Cumulative Gain (NDCG) <ref type="bibr" coords="7,282.94,531.90,129.07,8.64" target="#b8">[Järvelin and Kekäläinen, 2002]</ref>, Normalized Cumulative Gain (NCG) <ref type="bibr" coords="7,102.05,542.81,77.83,8.64" target="#b15">[Rosset et al., 2018]</ref>, and Average Precision (AP) <ref type="bibr" coords="7,299.77,542.81,45.67,8.64" target="#b18">[Zhu, 2004]</ref>. These are all computed using NIST judgments, since this year's test queries do not have the sparse judgments that we used in previous years.</p><p>In subsequent discussions, we employ NDCG@10 as our primary evaluation metric to analyze ranking quality produced by different methods. To analyze how different approaches compare beyond just the relevance of top-ranked results, we use NCG@100, which correlates more with how often relevant results are in the top-100 candidate set even if they are not eventually ranked as highly.</p><p>Neural vs. traditional methods. Figure <ref type="figure" coords="7,245.07,626.24,4.98,8.64">2</ref> summarizes the evaluation results by run type-i.e., comparing "nnlm" vs. "trad" runs. Across both document and passage ranking tasks, "nnlm" runs dramatically outperform "trad" runs this year. For the passage ranking task, the best performing "nnlm" run improves NDCG@10 over the best performing "trad" run by 125%, while the same was 38% in 2019, 42% in 2020, and 36% in 2021. On the other hand, for the document ranking task, the NDCG@10 gap between the best performing run in 'nnlm" and "trad" categories is 76% this year, compared to 29% in 2019, 23% in 2020, and 15% in 2021. Comparing percentage improvements across different year's tracks or across different tasks in the same year is not very meaningful due to differences in underlying data distributions. However, we posit that the selection of more difficult test queries this year may have contributed to the seemingly increased gap between "nnlm" and "trad" run performances.   Figure <ref type="figure" coords="9,101.34,476.80,3.88,8.64">2</ref>: NDCG@10 results by run type. As in the previous two years, "nnlm" runs continue to outperform over "trad" runs for both tasks.</p><p>Figure <ref type="figure" coords="9,101.86,528.78,4.98,8.64">3</ref> and 4 show a query-level comparison between the best "nnlm" and "trad" runs for the passage and the document ranking tasks, respectively. The best "nnlm" run outperforms the best "trad run" on 74 out of 76 (97%) queries for the passage ranking task-a big jump from 84% in 2019, 88% in 2020, and 89% in 2021. For the document ranking task, the best "nnlm" run wins on 71 out of 76 (93%) queries against the best "trad" run, which is again much higher than 84% in 2019 and 2020, and 72% in 2021.</p><p>Full ranking vs. reranking. This year for the passage ranking task, the best "fullrank" run has a 36% NDCG@10 improvement over the best "rerank" run, compared to 4% improvement in 2019, no improvement in 2020, and 6% improvement in 2021. For the document task this year, the best "fullrank" run has 125% higher NDCG@10 than the best "rerank" run, which we can compare with a 1% improvement in 2019, 5% improvement in 2020, and 4% improvement in 2021. If we compare Figure <ref type="figure" coords="9,262.46,648.06,4.98,8.64" target="#fig_3">5</ref> (a) and (c) (and similarly Figure <ref type="figure" coords="9,408.97,648.06,4.98,8.64" target="#fig_3">5</ref> (b) and (d)), we also notice a stronger correlation between NDCG@10 and NCG@100 metrics compared to previous years. While we reiterate that comparing percentage improvements across different tasks and across different years are not very meaningful, we note that these differences between best performing "fullrank" and "rerank" are particularly large this year compared to previous years of the track. There may be many contributing factors including, but not limited to: (i) Potential recent progress by the community in the "fullrank" setting, (ii) increased difficulty of this year's test set, and/or (iii) less interest from participating groups this year in the "rerank" setting leading to under-optimized "rerank" runs.  Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top.   <ref type="formula" coords="12,406.97,405.13,3.69,8.64">c</ref>) and (d) plot the NCG@100 for the same. We order the runs by their NDCG@10 performance along the x-axis in all four plots. The best run for both tasks correspond to the "fullrank" setting. This year the best single-stage dense retrieval run was 23% worse on NDCG@10 compared to the best passage ranking run and 36% worse on NDCG@10 compared to the best document ranking run. Again these numbers are very different compared to last year's where the best single-stage dense retrieval run was behind the best run on NDCG@10 by only 10% for passage ranking and 6% for document ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of near duplicates</head><p>The main motivation for processing near duplicates was to increase the number of distinct passages that would be seen in the assessing process. During judging we used deduped runs, that had been processed to retain only a single instance of each near-dupe cluster.</p><p>To think about the effect of dupes, we can consider four approaches:</p><p>• Do nothing: In previous years we left the near-dupes in the runs and potentially judged multiple results from the same near-dupe cluster.</p><p>• Dedupe corpus: It would be possible to dedupe the corpus ahead of time, reducing each near-dupe cluster to a single canonical passage ID, and perhaps somehow patching the document-passage mapping to still have a coherent collection. We have never done corpus-level deduping in any task for this track.</p><p>• Dedupe runs: During judging we mapped all passages from each near-dupe cluster to a single canonical passage ID. The evaluation with deduped runs should be similar to that for deduping the corpus, but not identical, since deduped top-100 lists may no longer have 100 results and some ranking methods may change when corpus statistics change.</p><p>• Expand qrels: After judging with dedupe runs, we expanded the qrels so that every passage in each labeled near-dupe cluster gets the same label, although judges only saw one of them. The evaluation with expanded qrels should be similar to the "do nothing" case above, since runs contain dupes and multiple near-dupe results can be labeled. Qrel expansion can generate a very large number of labels if something is labeled from a very large near-dupe cluster.</p><p>To examine the effect of duplicates, we compared evaluation with expanded qrels to evaluation with deduped runs.</p><p>The expanded qrels, which are the official qrels, have 386,416 judgments. Contributing to the large number of qrels are a few very large duplicate clusters. Table <ref type="table" coords="13,254.95,588.04,4.98,8.64" target="#tab_4">4</ref> shows the total number of passages with judgments for deduped runs and expanded qrels, as well as the number of passages with a non-zero relevance value in each qrels. For eight topics there is some judgment for a large duplicate class, leading to a large number of total expanded (E) qrels. One topic has a that class judged at level 1 and another topic has that class judged at level 2, but for most topics we either saw no large class or a large class at judgment level 0.</p><p>The effect of deduped runs vs expanded qrels on system scoring is shown in Figure <ref type="figure" coords="13,400.23,648.06,3.74,8.64">6</ref>. In the figure, the score computed for a run using deduped runs is plotted on the x-axis while the score computed for the run using the expanded qrels is plotted on the y-axis. Each dot in the plot represents one run, and each run submitted by a given group is plotted in the same color. P@10 scores are shown in the graph on the left of the figure, and nDCG@10 scores are shown in the graph on the right. The absolute value of the nDCG scores are very similar using the two qrels as evidenced by most dots lying on the diagonal line; relative scores between runs thus also preserved. There are somewhat more differences in scores for P@10, though the differences are still not very large and relative scores are still mostly stable. The fact that the absolute value of the scores is greater for the expanded qrels (most dots are above the line) for P@10 is an indication that runs did retrieve multiple instances of relevant near-duplicate passages in the top 10 ranks.</p><p>Overall, a major disadvantage of the deduped runs approach is that any future use of this year's test collection would require the runs also to be deduped, adding complexity and the chance of errors. Therefore, the official results and qrels use expanded qrels this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Reusability of test collection</head><p>In the 2021 track, we identified two problems related to having too many relevant results in the corpus <ref type="bibr" coords="14,481.26,631.67,58.74,8.64;14,72.00,642.58,24.90,8.64" target="#b16">Voorhees et al. [2022]</ref>. One problem is reusability. Reusing a test collection means using the same corpus, queries and judgments to evaluate a new IR system, but without doing any additional judging. Because there were too many relevant results in 2021 we ran out of judging resources, and there were too many unjudged relevant results. That means when we evaluate the NDCG@10 of the new IR system, and it retrieves some unjudged documents, there is too much uncertainty about whether the unjudged documents are relevant.</p><p>Our preferred way to handle this is to judge the pools and iteratively judge more candidates identified via classifier, until it is becoming hard to find additional relevant results, making us confident that we can evaluate a new system by correctly assuming that unjudged results are irrelevant. Based on simulation of leaving out certain runs, we established a rule of thumb for reaching sufficiently complete judgments, that the a query's relevance density should be 0.4 or lower. The relevance density is the proportion of judgments that are positive, using the binarization scheme described in Section 2.</p><p>The other problem with having too many relevant is the saturation of IR metrics. For example, if most IR systems are already getting a Precision@10 of 1.0 for a query, then the query is not useful in evaluation, particularly to identify small but significant differences in relevance between top-performing systems.</p><p>To handle these problems of reusability and saturation, we took a number of steps in 2022. Rather than doing separate judging efforts for passage and document tasks, we put all our judging resources into the passage task, and inferred document labels based on passage labels. This made it less likely that we would run out of judging resources before reaching our 0.4 relevance density threshold. We also ran a deduping algorithm, to avoid wasting judging resources on judging the same passage multiple times.</p><p>We also changed the query selection. The original MS MARCO data had one million queries, for which we had Bing's top-10 passages. The passage retrieval was of quite high quality and the passages were deduped by Bing. In all versions of MS MARCO, these Bing results were used to populate the corpus, even in v2 where we included as many URLs from Bing as were available. This also allowed us to evaluate using MS MARCO sparse labels because our test queries had at least one positively labeled result, and every v1 and v2 corpus was constructed to include as many Bing results as possible.</p><p>The change in query selection was to use some queries that were run through the same MS MARCO annotation, but the Bing top-10 was not used in corpus construction in v1 or v2. This means we no longer can evaluate the MS MARCO reciprocal rank, because although we have sparse labels, they are not in the corpus. It does make the queries more difficult, because rather than having 10 non-duplicated passages and their source URLs in the corpus, we now are not guaranteed to have any such results. This also makes the task more realistic, because in a real-world IR task we are not guaranteed that the corpus was augmented with Bing results, of course.</p><p>Our first analysis is of the relevance density, to see how many queries are now below our 0.4 rule of thumb indicating "sufficiently complete" judgments. Figure <ref type="figure" coords="15,242.23,375.60,4.98,8.64">7</ref> compares the passage task in 2022 to 2021. Sorting the queries each year by their relevance density, we can see that 2022 had more queries judged in total, and none of them had relevance density of greater than 0.4. They all met the stopping condition. By contrast in 2021 there were 17 topics that didn't reach the stopping condition. For more detail on the reusability of 2021 data, focused on the 2021 document judging, see <ref type="bibr" coords="15,87.21,419.24,86.14,8.64" target="#b16">Voorhees et al. [2022]</ref>.</p><p>This year there were 24,004 passage judgments, and although it could have theoretically been possible for none of those judgments to be on duplicate passages, around 2% of judgments this year were duplicates. Last year there were 10,828 judgments, 1715 of which were duplicates, around 15%. So, if we hadn't done deduping we could have had 64 topics rather than 76. More analysis is needed to understand how much extra statistical power we got, avoiding spending judging resources on duplicates, but the task would still have been possible without deduping. By contrast, had we kept the document task and spent half our budget on it, we could have had 38 topics reach their stopping condition. This is not enough, since we normally hope to have at least 50 topics for each task. We may have included some topics that didn't reach the stopping condition, having a relevance density plot more like the 2021 curve in Figure <ref type="figure" coords="15,100.50,522.90,3.74,8.64">7</ref>. So deduping was helpful, but eliminating document judging was crucial.</p><p>To understand the saturation due to too many relevant, consider the per-query metrics of all runs. Considering Precision@10 (Figure <ref type="figure" coords="15,150.62,550.20,4.15,8.64">8</ref>) we replicate the figure from <ref type="bibr" coords="15,278.70,550.20,89.86,8.64" target="#b16">Voorhees et al. [2022]</ref> showing that the 2021 document task has several queries where the median is 1.0. In the 2022 task, there are still some queries with this property, but fewer of them. In general there are more queries where the top runs have different Precision@10. We note though that the document task used different judging in the two years, with direct judging of documents in 2021, and inferred document juding in 2022.</p><p>For the passage task, which used the same judging scheme both years, we also see a few queries in 2021 that had a median P@10 of 1.0. In 2022 no query has median 1.0. Figure <ref type="figure" coords="15,324.77,621.13,4.98,8.64">9</ref> shows the same analysis for NDCG@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This is the fourth year of the TREC Deep Learning track. This year the goal was to create a complete collection which could reliably be used to evaluate the performance of different retrieval methods in the passage ranking task. By leveraging a set of harder topics, focused judgement, and passage deduplication the 2022 passage collection is a reusable collection. We also continued to observe healthy participation in the track although the number of participat- ing groups reduced slightly this year due to the delay in releasing the test queries. Deep learning models with large scale pretraining continued to outperform traditional retrieval methods, and single stage retrieval with deep models seems to gain some more ground this year. This report summarizes our analysis of submitted runs and the observed (mostly positive) impact of the changes in the track this year on building a more complete and consequently more reusable test collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Results Including Baselines</head><p>Baseline runs are included to enrich the pools and increase the diversity of approaches used in the evaluation. Baselines are not included in the main results tables, but they are included in our other analysis and in the tables in this appendix. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,72.00,708.41,468.00,8.64;11,72.00,719.32,468.00,8.64;11,72.00,730.23,148.50,8.64;11,301.02,743.40,9.96,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Comparison of the best "nnlm" and "trad" runs on individual test queries for the document ranking task. Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top.11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,82.56,202.65,9.79,7.17;12,82.56,184.26,9.79,7.17;12,82.56,165.87,9.79,7.17;12,82.56,147.48,9.79,7.17;12,82.56,129.09,9.79,7.17;12,82.56,110.69,9.79,7.17;12,82.56,92.30,9.79,7.17;12,82.56,73.91,9.79,7.17;12,73.91,124.69,7.17,33.95;12,112.60,88.55,52.51,7.17;12,160.46,123.46,48.91,7.17;12,266.90,81.79,24.50,7.17;12,266.90,90.83,20.90,7.17;12,93.64,219.72,186.05,7.77;12,321.23,202.65,9.79,7.17;12,321.23,184.26,9.79,7.17;12,321.23,165.87,9.79,7.17;12,321.23,147.48,9.79,7.17;12,321.23,129.09,9.79,7.17;12,321.23,110.69,9.79,7.17;12,321.23,92.30,9.79,7.17;12,321.23,73.91,9.79,7.17;12,312.59,124.69,7.17,33.95;12,351.28,80.70,52.51,7.17;12,484.66,158.38,48.91,7.17;12,505.58,81.79,24.50,7.17;12,505.58,90.83,20.90,7.17;12,328.32,219.72,194.03,7.77;12,82.56,360.20,9.79,7.17;12,82.56,338.74,9.79,7.17;12,82.56,317.28,9.79,7.17;12,82.56,295.83,9.79,7.17;12,82.56,274.37,9.79,7.17;12,82.56,252.91,9.79,7.17;12,82.56,231.46,9.79,7.17;12,73.91,282.75,7.17,32.93;12,266.90,239.34,24.50,7.17;12,266.90,248.38,20.90,7.17;12,94.63,377.27,184.06,7.77;12,321.23,360.20,9.79,7.17;12,321.23,338.74,9.79,7.17;12,321.23,317.28,9.79,7.17;12,321.23,295.83,9.79,7.17;12,321.23,274.37,9.79,7.17;12,321.23,252.91,9.79,7.17;12,321.23,231.46,9.79,7.17;12,312.59,282.75,7.17,32.93;12,505.58,239.34,24.50,7.17;12,505.58,248.38,20.90,7.17;12,329.32,377.27,192.04,7.77"><head></head><label></label><figDesc>NCG@100 for runs on the document ranking task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,72.00,394.22,468.00,8.64;12,72.00,405.13,468.00,8.64;12,72.00,415.72,467.99,8.96;12,72.00,426.95,166.02,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparing "fullrank" and "rerank" runs on ranking quality. Figure (a) and (b) plots the NDCG@10 for different runs on the passage and document ranking tasks, respectively, and Figure (c) and (d) plot the NCG@100 for the same. We order the runs by their NDCG@10 performance along the x-axis in all four plots. The best run for both tasks correspond to the "fullrank" setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="14,72.00,283.13,468.00,8.64;14,72.00,294.04,468.00,8.64;14,72.00,304.95,63.36,8.64"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure6: Changes in P@10 (left) and nDCG@10 (right) scores when passage ranking submissions are scored using or not using near duplicate passages. Each dot represents one run and runs with the same color dot were submitted by the same group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="16,212.99,686.75,186.03,8.64"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Precision@10 distribution per query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,130.50,72.00,350.99,234.92"><head></head><label></label><figDesc></figDesc><graphic coords="5,130.50,72.00,350.99,234.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,131.36,80.12,346.79,120.52"><head>Table 1 :</head><label>1</label><figDesc>TREC 2022 Deep Learning Track run submission statistics.</figDesc><table coords="6,316.84,93.04,161.30,8.96"><row><cell>Passage ranking Document ranking</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,110.26,146.54,391.48,503.64"><head>Table 2 :</head><label>2</label><figDesc>Summary of results for passage ranking runs. (For baselines seeAppendix A.)    </figDesc><table coords="8,110.26,160.93,391.48,489.26"><row><cell>run</cell><cell>group</cell><cell>subtask</cell><cell>neural</cell><cell>stage</cell><cell>dense ret.</cell><cell>NDCG@10</cell><cell>NCG@100</cell><cell>AP</cell></row><row><cell>pass3</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.7184</cell><cell>0.4313</cell><cell>0.2818</cell></row><row><cell>NLE_SPLADE_CBERT_DT5_RR</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.7145</cell><cell>0.4592</cell><cell>0.2950</cell></row><row><cell>NLE_SPLADE_CBERT_RR</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.7141</cell><cell>0.4565</cell><cell>0.2963</cell></row><row><cell>pass2</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.7105</cell><cell>0.4007</cell><cell>0.2577</cell></row><row><cell>NLE_SPLADE_RR</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.7092</cell><cell>0.4589</cell><cell>0.2977</cell></row><row><cell>pass1</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.7050</cell><cell>0.4007</cell><cell>0.2442</cell></row><row><cell>f_sum_mdt5</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.7030</cell><cell>0.3993</cell><cell>0.2698</cell></row><row><cell>srchvrs_pz2_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.6630</cell><cell>0.3660</cell><cell>0.2160</cell></row><row><cell>srchvrs_ptn1_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.6562</cell><cell>0.3660</cell><cell>0.2066</cell></row><row><cell>uogtr_se_gb</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.6508</cell><cell>0.3825</cell><cell>0.2252</cell></row><row><cell>uogtr_se_gt</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.6508</cell><cell>0.3824</cell><cell>0.2256</cell></row><row><cell>uogtr_e_gb</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.6501</cell><cell>0.3818</cell><cell>0.2257</cell></row><row><cell>uogtr_be_gb</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.6480</cell><cell>0.3558</cell><cell>0.2113</cell></row><row><cell>srchvrs_ptn2_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.6448</cell><cell>0.3660</cell><cell>0.2002</cell></row><row><cell>srchvrs_pz1_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.6414</cell><cell>0.3501</cell><cell>0.2096</cell></row><row><cell>srchvrs_ptn1_lcn_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.6367</cell><cell>0.3501</cell><cell>0.1996</cell></row><row><cell>uogtr_e_cprf_t5</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.6182</cell><cell>0.3621</cell><cell>0.2061</cell></row><row><cell>yorku22a</cell><cell>yorku22</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.6089</cell><cell>0.3747</cell><cell>0.2003</cell></row><row><cell>srchvrs_p2_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.6010</cell><cell>0.3492</cell><cell>0.1745</cell></row><row><cell>2systems</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5991</cell><cell>0.2958</cell><cell>0.1622</cell></row><row><cell>unicoil_reranked</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5910</cell><cell>0.2958</cell><cell>0.1605</cell></row><row><cell>cip_f2_r</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5860</cell><cell>0.3393</cell><cell>0.1761</cell></row><row><cell>cip_f3_r</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5852</cell><cell>0.3266</cell><cell>0.1708</cell></row><row><cell>srchvrs_p1_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5818</cell><cell>0.3400</cell><cell>0.1723</cell></row><row><cell>srchvrs_ptn3_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5800</cell><cell>0.3660</cell><cell>0.1687</cell></row><row><cell>6systems</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5783</cell><cell>0.3218</cell><cell>0.1604</cell></row><row><cell>4systems</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5761</cell><cell>0.2959</cell><cell>0.1530</cell></row><row><cell>c47</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5701</cell><cell>0.2958</cell><cell>0.1493</cell></row><row><cell>hierarchcal_combination</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5696</cell><cell>0.3554</cell><cell>0.1655</cell></row><row><cell>uogtr_s_cprf</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5682</cell><cell>0.3501</cell><cell>0.1866</cell></row><row><cell>p_dhr</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.5524</cell><cell>0.3420</cell><cell>0.1662</cell></row><row><cell>graph_colbert</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5482</cell><cell>0.3545</cell><cell>0.1656</cell></row><row><cell>tuvienna-pas-col</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.5386</cell><cell>0.3331</cell><cell>0.1677</cell></row><row><cell>webis-dl-duot5-g</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5314</cell><cell>0.1501</cell><cell>0.0887</cell></row><row><cell>NLE_ENSEMBLE_SUM</cell><cell>NLE</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5286</cell><cell>0.1826</cell><cell>0.0948</cell></row><row><cell>NLE_ENSEMBLE_CONDORCET</cell><cell>NLE</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5284</cell><cell>0.1826</cell><cell>0.0943</cell></row><row><cell>p_agg</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.5282</cell><cell>0.3119</cell><cell>0.1461</cell></row><row><cell>tuvienna-pas-unicol</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.5231</cell><cell>0.3212</cell><cell>0.1518</cell></row><row><cell>cip_f1</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.5121</cell><cell>0.3393</cell><cell>0.1469</cell></row><row><cell>NLE_T0pp</cell><cell>NLE</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5102</cell><cell>0.1826</cell><cell>0.0881</cell></row><row><cell>fused_3runs</cell><cell>UGA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5094</cell><cell>0.1826</cell><cell>0.0901</cell></row><row><cell>uogtr_t_cprf</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5078</cell><cell>0.3250</cell><cell>0.1646</cell></row><row><cell>yorku22b</cell><cell>yorku22</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.5076</cell><cell>0.2692</cell><cell>0.1130</cell></row><row><cell>uogtr_c_cprf</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5075</cell><cell>0.2488</cell><cell>0.1355</cell></row><row><cell>cip_f1_r</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5072</cell><cell>0.3563</cell><cell>0.1622</cell></row><row><cell>fused_2runs</cell><cell>UGA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5060</cell><cell>0.1826</cell><cell>0.0895</cell></row><row><cell>hierarchical_2runs</cell><cell>UGA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5001</cell><cell>0.1826</cell><cell>0.0885</cell></row><row><cell>cip_f2</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.4997</cell><cell>0.3563</cell><cell>0.1429</cell></row><row><cell>cip_r2</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4975</cell><cell>0.1826</cell><cell>0.0891</cell></row><row><cell>webis-dl-duot5</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4972</cell><cell>0.1501</cell><cell>0.0800</cell></row><row><cell>webis-dl-duot5-aug-1</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4925</cell><cell>0.1226</cell><cell>0.0781</cell></row><row><cell>webis-dl-duot5-aug-2</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4885</cell><cell>0.1226</cell><cell>0.0759</cell></row><row><cell>Infosense-2</cell><cell>InfoSense</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4848</cell><cell>0.1826</cell><cell>0.0846</cell></row><row><cell>cip_f3</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.4840</cell><cell>0.3266</cell><cell>0.1357</cell></row><row><cell>Infosense-1</cell><cell>InfoSense</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4832</cell><cell>0.1826</cell><cell>0.0830</cell></row><row><cell>cip_r3</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4669</cell><cell>0.1826</cell><cell>0.0795</cell></row><row><cell>IELab-3MP-UT</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.4658</cell><cell>0.2888</cell><cell>0.1101</cell></row><row><cell>IELab-3MP-RBC</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.4368</cell><cell>0.3220</cell><cell>0.1013</cell></row><row><cell>cip_r1</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.4320</cell><cell>0.1826</cell><cell>0.0719</cell></row><row><cell>IELab-3MP-DI</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>0.4148</cell><cell>0.2663</cell><cell>0.0832</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,73.91,80.12,444.47,369.83"><head>Table 3 :</head><label>3</label><figDesc>Summary of results for document ranking runs. (For baselines seeAppendix A.)    </figDesc><table coords="9,73.91,94.51,444.47,355.44"><row><cell></cell><cell>run</cell><cell></cell><cell>group</cell><cell>subtask</cell><cell>neural</cell><cell>stage</cell><cell>dense ret.</cell><cell>NDCG@10</cell><cell>NCG@100</cell><cell>AP</cell></row><row><cell></cell><cell cols="2">NLE_SPLADE_RR_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.7611</cell><cell>0.5787</cell><cell>0.3453</cell></row><row><cell></cell><cell cols="2">NLE_SPLADE_CBERT_RR_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.7601</cell><cell>0.5716</cell><cell>0.3387</cell></row><row><cell></cell><cell cols="2">NLE_SPLADE_CBERT_DT5_RR_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.7598</cell><cell>0.5782</cell><cell>0.3405</cell></row><row><cell></cell><cell>doc3</cell><cell></cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.7488</cell><cell>0.5246</cell><cell>0.2997</cell></row><row><cell></cell><cell cols="2">srchvrs_dtn1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5970</cell><cell>0.3492</cell><cell>0.1816</cell></row><row><cell></cell><cell cols="2">NLE_ENSEMBLE_SUM_doc</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5918</cell><cell>0.2593</cell><cell>0.1619</cell></row><row><cell></cell><cell cols="2">srchvrs_dtn2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5888</cell><cell>0.3492</cell><cell>0.1798</cell></row><row><cell></cell><cell cols="2">NLE_ENSEMBLE_CONDORCE_doc</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5882</cell><cell>0.2593</cell><cell>0.1609</cell></row><row><cell></cell><cell cols="2">NLE_T0pp_doc</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5843</cell><cell>0.2593</cell><cell>0.1587</cell></row><row><cell></cell><cell cols="2">srchvrs_d_lb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5760</cell><cell>0.3492</cell><cell>0.1777</cell></row><row><cell></cell><cell cols="2">srchvrs_d_lb1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5754</cell><cell>0.3492</cell><cell>0.1782</cell></row><row><cell></cell><cell cols="2">srchvrs_d_prd3</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5620</cell><cell>0.3492</cell><cell>0.1742</cell></row><row><cell></cell><cell cols="2">srchvrs_d_prd1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.5546</cell><cell>0.3492</cell><cell>0.1705</cell></row><row><cell></cell><cell cols="2">srchvrs_d_lb3</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.5302</cell><cell>0.2748</cell><cell>0.1407</cell></row><row><cell></cell><cell>doc1</cell><cell></cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4936</cell><cell>0.4739</cell><cell>0.2154</cell></row><row><cell></cell><cell cols="2">tuvienna</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.4868</cell><cell>0.3043</cell><cell>0.1294</cell></row><row><cell></cell><cell cols="2">tuvienna-unicol</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>0.4830</cell><cell>0.2985</cell><cell>0.1232</cell></row><row><cell></cell><cell>doc2</cell><cell></cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.4589</cell><cell>0.4739</cell><cell>0.2030</cell></row><row><cell></cell><cell cols="2">ceqe_custom_rerank</cell><cell>CERTH_ITI_M4D</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3811</cell><cell>0.2599</cell><cell>0.1090</cell></row><row><cell></cell><cell cols="2">rm3_term_filter_rerank</cell><cell>CERTH_ITI_M4D</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>0.3611</cell><cell>0.2425</cell><cell>0.1049</cell></row><row><cell></cell><cell cols="2">plm_128</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3387</cell><cell>0.2236</cell><cell>0.0905</cell></row><row><cell></cell><cell cols="2">plm_64</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.3227</cell><cell>0.2236</cell><cell>0.0909</cell></row><row><cell></cell><cell cols="2">plm_512</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>0.2721</cell><cell>0.2236</cell><cell>0.0816</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">nnlm</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell>best nnlm run</cell><cell cols="2">trad</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NDCG@10</cell><cell>0.5 0.6</cell><cell></cell><cell>best trad run</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,72.00,81.56,468.00,670.48"><head></head><label></label><figDesc>Comparison of the best "nnlm" and "trad" runs on individual test queries for the passage ranking task. Queries are sorted by difference in mean performance between "nnlm" and "trad" runs. Queries on which "nnlm" wins with large margin are at the top. 10</figDesc><table coords="10,72.00,81.56,457.31,635.48"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>nnlm nnlm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>trad trad</cell></row><row><cell>how do you root a yucca plant why does cat get restless at night how to dilute ceftriaxone vial the population of kings grant fayetteville prior to liberty hills number of instagram post about unicorn frappuccino who is gehan homes why are the cotswolds so named how fast does a rabbit grow definition secondary trauma what are the entities of the executive branch example of what a family advocate does what is unresponsiveness cities near weeki wachee springs when is trial by jury used what are the three countries in 1984 how many incidents of corneal abrasion business architect role definition ebola how can it be prevented yeast infection md how much was the saturn v what to do if your partner is possessive? how to check if someone has a restraining order how much average cost to plan a 8' tree? what is the weight a chevrolet suburban how many people watch nba basketball what is cold bridging description of mesquite plant how do I replace the burners on a ducane grill what is wrong with amazon prime how do i insert notes under a slide in powerpoint collins the good to great syracuse law school ranking what does london breed stand for what is a sulfa treat what does duped mean average bahamas temperature at the end of october who sang the song maybe what does auslan interpreted performance mean code for history of suicidal statements what does septrin heal? what is 311 for how to cook pork tenderloin steaks in oven how much magnolia bark to take for anxiety what do you call someone who writes biographies? why was the massachusetts bay colony founded definition of now what phylum includes the ascaris and the pinworm what is the outside of a shape functions of three regions of sm intestine what hazards come with making paint when a house goes into foreclosure what happens to items on the premises how to trim blank space in excel what can you do with heart of palm what vaccination should u give show piglets how to age steak what tax form do you get from short term disability home child care definition what is peace in french? what is the name of a baby nurse what phone to use in europe define: monsieur how to cook frozen ham steak on nuwave oven what does quotient of a power mean people who have fled the us because trump how often should you take your toddler to the potty when potty training how to transfer deposit to another account online westpac what does chicken by-product meal add to pet food? how much money do i need in bangkok how does magic leap optics work what type of videos can powerpoint play how to put together a scuba regulator motivational theories how to make ground beef empanadas where is the us open golf held this year define unstop what to do if antibiotics cause nausea how fast does a rabbit grow what does duped mean where is the us open golf held this year what is the weight a chevrolet suburban what does chicken by-product meal add to pet food? why are the cotswolds so named people who have fled the us because trump what to do if your partner is possessive? who is gehan homes how do you root a yucca plant how to put together a scuba regulator how to cook pork tenderloin steaks in oven how much was the saturn v what are the entities of the executive branch why was the massachusetts bay colony founded how does magic leap optics work average bahamas temperature at the end of october what is the outside of a shape how to transfer deposit to another account online westpac what phylum includes the ascaris and the pinworm home child care definition what tax form do you get from short term disability yeast infection md define: monsieur code for history of suicidal statements what is cold bridging how often should you take your toddler to the potty when potty training the population of kings grant fayetteville prior to liberty hills what is 311 for what are the three countries in 1984 what is wrong with amazon prime how much magnolia bark to take for anxiety what is peace in french? motivational theories define unstop how do I replace the burners on a ducane grill syracuse law school ranking business architect role definition description of mesquite plant how to age steak what does quotient of a power mean ebola how can it be prevented why does cat get restless at night how do i insert notes under a slide in powerpoint who sang the song maybe what is unresponsiveness example of what a family advocate does what do you call someone who writes biographies? when a house goes into foreclosure what happens to items on the premises what type of videos can powerpoint play what hazards come with making paint how to trim blank space in excel what is the name of a baby nurse collins the good to great how to cook frozen ham steak on nuwave oven cities near weeki wachee springs what is a sulfa treat how much average cost to plan a 8' tree? number of instagram post about unicorn frappuccino what phone to use in europe what does london breed stand for what does auslan interpreted performance mean what to do if antibiotics cause nausea how to check if someone has a restraining order what does septrin heal? how to make ground beef empanadas functions of three regions of sm intestine when is trial by jury used what vaccination should u give show piglets what can you do with heart of palm definition of now how many incidents of corneal abrasion how many people watch nba basketball how to dilute ceftriaxone vial definition secondary trauma how much money do i need in bangkok</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0 Figure 3: 0.0</cell><cell>0.2 0.2</cell><cell>0.4 NDCG@10 0.6 0.4 0.6 NDCG@10</cell><cell>0.8 0.8</cell><cell>1.0 1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,72.00,80.12,516.06,418.92"><head>Table 4 :</head><label>4</label><figDesc>Per-query counts of number of judged passages and respective labels for the deduped runs (O) and expanded qrels (E).</figDesc><table coords="13,77.98,102.33,510.08,396.72"><row><cell>Query</cell><cell>Total</cell><cell cols="2">Level 1</cell><cell cols="2">Level 2</cell><cell cols="2">Level 3</cell><cell>Query</cell><cell>Total</cell><cell cols="2">Level 1</cell><cell cols="2">Level 2</cell><cell cols="2">Level 3</cell></row><row><cell>O</cell><cell>E</cell><cell>O</cell><cell>E</cell><cell>O</cell><cell>E</cell><cell>O</cell><cell>E</cell><cell>O</cell><cell>E</cell><cell>O</cell><cell>E</cell><cell>O</cell><cell>E</cell><cell>O</cell><cell>E</cell></row><row><cell>2000511 252</cell><cell>1492</cell><cell>62</cell><cell>149</cell><cell>17</cell><cell>17</cell><cell>0</cell><cell>0</cell><cell>2030608 309</cell><cell>453</cell><cell>25</cell><cell>32</cell><cell>16</cell><cell cols="2">23 17</cell><cell>21</cell></row><row><cell cols="3">2000719 286 42710 115</cell><cell>127</cell><cell>41</cell><cell cols="3">43 47 52</cell><cell cols="6">2031726 425 43117 279 42865 107 180</cell><cell>0</cell><cell>0</cell></row><row><cell>2001532 319</cell><cell cols="2">369 107</cell><cell>112</cell><cell>74</cell><cell cols="3">92 30 38</cell><cell>2032090 256</cell><cell cols="2">267 120</cell><cell>128</cell><cell>34</cell><cell>34</cell><cell>0</cell><cell>0</cell></row><row><cell>2001908 226</cell><cell>276</cell><cell>9</cell><cell>9</cell><cell>17</cell><cell>18</cell><cell>0</cell><cell>0</cell><cell>2032949 384</cell><cell>460</cell><cell>73</cell><cell>82</cell><cell>39</cell><cell cols="2">44 47</cell><cell>57</cell></row><row><cell>2001975 333</cell><cell cols="2">406 127</cell><cell>152</cell><cell>35</cell><cell cols="3">46 68 74</cell><cell>2032956 347</cell><cell>403</cell><cell>5</cell><cell>5</cell><cell>11</cell><cell>11</cell><cell>1</cell><cell>1</cell></row><row><cell>2002146 224</cell><cell>270</cell><cell>13</cell><cell>15</cell><cell>40</cell><cell>49</cell><cell>0</cell><cell>0</cell><cell>2033232 361</cell><cell>387</cell><cell>8</cell><cell>10</cell><cell>10</cell><cell>11</cell><cell>1</cell><cell>1</cell></row><row><cell>2002269 282</cell><cell cols="2">312 134</cell><cell>149</cell><cell>51</cell><cell cols="3">54 24 27</cell><cell>2033396 261</cell><cell cols="2">677 104</cell><cell>129</cell><cell>65</cell><cell cols="2">73 20</cell><cell>23</cell></row><row><cell>2002533 423</cell><cell>888</cell><cell>2</cell><cell>2</cell><cell>9</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>2033470 321</cell><cell>583</cell><cell>66</cell><cell>69</cell><cell>42</cell><cell cols="3">44 83 104</cell></row><row><cell>2002798 266</cell><cell cols="2">322 181</cell><cell>226</cell><cell>18</cell><cell>22</cell><cell>0</cell><cell>0</cell><cell>2034205 356</cell><cell cols="2">461 102</cell><cell>123</cell><cell cols="2">75 100</cell><cell>0</cell><cell>0</cell></row><row><cell>2003157 222</cell><cell cols="2">246 138</cell><cell>153</cell><cell>2</cell><cell>2</cell><cell cols="2">8 11</cell><cell>2034676 339</cell><cell>1243</cell><cell>44</cell><cell>424</cell><cell>13</cell><cell>19</cell><cell>2</cell><cell>2</cell></row><row><cell>2003322 232</cell><cell>369</cell><cell>92</cell><cell>174</cell><cell>25</cell><cell cols="3">34 55 75</cell><cell>2035009 381</cell><cell>410</cell><cell>26</cell><cell>29</cell><cell>8</cell><cell>10</cell><cell>5</cell><cell>6</cell></row><row><cell>2003976 235</cell><cell>271</cell><cell>47</cell><cell>56</cell><cell>36</cell><cell>41</cell><cell>2</cell><cell>3</cell><cell>2035447 345</cell><cell>1421</cell><cell>4</cell><cell>5</cell><cell>23</cell><cell>25</cell><cell>1</cell><cell>1</cell></row><row><cell>2004237 331</cell><cell cols="2">414 169</cell><cell>201</cell><cell>61</cell><cell cols="3">78 23 24</cell><cell>2035565 281</cell><cell>341</cell><cell>62</cell><cell>70</cell><cell>25</cell><cell cols="2">31 39</cell><cell>57</cell></row><row><cell>2004253 268</cell><cell cols="2">283 149</cell><cell>158</cell><cell>54</cell><cell cols="3">58 19 20</cell><cell>2036182 318</cell><cell>4615</cell><cell>72</cell><cell>74</cell><cell>12</cell><cell>12</cell><cell>0</cell><cell>0</cell></row><row><cell>2005810 317</cell><cell>513</cell><cell>61</cell><cell>90</cell><cell>57</cell><cell>82</cell><cell cols="2">7 11</cell><cell>2036968 248</cell><cell>328</cell><cell>96</cell><cell>128</cell><cell>53</cell><cell cols="2">69 27</cell><cell>35</cell></row><row><cell>2005861 366</cell><cell>773</cell><cell>29</cell><cell>31</cell><cell>11</cell><cell cols="3">11 42 47</cell><cell>2037251 338</cell><cell>376</cell><cell>29</cell><cell>39</cell><cell>38</cell><cell cols="2">40 47</cell><cell>49</cell></row><row><cell>2006211 241</cell><cell cols="2">296 100</cell><cell>116</cell><cell>10</cell><cell>12</cell><cell cols="2">8 10</cell><cell cols="2">2037609 399 42864</cell><cell>51</cell><cell>56</cell><cell>21</cell><cell>25</cell><cell>4</cell><cell>5</cell></row><row><cell cols="2">2006375 252 42677</cell><cell>78</cell><cell>89</cell><cell>25</cell><cell cols="3">27 74 81</cell><cell>2037924 354</cell><cell>403</cell><cell>12</cell><cell>12</cell><cell>66</cell><cell cols="2">72 11</cell><cell>13</cell></row><row><cell>2006394 340</cell><cell>430</cell><cell>60</cell><cell>77</cell><cell>34</cell><cell>51</cell><cell>0</cell><cell>0</cell><cell>2038466 266</cell><cell cols="2">315 233</cell><cell>282</cell><cell>11</cell><cell>11</cell><cell>9</cell><cell>9</cell></row><row><cell>2006627 284</cell><cell>637</cell><cell>31</cell><cell>36</cell><cell>22</cell><cell cols="3">22 27 29</cell><cell>2038890 246</cell><cell>720</cell><cell>4</cell><cell>183</cell><cell>1</cell><cell>1</cell><cell>9</cell><cell>9</cell></row><row><cell>2007055 304</cell><cell cols="2">444 137</cell><cell>203</cell><cell>76</cell><cell>98</cell><cell>0</cell><cell>0</cell><cell>2039908 324</cell><cell>417</cell><cell>59</cell><cell>68</cell><cell>17</cell><cell cols="2">23 77</cell><cell>99</cell></row><row><cell>2007419 340</cell><cell>391</cell><cell>96</cell><cell>109</cell><cell>19</cell><cell cols="3">21 17 18</cell><cell>2040287 177</cell><cell>204</cell><cell>30</cell><cell>33</cell><cell>24</cell><cell cols="2">28 25</cell><cell>30</cell></row><row><cell>2008871 382</cell><cell cols="2">460 128</cell><cell cols="2">153 120</cell><cell>141</cell><cell>2</cell><cell>2</cell><cell>2040352 255</cell><cell>378</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell cols="2">6 24</cell><cell>46</cell></row><row><cell>2009553 216</cell><cell>258</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>1</cell><cell>1</cell><cell>2040613 285</cell><cell>358</cell><cell>37</cell><cell>38</cell><cell>32</cell><cell cols="2">33 23</cell><cell>24</cell></row><row><cell>2009871 282</cell><cell cols="2">341 106</cell><cell>126</cell><cell>35</cell><cell cols="3">43 55 59</cell><cell>2043895 318</cell><cell cols="2">350 171</cell><cell>185</cell><cell cols="3">95 107 11</cell><cell>13</cell></row><row><cell>2012431 318</cell><cell>412</cell><cell>87</cell><cell>98</cell><cell>54</cell><cell cols="3">80 34 46</cell><cell>2044423 143</cell><cell>178</cell><cell>6</cell><cell>6</cell><cell>4</cell><cell>5</cell><cell>7</cell><cell>12</cell></row><row><cell>2012536 416</cell><cell cols="2">491 245</cell><cell cols="2">285 108</cell><cell>133</cell><cell>0</cell><cell>0</cell><cell>2045272 423</cell><cell>487</cell><cell>94</cell><cell cols="3">114 109 121</cell><cell>5</cell><cell>5</cell></row><row><cell cols="2">2013306 313 43946</cell><cell>26</cell><cell>31</cell><cell cols="4">4 42396 10 12</cell><cell>2046371 346</cell><cell cols="2">486 108</cell><cell cols="3">144 124 166</cell><cell>8</cell><cell>13</cell></row><row><cell>2016333 320</cell><cell>351</cell><cell>74</cell><cell>78</cell><cell>19</cell><cell>22</cell><cell>1</cell><cell>1</cell><cell>2049417 366</cell><cell>471</cell><cell>39</cell><cell>48</cell><cell>24</cell><cell>30</cell><cell>5</cell><cell>8</cell></row><row><cell>2017299 265</cell><cell cols="2">306 185</cell><cell>214</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>5</cell><cell>2049687 258</cell><cell>312</cell><cell>47</cell><cell>50</cell><cell>10</cell><cell cols="2">11 13</cell><cell>15</cell></row><row><cell cols="2">2025747 336 49627</cell><cell cols="2">28 1776</cell><cell>23</cell><cell cols="3">24 25 33</cell><cell>2053884 255</cell><cell>276</cell><cell>3</cell><cell>3</cell><cell>27</cell><cell>28</cell><cell>0</cell><cell>0</cell></row><row><cell>2026150 320</cell><cell cols="2">384 140</cell><cell>168</cell><cell>36</cell><cell cols="3">42 24 24</cell><cell>2054355 315</cell><cell>387</cell><cell>70</cell><cell>81</cell><cell>37</cell><cell>38</cell><cell>3</cell><cell>4</cell></row><row><cell cols="2">2026558 355 42950</cell><cell>41</cell><cell>46</cell><cell>20</cell><cell>24</cell><cell>3</cell><cell>4</cell><cell>2055211 331</cell><cell>386</cell><cell>36</cell><cell>44</cell><cell>45</cell><cell>49</cell><cell>2</cell><cell>2</cell></row><row><cell>2027130 332</cell><cell cols="2">459 100</cell><cell>125</cell><cell>41</cell><cell cols="3">53 80 92</cell><cell>2055480 240</cell><cell>329</cell><cell>14</cell><cell>18</cell><cell>6</cell><cell>8</cell><cell>6</cell><cell>6</cell></row><row><cell>2027497 409</cell><cell cols="2">499 187</cell><cell>228</cell><cell>76</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>2055634 240</cell><cell cols="2">292 122</cell><cell>144</cell><cell>39</cell><cell cols="2">46 10</cell><cell>13</cell></row><row><cell>2028378 381</cell><cell cols="2">486 111</cell><cell>132</cell><cell>88</cell><cell cols="3">129 48 64</cell><cell>2055795 350</cell><cell cols="2">387 143</cell><cell>153</cell><cell>92</cell><cell cols="2">95 20</cell><cell>20</cell></row><row><cell cols="3">2029260 424 44035 112</cell><cell cols="2">118 102</cell><cell cols="3">114 47 49</cell><cell>2056158 476</cell><cell cols="2">535 104</cell><cell cols="3">121 159 179</cell><cell>2</cell><cell>2</cell></row><row><cell>2030323 242</cell><cell>274</cell><cell>75</cell><cell>84</cell><cell>62</cell><cell cols="3">68 33 39</cell><cell>2056323 231</cell><cell cols="2">271 106</cell><cell>121</cell><cell>5</cell><cell>6</cell><cell>2</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="19,92.85,138.22,426.29,501.77"><head>Table 5 :</head><label>5</label><figDesc>Summary of results for passage ranking runs. Including baseline runs. (1/2)</figDesc><table coords="19,92.85,152.60,426.29,487.39"><row><cell>run</cell><cell>group</cell><cell>subtask</cell><cell>neural</cell><cell>stage</cell><cell>dense ret.</cell><cell>baseline</cell><cell>NDCG@10</cell><cell>NCG@100</cell><cell>AP</cell></row><row><cell>pass3</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.7184</cell><cell>0.4313</cell><cell>0.2818</cell></row><row><cell>pass2</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.7105</cell><cell>0.4007</cell><cell>0.2577</cell></row><row><cell>pass1</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.7050</cell><cell>0.4007</cell><cell>0.2442</cell></row><row><cell>cip_f2_r</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5860</cell><cell>0.3393</cell><cell>0.1761</cell></row><row><cell>cip_f3_r</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5852</cell><cell>0.3266</cell><cell>0.1708</cell></row><row><cell>cip_f1</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.5121</cell><cell>0.3393</cell><cell>0.1469</cell></row><row><cell>cip_f1_r</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5072</cell><cell>0.3563</cell><cell>0.1622</cell></row><row><cell>cip_f2</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.4997</cell><cell>0.3563</cell><cell>0.1429</cell></row><row><cell>cip_r2</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4975</cell><cell>0.1826</cell><cell>0.0891</cell></row><row><cell>cip_f3</cell><cell>CIP</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.4840</cell><cell>0.3266</cell><cell>0.1357</cell></row><row><cell>cip_r3</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4669</cell><cell>0.1826</cell><cell>0.0795</cell></row><row><cell>cip_r1</cell><cell>CIP</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4320</cell><cell>0.1826</cell><cell>0.0719</cell></row><row><cell>tuvienna-pas-col</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.5386</cell><cell>0.3331</cell><cell>0.1677</cell></row><row><cell>tuvienna-pas-unicol</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.5231</cell><cell>0.3212</cell><cell>0.1518</cell></row><row><cell>Infosense-2</cell><cell>InfoSense</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4848</cell><cell>0.1826</cell><cell>0.0846</cell></row><row><cell>Infosense-1</cell><cell>InfoSense</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4832</cell><cell>0.1826</cell><cell>0.0830</cell></row><row><cell>NLE_SPLADE_CBERT_DT5_RR</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.7145</cell><cell>0.4592</cell><cell>0.2950</cell></row><row><cell>NLE_SPLADE_CBERT_RR</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.7141</cell><cell>0.4565</cell><cell>0.2963</cell></row><row><cell>NLE_SPLADE_RR</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.7092</cell><cell>0.4589</cell><cell>0.2977</cell></row><row><cell>SPLADE_ENSEMBLE_PP_RCIO</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5991</cell><cell>0.3823</cell><cell>0.2005</cell></row><row><cell>SPLADE_PP_ED_RCIO</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5917</cell><cell>0.3748</cell><cell>0.1923</cell></row><row><cell>SPLADE_PP_SD_RCIO</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5897</cell><cell>0.3748</cell><cell>0.1968</cell></row><row><cell>SPLADE_ENSEMBLE_PP</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5789</cell><cell>0.3784</cell><cell>0.1862</cell></row><row><cell>SPLADE_PP_ED</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5786</cell><cell>0.3680</cell><cell>0.1801</cell></row><row><cell>SPLADE_PP_SD</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5705</cell><cell>0.3702</cell><cell>0.1846</cell></row><row><cell>SPLADE_EFF_V</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5509</cell><cell>0.3419</cell><cell>0.1631</cell></row><row><cell>SPLADE_EFF_V_RCIO</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5452</cell><cell>0.3362</cell><cell>0.1725</cell></row><row><cell>NLE_ENSEMBLE_SUM</cell><cell>NLE</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5286</cell><cell>0.1826</cell><cell>0.0948</cell></row><row><cell>NLE_ENSEMBLE_CONDORCET</cell><cell>NLE</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5284</cell><cell>0.1826</cell><cell>0.0943</cell></row><row><cell>SPLADE_EFF_VI-BT</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5271</cell><cell>0.3210</cell><cell>0.1452</cell></row><row><cell>NLE_T0pp</cell><cell>NLE</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5102</cell><cell>0.1826</cell><cell>0.0881</cell></row><row><cell>SPLADE_EFF_VI-BT_RCIO</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5084</cell><cell>0.3061</cell><cell>0.1452</cell></row><row><cell>2systems</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5991</cell><cell>0.2958</cell><cell>0.1622</cell></row><row><cell>unicoil_reranked</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5910</cell><cell>0.2958</cell><cell>0.1605</cell></row><row><cell>6systems</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5783</cell><cell>0.3218</cell><cell>0.1604</cell></row><row><cell>4systems</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5761</cell><cell>0.2959</cell><cell>0.1530</cell></row><row><cell>c47</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5701</cell><cell>0.2958</cell><cell>0.1493</cell></row><row><cell>hierarchcal_combination</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5696</cell><cell>0.3554</cell><cell>0.1655</cell></row><row><cell>graph_colbert</cell><cell>UGA</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5482</cell><cell>0.3545</cell><cell>0.1656</cell></row><row><cell>fused_3runs</cell><cell>UGA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5094</cell><cell>0.1826</cell><cell>0.0901</cell></row><row><cell>fused_2runs</cell><cell>UGA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5060</cell><cell>0.1826</cell><cell>0.0895</cell></row><row><cell>hierarchical_2runs</cell><cell>UGA</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5001</cell><cell>0.1826</cell><cell>0.0885</cell></row><row><cell>uogtr_se</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6510</cell><cell>0.3826</cell><cell>0.2252</cell></row><row><cell>uogtr_se_gb</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.6508</cell><cell>0.3825</cell><cell>0.2252</cell></row><row><cell>uogtr_se_gt</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.6508</cell><cell>0.3824</cell><cell>0.2256</cell></row><row><cell>uogtr_e_gb</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.6501</cell><cell>0.3818</cell><cell>0.2257</cell></row><row><cell>uogtr_be_gb</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.6480</cell><cell>0.3558</cell><cell>0.2113</cell></row><row><cell>uogtr_be</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6235</cell><cell>0.3252</cell><cell>0.1896</cell></row><row><cell>uogtr_e_cprf_t5</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.6182</cell><cell>0.3621</cell><cell>0.2061</cell></row><row><cell>uogtr_s</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5697</cell><cell>0.3699</cell><cell>0.1831</cell></row><row><cell>uogtr_s_cprf</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5682</cell><cell>0.3501</cell><cell>0.1866</cell></row><row><cell>uogtr_c</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>yes</cell><cell>0.5217</cell><cell>0.2419</cell><cell>0.1319</cell></row><row><cell>uogtr_t_cprf</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5078</cell><cell>0.3250</cell><cell>0.1646</cell></row><row><cell>uogtr_c_cprf</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5075</cell><cell>0.2488</cell><cell>0.1355</cell></row><row><cell>uogtr_dph_bo1</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3050</cell><cell>0.1836</cell><cell>0.0433</cell></row><row><cell>uogtr_dph</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2905</cell><cell>0.1754</cell><cell>0.0410</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="20,107.97,200.30,396.07,551.74"><head>Table 6 :</head><label>6</label><figDesc>Summary of results for passage ranking runs. Including baseline runs. (2/2)</figDesc><table coords="20,107.97,214.68,396.07,537.36"><row><cell>run</cell><cell>group</cell><cell>subtask</cell><cell>neural</cell><cell>stage</cell><cell>dense ret.</cell><cell>baseline</cell><cell>NDCG@10</cell><cell>NCG@100</cell><cell>AP</cell></row><row><cell>webis-dl-duot5-g</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5314</cell><cell>0.1501</cell><cell>0.0887</cell></row><row><cell>webis-dl-duot5</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4972</cell><cell>0.1501</cell><cell>0.0800</cell></row><row><cell>webis-dl-duot5-aug-1</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4925</cell><cell>0.1226</cell><cell>0.0781</cell></row><row><cell>webis-dl-duot5-aug-2</cell><cell>Webis</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.4885</cell><cell>0.1226</cell><cell>0.0759</cell></row><row><cell>f_sum_mdt5</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.7030</cell><cell>0.3993</cell><cell>0.2698</cell></row><row><cell>p_d2q_bm25rocchio_mdt5</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6933</cell><cell>0.3724</cell><cell>0.2374</cell></row><row><cell>p_d2q_bm25rocchio_mt5</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6282</cell><cell>0.3724</cell><cell>0.2122</cell></row><row><cell>p_dhr</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.5524</cell><cell>0.3420</cell><cell>0.1662</cell></row><row><cell>p_tct</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>yes</cell><cell>0.5329</cell><cell>0.3155</cell><cell>0.1540</cell></row><row><cell>p_agg</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.5282</cell><cell>0.3119</cell><cell>0.1461</cell></row><row><cell>p_unicoil_exp_rocchio</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.4886</cell><cell>0.3069</cell><cell>0.1225</cell></row><row><cell>p_unicoil_exp</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.4614</cell><cell>0.2957</cell><cell>0.1050</cell></row><row><cell>p_unicoil_noexp_rocchio</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.4164</cell><cell>0.2552</cell><cell>0.0974</cell></row><row><cell>p_unicoil_noexp</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.4077</cell><cell>0.2383</cell><cell>0.0754</cell></row><row><cell>paug_d2q_bm25rocchio</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3801</cell><cell>0.2527</cell><cell>0.0860</cell></row><row><cell>paug_d2q_bm25rm3</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3754</cell><cell>0.2478</cell><cell>0.0818</cell></row><row><cell>p_d2q_bm25rocchio</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3712</cell><cell>0.2698</cell><cell>0.0868</cell></row><row><cell>p_d2q_bm25rm3</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3704</cell><cell>0.2695</cell><cell>0.0864</cell></row><row><cell>paug_d2q_bm25</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3609</cell><cell>0.2520</cell><cell>0.0735</cell></row><row><cell>p_d2q_bm25</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3599</cell><cell>0.2535</cell><cell>0.0748</cell></row><row><cell>paug_bm25</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2742</cell><cell>0.1684</cell><cell>0.0346</cell></row><row><cell>p_bm25rocchio</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2741</cell><cell>0.1820</cell><cell>0.0340</cell></row><row><cell>p_bm25rm3</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2724</cell><cell>0.1732</cell><cell>0.0326</cell></row><row><cell>p_bm25</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2692</cell><cell>0.1826</cell><cell>0.0325</cell></row><row><cell>paug_bm25rocchio</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2593</cell><cell>0.1506</cell><cell>0.0314</cell></row><row><cell>paug_bm25rm3</cell><cell>h2oloo</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2591</cell><cell>0.1520</cell><cell>0.0318</cell></row><row><cell>IELab-3MP-UT</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>no</cell><cell>0.4658</cell><cell>0.2888</cell><cell>0.1101</cell></row><row><cell>IELab-3MP-RBC</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>no</cell><cell>0.4368</cell><cell>0.3220</cell><cell>0.1013</cell></row><row><cell>IELab-3MP-DI</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>no</cell><cell>0.4148</cell><cell>0.2663</cell><cell>0.0832</cell></row><row><cell>IELab-3MP-DT5</cell><cell>ielab</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3620</cell><cell>0.2480</cell><cell>0.0713</cell></row><row><cell>srchvrs_pz2_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.6630</cell><cell>0.3660</cell><cell>0.2160</cell></row><row><cell>srchvrs_ptn1_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.6562</cell><cell>0.3660</cell><cell>0.2066</cell></row><row><cell>srchvrs_ptn2_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.6448</cell><cell>0.3660</cell><cell>0.2002</cell></row><row><cell>srchvrs_pz1_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.6414</cell><cell>0.3501</cell><cell>0.2096</cell></row><row><cell>srchvrs_ptn1_lcn_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.6367</cell><cell>0.3501</cell><cell>0.1996</cell></row><row><cell>srchvrs_p2_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.6010</cell><cell>0.3492</cell><cell>0.1745</cell></row><row><cell>srchvrs_p1_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5818</cell><cell>0.3400</cell><cell>0.1723</cell></row><row><cell>srchvrs_ptn3_colb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5800</cell><cell>0.3660</cell><cell>0.1687</cell></row><row><cell>srchvrs_p_bm25_mdl1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3194</cell><cell>0.2040</cell><cell>0.0414</cell></row><row><cell>srchvrs_p_bm25f_mdl1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3161</cell><cell>0.2080</cell><cell>0.0415</cell></row><row><cell>srchvrs_p_bm25f</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3153</cell><cell>0.2029</cell><cell>0.0405</cell></row><row><cell>srchvrs_p_bm25</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.2911</cell><cell>0.1801</cell><cell>0.0340</cell></row><row><cell>yorku22a</cell><cell>yorku22</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.6089</cell><cell>0.3747</cell><cell>0.2003</cell></row><row><cell>yorku22b</cell><cell>yorku22</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>no</cell><cell>no</cell><cell>0.5076</cell><cell>0.2692</cell><cell>0.1130</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="21,77.98,205.77,459.57,385.19"><head>Table 7 :</head><label>7</label><figDesc>Summary of results for document ranking runs. Including baseline runs.</figDesc><table coords="21,77.98,220.15,459.57,370.80"><row><cell>run</cell><cell>group</cell><cell>subtask</cell><cell>neural</cell><cell>stage</cell><cell>dense ret.</cell><cell>baseline</cell><cell>NDCG@10</cell><cell>NCG@100</cell><cell>AP</cell></row><row><cell>doc3</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.7488</cell><cell>0.5246</cell><cell>0.2997</cell></row><row><cell>doc1</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.4936</cell><cell>0.4739</cell><cell>0.2154</cell></row><row><cell>doc2</cell><cell>Ali</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.4589</cell><cell>0.4739</cell><cell>0.2030</cell></row><row><cell>ceqe_custom_rerank</cell><cell>CERTH_ITI_M4D</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.3811</cell><cell>0.2599</cell><cell>0.1090</cell></row><row><cell>rm3_term_filter_rerank</cell><cell>CERTH_ITI_M4D</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.3611</cell><cell>0.2425</cell><cell>0.1049</cell></row><row><cell>tuvienna</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.4868</cell><cell>0.3043</cell><cell>0.1294</cell></row><row><cell>tuvienna-unicol</cell><cell>DOSSIER</cell><cell>fullrank</cell><cell>nnlm</cell><cell>single</cell><cell>yes</cell><cell>no</cell><cell>0.4830</cell><cell>0.2985</cell><cell>0.1232</cell></row><row><cell>NLE_SPLADE_RR_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.7611</cell><cell>0.5787</cell><cell>0.3453</cell></row><row><cell>NLE_SPLADE_CBERT_RR_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.7601</cell><cell>0.5716</cell><cell>0.3387</cell></row><row><cell>NLE_SPLADE_CBERT_DT5_RR_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.7598</cell><cell>0.5782</cell><cell>0.3405</cell></row><row><cell>SPLADE_ENSEMBLE_PP_RCIO_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6584</cell><cell>0.5216</cell><cell>0.2933</cell></row><row><cell>SPLADE_PP_ED_RCIO_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6566</cell><cell>0.5122</cell><cell>0.2864</cell></row><row><cell>SPLADE_PP_SD_RCIO_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6471</cell><cell>0.5157</cell><cell>0.2835</cell></row><row><cell>SPLADE_PP_ED_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6448</cell><cell>0.4951</cell><cell>0.2656</cell></row><row><cell>SPLADE_ENSEMBLE_PP_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6402</cell><cell>0.5090</cell><cell>0.2740</cell></row><row><cell>SPLADE_PP_SD_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6269</cell><cell>0.5035</cell><cell>0.2692</cell></row><row><cell>SPLADE_EFF_V_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6094</cell><cell>0.4550</cell><cell>0.2345</cell></row><row><cell>SPLADE_EFF_V_RCIO_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.6031</cell><cell>0.4780</cell><cell>0.2524</cell></row><row><cell>NLE_ENSEMBLE_SUM_doc</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5918</cell><cell>0.2593</cell><cell>0.1619</cell></row><row><cell>NLE_ENSEMBLE_CONDORCE_doc</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5882</cell><cell>0.2593</cell><cell>0.1609</cell></row><row><cell>NLE_T0pp_doc</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5843</cell><cell>0.2593</cell><cell>0.1587</cell></row><row><cell>SPLADE_EFF_VI-BT_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5758</cell><cell>0.4333</cell><cell>0.2152</cell></row><row><cell>SPLADE_EFF_VI-BT_RCIO_D</cell><cell>NLE</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.5755</cell><cell>0.4421</cell><cell>0.2245</cell></row><row><cell>plm_128</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.3387</cell><cell>0.2236</cell><cell>0.0905</cell></row><row><cell>plm_64</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.3227</cell><cell>0.2236</cell><cell>0.0909</cell></row><row><cell>plm_512</cell><cell>UAmsterdam</cell><cell>rerank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.2721</cell><cell>0.2236</cell><cell>0.0816</cell></row><row><cell>uogtr_doc_dph_bo1</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3625</cell><cell>0.2921</cell><cell>0.1199</cell></row><row><cell>uogtr_doc_dph</cell><cell>UoGTr</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3603</cell><cell>0.2847</cell><cell>0.1099</cell></row><row><cell>srchvrs_dtn1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5970</cell><cell>0.3492</cell><cell>0.1816</cell></row><row><cell>srchvrs_dtn2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5888</cell><cell>0.3492</cell><cell>0.1798</cell></row><row><cell>srchvrs_d_lb2</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5760</cell><cell>0.3492</cell><cell>0.1777</cell></row><row><cell>srchvrs_d_lb1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5754</cell><cell>0.3492</cell><cell>0.1782</cell></row><row><cell>srchvrs_d_prd3</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5620</cell><cell>0.3492</cell><cell>0.1742</cell></row><row><cell>srchvrs_d_prd1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>yes</cell><cell>no</cell><cell>0.5546</cell><cell>0.3492</cell><cell>0.1705</cell></row><row><cell>srchvrs_d_lb3</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>nnlm</cell><cell>multi</cell><cell>no</cell><cell>no</cell><cell>0.5302</cell><cell>0.2748</cell><cell>0.1407</cell></row><row><cell>srchvrs_d_bm25_pass_mf</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.4318</cell><cell>0.3140</cell><cell>0.1340</cell></row><row><cell>srchvrs_d_bm25_pass_mdl1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.4269</cell><cell>0.3192</cell><cell>0.1336</cell></row><row><cell>srchvrs_d_bm25_p_mf_mdl1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.4243</cell><cell>0.3035</cell><cell>0.1286</cell></row><row><cell>srchvrs_d_bm25_mf_mdl1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3883</cell><cell>0.2804</cell><cell>0.1082</cell></row><row><cell>srchvrs_d_bm25_mf</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3841</cell><cell>0.2829</cell><cell>0.1116</cell></row><row><cell>srchvrs_d_bm25_mdl1</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>multi</cell><cell>no</cell><cell>yes</cell><cell>0.3817</cell><cell>0.2956</cell><cell>0.1157</cell></row><row><cell>srchvrs_d_bm25</cell><cell>srchvrs</cell><cell>fullrank</cell><cell>trad</cell><cell>single</cell><cell>no</cell><cell>yes</cell><cell>0.3388</cell><cell>0.2742</cell><cell>0.1048</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.14,684.27,57.40,7.77"><p>We note that the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_1" coords="2,167.36,684.27,372.64,7.77;2,72.00,694.24,468.00,7.77;2,72.00,704.20,468.00,7.77;2,72.00,714.16,306.31,7.77"><p>update of the paper<ref type="bibr" coords="2,237.37,684.27,66.08,7.77" target="#b0">[Bajaj et al., 2016]</ref> has an expanded author list, reflecting the expansion of the dataset to one million queries, which was planned by the original 2016 authors, and the addition of a ranking task, which was a new idea in 2018 not planned by the 2016 authors. The 2016 version and author list<ref type="bibr" coords="2,339.20,704.20,76.23,7.77" target="#b14">[Nguyen et al., 2016]</ref> reflect a preliminary release of the MS MARCO data, with 100 thousand queries and a natural language generation task.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,89.93,622.81,102.97,10.75;6,72.00,647.67,430.68,9.03"><p>Results and analysisSubmitted runs A total of 14 groups participated in the TREC 2022 Deep Learning Track. Among them,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,512.88,648.06,27.12,8.64;6,72.00,658.97,468.00,8.64;6,72.00,669.88,468.00,8.64;6,72.00,680.79,468.00,8.64;6,72.00,691.38,468.00,8.96;6,72.00,702.29,468.00,8.96;6,72.00,713.51,79.68,8.64"><p>groups participated in both the passage and the document ranking tasks, and of the remaining seven groups participated only in the passage ranking task and another two groups only in the document ranking task. Similar to previous years, we also solicited baseline runs from the participating groups to enrich the judgment pools. Across all groups, we received a total of 142 run submissions, including 100 passage ranking runs and 42 document ranking runs. This includes 59 baseline runs-40 for passage ranking and 19 for document ranking. Table1summarizes the submissions statistics for this year's track.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="18,72.00,165.14,468.00,8.64;18,81.96,176.05,458.04,8.64;18,81.96,186.78,193.32,8.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="18,276.80,176.05,263.20,8.64;18,81.96,186.96,26.39,8.64">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,72.00,201.85,468.00,8.64;18,81.96,212.58,375.67,8.88" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="18,257.67,201.85,282.33,8.64;18,81.96,212.76,57.87,8.64">Autonomy and reliability of continuous active learning for technologyassisted review</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno>CoRR, abs/1504.06868</idno>
		<ptr target="http://arxiv.org/abs/1504.06868" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,227.66,468.00,8.64;18,81.96,238.39,299.50,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,413.04,227.66,126.96,8.64;18,81.96,238.57,53.81,8.64">Overview of the trec 2019 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,153.88,238.39,197.89,8.59">Proceedings of Text REtrieval Conference (TREC)</title>
		<meeting>Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,253.46,468.00,8.64;18,81.96,264.19,232.01,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,343.01,253.46,181.46,8.64">Overview of the trec 2020 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,81.96,264.19,197.89,8.59">Proceedings of Text REtrieval Conference (TREC)</title>
		<meeting>Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,279.26,468.00,8.64;18,81.96,289.99,298.87,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,400.12,279.26,139.88,8.64;18,81.96,290.17,124.04,8.64">Ms marco: Benchmarking ranking models in the large-data regime</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,224.67,289.99,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1566" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,305.07,468.00,8.64;18,81.96,315.80,428.12,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,499.43,305.07,40.58,8.64;18,81.96,315.98,253.85,8.64">Trec deep learning track: Reusable test collections in the large data regime</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,354.48,315.80,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2369" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,330.87,468.00,8.64;18,81.96,341.60,299.50,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,408.63,330.87,131.38,8.64;18,81.96,341.78,53.81,8.64">Overview of the trec 2021 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,153.88,341.60,197.89,8.59">Proceedings of Text REtrieval Conference (TREC)</title>
		<meeting>Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,356.49,460.86,8.82" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Prashansa</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12852</idno>
		<title level="m" coord="18,232.89,356.67,132.24,8.64">On survivorship bias in ms marco</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,72.00,371.39,465.90,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,194.99,371.57,199.56,8.64">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,402.23,371.39,41.90,8.59">ACM TOIS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,386.46,468.00,8.64;18,81.96,397.19,458.04,8.82;18,81.96,408.28,72.23,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,297.66,386.46,242.34,8.64;18,81.96,397.37,39.09,8.64">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,143.65,397.19,366.17,8.59">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,423.17,468.00,8.64;18,81.96,433.90,458.04,8.82;18,81.96,444.99,26.84,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,401.11,423.17,138.89,8.64;18,81.96,434.08,309.58,8.64">Significant improvements over the state of the art? a case study of the ms marco document ranking leaderboard</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,412.19,433.90,47.23,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2283" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,459.89,468.00,8.64;18,81.96,470.62,458.04,8.82;18,81.96,481.52,458.03,8.82;18,81.96,492.61,458.04,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="18,494.14,459.89,45.87,8.64;18,81.96,470.79,391.29,8.64">Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,490.65,470.62,49.35,8.59;18,81.96,481.52,458.03,8.82;18,81.96,492.61,11.83,8.64">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;21</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2356" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,507.51,468.00,8.64;18,81.96,518.24,365.64,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,392.51,507.51,147.50,8.64;18,81.96,518.42,270.97,8.64">Fostering coopetition while plugging leaks: The design and implementation of the ms marco leaderboards</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,371.41,518.24,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,533.31,468.00,8.64;18,81.96,544.04,458.04,8.82;18,81.96,554.95,100.17,8.82" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="18,485.78,533.31,54.23,8.64;18,81.96,544.22,393.00,8.64">Incorporating query term independence assumption for efficient retrieval and ranking using deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Corby</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03693</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="18,72.00,570.02,468.00,8.64;18,81.96,580.75,458.04,8.88;18,81.96,592.24,96.61,8.30" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="18,484.23,570.02,55.77,8.64;18,81.96,580.93,238.08,8.64">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>CoRR, abs/1611.09268</idno>
		<ptr target="http://arxiv.org/abs/1611.09268" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,606.73,468.00,8.64;18,81.96,617.46,347.47,8.82" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="18,396.79,606.73,143.21,8.64;18,81.96,617.64,150.65,8.64">Optimizing query evaluations using reinforcement learning for web search</title>
		<author>
			<persName coords=""><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,250.98,617.46,46.72,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1193" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,632.36,468.00,8.82;18,81.96,643.27,52.29,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="18,280.21,632.54,221.79,8.64">Too many relevants: Whither cranfield test collections?</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,518.59,632.36,21.41,8.59;18,81.96,643.27,22.82,8.59">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,658.34,468.00,8.64;18,81.96,669.25,458.04,8.64;18,81.96,680.16,92.33,8.64" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="18,359.46,658.34,180.54,8.64;18,81.96,669.25,168.59,8.64">Investigating passage-level relevance and its role in document-level relevance judgment</title>
		<author>
			<persName coords=""><forename type="first">Zhijing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,258.98,669.25,37.14,8.64">SIGIR&apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,694.87,468.00,8.82;18,81.96,705.78,127.40,8.82" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="18,115.84,695.05,159.98,8.64">Recall, precision and average precision</title>
		<author>
			<persName coords=""><forename type="first">Mu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">30</biblScope>
			<pubPlace>Waterloo, 2</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics and Actuarial Science, University of Waterloo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
