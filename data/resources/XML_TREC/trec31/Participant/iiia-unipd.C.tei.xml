<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.58,115.90,334.19,12.68;1,228.76,133.83,157.84,12.68;1,184.09,151.77,247.17,12.68">Question Answering-Based Query Expansion for Conversational Search: IIIA@UNIPD at TREC CAsT 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.98,189.43,81.55,8.80"><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>Via Gradenigo 6/b</addrLine>
									<postCode>35138</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.31,189.43,53.59,8.80"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>Via Gradenigo 6/b</addrLine>
									<postCode>35138</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.63,201.39,79.00,8.80"><forename type="first">Mattia</forename><surname>Romanello</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>Via Gradenigo 6/b</addrLine>
									<postCode>35138</postCode>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.58,115.90,334.19,12.68;1,228.76,133.83,157.84,12.68;1,184.09,151.77,247.17,12.68">Question Answering-Based Query Expansion for Conversational Search: IIIA@UNIPD at TREC CAsT 2022</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FCD6CF852181028C381C70D42BB00DE5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational Search task is becoming ubiquitous in our daily interaction with information systems. Nevertheless, it remains an extremely complex task due to its profoundly different nature from adhoc retrieval, and the challenges presented by the way a user interacts with the system. CAsT TREC Track explicitly aims at fostering the development of conversational systems and the discussion of the linked challenges within the research community. In this work, we describe the approach that we propose to CAsT 2022 to tackle the conversational task. In particular, our approach is based on expanding and rewriting the utterance at hand with the response to the previous utterance, using a QA model to extract it from the first passage retrieved in response to the previous query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The conversational search task is constantly drawing more and more interest from both the academy and industry. Conversational agents that rely on conversational search modules are increasingly more popular, both in the form of vocal assistants but also as textual chatbots meant to help users to deal with simple tasks. In this context, the CAsT TREC track is meant to foster the development of new conversational search systems, while also stimulating the discussion within the community on the rising challenges linked to this specific task. CAsT is currently at is fourth iteration <ref type="bibr" coords="1,283.46,517.93,8.19,8.80" target="#b0">[1]</ref><ref type="bibr" coords="1,291.65,517.93,4.09,8.80" target="#b1">[2]</ref><ref type="bibr" coords="1,295.74,517.93,8.19,8.80" target="#b2">[3]</ref> with increasingly growing interest from the community. Differently from previous years, two tasks were considered in this year's CAsT Track:</p><p>-Primary task: retrieve the candidate response.</p><p>-Mixed initiative task: produce a response to the user's utterance, including clarifying questions, requests for feedback or task elicitation.</p><p>In our experiments, we focus only on the primary task. In the following sections, we describe our approach to the CAsT 2022 TREC track which relies on the following steps.</p><p>-Query expansion based on the previous utterance and the response to it, obtained via a QA approach;</p><p>-Query rewriting based on POS tagging and coreference resolution via pretrained models; -First stage retrieval based on classical approaches that exploit the Lucene implementation of BM25 and LMD; -Re-ranking based on BERT <ref type="bibr" coords="2,275.46,164.92,9.96,8.80" target="#b3">[4]</ref>.</p><p>The work is organized as follows: Section 2 reports the methodology adopted to address the task, Section 3 contains the description of the runs submitted, Section 4 describes the achieved results and, Section 5 draws our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we describe the methodology adopted to tackle the CAsT 2022 TREC Track. Notice that, our focus is only on the primary task. Figure <ref type="figure" coords="2,451.30,265.53,4.98,8.80" target="#fig_0">1</ref> illustrates the complete pipeline. query expansion; utterance rewriting; first stage retrieval; reranking.</p><p>We describe each of these steps in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Expansion</head><p>Being usually self-explanatory, we did not carry out any expansion on the first utterance of the conversation. Therefore, our expansion procedure is applied only starting from the second turn of the conversation. Our pipeline relies on two different languages: Python and Java. More in detail, Java and the Lucene library are used to compute the first stage run, using BM25 <ref type="bibr" coords="3,467.31,166.75,9.96,8.80" target="#b4">[5]</ref>. On the other hand, python is used for more machine-learning-oriented passages in our pipeline, such as the BERT-reranking, Question Answering and carrying out coreference resolution with the NeuralCoref Library.</p><p>To expand the query, we considered the following signals:</p><p>previous utterance; previous response; current utterance</p><p>The rationale behind our approach is that the previous utterance itself can provide the required context (entities) to answer the current utterance. Similarly, the utterance issued by the user in the current turn might be related to the response provided by the system to the user. Notice that, the concept of response can be either a specific entity that represents the answer to the question of the user, or the entire paragraph returned by the system to satisfy the user's information need. We experimented with both solutions. Our empirical findings highlight that using a short answer (often a single entity) produces better results than using the entire paragraph. We, therefore, apply the deepset/roberta-base-squad2<ref type="foot" coords="3,275.95,383.43,3.97,6.16" target="#foot_0">1</ref> pre-trained question-answering model to compute the response to the previous utterance, using the first retrieved paragraph as context.</p><p>Query Rewriting Once the response to the previous utterance has been computed, we rewrite the current utterance with the following approach:</p><p>1. first of all, the strings are concatenated into a single string: previous utterance + previous response + current utterance. 2. We compute the Part Of Speech (POS) tagging of the string computed previously. Such tagging is used to resolve complex scenarios where the coreferences in the current utterances cannot be resolved automatically. 3. We apply a coreference resolution algorithm to obtain a rewritten string that does not contain anaphoras, coreferences or ellipses. 4. The result is further refined, by taking only the last part of the string, which corresponds to the current utterance.</p><p>To carry out the coreference resolution we apply a well-known approach using the pre-trained NeuralCoref model 2 . Notice that, NeuralCoref may fail in solving the coreferences, leaving some pronouns in the sentence untouched. In that case, we apply a greedy algorithm to force the resolution of the coreference. In particular, we consider all the tokens annotated as nouns by the POS tagging procedure (step 2), and replace the remaining pronouns using the token having the highest weight provided by NeuraCoref.</p><p>Similarly, we also noticed that, if the query doesn't end with "?", it is often a clarification question related to the previous utterance. Therefore, in the case that the query was not terminated with a question mark, we append the previous utterance to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Stage Retireval</head><p>To carry out first stage retrieval, we used the Lucene retrieval library<ref type="foot" coords="4,203.25,211.99,3.97,6.16" target="#foot_2">3</ref> with the default parameters for the similarity functions used. In particular, we considered two similarity functions: BM25 and Language Model with Dirichlet (LMD) smoothing. The settings for BM25 are:</p><formula xml:id="formula_0" coords="4,140.99,256.09,48.34,20.51">-k2 = 1.2 -b = 0.75</formula><p>While, in the case of LMD <ref type="bibr" coords="4,269.01,286.70,9.96,8.80" target="#b5">[6]</ref>, we applied the following hyperparameters:</p><formula xml:id="formula_1" coords="4,140.99,305.33,46.04,8.81">-µ = 200</formula><p>Reranking Once documents have been retrieved using the first-stage retrieval function, we re-rank the documents by using the BERT classifier. Notice that we use a pre-trained version of BERT, without additional finetuning. In particular, we use cross-encoder/ms-marco-TinyBERT-L-2-v2 <ref type="foot" coords="4,369.81,362.53,3.97,6.16" target="#foot_3">4</ref> . Besides the documents retrieved by the first-stage approach, we provide to the classification model a second expanded version of the current utterance. In particular, we add to it the list of nouns identified using the POS tagging approach from the first utterance, which is expected to provide the general context of the conversation. Finally, we re-rank the first 75 documents obtained after the first stage retrieval pass using the BERT classifier. We experimented with different cutoffs (30, 50, 75, 100), achieving the best empirical performance for 75 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improved Coreference Resolution</head><p>To allow NeuralCoref to solve pronouns coreferences properly the query must contain all the necessary information and it must be written in a logical sense. The same utterance written in two different ways causes the results to change dramatically. Let's consider the topic 106 of TREC CAsT 2021 as reported in Table <ref type="table" coords="4,177.91,543.35,3.87,8.80" target="#tab_0">1</ref>. Considering utterance 3, the pronoun "it" can refer to the response of the previous utterance or the main argument of the previous utterance. Suppose that NeuralCoref replaces the pronoun "it" with "breast cancer" (the topic of the previous utterance). If the pronoun "it" is replaced with "Lobular Carcinoma" (response to the previous utterance) the result will be different, as it refers to two quite different contexts. To avoid this problem (if the algorithm fails the substitution: the resulting query is wrong) and increase the performance, we used the automatic_resolved_utterance provided by TREC CaST organizers. We use a combination between NeuralCoref and the automatic_resolved_utterance Therefore, if the utterance and its rewritten version were the same, instead of applying the POS-based approach, we resort to using directly the automatic_resolved_utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Improved Query Expansion</head><p>One of the biggest problems of our approach, when applied to the CAsT 2021 dataset, is related to the high number of utterances for which no relevant document was retrieved. We noticed that the problem tends to be highly related to the fact that the retrieved documents tend to be relevant to the previous query, and not to the current one. It is therefore likely that our query expansion procedure, based on concatenating the current utterance with the previous one, while increasing the recall, reduced the precision -which is the main focus of the conversational task. We, therefore, replace the expansion of the query based on the concatenation of the previous utterance, previous response and current utterance, by considering only the current utterance and the response to the previous one, without considering the previous utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs</head><p>We submitted 4 runs to CAsT 2022 TREC track: DEI-run1 : this run is the result of the pipeline as described in 2.1. It first expands the query considering the previous utterance and the response to it, using the QA-based approach. It rewrites the run, using the NeuralCoref based approach. It uses BM25 as a first-stage retrieval model. It re-ranks the result using the BERT approach. DEI-run2 : this run replaces the rewriting of the query described in 2.1, with the one proposed in 2.2. Similarly to the previous case, it expands the query in the same way (considering the previous utterance and the answer to it), it uses BM25 to carry out first-stage retrieval and reranks the result with the BERT approach. DEI-run4 : this run further aims at improving the previous approaches using the improved query expansion approach described in subsection 2.3. It also adopts the same strategy as described in subsection 2.2 to resolve coreferences. We use BM25 as a first-stage retriever. DEI-run5 : The final run is equivalent to DEI-run4, but it employs LMD as a first-stage retriever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="6,162.46,208.20,4.98,8.80" target="#tab_1">2</ref> reports the performance achieved by the submitted runs. It is possible to observe, while they have generally uniform performance, DEI-run4 is the best performing one. Figures 2 to 9 report the difference in performance with respect to the median. Notice that, while in general the approach tend to attain low performance, we can observe that almost all approaches deal correctly with topic 137, while topic 146 is the one on which all versions of the approach fail the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we presented an approach to tackle the conversational search task. In particular, we detail our participation in the CAsT 2022 TREC track. The method, while being unable to achieve high performances, can deal with the task with low computational cost and a low degree of complexity. Notice that, given the limited availability of computational resources, we exploited pretrained models for all the tasks based on machine learning approaches. In the future, we plan to continue our work, by fine-tuning the different components of the pipeline.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,244.53,526.33,126.30,8.80"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The complete pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,195.34,644.42,224.68,8.80"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: DEI-run5 performance with strict evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,149.22,127.30,316.93,72.28"><head>Table 1 :</head><label>1</label><figDesc>topic 106 from CAsT 2022</figDesc><table coords="5,149.22,142.83,316.93,56.75"><row><cell cols="2">Turn Utterance</cell></row><row><cell>1</cell><cell>I just had a breast biopsy for cancer. What are the most common types?</cell></row><row><cell>2</cell><cell>Once it breaks out, how likely is it to spread?</cell></row><row><cell>3</cell><cell>How deadly is it?</cell></row><row><cell></cell><cell>...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.94,274.53,345.48,72.28"><head>Table 2 :</head><label>2</label><figDesc>Results achieved on CAsT 2022 collection by the proposed approaches</figDesc><table coords="6,153.68,290.05,308.00,56.76"><row><cell></cell><cell cols="5">median DEI-run1 DEI-run2 DEI-run4 DEI-run5</cell></row><row><cell>ap (lenient)</cell><cell>0.1749</cell><cell>0.1483</cell><cell>0.1505</cell><cell>0.1505</cell><cell>0.1355</cell></row><row><cell cols="2">ndcg@20 (lenient) 0.3172</cell><cell>0.2758</cell><cell>0.2711</cell><cell>0.2759</cell><cell>0.2505</cell></row><row><cell>ap (strict)</cell><cell>0.1479</cell><cell>0.1294</cell><cell>0.1286</cell><cell>0.1281</cell><cell>0.1172</cell></row><row><cell>ndcg@20 (strict)</cell><cell>0.3204</cell><cell>0.2758</cell><cell>0.2711</cell><cell>0.2759</cell><cell>0.2505</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.79,210.89,7.92"><p>https://huggingface.co/deepset/roberta-base-squad2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.74,177.97,7.92"><p>https://github.com/huggingface/neuralcoref</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,645.79,107.57,7.92"><p>https://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,656.74,269.25,7.92"><p>https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L-2-v2</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,602.48,342.24,7.92;6,146.91,613.44,199.47,7.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,278.21,602.48,36.05,7.92">Trec cast</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13624</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,340.15,602.48,140.44,7.92;6,146.91,613.44,33.24,7.92">The conversational assistance track overview</title>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,138.35,624.13,342.24,7.92;6,146.91,635.09,180.52,7.92" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,287.79,624.13,192.80,7.92;6,146.91,635.09,33.24,7.92">Cast 2020: The conversational assistance track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="6,138.35,645.79,342.24,7.92;6,146.91,656.74,180.52,7.92" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,287.79,645.79,192.80,7.92;6,146.91,656.74,33.24,7.92">Cast 2021: The conversational assistance track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,138.35,119.62,342.24,7.92;11,146.91,130.58,333.68,7.92;11,146.91,141.54,333.68,7.92;11,146.91,152.50,333.68,7.92;11,146.91,163.46,321.92,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,342.00,119.62,138.60,7.92;11,146.91,130.58,211.03,7.92">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,381.57,130.58,99.03,7.92;11,146.91,141.54,333.68,7.92;11,146.91,152.50,254.05,7.92">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s" coord="11,281.27,163.46,90.13,7.92">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,174.42,342.24,7.92;11,146.91,185.37,333.68,7.93;11,146.91,196.33,297.01,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,302.40,174.42,178.19,7.92;11,146.91,185.37,86.78,7.92">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
		<ptr target="https://doi.org/10.1561/1500000019" />
	</analytic>
	<monogr>
		<title level="j" coord="11,248.58,185.37,120.83,7.92">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,207.29,342.25,7.92;11,146.91,218.25,333.68,7.92;11,146.91,229.20,24.94,7.93;11,188.04,229.21,32.25,7.92;11,236.49,229.21,17.91,7.92;11,270.59,229.21,24.57,7.92;11,311.36,229.21,169.23,7.92;11,146.91,240.17,166.68,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,270.15,207.29,210.45,7.92;11,146.91,218.25,258.75,7.92">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130348.3130377</idno>
		<ptr target="https://doi.org/10.1145/3130348.3130377" />
	</analytic>
	<monogr>
		<title level="j" coord="11,418.90,218.25,61.70,7.92">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2017-08">aug 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
