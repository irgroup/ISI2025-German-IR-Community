<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,56.03,83.74,499.06,15.10">Webis at TREC 2022: Deep Learning and Health Misinformation</title>
				<funder ref="#_PHD6Wkk">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder ref="#_EgjmWUB">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,80.23,113.49,112.73,10.59;1,194.94,110.76,1.38,7.53"><forename type="first">Alexander</forename><surname>Bondarenko</surname></persName>
						</author>
						<author>
							<persName coords="1,275.45,113.49,55.47,10.59;1,332.92,110.76,1.38,7.53"><forename type="first">Maik</forename><surname>Fröbe</surname></persName>
						</author>
						<author>
							<persName coords="1,433.07,113.49,73.02,10.59;1,508.08,110.76,1.38,7.53"><forename type="first">Lukas</forename><surname>Gienapp</surname></persName>
						</author>
						<author>
							<persName coords="1,85.91,151.99,100.36,10.59;1,188.26,149.26,1.38,7.53"><forename type="first">Alexander</forename><surname>Pugachev</surname></persName>
						</author>
						<author>
							<persName coords="1,252.78,151.99,99.63,10.59;1,354.40,149.26,1.38,7.53"><forename type="first">Jan</forename><forename type="middle">Heinrich</forename><surname>Reimer</surname></persName>
						</author>
						<author>
							<persName coords="1,429.62,151.99,85.55,10.59"><forename type="first">Ferdinand</forename><surname>Schlatt</surname></persName>
						</author>
						<author>
							<persName coords="1,90.28,202.45,98.65,10.59"><forename type="first">Ekaterina</forename><surname>Artemova</surname></persName>
						</author>
						<author>
							<persName coords="1,177.10,250.65,79.45,10.59"><forename type="first">Pavel</forename><surname>Braslavski</surname></persName>
						</author>
						<author>
							<persName coords="1,344.41,250.65,77.61,10.59"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Schiller-Universität Jena</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Friedrich-Schiller-Universität Jena</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">HSE University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Martin-Luther-Universität Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Martin-Luther-Universität Halle-Wittenberg</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Ludwig-Maximilians-Universität München Martin Potthast Leipzig University and ScaDS.AI Benno Stein Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">HSE University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Ural Federal University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Friedrich-Schiller-Universität Jena</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,56.03,83.74,499.06,15.10">Webis at TREC 2022: Deep Learning and Health Misinformation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">518F0F768F0FB64BDC0DACD611CB4D22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the Webis group's participation in the TREC 2022 Deep Learning and Health Misinformation tracks. Our runs submitted to the Deep Learning track focus on improving the pairwise retrieval model duoT5 by combining a greedy aggregation algorithm with document augmentation. In the Health Misinformation track, our submissions to the Answer Prediction task exploit pre-trained question answering and claim verification models, whose input is extended by evidence information from PubMed abstracts. For the Web Retrieval task, we explore re-ranking based on the closeness of the predicted answers for each web document in the initial ranking to the predicted "true" answer of a topic's question.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We participated in two TREC 2022 tracks: Deep Learning and Health Misinformation. As for the Deep Learning track, with our four runs we investigate whether the default aggregation of pairwise preferences in duoT5 can be further improved. The default implementation of duoT5 already is very effective by deriving and symsum-wise aggregating pairwise preferences for all possible pairs of documents. We investigate whether a different greedy aggregation scheme or whether deriving each pairwise preference multiple times using perturbed variants of the query or the documents can help. Our results show that a greedy aggregation improves the effectiveness of duoT5 substantially, but calculating the pairwise preferences multiple times with perturbations does not improve the effectiveness.</p><p>As for the Health Misinformation track, in our 20 runs (10 for each task) we use several pre-trained question answering (QA) and scientific claim verification systems to predict a "correct" yes/no answer to a topic's question. As input to the systems, we use PubMed abstracts to add evidence information (context) from trustworthy sources to the questions. The predicted answer is then used as an estimated true answer to construct rankings where documents are simultaneously sorted by their topical relevance and the predicted correctness of the contained information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DEEP LEARNING TRACK</head><p>We submitted the results of four approaches to the TREC Deep Learning track. All four systems are implemented in PyTerrier <ref type="bibr" coords="1,543.61,368.05,14.60,7.94" target="#b15">[16]</ref> where we first re-rank the task's official top-100 document candidates using monoT5 <ref type="bibr" coords="1,402.34,389.96,13.49,7.94" target="#b18">[19]</ref>. Then, we calculate duoT5 preference scores for all pairs (sometimes in multiple variants) of documents in the top-50 of the monoT5 ranking and compare the official duoT5 aggregation of the duoT5 ranking with a greedy aggregation algorithm. Two variants calculate for each document pair multiple pairwise preferences where we create augmentations of querydocument-document triples (e.g., replacing the original query with queries generated via docT5query <ref type="bibr" coords="1,445.33,466.68,14.77,7.94" target="#b16">[17]</ref> pre-calculated by Ma et al. <ref type="bibr" coords="1,317.96,477.64,12.87,7.94" target="#b13">[14]</ref>). We use ir datasets <ref type="bibr" coords="1,408.18,477.64,14.63,7.94" target="#b14">[15]</ref> to access the passages for re-ranking. We used existing models from Hugging Face for MonoT5 1 and DuoT5 2 which are trained on version 1 of MS MARCO (i.e., we do not train models). Using models trained on version 1 is recommended <ref type="bibr" coords="1,349.38,521.47,10.57,7.94" target="#b4">[5]</ref> (e.g., version 2 has more noisy labels <ref type="bibr" coords="1,498.31,521.47,9.14,7.94" target="#b5">[6]</ref>). Calculating all pairwise preferences (including all augmentations) took roughly 5 hours on a single core of an A100 GPU.</p><p>We submitted four runs out of which three were pooled and one is the baseline:</p><p>Webis-dl-duot5. We re-rank the top-100 candidates of the official baseline with monoT5 and re-rank the top-50 of the monoT5 ranking with duoT5. We use the implementation of monoT5 and duoT5 in PyTerrier with the models trained on version 1 of MS MARCO mentioned above.</p><p>Webis-dl-duot5-g. We re-rank the top-100 candidates of the official baseline with monoT5 and re-rank the top-50 of the monoT5 ranking using duoT5 and greedy aggregation. We use the implementation of monoT5 and duoT5 in PyTerrier with the models Table <ref type="table" coords="2,77.24,86.41,3.38,7.94">1</ref>: The effectiveness of our four runs for the re-ranking scenario in the TREC Deep Learning track. We report nDCG@10 and the mean reciprocal rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run nDCG@10 MRR</head><p>Webis-dl-duot5 0.497 0.711 Webis-dl-duot5-g 0.531 0.823 Webis-dl-duot5-aug-1 0.493 0.713 Webis-dl-duot5-aug-2 0.489 0.648 trained on version 1 of MS MARCO mentioned above. To aggregate the pairwise preferences into retrieval scores, we use a greedy approach proposed by Cohen et al. <ref type="bibr" coords="2,187.69,217.23,9.43,7.94" target="#b3">[4]</ref>, as previous experiments showed that greedy aggregation is more effective than the default sym-sum aggregation <ref type="bibr" coords="2,138.39,239.15,9.52,7.94" target="#b8">[9]</ref>. The greedy aggregation algorithm is proven to closely approximate the best total order in terms of the number of violated preferences <ref type="bibr" coords="2,170.12,261.07,9.39,7.94" target="#b3">[4]</ref>.</p><p>Webis-dl-duot5-aug-1. We re-rank the top-25 results of our webisdl-duot5-g run by aggregating multiple pairwise preferences obtained via duoT5 on augmented document pairs. Out of 9 augmentation patterns ((1) no augmentation, <ref type="bibr" coords="2,181.59,310.38,8.64,7.94" target="#b1">(2,</ref><ref type="bibr" coords="2,192.46,310.38,6.02,7.94" target="#b2">3,</ref><ref type="bibr" coords="2,200.71,310.38,6.70,7.94" target="#b3">4)</ref> using only the first one, two, or three sentence(s) of the passages, and <ref type="bibr" coords="2,222.00,321.34,8.85,7.94" target="#b4">(5,</ref><ref type="bibr" coords="2,233.10,321.34,6.17,7.94" target="#b5">6,</ref><ref type="bibr" coords="2,241.50,321.34,6.17,7.94" target="#b6">7,</ref><ref type="bibr" coords="2,249.92,321.34,6.17,7.94" target="#b7">8,</ref><ref type="bibr" coords="2,258.34,321.34,6.87,7.94" target="#b8">9)</ref> expand the passages with a query obtained via docT5query), two methods to aggregate pairwise scores (greedy and sym-sum), and five methods to aggregate augmented scores (min, max, mean, median, sum), we selected the combination with the highest nDCG@10 on the TREC 2020 DL data. This hyperparameter optimization yielded an approach that used five augmentations ((1) no augmentation, (2) both passages in a comparison shortened to the first two sentences, (3-5) variants of passage expansions) that are aggregated into a single pairwise score using min aggregation, and the pairwise scores are aggregated into retrieval scores using greedy aggregation.</p><p>Webis-dl-duot5-aug-2. We re-rank the top-25 results of our webisdl-duot5-g run by aggregating multiple pairwise preferences obtained via duoT5 on augmented document pairs. Out of 9 augmentation patterns ((1) no augmentation, <ref type="bibr" coords="2,193.86,480.24,8.99,7.94" target="#b1">(2,</ref><ref type="bibr" coords="2,205.19,480.24,6.26,7.94" target="#b2">3,</ref><ref type="bibr" coords="2,213.79,480.24,6.98,7.94" target="#b3">4)</ref> using only the first one, two, or three sentence(s) of the passages, and <ref type="bibr" coords="2,247.91,491.20,8.99,7.94" target="#b4">(5,</ref><ref type="bibr" coords="2,259.88,491.20,6.26,7.94" target="#b5">6,</ref><ref type="bibr" coords="2,269.12,491.20,6.26,7.94" target="#b6">7,</ref><ref type="bibr" coords="2,278.36,491.20,6.26,7.94" target="#b7">8,</ref><ref type="bibr" coords="2,287.61,491.20,6.98,7.94" target="#b8">9)</ref> expand the passages with a query obtained via docT5query), two methods to aggregate pairwise scores (greedy and sym-sum), and five methods to aggregate augmented scores (min, max, mean, median, sum), we selected the combination with the highest MRR on the TREC 2020 DL data. This hyperparameter optimization yielded an approach that used six augmentations ((1) no augmentation, (2,3) both passages in a comparison shortened to the first one and first three sentences, (4-6) variants of passage expansions) that are aggregated into a single pairwise score using sum aggregation, and the pairwise scores are aggregated into retrieval scores using greedy aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation</head><p>Table <ref type="table" coords="2,76.37,646.62,4.25,7.94">1</ref> shows the effectiveness of our four approaches in terms of nDCG@10 and the mean reciprocal rank (MRR). We follow the recommendation of the organizers of the shared task and report evaluation results with duplicate documents (even when not removing duplicates might come with disadvantages <ref type="bibr" coords="2,247.81,690.46,9.44,7.94" target="#b6">[7,</ref><ref type="bibr" coords="2,259.98,690.46,6.08,7.94" target="#b7">8]</ref>). Both augmentation runs decrease the effectiveness of duoT5. However, the greedy variant substantially improves upon the original duoT5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HEALTH MISINFORMATION TRACK</head><p>In our 10 runs submitted to the Answer Prediction task, we use various pre-trained QA and claim verification systems to infer correct answers to the yes/no health questions from 50 topics by aggregating the answers predicted for the top-𝑘 PubMed abstracts retrieved when using a topic's question field as a query. We also use the predicted answers as candidate "true" answers in our 10 runs submitted to the Web Retrieval task. In our ranking approaches, we order documents by combining document topical relevance with the predicted correctness of the contained information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Answer Prediction Task: Our Runs</head><p>To predict a "correct" answer to the 50 yes/no health questions like "Are vaccines linked to autism?", we test pre-trained question answering and claim verification models. As input to the models, we use a topic's question field and relevant evidence information extracted from trustworthy sources like PubMed. <ref type="foot" coords="2,497.62,296.85,3.38,6.44" target="#foot_0">3</ref>Runs using QA models. For each topic, we first retrieve 20 or 1000 PubMed abstracts as evidence candidates by submitting topics' questions to one of the following retrieval systems: (1) PubMed API, <ref type="foot" coords="2,333.67,348.36,3.38,6.44" target="#foot_1">4</ref> (2) Google Custom Search API, 5 or (3) BM25 retrieval <ref type="bibr" coords="2,543.35,350.51,14.85,7.94" target="#b20">[21]</ref> (Elasticsearch<ref type="foot" coords="2,366.94,359.32,3.38,6.44" target="#foot_2">6</ref> implementation) on an index of 33.5 million PubMed abstracts. <ref type="foot" coords="2,351.69,370.28,3.38,6.44" target="#foot_3">7</ref> For each question-abstract pair, we then let a QA model predict an answer score between 0 (no) and 1 (yes). As QA models, we use (1) a BioLinkBERT-large model <ref type="bibr" coords="2,467.25,394.34,14.85,7.94" target="#b24">[25]</ref> pre-trained on Pub-MedQA <ref type="bibr" coords="2,349.18,405.30,13.49,7.94" target="#b10">[11]</ref>, <ref type="foot" coords="2,366.04,403.15,3.38,6.44" target="#foot_4">8</ref> (2) a RoBERTa-BoolQ-base model <ref type="bibr" coords="2,499.15,405.30,14.85,7.94" target="#b12">[13]</ref> pre-trained on the BoolQ dataset [2],<ref type="foot" coords="2,407.44,414.11,3.38,6.44" target="#foot_5">9</ref> and (3) a UnifiedQA-T5-large model <ref type="bibr" coords="2,543.61,416.26,14.59,7.94" target="#b11">[12]</ref> pre-trained on various question answering datasets. <ref type="foot" coords="2,510.27,425.07,6.76,6.44" target="#foot_6">10</ref> We do not fine tune the models. As the final answer score for the topics' questions (in the range from 0 to 1), our runs use different ways of aggregating the predicted answer scores for each evidence document. Following the task requirements, we include in each run a numerical score and a binary yes/no answer label (using a decision threshold of 0.5). Our submitted runs are:</p><p>Webis-goo-boolq-abs. For each task topic, we use the question field as a query to the Google Custom Search API (limited to a search in PubMed). For each of the up top-20 returned PubMed abstracts, the pre-trained RoBERTa-BoolQ-base model predicts the probability of whether the abstract is on the 'yes'-side of an answer. The final answer prediction score for a topic is the average of the individual answer scores across all abstracts.</p><p>Webis-goo-lbert-abs. Analogous to the previous run, but using the pre-trained BioLinkBERT-large QA model.</p><p>Webis-goo-lbert-title-abs. Analogous to the previous run, but prepending a returned abstract's title to the abstract content before passing to the QA model.</p><p>Webis-nlm-boolq-abs. Similar to the first run, but using the native PubMed search API as the retrieval system and a topic's query fields as queries (since the API does not process well natural language questions). For topics where no or only one result is retrieved, we reformulated the query by hand (e.g., using synonyms) until at least two abstracts are found.</p><p>Webis-nlm-lbert-abs. Analogous to the previous run, but the pretrained BioLinkBERT-large QA model is used.</p><p>Webis-uniqa-dis. For this run, we use BM25 to retrieve 1000 abstracts from the PubMed abstracts index (topic question fields are used as queries). Then, we re-rank the results with monoT5 <ref type="bibr" coords="3,264.49,245.70,14.60,7.94" target="#b18">[19]</ref> and again re-rank the top-50 results (of the first re-ranking step) with duoT5 <ref type="bibr" coords="3,78.74,267.62,13.29,7.94" target="#b18">[19]</ref>. <ref type="foot" coords="3,95.36,265.47,6.76,6.44" target="#foot_7">11</ref> We use PyTerrier <ref type="bibr" coords="3,169.90,267.62,14.67,7.94" target="#b15">[16]</ref> to implement re-ranking. The predicted answer scores returned for 1000 question-abstract pairs using the UnifiedQA-T5-large model are aggregated in the topic's final answer prediction score by discounting ranking positions, assuming that answers from higher ranked (i.e., more relevant) abstracts might be closer to the true answer and thus should contribute more to the final topic's answer score.</p><p>We aggregate the topic answer score as follows: <ref type="bibr" coords="3,245.52,344.33,9.70,7.94" target="#b0">(1)</ref> Given the predicted answer scores score 𝑖 for the abstract at rank 𝑖 we compute the discounted cumulative answer score DCA for top-𝑘 documents:</p><formula xml:id="formula_0" coords="3,133.13,380.79,80.39,24.75">DCA 𝑘 = 𝑘 ∑︁ 𝑖=1 score 𝑖 log 2 𝑖 + 1</formula><p>Then, (2) the normalization factor is computed as the maximum achievable (ideal) discounted cumulative answer score IDCA:</p><formula xml:id="formula_1" coords="3,131.80,437.98,83.05,24.75">IDCA 𝑘 = 𝑘 ∑︁ 𝑖=1 1 log 2 𝑖 + 1</formula><p>Finally, (3) we use the normalized discounted cumulative answer score nDCA 𝑘 (with 𝑘 = 1000) as the prediction score for the questions from each topic:</p><formula xml:id="formula_2" coords="3,140.96,502.46,63.66,20.17">nDCA 𝑘 = DCA 𝑘 IDCA 𝑘</formula><p>Runs using claim verification models. For each topic's question, we first retrieve 1000 abstracts from the index of 33.5 million PubMed abstracts using BM25 (topic question fields are used as queries). Then, we re-rank the top-1000 results with monoT5 followed by re-ranking with duoT5 the top-50 results from the first re-ranking step (analogous to the Webis-uniqa-dis run). For each question-abstract pair, we collect predicted answer scores returned by the claim verification model LongChecker <ref type="bibr" coords="3,222.86,606.44,13.56,7.94" target="#b22">[23,</ref><ref type="bibr" coords="3,238.66,606.44,11.57,7.94" target="#b23">24]</ref> pre-trained on the FEVER dataset <ref type="bibr" coords="3,138.70,617.39,14.85,7.94" target="#b21">[22]</ref> <ref type="foot" coords="3,153.55,615.25,6.76,6.44" target="#foot_8">12</ref> or the Vera model <ref type="bibr" coords="3,234.49,617.39,14.85,7.94" target="#b17">[18]</ref> pre-trained on the data from the TREC 2019 Health Misinformation track <ref type="bibr" coords="3,282.82,628.35,9.46,7.94" target="#b0">[1]</ref>. We do not fine-tune the models. Even though the models were originally trained to predict the support/refute probabilities (given a claim and a text passage), we take these predictions as yes/no Webis-longck-dis. After the retrieval and re-ranking steps, the answer is predicted by aggregating the prediction scores returned by LongChecker (using questions and abstracts plus titles as a context input) with a ranking position discount (nDCA 1000 , analogous to the run Webis-uniqa-dis).</p><p>Webis-verasent-dis. Analogous to the previous run, but using the Vera model for answer prediction. Since Vera has a 512-token input limitation, we use as input prompt only the "most relevant" sentences from abstracts found using heuristics proposed by Zhang et al. <ref type="bibr" coords="3,337.72,429.94,14.72,7.94" target="#b25">[26]</ref> that use a handcrafted list of indicator words.</p><p>Runs using a combination of QA and claim verification. For the following runs, we use the same retrieval (BM25) and re-ranking steps (monoT5 and duoT5) and average two prediction scores returned by UnifiedQA and LongChecker. The final answer prediction for each topic is again aggregated using nDCA 1000 from the individual answer scores for the 1000 question-abstract pairs. Webis-longck-uniqa-dis. For this run, after re-ranking, the final score is calculated from the averaged UnifiedQA (only abstract as context input) and LongChecker (abstract and title as context input) predictions discounted by the ranked positions.</p><p>Webis-longck-uniqa-ax-dis. Analogous to the previous run, but additionally top-1000 PubMed abstracts are also axiomatically reranked <ref type="bibr" coords="3,346.04,591.03,14.82,7.94" target="#b9">[10]</ref> based on their publication date to resolve potentially contradicting information from different publications. This way, the final predicted answer scores are more influenced by the more recently published abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Prediction Task: Evaluation</head><p>The results for our runs submitted to the Answer Prediction task as provided by the track organizers are reported in Table <ref type="table" coords="3,520.95,669.74,3.12,7.94" target="#tab_0">2</ref>. Overall, we observe that discounting the answer prediction scores based on the rank of retrieved evidence documents more often than simple averaging yields higher AUC and accuracy scores. Similarly, using claim verification systems is more accurate than QA systems for predicting an answer. These differences however might also be caused by the retrieval approaches used to find evidence documents (at this point we have not evaluated their retrieval effectiveness) or by the differences in datasets both types of systems were trained on. The answer predictor that is based on the Vera claim verification model that uses only the "most relevant" sentence selection from PubMed abstracts is the most accurate in predicting correct answers. However, the approach that uses LongChecker has the lowest false positive rate, worth considering because this error type is most harmful for health-related questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Web Retrieval Task: Our Runs</head><p>After predicting the answer to a topic's question, we retrieve documents with BM25 from one billion documents of the C4 corpus <ref type="bibr" coords="4,279.45,253.17,14.60,7.94" target="#b19">[20]</ref> that we indexed with Elasticsearch. Then, we re-rank the results with monoT5 and again re-rank the top-50 results (of the first reranking step) with duoT5 (the same models as in Section 3. <ref type="figure" coords="4,269.06,286.05,2.99,7.94">1</ref>). The answer score for each retrieved document from C4 is then predicted in a similar way as described in Section 3.1. To combine a document's answer score with the retrieval score for the final ranking, we first calculate the difference of the predicted answer scores between the topic 𝑇 (predicted "true" answer) and each document 𝐷:</p><formula xml:id="formula_3" coords="4,105.72,354.77,136.14,8.97">Δanswer = |answer(𝑇 ) -answer(𝐷)|</formula><p>Our runs further use the closeness 1 -Δanswer to the predicted "true" topic answer to boost the initial retrieval scores.</p><p>Runs with a linear score boosting. For each run in this group, we use BM25 to retrieve 1000 abstracts from the PubMed abstracts index (topic question fields are used as queries). Then, we re-rank the results with monoT5 and again re-rank the top-50 results (of the first re-ranking step) with duoT5 using PyTerrier <ref type="bibr" coords="4,214.68,443.86,13.22,7.94" target="#b15">[16]</ref>. Answer conflicts are resolved with axiomatic re-ranking (more recently published abstracts are ranked higher). For each of the 1000 question-abstract pairs, we collect answer prediction scores returned by pre-trained claim verification and/or QA models for each retrieved abstract and then aggregate the final topics' answer score by discounting ranking positions with nDCA 1000 . We then retrieve 1000 documents from C4 using Elasticsearch's BM25, re-rank with monoT5, and the top-50 of the first re-ranker with duoT5. Document answer scores are predicted with the same claim verification and QA models as in Section 3.1. Using the aggregated topic answer score and the individual answer scores for each retrieved C4 document, we boost the retrieval score linearly based on the closeness between a re-ranked document's answer score and the predicted topic answer: score lin (𝐷) = score duoT5 (𝐷) * (1 -Δanswer)</p><p>Webis-longck-ax-lin. Predict answer scores using the pre-trained claim verification model LongChecker 13 with abstract texts, abstract titles, and document text as a context input.</p><p>Webis-uniqa-ax-lin. Predict answer scores using the pre-trained QA model UnifiedQA-T5-large 14 with abstract texts and document text as context input. 13 Using fever sci checkpoints from https://github.com/dwadden/multivers 14 https://huggingface.co/allenai/unifiedqa-t5-large Webis-longck-uniqa-ax-lin. Predict answer scores using the averaged UnifiedQA and LongChecker scores (abstract texts, abstract titles (only LongChecker), and document text as a context input).</p><p>Runs with a polynomial score boosting. Using the aggregated topic answer score and the individual answer scores for each retrieved from C4 document, we boost retrieval scores as follows:</p><formula xml:id="formula_4" coords="4,352.78,161.46,170.14,12.08">score pol (𝐷) = score duoT5 (𝐷) * (1 -Δanswer 2 )</formula><p>Webis-longck-ax-pol. Predict answer scores for abstracts and documents using the pre-trained claim verification model LongChecker (fever sci checkpoints, again) with abstract texts, abstract titles, and document text as a context input.</p><p>Webis-uniqa-ax-pol. Predict answer scores using the pre-trained QA model UnifiedQA-T5-large (abstract texts and document text as a context input).</p><p>Webis-longck-uniqa-ax-pol. Predict answer scores using the averaged UnifiedQA and LongChecker scores (abstract texts, abstract titles (only LongChecker), and document text as a context input).</p><p>Webis-longck-uniqa-pol. Analogous to the previous run, but without axiomatic re-ranking by the publication date.</p><p>Runs with a weighted score combination. Using the aggregated topic answer score and the individual answer scores for each retrieved document, we combine a C4 document retrieval score with the closeness of that document's answer score to the predicted topic answer weighted by a trade-off 𝛼:</p><formula xml:id="formula_5" coords="4,330.58,393.87,214.55,10.35">score com (𝐷) = 𝛼 • score duoT5 (𝐷) + (1 -𝛼) • (1 -Δanswer)</formula><p>Webis-longck-ax-com. Predict answer scores with the pre-trained claim verification model LongChecker (fever sci checkpoints, again) with abstract texts, abstract titles, and document text as a context input (𝛼 = 0.75).</p><p>Webis-uniqa-ax-com. Predict answer scores using the pre-trained QA model UnifiedQA-T5-large (abstract texts and document text as a context input; 𝛼 = 0.75).</p><p>Webis-longck-uniqa-ax-com. Using the averaged UnifiedQA and LongChecker answer scores (abstract texts, abstract titles (only LongChecker), and document text as a context input; 𝛼 = 0.75).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Web Retrieval Task: Evaluation</head><p>For the Web Retrieval task, the retrieval effectiveness of the submitted runs is evaluated using nDCG, precision, and a compatibility measure <ref type="bibr" coords="4,351.13,577.56,10.59,7.94" target="#b2">[3]</ref> w.r.t the usefulness, correctness, and helpfulness and harmfulness of documents. The results for our runs in Table <ref type="table" coords="4,533.36,588.52,4.09,7.94" target="#tab_1">3</ref> show that none of our approaches significantly outperforms the median retrieval effectiveness (some runs are, however, significantly worse). The runs featuring linear or polynomial score boosting (cf. runs (a)-(g) in Table <ref type="table" coords="4,361.72,632.35,3.47,7.94" target="#tab_1">3</ref>) have significantly worse effectiveness (both nDCG and precision) on binary 'useful' and 'correct' relevance judgments as well as for graded 'usefulness' judgments. They are also significantly less compatible with 'helpful' results. Runs featuring a score combination (with trade-off 𝛼 = 0.75, cf. runs (h)-(j) in Table <ref type="table" coords="4,339.34,687.15,3.35,7.94" target="#tab_1">3</ref>) achieve a significantly improved effectiveness (nDCG) and are significantly more compatible with 'helpful' results compared  <ref type="table" coords="5,286.79,323.24,2.88,7.94" target="#tab_1">3</ref>).</p><p>Compared to the runs featuring the LongChecker model <ref type="bibr" coords="5,252.61,334.20,13.40,7.94" target="#b22">[23,</ref><ref type="bibr" coords="5,267.54,334.20,10.05,7.94" target="#b23">24]</ref>, the runs using UnifiedQA <ref type="bibr" coords="5,136.55,345.16,14.76,7.94" target="#b11">[12]</ref> or a combination of both have slightly improved effectiveness and compatibility with linear or polynomial score boosting, but the opposite effect can be observed when using a weighted score combination. Thus, whether the claim verification or QA models are better suited for the task is inconclusive. A weighted combination of the topical relevance with an answer closeness to the predicted "true" is so far the most promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In the Deep Learning track, we investigated the effectiveness of four duoT5 variants. We found that a greedy aggregation is substantially more effective than the original duoT5 at the same efficiency.</p><p>In the Health Misinformation track's Answer Prediction task, we investigated the effectiveness of pre-trained question answering and scientific claim verification models in predicting correct answers to yes/no health questions. As input to the models, we used questions and retrieved PubMed abstracts that potentially contain trustworthy evidence information. The experimental results show the investigated claim verification models to be more effective for the task than the investigated question answering models (a possible reason might be the different datasets that were originally used for training). For the Web Retrieval task, all our runs are not particularly effective (rather below the median across all evaluation metrics) but a weighted combination of the topical relevance with documents' closeness to the predicted "true" answer is significantly more effective than our linear or polynomial score boosting. As for using a predicted "true" answer during retrieval, there was no significant difference between using question answering or claim verification models even though in the Answer Prediction task the claim verification is more effective. One of the possible reasons might be that our current re-ranking approaches do not consider borderline answer predictions (close to the 0.5 answer threshold), which needs further investigation and is an interesting direction for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,317.62,86.41,242.19,230.93"><head>Table 2 :</head><label>2</label><figDesc>The Health Misinformation track's answer prediction results provided by the organizers (sorted by the AUC scores or by the next metric in case of ties). Reported are AUC and accuracy scores and false and true positive rates.</figDesc><table coords="3,317.75,133.46,240.62,183.87"><row><cell>Run</cell><cell>AUC</cell><cell>Acc.</cell><cell>FPR</cell><cell>TPR</cell></row><row><cell>Webis-verasent-dis</cell><cell>0.81</cell><cell>0.70</cell><cell>0.40</cell><cell>0.80</cell></row><row><cell>Webis-longck-dis</cell><cell>0.79</cell><cell>0.64</cell><cell>0.36</cell><cell>0.64</cell></row><row><cell>Webis-nlm-boolq-abs</cell><cell>0.69</cell><cell>0.52</cell><cell>0.96</cell><cell>1.00</cell></row><row><cell>Webis-longck-uniqa-dis</cell><cell>0.66</cell><cell>0.62</cell><cell>0.48</cell><cell>0.72</cell></row><row><cell>Webis-uniqa-dis</cell><cell>0.66</cell><cell>0.62</cell><cell>0.48</cell><cell>0.72</cell></row><row><cell>Webis-longck-uniqa-ax-dis</cell><cell>0.66</cell><cell>0.60</cell><cell>0.48</cell><cell>0.68</cell></row><row><cell>Webis-goo-boolq-abs</cell><cell>0.65</cell><cell>0.52</cell><cell>0.96</cell><cell>1.00</cell></row><row><cell>Webis-goo-lbert-abs</cell><cell>0.48</cell><cell>0.50</cell><cell>0.88</cell><cell>0.88</cell></row><row><cell>Webis-goo-lbert-title-abs</cell><cell>0.48</cell><cell>0.50</cell><cell>0.92</cell><cell>0.92</cell></row><row><cell>Webis-nlm-lbert-abs</cell><cell>0.48</cell><cell>0.50</cell><cell>0.80</cell><cell>0.80</cell></row><row><cell>Median all participants</cell><cell>0.71</cell><cell>0.64</cell><cell>0.48</cell><cell>0.80</cell></row><row><cell cols="5">answer prediction scores. The final answer prediction score for each</cell></row><row><cell cols="5">topic is calculated using nDCA (as described above) aggregated for</cell></row><row><cell>1000 question-abstract pairs.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.50,86.41,506.23,244.78"><head>Table 3 :</head><label>3</label><figDesc>The Health Misinformation track's official effectiveness results for our runs. U: useful, Co: correct, Incor.: incorrect. Significant differences to other runs are marked with superscripts (Student's 𝑡-test, 𝑝 &lt; 0.0009 = 0.05/55, Bonferroni-corrected).</figDesc><table coords="5,53.80,111.54,490.67,219.65"><row><cell>Run</cell><cell cols="2">Compatibility</cell><cell>nDCG (binary)</cell><cell cols="2">P@10 (binary)</cell><cell>nDCG (graded)</cell></row><row><cell></cell><cell>Help</cell><cell>Harm</cell><cell>U &amp; Co</cell><cell>U &amp; Co</cell><cell>Incor.</cell><cell>Useful</cell></row><row><cell>(a) Webis-longck-ax-lin</cell><cell>0.11 fghijk</cell><cell>0.07 eij</cell><cell>0.43 bdefghijk</cell><cell>0.27 hijk</cell><cell>0.09 ij</cell><cell>0.49 bcdefghijk</cell></row><row><cell>(b) Webis-uniqa-ax-lin</cell><cell>0.15 ehijk</cell><cell>0.12</cell><cell>0.50 aehijk</cell><cell>0.31 hijk</cell><cell>0.13</cell><cell>0.56 acehijk</cell></row><row><cell>(c) Webis-longck-uniqa-ax-lin</cell><cell>0.14 fghijk</cell><cell>0.07 eij</cell><cell>0.48 efghijk</cell><cell>0.34 hik</cell><cell>0.08 ij</cell><cell>0.52 abefghijk</cell></row><row><cell>(d) Webis-longck-ax-pol</cell><cell>0.15 hijk</cell><cell>0.09</cell><cell>0.47 ahijk</cell><cell>0.32 hik</cell><cell>0.11</cell><cell>0.54 aehijk</cell></row><row><cell>(e) Webis-uniqa-ax-pol</cell><cell>0.18 bhk</cell><cell>0.14 ac</cell><cell>0.52 abchijk</cell><cell>0.36 hik</cell><cell>0.15</cell><cell>0.58 abcdhijk</cell></row><row><cell>(f) Webis-longck-uniqa-ax-pol</cell><cell>0.17 achik</cell><cell>0.08</cell><cell>0.51 achijk</cell><cell>0.38 hk</cell><cell>0.10</cell><cell>0.57 achijk</cell></row><row><cell>(g) Webis-longck-uniqa-pol</cell><cell>0.17 achik</cell><cell>0.08</cell><cell>0.52 achijk</cell><cell>0.38 hk</cell><cell>0.10</cell><cell>0.57 achijk</cell></row><row><cell>(h) Webis-longck-ax-com</cell><cell>0.27 abcdefg</cell><cell>0.15</cell><cell>0.58 abcdefg</cell><cell>0.55 abcdefg</cell><cell>0.18</cell><cell>0.66 abcdefg</cell></row><row><cell>(i) Webis-uniqa-ax-com</cell><cell>0.26 abcdfg</cell><cell>0.17 ac</cell><cell>0.58 abcdefg</cell><cell>0.52 abcde</cell><cell>0.23 ac</cell><cell>0.66 abcdefg</cell></row><row><cell>(j) Webis-longck-uniqa-ax-com</cell><cell>0.25 abcd</cell><cell>0.17 ac</cell><cell>0.57 abcdefg</cell><cell>0.48 ab</cell><cell>0.23 ac</cell><cell>0.65 abcdefg</cell></row><row><cell>(k) Median all participants</cell><cell>0.24</cell><cell>0.13</cell><cell>0.61</cell><cell>0.53</cell><cell>0.16</cell><cell>0.69</cell></row><row><cell cols="3">to the runs with linear and polynomial score boosting. Axiomatic</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">re-ranking at the answer prediction stage does not significantly</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">change compatibility or effectiveness (cf. runs (f) and (g) in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,321.00,618.99,94.94,6.18"><p>https://pubmed.ncbi.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,320.88,627.39,190.74,6.18"><p>https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,321.00,644.20,65.71,6.18"><p>https://www.elastic.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="2,320.88,652.60,168.55,6.18"><p>Obtained from https://pubmed.ncbi.nlm.nih.gov/download/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="2,321.00,661.00,181.38,6.18"><p>https://huggingface.co/Tejas21/bio-linkbert-large pubmedqa-hf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="2,321.00,669.41,181.87,6.18"><p>https://huggingface.co/luztraplet/roberta-large-finetuned-boolq</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6" coords="2,323.42,677.81,138.93,6.18"><p>https://huggingface.co/allenai/unifiedqa-t5-large</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7" coords="3,59.27,677.65,235.23,6.18;3,53.80,685.89,96.62,6.18"><p>https://huggingface.co/castorini/monot5-3b-med-msmarco, https://huggingface.co/ castorini/duot5-3b-med-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8" coords="3,59.27,694.29,190.18,6.18"><p>Using fever sci checkpoints https://github.com/dwadden/multivers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work has been partially supported by the <rs type="funder">DFG</rs> in the project "<rs type="projectName">ACQuA 2</rs>.0: <rs type="programName">Answering Comparative Questions with Arguments" (project 376430233) as part of the priority program</rs> "<rs type="projectName">RATIO: Robust Argumentation Machines</rs>" (<rs type="grantNumber">SPP 1999</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_PHD6Wkk">
					<orgName type="project" subtype="full">ACQuA 2</orgName>
					<orgName type="program" subtype="full">Answering Comparative Questions with Arguments&quot; (project 376430233) as part of the priority program</orgName>
				</org>
				<org type="funded-project" xml:id="_EgjmWUB">
					<idno type="grant-number">SPP 1999</idno>
					<orgName type="project" subtype="full">RATIO: Robust Argumentation Machines</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,334.39,413.21,223.81,6.18;5,334.39,421.18,223.81,6.18;5,334.39,429.15,165.66,6.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,377.81,421.18,119.83,6.18">Overview of the TREC 2019 Decision Track</title>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Maistro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<idno>TREC 2019. NIST</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,509.66,421.18,48.54,6.17;5,334.39,429.15,111.97,6.17">Proceedings of the Twenty-Eighth Text REtrieval Conference</title>
		<meeting>the Twenty-Eighth Text REtrieval Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,437.12,223.81,6.18;5,334.39,445.09,223.81,6.18;5,334.39,453.07,224.51,6.18;5,334.39,461.04,223.81,6.17;5,334.39,469.01,138.85,6.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,429.52,445.09,128.68,6.18;5,334.39,453.07,69.89,6.18">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,415.49,453.07,143.40,6.17;5,334.39,461.04,223.81,6.17;5,334.39,469.01,102.22,6.18">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. ACL</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. ACL</meeting>
		<imprint>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,476.98,224.99,6.18;5,334.39,484.95,223.81,6.18;5,334.39,492.92,224.89,6.17;5,334.14,500.89,43.06,6.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,517.42,476.98,41.96,6.18;5,334.39,484.95,133.69,6.18">Offline Evaluation by Maximum Similarity to an Ideal Ranking</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vtyurina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,480.37,484.95,77.83,6.17;5,334.39,492.92,222.15,6.17">Proceedings of the 29th ACM International Conference on Information and Knowledge Management, CIKM 2020</title>
		<meeting>the 29th ACM International Conference on Information and Knowledge Management, CIKM 2020</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,508.86,223.95,6.18;5,334.39,516.83,129.04,6.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,505.55,508.86,52.78,6.18;5,334.39,516.83,17.76,6.18">Learning to Order Things</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,357.62,516.83,50.62,6.17">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="243" to="270" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,524.80,223.81,6.18;5,334.18,532.77,224.02,6.18;5,334.39,540.74,82.19,6.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,510.08,524.80,48.12,6.18;5,334.18,532.77,93.66,6.18">Overview of the TREC 2021 Deep Learning Track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<idno>TREC 2021. NIST</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,440.40,532.77,117.80,6.17;5,334.39,540.74,28.93,6.17">Proceedings of the Thirtieth Text REtrieval Conference</title>
		<meeting>the Thirtieth Text REtrieval Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,548.71,224.99,6.18;5,334.39,556.68,223.81,6.18;5,334.39,564.65,224.58,6.18;5,334.23,572.62,18.33,6.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,540.54,548.71,18.84,6.18;5,334.39,556.68,173.14,6.18">Noise-Reduction for Automatically Transferred Relevance Judgments</title>
		<author>
			<persName coords=""><forename type="first">Maik</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,519.75,556.68,38.46,6.17;5,334.39,564.65,193.87,6.17">Proceedings of the 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting>the 13th International Conference of the CLEF Association, CLEF 2022</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="48" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,580.59,223.81,6.18;5,334.39,588.56,223.81,6.18;5,334.39,596.53,223.81,6.17;5,334.39,604.50,152.33,6.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,383.18,588.56,164.30,6.18">Sampling Bias Due to Near-Duplicates in Learning to Rank</title>
		<author>
			<persName coords=""><forename type="first">Maik</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Heinrich Reimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,334.39,596.53,223.81,6.17;5,334.39,604.50,98.26,6.17">Proceedings of the 43rd International ACM Conference on Research and Development in Information Retrieval, SIGIR 2020</title>
		<meeting>the 43rd International ACM Conference on Research and Development in Information Retrieval, SIGIR 2020</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997-2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,612.47,223.81,6.18;5,334.39,620.44,223.81,6.18;5,334.39,628.41,224.58,6.18;5,334.23,636.38,18.33,6.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,530.39,612.47,27.82,6.18;5,334.39,620.44,213.26,6.18">The Effect of Content-Equivalent Near-Duplicates on the Evaluation of Search Engines</title>
		<author>
			<persName coords=""><forename type="first">Maik</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">Philipp</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,334.39,628.41,194.72,6.17">Proceedings of the 42nd European Conference on IR Research, ECIR 2020</title>
		<meeting>the 42nd European Conference on IR Research, ECIR 2020</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,644.35,223.82,6.18;5,334.39,652.32,223.82,6.18;5,334.39,660.29,223.81,6.17;5,334.39,668.26,52.23,6.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,539.21,644.35,18.99,6.18;5,334.39,652.32,146.98,6.18">Sparse Pairwise Re-ranking with Pre-trained Transformers</title>
		<author>
			<persName coords=""><forename type="first">Lukas</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maik</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,493.88,652.32,64.33,6.17;5,334.39,660.29,223.81,6.17;5,334.39,668.26,11.14,6.17">Proceedings of the 2022 ACM SIGIR International Conference on the Theory of Information Retrieval, ICTIR 2022</title>
		<meeting>the 2022 ACM SIGIR International Conference on the Theory of Information Retrieval, ICTIR 2022</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="72" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.39,676.23,223.81,6.18;5,334.39,684.20,223.82,6.18;5,334.39,692.17,194.97,6.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,528.17,676.23,30.03,6.18;5,334.39,684.20,51.73,6.18">Axiomatic Result Re-Ranking</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,398.75,684.20,159.46,6.17;5,334.39,692.17,147.38,6.17">Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016</title>
		<meeting>the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,88.10,223.81,6.18;6,70.23,96.07,223.81,6.18;6,70.23,104.04,224.51,6.17;6,70.23,112.01,224.58,6.17;6,70.23,119.98,109.00,6.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,82.89,96.07,199.81,6.18">PubMedQA: A Dataset for Biomedical Research Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,70.23,104.04,224.51,6.17;6,70.23,112.01,224.58,6.17;6,70.23,119.98,72.38,6.18">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. ACL</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. ACL</meeting>
		<imprint>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,127.95,224.58,6.18;6,70.23,135.92,224.99,6.18;6,70.23,143.89,223.81,6.18;6,70.23,151.86,119.10,6.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,187.75,135.92,107.47,6.18;6,70.23,143.89,85.33,6.18">UnifiedQA: Crossing Format Boundaries With a Single QA System</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,167.99,143.89,126.06,6.17;6,70.23,151.86,82.48,6.18">Findings of the Association for Computational Linguistics: EMNLP 2020. ACL</title>
		<imprint>
			<biblScope unit="page" from="1896" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,159.83,224.58,6.18;6,70.23,167.80,224.05,6.18;6,70.23,175.77,222.07,6.18" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="6,259.05,167.80,35.23,6.18;6,70.23,175.77,136.52,6.18">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,183.74,223.81,6.18;6,70.23,191.71,223.81,6.18;6,70.00,199.68,224.04,6.18;6,70.23,207.65,222.23,6.18" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,263.70,183.74,30.35,6.18;6,70.23,191.71,223.81,6.18;6,70.00,199.68,29.27,6.18">Document Expansion Baselines and Learned Sparse Lexical Representations for MS MARCO V1 and V2</title>
		<author>
			<persName coords=""><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,112.68,199.68,181.37,6.17;6,70.23,207.65,169.49,6.17">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2022</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2022</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="3187" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,215.62,224.58,6.18;6,70.23,223.59,223.81,6.18;6,70.23,231.56,223.81,6.17;6,70.23,239.53,145.05,6.18" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,129.37,223.59,120.64,6.18">Simplified Data Wrangling with ir datasets</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,262.02,223.59,32.03,6.17;6,70.23,231.56,223.81,6.17;6,70.23,239.53,90.98,6.17">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2021</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2021</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2429" to="2436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,247.50,224.64,6.18;6,70.23,255.47,224.51,6.18;6,70.23,263.44,223.82,6.17;6,70.23,271.41,122.65,6.18" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,267.08,247.50,27.79,6.18;6,70.23,255.47,200.84,6.18">PyTerrier: Declarative Experimentation in Python from BM25 to Dense Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,283.23,255.47,11.51,6.17;6,70.23,263.44,223.82,6.17;6,70.23,271.41,68.58,6.17">Proceedings of the 30th ACM International Conference on Information and Knowledge Management, CIKM 2021</title>
		<meeting>the 30th ACM International Conference on Information and Knowledge Management, CIKM 2021</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="4526" to="4533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.23,279.38,223.81,6.18;6,70.23,287.35,43.02,6.18" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="6,168.13,279.38,101.55,6.18">From doc2query to docTTTTTquery</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Online preprint</note>
</biblStruct>

<biblStruct coords="6,70.23,295.32,224.99,6.18;6,70.23,303.29,223.81,6.18;6,70.23,311.26,223.81,6.18;6,70.23,319.23,202.05,6.18" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="6,283.20,295.32,12.02,6.18;6,70.23,303.29,223.81,6.18;6,70.23,311.26,17.22,6.18">Prediction Techniques for Reducing Harmful Misinformation in Consumer Health Search</title>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,99.06,311.26,194.98,6.17;6,70.23,319.23,147.98,6.17">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2021</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2021</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2066" to="2070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,88.10,223.81,6.18;6,334.39,96.07,224.88,6.18;6,334.39,104.04,79.75,6.18" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="6,485.98,88.10,72.22,6.18;6,334.39,96.07,221.76,6.18">The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models</title>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/2101.05667</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,112.01,224.58,6.18;6,334.39,119.98,223.81,6.18;6,334.39,127.95,224.57,6.18;6,334.39,135.92,78.87,6.18" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="6,497.70,119.98,60.50,6.18;6,334.39,127.95,176.10,6.18">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,516.14,127.95,42.82,6.17;6,334.39,135.92,10.66,6.17">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,143.89,224.58,6.18;6,334.39,151.86,223.82,6.18;6,334.39,159.83,109.18,6.18" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="6,389.51,151.86,47.10,6.18">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,449.49,151.86,108.71,6.17;6,334.39,159.83,28.93,6.17">Proceedings of The Third Text REtrieval Conference</title>
		<meeting>The Third Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
	<note>NIST</note>
</biblStruct>

<biblStruct coords="6,334.39,167.80,224.89,6.18;6,334.39,175.77,224.51,6.18;6,334.39,183.74,223.81,6.17;6,334.39,191.71,224.89,6.17;6,334.14,199.68,40.89,6.18" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="6,334.39,175.77,188.77,6.18">FEVER: a Large-scale Dataset for Fact Extraction and VERification</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,535.45,175.77,23.45,6.17;6,334.39,183.74,223.81,6.17;6,334.39,191.71,224.89,6.17;6,334.14,199.68,10.75,6.18">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. ACL</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. ACL</meeting>
		<imprint>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,207.65,223.81,6.18;6,334.39,215.62,223.81,6.18;6,334.39,223.59,145.28,6.18" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Longchecker</surname></persName>
		</author>
		<idno>Context. CoRR abs/2112.01640</idno>
		<title level="m" coord="6,408.58,215.62,149.62,6.18;6,334.39,223.59,37.04,6.18">Improving Scientific Claim Verification by Modeling Full-Abstract</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,231.56,223.81,6.18;6,334.39,239.53,224.99,6.18;6,334.39,247.50,223.81,6.18;6,334.39,255.47,106.74,6.18" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="6,395.72,239.53,163.66,6.18;6,334.39,247.50,89.14,6.18">Improving Scientific Claim Verification with Weak Supervision and Full-Document Context</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Multivers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.16,247.50,123.04,6.17;6,334.39,255.47,83.08,6.18">Findings of the Association for Computational Linguistics: NAACL 2022. ACL</title>
		<imprint>
			<biblScope unit="page" from="61" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,263.44,223.81,6.18;6,334.39,271.41,223.81,6.18;6,334.39,279.38,213.47,6.18" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="6,491.95,263.44,66.25,6.18;6,334.39,271.41,108.12,6.18">LinkBERT: Pretraining Language Models with Document Links</title>
		<author>
			<persName coords=""><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,453.47,271.41,104.74,6.17;6,334.39,279.38,176.84,6.18">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022. ACL</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022. ACL</meeting>
		<imprint>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,334.39,287.35,224.89,6.18;6,334.39,295.32,223.81,6.18;6,334.39,303.29,223.81,6.18;6,334.39,311.26,223.81,6.17;6,334.39,319.23,65.20,6.18" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="6,334.39,295.32,223.81,6.18;6,334.39,303.29,93.12,6.18">Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search</title>
		<author>
			<persName coords=""><forename type="first">Dake</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Vakili Tahami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,439.88,303.29,118.32,6.17;6,334.39,311.26,223.81,6.17;6,334.39,319.23,11.14,6.17">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2022</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2022</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2099" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
