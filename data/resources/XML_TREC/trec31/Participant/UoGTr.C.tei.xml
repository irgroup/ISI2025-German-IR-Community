<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.13,84.23,415.73,15.44;1,179.62,104.15,253.26,15.44">University of Glasgow Terrier Team at the TREC 2022 Conversational Assistance Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.52,127.58,105.66,10.59"><forename type="first">Sarawoot</forename><surname>Kongyoung</surname></persName>
							<email>s.kongyoung.1@research.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.66,127.58,83.67,10.59"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craig.macdonald@glasgow.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,443.84,127.58,53.61,10.59"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>iadh.ounis@glasgow.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.13,84.23,415.73,15.44;1,179.62,104.15,253.26,15.44">University of Glasgow Terrier Team at the TREC 2022 Conversational Assistance Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCB28FDE83E2E0CB812A87BE8ACBDB37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our methods and submitted runs for the TREC 2022 Conversational Assistance Track. In our participation, we leverage Multi-Task Learning (MTL) methods to enhance the performance of the conversational search system. For the main task, we use our recently proposed monoQA model, which applies Multi-Task Learning (MTL) on reranking and answer extraction by sharing a single text generation model, predicts both the answer and the reranking score simultaneously. For the mixed-initiative sub-task, we propose T5MI, which is trained on the ClariQ dataset, to determine whether a user utterance needs to ask clarifying questions, as well as to generate useful clarifying questions. This year, we submitted three runs based on the data used in the testing step consisting of 1) uogTr-MT: using the provided manually rewritten utterances as the queries; 2) uogTr-AT: using the raw utterances and the provided provenances as the context for rewriting the queries; 3) uogTr-MI-HB: using the raw utterances and the output from the mixed-initiative sub-task as the context for rewriting the queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>CAsT 2022 is the fourth year of the Conversational Assistance Track in TREC. The CAsT track tackles information retrieval tasks in a conversational context. Similar to previous years, the canonical responses to each user's utterance and explicit feedback are provided. The main difference with last year's setup is that a new sub-task focusing on mixed-initiative has been added. For each turn in the conversation, the system may give a response or ask a question.</p><p>In this work, to address the conversational search task of the Conversational Assistance track, we followed a multi-stage framework consisting of a query rewriting, a query and document expansions, a retriever, a reranker, and a reader, as illustrated in Figure <ref type="figure" coords="1,254.97,537.11,3.01,7.94" target="#fig_0">1</ref>. In particular, we leverage Multi-Task Learning (MTL) consisting of monoQA (our recently proposed MTL model, which combines reranking and answer extraction) <ref type="bibr" coords="1,122.04,569.99,10.43,7.94" target="#b5">[6]</ref> and T5MI (which applies MTL of clarification need classification and clarifying question generation) to address the conversational search task. Firstly, the query rewriting model takes the raw user utterance and its context as an input sequence and reformulates it into a fully specified query. Secondly, each document is expanded before indexing and each query is expanded before feeding it into the retriever. Thirdly, the retriever retrieves the top 𝐾 relevant passages from the text collection based on a query rewritten by the query rewriting model. The reranker and the reader then respectively rerank and identify an answer in the top 𝐾 passages.</p><p>The structure of the remainder of this paper is structured as follows: Section 2 discusses our multi-stage pipeline setup; Section 3 describes our mixed-initiative method; Section 4 describes our submitted runs; Section 5 reports our results; Concluding remarks follow in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A MULTI-STAGE PIPELINE FOR A CONVERSATIONAL SEARCH SYSTEM</head><p>In this section, we describe our multi-stage pipeline to address the conversational search task as illustrated in Figure <ref type="figure" coords="1,493.35,397.24,3.01,7.94" target="#fig_0">1</ref>. In the following, we describe each of these stages in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">T5 Query Rewriting Model</head><p>To deal with the ambiguity of conversational questions, we use a T5 <ref type="bibr" coords="1,329.41,455.99,14.69,7.94" target="#b17">[17]</ref> query rewriting (T5QR) model, which has been fine-tuned using the CANARD <ref type="bibr" coords="1,392.56,466.95,10.57,7.94" target="#b2">[3]</ref> conversational question rewriting dataset. By doing this, we follow the CAsT baseline rewriting configuration by using all historical utterances 𝑢 1:𝑘 -1 and all canonical response passages 𝑟 1:𝑘 -1 as the context. For example, the specific user utterance at turn 𝑘 (𝑢 𝑘 ) can be reformulated as follows:</p><formula xml:id="formula_0" coords="1,341.29,525.50,217.45,17.51">𝑢 ′ 𝑘 = 𝑇 5𝑄𝑅(𝑢 1 |||𝑟 1 |||...|||𝑢 𝑘 -1 |||𝑟 𝑘 -1 |||𝑢 𝑘 ) (1)</formula><p>where 𝑢 ′ 𝑘 is the rewritten query, which corresponds to the raw utterance 𝑢 𝑘 and its context (𝑢 1:𝑘 -1 ; 𝑟 1:𝑘 -1 ). '|||' is used to separate tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query &amp; Document Expansions</head><p>Query expansion: Following our participation in the TREC 2021 Deep Learning track <ref type="bibr" coords="1,399.33,624.11,13.49,7.94" target="#b19">[19]</ref>, we use the Bo1 <ref type="bibr" coords="1,481.43,624.11,10.68,7.94" target="#b1">[2]</ref> query expansion method to improve the rewritten queries for document retrieval. Document expansion: Following doc2query <ref type="bibr" coords="1,484.32,646.03,13.22,7.94" target="#b12">[13]</ref>, we employ document expansion with predicted queries. In particular, for each document in the three collections (MS MARCO V2, KILT, WaPo), we add three predicted queries to the text of the document: where 𝑑 and 𝑑 ′ are, respectively, the original document and the expanded document that is obtained by appending the predicted queries q1:3 . We apply the docTTTTTquery <ref type="bibr" coords="2,212.51,263.80,14.59,7.94" target="#b12">[13]</ref> model to generate a predicted query from each document.</p><formula xml:id="formula_1" coords="1,377.38,693.53,181.36,16.69">𝑑 ′ = 𝑑 ⊕ ( q1 ⊕ q2 ⊕ q3 )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hybird of Sparse and Dense Retrieval</head><p>For passage retrieval, following our participation in the TREC 2021 Deep Learning track <ref type="bibr" coords="2,133.80,320.67,13.49,7.94" target="#b19">[19]</ref>, we implement a hybrid of sparse and dense retrieval model as shown in Figure <ref type="figure" coords="2,204.16,331.63,3.02,7.94" target="#fig_0">1</ref>. In the hybrid of sparse and dense retrieval pipeline, we combine spare retrieval, namely DPH with the Bo1 <ref type="bibr" coords="2,121.51,353.54,10.43,7.94" target="#b1">[2]</ref> query expansion mechanism on the inverted index with TCT-ColBERT <ref type="bibr" coords="2,151.41,364.50,9.42,7.94" target="#b7">[8,</ref><ref type="bibr" coords="2,163.07,364.50,11.58,7.94" target="#b9">10]</ref> on the FAISS <ref type="bibr" coords="2,226.74,364.50,10.65,7.94" target="#b3">[4]</ref> index. In order to create a hybrid of the sparse and dense retrieval models, we take the union of the passages returned by the dense retrieval model and those returned passages by the sparse retrieval model. The merged passages are then reranked and the answer is extracted using the monoQA model, which is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">monoQA: Reranker &amp; Generative Reader</head><p>In our participation this year, we deploy our recently proposed monoQA <ref type="bibr" coords="2,89.85,465.21,10.66,7.94" target="#b5">[6]</ref> MTL model, which is fine-tuned simultaneously for both reranking (in order to improve the precision of the top retrieved passages) and extracting the answer on the OR-QuAC dataset <ref type="bibr" coords="2,82.50,498.09,13.49,7.94" target="#b16">[16]</ref>. Our MTL-based monoQA model has been shown to outperform several existing multi-stage pipeline systems, such as the ORConvQA systems proposed by <ref type="bibr" coords="2,192.84,520.00,9.37,7.94" target="#b6">[7,</ref><ref type="bibr" coords="2,204.45,520.00,6.15,7.94" target="#b8">9,</ref><ref type="bibr" coords="2,212.84,520.00,10.32,7.94" target="#b14">15,</ref><ref type="bibr" coords="2,225.41,520.00,11.55,7.94" target="#b16">16]</ref> or the separate applications of the monoT5 <ref type="bibr" coords="2,154.94,530.96,14.65,7.94" target="#b11">[12]</ref> and UnifiedQA <ref type="bibr" coords="2,228.29,530.96,10.47,7.94" target="#b4">[5]</ref> models. In particular, compared to the separate applications of the monoT5 and UnifiedQA models for reranking and extracting the answer, our MTL-based monoQA model is twice as fast for inference. This motivates our use of monoQA in our participation in the CAsT Track this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MIXED-INITIATIVE SUB-TASK</head><p>In this section, we describe in detail our method and system implementation to address the mixed-initiative sub-task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">T5MI: Clarification Need Classification &amp; Clarifying Question Generation</head><p>The upper part of Figure <ref type="figure" coords="2,145.24,679.57,4.14,7.94" target="#fig_1">2</ref> shows our proposed model, which uses the T5 model, a large pre-trained language model designed for text generation, namely T5MI. We leverage Multi-Task Learning with a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Selection</head><p>As shown in the bottom part of Figure <ref type="figure" coords="2,463.93,273.65,3.13,7.94" target="#fig_1">2</ref>, we use a Generalizable T5-based dense Retriever (GTR) model <ref type="bibr" coords="2,455.89,284.61,14.60,7.94" target="#b10">[11]</ref> to retrieve the clarifying questions from the questions pool provided by the organisers. By doing this, we use the provided checkpoint gtr-t5-xxl, without further fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question Ranking</head><p>To rank the clarifying questions obtained from both the T5MI and the GTR models, we score them using a T5 model, denoted as T5Ranking, trained on the ClariQ <ref type="bibr" coords="2,438.40,377.38,10.42,7.94" target="#b0">[1]</ref> dataset for pointwise question classification, as illustrated in the right part of Figure <ref type="figure" coords="2,516.50,388.34,3.10,7.94" target="#fig_1">2</ref>. Once the ranking of questions was submitted to TREC, for each turn, the organisers collected user feedback for the top-one question from the question ranking. We then use the provided feedback for each clarifying question as the context for rewriting the raw utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RUNS</head><p>In this section, we describe our submitted run for the Mixed-Initiative sub-task in Section 4.1. The details of our submitted runs for the Conversational Search task are provided in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mixed-Initiative Sub-task</head><p>Official Runs</p><p>• uogTr-MI: Applies the T5 query rewriting (T5QR) model using the full context of the provided canonical responses and all historical utterances to rewrite the current utterance.</p><p>Next, the rewritten utterance is used as input for the MTL T5MI model to identify whether the utterance needs to ask a clarifying question, as well as to generate clarifying questions. In addition, to retrieve the clarifying question from the provided question pool, we adopt a GTR dense retrieval for indexing and retrieving the relevant questions. Finally, we combine the generated and retrieved clarifying questions and rank them using the T5Ranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Provided Baselines</head><p>For comparison with our results in the mixed-initiative sub-task, we include the results of the baseline methods provided by the track organisers <ref type="bibr" coords="2,358.26,701.49,13.36,7.94" target="#b13">[14]</ref>, namely: • GPT-3 Raw: Uses the raw user utterance at each turn as input to GPT3. • GPT-3 Rewrite: Uses the automatic rewrite as input to GPT3.</p><p>• GPT-3 Full: Uses the full conversation history (user utterances and system responses) as input to GPT-3. • T5 Raw: Uses the raw user utterance as input to T5.</p><p>• T5 Rewrite: Uses the automatic rewrite as input to T5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection:</head><p>The selection baseline runs rank questions from the question pool:</p><p>• BM25: Ranks questions using BM25 with the automatic rewrite of the current turn query. • miniLM-BERT: Generates a candidate pool of questions using the all-MiniLM-L6-v2 model <ref type="bibr" coords="3,202.48,446.22,14.59,7.94" target="#b18">[18]</ref> from Sentence Transformers, then reranks them using a BERT model trained on the ClariQ dataset for pointwise question classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Conversational Search Task</head><p>We submitted 3 runs to the Conversation Search task. Table <ref type="table" coords="3,277.32,504.66,4.24,7.94">1</ref> describes the features we used in each of our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Official Runs</head><p>• uogTr-MT: Produces a run using the manually rewritten utterances provided by the track organisers. • uogTr-AT: Applies a T5 rewriting query model using the full text of the provided canonical responses and all historical utterances to rewrite the current utterance. • uogTr-MI-HB: Applies a T5 rewriting query model using the output from the mixed-initiative sub-task as the context and all historical utterances to rewrite the current utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Provided Baselines</head><p>For comparison with our runs' results, we include the results of the baseline methods provided by the track organisers <ref type="bibr" coords="3,240.41,654.15,13.36,7.94" target="#b13">[14]</ref>, namely:</p><p>• BM25_T5_BART_automatic: An automatic baseline run that has been provided by the organisers. The top 1000 passages were retrieved and re-ranked using BM25 and a T5-reranker using each turn's automatic rewrite. To generate the response answer, this baseline system used a BART model to summarise the top three passages from the retrieval stage. • BM25_T5_BART_manual: A manual baseline run that has been provided by the organisers. The top 1000 documents were retrieved and re-ranked using BM25 and a T5-re-ranker using each turn's manual rewrite. To generate the response answer, this baseline system used a BART model to summarise the top three passages from the retrieval stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We first report our evaluation results for the Mixed-Initiative subtask in Section 5.1. The evaluation results for the Conversation Search task are provided in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mixed-Initiative Sub-task Results</head><p>Table <ref type="table" coords="3,339.68,471.35,4.16,7.94" target="#tab_0">2</ref> shows the obtained effectiveness results for our submitted runs and the baseline runs provided by the track organisers in terms of P@1. All evaluation metrics are calculated using the official qrels. Following the guidelines provided by the organisers <ref type="bibr" coords="3,528.04,504.23,13.22,7.94" target="#b13">[14]</ref>, the performance evaluation is conducted using the P@1 metric (with a threshold of 2), taking the average across 205 users' utterances. This evaluation is based on three criteria: Relevance@1, Novelty@1, and Diversity@1. Diversity measures the number of options provided in the question. Novelty evaluates whether the question adds new information to the conversation. Relevance assesses whether the question logically flows from previous utterances.</p><p>From the table, we observe that our run, namely uogTr-MI, achieves the highest performance compared to both the generation and selection baselines on Relevance@1 and Novelty@1. However, the highest performance for Diversity@1 is achieved by the baseline miniLM-BERT. Furthermore, we use the T5QR model (see Section 2.1) to reformulate the current utterance by using clarifying questions from our uogTr-MI run and the user feedback provided by the organisers. This rewritten utterance is then processed through the retrieval pipeline, as outlined in Sections 2.3-2.4, and submitted as the run named uogTr-MI-HB. We present the performance of uog-TR-MI-HB in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Conversational Search task Results</head><p>Table <ref type="table" coords="4,75.95,240.37,4.25,7.94" target="#tab_1">3</ref> presents the obtained effectiveness results of all our runs in comparison to the provided baselines. The table also shows the TREC per-topic best and median scores across all participating systems, in terms of NDCG@20 and MAP@1000. This year, a run can be evaluated under two relevance thresholds, lenient: where passages at least slightly meet the need of the request at that turn (relevance level 1); and strict, where passages must at least "moderately meet" the need (relevance level 2). Table <ref type="table" coords="4,226.07,317.08,4.25,7.94" target="#tab_1">3</ref> (top-half) shows the results of the lenient evaluation, whereas Table <ref type="table" coords="4,239.54,328.04,4.10,7.94" target="#tab_1">3</ref> (bottom-half) shows the results of the strict evaluation. Firstly, we analyse the performance of the automatic runs. The results from the table indicate that uogTr-MI-HB outperforms uogTr-AT and BM25_T5_BART_automatic on all measures and under both the lenient and strict evaluation criteria. This is due to the use of the output from the mixed-initiative sub-task as context to rewrite raw utterances. However, our automatic submitted runs (uogTr-MI and uogTr-AT) perform lower than the TREC median. From the table, we also observe that our uogTr-AT run has a lower performance compared to the results of the baseline BM25_T5_BART_automatic.</p><p>On closer inspection, we observe that the query rewriting model in the baseline uses only the last three (at most) canonical responses as the context while our run uses all of the historical canonical responses. Hence, this might explain the reduced rewriting effectiveness we experienced.</p><p>Finally, we analyse the performance of the manual runs. According to the table, our manual run, namely uogTr-MT, outperforms the provided baseline, namely BM25_T5_BART_manual, on all measures and all evaluation criteria (lenient and strict). Moreover, our submitted manual run (uogTr-MT) performs better than the TREC median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>Overall, our participation in the TREC Conversational Assistance track has been valuable in increasing our understanding of effectively leveraging the Multi-Task Learning (MTL) methods to address the Conversational task. We found that our most effective run uogTr-MT using our MTL monoQA model outperform the TREC median on all measures and all evaluation criteria. Moreover, for the mixed-initiative sub-task, our uogTr-MI run -which uses our MTL T5MI model -outperform all of the baselines provided by the organisers on all measures. For future work, we plan to incorporate the query rewriting, retrieval, and reader (answer extractor) components into a single language model for improved efficiency and for simplifying the stages of the pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,340.62,278.75,194.91,7.70;1,317.96,169.32,240.25,95.44"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall architecture of our system.</figDesc><graphic coords="1,317.96,169.32,240.25,95.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,53.80,202.61,241.85,7.70;2,53.80,213.57,17.90,7.70;2,53.80,83.69,240.24,104.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall framework of our mixed-initiative system.</figDesc><graphic coords="2,53.80,83.69,240.24,104.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,341.67,85.73,192.52,7.70;2,331.38,107.91,8.80,4.70;2,356.56,107.91,6.83,4.70;2,430.35,107.91,19.36,4.70;2,478.30,107.91,76.97,4.70;2,425.26,114.63,25.65,4.70;2,320.90,121.34,76.39,4.70;2,356.56,127.82,44.06,4.70;2,416.08,121.34,90.37,4.70;2,523.98,121.34,19.66,4.70;2,320.90,134.30,70.76,4.70;2,356.56,140.78,24.68,4.70;2,416.08,134.30,90.37,4.70;2,523.98,134.30,19.66,4.70;2,320.90,147.26,70.76,4.70;2,356.56,153.74,53.63,4.70;2,416.08,147.26,89.71,4.70;2,523.98,147.26,19.66,4.70;2,317.96,191.84,240.25,7.94;2,317.96,202.80,240.24,7.94;2,317.96,213.76,240.25,7.94;2,317.96,224.72,240.25,7.94;2,317.96,235.68,188.58,7.94"><head>Table 1 :</head><label>1</label><figDesc>Pipeline components used for each run Run QR Retrieval QE &amp; DE Re-ranker &amp; Reader official runs uogTr-MT provided manually rewritten utterances DPH &amp; TCT-ColBERT Bo1 + doc2query monoQA uogTr-AT raw utterances+ full context DPH &amp; TCT-ColBERT Bo1 + doc2query monoQA uogTr-MI-HB raw utterances+ output from the sub-task DPH &amp; TCT-ColBERT Bo1+ doc2query monoQA text generation model to effectively address the tasks of clarification need classification and clarifying question generation. Our T5MI model aims to identify whether a user utterance requires a clarifying question and, accordingly, generates a clarifying question using the rewritten question from the query rewriting model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,53.50,85.73,504.89,215.44"><head>Table 2 :</head><label>2</label><figDesc>Results on the TREC Conversational Assistance Track 2022 Mixed-Initiative sub-task. The best performing run for each measure is emphasised.</figDesc><table coords="3,53.80,119.77,361.00,181.39"><row><cell>Approach</cell><cell cols="3">Relevance@1 Novelty@1 Diversity@1</cell></row><row><cell></cell><cell cols="2">generation baselines</cell><cell></cell></row><row><cell>GPT-3 Raw</cell><cell>0.433</cell><cell>0.263</cell><cell>0.356</cell></row><row><cell>GPT-3 Rewrite</cell><cell>0.454</cell><cell>0.346</cell><cell>0.371</cell></row><row><cell>GPT-3 Full</cell><cell>0.119</cell><cell>0.073</cell><cell>0.082</cell></row><row><cell>T5 Raw</cell><cell>0.232</cell><cell>0.166</cell><cell>0.185</cell></row><row><cell>T5 Rewrite</cell><cell>0.320</cell><cell>0.229</cell><cell>0.210</cell></row><row><cell></cell><cell cols="2">selection baselines</cell><cell></cell></row><row><cell>BM25</cell><cell>0.345</cell><cell>0.293</cell><cell>0.307</cell></row><row><cell>miniLM-BERT</cell><cell>0.371</cell><cell>0.317</cell><cell>0.395</cell></row><row><cell cols="3">hybrid of generation and selection</cell><cell></cell></row><row><cell>uogTr-MI</cell><cell>0.567</cell><cell>0.494</cell><cell>0.369</cell></row><row><cell cols="2">Generation: The generation baseline runs use GPT3 and T5 with</cell><cell></cell><cell></cell></row><row><cell cols="2">different types of input to generate questions. The T5 model was</cell><cell></cell><cell></cell></row><row><cell>finetuned on the ClariQ dataset:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.50,85.73,504.89,120.30"><head>Table 3 :</head><label>3</label><figDesc>Results on the TREC Conversational Assistance Track 2022 Conversational Search task. The best performing run for each measure is emphasised.</figDesc><table coords="4,113.89,119.77,381.98,86.25"><row><cell>Measure</cell><cell>Min</cell><cell>TREC Per-Topic Median Max</cell><cell cols="5">Baseline automatic manual uogTr-AT uogTr-MI-HB uogTr-MT Official Runs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Lenient</cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell cols="2">0.0180 0.1768 0.4397</cell><cell>0.1628</cell><cell>0.2377</cell><cell>0.1448</cell><cell>0.1679</cell><cell>0.3391</cell></row><row><cell cols="3">NDCG@20 0.0356 0.3203 0.6674</cell><cell>0.3048</cell><cell>0.4333</cell><cell>0.2993</cell><cell>0.3105</cell><cell>0.4950</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strict</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell cols="2">0.0128 0.1479 0.4265</cell><cell>0.1498</cell><cell>0.2309</cell><cell>0.1343</cell><cell>0.1700</cell><cell>0.2489</cell></row><row><cell cols="3">NDCG@20 0.0356 0.3204 0.6674</cell><cell>0.3048</cell><cell>0.4333</cell><cell>0.2993</cell><cell>0.3143</cell><cell>0.4639</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Dr Sean MacAvaney</rs> for his assistance in preparing the format of the runs.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="4,333.39,320.23,224.81,6.18;4,333.39,328.20,224.81,6.18;4,333.39,336.12,225.88,6.23;4,333.39,344.14,85.92,6.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,423.22,328.20,134.98,6.18;4,333.39,336.17,49.59,6.18">Generating clarifying questions for open-domain dialogue systems</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Kiseleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleksandr</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikhail</forename><surname>Burtsev</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2009.11352</idno>
		<idno type="arXiv">arXiv:2009.11352</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2009.11352" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ClariQ). arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,352.11,224.81,6.18;4,333.39,360.08,225.88,6.18;4,333.39,368.00,190.08,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,492.50,352.11,65.70,6.18;4,333.39,360.08,222.41,6.18">Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582416</idno>
		<ptr target="https://doi.org/10.1145/582415.582416" />
	</analytic>
	<monogr>
		<title level="j" coord="4,333.39,368.00,58.77,6.23">ACM Trans. Inf. Syst</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,376.02,224.99,6.18;4,333.18,383.94,226.09,6.23;4,333.39,391.96,97.70,6.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,511.85,376.02,46.54,6.18;4,333.18,383.99,140.52,6.18">Can You Unpack That? Learning to Rewrite Questions-in-Context</title>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denis</forename><surname>Peskov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D19-1605" />
	</analytic>
	<monogr>
		<title level="m" coord="4,486.75,383.94,34.76,6.23">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5918" to="5924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,399.93,224.99,6.18;4,333.39,407.85,225.64,6.23;4,333.17,415.87,114.83,6.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,491.39,399.93,66.99,6.18;4,333.39,407.90,48.77,6.18">Billion-Scale Similarity Search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2019.2921572</idno>
		<ptr target="https://doi.org/10.1109/TBDATA.2019.2921572" />
	</analytic>
	<monogr>
		<title level="m" coord="4,395.33,407.85,97.53,6.23">Proc. IEEE Transactions on Big Data</title>
		<meeting>IEEE Transactions on Big Data</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="535" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,423.84,225.58,6.18;4,333.39,431.81,224.81,6.18;4,333.39,439.73,225.63,6.23;4,333.17,447.75,131.78,6.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,473.20,431.81,85.00,6.18;4,333.39,439.78,110.54,6.18">UnifiedQA: Crossing Format Boundaries with a Single QA System</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.171</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.findings-emnlp.171" />
	</analytic>
	<monogr>
		<title level="m" coord="4,458.85,439.73,71.05,6.23">Proc. EMNLP. 1896-1907</title>
		<meeting>EMNLP. 1896-1907</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,455.72,225.99,6.18;4,332.84,463.69,226.54,6.18;4,333.39,471.61,225.88,6.23;4,333.39,479.63,72.70,6.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,512.70,455.72,46.68,6.18;4,332.84,463.69,226.54,6.18;4,333.39,471.66,80.68,6.18">monoQA: Multi-Task Learning of Reranking and Answer Extraction for Open-Retrieval Conversational Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Sarawoot</forename><surname>Kongyoung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.485" />
	</analytic>
	<monogr>
		<title level="m" coord="4,426.63,471.61,33.62,6.23">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7207" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,487.60,224.81,6.18;4,333.19,495.57,225.01,6.18;4,333.15,503.49,225.51,6.23;4,333.39,511.51,47.58,6.18" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Tingting</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.00266</idno>
		<idno type="arXiv">arXiv:2204.00266</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.00266" />
		<title level="m" coord="4,359.76,495.57,198.44,6.18;4,333.15,503.54,30.20,6.18">Multifaceted Improvements for Conversational Open-Domain Question Answering</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,519.48,224.81,6.18;4,333.39,527.40,224.81,6.23;4,333.39,535.37,188.28,6.23" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2010.11386</idno>
		<idno type="arXiv">arXiv:2010.11386</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2010.11386" />
		<title level="m" coord="4,512.13,519.48,46.07,6.18;4,333.39,527.45,173.89,6.18">Distilling dense representations for ranking using tightly-coupled teachers</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,543.39,224.99,6.18;4,333.39,551.31,225.63,6.23;4,333.17,559.33,119.88,6.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,496.96,543.39,61.42,6.18;4,333.39,551.36,113.73,6.18">Contextualized Query Embeddings for Conversational Search</title>
		<author>
			<persName coords=""><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.77</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.emnlp-main.77" />
	</analytic>
	<monogr>
		<title level="m" coord="4,461.13,551.31,35.28,6.23">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1004" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,567.30,224.81,6.18;4,333.39,575.27,225.88,6.18;4,333.39,583.19,214.17,6.23" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,503.17,567.30,55.03,6.18;4,333.39,575.27,223.16,6.18">In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.repl4nlp-1.17</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.repl4nlp-1.17" />
	</analytic>
	<monogr>
		<title level="m" coord="4,340.99,583.19,41.79,6.23">Proc. RepL4NLP</title>
		<meeting>RepL4NLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,591.21,225.58,6.18;4,333.16,599.18,225.04,6.18;4,333.39,607.10,225.89,6.23;4,333.39,615.12,72.70,6.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="4,542.46,599.18,15.75,6.18;4,333.39,607.15,115.23,6.18">Large dual encoders are generalizable retrievers</title>
		<author>
			<persName coords=""><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.669" />
	</analytic>
	<monogr>
		<title level="m" coord="4,460.44,607.10,33.28,6.23">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,623.09,224.81,6.18;4,333.39,631.01,225.88,6.23;4,333.23,639.03,173.74,6.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="4,529.04,623.09,29.16,6.18;4,333.39,631.06,160.77,6.18">Document Ranking with a Pretrained Sequence-to-Sequence Model</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.63</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.findings-emnlp.63" />
	</analytic>
	<monogr>
		<title level="m" coord="4,506.86,631.01,49.60,6.23">Proc. EMNLP 2020</title>
		<meeting>EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,647.00,224.81,6.18;4,333.39,654.92,225.26,6.23;4,333.39,662.94,164.94,6.18" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="4,501.41,647.00,56.79,6.18;4,333.39,654.97,47.09,6.18">From doc2query to docTTTTTquery</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename><surname>Epistemic</surname></persName>
		</author>
		<ptr target="https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_docTTTTTquery-v2.pdf" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Online preprint</note>
</biblStruct>

<biblStruct coords="4,333.39,670.91,225.99,6.18;4,333.39,678.88,224.99,6.18;4,333.39,686.80,217.70,6.23" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="4,402.90,678.88,155.48,6.18;4,333.39,686.85,173.09,6.18">CAsT 2022: TREC CAsT 2022 Going Beyond User Ask and System Retrieve with Initiative and Response Generation</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Owoicho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanne</forename><forename type="middle">R</forename><surname>Trippas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,518.87,686.80,28.79,6.23">Proc. TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.39,694.82,225.88,6.18;4,333.39,702.79,225.88,6.18" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="4,350.53,702.79,205.43,6.18">Weakly-Supervised Open-Retrieval Conversational Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,69.23,89.05,198.45,6.23" xml:id="b15">
	<monogr>
		<idno type="DOI">10.1007/978-3-030-72113-8_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72113-8_35" />
		<title level="m" coord="5,76.83,89.05,27.32,6.23">Proc. ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<biblScope unit="page" from="529" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,69.23,97.07,225.88,6.18;5,69.23,104.99,225.89,6.23;5,69.23,113.01,113.14,6.18" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="5,86.04,105.04,138.95,6.18">Open-retrieval conversational question answering</title>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401110</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401110" />
	</analytic>
	<monogr>
		<title level="m" coord="5,237.31,104.99,28.86,6.23">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,69.23,120.98,225.58,6.18;5,69.23,128.95,224.81,6.18;5,69.23,136.87,225.88,6.23;5,69.07,144.89,129.35,6.18" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="5,236.32,128.95,57.72,6.18;5,69.23,136.92,179.08,6.18">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="m" coord="5,261.58,136.87,33.54,6.23;5,69.07,144.89,12.07,6.18">Proc. JMLR. 1-67</title>
		<meeting>JMLR. 1-67</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,89.10,225.88,6.18;5,333.39,97.07,224.81,6.18;5,333.39,104.99,225.26,6.23;5,333.39,113.01,46.93,6.18" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="5,333.39,97.07,224.81,6.18;5,333.39,105.04,74.17,6.18">MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</title>
		<author>
			<persName coords=""><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.5555/3495724.3496209</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.5555/3495724.3496209" />
	</analytic>
	<monogr>
		<title level="m" coord="5,421.00,104.99,27.81,6.23">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,120.98,224.99,6.18;5,333.39,128.90,225.88,6.23;5,333.39,136.92,154.04,6.18" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="5,529.17,120.98,29.21,6.18;5,333.39,128.95,179.27,6.18">University of Glasgow Terrier Team at the TREC 2021 Deep Learning Track</title>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec30/papers/uogTr-DL.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="5,527.41,128.90,28.47,6.23">Proc. TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
