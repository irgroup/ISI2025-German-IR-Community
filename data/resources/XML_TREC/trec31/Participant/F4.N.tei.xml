<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.82,170.30,273.75,14.25">HNUST @ TREC 2022 NeuCLIR Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.77,219.68,37.59,9.05"><forename type="first">Ge</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Hunan University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">HNUST</orgName>
								<address>
									<postCode>411201</postCode>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.58,219.68,37.50,9.05"><forename type="first">Qiwen</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Hunan University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">HNUST</orgName>
								<address>
									<postCode>411201</postCode>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.05,219.68,70.81,9.05"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Hunan University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">HNUST</orgName>
								<address>
									<postCode>411201</postCode>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.31,219.68,45.73,9.05"><forename type="first">Dong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Hunan University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">HNUST</orgName>
								<address>
									<postCode>411201</postCode>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.82,170.30,273.75,14.25">HNUST @ TREC 2022 NeuCLIR Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6918EAFDB76B765624B4B8CD8E8B8A47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-language information retrieval</term>
					<term>Lexical-based CLIR method</term>
					<term>Neural-based CLIR method</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rapid development of deep learning, neural-based cross-language information retrieval (CLIR) has attracted extensive attention from researchers. To explore the effectiveness of neural-based CLIR, large-scale efforts, and new platforms are in need. To that end, the TREC 2022 NeuCLIR track presents a cross-language information retrieval challenge. This paper describes our first participation in the TREC 2022 NeuCLIR track. We explored two approaches for CLIR: (1) the lexical-based CLIR method and (2) the neural-based CLIR method, where the lexical-based method consists of two steps of translation and retrieval, and the neural-based method introduces the DISTILDistilmBERT model, an end-to-end neural network. In our preliminary results, the lexical-based CLIR method performs better than the neuralbased method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The demand for multilingual information on the Internet is growing rapidly, and CLIR has attracted extensive attention from researchers. CLIR is the task of retrieving documents in the target language ùêø ùë° with queries written in the source language ùêø ùë† , which can help users retrieve information from different languages <ref type="bibr" coords="1,396.93,651.52,12.78,9.05" target="#b0">[1,</ref><ref type="bibr" coords="1,413.07,651.52,7.22,9.05" target="#b1">2]</ref>. The neural network models have developed rapidly and are widely used in information retrieval tasks with advanced performance. However, CLIR does not perform very well in neural-based retrieval methods <ref type="bibr" coords="2,249.51,161.84,13.41,9.05" target="#b2">[3,</ref><ref type="bibr" coords="2,267.37,161.84,7.29,9.05" target="#b3">4]</ref>. The TREC 2022 NeuCLIR track proposed a neural CLIR challenge to explore the effectiveness of neural-based CLIR. In this track, the topics are written in English and the document collections are written in Chinese, Persian, and Russian respectively. For each topic, the retrieval system needs to return a ranked list of 1000 documents drawn from three document collections separately. This paper describes our first participation in the TREC 2022 NeuCLIR track. The track comprises three tasks in total, we only focus on the Ad Hoc CLIR. In this work, we explored two approaches for CLIR. The first one is the lexical-based CLIR method. This method consists of two steps, the first step is the translation of English topics into the target language, and the second step is retrieval based on probabilistic models (TF_IDF <ref type="bibr" coords="2,156.87,317.87,13.74,9.05" target="#b4">[5]</ref>, BM25 <ref type="bibr" coords="2,197.24,317.87,14.89,9.05" target="#b5">[6]</ref>, and PL2 <ref type="bibr" coords="2,248.85,317.87,12.77,9.05" target="#b6">[7]</ref>). The second one is the neural-based CLIR method.</p><p>Nowadays, multilingual text encoders pre-trained on more than 100 languages, have become the standard for multilingual representation learning and cross-language transfer in natural language processing, such as mBERT <ref type="bibr" coords="2,357.51,364.67,16.66,9.05" target="#b7">[8]</ref> or XLM <ref type="bibr" coords="2,406.96,364.67,15.59,9.05" target="#b8">[9]</ref>. However, Litschko et al. argued that pre-trained multilingual encoders produce poor sentence embeddings, resulting in lower performance on unsupervised text similarity tasks <ref type="bibr" coords="2,450.98,395.87,15.88,9.05" target="#b9">[10]</ref>.</p><p>And they think encoders specialized for semantic similarity like DISTILDistilmBERT are supposed to encode sentence meaning more accurately. Therefore, we use the pretrained multilingual encoder DISTILDistilmBERT to complete the end-to-end neural-based CLIR. Due to the time constraint and the limited resources, we only managed to submit three baseline results based on the PL2 model. The remainder of this paper is structured as follows. In Section 2 we give an overview of the NeuCLIR track. Next, in Section 3 we present the implementation details of the CLIR system used in this work. And Section 4 analyzes the experimental results. Lastly, we conclude our findings and works in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>The TREC 2022 NeuCLIR track includes three tasks. The main task is Ad Hoc CLIR.</p><p>For each English topic, the retrieval system needs to return a ranked list of 1000 documents drawn from the target language document collections. The remaining two tasks are: (1) a reranking task based on the Ad Hoc CLIR; (2) a monolingual retrieval task, which is the same as the Ad Hoc CLIR, except that the English topic is manually translated into the target language. An overview of the datasets provided by NeuCLIR is shown in Table <ref type="table" coords="3,197.23,193.04,3.76,9.05" target="#tab_0">1</ref>, and all data are stored in JSONL format. 3 Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Lexical-based CLIR Method</head><p>Traditional CLIR methods use existing machine translation systems to translate documents or queries so that queries and documents are in the same language <ref type="bibr" coords="3,449.64,534.73,16.95,9.05" target="#b10">[11]</ref>.</p><p>During the whole experiment, the lexical-based CLIR method we used can be summarized into two stages: translation and retrieval. Fig. <ref type="figure" coords="3,359.92,565.93,4.98,9.05">1</ref> shows an overview of our lexical-based CLIR system. We preprocessed the data as follows. Firstly, We translated the topics into the target language by calling the Google Translate API <ref type="foot" coords="3,403.27,596.15,3.24,5.83" target="#foot_0">1</ref> . And each topic is composed of a short title and a sentence-length description from the given dataset.</p><p>Then, for the Chinese document collection, we converted traditional Chinese to simplified Chinese with the tools provided by NeuCLIR<ref type="foot" coords="4,347.71,145.23,3.24,5.83" target="#foot_1">2</ref> . Finally, we indexed all target language document collections by the platform PyTerrier<ref type="foot" coords="4,353.83,160.83,3.24,5.83" target="#foot_2">3</ref> . For the retrieval step, three probabilistic models of TF_IDF, BM25, and PL2 in PyTerrier were used to conduct experiments.</p><p>Fig. <ref type="figure" coords="4,215.78,311.72,4.50,8.04">1</ref> The Pipeline of the Lexical-based CLIR Method</p><p>We performed a test experiment on the partial training set of the dataset HC4 <ref type="bibr" coords="4,447.04,338.87,19.03,9.05" target="#b11">[12]</ref>.</p><p>The preliminary test results are shown in Table <ref type="table" coords="4,315.04,354.47,3.76,9.05" target="#tab_1">2</ref>. From the test results, the PL2 model outperformed the other two models. For this, the three baseline results we finally submitted were all obtained by the PL2 model. After the submission, we conducted experiments on the complete HC4 training data set and the results are shown in Table <ref type="table" coords="4,124.82,416.87,3.77,9.05" target="#tab_2">3</ref>. However, the retrieval based on the PL2 model does not get the best results in this work. In addition, these results also show that the retrieval performance improves when traditional Chinese documents are converted to simplified Chinese ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Neural-based CLIR Method</head><p>In the neural-based method, our experiment mainly includes three steps: data preprocessing, model construction, and retrieval. Firstly, we preprocess the data which is in English, Persian, Chinese, and Russian. Then, the text is encoded using a multilingual neural model. Finally, the relevant documents are retrieved and returned.</p><p>Fig. <ref type="figure" coords="5,143.39,497.77,4.98,9.05">2</ref> shows an overview of our neural-based CLIR system. Next, the details of these three steps are described as follows.</p><p>Fig. <ref type="figure" coords="5,216.98,616.45,4.50,8.04">2</ref> The Pipeline of the Neural-based CLIR Method Data preprocessing includes four parts: tokenization, stop word removal, stemming, and term extension. For each topic, we directly used the data provided by the task organizers and removed the invalid characters. For the document, we took the NeuCLIR1 document collections provided by the task organizers and extracted 'title' and 'detail' from the document collections as document data. For the Persian and Russian document collections, we used the NLTK<ref type="foot" coords="6,336.67,223.23,3.24,5.83" target="#foot_3">4</ref> package. And for the Chinese document collection, we used the jieba <ref type="foot" coords="6,280.37,238.83,3.24,5.83" target="#foot_4">5</ref> package.</p><p>The neural model DISTILDistilmBERT we chose is introduced by Litschko <ref type="bibr" coords="6,423.64,255.47,16.72,9.05" target="#b9">[10]</ref>. It is a multilingual model based on knowledge distillation between the teacher-student framework. The teacher model is Sentence-BERT <ref type="bibr" coords="6,333.69,286.67,18.62,9.05" target="#b11">[12]</ref>, and the student model is DsitilmBERT <ref type="bibr" coords="6,175.81,302.27,18.54,9.05" target="#b12">[13]</ref>. Sentence-BERT is a modification of pre-trained BERT, which can derive semantically meaningful sentence embeddings. The teacher model first learns professional semantic similarity knowledge, then injects this knowledge into the student model, so that the student model can simulate the output of the teacher model.</p><p>We used the trained student model to encode the topic and document respectively.</p><p>After vector length normalization, the model performs retrieval by computing cosine similarity. Since the document collections are all very large and one retrieval is very time-consuming, we divide the documents into 10 copies and select the top 2000 documents for each retrieval. The final results are generated from these 20,000 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>In this work, both the lexical-based method and the neural method have experimented on the NeuCLIR1 dataset. According to the latest relevance judgments released by the TREC 2022 NeuCLIR track, the evaluations of our results using ir-measures<ref type="foot" coords="6,449.50,564.08,3.24,5.83" target="#foot_5">6</ref> are shown in Table <ref type="table" coords="6,187.66,580.69,3.76,9.05" target="#tab_3">4</ref>. Among them, bold represents the optimal result, and * represents the result we submitted to the organizing committee.</p><p>The results in Table <ref type="table" coords="7,235.27,146.24,4.98,9.05" target="#tab_3">4</ref> show that the neural-based method based on the DISTILDistilmBERT model performs best in the target language Russian and worst in  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,124.82,177.44,345.64,9.05;7,124.82,193.04,345.75,9.05;7,124.82,208.64,345.84,9.05;7,124.82,224.24,345.78,9.05;7,124.82,239.84,345.97,9.05;7,124.82,255.47,345.94,9.05;7,124.82,271.07,345.65,9.05;7,124.82,286.67,345.51,9.05;7,124.82,302.27,215.12,9.05"><head></head><label></label><figDesc>Persian. The reason may be that the Russian document collection has the largest number of documents and the Persian the least. And the evaluations of the results retrieved by the DISTILDistilmBERT model are all lower than that of probabilistic models. And as stated in Section 3.1, the retrieval performance improves when the traditional Chinese documents are converted to simplified ones. To sum up, in this work, the lexical-based retrieval method with probabilistic models outperforms the neural-based retrieval method with DISTILDistilmBERT model, and TF_IDF performed best in the probability models. Due to the time constraint and the limited resources, we only managed to submit three baseline results based on the PL2 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,131.30,220.94,326.02,191.33"><head>Table 1</head><label>1</label><figDesc>An Overview of the Datasets Provided by NeuCLIR</figDesc><table coords="3,131.30,243.17,326.02,169.11"><row><cell></cell><cell>Topics</cell><cell></cell><cell cols="2">Documents</cell></row><row><cell>Dataset Name</cell><cell>Language</cell><cell>Quantity</cell><cell>Language</cell><cell>Quantity</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Chinese</cell><cell>646305</cell></row><row><cell>HC4-train</cell><cell>English</cell><cell>27</cell><cell>Persian</cell><cell>486486</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Russian</cell><cell>4721064</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Chinese</cell><cell>3,179,209</cell></row><row><cell>NeuCLIR1</cell><cell>English</cell><cell>136</cell><cell>Persian</cell><cell>2,232,016</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Russian</cell><cell>4,627,543</cell></row><row><cell></cell><cell>Chinese</cell><cell></cell><cell>En-Zho</cell><cell>3,179,209</cell></row><row><cell>NeuCLIR-translations</cell><cell>Persian</cell><cell>136</cell><cell>En-Fas</cell><cell>2,232,016</cell></row><row><cell></cell><cell>Russian</cell><cell></cell><cell>En-Rus</cell><cell>4,627,543</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.78,475.99,329.53,94.70"><head>Table 2</head><label>2</label><figDesc>Test Results Retrieved by Probabilistic Models</figDesc><table coords="4,134.78,498.22,329.53,72.48"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Metrics</cell><cell></cell></row><row><cell>Language</cell><cell>Method</cell><cell>RR(rel=2)</cell><cell>AP(rel=2)</cell><cell>nDCG@10</cell><cell>nDCG@100</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.0603</cell><cell>0.0502</cell><cell>0.2231</cell><cell>0.3958</cell></row><row><cell>Chinese</cell><cell>BM25</cell><cell>0.0554</cell><cell>0.0483</cell><cell>0.2255</cell><cell>0.3988</cell></row><row><cell></cell><cell>PL2</cell><cell>0.0790</cell><cell>0.0577</cell><cell>0.2484</cell><cell>0.4095</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,133.70,146.54,327.46,239.69"><head>Table 3</head><label>3</label><figDesc>Results Retrieved by Probabilistic Models on the Training Set of HC4 Dataset</figDesc><table coords="5,133.70,168.77,327.46,217.47"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Metrics</cell><cell></cell></row><row><cell>Language</cell><cell>Method</cell><cell>RR(rel=2)</cell><cell>AP(rel=2)</cell><cell>nDCG@10</cell><cell>nDCG@100</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.1083</cell><cell>0.0532</cell><cell>0.2447</cell><cell>0.3853</cell></row><row><cell>Chinese</cell><cell>BM25</cell><cell>0.1058</cell><cell>0.0506</cell><cell>0.2350</cell><cell>0.3804</cell></row><row><cell></cell><cell>PL2</cell><cell>0.1068</cell><cell>0.0521</cell><cell>0.2272</cell><cell>0.3707</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.1082</cell><cell>0.0538</cell><cell>0.2323</cell><cell>0.3835</cell></row><row><cell>Simplified Chinese</cell><cell>BM25</cell><cell>0.1056</cell><cell>0.0510</cell><cell>0.2360</cell><cell>0.3816</cell></row><row><cell></cell><cell>PL2</cell><cell>0.1068</cell><cell>0.0518</cell><cell>0.2278</cell><cell>0.3714</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.3381</cell><cell>0.2836</cell><cell>0.4542</cell><cell>0.6102</cell></row><row><cell>Persian</cell><cell>BM25</cell><cell>0.3239</cell><cell>0.2802</cell><cell>0.5017</cell><cell>0.6336</cell></row><row><cell></cell><cell>PL2</cell><cell>0.2596</cell><cell>0.2580</cell><cell>0.4065</cell><cell>0.5667</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.2622</cell><cell>0.1904</cell><cell>0.3340</cell><cell>0.4615</cell></row><row><cell>Russian</cell><cell>BM25</cell><cell>0.2381</cell><cell>0.1702</cell><cell>0.2961</cell><cell>0.4442</cell></row><row><cell></cell><cell>PL2</cell><cell>0.2286</cell><cell>0.1793</cell><cell>0.2928</cell><cell>0.4235</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,132.62,330.17,332.27,287.95"><head>Table 4</head><label>4</label><figDesc>Final Scores for Both Retrieval Methods This paper briefly introduces what is a CLIR task and the partial information about the TREC 2022 NeuCLIR track, and then describes the work we tried with the lexicalbased CLIR method and the neural-based CLIR method. Finally, according to the latest relevance judgments released by NeuCLIR, the final results of the two methods were evaluated using ir-measures. The evaluations show that the lexical-based CLIR method based on the probability models performs better than the neural-based CLIR method based on the DISTILDistilmBERT model in this work.</figDesc><table coords="7,132.62,352.40,332.27,265.73"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Metrics</cell><cell></cell></row><row><cell>Language</cell><cell>Method</cell><cell>map</cell><cell>nDCG@20</cell><cell cols="2">recall@100 recall@1000</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.1060</cell><cell>0.1883</cell><cell>0.2012</cell><cell>0.3703</cell></row><row><cell>Chinese</cell><cell>BM25</cell><cell>0.0920</cell><cell>0.1747</cell><cell>0.1993</cell><cell>0.3548</cell></row><row><cell></cell><cell>PL2</cell><cell>0.0839*</cell><cell>0.1567*</cell><cell>0.1649*</cell><cell>0.3066*</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.2392</cell><cell>0.3468</cell><cell>0.4848</cell><cell>0.7111</cell></row><row><cell>Simplified</cell><cell>BM25</cell><cell>0.2240</cell><cell>0.3254</cell><cell>0.4686</cell><cell>0.6975</cell></row><row><cell>Chinese</cell><cell>PL2</cell><cell>0.1852*</cell><cell>0.2788*</cell><cell>0.3681*</cell><cell>0.6258*</cell></row><row><cell></cell><cell>DISTILDistilmBERT</cell><cell>0.1079</cell><cell>0.1869</cell><cell>0.2348</cell><cell>0.4802</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.2080</cell><cell>0.3155</cell><cell>0.4850</cell><cell>0.7273</cell></row><row><cell></cell><cell>BM25</cell><cell>0.1081</cell><cell>0.1798</cell><cell>0.3583</cell><cell>0.5736</cell></row><row><cell>Persian</cell><cell>PL2</cell><cell>0.1109*</cell><cell>0.1810*</cell><cell>0.3303*</cell><cell>0.5766*</cell></row><row><cell></cell><cell>DISTILDistilmBERT</cell><cell>0.0821</cell><cell>0.1605</cell><cell>0.2946</cell><cell>0.5376</cell></row><row><cell></cell><cell>TF_IDF</cell><cell>0.2487</cell><cell>0.3542</cell><cell>0.4482</cell><cell>0.7122</cell></row><row><cell></cell><cell>BM25</cell><cell>0.2242</cell><cell>0.3132</cell><cell>0.4207</cell><cell>0.6850</cell></row><row><cell>Russian</cell><cell>PL2</cell><cell>0.2135*</cell><cell>0.2982*</cell><cell>0.3794*</cell><cell>0.6641*</cell></row><row><cell></cell><cell>DISTILDistilmBERT</cell><cell>0.1491</cell><cell>0.2537</cell><cell>0.3066</cell><cell>0.5615</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,132.38,672.46,180.41,8.18"><p>http://translate.google.cn/m?q=%s&amp;tl=%s&amp;sl=%s</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,132.38,656.86,180.30,8.18"><p>https://github.com/NeuCLIR/download-collection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,132.38,672.46,149.38,8.18"><p>https://pyterrier.readthedocs.io/en/latest/#</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,132.38,641.26,70.69,8.18"><p>http://www.nltk.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,132.38,656.86,105.03,8.18"><p>https://pypi.org/project/jieba/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,132.38,672.46,103.49,8.18"><p>https://ir-measur.es/en/latest/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,128.23,370.97,342.63,8.18;8,141.86,386.57,178.66,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,252.15,370.97,218.70,8.18;8,141.86,386.57,18.20,8.18">Neural Topic-enhanced Cross-lingual Word Embeddings for CLIR</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,166.92,386.57,76.64,8.18">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">608</biblScope>
			<biblScope unit="page" from="809" to="824" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.23,402.17,342.65,8.18;8,141.86,417.77,235.73,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,311.22,402.17,159.65,8.18;8,141.86,417.77,77.01,8.18">Translation Techniques in Cross-language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Truran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brailsford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,224.12,417.77,94.50,8.18">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.23,433.39,342.30,8.18;8,141.86,448.99,328.59,8.18;8,141.86,464.59,328.61,8.18;8,141.86,480.19,164.27,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,281.86,433.39,188.66,8.18;8,141.86,448.99,57.54,8.18">Training Effective Neural CLIR by Bridging the Translation Gap</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,221.35,448.99,249.10,8.18;8,141.86,464.59,223.00,8.18">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.23,495.79,342.36,8.18;8,141.86,511.39,328.68,8.18;8,141.86,526.99,328.87,8.18;8,141.86,542.59,269.04,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,249.77,495.79,220.82,8.18;8,141.86,511.39,153.56,8.18">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,314.64,511.39,155.90,8.18;8,141.86,526.99,324.90,8.18">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.23,558.19,342.25,8.18;8,141.86,573.79,328.71,8.18;8,141.86,589.39,191.99,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,192.68,558.19,277.79,8.18;8,141.86,573.79,51.58,8.18">A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,212.00,573.79,258.58,8.18;8,141.86,589.39,26.38,8.18">Proceedings of the 14th International Conference on Machine Learning (ICML)</title>
		<meeting>the 14th International Conference on Machine Learning (ICML)<address><addrLine>Nashville, Tennessee, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="143" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.23,605.02,342.59,8.18;8,141.86,620.62,328.63,8.18;8,141.86,636.22,328.69,8.18;8,141.86,651.82,138.59,8.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,251.17,605.02,200.34,8.18">Versioning a Full-text Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Anick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,141.86,620.62,328.63,8.18;8,141.86,636.22,175.46,8.18">Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="98" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.23,146.54,342.26,8.18;9,141.86,162.14,328.94,8.18;9,141.86,177.74,70.04,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,260.81,146.54,209.67,8.18;9,141.86,162.14,163.22,8.18">Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,311.65,162.14,159.15,8.18">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.23,193.34,342.52,8.18;9,141.86,208.94,328.56,8.18;9,141.86,224.54,328.82,8.18;9,141.86,240.14,330.99,8.18;9,141.86,255.77,107.86,8.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,308.37,193.34,162.38,8.18;9,141.86,208.94,153.85,8.18">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,315.25,208.94,155.17,8.18;9,141.86,224.54,328.82,8.18;9,141.86,240.14,121.26,8.18">Proceedings of the 17th Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the 17th Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.23,271.37,342.41,8.18;9,141.86,286.97,328.85,8.18;9,141.86,302.57,74.02,8.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,232.27,271.37,153.16,8.18">Cross-lingual Language Model Pretraining</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,403.56,271.37,67.07,8.18;9,141.86,286.97,263.53,8.18">Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 33rd Conference on Neural Information Processing Systems (NeurIPS)<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,318.17,338.00,8.18;9,141.86,333.77,328.91,8.18;9,141.86,349.37,287.40,8.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,311.05,318.17,159.35,8.18;9,141.86,333.77,136.49,8.18">Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Litschko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vuliƒá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,295.69,333.77,175.09,8.18;9,141.86,349.37,71.05,8.18">Proceedings of the 43rd European Conference on IR Research (ECIR)</title>
		<meeting>the 43rd European Conference on IR Research (ECIR)</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="342" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.16,364.97,338.61,8.18;9,141.86,380.57,328.93,8.18;9,141.86,396.17,328.70,8.18;9,141.86,411.77,65.34,8.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,195.96,364.97,203.29,8.18">A Study of Neural Matching Models for Cross-lingual IR</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,417.17,364.97,53.60,8.18;9,141.86,380.57,328.93,8.18;9,141.86,396.17,107.20,8.18">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1637" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,427.39,338.18,8.18;9,141.86,442.99,328.87,8.18;9,141.86,458.59,220.68,8.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,293.19,427.39,177.40,8.18;9,141.86,442.99,18.20,8.18">HC4: A New Suite of Test Collections for Ad Hoc CLIR</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,179.87,442.99,253.13,8.18">Proceedings of the 44th European Conference on IR Research (ECIR)</title>
		<meeting>the 44th European Conference on IR Research (ECIR)<address><addrLine>Stavanger, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,474.19,338.07,8.18;9,141.86,489.79,328.75,8.18;9,141.86,505.39,330.99,8.18;9,141.86,520.99,65.34,8.18" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,238.64,474.19,231.83,8.18;9,141.86,489.79,83.65,8.18">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,244.15,489.79,226.46,8.18;9,141.86,505.39,141.33,8.18">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4512" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
