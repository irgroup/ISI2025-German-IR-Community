<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.43,115.96,290.49,12.62">CIP at TREC 2022 Deep Learning Track</title>
				<funder>
					<orgName type="full">University of Chinese Academy of Sciences</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.87,154.73,38.46,8.74"><forename type="first">Jian</forename><surname>Luo</surname></persName>
							<email>luojian222@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.23,154.73,51.62,8.74"><forename type="first">Xinlin</forename><surname>Peng</surname></persName>
							<email>pengxinlin22@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.74,154.73,65.04,8.74"><forename type="first">Xuanang</forename><surname>Chen</surname></persName>
							<email>chenxuanang19@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.67,154.73,32.25,8.74"><forename type="first">Ben</forename><surname>He</surname></persName>
							<email>benhe@ucas.ac.cn</email>
						</author>
						<author>
							<persName coords="1,249.73,166.68,30.58,8.74"><surname>Le Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.24,166.68,50.92,8.74"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
							<email>yfsun@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.43,115.96,290.49,12.62">CIP at TREC 2022 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DFB74648B2E5E96CA183E96D51098C62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the CIP participation in the TREC 2022 Deep Learning Track. We submitted runs to both full ranking and re-ranking subtasks of the passage ranking task. In the full ranking subtask, we adopt a query noise resistant dense retrieval model RoDR. In the re-ranking subtask, we adopt localized contrastive estimation loss and hinge loss rather than pointwise cross-entropy loss for training rerankers. Besides, We utilize both the MS MARCO v1 and v2 passage datasets to generate hopefully sufficient training data, and our models are fine-tuned on these two kinds of training data one by one selectively. Additionally, we introduce docT5query to further enhance the performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CIP participation in the TREC 2022 Deep Learning (DL) track focuses on the full ranking and re-ranking subtasks of the passage ranking task.</p><p>In the full ranking subtask, our submissions are based on the RoDR model <ref type="bibr" coords="1,467.30,491.42,9.96,8.74" target="#b0">[1]</ref>, which adopts a query noise resistant dense retrieval training method. We submitted six runs for this subtask including three official runs and three additional runs. The official runs are re-ranked after dense retrieval, and the additional runs don't go though the re-ranking process. The training of dense retrieval models is performed on both MS MARCO v1 and v2 passage datasets. The BM25 <ref type="bibr" coords="1,470.07,551.20,10.52,8.74" target="#b1">[2]</ref> retrieval results of docT5query <ref type="bibr" coords="1,275.70,563.15,10.52,8.74" target="#b2">[3]</ref> are used to integrated with dense retrieval results to boost the performance.</p><p>In the re-ranking subtask, our main model is based on the BERT re-ranker <ref type="bibr" coords="1,467.30,596.34,9.96,8.74" target="#b3">[4]</ref>. Besides, we adopt Localized Contrastive Estimation (LCE) <ref type="bibr" coords="1,402.40,608.30,10.52,8.74" target="#b4">[5]</ref> and hinge loss rather than normal cross-entropy loss for model training. We use both the MS MARCO v1 and v2 passage datasets in turn with the initial top100 ranking files for fine-tuning the BERT re-ranker. Akin to the setting in full ranking subtask, the BM25 <ref type="bibr" coords="1,181.40,656.12,10.52,8.74" target="#b1">[2]</ref> retrieval results of docT5query <ref type="bibr" coords="1,332.44,656.12,10.52,8.74" target="#b2">[3]</ref> are also used in this part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Full Ranking</head><p>Dense retrieval (DR) technique has been shown effective in passage retrieval. DR models employ pre-trained language models (PLMs), like BERT <ref type="bibr" coords="2,446.79,177.39,9.96,8.74" target="#b5">[6]</ref>, and dual-encoder architecture to separately encode queries and passages into lowdimensional dense vectors, and adopt a lightweight similarity mechanism (e.g., dot product) for efficient ad-hoc retrieval. In this subtask, we use a query noise resistant DR model RoDR <ref type="bibr" coords="2,255.93,225.21,9.96,8.74" target="#b0">[1]</ref>. A noisy query training set for the origin query training set is generated and RoDR uses KL loss to align the in-batch local ranking between noisy query and origin query. The final training loss is the sum of KL loss and standard cross entropy DR loss. Except for standard DR, two more powerful DR models (Tas-Balanced <ref type="bibr" coords="2,343.47,273.03,10.52,8.74" target="#b6">[7]</ref> and PAIR <ref type="bibr" coords="2,407.96,273.03,10.79,8.74" target="#b7">[8]</ref>) are used for effectiveness. The docT5query is used to generate queries for passages. Each passage is appended with its predicted queries and then indexed by BM25. After dense retrieval, these BM25 scores on docT5query are interpolated with the dense retrieval scores for benefiting from multi-way matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Re-ranking</head><p>As the task of query-based passage re-ranking can be treated as a binary classification problem, pre-trained language models such as BERT have been widelyused. Even through being simply fine-tuned for passage re-ranking using the cross-entropy loss, binary classification model based on BERT-Large achieved decent results <ref type="bibr" coords="2,201.31,421.08,9.96,8.74" target="#b3">[4]</ref>. In this subtask, we use Localized Contrastive Estimation (LCE) <ref type="bibr" coords="2,166.10,433.04,10.52,8.74" target="#b4">[5]</ref> loss to fine-tune our BERT re-ranker, because it can further improve the re-ranking effectiveness in our experiments. For each query, we generate a passage group which contains a relevant (positive) passage example and several non-relevant (negative) passage examples. The negative passage examples were sampled from the top retrieval results. Based on our participation in the TREC 2021 DL track <ref type="bibr" coords="2,204.30,492.81,9.96,8.74" target="#b8">[9]</ref>, we remain to use the pairwise hinge loss to fine-tune the BERT re-ranker. Similar to full ranking subtask, we combine the re-ranking results with BM25 results on docT5query for better effectiveness. Note that when applying re-ranking models to our full rank retrieval results, we do not integrate the re-ranking results with docT5query results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>As for RoDR retriever, we adopt three kinds of base model: standard DR (based on bert-base-uncased), TAS-Balanced (based on distil-bert) <ref type="bibr" coords="4,393.55,357.08,10.52,8.74" target="#b6">[7]</ref> and PAIR (based on ERNIE-2.0 base) <ref type="bibr" coords="4,225.77,369.04,9.96,8.74" target="#b7">[8]</ref>. Those the models are fine-tuned on above two kinds of passage training data as described in Section 3.1 with the training order of Passage v1, Passage v2. And the trained models are denoted as RoDR w/ TAS and RoDR w/ PAIR. Besides, as for BERT re-ranker, we adopt the pre-trained BERT-Large models (bert-large-uncased), and they are fine-tuned on Passage v1 and Passage v2 datasets with above two kinds of training loss. The query has up to 32 tokens and the passage has up to 128 tokens for retrieval, while the query has up to 64 tokens and the passage has up to 512 tokens for re-ranking. As for docT5query ensemble, we adopt the original set of MS MARCO V2 Passage Expansion<ref type="foot" coords="4,180.01,475.06,3.97,6.12" target="#foot_0">1</ref> . Twenty queries are generated and expanded to per passage in MS MARCO v2 then the expanded passages are indexed with Anserini and finally the BM25 retrieval is utilized to obtained the results. Ensemble is based on the top 2k of BM25 retrieval results because the top results of our neural models could be ranked very far behind in BM25 retrieval results. Then the scores of BM25 results and our models' results are normalized and combined with a weight α. For our submitted official runs, the ensemble results of full ranking are further re-ranked by our models in the re-ranking subtask. The summary of our submitted runs in full ranking and re-ranking subtasks is shown as Table <ref type="table" coords="4,472.84,572.28,3.87,8.74" target="#tab_0">1</ref>.</p><p>We carry out our experiments on five TITAN RTX 24G GPUs. For the DR retrieval training, the learning rate is set as 5e-6 for the whole fine-tuning procedure with batch size of 16. And for the BERT re-ranker training, the learning rate is set as 1e-5 and the batch size is set as 64. Besides, the DR retrieval is trained for 4 epoch using both Passage v1 and Passage v2. And the BERT re-ranker is trained for 2 epochs using Passage v1 and 2 epochs using Passage v2. We save a checkpoint per 5,000 training steps, and select the checkpoint according to the best NDCG@10 in validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The evaluation results of our submitted runs for passage full ranking and reranking subtasks are shown in Table <ref type="table" coords="5,293.16,206.33,3.87,8.74" target="#tab_1">2</ref>. cip f1, cip f2, cip f3, cip f1 r, cip f2 r and cip f3 r are the runs of the full ranking subtask, and the first three are additional runs the last three are official runs. cip r1, cip r2 and cip r3 are the runs of the re-ranking subtask. For a more comprehensive comparison, we also present the validation results on test queries from TREC 2021 DL test set. From the above results, we find that cip f3 r outperforms other runs on Validation, meanwhile cip f2 r behaves better than other runs on TREC 2022 DL test set in terms of NDCG@10, P@10 and MAP, and cip f1 r achieved relatively balanced and good results on both validation and dev set of MSMARCO v2 in our experiment. Thus, cip f2 r is the best performing run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we describe the system based on PLM for both passage full ranking and re-ranking subtasks in TREC 2022 Deep Learning track. Our experiments demonstrate again that adopting re-ranking after full ranking can obtain better result and integrating with sparse retrieval result can improve the dense retrieval result. Meanwhile, we use a query noise resistant training strategy in full ranking retrieval model. In future work, we plan to investigate the matching strategy between full ranking retrieval methods and re-ranking retrieval methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,599.00,50.48,8.77;2,134.77,620.22,345.82,8.77;2,134.77,632.21,345.82,8.74;2,134.77,644.16,345.83,8.74;2,134.77,656.12,121.38,8.74;3,134.77,118.96,345.82,8.77;3,134.77,130.95,345.82,8.74;3,134.77,142.90,228.59,8.74;3,141.41,160.89,120.45,8.77;3,140.99,180.93,339.60,8.77;3,151.70,192.91,328.89,8.74;3,151.70,204.87,328.89,8.74;3,151.70,216.82,328.89,8.74;3,151.70,228.78,291.54,8.74;3,140.99,240.73,339.60,8.77;3,151.70,252.72,309.12,8.74;3,141.41,272.69,114.20,8.77;3,140.99,292.73,339.60,8.77;3,151.70,304.72,328.89,8.74;3,151.70,316.67,328.89,8.74;3,151.70,328.63,328.89,8.74;3,140.99,340.58,339.60,8.77;3,151.70,352.56,328.89,8.74;3,151.70,364.52,328.89,8.74;3,151.70,376.47,328.89,8.74;3,151.70,388.43,13.01,8.74;3,134.77,408.41,345.82,8.77;3,134.77,420.39,345.82,8.74;3,134.77,432.35,345.82,8.74;3,134.77,444.30,345.82,8.74;3,134.77,456.26,345.82,8.74;3,134.77,468.21,104.08,8.74"><head>3. 1</head><label>1</label><figDesc>DataNoisy query. Following RoDR<ref type="bibr" coords="2,276.72,620.25,9.96,8.74" target="#b0">[1]</ref>, we choose eight types of textual noise (like injected misspellings) for training. Each query in training set will be transferred to its noisy variation. In noisy queries training set, the number of queries for each noise type is the same.Training data. Although the MS MARCO v2 corpus has been released, we still use the passage dataset in the MS MARCO v1 corpus. Thus, in our experiments, totally two datasets are used for our model training: For full ranking models: -Passage v1: In the MS MARCO v1 passage dataset, a training sample consists of a query, a noisy version of the query, a positive passage and n negative passages. The number of negative passages is set as eight and negative passages are sampled from the top candidates of the base model's retrieval results. We construct about 0.40 million training samples. -Passage v2: In the MS MARCO v2 passage dataset, we construct about 0.28 million training samples with the same settings of the Passage v1. For re-ranking models: -Passage v1: In the MS MARCO v1 passage dataset, we generate 0.50 million training samples, each of those consists of a query, a positive passage and ten negative passages. The negative passages are sampled from the top candidates of the official provided top-1000 file for Passage v1 train queries. -Passage v2: In the MS MARCO v2 passage dataset, other settings are as same as the Passage v1, while the negative passages are sampled from the top candidates of the official provided top-100 file for Passage v2. we construct about 0.28 million training samples with the same settings of the Passage v1. Validation/Dev data. For our full rank models, we use the official dev sets (3,903 queries in Dev 1 and 4,281 queries in Dev 2) and official validation set (53 queries in TREC 2021 DL) of TREC 2022 DL for our model validation. When BERT re-ranker is trained, we use the official dev sets and official validation set. Additionally we use the official top-100 candidates of dev queries with Passage v2 for model validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,497.83,345.83,140.95"><head>Table 1 .</head><label>1</label><figDesc>The summary of submitted runs. α means the interpolation weight of our models' retrieval/re-ranking results in docT5query ensemble.</figDesc><table coords="3,155.03,531.35,308.37,107.42"><row><cell>Run ID</cell><cell>Retriever(α)</cell><cell cols="3">Re-ranker(α) Passage v1 Passage v2</cell></row><row><cell>cip f1</cell><cell>RoDR(0.5)</cell><cell>-</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="2">cip f2 RoDR w/ TAS(0.5)</cell><cell>-</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="2">cip f3 RoDR w/ PAIR(0.6)</cell><cell>-</cell><cell></cell><cell>✓</cell></row><row><cell>cip r1</cell><cell>-</cell><cell>LCE-Reranker(0.6)</cell><cell></cell><cell>✓</cell></row><row><cell>cip r2</cell><cell>-</cell><cell>Hinge-Reranker</cell><cell>✓</cell><cell></cell></row><row><cell>cip r3</cell><cell>-</cell><cell>LCE-Reranker(0.5)</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="2">cip f1 r RoDR w/ TAS(0.5)</cell><cell>LCE-Reranker</cell><cell>✓</cell><cell>✓</cell></row><row><cell>cip f2 r</cell><cell>RoDR(0.5)</cell><cell>LCE-Reranker</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="3">cip f3 r RoDR w/ PAIR(0.6) Hinge-Reranker</cell><cell>✓</cell><cell>✓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,115.91,345.83,182.49"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results on TREC 2022 DL test queries in the passage retrieval task. Dev shows the average score of the two dev sets (3,903 queries for Dev 1 and 4,281 queries for Dev 2) of TREC 2022 DL. Validation consists the 53 test queries from TREC 2021 DL track. The TREC 2022 DL test set consists of 76 queries. The best values are highlighted in boldface.</figDesc><table coords="4,176.61,180.52,262.14,117.88"><row><cell>Run ID</cell><cell cols="5">Dev NDCG@10 NDCG@10 MAP NDCG@10 P@10 Validation TREC 2022 DL Test</cell></row><row><cell>cip f1</cell><cell>0.1270</cell><cell>0.6385</cell><cell>0.1503</cell><cell>0.4987</cell><cell>0.4079</cell></row><row><cell>cip f2</cell><cell>0.1231</cell><cell>0.6279</cell><cell>0.1480</cell><cell>0.4929</cell><cell>0.4013</cell></row><row><cell>cip f3</cell><cell>0.1240</cell><cell>0.6326</cell><cell>0.1406</cell><cell>0.4781</cell><cell>0.3763</cell></row><row><cell>cip r1</cell><cell>0.1506</cell><cell>0.6014</cell><cell>0.0791</cell><cell>0.4264</cell><cell>0.2882</cell></row><row><cell>cip r2</cell><cell>0.1462</cell><cell>0.6569</cell><cell>0.0930</cell><cell>0.4839</cell><cell>0.3605</cell></row><row><cell>cip r3</cell><cell>0.1491</cell><cell>0.6212</cell><cell>0.0846</cell><cell>0.4549</cell><cell>0.3342</cell></row><row><cell>cip f1 r</cell><cell>0.2080</cell><cell>0.5230</cell><cell>0.1599</cell><cell>0.5007</cell><cell>0.4461</cell></row><row><cell>cip f2 r</cell><cell>0.2094</cell><cell>0.4814</cell><cell>0.1752</cell><cell>0.5776</cell><cell>0.4882</cell></row><row><cell>cip f3 r</cell><cell>0.1762</cell><cell>0.7034</cell><cell>0.1736</cell><cell>0.5740</cell><cell>0.4789</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,656.80,190.86,7.86"><p>https://github.com/castorini/docTTTTTquery</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">University of Chinese Academy of Sciences</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,557.67,342.25,7.86;5,146.91,568.63,333.68,7.86;5,146.91,579.59,168.23,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,321.75,557.67,158.84,7.86;5,146.91,568.63,71.06,7.86">Towards robust dense retrieval via local ranking alignment</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,238.32,568.63,35.38,7.86">IJCAI-22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1980" to="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,590.71,342.24,7.86;5,146.91,601.67,333.68,7.86;5,146.91,612.63,58.64,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,227.72,590.71,252.88,7.86;5,146.91,601.67,24.93,7.86">BM25 pseudo relevance feedback using anserini at waseda university</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="5,191.22,601.67,169.70,7.86">SIGIR 2019. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2409</biblScope>
			<biblScope unit="page" from="62" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,623.76,342.24,7.86;5,146.91,634.69,149.31,7.89" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,326.64,623.76,153.95,7.86;5,146.91,634.71,14.75,7.86">Document expansion by query prediction</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR abs/1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,645.84,342.24,7.86;5,146.91,656.80,97.80,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m" coord="5,267.46,645.84,134.82,7.86">Passage re-ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,138.35,119.67,342.24,7.86;6,146.91,130.63,333.68,7.86;6,146.91,141.59,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,255.39,119.67,225.20,7.86;6,146.91,130.63,30.27,7.86">Rethink training of bert rerankers in multi-stage retrieval pipeline</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,202.04,130.63,243.31,7.86">The 43rd European Conference On Information Retrieval</title>
		<imprint>
			<publisher>ECIR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,152.55,342.24,7.86;6,146.91,163.51,333.68,7.86;6,146.91,174.47,222.80,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,344.98,152.55,135.61,7.86;6,146.91,163.51,199.13,7.86">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,367.17,163.51,68.18,7.86">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,185.43,342.24,7.86;6,146.91,196.39,333.68,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,357.40,185.43,123.19,7.86;6,146.91,196.39,204.40,7.86">Efficiently teaching an effective dense retriever with balanced topic aware sampling</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,372.59,196.39,23.62,7.86">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,207.34,342.24,7.86;6,146.91,218.30,333.68,7.86;6,146.91,229.26,333.68,7.86;6,146.91,240.22,245.84,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,159.59,218.30,321.00,7.86;6,146.91,229.26,32.07,7.86">PAIR: leveraging passage-centric similarity relation for improving dense passage retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,201.44,229.26,152.50,7.86;6,381.58,229.26,78.07,7.86">ACL/IJCNLP 2021. Findings of ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2173" to="2183" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021</note>
</biblStruct>

<biblStruct coords="6,138.35,251.18,342.24,7.86;6,146.91,262.14,55.03,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,298.52,251.18,163.21,7.86">CIP at TREC 2021 deep learning track</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,146.91,262.14,26.37,7.86">TREC</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
