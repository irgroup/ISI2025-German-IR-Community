<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,176.84,110.96,258.32,15.12;1,194.95,132.88,222.11,15.12">Multi-Faceted Question Fusion in the TREC 2022 CrisisFACTS Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,115.72,177.32,114.03,10.48"><forename type="first">Nathaniel</forename><forename type="middle">W</forename><surname>Rollings</surname></persName>
							<email>nrolling@umd.edu</email>
						</author>
						<author>
							<persName coords="1,276.90,177.32,82.26,10.48"><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Rankel</surname></persName>
						</author>
						<author>
							<persName coords="1,406.31,177.32,89.97,10.48"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
						</author>
						<title level="a" type="main" coord="1,176.84,110.96,258.32,15.12;1,194.95,132.88,222.11,15.12">Multi-Faceted Question Fusion in the TREC 2022 CrisisFACTS Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9ED32944D7747D1466BD8562337B87C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To address the challenges of multi-faceted questions in rapidly evolving environments, this paper introduces a system with an architecture based on recency-weighted and score-weighted reciprocal rank fusion of per-facet ranked lists. In the absence of existing data for parameter tuning, a small test collection built to support formative evaluation was developed and employed in system refinement. Issues of duplication were addressed through pruning near duplicates and, in one variation, synthesizing rather than simply selecting responses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the event of a crisis, responders need regular updates to multi-faceted standing questions but have limited time available to consume information, so prioritization of provided information is critical. However, as the environment is quickly changing, previously useful information may become outdated in short order. Management of crisis response requires updated information-a report on the size of a wildfire from the previous day is of little use if the fire has since doubled in size. This updated information may appear on one or many platforms and, particularly on social media sites, may appear many times on a given platform.</p><p>The 2022 Text Retrieval Conference's (TREC-2022) inaugural CrisisFACTS Track centers on providing relevant and recent answers to the questions that crisis response managers would include in daily reports. The goal is to provide this as a single ranked list each day, emphasizing coverage of each facet for which information is available for that day. Each of the three key issues-facet coverage, relevance, and recencyplays a role in the design of the system discussed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The CrisisFACTS Task</head><p>The goal of a CrisisFACTS system is to select materials for inclusion in summary reports on the organized response to a natural disaster. These multifaceted reports are typically prepared daily, so the task is to organize content on which each daily report can be based.</p><p>Facets (which within the track are called "queries"<ref type="foot" coords="1,317.49,643.42,3.97,6.12" target="#foot_0">1</ref> ) were drawn from information found in Federal Emergency Management Agency (FEMA) Incident Command System (ICS)   by command center personnel each day during an event. For a given event, there are between 48 and 52 facets, as shown in Table <ref type="table" coords="2,185.94,232.91,3.87,8.74" target="#tab_0">1</ref>. While the set of facets for each event is identical across the days of the event (although on some days there may be no relevant content to be found some facets), the facet sets do differ between event types. Each facet includes a "query" field containing a question describing the facet (e.g., "Have airports closed") and an "indicative terms" field that contains keywords similar to what might be seen in a Web query (e.g., "airport closed"). We refer to these as the "question" and the "indicative terms."</p><p>The TREC-2022 CrisisFACTS track provides a collection of short text passages extracted from social media (Twitter and Facebook), news sources, and Reddit relating to one of eight different natural disasters, referred to generically as "events". Each event extends over several days, ranging from two days for the shortest event to eighteen for the longest (although the track created relevance judgments for no more than six days for any one event). The passages are provided as JSON, with each entry including a Unix timestamp, the source, and the extracted text passage. There were 45 to 14,000 passages per event-day pair in the dataset for this year's track.</p><p>Runs are to be submitted by participating teams as a set of ranked lists, one for each event-day pair, using start and stop times for each event-day pair that are specified as Unix timestamps. The top k passages of this list are evaluated by the track organizers, with k varying by event-day pair in a manner not known in advance by the participating teams. However, the track organizers recommend submitting at least the top 100 passages per event-day pair for evaluation. Evaluation for the track involves two types of metrics: summarization metrics and fact-matching metrics. The summarization metrics include Rouge-2 F 1 and BERTScore F 1 ; they compare the top k passages of a run against gold-standard summaries for each eventday pair. The fact-matching metrics compare the top k passages to a set of gold-standard facts previously developed by the assessors.</p><p>As this is the first year of the track, there was no labeled data available for training or parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Design</head><p>Our overall system is shown in Figure <ref type="figure" coords="2,245.48,598.51,3.87,8.74" target="#fig_0">1</ref>. The first step is to create one ranked list for each facet. Next, these per-facet ranked lists are fused to create a single ranked list. This fusion process is based on rank, score, and recency. Finally, duplicated content is suppressed in a result selection step using either extractive or abstractive techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Facet Ranking</head><p>Our system employs DFReeKLIM -a parameter-free divergence from randomness model from the TREC-2011 Microblogging Track <ref type="bibr" coords="3,189.52,332.74,10.52,8.74" target="#b2">[3]</ref> -as the initial step in generating per-facet ranked lists. This model uses a facet's indicative terms to generate a ranked list, and this initial result is then passed into an RM3 blind relevance feedback (BRF) algorithm <ref type="bibr" coords="3,231.89,356.65,9.96,8.74" target="#b0">[1]</ref>. BRF seeks to identify additional terms related to a particular facet that may not be generalizable outside of this event. For example, if a user wants to know what evacuation shelters are available (the facet) and it turns out that high schools are frequently being used as shelters, BRF may aid in automatically adding the term high schools to the query for this facet for this event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion</head><p>The next step is to combine the per-facet ranked lists for an event-day pair into a single ranked list for that event-day pair. Reciprocal rank fusion (RRF) is a well-established technique, although it is more traditionally used to combine ranked lists from different systems rather than from the same system with different queries <ref type="bibr" coords="3,72.00,498.57,9.96,8.74" target="#b4">[5]</ref>. Given the set of passages D provided by the track for an event-day pair, the set of facets Q specified for that event, and a ranking r q for each q âˆˆ Q, eq. ( <ref type="formula" coords="3,289.43,510.52,4.24,8.74" target="#formula_0">1</ref>) shows the traditional reciprocal rank fusion algorithm. The constant 60 in the denominator is a default suggested by Cormack et al <ref type="bibr" coords="3,408.24,522.48,9.96,8.74" target="#b4">[5]</ref>.</p><formula xml:id="formula_0" coords="3,237.56,553.81,302.44,26.80">RRF (d âˆˆ D) = qâˆˆQ 1 60 + r q (d)<label>(1)</label></formula><p>While RRF provides an initial step toward our fusion algorithm, it does not consider two potential problems:</p><p>1. To the extent that retrieval scores are informative, we may be able to improve over the use of reciprocal rank alone. This information might, for example, be leveraged to identify scenarios where the first few ranks received much higher scores than the next lower-ranked passages.</p><p>2. Neither RRF nor DFreeKLIM are sensitive to recency. Passages marked as slightly less relevant but much more recent may be more useful than older passages that DFReeKLIM may have given a higher rank to.</p><p>The first issue was addressed by incorporating each passage's normalized score weight per query into the numerator of the RRF calculation, shown in eq. ( <ref type="formula" coords="4,308.22,144.90,3.87,8.74" target="#formula_1">2</ref>). This step, which Mansouri et al called "modified RRF" <ref type="bibr" coords="4,101.80,156.85,9.96,8.74" target="#b6">[7]</ref>, is equivalent to performing a normalized CombSUM <ref type="bibr" coords="4,352.01,156.85,10.52,8.74" target="#b5">[6]</ref> on the relevance scores by facet. In the equation below, the w q is the min-max normalized per-facet relevance score.</p><formula xml:id="formula_1" coords="4,230.74,197.93,309.26,26.80">CS RRF (d âˆˆ D) = qâˆˆQ w q k + r q (d)<label>(2)</label></formula><p>This approach still neglects any consideration of recency, however. To that end, a normalized recency score, t, is calculated as shown in eq. ( <ref type="formula" coords="4,247.58,256.73,4.24,8.74" target="#formula_2">3</ref>) with the timestamp of the passage as o, the start time for the event-day pair as s, and the event end time as n. This t is interpolated with the normalized score in the overall fusion calculation as shown in eq. ( <ref type="formula" coords="4,257.33,280.64,3.87,8.74" target="#formula_3">4</ref>). Tuning of the interpolation factor Î» was performed using the formative evaluation process described in Section 4; a value of 0.9 was chosen. This Î» puts heavy emphasis on the relevance of documents, while ensuring similar but outdated passages are pushed down the list -an important consideration for our result selection process that is discussed in the next section.</p><formula xml:id="formula_2" coords="4,281.67,347.23,258.33,22.31">t = o q -s n -s<label>(3)</label></formula><formula xml:id="formula_3" coords="4,207.78,392.32,332.22,26.80">T W CS RRF (d âˆˆ D) = qâˆˆQ Î»w q + (1 -Î»)t k + r q (d)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Result Selection</head><p>We tried two ways of selecting which results to retain in our final ranked list. In our first approach, which we call extractive deduplication, we simply remove near duplicates from the result set. Our second approach, which we call abstractive deduplication, first generates a summary for each near-duplicate cluster and then reruns the entire passage ranking process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Extractive Deduplication</head><p>The fusion process generates a single ranked list, as required by the track, but nothing has yet been done to address the potential for duplicate content for one facet displacing content that might be relevant to other facets. These problems occur because nearly identical passages published in different streams, and in some cases multiple times within the same stream (e.g., several accounts tweeting the same information), could all receive similar scores from the fusion techniques described in the previous section. To resolve this issue, we use the Universal Sentence Encoder <ref type="bibr" coords="4,227.89,642.70,10.52,8.74" target="#b3">[4]</ref> to embed the text of each passage in the top 100 passages of the fused list and find the cosine similarity with each other passage in the top 100. <ref type="foot" coords="4,389.97,653.08,3.97,6.12" target="#foot_1">2</ref> Passages with a similarity above a threshold of 0.7 (selected through tuning using the formative evaluation technique discussed in Section 4) were clustered using single link clustering.</p><p>Once clusters were identified, all but the highest-ranked passage in each cluster were removed. However, if an element to be removed happened to have been the top-ranked passage in one of the facet-specific lists, it was retained in the fused list. Retaining a facet's top-ranked passage (if it made it into the top 100 passages in the fused list) was intended to minimize the risk of coverage loss from extractive deduplication. More refined approaches that consider facet relevance scores might be crafted, but we did not expect our formative evaluation technique (see Section 4) to be sufficiently discriminating to tune the parameters of such an approach.</p><p>Once clustered items are removed, the list is consolidated and passages below each of the removed elements move up to fill the gaps. Since passages outside of the original top 100 may move into the top 100, we repeat the process in order to identify any new clusters and remove duplicates. When the top 100 passages remain constant after a clustering and deduplication iteration or fewer than 100 total passages remain, the list is considered to be finalized and the final ranked list is output for submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Abstractive Deduplication</head><p>While the extractive deduplication process removes highly related passages, it will sometimes remove passages containing some unique information. Instead of simply removing the passage, an alternative approach used in one variant of our system is to summarize the information within a cluster. Summarization was performed by aggregating the text from each passage in a cluster and then constructing a summary of that text with the Hugging Face sshleifer/distilbart-cnn-12-6 summarization model <ref type="bibr" coords="5,406.30,370.50,9.96,8.74" target="#b7">[8]</ref>. We set the desired output length from that model to be 15 words longer than the mean length of the passages being summarized. The summarized text was then temporarily added to the collection and the entire system-starting from facet ranking-was rerun. On this subsequent run, the summarization process was not repeated. Instead, extractive deduplication was performed, with the additional constraint that a summarized passage, if in the top 100, would not be removed in this process.</p><p>One additional change when using the summarization system is the removal of the recency-weighting component of the fusion calculation (by setting Î» to 0 in eq. ( <ref type="formula" coords="5,343.07,466.14,3.87,8.74" target="#formula_3">4</ref>)). This was necessary when the full system was run for the second time since it was not clear how the timestamp of a summarized element should best be set, so our summaries lacked timestamps. To facilitate comparison with our other runs, we therefore also deleted recency weighting when the system was run for the first time; our results with summarization should therefore be compared to our run in which recency-weighting was ablated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Formative Evaluation</head><p>The lack of labeled data (e.g., gold-standard facts, or gold-standard summaries) in this first year made parameter tuning and formative evaluation challenging. After checking with the organizers, we therefore chose to manually annotate a small portion of the collection for this purpose. To do this, the first author of this paper created a set of regular expressions to score 11 facets for each day in event 1 (the Lilac Wildfire). Our intent was to loosely model the Comprehensiveness fact-matching evaluation metric. The 11 facets were selected because they had easily identifiable and exhaustively enumerable answers. For example, the facet regarding the containment level of the fire is well defined for each day, but finding definitive answers for a facet on obstacles facing firefighters, an open set, would have been more difficult.</p><p>What we call the Formative Evaluation score for each day was calculated using eq. ( <ref type="formula" coords="6,444.83,75.16,3.87,8.74" target="#formula_4">5</ref>), with i representing a fact in the set of ground truth facts in the set I of regular expressions, S being the top 25 passages from the final ranked list, and w being the weight associated with a fact. This weight w is determined by the number of distinct elements that would be required to fully address the facet, and it sums to one for each facet of an event-day pair. For example, consider a situation where there is a facet requiring the location of all available shelters for evacuees and three shelters are open during the time window for the event-day pair undergoing evaluation. Each unique correct shelter location included in the top 25 passages would thus add 1  3 to the Formative Evaluation score. Whether these locations were spread across one, two, or three passages, the full point would be awarded if all three locations appear in the top 25 passages. The final Formative Evaluation score is the mean score over the 7 days of the Lilac Wildfire.</p><formula xml:id="formula_4" coords="6,247.42,181.18,292.59,63.98">3 score(s) = iâˆˆI w i âˆˆ S 0 i / âˆˆ S<label>(5)</label></formula><p>One issue with this approach is that selection of these specific easily-answerable facets risks introducing some bias. Facets without such clear answers could have notably different properties that could impact the effectiveness of some techniques or parameter values in ways that would not be well characterized by our Formative Evaluation score.</p><p>Another unaddressed concern is the presence of inaccurate information in the top results. If a fire has spread to 100 acres by the end of the day for an event-day pair, the system's top result in the fused list could list the fire as being only 50 acres, but as long as some other lower-ranked passage still in the top 25 states that it is at 100 acres the Formative Evaluation score would award full credit for correctly addressing the facet. A real user would, however, see the incorrect answer before the correct answer, which is clearly undesirable.</p><p>Finally, the design of our Formative Evaluation metric was motivated by one specific fact-matching metric (Comprehensiveness). Tailoring formative evaluation for different metrics might have resulted in different choices of system designs, or perhaps different parameter choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submissions</head><p>We submitted a total of six runs, as follows:</p><p>â€¢ mrr main: Our primary submission implemented the system as shown in Figure <ref type="figure" coords="6,452.10,541.60,3.87,8.74" target="#fig_0">1</ref>, but without summarization. The next four runs were based off of this system, with various ablations.</p><p>â€¢ combsum: This version of the system removed the reciprocal rank portion (the denominator) of eq. ( <ref type="formula" coords="6,531.51,573.30,4.24,8.74" target="#formula_3">4</ref>) from the main system and relied on the sum of the interpolated score and recency weights.</p><p>â€¢ mrr no dd : This run removed the result selection step from the main system.</p><p>â€¢ mrr all : A misnamed run, it actually set the Î» in eq. ( <ref type="formula" coords="6,330.94,624.75,4.24,8.74" target="#formula_3">4</ref>) to one in order to ignore recency. Comparison with the mrr main system thus illustrates the effectiveness of recency weighting.</p><p>â€¢ mrr nobrf : This run omitted RM3 blind relevance feedback from the per-facet ranking process. â€¢ rr now : This run removed consideration of the normalized score weight by setting Î» to 0 in eq. ( <ref type="formula" coords="7,520.69,274.79,3.87,8.74" target="#formula_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component Ablation</head><p>â€¢ mrr sum: This run included the summarization step. It was built as an addition to mrr all, and thus did not use recency-weighting in its fusion step (because our summaries have no timestamp).</p><p>The first six of those runs satisfy the track's definition of an extractive run. Run mrr sum is an abstractive run as defined in the track guidelines. All runs are automatic runs as defined in the track guidelines, although we note that our use of a manually constructed tuning set for eleven facets from event CrisisFACTS-001 as a basis for formative evaluation makes our results for that event less comparable to automatic runs from teams that did not make use of our tuning set (which we did share with other track participants).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The fact-based assessment scores from the submitted runs are shown in Table <ref type="table" coords="7,417.93,462.78,3.87,8.74" target="#tab_1">2</ref>, and the F 1 values for the BERTScore calculated using ICS-209 forms are shown in Table <ref type="table" coords="7,357.56,474.74,3.87,8.74" target="#tab_2">3</ref>. The first row provides the official run names, while the second row contains a code indicating the features included in that run, as specified below. The final row of Table <ref type="table" coords="7,174.26,498.65,4.98,8.74" target="#tab_1">2</ref> shows the macroaverage excluding the first event, as some facets from that event were used for formative evaluation.</p><p>â€¢ B -includes RM3 blind relevance feedback in the facet ranking step.</p><p>â€¢ S -includes per-facet relevance score weighting in the fusion step.</p><p>â€¢ T -includes recency weighting in the fusion step.</p><p>â€¢ R -includes reciprocal rank fusion in the fusion step.</p><p>â€¢ D -includes extractive deduplication in the result selection step.</p><p>â€¢ A -includes abstractive deduplication in the result selection step.</p><p>While there are some interesting trends in the results for both tables, the differences between our six runs, and between any one run and the median across all submissions to the track, were not found to be  <ref type="table" coords="8,490.05,254.08,23.67,8.74" target="#tab_1">1 or 2</ref>, even before applying a Bonferroni correction for repeated tests.</p><p>While not significantly different, the relatively good Comprehensiveness score for mrr nobrf warrants further investigation. Based on our formative evaluation, this system had been second to last of our submitted runs. The results from mrr nobrf also serve to highlight one aspect of the formative evaluation process that we mentioned in Section 4. The lack of BRF in this run pushed it into nearly last place in the formative evaluation, yet it was first when evaluated by the Comprehensiveness metric, and consistently in the top three when evaluated by BERTScore. This disparity may be the result of the bias of our formative evaluation toward easily answerable questions.</p><p>Per-event macroaverage rankings for a variety of metrics are shown in Table <ref type="table" coords="8,415.44,385.59,3.87,8.74" target="#tab_3">4</ref>. As can be seen, Formative Evaluation was not strongly predictive of Comprehensiveness system preference order, although without statistically significant differences it is hard to read too much into that observation. Also of note, deleting extractive deduplication (mrr no dd ) seems to have affected BERTScore evaluation based on the ICS-209 forms somewhat differently from BERTScore evaluation based on NIST-written summaries or based on Wikipedia pages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component Ablation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>The result of the runs we submitted to the TREC-2022 CrisisFACTS track have shown the task to be tractable, and we have seen some promising results from relatively simple fusion and deduplication techniques.</p><p>We have not, however, been able to measure statistically significant differences between the techniques that we have tried, and we are looking forward to learning whether such differences can be observed between systems from different teams using this first year's test collection.</p><p>There are a number of potential directions for future work. The track casts the task as a ranking problem, and in the second year of the track it will be a ranking problem for which some training data will be available. This suggests that investigation of learning-to-rank techniques may be productive. Alaofi et al have challenged us to consider where queries come from <ref type="bibr" coords="9,333.63,216.53,9.96,8.74" target="#b1">[2]</ref>, suggesting that we might also productively use manual runs to explore the effect of for the same set of facets. Moreover, we might also hope to find some hierarchical structure in the facets that are of interest for an event, and even simple fusion techniques of the type we have explored this year might be adapted to exploit such structures. These and other opportunities cause us to see CrisisFACTS as a rich venue for future research, and we are glad to hear that the track will continue for a second year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,197.84,258.61,216.32,8.74;3,189.00,72.00,233.99,171.50"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The primary system used in this paper.</figDesc><graphic coords="3,189.00,72.00,233.99,171.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,438.49,656.95,101.50,8.74"><head>Table 1 :</head><label>1</label><figDesc>Event information.</figDesc><table coords="1,438.49,656.95,101.50,8.74"><row><cell>forms that are prepared</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,77.73,84.96,455.56,168.07"><head>Table 2 :</head><label>2</label><figDesc>Comprehensiveness at k, averaged over days per event, ablations ordered by microaverge.</figDesc><table coords="7,77.73,84.96,455.56,146.67"><row><cell></cell><cell></cell><cell></cell><cell>BSTRD</cell><cell>STRD</cell><cell>BSTD</cell><cell>BTRD</cell><cell>BSRD</cell><cell>BSTR</cell></row><row><cell>Event</cell><cell># Days</cell><cell cols="7">Median mrr main mrr nobrf combsum rr now mrr all mrr no dd</cell></row><row><cell>001</cell><cell>5</cell><cell>0.1042</cell><cell>0.0801</cell><cell>0.0986</cell><cell>0.1157</cell><cell cols="2">0.1131 0.0892</cell><cell>0.0861</cell></row><row><cell>002</cell><cell>3</cell><cell>0.2709</cell><cell>0.3404</cell><cell>0.3565</cell><cell>0.2981</cell><cell>0.2450</cell><cell>0.3532</cell><cell>0.3275</cell></row><row><cell>003</cell><cell>4</cell><cell>0.2957</cell><cell>0.3460</cell><cell>0.3391</cell><cell>0.3557</cell><cell>0.3297</cell><cell>0.3365</cell><cell>0.3346</cell></row><row><cell>004</cell><cell>6</cell><cell>0.0721</cell><cell>0.0489</cell><cell>0.0515</cell><cell>0.0513</cell><cell>0.0251</cell><cell>0.0550</cell><cell>0.0760</cell></row><row><cell>005</cell><cell>2</cell><cell>0.1898</cell><cell>0.2398</cell><cell>0.2488</cell><cell>0.1309</cell><cell cols="2">0.1655 0.2809</cell><cell>0.2277</cell></row><row><cell>006</cell><cell>2</cell><cell>0.0273</cell><cell>0.0182</cell><cell>0.1455</cell><cell>0.0273</cell><cell>0.0273</cell><cell>0.0182</cell><cell>0.0182</cell></row><row><cell>007</cell><cell>2</cell><cell>0.0412</cell><cell>0.0236</cell><cell>0.0412</cell><cell>0.0530</cell><cell cols="2">0.0765 0.0236</cell><cell>0.0236</cell></row><row><cell>008</cell><cell>3</cell><cell>0.2055</cell><cell>0.2308</cell><cell>0.2087</cell><cell>0.2820</cell><cell>0.2078</cell><cell>0.2035</cell><cell>0.2126</cell></row><row><cell></cell><cell>Microaverage</cell><cell>0.1512</cell><cell>0.1613</cell><cell>0.1750</cell><cell>0.1656</cell><cell>0.1656</cell><cell>0.1643</cell><cell>0.1624</cell></row><row><cell></cell><cell>Macroaverage</cell><cell>0.1508</cell><cell>0.1660</cell><cell>0.1862</cell><cell>0.1642</cell><cell>0.1487</cell><cell>0.1700</cell><cell>0.1633</cell></row><row><cell cols="3">Macroaverage excluding 001 0.1575</cell><cell>0.1782</cell><cell>0.1987</cell><cell>0.1712</cell><cell>0.1538</cell><cell>0.1816</cell><cell>0.1743</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.00,73.48,461.50,189.34"><head>Table 3 :</head><label>3</label><figDesc>ICS BERTScore F 1 values, averaged over days per event, ablations ordered by macroaverage. statistically significant at p &lt; 0.05 by two-tailed paired t-tests for either of the metrics in Tables</figDesc><table coords="8,77.77,73.48,455.73,136.11"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Component Ablation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>BSTRD</cell><cell>BSTR</cell><cell>BSRD</cell><cell>STRD</cell><cell>BSTD</cell><cell cols="2">BTRD BSRDA</cell></row><row><cell cols="10">Event # Days Median mrr main mrr no dd mrr all mrr nobrf combsum rr now mrr sum</cell></row><row><cell>001</cell><cell>5</cell><cell>0.5121</cell><cell>0.4973</cell><cell>0.5163</cell><cell>0.5056</cell><cell>0.5113</cell><cell>0.4945</cell><cell>0.4927</cell><cell>0.4956</cell></row><row><cell>002</cell><cell>3</cell><cell>0.5300</cell><cell>0.5224</cell><cell>0.5735</cell><cell>0.5225</cell><cell>0.5158</cell><cell>0.5300</cell><cell>0.5209</cell><cell>0.5018</cell></row><row><cell>003</cell><cell>4</cell><cell>0.5054</cell><cell>0.5054</cell><cell>0.4977</cell><cell>0.5146</cell><cell>0.5094</cell><cell>0.4999</cell><cell>0.4878</cell><cell>0.5186</cell></row><row><cell>004</cell><cell>6</cell><cell>0.4774</cell><cell>0.4592</cell><cell>0.4592</cell><cell>0.4604</cell><cell>0.4696</cell><cell>0.4573</cell><cell>0.4581</cell><cell>0.4799</cell></row><row><cell>005</cell><cell>2</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>006</cell><cell>2</cell><cell>0.4939</cell><cell>0.4940</cell><cell>0.4989</cell><cell>0.4954</cell><cell>0.4934</cell><cell>0.4901</cell><cell>0.4935</cell><cell>0.4870</cell></row><row><cell>007</cell><cell>2</cell><cell>0.5059</cell><cell>0.5103</cell><cell>0.5103</cell><cell>0.5138</cell><cell>0.5115</cell><cell>0.5101</cell><cell>0.5044</cell><cell>0.5098</cell></row><row><cell>008</cell><cell>3</cell><cell>0.4944</cell><cell>0.4958</cell><cell>0.4958</cell><cell>0.4945</cell><cell>0.4923</cell><cell>0.4827</cell><cell>0.4846</cell><cell>0.5053</cell></row><row><cell cols="2">Macroaverage</cell><cell>0.4399</cell><cell>0.4356</cell><cell>0.4439</cell><cell>0.4383</cell><cell>0.4379</cell><cell>0.4331</cell><cell>0.4303</cell><cell>0.4372</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,77.94,487.47,455.38,150.19"><head>Table 4 :</head><label>4</label><figDesc>Run rankings for different metrics, ablations ordered by formative evaluation score.</figDesc><table coords="8,77.94,487.47,455.38,128.74"><row><cell></cell><cell>BSTRD</cell><cell cols="2">BSRD BTRD</cell><cell>BSTD</cell><cell>STRD</cell><cell>BSTR</cell><cell>BSRDA</cell></row><row><cell>Metric</cell><cell cols="7">mrr main mrr all rr now combsum mrr nobrf mrr no dd mrr sum</cell></row><row><cell>Formative Evaluation</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>7</cell><cell>4</cell></row><row><cell>Comprehensiveness</cell><cell>3</cell><cell>2</cell><cell>7</cell><cell>4</cell><cell>1</cell><cell>5</cell><cell>6</cell></row><row><cell>Redundancy/Verbosity</cell><cell>5</cell><cell>4</cell><cell>1</cell><cell>3</cell><cell>7</cell><cell>2</cell><cell>6</cell></row><row><cell>ics.bertscore.f1</cell><cell>5</cell><cell>2</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>1</cell><cell>4</cell></row><row><cell>ics.rouge2.f1</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>6</cell><cell>7</cell><cell>5</cell><cell>1</cell></row><row><cell>nist.bertscore.f1</cell><cell>6</cell><cell>5</cell><cell>2</cell><cell>4</cell><cell>3</cell><cell>7</cell><cell>1</cell></row><row><cell>nist.rouge2.f1</cell><cell>6</cell><cell>4</cell><cell>3</cell><cell>1</cell><cell>5</cell><cell>7</cell><cell>2</cell></row><row><cell>wiki.bertscore.f1</cell><cell>6</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>7</cell><cell>2</cell></row><row><cell>wiki.rouge2.f1</cell><cell>6</cell><cell>5</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>7</cell><cell>4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,675.20,452.75,6.99;1,72.00,684.67,368.62,6.99"><p>In the CrisisFACTS track, "query" refers both to a facet and to the field in the facet description that contains a natural language question. For clarity, we refer to the first of those as a facet and the second as a question.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,87.24,673.58,452.75,6.99;4,72.00,683.05,468.00,6.99"><p>We selected 100 as a cutoff because the track guidelines suggested submitting a ranked list containing at least 100 passages per event-day pair. However, in actual practice, the k selected by the organizers for evaluation was sometimes larger than 100.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,87.24,675.20,452.75,6.99;6,72.00,684.67,217.44,6.99"><p>Only five days of Event CrisisFACTS-001 (the Lilac Wildfire) were annotated for the official evaluation, but our Formative Evaluation score is averaged over seven days of that event.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,87.50,348.70,452.50,7.61;9,87.50,358.97,452.50,9.30;9,87.50,370.92,75.97,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,156.44,358.97,180.64,8.74">UMass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,357.96,358.97,114.50,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004<address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,392.53,452.50,7.61;9,87.50,402.80,452.50,9.30;9,87.50,414.76,452.50,8.74;9,87.50,426.71,22.69,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,181.72,402.80,129.62,8.74">Where do queries come from?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alaofi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">L</forename><surname>Saling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,329.30,402.80,210.70,8.74;9,87.50,414.76,297.51,8.74">SIGIR &apos;22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2850" to="2862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,448.32,452.50,7.61;9,87.50,458.60,452.51,9.30;9,87.50,470.55,277.39,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,275.39,458.60,260.07,8.74">IASI-CNR, UNIVAQ at microblogging track of TREC 2011</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amodeo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Di Nicola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marcone</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,99.95,470.55,117.15,8.74">Proceedings of TREC 2011</title>
		<meeting>TREC 2011<address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,492.16,452.50,7.61;9,87.50,502.43,452.50,9.30;9,87.50,514.39,173.85,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,459.40,502.43,80.60,8.74;9,87.50,514.39,31.75,8.74">Universal sentence encoder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>CoRR abs/1803.11175</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,534.31,452.50,9.30;9,87.50,546.27,380.21,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,362.66,534.31,177.34,8.74;9,87.50,546.27,192.50,8.74">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>BÃ¼ttcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,301.82,546.27,45.06,8.74">SIGIR &apos;09</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,566.19,452.50,9.30;9,87.50,578.15,243.45,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,243.81,566.19,148.03,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,418.70,566.19,121.29,8.74;9,87.50,578.15,47.99,8.74">The Second Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,598.07,452.50,9.30;9,87.50,610.03,203.99,8.74" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<title level="m" coord="9,319.55,598.07,220.45,8.74;9,87.50,610.03,199.74,8.74">DPRL systems in the CLEF 2022 ARQMath lab: Introducing MathAMR for math-aware search</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,87.50,629.95,452.51,9.30;9,87.50,641.91,83.14,8.74" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<ptr target="https://huggingface.co/sshleifer/distilbart-cnn-12-6.Ac-cessed" />
		<imprint>
			<biblScope unit="page" from="2022" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
