<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.98,116.75,339.39,12.62;1,174.69,134.69,265.97,12.62">The University of Stavanger (IAI) at the TREC 2022 Conversational Assistance Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.57,172.36,84.07,8.74;1,228.64,170.78,1.83,6.12"><forename type="first">Weronika</forename><surname>Lajewska</surname></persName>
							<email>weronika.lajewska@uis.no</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
								<address>
									<settlement>Stavanger</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.88,172.36,76.30,8.74;1,315.18,170.78,1.83,6.12"><forename type="first">Nolwenn</forename><surname>Bernard</surname></persName>
							<email>nolwenn.m.bernard@uis.no</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
								<address>
									<settlement>Stavanger</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.41,172.36,55.99,8.74;1,381.40,170.78,1.83,6.12"><forename type="first">Ivica</forename><surname>Kostric</surname></persName>
							<email>ivica.kostric@uis.no</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
								<address>
									<settlement>Stavanger</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.64,172.36,52.86,8.74;1,444.50,170.78,1.83,6.12"><forename type="first">Ivan</forename><surname>Sekulić</surname></persName>
							<email>ivan.sekulic@usi.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">Università della Svizzera italiana</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.59,184.31,68.02,8.74;1,339.61,182.74,1.83,6.12"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>krisztian.balog@uis.no</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
								<address>
									<settlement>Stavanger</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.98,116.75,339.39,12.62;1,174.69,134.69,265.97,12.62">The University of Stavanger (IAI) at the TREC 2022 Conversational Assistance Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FC6BAFB5AC64C93CC95BA675548F1161</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conversational AI</term>
					<term>conversational search</term>
					<term>TREC CAsT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the IAI group at the University of Stavanger in the TREC 2022 Conversational Assistance track. We employ an established two-stage passage ranking architecture, i.e., first-pass passage retrieval (with standard BM25 ranking and pseudo-relevance feedback) followed by re-ranking (with mono and duo T5) using a rewritten query (with a T5 model fine-tuned on the CANARD dataset). In our runs, we experiment with intent classification based on MSDialog-Intent and term expansion using beam search scores for query rewriting as well as with clarifying questions for the mixed-initiative subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Conversational Assistance track (CAsT) has been a part of the Text REtrieval Conference (TREC) since 2019. It aims to advance conversational information seeking research and to create a large-scale reusable test collection for open-domain conversational search <ref type="bibr" coords="1,308.59,476.79,9.96,8.74" target="#b5">[6]</ref>. The track addresses conversational search that is about to satisfy a "user's information need expressed or formalized through multiple turns in a conversation" <ref type="bibr" coords="1,342.82,500.70,9.96,8.74" target="#b5">[6]</ref>. The topics are complex, diverse, and answerable. They require content from multiple information sources and vary in types of conversational structural patterns. The main differences in TREC CAsT'22 are the focus on fluent responses that contain only relevant information (summaries), multiple information needs in a shared topic, and a mixed-initiative subtask.</p><p>The focus of our participation is to investigate different query rewriting approaches in order to improve end-to-end system performance. We follow an established two-stage retrieve-then-rerank pipeline architecture, i.e., first-pass passage retrieval followed by re-ranking. We use sparse retrieval with BM25 on queries expanded with pseudo-relevance feedback. For the second step, we use a pointwise monoT5 re-ranker followed by a pairwise duoT5 re-ranker. As a baseline for the query rewriting module, we use a T5 model fine-tuned on the CANARD dataset. Our experiments on CAsT'21 suggest that this is a strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>What makes conversational search, in the TREC CAsT sense, different from passage retrieval, and truly conversational, is the element of query rewriting. Approaches to query rewriting can be categorized into three main groups: featurebased unsupervised approaches <ref type="bibr" coords="2,276.12,179.69,14.61,8.74" target="#b10">[11]</ref>, feature-based supervised approaches <ref type="bibr" coords="2,462.33,179.69,14.61,8.74" target="#b12">[13]</ref>, and supervised neural approaches. The last group is characterized by the utilization of large pre-trained language models. In particular, generative models such as GPT-2 <ref type="bibr" coords="2,202.61,215.55,15.50,8.74" target="#b21">[22]</ref> or T5 <ref type="bibr" coords="2,248.86,215.55,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,262.61,215.55,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,273.59,215.55,12.73,8.74" target="#b24">25]</ref> are used. They are mostly fine-tuned on the CANARD dataset <ref type="bibr" coords="2,218.57,227.51,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,232.92,227.51,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,244.49,227.51,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="2,261.05,227.51,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="2,277.60,227.51,12.73,8.74" target="#b21">22]</ref> or on QReCC <ref type="bibr" coords="2,358.94,227.51,14.61,8.74" target="#b24">[25]</ref>. In terms of end-to-end conversational search performance, systems using a combination of term-based query expansion with generative models for query reformulation are shown to provide the best results <ref type="bibr" coords="2,244.57,263.38,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,264.43,263.38,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="2,281.51,263.38,11.62,8.74" target="#b21">22]</ref>. Mele et al. <ref type="bibr" coords="2,354.02,263.38,15.50,8.74" target="#b13">[14]</ref> propose a flexible query rewriting method based on the classification of utterances. That is, the strategy used to define the context for the rewrite depends on the utterance class (self-explanatory, referring to the first, or referring to the previous topic in the conversation). In our runs, we experiment with a T5 generative language model, BERT-based intent classification, and term expansion using beam search scores.</p><p>The majority of conversational search systems use a two-stage retrieve-thenrerank architecture <ref type="bibr" coords="2,223.74,347.25,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="2,243.81,347.25,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="2,261.12,347.25,12.73,8.74" target="#b21">22,</ref><ref type="bibr" coords="2,278.44,347.25,11.62,8.74" target="#b25">26]</ref>. Approaches to first-pass retrieval can be categorized into two main groups: sparse retrieval and dense-sparse retrieval. Sparse retrieval models include BM25 <ref type="bibr" coords="2,309.89,371.16,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="2,330.13,371.16,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="2,347.59,371.16,12.73,8.74" target="#b21">22,</ref><ref type="bibr" coords="2,365.06,371.16,11.62,8.74" target="#b25">26]</ref>, query likelihood with Dirichlet smoothing <ref type="bibr" coords="2,223.28,383.11,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,241.56,383.11,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="2,257.06,383.11,11.62,8.74" target="#b22">23]</ref>, or sequential dependency model <ref type="bibr" coords="2,415.76,383.11,9.96,8.74" target="#b6">[7]</ref>. The second group of first-pass retrieval methods takes advantage of dense embedding spaces, e.g., ANCE <ref type="bibr" coords="2,187.40,407.02,15.50,8.74" target="#b23">[24,</ref><ref type="bibr" coords="2,205.90,407.02,11.62,8.74" target="#b24">25]</ref>. Approaches to the re-ranking of candidates retrieved in the first stage are based on large, computationally expensive transformers models. These models are either based on BERT <ref type="bibr" coords="2,315.45,430.93,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,334.41,430.93,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="2,350.60,430.93,9.22,8.74" target="#b20">[21]</ref><ref type="bibr" coords="2,359.82,430.93,4.61,8.74" target="#b21">[22]</ref><ref type="bibr" coords="2,364.44,430.93,13.84,8.74" target="#b22">[23]</ref><ref type="bibr" coords="2,381.74,430.93,12.73,8.74" target="#b25">26]</ref> or on T5 <ref type="bibr" coords="2,439.91,430.93,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,453.88,430.93,7.75,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,465.10,430.93,11.62,8.74" target="#b24">25]</ref>. Re-ranking is often performed in pointwise and pairwise fashion as proposed in <ref type="bibr" coords="2,146.63,454.84,14.61,8.74" target="#b15">[16]</ref>. All our runs use this standard two-stage pipeline with BM25 combined with pseudo-relevance feedback for first-pass retrieval and a pointwise/pairwise T5 re-ranker.</p><p>The mixed-initiative paradigm of conversational search offers the possibility for the system to ask clarifying questions at any point of the interaction <ref type="bibr" coords="2,467.30,502.85,9.96,8.74" target="#b2">[3]</ref>. Asking clarifying questions is proven to be beneficial to conversational search, as the system can provide the user with more relevant results after receiving the answer <ref type="bibr" coords="2,168.50,538.71,9.96,8.74" target="#b0">[1]</ref>. Approaches to asking clarifying questions range from selecting the appropriate question from the pre-defined pool of questions <ref type="bibr" coords="2,397.12,550.67,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,410.93,550.67,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,421.96,550.67,12.73,8.74" target="#b17">18]</ref> to clarifying question generation <ref type="bibr" coords="2,239.38,562.62,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="2,258.08,562.62,11.62,8.74" target="#b26">27]</ref>. Our mixed-initiative submission uses clarifying questions selected from the candidate pool based on question-query semantic similarity to extend the current query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We briefly cover the approaches used in implementing different components of the system: indexer, query rewriter, first-pass retriever, and re-ranker. The Fig. <ref type="figure" coords="3,154.40,207.10,4.13,7.89">1</ref>. Architecture of our retrieve-then-rerank pipeline with a query rewriting module.</p><p>schema of the architecture used is presented in Figure <ref type="figure" coords="3,374.00,241.17,3.87,8.74">1</ref>. We focus on the main task in Sections 3.1-3.4 and on the mixed-initiative subtask in Section 3.5. Specific runs are discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing</head><p>The default index used by our baseline <ref type="bibr" coords="3,311.42,316.56,10.52,8.74" target="#b8">[9]</ref> is based on Elasticsearch (v7.13.3). Each passage consists of three fields: title, body, and catch all, which is a concatenation of the two. The passages are preprocessed using the Elasticsearch built-in analyzer which is responsible for tokenization (removing punctuation symbols and dividing passages to terms using the Unicode Text Segmentation algorithm), stopwords removal (based on English corpus from the NLTK toolkit), and stemming (KStem).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Rewriting</head><p>Baseline The default query rewriter in our baseline is based on a T5 model fine-tuned with the CANARD dataset. We use a trained model shared on Hugging Face (castorini/t5-base-canard<ref type="foot" coords="3,308.72,462.11,3.97,6.12" target="#foot_0">1</ref> ). Our implementation is based on the Hugging Face transformers library. <ref type="foot" coords="3,288.22,474.07,3.97,6.12" target="#foot_1">2</ref> All the previous rewritten utterances and the canonical response for the last utterance are used as context to reformulate the current question. If the length of the input sentence exceeds 512, the answer passage is cut off.</p><p>Sparse Query Rewriter Using a fine-tuned T5 model, we generate the top 10 query rewrites according to their beam search score. Each rewrite is split into terms which are stored together with the beam search score in a weighted bag-ofwords (BOW) fashion. For each unique term in the obtained collection of terms, the term weights are summed up from all rewrites. The collection of unique terms and weights is first normalized such that all weights sum up to 1. Then, it is converted into a compound Elasticsearch query consisting of weighted term queries. Effectively, this acts both as a query rewriting and a query expansion method. The highest scoring rewrite is the query rewrite, while the other rewrites determine the term-query importance and further extend the rewritten query with additional terms obtained from the context (i.e., the conversation history). The normalization step is needed so that we can combine the results with that of other query modeling approaches, e.g., pseudo-relevance feedback (PRF). The terms obtained by PRF can be linearly interpolated with the weighted BOW terms.</p><p>Intent Classification We train a BERT intent classifier using the MSDialog-Intent dataset <ref type="bibr" coords="4,197.67,372.19,14.61,8.74" target="#b16">[17]</ref>, which has 12 intent classes. For each query, the intent classification is performed at the sentence level. We identify 4 classes: Junk (JK), Greetings/Gratitude (GG), Negative Feedback (NF), and Positive Feedback (PF); we assume that these intents can introduce some noise in the query and thus negatively impact its rewrite. For example, in the query "Okay. Are there better calorie-burning alternatives to try?" the classifier returns the label PF for the first sentence and FQ (Follow Up Question) for the second. We believe that the first sentence (i.e., "Okay") will not provide useful context to perform the rewrite. Therefore, the idea is to remove the sentences with these classes in the case of multi-sentence queries before performing the query rewrite. Furthermore, if negative feedback is detected, we replace the last rewritten query with the raw query in the context based on the assumption that it can reduce the propagation of potential noise from the last query rewrite. Table <ref type="table" coords="4,358.13,515.66,4.98,8.74" target="#tab_1">1</ref> presents an example where negative feedback is detected in the first sentence of turn 2-9. Therefore, the last query in the context is the raw query instead of the rewritten query of the turn 2-7. In this example, one can think that they and them from the raw query in turn 2-7 do not correspond to developed countries and Paris Agreement in the rewritten query of the same turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">First-pass Retrieval</head><p>Baseline We rank passages in first-pass retrieval using BM25 with the parameters tuned on 2020 and 2021 CAsT datasets (k1 = 0.95, b = 0.45) using the catch all index field. The top 1000 candidates for each turn are selected for re-ranking.</p><p>Pseudo-Relevance Feedback Optionally, pseudo-relevance feedback (PRF) can be added to the pipeline for query expansion. To generate an expanded query, we use the popular RM3 method. RM3 extends the initial query with the highest-weighting terms from top-k scoring documents (we use k = 10 and the number of terms m = 10). The term weights are normalized such that weights of all terms to be added sum to 1 and linearly interpolated with the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Passage Re-ranking</head><p>Baseline The passages retrieved during first-pass retrieval are reranked with a neural re-ranker. Our baseline re-ranker is a pointwise monoT5 re-ranker, followed by a pairwise duoT5 re-ranker <ref type="bibr" coords="5,294.85,263.18,14.61,8.74" target="#b15">[16]</ref>. We use the monoT5 model introduced by Nogueira et al. <ref type="bibr" coords="5,212.00,275.13,14.61,8.74" target="#b14">[15]</ref>, published on Hugging Face (castorini/monot5-base-msmarco<ref type="foot" coords="5,160.92,285.51,3.97,6.12" target="#foot_2">3</ref> ). The tokenizer associated with this model is used for encoding the querypassage pairs for the input. The top 50 passages from monoT5 are passed for re-ranking with duoT5. Our implementation of duoT5 is based on the Hugging Face transformers library and the castorini/duot5-base-msmarco model published on Hugging Face. <ref type="foot" coords="5,238.74,333.33,3.97,6.12" target="#foot_3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Mixed-initiative</head><p>Our approach to the mixed-initiative sub-task consists of two modules: clarifying question selection and answer processing.</p><p>Clarifying question selection. We select clarifying questions to ask from the candidate pool of questions provided by the organizers. In order to ensure that the selected questions are of high quality, we first filter out potentially misleading, unreliable, and faulty questions from the pool. The aim of this step is to remove questions that resemble queries, rather than clarifying questions (e.g., "What is food like in Nigeria?" vs. "Would you like to know more about food in Nigeria?"). To this end, we utilize a RoBERTa-based <ref type="bibr" coords="5,411.33,496.39,15.50,8.74" target="#b11">[12]</ref> classifier to distinguish between regular questions taken from CAsT'20/21 and clarifying questions taken from ClariQ <ref type="bibr" coords="5,265.13,520.30,9.96,8.74" target="#b1">[2]</ref>. Specifically, we fine-tune the roberta-base<ref type="foot" coords="5,476.12,518.73,3.97,6.12" target="#foot_4">5</ref> model on several hundred positive and negative examples, achieving the accuracy of 97% on a development set comprising 400 questions taken from CAsT'21 and ClariQ, which are not part of the training set. Finally, we apply the classifier to the provided candidate question pool and filter out around 20% of the questions. We formulate the task of asking clarifying questions as a ranking task. More specifically, for each query, we rank the potential clarifying question candidates based on their semantic similarity. To this end, we use MPNet <ref type="bibr" coords="5,399.04,603.99,15.50,8.74" target="#b19">[20]</ref> from Sentence-Transformers, trained for general-purpose semantic matching. For each query, we select the clarifying question with the highest score, as predicted by MPNet. (The run corresponding to this method is called uis clearboat.) Clarifying question generation. We utilize a template-based method for generating clarifying questions. First, we employ a topic model on the top 200 passages retrieved in response to the original utterance. Specifically, we utilize Top2Vec <ref type="bibr" coords="6,175.85,196.87,9.96,8.74" target="#b3">[4]</ref>, a neural topic model that automatically detects topics from text. Then, we formulate a template-based question with up to top three extracted topics (e.g., "Are you interested in Topic 1, Topic 2, or Topic 3?"). We adjust the template accordingly if less than 3 topics are extracted from the passages. (This method for constructing clarifying questions generated the run uis vagueboat.) Answer processing. We define three possible actions, based on the current utterance, the clarifying question asked, and the answer from the organizers. Specifically: <ref type="bibr" coords="6,133.99,318.71,12.73,8.74" target="#b0">(1)</ref> In case the answer is affirmative (e.g., "yes" or "Yes, that is what I'm looking for"), we expand the current utterance by appending the clarifying question asked. <ref type="bibr" coords="6,133.99,354.61,12.73,8.74" target="#b1">(2)</ref> In case the answer is deemed useful, i.e., a certain degree of the underlying information need is expressed, we expand the current utterance by appending the answer. (3) If neither (1) nor ( <ref type="formula" coords="6,233.49,390.51,4.24,8.74">2</ref>) is selected, we do not expand the utterance.</p><p>To classify a (utterance, clarifying question, answer) triplet in one of the aforementioned classes, we fine-tune RoBERTa on 150 samples from ClariQ, which we manually annotated. We perform a grid search for the optimal parameters and settle for a learning rate of 4 × 10 -4 for 2 epochs. The classifier is then run on the CAsT'22 data. Finally, in cases ( <ref type="formula" coords="6,308.63,458.36,4.24,8.74">1</ref>) and ( <ref type="formula" coords="6,343.35,458.36,3.87,8.74">2</ref>), where the original utterance was expanded, we run a T5-based model for query rewriting, explained in Section 3.2. The rewritten utterance is further fed into the standard retrieval and re-ranking pipeline explained in Sections 3.3-3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>This section contains a high-level description of our submitted runs. The differences between runs lie mainly in the query rewriting component.</p><p>uis duoboat This run is considered as our baseline. It uses the most basic versions of our components. First-pass retrieval is based on BM25 with the parameters tuned on 2020 and 2021 CAsT datasets. It is followed by monoT5 re-ranking and duoT5 re-ranking fine-tuned on MS MARCO. Query rewriting is performed with a HuggingFace model fine-tuned on the CANARD dataset, using previously rewritten utterances and the last canonical response as context. uis sparseboat This run is similar to uis duoboat with the addition of sparse query rewriting and pseudo-relevance feedback. uis cargoboat For this run, the input query is pre-processed based on its intents before performing sparse query rewriting (cf. Section 3.2). The firstpass retrieval is based on BM25 with the same parameters as our baseline and additionally employs pseudo-relevance feedback. Finally, the re-ranking stage is the same as in uis duoboat. uis mixedboat For each query, we select the highest scoring clarifying question and process the given answer as described in Section 3.5 (uis clearboat).</p><p>The expanded query is then fed into the pipeline of BM25 first-pass retrieval with pseudo-relevance feedback and mono/duo T5 re-ranking (as in uis duoboat).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This section presents the performance our runs on the TREC CAsT'21 and '22 datasets, and discusses the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on TREC CAsT 2021</head><p>Table <ref type="table" coords="7,162.59,488.75,4.98,8.74" target="#tab_3">3</ref> reports the performance of the systems used for generating this year's submission runs on the TREC CAsT'21 dataset. Additionally, we report on the performance of the basic retrieve-then-re-rank system with raw (No rewrites), automatically rewritten (TREC-Auto), and manually rewritten (TREC-Manual) queries, as provided by the track organizers. We find that our baseline system (uis duoboat) outperforms the system using TREC-Auto in the re-ranking stage (higher values of NDCG at early rank positions) but achieves lower results in first-pass retrieval, which is most likely caused by a worse-performing query rewriting module. Our top-priority run (uis sparseboat) achieves the highest results in almost all reported metrics (the only exception is MRR). The most noticeable improvement is in first-pass retrieval, where we observe a drop of only 4% in recall compared to the system using manual rewrites. The uis cargoboat run outperforms our baseline system in first-pass retrieval. However, the intent classification used in query rewriting yields lower performance in the re-ranking stage (NDCG at early positions is lower than for the baseline). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Clarifying Question Evaluation</head><p>Table <ref type="table" coords="8,161.69,393.73,4.98,8.74" target="#tab_4">4</ref> reports the performance of our clarifying question construction methods described in Section 3.5, i.e., uis clearboat and uis vagueboat. Performance is measured in terms of relevance, novelty, and diversity, and compared against organizers' baselines and the top performing MI subtask submission (GPT-3 full context). The results show significant improvements of our question selection method (uis clearboat) over relevant baselines across all three metrics. On the other hand, the clarifying question generation method (uis vagueboat) demonstrates significantly worse performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on TREC CAsT 2022</head><p>Table <ref type="table" coords="8,162.47,528.77,4.98,8.74" target="#tab_5">5</ref> reports the performance of the submitted runs on TREC CAsT'22 according to the evaluation results provided by the organizers. These correspond to a strict evaluation, where passages must be of at least relevance level 2. Note that the numbers slightly differ from those we reported in our working notes paper, due to a bug in the organizers' evaluation script. We include the updated numbers here, which we received from the organizers in private communication.</p><p>For reference, we include the results of the baseline systems provided by the organizers (BM25 T5 BART automatic and BM25 T5 BART manual), 6 the best performing participant system (udinfo m b2021) as well as the median for the main evaluation measure (NDCG@3). Note that these values are taken from the 6 https://github.com/daltonj/treccastweb/tree/master/2022/baselines/main task The priorities assigned by us for the runs are in accordance with the performance of the corresponding systems (with the exception of the uis cargoboat run performing slightly better than uis mixedboat). Similarly to the evaluation on TREC CAsT'21, uis sparseboat achieves the highest results in all reported metrics. The most noticeable differences with other runs are observed in recall. The run uis cargoboat, which differs from uis sparseboat only in adding intent classification, yields lower performance. It implies that our intent handling is removing some important information from the queries, which negatively impacts both first-pass retrieval and end-to-end performance. The performance of uis mixedboat is comparable to that of uis cargoboat. However, it is difficult to reason about the impact of mixed-initiative components on the results, due to significant differences between the underlying ranking pipelines. Our baseline (uis duoboat) outperforms the organizers' baseline with automatic query rewrites (BM25 T5 BART automatic) in first-pass retrieval. However, the reranking stage is less effective in pushing the most relevant documents to the top of the final ranking (NDCG@3 is lower than the organizers' baseline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In TREC CAsT'22, our team experimented with different approaches for query rewriting. All the experiments with this module were performed using a wellestablished two-stage retrieval-re-ranking pipeline using BM25 combined with pseudo-relevance feedback for first-pass retrieval and a pointwise/pairwise reranker. Results indicate that our sparse query rewriting works well, while leveraging intent classification in query rewriting causes a slight drop in performance. Performance analysis of the mixed-initiative run remains to be done as evaluation details for this sub-task were not available at the time of writing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,116.41,345.83,119.13"><head>Table 1 .</head><label>1</label><figDesc>Illustration of intent classification-based query rewriting (topic 132). Intents are placed at the end of each sentence (CQ FQ: clarifying/follow up question, FQ IR: follow up question, information request).</figDesc><table coords="4,139.75,161.26,335.87,74.28"><row><cell cols="2">Turn Raw query</cell><cell>Rewritten query</cell></row><row><cell>2-5</cell><cell>How are developed countries helping with</cell><cell>How are developed countries helping with</cell></row><row><cell></cell><cell>that? [CQ FQ]</cell><cell>climate change adaptation?</cell></row><row><cell>2-7</cell><cell>Are they meeting them? [FQ IR]</cell><cell>Are developed countries meeting the Paris</cell></row><row><cell></cell><cell></cell><cell>Agreement?</cell></row><row><cell>2-9</cell><cell>That's not too relevant to my question.</cell><cell>By the way, is COP26 related to last year's</cell></row><row><cell></cell><cell>[NF] By the way, is that related to last</cell><cell>conference?</cell></row><row><cell></cell><cell>year's conference? [FQ IR]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,139.75,115.91,338.12,98.96"><head>Table 2 .</head><label>2</label><figDesc>Overview of submitted runs.</figDesc><table coords="7,139.75,136.61,338.12,78.26"><row><cell>RunID</cell><cell>Query rewriting</cell><cell></cell><cell>First-pass retr.</cell><cell>Re-ranking</cell><cell>Task Priority</cell></row><row><cell>uis duoboat</cell><cell cols="2">T5 fine-tuned on CA-</cell><cell>BM25</cell><cell cols="2">mono/duoT5 Main</cell><cell>4</cell></row><row><cell></cell><cell cols="2">NARD (T5 CANARD)</cell><cell></cell><cell></cell></row><row><cell cols="3">uis sparseboat sparse query rewriting</cell><cell>BM25 + PRF</cell><cell cols="2">mono/duoT5 Main</cell><cell>1</cell></row><row><cell>uis cargoboat</cell><cell cols="2">sparse query rewriting</cell><cell>BM25 + PRF</cell><cell cols="2">mono/duoT5 Main</cell><cell>3</cell></row><row><cell></cell><cell cols="2">with intent classification</cell><cell></cell><cell></cell></row><row><cell cols="2">uis mixedboat mixed-initiative</cell><cell>query</cell><cell>BM25 + PRF</cell><cell cols="2">mono/duoT5 MI</cell><cell>2</cell></row><row><cell></cell><cell>rewriting</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,139.75,115.91,347.41,101.10"><head>Table 3 .</head><label>3</label><figDesc>Performance of our approaches on TREC CAsT'21.</figDesc><table coords="8,139.75,138.35,347.41,78.66"><row><cell>Approach/RunID</cell><cell cols="4">R@1000 MAP MRR NDCG NDCG@3 NDCG@5</cell></row><row><cell>No rewrites + BM25 + monoT5</cell><cell>0.3497</cell><cell>0.1217 0.2875 0.2605</cell><cell>0.2051</cell><cell>0.2041</cell></row><row><cell>TREC-Auto + BM25 + monoT5</cell><cell>0.6319</cell><cell>0.2684 0.5575 0.4842</cell><cell>0.3972</cell><cell>0.3969</cell></row><row><cell>TREC-Manual + BM25 + monoT5</cell><cell>0.7733</cell><cell>0.3858 0.7326 0.6293</cell><cell>0.5611</cell><cell>0.5654</cell></row><row><cell>uis duoboat</cell><cell>0.6037</cell><cell>0.2544 0.5563 0.4724</cell><cell>0.4110</cell><cell>0.4048</cell></row><row><cell>uis sparseboat</cell><cell cols="2">0.7424 0.2986 0.5979 0.5475</cell><cell>0.4405</cell><cell>0.4380</cell></row><row><cell>uis cargoboat</cell><cell>0.6930</cell><cell>0.2977 0.6605 0.5132</cell><cell>0.3870</cell><cell>0.3918</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,135.08,231.59,345.20,113.06"><head>Table 4 .</head><label>4</label><figDesc>Clarification question evaluation of MI runs using crowdsourced jugdments.</figDesc><table coords="8,139.75,254.03,314.75,90.62"><row><cell>Approach/RunID</cell><cell cols="3">Relevance @1 Novelty @1 Diversity @1</cell></row><row><cell>(baseline) T5 raw</cell><cell>0.232</cell><cell>0.166</cell><cell>0.185</cell></row><row><cell>(baseline) T5 rewrite</cell><cell>0.320</cell><cell>0.229</cell><cell>0.210</cell></row><row><cell>(baseline) bm25 baseline mi</cell><cell>0.345</cell><cell>0.293</cell><cell>0.307</cell></row><row><cell>(baseline) miniLM bert sample mi run</cell><cell>0.371</cell><cell>0.317</cell><cell>0.395</cell></row><row><cell>TREC best</cell><cell>0.852</cell><cell>0.536</cell><cell>0.607</cell></row><row><cell>uis vagueboat</cell><cell>0.237</cell><cell>0.322</cell><cell>0.381</cell></row><row><cell>uis clearboat</cell><cell>0.639</cell><cell>0.488</cell><cell>0.371</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,134.77,116.41,345.83,191.07"><head>Table 5 .</head><label>5</label><figDesc>Performance of our approaches on TREC CAsT'22 (strict evaluation).</figDesc><table coords="9,134.77,139.35,345.83,168.13"><row><cell>Approach/RunID</cell><cell cols="5">R@1000 MAP MRR NDCG NDCG@3</cell></row><row><cell>BM25 T5 BART automatic</cell><cell>0.324</cell><cell>0.150</cell><cell>0.527</cell><cell>0.299</cell><cell>0.362</cell></row><row><cell>BM25 T5 BART manual</cell><cell>0.465</cell><cell>0.231</cell><cell>0.716</cell><cell>0.423</cell><cell>0.503</cell></row><row><cell>TREC best</cell><cell>0.771</cell><cell>0.246</cell><cell>0.656</cell><cell>0.557</cell><cell>0.452</cell></row><row><cell>TREC median</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.347</cell></row><row><cell>uis duoboat</cell><cell>0.365</cell><cell>0.154</cell><cell>0.476</cell><cell>0.323</cell><cell>0.345</cell></row><row><cell>uis sparseboat</cell><cell>0.507</cell><cell>0.189</cell><cell>0.566</cell><cell>0.409</cell><cell>0.388</cell></row><row><cell>uis cargoboat</cell><cell>0.450</cell><cell>0.180</cell><cell>0.526</cell><cell>0.377</cell><cell>0.373</cell></row><row><cell>uis mixedboat</cell><cell>0.445</cell><cell>0.186</cell><cell>0.499</cell><cell>0.374</cell><cell>0.363</cell></row><row><cell cols="6">notebook version of the track overview paper and might have changed slightly</cell></row><row><cell cols="6">due to the aforementioned bug; at the time of writing, the updated results for</cell></row><row><cell cols="2">other runs were not available.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.84,194.46,7.86"><p>https://huggingface.co/castorini/t5-base-canard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.80,184.28,7.86"><p>https://github.com/huggingface/transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,634.88,225.23,7.86"><p>https://huggingface.co/castorini/monot5-base-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.73,645.84,214.48,7.86"><p>https://huggingface.co/castorini/duo5-base-msmarco</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.73,656.80,145.55,7.86"><p>https://huggingface.co/roberta-base</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,152.20,143.37,328.39,6.12;10,152.20,151.34,328.39,6.12;10,152.20,159.31,328.39,6.12;10,152.20,167.28,50.88,6.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,356.25,143.37,124.34,6.12;10,152.20,151.34,146.13,6.12">Asking clarifying questions in opendomain information-seeking conversations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,319.13,151.38,161.46,6.08;10,152.20,159.31,303.91,6.12">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;19</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,175.86,328.39,6.12;10,152.20,183.83,328.39,6.12;10,152.20,191.80,281.65,6.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,396.71,175.86,83.89,6.12;10,152.20,183.83,194.90,6.12">Building and evaluating open-domain dialogue corpora with clarifying questions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiseleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Burtsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,366.31,183.86,114.29,6.08;10,152.20,191.80,195.31,6.12">Findings of the Association for Computational Linguistics: EMNLP 2021, EMNLP &apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4473" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,200.37,328.39,6.12;10,152.20,208.34,148.41,6.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,290.48,200.37,93.98,6.12">Mixed-initiative interaction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Guinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,390.53,200.41,90.06,6.08;10,152.20,208.38,78.81,6.08">IEEE Intelligent Systems and their Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,216.91,322.12,6.12" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Angelov</surname></persName>
		</author>
		<idno>arXiv, cs.CL/2008.09470</idno>
		<title level="m" coord="10,196.95,216.91,160.92,6.12">Top2Vec: Distributed representations of topics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,225.49,328.40,6.12;10,152.20,233.46,328.39,6.12;10,152.20,241.43,272.52,6.12" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,197.76,233.46,268.07,6.12">Query expansion with semantic-based ellipsis reduction for conversational IR</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-T</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,152.20,241.43,248.58,6.12">The Twenty-Ninth Text REtrieval Conference Proceedings, TREC &apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,250.00,328.40,6.12;10,152.20,257.97,321.22,6.12" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,284.53,250.00,196.06,6.12;10,152.20,257.97,28.72,6.12">TREC CAsT 2019: The conversational assistance track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,198.03,257.97,251.46,6.12">The Twenty-Eighth Text REtrieval Conference Proceedings, TREC &apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,266.54,328.40,6.12;10,152.20,274.51,328.40,6.12;10,152.20,282.48,104.56,6.12" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,252.01,266.54,228.59,6.12;10,152.20,274.51,140.59,6.12">Glasgow representation and information learning lab (GRILL) at the conversational assistance track 2020</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,312.86,274.55,167.74,6.08;10,152.20,282.48,80.62,6.12">The Twenty-Ninth Text REtrieval Conference Proceedings, TREC &apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,291.06,328.39,6.12;10,152.20,299.03,328.39,6.12;10,152.20,307.00,328.39,6.12;10,152.20,314.97,58.48,6.12" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,470.31,291.06,10.28,6.12;10,152.20,299.03,328.39,6.12;10,152.20,307.00,117.80,6.12">An exploration study of multi-stage conversational passage retrieval: Paraphrase query expansion and multi-view point-wise ranking</title>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-T</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,286.94,307.03,193.65,6.08;10,152.20,314.97,34.54,6.12">The Thirtieth Text REtrieval Conference Proceedings, TREC &apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,323.54,328.39,6.12;10,152.20,331.51,328.39,6.12;10,152.20,339.48,104.56,6.12" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,354.20,323.54,126.39,6.12;10,152.20,331.51,163.51,6.12">The University of Stavanger (IAI) at the TREC 2021 conversational assistance track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kostric</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Book</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Linjordet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Setty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,332.80,331.55,147.79,6.08;10,152.20,339.48,80.62,6.12">The Thirtieth Text REtrieval Conference Proceedings, TREC &apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,348.06,328.39,6.12;10,152.20,356.03,328.39,6.12;10,152.20,364.00,148.82,6.12" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,247.08,348.06,233.51,6.12;10,152.20,356.03,61.26,6.12">Making information seeking easier: An improved pipeline for conversational search</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,232.35,356.06,248.24,6.08;10,152.20,364.00,62.48,6.12">Findings of the Association for Computational Linguistics: EMNLP 2020, EMNLP &apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3971" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,372.57,328.40,6.12;10,152.20,380.54,328.39,6.12;10,152.20,388.51,259.03,6.12" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,406.14,372.57,74.46,6.12;10,152.20,380.54,328.39,6.12;10,152.20,388.51,30.34,6.12">Multi-stage conversational passage retrieval: An approach to fusing term importance estimation and neural query rewriting</title>
		<author>
			<persName coords=""><forename type="first">S.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,189.46,388.54,155.86,6.08">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,397.08,328.40,6.12;10,152.20,405.05,328.39,6.12;10,152.20,413.02,84.61,6.12" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Roberta</surname></persName>
		</author>
		<idno>cs.CL/1907.11692</idno>
		<title level="m" coord="10,265.50,405.05,180.44,6.12">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,421.60,328.40,6.12;10,152.20,429.57,328.39,6.12;10,152.20,437.54,328.39,6.12;10,152.20,445.51,18.14,6.12" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,439.91,421.60,40.69,6.12;10,152.20,429.57,109.93,6.12">Topic propagation in conversational search</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Muntean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,278.85,429.60,201.74,6.08;10,152.20,437.54,263.56,6.12">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2057" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,454.08,328.39,6.12;10,152.20,462.05,328.39,6.12;10,152.20,470.02,46.90,6.12" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,449.06,454.08,31.53,6.12;10,152.20,462.05,154.44,6.12">Adaptive utterance rewriting for conversational search</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">I</forename><surname>Muntean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,313.73,462.08,139.38,6.08">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">102682</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,478.59,328.39,6.12;10,152.20,486.56,328.39,6.12;10,152.20,494.53,87.74,6.12" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,276.00,478.59,204.59,6.12;10,152.20,486.56,19.32,6.12">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,187.33,486.56,293.26,6.12;10,152.20,494.53,9.34,6.12">Findings of the Association for Computational Linguistics: EMNLP 2020, EMNLP &apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,503.11,328.39,6.12;10,152.20,511.08,270.04,6.12" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,286.59,503.11,194.00,6.12;10,152.20,511.08,155.02,6.12">The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>arXiv, cs.IR/2101.05667</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,519.65,328.39,6.12;10,152.20,527.62,328.40,6.12;10,152.20,535.59,328.39,6.12;10,152.20,543.56,50.88,6.12" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,408.18,519.65,72.40,6.12;10,152.20,527.62,204.32,6.12">Analyzing and characterizing user intent in information-seeking conversations</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Trippas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,375.61,527.66,104.99,6.08;10,152.20,535.59,303.91,6.12">The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="989" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,552.13,328.40,6.12;10,152.20,560.10,328.39,6.12;10,152.20,568.07,144.21,6.12" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,453.13,552.13,27.47,6.12;10,152.20,560.10,180.84,6.12">Leading conversational search by suggesting useful questions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,350.79,560.14,129.81,6.08;10,152.20,568.07,57.87,6.12">Proceedings of The Web Conference 2020, WWW &apos;20</title>
		<meeting>The Web Conference 2020, WWW &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1160" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,576.65,328.39,6.12;10,152.20,584.62,328.39,6.12;10,152.20,592.59,274.15,6.12" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,305.26,576.65,175.33,6.12;10,152.20,584.62,105.13,6.12">Towards facet-driven generation of clarifying questions for conversational search</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sekulić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aliannejadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.37,584.65,205.22,6.08;10,152.20,592.59,195.75,6.12">Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR &apos;21</title>
		<meeting>the 2021 ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR &apos;21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="167" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,601.16,328.39,6.12;10,152.20,609.13,198.16,6.12" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="10,317.93,601.16,162.66,6.12;10,152.20,609.13,81.25,6.12">MPNet: Masked and permuted pre-training for language understanding</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv, cs.CL/2004.09297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,617.70,328.39,6.12;10,152.20,625.68,328.39,6.12;10,152.20,633.65,166.70,6.12" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,322.86,617.70,157.73,6.12;10,152.20,625.68,32.88,6.12">Question rewriting for conversational question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Anantha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,204.57,625.71,276.02,6.08;10,152.20,633.65,88.30,6.12">Proceedings of the 14th ACM International Conference on Web Search and Data Mining, WSDM &apos;21</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining, WSDM &apos;21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,152.20,642.22,328.39,6.12;10,152.20,650.19,328.39,6.12;10,152.20,658.16,87.74,6.12" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,329.84,642.22,150.75,6.12;10,152.20,650.19,120.23,6.12">A comparison of question rewriting methods for conversational passage retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Voskarides</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,288.34,650.19,192.25,6.12;10,152.20,658.16,9.34,6.12">European Conference on Information Retrieval, ECIR &apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="418" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,152.20,121.03,328.39,6.12;11,152.20,129.00,328.39,6.12;11,152.20,136.97,328.39,6.12;11,152.20,144.95,18.14,6.12" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,359.44,121.03,121.15,6.12;11,152.20,129.00,107.68,6.12">Query resolution for conversational search with limited supervision</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Voskarides</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,277.41,129.04,203.18,6.08;11,152.20,136.97,270.01,6.12">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="921" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,152.20,152.92,328.39,6.12;11,152.20,160.89,328.39,6.12;11,152.20,168.86,238.11,6.12" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="11,152.20,160.89,290.41,6.12">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Overwijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,459.77,160.92,20.83,6.08;11,152.20,168.86,214.17,6.12">International Conference on Learning Representations, ICLR &apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,152.20,176.83,328.40,6.12;11,152.20,184.80,321.34,6.12" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="11,304.38,176.83,176.21,6.12;11,152.20,184.80,49.40,6.12">WaterlooClarke at the TREC 2021 conversational assistant track</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arabzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,218.45,184.80,231.16,6.12">The Thirtieth Text REtrieval Conference Proceedings, TREC &apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,152.20,192.77,328.39,6.12;11,152.20,200.74,328.39,6.12;11,152.20,208.71,284.95,6.12" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="11,383.57,192.77,97.02,6.12;11,152.20,200.74,83.01,6.12">Few-shot generative conversational query rewriting</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,253.68,200.77,226.91,6.08;11,152.20,208.74,198.28,6.08">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1933" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,152.20,216.68,328.39,6.12;11,152.20,224.65,328.40,6.12;11,152.20,232.62,50.88,6.12" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="11,372.53,216.68,108.06,6.12;11,152.20,224.65,85.21,6.12">Generating clarifying questions for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,258.07,224.65,196.68,6.12">Proceedings of The Web Conference 2020, WWW &apos;20</title>
		<meeting>The Web Conference 2020, WWW &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="418" to="428" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
