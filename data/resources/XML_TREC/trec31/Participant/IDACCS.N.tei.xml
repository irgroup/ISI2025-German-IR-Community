<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.87,164.85,333.52,15.12;1,138.81,186.77,333.62,15.12;1,266.70,208.68,77.85,15.12">Extremely Fast Fine-Tuning for Cross Language Information Retrieval via Generalized Canonical Correlation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-31">October 31, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.51,241.17,80.38,10.48"><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
						</author>
						<author>
							<persName coords="1,264.41,241.17,71.69,10.48"><forename type="first">Neil</forename><forename type="middle">P</forename><surname>Molino</surname></persName>
						</author>
						<author>
							<persName coords="1,368.30,241.17,68.44,10.48"><forename type="first">Julia</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main" coord="1,138.87,164.85,333.52,15.12;1,138.81,186.77,333.62,15.12;1,266.70,208.68,77.85,15.12">Extremely Fast Fine-Tuning for Cross Language Information Retrieval via Generalized Canonical Correlation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-31">October 31, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">2E162A7F1280D19362910DC6C88FCC20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, work using language agnostic transformer neural sentence embeddings show promise for a robust multilingual sentence representation. Our submission to TREC was to test how well these embeddings could be fine-tuned cheaply to perform the task of cross-lingual information retrieval. We explore the use of the MS MARCO dataset with machine translations as a model problem. We demonstrate that a single generalized canonical correlation analysis (GCCA) model trained on previous queries significantly improves the ability of sentence embeddings to find relevant passages. The dominant computational cost for training is computing dense singular value decompositions (SVDs) of matrices derived from the fine-tuning training data. (The number of SVDs used is the number of languages retrieval views and query views plus 1). This approach illustrates that GCCA methods can be used as a rapid training alternative to fine-tuning a neural net, allowing models to be fine-tuned frequently based on a user's previous queries. This model was then used to prepare submissions for the re-ranking NeuCLIR task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work using language agnostic transformer neural sentence embeddings show promise. In this notebook we describe some very low cost methods to fine-tune and adapt sentence embeddings to CLIR. The approach is based on classical statistics and the cost to perform the fine-tuning is considerably less than computing the sentence embeddings.</p><p>There are settings where we have a small amount of labeled data, but not enough to even do a fine-tuning of a transformer-based large language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Sentences embedding from a modern neural net model, such as produced by a recurrent neural net or a transformer model provide a rich semantic repre-sentation of the content of a sentence. It is natural to use language agnostic embeddings to find related text. In particular, <ref type="bibr" coords="2,342.86,139.92,10.52,8.74">[6]</ref> shows such representations, along with positional information, are very strong methods for finding bitext pairs of across languages. Here, we explore to what extend sentence embeddings can be used for cross-lingual question and answering and more generally cross-lingual information retrieval (CLIR). In the question and answering setting there is some promise in that answers to questions often use much of the same vocabulary and a sentence embedding will reflect this overlap in word usage, especially the overlap include bigrams and trigrams. We test this approach uses translations of the <ref type="bibr" coords="2,236.47,235.56,50.69,8.74">TREC 2002</ref><ref type="bibr" coords="2,291.95,235.56,19.19,8.74">TREC -2004</ref> Novelty track data<ref type="foot" coords="2,394.85,233.98,3.97,6.12" target="#foot_0">1</ref> as well as the MS MARCO data translations <ref type="bibr" coords="2,250.71,247.51,9.96,8.74" target="#b2">[3]</ref>. Both of these datasets, provide aligned sentences in a CLIR where the queries are English and the relevant text is in English as well as machine generated translations in Russian, English, and Farsi.</p><p>Computing a similarity of a query and all candidate passages can be done by embedding both the query and the passages. For specificity let us denote the embedding query by the vector q and passages p i with q, p i ∈ R n . For ease of notation we assume the sentence embeddings are on the unit sphere. Thus, the cosine similarity is simply the inner product and we can find the nearest vector to a query q via p = arg max j q T p j .</p><p>If a query has multiple parts the inner product scores can be combined via a reduction operator such as maximum or summation. The dimension n is generally fairly large, typically 512 to 1024, and it is natural to ask if queries and relevant passages can be made to be closer in a lower dimensional space by employing classical statistics and linear algebra. A natural choice is canonical correlation analysis (CCA) and its generalizations. Previously, the paper [7] uses a correlation matrix method which is a variant of CCA in an image classification. Here, we employ CCA and a sumcorr (sums of correlation) which is a generalized canonical correlation (GCCA). There are other generalizations, but we limit our attention to a regularized sumcorr. Employing a GCCA allows us to consider embedding multiple parts of a query (e.g., title, description, and narrative) as well as one or more translations of relevant passages into the same space.</p><p>The GCCA procedure produce linear transformations A (k) for k = 1, . . . , ν, which project the n dimensional vectors into d dimensional space, where ν is the number of views. To ease notation, let X (k) be a mean 0 random variable of dimension n for the k-th view. These random variable are, for example sentence embeddings for aligned data, normalized so each component is mean 0. The matrices A (k) are conceptually recovered one row at a time solving the optimization problems</p><formula xml:id="formula_0" coords="3,145.95,125.27,256.11,47.29">1. a (k) 1 are the solution of arg max 1≤i&lt;j≤ν Cor(a (i) 1 X (i) , a (j) 1 X (j) )</formula><p>2. for i = 2, . . . , m : a (k) i are the solution of 1 subject to:</p><formula xml:id="formula_1" coords="3,181.14,245.82,248.97,12.69">Cor(a T X (k) , a T j X (k) ) = 0 ∀j &lt; i, k = 1, . . . , ν.</formula><p>The specific implementation of GCCA is the function MCCA from the package Multiview Learn or mvlearn <ref type="bibr" coords="3,256.35,283.02,15.03,7.01">[4]</ref>. In our application a view is a part of a query or a passage. MCCA projects the views so as to maximize a regularized sum of all pairs of correlations between the views. MCCA gives a number options for regularization. The one we found most useful for CLIR first uses principal component analysis (PCA) on each of the views to reduce the dimension. Each PCA is computed via an SVD on the view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Retrieval Task on TREC Novelty Track</head><p>This section describes results for fine-tuning sentence embeddings for an information retrieval task for the NIST TREC 2002-2004 Novelty track. In each case a baseline is reported which scores sentences for relevance via an inner product between the query vectors and the document sentence vectors. For each document set we used two query vectors, the topic and the first sentence of the description. The score for a baseline sentence is simply the sum of the two inner products.</p><p>We can build these models in multiple ways. We consider three ways:</p><p>• Training bitext data of parallel in-domain corpora. (Here we used the machine translation data of the TREC training data portion in the 5-fold cross validation.)</p><p>• Training bitext data from a large corpus, which may be out-of-domain.</p><p>(Here we used UN 6-way corpora<ref type="foot" coords="3,306.59,574.52,3.97,6.12" target="#foot_1">2</ref> but just 2M bitext pairs).</p><p>• 3-tuples of (query topic, query description, document sentence) for the TREC training data portion in the 5-fold cross validation.)</p><p>All the results presented use a generalized canonical correlation analysis, which implements a regularized version of the all-pairs method <ref type="bibr" coords="3,423.41,639.85,10.52,8.74" target="#b1">[2]</ref> [5]. The package mvlearn <ref type="foot" coords="4,211.92,126.39,3.97,6.12" target="#foot_2">3</ref> [4] provides the method MCCA and gives an easy and efficient code to compute this decomposition and save the result in class.</p><p>The Python package mvlearn was used to build lower dimensional approximation of the LaBSE sentence embeddings <ref type="bibr" coords="4,329.52,163.83,9.96,8.74" target="#b0">[1]</ref>. The dimension reduction is a generalization of principal component analysis to multiview data.</p><p>Results on TREC 2002 and 2004 are given in Tables <ref type="table" coords="4,393.52,187.74,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="4,423.71,187.74,4.98,8.74">2</ref> the AUCs are reported based on a 5-fold cross validation comparing the baseline LaBSE vectors, the mvlearn 200-dimensional representations, as well as naive Bayes model for both of these. We report just the first of the bitext training methods, so as to not belabor the reader, as the other two approaches give comparable results. In each table the English-only task, i.e., the one originally proposed by NIST is compared with an English-Russian (en/ru) and English-Chinese (en/zh) version. In the 2002 data, the mvlearn approach is the best of the four and the naive Bayes model reduces the performance for both the baseline and mvlearn.</p><p>The story is different in the 2004 data. Here, mvlearn without naive Bayes, lags behind the baseline; but with naive Bayes performs comparably. The differences were confirmed via a paired Wilcoxon test and indeed Baseline + naive Bayes is significantly better than the Baseline. MCCA(200,0.5) + naive Bayes gives results which are not significantly different than Baseline + naive Bayes.</p><p>The 2004 data is perhaps more reliable than the 2002 data for two reasons. First, 2002 was the first year for the NIST Novelty Task and there is usually a "learning curve" in evaluations for the task organizers. Second, the 2004 data has documents which contain no relevant sentences, which is closer to the CLEF task being used at SCALE 2021. So, indeed we can use the 2004 data as a "pre-filtered" data set to evaluate document retrieval. These are documents that were returned by the query engine used by NIST in 2004, so they may be viewed as a re-ranking task, not unlike for the NeuCLIR 2022 task. for each set. Each topic has a total of 25 relevant documents. Depending on the topic there is 0 to 50 non-relevant documents. The sentence scores used in the previous section were "promoted" to document scores by computing the mean sentence score for each document in the cluster. As before a 5-fold crossvalidation was done. Here, we compute the average mean precision over the 5-fold experiment. In addition to using the training data bitext sentence pairs en-ru and en-zh, we also include bitext from the UN 6-way corpus to illustrate the degradation for not having genre-specific bitext. Thus we replace building the multiview dimensionality reduction using the training newswire data with 2M bitext pairs from UN parallel corpora. The upshot is the loss is small. As the MCCA decomposition is based on only two views, it is a regularized variant of canonical correlation. For comparison sake we compare results using Python sklearn PLSSVD in the results following. To this end, we hold back a random 1% of the 800K query-passage pairs and compute a multiview embedding using 5 views: English query, and relevant passages in each of the four languages: English, Farsi, Russian, and Chinese. Based on our results with the TREC Novelty data we considered number of components set around 200 and found at least when computing one joint 5-view embedding a larger number of components gives better results. Here, we see that dimension 200 is still close to the best choice. Note that these results are computed with a joint projection computed based on all 5-views simultaneously. Recall the results presented so far are based on a 2-view model, one for each language.<ref type="foot" coords="6,174.73,305.71,3.97,6.12" target="#foot_4">4</ref> This model is convenient to compute and could be tuned further, but clearly shows that the training data of MS MARCO does improve results of using LaBSE for CLIR in all cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TREC NeuCLIR 2022 Submission</head><p>This year's TREC NeuCLIR track had three tasks. They were:</p><p>1. The Ad Hoc CLIR Task was the main task in the NeuCLIR track. There is a large document collection in Chinese, Persian, and Russian. There is also a set of English topics. For each topic, the CLIR system produces a ranked list of the 1000 most relevant documents to the topic.</p><p>2. The Reranking Task is very similar to the ad hoc one. Again, there is a set of English topics, but the system also receives a ranked list of 1000 documents. The submission is again a ranked list of the documents returned for each topic ordered from most relevant to least relevant. In this case, however, the only documents allowable in the list are those that were present in the original ranking.</p><p>3. There was also a Monlingual Retrieval Task. This can often serves as a reference point for CLIR systems. In this track, the monolignual task was very similar to the ad hoc one. The topic (query) files are actually human translations of the English topic files.</p><p>We limited our submissions, due to time, to the re-ranking problem and the ad hoc task for the NeuCLIR track i.e., we did not attempt the Monolingual task.</p><p>Our overall strategy was very similar to the one described previously. We used a language agnostic sentence representation. Prior experience had success with the LaBSE model, so we continued using that as our foundational model. We combined this with a five-view 200-dimension GCCA model trained on the translated MS MARCO data.</p><p>For both the Ad Hoc task and the Reranking task, the topic/query file contained a topic title and a topic description field. The actual articles contained a title and a text field. First we would compute the LaBSE representation, y field name , of each of these four fields. Our overall score, s, consisted of the maximum of four inner products:</p><formula xml:id="formula_2" coords="8,227.07,418.87,157.10,54.48">s = max( &lt; x topic title , x title &gt;, &lt; x topic title , x text &gt;, &lt; x topic description , x title &gt;, &lt; x topic description , x text &gt;)</formula><p>In general, we will compute x field = f (y field ) i.e, the actual representation used in the inner product of some function of the LaBSE representation of the field's text. We note that from sentence transformers the LaBSE embedding vectors are normalized automatically. They lie on the unit sphere. So, our three scores that we compute are:</p><formula xml:id="formula_3" coords="8,145.95,555.36,283.04,28.66">1. A BASELINE which is simply the identify function f (x) = x. 2. A CCA-unnormalized score which is f (x) = CCA(x).</formula><p>Here, the appropriate view (language) is applied e.g., we transform NeuCLIR Russian sentences with the MCCA Russian view trained on MS MARCO.</p><p>3. A CCA-normalized score which is f (x) = CCA(x) ||CCA(x)|| . Since the output of MCCA is not necessarily on the unit sphere, we project back. Here, again, we use the transformation for the appropriate language.</p><p>Since the reranking task included a list of 1000 ranked articles per topic, we opted to compare our results with that. Specifically, we looked at the Jaccard similarity between the articles our ad hoc systems (Baseline, CCA-normalized, and CCA-unnormalized) with the given files.</p><p>The tables below show the Jaccard similarities for Russian, Farsi, and Chinese. In all three cases, we can look at the average Jaccard similarity across the 114 topics. In general, the normalized CCA was better than both LaBSE without modification or LaBSE and CCA unnormalized. Unnormalized CCA was better than the baseline in both Russian and Farsi. See, for example, the second (mean) row in tables 8 and 10. In Chinese however, the unnormalized CCA actually under-performed the unmodified LaBSE baseline. See, e.g., Table <ref type="table" coords="9,460.88,247.51,3.87,8.74">9</ref>. We do note that, in absolute terms, these Jaccard similarities are not very high. It caused us to question the quality of this system relative to the one given for the reranking task. They are too high to be random, however, so we think our system is picking up on something, and based on the results we can confirm this.</p><p>The following plots summarize our submissions for NeuCLIR. In both Figure <ref type="figure" coords="10,150.94,384.54,4.98,8.74" target="#fig_0">1</ref> for Russian and Figure <ref type="figure" coords="10,262.55,384.54,4.98,8.74" target="#fig_2">2</ref> for Farsi, the results mirrored the results in the training data. The GCCA fine-tuning improves the retrieval and the normalized vectors give a further improvement. In Figure <ref type="figure" coords="10,369.71,408.45,4.98,8.74">3</ref> for Chinese, the three submissions have precision-recall curves which intertwine, so we do not see any improvement here. In all three languages, performance was boosted in the reranking runs. The evaluation largely validated that lightweight fine-tuning can give significant improvement for CLIR. As the approach is much less expensive than computing the embeddings, it could be employed in a real-time setting using little computational resources to fine-tune results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We demonstrate 200 dimensional embeddings could also be used in the CLIR task effectively. In particular, on one TREC data set a 3-4% improvement in performance was achieved using the CCA vectors. On a second TREC data set, which is believed to be closer to the operational task, a naive Bayes in conjunction with CCA vectors achieved a 5-8% improvement in performance. Of greater promise is that if 200-long vectors are used, the storage requirement would be 74% less for comparable performance.</p><p>For the MS MARCO data we built a single model using 800K questionanswer tuples, which included the query, and relevant answers in English, as well as translations into Farsi, Russian, and Chinese. This model gave great improvement (as measured by Precision at 1) across all languages. While the approach is not meant to compete with a full neural net finetuning, it does provide an extremely cheap alternative to neural net fine-tuning for CLIR or other tasks that depend on computing similarities of sentence representation. It is key to note that the training projection on a CPU is 6 times faster than computing the LaBSE embeddings on a GPU. Such low-cost finetuning, which was shown to require as little as 5000 aligned tuples, could allow customization at a user level for IR tasks.  [5] Arthur Tenenhaus and Michel Tenenhaus. Regularized generalized canonical correlation analysis. Psychometrika, 76(2):257-284, April 2011.</p><p>[6] Brian Thompson and Philipp Koehn. Exploiting sentence order in document alignment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5997-6007, Online, November 2020. Association for Computational Linguistics.</p><p>[7] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12310-12320. PMLR, 18-24 Jul 2021.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,201.15,323.18,208.94,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NeuCLIR precision-recall for Russian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,207.53,324.57,196.18,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NeuCLIR precision-recall for Farsi.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,133.77,469.57,343.71,184.78"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,133.77,469.57,343.71,184.78"><row><cell></cell><cell></cell><cell cols="2">TREC 2002 Results</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Baseline</cell><cell>MCCA(200,0.5)</cell></row><row><cell cols="3">CLIR Task Baseline MCCA(200,0.5)</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NB</cell><cell>NB</cell></row><row><cell>en/en</cell><cell>0.727</cell><cell>0.751</cell><cell>0.683</cell><cell>0.635</cell></row><row><cell>en/ru</cell><cell>0.698</cell><cell>0.726</cell><cell>0.701</cell><cell>0.668</cell></row><row><cell>en/zh</cell><cell>0.718</cell><cell>0.729</cell><cell>0.681</cell><cell>0.632</cell></row><row><cell cols="5">3.2 Document Retrieval Task on TREC Novelty Track</cell></row><row><cell>2004</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">The below table gives results for document reranking problem using the TREC</cell></row><row><cell cols="5">2004 Novelty data. These data have 50 queries (topics) with 25 to 75 documents</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,146.03,432.85,319.19,88.64"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="5,146.03,432.85,319.19,88.64"><row><cell></cell><cell></cell><cell cols="3">: TREC 2004 Document Retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Baseline</cell><cell>MCCA[PLSSVD]</cell></row><row><cell cols="3">CLIR Task Baseline MCCA[PLSSVD]</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NB</cell><cell>NB</cell></row><row><cell>en/en</cell><cell>0.783</cell><cell>0.782</cell><cell>0.810</cell><cell>0.803</cell></row><row><cell>en/ru</cell><cell>0.785</cell><cell>0.781 [0.777]</cell><cell>0.800</cell><cell>0.803 [0.791]</cell></row><row><cell>en/zh</cell><cell>0.779</cell><cell>0.774 [0.776]</cell><cell>0.809</cell><cell>0.789 [0.789]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,133.77,534.61,344.03,121.29"><head>Table 4 :</head><label>4</label><figDesc>Document Retrieval F1 scores for TREC 2004 Using UN 6-way Parallel Corpora for Training.4 Application to Passage Retrieval MS MARCO MS MARCO data has much larger query relevant document training set, approximately 800K query-passage pairs in the training data. So, using these data gives more opportunities. The translation team translated the entire English collection of MS MARCO which consists of approximately 8M passages. Translations were made available into Russian and Chinese as with the TREC Novelty track as an added bonus Farsi was included as well. Our first experiment is to hold back a random part of the training data to see if reducing the query and passage embeddings via mvlearn can improve the ability of the LaBSE vector in CLIR.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,133.77,361.97,343.71,90.57"><head>Table 5 :</head><label>5</label><figDesc>MS MARCO CLIR document retrieval precision at 1 for the held out 1% of the training data, reg=0.0.</figDesc><table coords="6,147.51,395.59,316.23,56.96"><row><cell cols="5">CLIR Task Baseline MCCA(200,0) MCCA(300,0) MCCA(400,0)</cell></row><row><cell>en/en</cell><cell>0.498</cell><cell>0.632</cell><cell>0.638</cell><cell>0.643</cell></row><row><cell>en/fa</cell><cell>0.338</cell><cell>0.456</cell><cell>0.462</cell><cell>0.462</cell></row><row><cell>en/ru</cell><cell>0.468</cell><cell>0.581</cell><cell>0.584</cell><cell>0.584</cell></row><row><cell>en/zh</cell><cell>0.389</cell><cell>0.493</cell><cell>0.496</cell><cell>0.499</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,133.77,171.18,357.72,453.24"><head>Table 6</head><label>6</label><figDesc></figDesc><table coords="7,133.77,171.18,357.72,453.24"><row><cell cols="6">: MS MARCO CLIR document retrieval precision at 1 for the held out</cell></row><row><cell cols="5">1% of the GEP training data, reg=1.0 and reg=0.0, signal ranks = dim.</cell><cell></cell></row><row><cell cols="6">CLIR Task Baseline MCCA(200,0) MCCA(300,0) MCCA(400,0)</cell></row><row><cell>en/en</cell><cell>0.498</cell><cell>0.574</cell><cell>0.580</cell><cell>0.579</cell><cell></cell></row><row><cell>en/fa</cell><cell>0.338</cell><cell>0.400</cell><cell>0.401</cell><cell>0.402</cell><cell></cell></row><row><cell>en/ru</cell><cell>0.468</cell><cell>0.506</cell><cell>0.511</cell><cell>0.513</cell><cell></cell></row><row><cell>en/zh</cell><cell>0.389</cell><cell>0.444</cell><cell>0.447</cell><cell>0.448</cell><cell></cell></row><row><cell>en/en</cell><cell>0.498</cell><cell>0.667</cell><cell>0.662</cell><cell>0.651</cell><cell></cell></row><row><cell>en/fa</cell><cell>0.338</cell><cell>0.481</cell><cell>0.475</cell><cell>0.472</cell><cell></cell></row><row><cell>en/ru</cell><cell>0.468</cell><cell>0.603</cell><cell>0.593</cell><cell>0.595</cell><cell></cell></row><row><cell>en/zh</cell><cell>0.389</cell><cell>0.516</cell><cell>0.517</cell><cell>0.513</cell><cell></cell></row><row><cell cols="6">Table 7: MS MARCO CLIR document retrieval precision at 1 for the held out</cell></row><row><cell cols="6">1% of the GEP training on varying lengths of data, reg=0.0, signal ranks =</cell></row><row><cell>dim.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">CLIR Task Train Baseline MCCA(200,0) MCCA(300,0) MCCA(400,0)</cell></row><row><cell>en/en</cell><cell>500K</cell><cell>0.498</cell><cell>0.667</cell><cell>0.662</cell><cell>0.651</cell></row><row><cell>en/en</cell><cell>50K</cell><cell>0.498</cell><cell>0.663</cell><cell>0.655</cell><cell>0.638</cell></row><row><cell>en/en</cell><cell>5K</cell><cell>0.498</cell><cell>0.623</cell><cell>0.572</cell><cell>0.505</cell></row><row><cell>en/fa</cell><cell>500K</cell><cell>0.338</cell><cell>0.481</cell><cell>0.475</cell><cell>0.472</cell></row><row><cell>en/fa</cell><cell>50K</cell><cell>0.338</cell><cell>0.476</cell><cell>0.465</cell><cell>0.456</cell></row><row><cell>en/fa</cell><cell>5K</cell><cell>0.338</cell><cell>0.439</cell><cell>0.407</cell><cell>0.362</cell></row><row><cell>en/ru</cell><cell>500K</cell><cell>0.468</cell><cell>0.603</cell><cell>0.593</cell><cell>0.595</cell></row><row><cell>en/ru</cell><cell>50K</cell><cell>0.468</cell><cell>0.599</cell><cell>0.586</cell><cell>0.581</cell></row><row><cell>en/ru</cell><cell>5K</cell><cell>0.468</cell><cell>0.562</cell><cell>0.514</cell><cell>0.463</cell></row><row><cell>en/zh</cell><cell>500K</cell><cell>0.389</cell><cell>0.516</cell><cell>0.517</cell><cell>0.513</cell></row><row><cell>en/zh</cell><cell>50K</cell><cell>0.389</cell><cell>0.518</cell><cell>0.514</cell><cell>0.500</cell></row><row><cell>en/zh</cell><cell>5K</cell><cell>0.389</cell><cell>0.482</cell><cell>0.441</cell><cell>0.384</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,139.75,279.28,347.86,344.22"><head>Table 8 :</head><label>8</label><figDesc>NeuCLIR Russian.</figDesc><table coords="9,139.75,310.96,347.86,312.54"><row><cell>RUSSIAN</cell><cell></cell><cell>Baseline</cell><cell cols="2">CCA-normalized CCA-unnormalized</cell></row><row><cell></cell><cell>topic id</cell><cell>jaccard1</cell><cell>jaccard2</cell><cell>jaccard3</cell></row><row><cell>count</cell><cell cols="2">114.000000 114.000000</cell><cell>114.000000</cell><cell>114.000000</cell></row><row><cell>mean</cell><cell>64.236842</cell><cell>0.064858</cell><cell>0.069954</cell><cell>0.066649</cell></row><row><cell>std</cell><cell>39.928543</cell><cell>0.085305</cell><cell>0.083329</cell><cell>0.080584</cell></row><row><cell>min</cell><cell>0.000000</cell><cell>0.001001</cell><cell>0.000500</cell><cell>0.000500</cell></row><row><cell>25%</cell><cell>30.250000</cell><cell>0.016260</cell><cell>0.017812</cell><cell>0.016777</cell></row><row><cell>50%</cell><cell>60.000000</cell><cell>0.035197</cell><cell>0.040583</cell><cell>0.037883</cell></row><row><cell>75%</cell><cell>99.750000</cell><cell>0.079768</cell><cell>0.087548</cell><cell>0.085924</cell></row><row><cell>max</cell><cell>136.000000</cell><cell>0.492537</cell><cell>0.512859</cell><cell>0.483680</cell></row><row><cell></cell><cell></cell><cell cols="2">Table 9: NeuCLIR Chinese.</cell><cell></cell></row><row><cell>CHINESE</cell><cell></cell><cell cols="3">BASELINE CCA-normalized CCA-unnormalized</cell></row><row><cell></cell><cell>topic id</cell><cell>jaccard1</cell><cell>jaccard2</cell><cell>jaccard3</cell></row><row><cell>count</cell><cell cols="2">114.000000 114.000000</cell><cell>114.000000</cell><cell>114.000000</cell></row><row><cell>mean</cell><cell>64.236842</cell><cell>0.067544</cell><cell>0.070705</cell><cell>0.066279</cell></row><row><cell>std</cell><cell>39.928543</cell><cell>0.079115</cell><cell>0.082964</cell><cell>0.078340</cell></row><row><cell>min</cell><cell>0.000000</cell><cell>0.000000</cell><cell>0.000000</cell><cell>0.000000</cell></row><row><cell>25%</cell><cell>30.250000</cell><cell>0.021581</cell><cell>0.020278</cell><cell>0.018849</cell></row><row><cell>50%</cell><cell>60.000000</cell><cell>0.047123</cell><cell>0.050144</cell><cell>0.047669</cell></row><row><cell>75%</cell><cell>99.750000</cell><cell>0.088139</cell><cell>0.085482</cell><cell>0.079622</cell></row><row><cell>max</cell><cell>136.000000</cell><cell>0.598721</cell><cell>0.628664</cell><cell>0.616815</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,149.01,623.71,328.47,6.99;2,133.77,633.17,281.26,6.99"><p>The authors are grateful to Paul McNamee and Liam Dugan of the Johns Hopkins Human Language Technology Center of Excellence who provided these translations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,149.01,659.34,240.34,6.99"><p>https://conferences.unite.un.org/uncorpus/en/downloadoverview</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,149.01,665.10,94.62,6.99"><p>https://mvlearn.github.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="5,149.01,666.09,347.23,6.99"><p>https://scikit-learn.org/stable/modules/generated/sklearn.cross decomposition.PLSSVD.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="6,149.01,631.13,170.98,6.99"><p>Preliminary testing seems to indicate that the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5" coords="6,322.84,631.13,154.63,6.99;6,133.77,640.60,343.71,6.99;6,133.77,650.06,343.71,6.99;6,133.77,659.53,251.30,6.99"><p>5-view models may be superior 2-view vs. 5-view: zh:0.47 vs 0.50, fa:0.44 vs 0.46, ru:0.57 vs 0.58, en:0.63 vs 0.64. Note the 2-view model is shown for dimension 150, which seemed a bit better than dimension 200. It is likely that the extra views need more dimensions, but seem to improve results.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,149.27,517.37,328.21,8.74;11,149.27,529.32,328.21,8.74;11,149.27,541.28,328.21,8.74;11,149.27,553.24,328.21,8.74;11,149.27,565.19,166.05,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,185.14,529.32,202.89,8.74">Language-agnostic BERT sentence embedding</title>
		<author>
			<persName coords=""><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,414.94,529.32,62.53,8.74;11,149.27,541.28,328.21,8.74;11,201.76,553.24,56.04,8.74">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="878" to="891" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct coords="11,149.27,585.12,328.22,8.74;11,149.27,597.07,156.61,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,262.89,585.12,210.52,8.74">Canonical analysis of several sets of variables</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Kettenring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,149.27,597.07,46.27,8.74">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1971</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,149.27,617.00,328.22,8.74;11,149.27,628.95,328.21,8.74;11,149.27,640.91,328.21,8.74;11,149.27,652.86,328.21,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,385.27,628.95,92.21,8.74;11,149.27,640.91,262.66,8.74">Transfer learning approaches for building cross-language dense retrieval models</title>
		<author>
			<persName coords=""><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawn</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<editor>Matthias Hagen, Suzan Verberne, Craig Macdonald, Christin Seifert</editor>
		<imprint>
			<publisher>Krisztian Balog</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
