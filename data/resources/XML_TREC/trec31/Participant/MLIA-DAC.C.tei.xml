<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,194.88,115.90,225.61,12.68;1,165.05,142.80,285.27,12.68">MLIA-DAC@TREC CAsT 2022: Sparse Contextualized Query Embedding</title>
				<funder ref="#_ZgnNWyb">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_WVtj5wD">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.36,186.80,53.26,8.80"><forename type="first">Le</forename><forename type="middle">Hai</forename><surname>Nam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Université</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<region>ISIR</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.18,186.80,67.67,8.80"><forename type="first">Thomas</forename><surname>Gerald</surname></persName>
							<email>thomas.gerald@lisn.upsaclay.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Paris Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">SATT</orgName>
								<orgName type="institution" key="instit4">LISN</orgName>
								<address>
									<postCode>91400</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.39,186.80,72.10,8.80"><forename type="first">Thibault</forename><surname>Formal</surname></persName>
							<email>thibault.formal@corbonne-universite.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Université</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<region>ISIR</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Naver Labs Europe</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.39,186.80,57.41,8.80"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
							<email>nie@iro.umontreal.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,436.35,186.80,41.64,8.80;1,236.88,204.74,48.92,8.80"><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
							<email>benjamin@piwarski.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Université</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<region>ISIR</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.72,204.74,58.29,8.80"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
							<email>laure.soulier@sorbonne-universite.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Université</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<region>ISIR</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,194.88,115.90,225.61,12.68;1,165.05,142.80,285.27,12.68">MLIA-DAC@TREC CAsT 2022: Sparse Contextualized Query Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46CF6335D06B6394763713DAF51AA6B0</idno>
					<note type="submission">submitted to TREC CAsT 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We extend SPLADE, a sparse information retrieval model, as our first stage ranker for the conversational task. This end-to-end approach achieves a high recall (as measure on TREC CAsT 2021). To further increase the effectiveness of our approach, we train a T5-based re-ranker. This working note fully describes our model and the four runs</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most Conversational Information Retrieval models are a two-step pipeline: Contextual Query Rewriting and ad-hoc Information Retrieval (IR). Most prior works rely on a Historical Query Expansion step <ref type="bibr" coords="1,359.33,620.19,14.61,8.80" target="#b10">[11]</ref>, i.e. a query expansion mechanism that takes into account all past queries and their associated answers.</p><p>Such query expansion model is learned on the CANARD dataset <ref type="bibr" coords="1,427.51,656.06,9.96,8.80" target="#b1">[2]</ref>, which is composed of a series of questions and their associated answers, together with a disambiguated query, referred to as gold query in this paper. However, relying on a reformulation step is computationally costly and might be sub-optimal as underlined in <ref type="bibr" coords="2,194.20,172.73,10.51,8.80" target="#b6">[7,</ref><ref type="bibr" coords="2,204.71,172.73,7.01,8.80" target="#b7">8]</ref>. Krasakis et al. <ref type="bibr" coords="2,281.89,172.73,10.51,8.80" target="#b6">[7]</ref> proposed to use ColBERT <ref type="bibr" coords="2,409.65,172.73,10.51,8.80" target="#b5">[6]</ref> in a zero-shot manner, replacing the query by the sequence of queries, without any training of the model. Lin et al. <ref type="bibr" coords="2,226.83,208.60,10.51,8.80" target="#b7">[8]</ref> proposed to learn a dense contextualized representation of the query history, optimizing a learning-to-rank loss over a dataset composed of weak labels. This makes the training process complex (labels are not reliable) and long.</p><p>We propose a much lighter training process for the first-stage ranker, where we focus on queries and do not make use of any passage -and thus of a learningto-rank training. It moreover sidesteps the problem of having to derive weak labels from the CANARD dataset. Shortly, we require that the representation of the query matches that of the disambiguated query (i.e. the gold query). We then train a second-stage ranker (i.e. re-ranker). Leveraging the fact that our first-stage ranker outputs weights over the (BERT) vocabulary, we propose a simple mechanism that provides a conversational context to the re-ranker in the form of keywords selected by SPLADE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">First stage</head><p>The original SPLADE model <ref type="bibr" coords="2,266.95,501.26,10.51,8.80" target="#b2">[3]</ref> scores a document using the dot product between the sparse representation of a document ( d) and of a query (q):</p><formula xml:id="formula_0" coords="2,279.03,549.50,201.57,11.37">s(q, d) = q • d<label>(1)</label></formula><p>The which encodes the current query in the context of the past k answers. Formally, the contextualized query representation qn,k is:</p><formula xml:id="formula_1" coords="3,253.53,291.40,227.06,10.68">qn,k = qqueries n + qanswers n,k<label>(2)</label></formula><p>where we use two versions of SPLADE parameterized by θ queries for the full query history and θ answers,k for the answers. These parameters are learned by optimizing the loss defined in Eq. ( <ref type="formula" coords="3,288.01,360.15,3.87,8.80" target="#formula_8">8</ref>).</p><p>Following <ref type="bibr" coords="3,194.50,382.09,9.96,8.80" target="#b7">[8]</ref>, we define qqueries n to be the query representation produced by encoding the concatenation of the current query and all the previous ones:</p><formula xml:id="formula_2" coords="3,153.87,432.90,326.73,10.68">qqueries n = SP LADE([CLS] q n [SEP] q 1 [SEP] . . . [SEP] q n-1 ; θ queries )<label>(3)</label></formula><p>using a set of specific parameters θ queries .</p><p>Following prior work <ref type="bibr" coords="3,241.89,487.71,9.96,8.80" target="#b0">[1]</ref>, we can consider a various number of answers k, and in particular, we choose k = 1 (the last answer). Formally, the representation</p><formula xml:id="formula_3" coords="3,135.51,523.64,34.48,10.98">qanswers n,k</formula><p>is computed as:</p><formula xml:id="formula_4" coords="3,194.93,551.29,285.66,30.55">qanswers n,k = 1 k n-1 i=n-k SP LADE(q n [SEP] a i ; θ queries,k )<label>(4)</label></formula><p>Training Based on the above, training aims at obtaining a good representation qn for the last issued query q n , i.e. to contextualize q n using the previous queries and answers. To do so, we can leverage the gold query q * n , that is, a (hopefully) contextualized and unambiguous query. We can compute the representation q * n of this query by using the original SPLADE model, i.e.</p><formula xml:id="formula_5" coords="4,242.73,149.80,237.86,12.69">q * n = SP LADE(q * n ; θ SP LADE )<label>(5)</label></formula><p>We propose a modified MSE loss, whose first component is the standard MSE loss:</p><formula xml:id="formula_6" coords="4,229.02,229.23,251.57,10.68">Loss M SE (q n,k , q * n ) = M SE(q n,k , q * n )<label>(6)</label></formula><p>In our experiments, we observed that models trained with the direct MSE do not capture well words from the context, especially for words from the answers. We thus added an asymmetric MSE, designed to encourage term expansion from past answers, but avoid introducing noise by restricting the terms to those present in the gold query q * n . Formally, our asymmetric loss is:</p><formula xml:id="formula_7" coords="4,198.18,356.84,282.41,14.58">Loss asym (q answers n,k , q * n ) = max(q * n -qanswers n,k<label>, 0) 2 (7)</label></formula><p>where the maximum is component-wise. This loss thus pushes the answer-biased The final loss we optimize is a simple linear combination of the losses defined above, and only relies on computing two query representations:</p><formula xml:id="formula_8" coords="4,175.81,540.76,304.78,12.69">Loss(q n,k , q * n ) = Loss M SE (q n,k , q * n ) + Loss asym (q answers n,k , q * n )<label>(8)</label></formula><p>Implementation details. For the first-stage, we initialize both encoders (one encoding the queries, and the other encoding the previous answer) with pre-trained weights from SPLADE model for adhoc retrieval. We use the ADAM optimizer with train batch size 16, learning rate 2e-5 for the first encoder and 3e-5 for the second. We fine-tune for only 1 epoch over the CANARD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Reranking</head><p>We perform reranking using a T5Mono <ref type="bibr" coords="5,311.86,154.36,10.51,8.80" target="#b8">[9]</ref> approach, where we enrich the raw query q n with keywords identified by the first-stage ranker. The enriched query q + n for conversational turn n is as follows:</p><p>q + n = q n . Context : q 1 q 2 . . . q n-1 . Keywords : w 1 , w 2 , ..., w K</p><p>where the w i are the top-K most important words that we select by leveraging the first-stage ranker as follows. First, to reduce noise, we only consider words that appear either in any query q i or in the associated answers a i (for i ≤ n -1).</p><p>Second, we order words by using the maximum SPLADE weight over tokens that compose the word. <ref type="foot" coords="5,237.77,326.15,3.97,6.16" target="#foot_1">6</ref> In this work, we choose K = 10.</p><p>We denote the T5 model fine-tuned for this input as T 5 + . As in the original paper <ref type="bibr" coords="5,161.66,365.48,9.96,8.80" target="#b8">[9]</ref>, the relevance score of a document d for the query q n is the probability of generating the token "true" given a prompt pt(q + n , d) = "Query: q + n . Document: d. Relevant:": score(q + n , d; θ) = p T 5 (true|pt(q + n , d); θ) p T 5 (true|pt(q + n , d); θ) + p T 5 (false|pt(q + n , d); θ) <ref type="bibr" coords="5,462.88,434.22,17.71,8.80" target="#b9">(10)</ref> where θ are the parameters of the T5Mono model.</p><p>Differently to the first stage training, we fine-tune the ranker by aligning the scores of the documents, and not the weight of a query (which is obviously not possible with the T5 model). Here the "gold" score of a document is computed using the original T5Mono with the gold query q * n . The T5 model is initialized with weights made public by the original authors<ref type="foot" coords="5,349.08,557.11,3.97,6.16" target="#foot_2">7</ref> , denoted as θ T 5 .</p><p>More precisely, we finetune the pre-trained T5Mono model using the MSE-Margin loss <ref type="bibr" coords="5,187.05,596.44,9.96,8.80" target="#b3">[4]</ref>. The loss function for the re-ranker (at conversation turn n, given documents d 1 and d 2 ) is calculated as follows:</p><formula xml:id="formula_10" coords="6,142.13,147.91,338.46,30.63">L R = s(q + n , d 1 ; θ T 5+ ) -s(q + n , d 2 ; θ T 5+ ) -(s(q * n , d 1 ; θ T 5 ) -s(q * n , d 2 ; θ T 5 )) 2<label>(11)</label></formula><p>We optimize the θ T 5+ parameters by keeping the original θ T 5 to evaluate the score of gold queries.</p><p>We also experimented with a simple MSE Loss.</p><formula xml:id="formula_11" coords="6,226.75,253.13,253.84,14.58">L R = s(q + n , d; θ T 5+ ) -s(q * n , d; θ T 5 ) 2<label>(12)</label></formula><p>Implementation details We initialize θ T 5+ as θ T 5 , and fine-tune for 3 epochs, with a batch size of 8 and a learning rate 1e-4. We sample pairs (d 1 , d 2 ) using the first-stage top-1000 documents: d 1 is sampled among the top-3, and d 2 among the remaining 997 to push the model to focus on important differences in scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>To train our model, we used the TREC CAsT 2020 and TREC CAsT 2021 dataset, and the CANARD conversational dataset.</p><p>As answers from CANARD are short (short sentences extracted from Wikipedia -contrarily to CAsT ones), we expand them to reduce the discrepancy between training and inference. For each sentence, we find the Wikipedia passage it appears in (if it exists in ORConvQA <ref type="bibr" coords="6,284.54,509.46,14.76,8.80" target="#b9">[10]</ref>), and sample a short snippet of 3 adjacent sentences from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions</head><p>We submitted 4 runs, listed below. The first one focuses on the first stageranker, to evaluate its full ranking performance, while the three next ones use a second-stage ranker for optimal performance (starting with a MSE loss up to an ensemble of re-rankers).</p><p>MLIA_DAC_splade First stage ranker only: we rank passages by using our conversational SPLADE model, i.e. after training the query representation is given by Eq. ( <ref type="formula" coords="7,197.96,154.80,4.24,8.80" target="#formula_1">2</ref>) with the asymmetric loss (Eq. 8). This allows to evaluate our main component, i.e. the first stage ranker.</p><p>splade_t5mse As MLIA_DAC_splade, but using a re-ranker (as described in section 1.2) trained with the MSE Loss (Eq. 12).</p><p>splade_t5mm As MLIA_DAC_splade, but using a re-ranker (as described in section 1.2) trained with the MSE-Margin loss (Eq. 11). This loss is supposed to increase the robustness of the re-ranker <ref type="bibr" coords="7,324.88,289.81,10.51,8.80" target="#b4">[5]</ref> (but was designed for first-stage rankers).</p><p>splade_t5mm_ens As splade_t5mm but with an ensemble of 3 T5-Reranker for the second stage (the difference in training is due to the different sampled triplets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We present our results at cut off 1000, in comparison with the organizer's baseline "BM25_T5_BART_automatic". We see that the first stage ranker performed well, approaching the rerankers' performances. The simple MSE Loss actually deteriorates performance compared to the first stage ranker, justifying the usage of MSEMargin Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,411.55,101.25,8.80;4,205.25,416.47,11.53,6.12;4,240.74,411.55,239.86,8.80;4,134.77,429.48,345.83,8.80;4,135.51,447.47,34.48,8.74;4,139.21,452.34,11.53,6.12;4,174.16,447.41,306.43,8.80;4,134.77,465.35,71.74,8.80"><head></head><label></label><figDesc>from the gold answer. Contrarily to MSE, it does not impose (directly) an upper bound on the components of the qanswers n,k representation -this is done indirectly through the final loss function described below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,155.29,499.89,301.47,79.95"><head>Table 1 .</head><label>1</label><figDesc>Results</figDesc><table coords="7,155.29,499.89,301.47,68.97"><row><cell></cell><cell>Recall MAP MRR nDCG nDCG@3</cell></row><row><cell cols="2">BM25_T5_BART_automatic 0.3244 0.1498 0.5272 0.2987 0.3619</cell></row><row><cell>MLIA_DAC_splade</cell><cell>0.6384 0.1619 0.5143 0.4327 0.3482</cell></row><row><cell>splade_t5mse</cell><cell>0.6384 0.1270 0.4101 0.3933 0.2711</cell></row><row><cell>splade_t5mm</cell><cell>0.6384 0.2018 0.5742 0.4704 0.4005</cell></row><row><cell>splade_t5mm_ens</cell><cell>0.6384 0.2193 0.5923 0.4832 0.4159</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="2,144.73,645.79,15.87,7.92;2,177.19,645.79,29.74,7.92;2,223.51,645.79,13.82,7.92;2,253.91,645.79,9.48,7.92;2,279.97,645.79,22.78,7.92;2,319.33,645.79,8.19,7.92;2,344.11,646.48,136.48,7.47;2,144.73,657.44,156.30,7.47"><p>The weights can be found at https://huggingface.co/naver/ splade-cocondenser-ensembledistil</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="5,144.73,623.87,335.87,7.92;5,144.73,634.83,234.33,7.92"><p>To improve coherence, we chose to make keywords follow their order of appearance in the context, but did not vary this experimental setting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="5,144.73,645.79,335.86,8.17;5,144.73,657.44,90.42,7.47"><p>We used the Huggingface checkpoint https://huggingface.co/castorini/ monot5-base-msmarco</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We would like to thank <rs type="institution">ANR</rs> for supporting this work under the following grants: <rs type="funder">ANR</rs> <rs type="projectName">JCJC SESAMS</rs> (<rs type="grantNumber">ANR-18-CE23-0001</rs>) and <rs type="projectName">ANR COST</rs> (<rs type="grantNumber">ANR-18-CE23-0016-003</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ZgnNWyb">
					<idno type="grant-number">ANR-18-CE23-0001</idno>
					<orgName type="project" subtype="full">JCJC SESAMS</orgName>
				</org>
				<org type="funded-project" xml:id="_WVtj5wD">
					<idno type="grant-number">ANR-18-CE23-0016-003</idno>
					<orgName type="project" subtype="full">ANR COST</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.95,247.94,337.64,7.92;8,151.52,264.38,86.93,7.92" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,285.66,247.94,194.93,7.92;8,151.52,264.38,58.26,7.92">Waterlooclarke at the trec 2020 conversational assistant track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Arabzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,280.46,337.64,7.92;8,151.52,296.90,329.08,7.92;8,151.52,313.34,329.08,7.92;8,151.52,329.78,329.07,7.92;8,151.52,346.22,329.07,8.17;8,151.52,363.35,303.12,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,331.19,280.46,149.40,7.92;8,151.52,296.90,119.86,7.92">Can You Unpack That? Learning to Rewrite Questions-in-Context</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Peskov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1605</idno>
		<ptr target="https://aclanthology.org/D19-1605" />
	</analytic>
	<monogr>
		<title level="m" coord="8,295.64,296.90,184.95,7.92;8,151.52,313.34,329.08,7.92;8,151.52,329.78,259.44,7.92">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong; China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">Nov 2019</date>
			<biblScope unit="page" from="5918" to="5924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,378.73,337.63,7.92;8,151.52,395.17,329.07,7.92;8,151.52,411.61,329.08,7.92;8,151.52,428.05,329.07,7.92;8,151.52,444.49,329.07,8.17;8,151.52,461.62,255.06,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,377.64,378.73,102.95,7.92;8,151.52,395.17,273.57,7.92">From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3531857</idno>
		<ptr target="http://doi.org/10.1145/3477495.3531857" />
	</analytic>
	<monogr>
		<title level="m" coord="8,445.38,395.17,35.21,7.92;8,151.52,411.61,329.08,7.92;8,151.52,428.05,122.03,7.92;8,345.83,428.05,134.76,7.92;8,151.52,444.49,69.58,7.92">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="page" from="2353" to="2359" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;22, Association for Computing Machinery</note>
</biblStruct>

<biblStruct coords="8,142.95,477.00,337.63,7.92;8,151.52,493.44,329.07,7.92;8,151.52,509.87,127.92,7.93" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,448.58,477.00,32.01,7.92;8,151.52,493.44,325.56,7.92">Improving efficient neural ranking models with cross-architecture knowledge distillation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Althammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sertkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<idno>ArXiv abs/2010.02666</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,525.96,337.64,7.92;8,151.52,542.40,329.07,8.17;8,151.52,559.53,122.36,7.47" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,439.37,525.96,41.23,7.92;8,151.52,542.40,324.37,8.17">Improving efficient neural ranking models with cross-architecture knowledge distillation http</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Althammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sertkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<idno>arxiv.org/abs/2010.02666</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,574.91,337.64,7.92;8,151.52,591.35,319.33,8.17" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,264.61,574.91,215.99,7.92;8,151.52,591.35,170.37,7.92">ColBERT: Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.12832" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,607.43,337.64,7.92;8,151.52,623.87,329.07,7.92;8,151.52,640.31,329.08,7.92;8,151.52,656.74,329.07,7.92;9,151.52,119.62,329.07,8.17;9,151.52,136.76,70.60,7.47" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,323.33,607.43,157.26,7.92;8,151.52,623.87,89.34,7.92">Zero-shot Query Contextualization for Conversational Search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Krasakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3531769</idno>
		<ptr target="https://doi.org/10.1145/3477495.3531769" />
	</analytic>
	<monogr>
		<title level="m" coord="8,265.66,623.87,214.94,7.92;8,151.52,640.31,280.00,7.92;8,175.32,656.74,39.34,7.92">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="page" from="1880" to="1884" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;22</note>
</biblStruct>

<biblStruct coords="9,142.95,152.50,337.64,7.92;9,151.52,168.94,173.86,8.17" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,271.46,152.50,209.13,7.92;9,151.52,168.94,24.90,7.92">Contextualized query embeddings for conversational search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2104.08707" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,185.37,337.63,7.92;9,151.52,201.81,329.07,7.92;9,151.52,218.25,329.07,7.92;9,151.52,234.69,329.08,8.17;9,151.52,251.83,221.69,7.47" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,350.33,185.37,130.26,7.92;9,151.52,201.81,143.12,7.92">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.63</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.63" />
	</analytic>
	<monogr>
		<title level="m" coord="9,317.57,201.81,163.02,7.92;9,151.52,218.25,139.53,7.92">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,267.57,337.97,7.92;9,151.52,284.00,329.07,8.17;9,151.52,301.14,184.46,7.47" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,402.47,267.57,78.11,7.92;9,151.52,284.00,122.49,7.92">Open-retrieval conversational question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401110</idno>
		<ptr target="http://arxiv.org/abs/2005.11364" />
		<imprint>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,316.88,337.98,7.92;9,151.52,333.32,329.07,8.17;9,151.52,349.76,213.03,8.17" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,380.94,316.88,99.65,7.92;9,151.52,333.32,50.55,7.92">Conversational Information Seeking</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Trippas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2201.08808</idno>
		<idno type="arXiv">arXiv:2201.08808</idno>
		<ptr target="http://arxiv.org/abs/2201.08808" />
		<imprint>
			<date type="published" when="2022-01">Jan 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
