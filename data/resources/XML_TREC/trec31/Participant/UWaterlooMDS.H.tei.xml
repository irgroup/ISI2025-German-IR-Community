<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,81.07,102.08,449.85,15.12">UWaterlooMDS at the TREC 2022 Health Misinformation Track</title>
				<funder>
					<orgName type="full">Digital Research Alliance of Canada</orgName>
				</funder>
				<funder ref="#_2Ank4BW #_c64VN4A">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.94,134.57,102.41,10.48;1,256.36,132.95,1.88,6.99"><forename type="first">Amir</forename><surname>Vakili Tahami</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Management Sciences</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.77,134.57,61.62,10.48;1,329.39,132.95,1.88,6.99"><forename type="first">Dake</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Management Sciences</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.57,134.57,90.23,10.48;1,453.80,132.95,1.88,6.99"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Management Sciences</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,81.07,102.08,449.85,15.12">UWaterlooMDS at the TREC 2022 Health Misinformation Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B4566EA1691141D52FE812DDDDFA0368</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we report our participation in the TREC 2022 Health Misinformation Track. With the aim to foster research on retrieval algorithms to promote correct information over misinformation for health-related queries, this year's track had two tasks: web retrieval and answer prediction. We reused our previous method with minor modifications to create our baselines. To overcome some limitations of our previous methods, we investigated a document-aware sentence-level passage extraction model based on the BigBird transformer. The upgraded pipeline with this model achieved our best automatic performance on the web retrieval task but failed to beat our baselines on the answer prediction task. Meanwhile, our manual runs still outperformed our automatic runs by great margins on both tasks, showing room for further improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC Health Misinformation Track <ref type="bibr" coords="1,240.32,485.06,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,255.74,485.06,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="1,268.39,485.06,7.75,8.74" target="#b3">4]</ref> aims to foster research on designing and building retrieval systems that are capable of retrieving documents that aid users in reaching the correct decisions for their health questions. Given a question, these systems must identify documents that agree with the current consensus of the medical community. And when displaying results on a search engine results page, they should suppress incorrect documents and promote correct ones to reduce the risk of users reaching harmful decisions regarding their health issues.</p><p>Different from previous years, the correct answers to topics this year were not provided by the track organizers. Participants had to determine the correct answers themselves and retrieve the correct documents based on their predicted answers. Specifically, there were two tasks this year: answer prediction and web retrieval. Another difference was that topics this year were no longer framed as "X (potential treatment) for Y (health issue)". They are health-related questions in more general forms, such as "Topic 154: Can cell phones cause cancer?" and "Topic 156: Can mosquito bites make you sick?". This shift made our previous approach <ref type="bibr" coords="1,481.33,244.16,15.50,8.74" target="#b9">[10]</ref> no longer directly applicable, where we assumed the topics to be in the format of "X (potential treatment) for Y (health issue)". Therefore, this year, we replaced the heuristic sentence selection component of our stance detection model with neural models to deal with this mismatch. Experiment results confirmed the limitations of our original pipeline and the improved generalizability of the modified pipeline, which will be covered in Section 4. We also experimented with other alternative techniques for automatic runs such as extracting passages from documents using a sentence classifier that took as input the entire document using a BigBird transformer <ref type="bibr" coords="1,433.62,399.58,9.96,8.74" target="#b8">[9]</ref>. This technique ensured the entire document context was taken into account when selecting sentences for query-dependant extracted passages. For answer prediction, we experimented with a technique that averaged the transformer outputs of the top-ranked 16 documents as a way to determine the correct answer for a given query.</p><p>We also produced manual runs for both tasks. For answer prediction, the first two authors manually used Google to find relevant documents and determined the correct answers based on those top-ranked results along with their perceived credibility. Note that the production of this run was independent of the track's organizers to avoid cheating. For web retrieval, the first two authors manually assessed passages selected by neural models from top documents ranked by neural rerankers. For each topic, the aim was to find at least ten correct (based on the answer prediction run) and useful documents from credible sources and to sort them in the order of preference.</p><p>The results show that our previous logistic regression model still worked well as an automatic answer prediction technique, achieving an AUC score of 0.864 and an accuracy of 70%. Compared with the new top-16-document aggregator, the accuracy and AUC score of our previous method were much better, which indicates that looking at more documents using a simple approach may be more effective than looking at fewer documents using a complex approach. For the web retrieval task, our previous pipeline could improve the compatibility difference score to 0.076 from Mono-T5 reranked results with a score of 0.052. Our new BigBird passage extraction paired with the Mono-T5 reranker achieved a compatibility difference score of 0.089, beating our previous pipeline. Finally, as expected, our manual runs achieved the best performance among our runs in both tasks, much better than our automatic runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods and Materials</head><p>In general, our methods were composed of several stages, including initial retrieval, passage extraction, neural reranking, stance detection, answer prediction, and final reranking based on predicted answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Health Misinformation Track</head><p>Since 2019, the TREC Health Misinformation Track has been focusing on general consumer health questions, with a small divergence in 2020 which focused on COVID-related health questions. In 2022, this track had two tasks: web retrieval and answer prediction.</p><p>â€¢ Answer Prediction: Participants were asked to figure out answers (yes or no) to the provided 50 health questions. Runs were evaluated using classic classification metrics: True Positive Rate (TPR), False Positive Rate (FPR), Accuracy, and Area Under the Curve (AUC).</p><p>â€¢ Web Retrieval: Participants were asked to retrieve useful and correct documents based on their predicted answers from the Answer Prediction task. Runs were evaluated using the Compatibility metric <ref type="bibr" coords="2,186.60,531.85,9.96,8.74" target="#b4">[5]</ref>.</p><p>Topics and qrels from the 2019 track and 2021 track were used as training examples for our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stage 1: Initial Retrieval</head><p>We used Anserini's BM25 to retrieve the initial set of relevant documents. For each topic, we retrieved the top 1000 documents using the default parameters of k 1 = 0.9 and b = 0.4. The index was built from the entire c4.noclean collection using Anserini's Index-Collection program with its default English analyzer which used Apache Lucene's (v8.0) implementations of the standard tokenizer, Porter stemmer, and some typical text cleansing techniques such as stop word filtering and lowercase conversions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stage 2: Passage Extraction and Neural Reranking</head><p>One of the key components in this pipeline was the method for passage extraction. Modern solutions to question answering tasks include the use of BERTbased transformers. However, web documents are normally much longer than the typical 512-token limit of most transformer models.</p><p>In previous years, a fairly common approach was to extract short passages and feed them into transformer models <ref type="bibr" coords="2,375.26,207.81,9.96,8.74" target="#b6">[7]</ref>. For example, a web document was first divided into sliding windows of six sentences with steps of three sentences. Each passage (window of six sentences) was then independently fed into the Mono-T5 <ref type="bibr" coords="2,327.12,255.63,10.52,8.74" target="#b5">[6]</ref> transformer model. After obtaining the relevance score for each of these passages from Mono-T5, the top-scoring passage was selected as the extracted passage to represent the document. Mono-T5 is a T5-based transformer model that was fine-tuned on the MS-MARCO passage retrieval dataset <ref type="bibr" coords="2,493.81,315.41,12.64,8.74" target="#b1">[2]</ref> for relevance reranking. Extracting passages in this manner has generally been shown to be effective in text retrieval. In practice, this method has two shortcomings. Firstly, it is computationally expensive. For a document with 20 passages, we need to run a transformer model 20 times to get an extracted passage. Secondly, relevant information to a query may be spread across a web page in nonconsecutive text spans. Only one continuous span for the extracted passage probably won't contain all relevant information from a document.</p><p>Another method was to use domain-specific heuristics to extract relevant sentences from a document. In the 2021 track, we used predefined keywords to determine whether a sentence should appear in the extraction <ref type="bibr" coords="2,358.22,508.32,14.61,8.74" target="#b9">[10]</ref>. Since queries are about health issues and treatments, certain words could be good indicators of relevance. We scored each sentence based on the frequencies of words such as "dangerous" and "effective" as well as query terms. While this approach worked well previously, its generalizability was limited.</p><p>As mentioned, web documents tend to be longer than the 512-token limit commonly seen in transformer models. And answers to questions can also come from multiple non-consecutive sentences. To obtain a more powerful passage extraction model this year, we fine-tuned the BigBird transformer model <ref type="bibr" coords="2,529.48,653.42,10.52,8.74" target="#b8">[9]</ref> on the MASH-QA dataset <ref type="bibr" coords="2,438.52,665.38,14.61,8.74" target="#b10">[11]</ref>. The MASH-QA dataset was specifically designed for this problem. It is a question answering dataset in the medical domain where answers need to be extracted from multiple non-consecutive parts within each document. Docu-ments in this dataset come from WebMD<ref type="foot" coords="3,249.52,73.58,3.97,6.12" target="#foot_0">1</ref> , and questions are from users of this site. The answers are excerpts selected by medical experts from documents on this website. Labeled relevant text spans are nonconsecutive and spread over each document. The Big-Bird <ref type="bibr" coords="3,93.89,134.93,10.52,8.74" target="#b8">[9]</ref> model has a higher token-length limit of 4096 instead of the typical 512-token limit. We split each document into sentences using spacy<ref type="foot" coords="3,236.79,157.27,3.97,6.12" target="#foot_1">2</ref> and prefixed each sentence with a special token. We then added a linear classifier on top of each special token at the final layer and used the final layer representation of that token to classify the sentence as relevant to answering the question or not. In this way, when classifying sentences, we take into account the context of the entire document rather than classifying each sentence in isolation. Classifying sentences individually without the surrounding context was found to have subpar performance for question answering and passage extraction tasks <ref type="bibr" coords="3,167.10,290.35,14.61,8.74" target="#b10">[11]</ref>. During inference, if there still were documents longer than 4096 tokens, we simply split those documents into multiple chunks.</p><p>As the last step at this stage, we reranked those extracted passages using the Mono-T5 model. The obtained ranked list of passages was utilized in the next question answering module in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Stage 3: Stance Detection and Answer Prediction</head><p>With the retrieved information from the last stage, we could build a model to get answers from those relevant passages with respect to those health-related questions. Specifically, given a relevant passage and a question (with a yes or no answer), this module needs to predict the answer indicated by the passage. We concatenated the question and the relevant passage together and fed them into the transformer. Then the final layer representation of the prepended [CLS] token was fed into a nonlinear classifier. We finetuned this model on the 2019 qrels by converting the document-level supportiveness judgments into yes or no answers. We applied this module to predict the answer for every retrieved document. Then we could figure out the correct answer to the health question by looking at answers from those top-ranked documents.</p><p>For each topic, we took the top 16 passages from Mono-T5 and prepended each passage with the question. Besides, we prepended with some auxiliary features. One auxiliary feature was a special [HON] token if the host has a HONcode certificate. HONcode was a non-profit organization that handed out certificates to websites if they followed an 8-point code of conduct that promoted principles such as transparency, attribution, confidentiality, etc. We also prepended documents with their hostnames. These two features were found to yield a slight bump to the compatibility metric on previous years' data.</p><p>Finally, we fed the 16 query-passage pairs with those two additional features to a BERT-based classifier and averaged the output (logits). If the final result was greater than zero, the answer was predicted to be "yes". Otherwise, the answer was predicted to be "no". To train this answer prediction model, we used the White and Hassan topics provided by Zhang et al. <ref type="bibr" coords="3,335.88,242.79,14.61,8.74" target="#b9">[10]</ref>, which was originally from White and Hassan <ref type="bibr" coords="3,328.65,254.74,9.96,8.74" target="#b7">[8]</ref>. We used the 2019 topics and the 2021 topics as development sets when tuning the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Stage 4: Final Reranking</head><p>At the final reranking stage, we needed to rerank documents based on their levels of agreement with our predicted answers, i.e., their correctness. From the last stage, we had a yes/no logit for each querydocument pair. We then combined it with the document's Mono-T5 relevance score using the following formula:</p><formula xml:id="formula_0" coords="3,380.63,395.13,87.35,23.25">S â€² q,d = 2S q,d 1 + e Î±â€¢L q,d â€¢ Lq</formula><p>where S q,d is the original Mono-T5 score, S â€² q,d is the new ranking score, L q,d is the the yes/no logit for the document d, Î± is a hyperparameter, and Lq is the answer prediction logit. The intuition behind this formula is that documents that are heavily in disagreement with our predicted answer will have their score suppressed near 0, thus disappearing from the top of the final ranked list. Meanwhile, documents that are in agreement will have their scores boosted up to 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Manual</head><p>In addition to the automatic methods mentioned above, we also performed manual assessments to figure out correct answers and find high-quality documents in terms of their usefulness, correctness, and credibility.</p><p>For the answer prediction task, the first two authors, which we'll refer to as assessors, independently determined their perceived answers to all those 50 topics, using search engines like Google to find credible evidence sources, such as healthline.com and webmd.com. Then, they discussed and reached an agreement on the final answers to those 50 topics.</p><p>For the web retrieval task, the assessors manually judged the passages (six sentences) selected by Mono-T5 from top documents ranked by Mono-T5, with respect to usefulness, correctness, and credibility. Correct documents here mean that their stances align with our perceived answers from above. The idea is that Mono-T5 is good at finding relevant passages from the document and assessing passages is much faster than assessing the full documents, though at the cost of potentially lower accuracy. Each of the assessors worked on 25 topics to find at least 10 most useful and correct documents for each topic.</p><p>Two passes were performed over the documents and each pass followed the order of documents ranked by Mono-T5. In the first pass, assessors primarily focused on credibility. If the source seemed credible (well-known credible or HONCode-certified 3 ), the assessors then further judged its usefulness and correctness. If the content was also useful and correct, then a very useful and correct document was found. Assessors kept finding and judging until they reached the 200th document or found 10 very useful and correct documents. If they did not find 10 very useful and correct documents after looking through the top 200 documents, they would start over and focus on the usefulness and correctness instead until they found 10 useful and correct documents (including those found in the first pass). After those two passes, all documents were then ranked in the following order: very useful and correct, useful and correct, and not judged. Documents in the same class were ranked by their scores from Mono-T5. Finally, for each topic, assessors performed a preference ordering of the top 10 documents based on their personal judgments of which the best document was, which the second best document was, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs</head><p>Some of our submitted runs reused our previous approach <ref type="bibr" coords="4,105.95,561.02,15.50,8.74" target="#b9">[10]</ref> with minor modifications, while others were new and have been described in Section 2.</p><p>For the auxiliary task: answer prediction, we produced the following runs:</p><formula xml:id="formula_1" coords="4,81.96,616.50,149.00,9.96">â€¢ WatS-AP-Baseline [automatic]:</formula><p>Basically, the same pipeline from our previous work <ref type="bibr" coords="4,116.99,641.07,15.50,8.74" target="#b9">[10]</ref> using the question field, with minor modifications in the stance detection model to adapt to the topic format shift.</p><p>â€¢ WatS-AP-Baseline-L1 [automatic]:</p><p>The difference to WatS-AP-Baseline was that 3 https://myhon.ch/en/ L1 regularization was enabled in the logistic regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ WatS-AP-MT5 [automatic]:</head><p>The difference to WatS-AP-Baseline was that the intuitive sentence selection algorithm was replaced by Mono-T5 and one additional reranking stage was added after the first retrieval using Mono-T5.</p><p>â€¢ WatS-AP-MT5-L1 [automatic]:</p><p>The difference to WatS-AP-MT5 was that L1 regularization was enabled in the logistic regression model.</p><p>â€¢ WatS-BB75-MT5-TA [automatic]:</p><p>We used passages extracted by BigBird and aggregated the top 16 passages along with their auxiliary features to predict a yes/no answer to the question.</p><p>â€¢ WatS-AP-Manual [manual]: Manual determination of the correct answers using relevant documents returned by Google.</p><p>For the core task: web retrieval, we produced the following runs:</p><formula xml:id="formula_2" coords="4,320.95,385.55,117.61,9.96">â€¢ WatS-Query [automatic]:</formula><p>The query field was used as the query for BM25 retrieval.</p><p>â€¢ WatS-Question [automatic]:</p><p>The question field was used as the query for BM25 retrieval.</p><p>â€¢ WatS-MT5-MT5 [automatic]:</p><p>We reranked WatS-Question by extracting passages from documents using spans of 6 sentences with steps of 3 sentences. These passages along with the question are scored with Mono-T5 and the documents are reranked using their bestscoring passages.</p><p>â€¢ WatS-Trust [automatic]:</p><p>The same pipeline from our previous work using the question field, with minor modifications in the stance detection model to adapt to the topic format shift. This run was based on predicted answers from WatS-AP-Baseline.</p><p>â€¢ WatS-Trust-L1 [automatic]:</p><p>The difference to WatS-Trust was that this run was based on predicted answers from WatS-AP-Baseline-L1.</p><p>â€¢ WatS-Trust-MT5 [automatic]:</p><p>The difference to WatS-Trust was that the heuristic sentence selection algorithm was replaced by Mono-T5 and one additional reranking stage was added after the first retrieval using Mono-T5. This run was based on predicted answers from WatS-AP-MT5.</p><p>â€¢ WatS-Trust-MT5-L1 [automatic]:</p><p>The difference to WatS-Trust-MT5 was that this run was based on predicted answers from WatS-AP-MT5-L1.</p><p>â€¢ WatS-Bigbird2 75-MT5 [automatic]:</p><p>For passage extraction, we used BigBird and classified each sentence as relevant to the question or not. Sentences with classification scores higher than 75% were included in the document passage. Documents with no relevant sentences were discarded. Documents were reranked based on the Mono-T5 scores of these extracted passage and question pairs.</p><p>â€¢ WatS-Bigbird2 75-MT5-TA1 [automatic]:</p><p>We reranked WatS-Bigbird2 75-MT5 using our answer prediction model with an Î± of 0.2.</p><p>â€¢ WatS-Bigbird2 75-MT5-TA2 [automatic]: Same as WatS-Bigbird2 75-MT5-TA1 except that Î± is 0.1.</p><p>â€¢ WatS-Manual [manual]: As is described in Section 2.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we first analyze our runs for the answer prediction task, which our web retrieval runs depend upon, and then analyzed runs for the web retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Answer Prediction</head><p>Table <ref type="table" coords="5,100.32,569.73,4.98,8.74" target="#tab_0">1</ref> shows the classification performance of our answer prediction runs. From the comparison between AP-Baseline and AP-MT5 and the comparison between AP-Baseline-L1 and AP-MT5-L1, we can see that replacing the heuristic sentence selection algorithm with Mono-T5 and adding an additional reranking stage after the initial retrieval indeed led to obvious improvements in predicting correct answers. This confirms our intuition that the sentence selection method lacks generalizability which may result in suboptimal performance of the stance detection model and therefore the final answer prediction performance.  BB75-MT5-TA had relatively poor accuracy and AUC, demonstrating that the top 16 documents may not be sufficient for deriving answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Web Retrieval</head><p>From Table <ref type="table" coords="5,366.04,581.12,3.87,8.74" target="#tab_1">2</ref>, we can observe that none of our automatic methods could outperform the manual run, in terms of the helpful compatibility score and the compatibility difference (main metric).</p><p>Comparing Trust and Trust-MT5-L1, it is interesting to see that those two runs were not so different in terms of the compatibility difference, even though the answer prediction run AP-MT5-L1 that Trust-MT5-L1 was based on performed way better than AP-Baseline that Trust was based on.</p><p>Our new BigBird passage extraction paired with a Mono-T5 reranker had a relatively poor helpful com-patibility score compared to using Mono-T5 for both passage extraction and reranking (0.217 v.s. 0.246). However, our reranking using BigBird and Mono-T5 produced our best automatic run, in terms of the main metric -compatibility difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>During our manual assessment of Mono-T5 selected passages from Mono-T5 reranked documents, we noticed a lot of nearly identical passages, some of which were from spam web pages that directly copied contents from other sources. We expect a better quality of retrieved documents had we also performed spam filtering on the document collection or on the returned documents from the first-stage retrieval.</p><p>Retrospectively, we did not properly use tools like Mono-T5 when producing the manual run. We should modify the query to include the answers so that the retrieval algorithm can find more documents aligned with our answers rather than documents that oppose them, which may help us find more correct documents or at least save our efforts to assess more documents further down the list of retrieved documents.</p><p>As mentioned, our BigBird aggregator failed to predict answers as well as our previous logistic regression aggregator. Especially in topics where the top results were mostly incorrect, this approach would do poorly. However, even with relatively poor quality of answer prediction (AUC: 0.691), our reranker still managed to promote correct information and reduce misinformation, improving the compatibility difference from 0.007 to 0.089. To some extent, the reranking compensated the question answering module. If we were to combine the BigBird passage extraction with the more effective logistic regression-based answer prediction, we might have an effective question answering and reranking pipeline with relatively less expensive computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>With more complex models being released in the field of NLP, we expect to see more improvements if the language models used in our methods are replaced with these larger and more powerful models. Besides, improving the ratio of correct documents retrieved with BigBird and combining it with our logistic regression-based answer prediction model can be an effective approach, while being relatively computationally efficient compared with other methods proposed in previous years' tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we described our participation in the TREC 2022 Health Misinformation Track. For the answer prediction task, our previous method worked fairly well compared with new methods proposed in this paper. We believe that simple aggregation techniques that look at more relevant documents are likely to work better than more complex aggregators looking at fewer relevant documents. For the web retrieval task, our manual run outperformed our automatic runs by a great margin, though it could be even better if we performed manual assessments in a more structured and consistent way. Our new automatic method achieved better performance than our previous approaches, showing the potentials of documentaware sentence-level passage extraction, which still needs further refinement to reach its maximum effectiveness for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,310.98,77.11,229.02,314.73"><head>Table 1 :</head><label>1</label><figDesc>Answer Prediction Task: Classification Performance. TPR: True Positive Rate, FPR: False Positive Rate, Acc: Accuracy, AUC: Area Under the Curve. Due to the space limit, the common prefix WatS-of those runs is omitted in this table.</figDesc><table coords="5,316.58,77.11,217.82,314.73"><row><cell></cell><cell cols="2">TPR FPR</cell><cell>Acc</cell><cell>AUC</cell></row><row><cell>AP-Baseline</cell><cell cols="4">0.800 0.680 0.560 0.557</cell></row><row><cell cols="5">AP-Baseline-L1 0.720 0.640 0.540 0.565</cell></row><row><cell>AP-MT5</cell><cell cols="4">0.960 0.680 0.640 0.813</cell></row><row><cell>AP-MT5-L1</cell><cell cols="4">1.000 0.600 0.700 0.864</cell></row><row><cell>BB75-MT5-TA</cell><cell cols="4">0.440 0.120 0.660 0.691</cell></row><row><cell>AP-Manual</cell><cell cols="4">0.880 0.000 0.940 0.940</cell></row><row><cell>Run</cell><cell></cell><cell cols="3">C(help) C(harm) C(â–³)</cell></row><row><cell>BM25-Query</cell><cell></cell><cell>0.171</cell><cell>0.140</cell><cell>0.031</cell></row><row><cell>BM25-Question</cell><cell></cell><cell>0.193</cell><cell>0.149</cell><cell>0.044</cell></row><row><cell>Bigbird2 75-MT5</cell><cell></cell><cell>0.217</cell><cell>0.209</cell><cell>0.007</cell></row><row><cell cols="2">Bigbird2 75-MT5-TA1</cell><cell>0.244</cell><cell>0.171</cell><cell>0.073</cell></row><row><cell cols="2">Bigbird2 75-MT5-TA2</cell><cell>0.242</cell><cell>0.153</cell><cell>0.089</cell></row><row><cell>MT5-MT5</cell><cell></cell><cell>0.246</cell><cell>0.194</cell><cell>0.052</cell></row><row><cell>Trust</cell><cell></cell><cell>0.205</cell><cell>0.142</cell><cell>0.063</cell></row><row><cell>Trust-L1</cell><cell></cell><cell>0.187</cell><cell>0.153</cell><cell>0.034</cell></row><row><cell>Trust-MT5</cell><cell></cell><cell>0.245</cell><cell>0.188</cell><cell>0.056</cell></row><row><cell>Trust-MT5-L1</cell><cell></cell><cell>0.253</cell><cell>0.177</cell><cell>0.076</cell></row><row><cell>Manual</cell><cell></cell><cell>0.284</cell><cell>0.140</cell><cell>0.145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,310.98,406.72,229.02,80.47"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="5,310.98,406.72,229.02,80.47"><row><cell>: Web Retrieval Task: Overall Performance</cell></row><row><cell>Using the Compatibility Metric [5]. C(help): helpful</cell></row><row><cell>Compatibility score, C(harm): harmful Compatibil-</cell></row><row><cell>ity score, C(â–³): difference between the helpful Com-</cell></row><row><cell>patibility score and the harmful Compatibility score.</cell></row><row><cell>Due to the space limit, the common prefix WatS-of</cell></row><row><cell>those runs is omitted in this table.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,87.24,705.63,93.15,6.64"><p>https://www.webmd.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,87.24,715.13,71.98,6.64"><p>https://spacy.io/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs> (<rs type="grantNumber">RGPIN-2020-04665</rs>, <rs type="grantNumber">RGPAS-2020-00080</rs>), and in part by the <rs type="funder">Digital Research Alliance of Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2Ank4BW">
					<idno type="grant-number">RGPIN-2020-04665</idno>
				</org>
				<org type="funding" xml:id="_c64VN4A">
					<idno type="grant-number">RGPAS-2020-00080</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,331.46,433.06,208.54,8.74;6,331.46,445.01,208.54,8.74;6,331.46,456.97,208.54,8.74;6,331.46,468.92,74.25,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,365.06,456.97,174.94,8.74;6,331.46,468.92,22.86,8.74">Overview of the TREC 2019 Decision Track</title>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Maistro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,374.66,468.92,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.46,488.15,208.54,8.74;6,331.46,500.10,208.54,8.74;6,331.46,512.06,208.55,8.74;6,331.46,524.01,208.54,8.74;6,331.46,535.97,208.54,8.74;6,331.46,547.92,208.54,8.74;6,331.46,559.88,30.44,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,447.03,524.01,92.97,8.74;6,331.46,535.97,208.54,8.74;6,331.46,547.92,53.25,8.74">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName coords=""><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,331.46,579.10,208.54,8.74;6,331.46,591.06,208.54,8.74;6,331.46,603.02,208.54,8.74;6,331.46,614.97,128.24,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,359.33,603.02,180.67,8.74;6,331.46,614.97,76.85,8.74">Overview of the TREC 2020 Health Misinformation Track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saira</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Maistro</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,428.65,614.97,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.46,634.20,208.54,8.74;6,331.46,646.15,208.54,8.74;6,331.46,658.11,199.70,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,434.59,646.15,105.41,8.74;6,331.46,658.11,148.30,8.74">Overview of the TREC 2021 Health Misinformation Track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Maistro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,500.11,658.11,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.46,677.33,208.54,8.74;6,331.46,689.29,208.54,8.74;6,331.46,701.24,208.54,8.74;6,331.46,713.20,140.53,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,437.49,689.29,102.51,8.74;6,331.46,701.24,21.26,8.74">Assessing Top-k Preferences</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Vtyurina</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,363.47,701.24,176.52,8.74;6,331.46,713.20,56.04,8.74">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>3 (2021</note>
</biblStruct>

<biblStruct coords="7,92.48,75.16,208.53,8.74;7,92.48,87.11,161.15,8.74;7,274.87,87.11,26.15,8.74;7,92.48,99.07,208.54,8.74;7,92.48,111.02,208.54,8.74;7,92.48,122.98,208.54,8.74;7,92.48,134.93,37.64,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,274.87,87.11,26.15,8.74;7,92.48,99.07,208.54,8.74;7,92.48,111.02,67.92,8.74">Document Ranking with a Pretrained Sequence-to-Sequence Model</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,181.39,111.02,119.63,8.74;7,92.48,122.98,203.91,8.74">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,154.86,208.54,8.74;7,92.48,166.81,208.54,8.74;7,92.48,178.77,208.54,8.74;7,92.48,190.72,208.54,8.74;7,92.48,202.68,208.54,8.74;7,92.48,214.64,208.54,8.74;7,92.48,226.59,160.27,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,251.56,166.81,49.46,8.74;7,92.48,178.77,208.54,8.74;7,92.48,190.72,190.85,8.74">Vera: Prediction Techniques for Reducing Harmful Misinformation in Consumer Health Search</title>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,92.48,202.68,208.54,8.74;7,92.48,214.64,208.54,8.74;7,92.48,226.59,105.20,8.74">Proceedings of the 44th International ACM SI-GIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SI-GIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2066" to="2070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,246.52,208.54,8.74;7,92.48,258.47,208.54,8.74;7,92.48,270.43,205.36,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,279.99,246.52,21.03,8.74;7,92.48,258.47,144.91,8.74">Content Bias in Online Health Search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ryen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,246.01,258.47,55.01,8.74;7,92.48,270.43,126.35,8.74">ACM Transactions on the Web (TWEB)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,290.35,208.54,8.74;7,92.48,302.31,208.54,8.74;7,92.48,314.26,208.54,8.74;7,92.48,326.22,208.54,8.74;7,92.48,338.17,22.69,8.74;7,131.17,338.17,169.85,8.74;7,92.48,350.13,208.54,8.74;7,92.48,362.08,131.12,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,131.17,338.17,169.85,8.74;7,92.48,350.13,41.64,8.74">Big Bird: Transformers for Longer Sequences</title>
		<author>
			<persName coords=""><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,152.39,350.13,148.63,8.74;7,92.48,362.08,83.26,8.74">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,382.01,208.54,8.74;7,92.48,393.96,208.54,8.74;7,92.48,405.92,208.54,8.74;7,92.48,417.87,208.54,8.74;7,92.48,429.83,208.54,8.74;7,92.48,441.78,208.53,8.74;7,92.48,453.74,195.66,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,272.62,393.96,28.39,8.74;7,92.48,405.92,208.54,8.74;7,92.48,417.87,208.54,8.74;7,92.48,429.83,26.83,8.74">Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search</title>
		<author>
			<persName coords=""><forename type="first">Dake</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Vakili Tahami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,139.87,429.83,161.15,8.74;7,92.48,441.78,208.53,8.74;7,92.48,453.74,140.59,8.74">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2099" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.48,473.66,208.54,8.74;7,92.48,485.62,208.53,8.74;7,92.48,497.57,208.54,8.74;7,92.48,509.53,208.54,8.74;7,92.48,521.48,168.34,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,262.21,485.62,38.80,8.74;7,92.48,497.57,191.78,8.74">Question Answering with Long Multiple-span Answers</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aman</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chandan</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,92.48,509.53,208.54,8.74;7,92.48,521.48,112.80,8.74">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3840" to="3849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
