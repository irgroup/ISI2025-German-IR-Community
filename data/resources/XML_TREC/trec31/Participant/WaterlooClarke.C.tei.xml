<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.58,96.74,416.84,15.12;1,286.49,118.66,39.03,15.12">WaterlooClarke at the TREC 2022 Conversational Assistant Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-31">October 31, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.76,151.14,53.73,10.48"><forename type="first">Siqing</forename><surname>Huo</surname></persName>
						</author>
						<author>
							<persName coords="1,255.50,151.14,49.09,10.48"><forename type="first">Xinyi</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName coords="1,314.33,151.14,105.90,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
						</author>
						<title level="a" type="main" coord="1,97.58,96.74,416.84,15.12;1,286.49,118.66,39.03,15.12">WaterlooClarke at the TREC 2022 Conversational Assistant Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-31">October 31, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">88ABBFBAA18CCAD93853C137341B0A78</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The WaterlooClarke group submitted three runs to TREC Conversational Assistant Track (CAsT) 2022:</p><formula xml:id="formula_0" coords="1,99.99,271.89,76.37,49.81">• UWCmanual22 • UWCauto22 • UWCcano22</formula><p>This report describes the techniques used and the results of these three runs. The pipeline to generate each run contains three main steps: 1) query reformulation, 2) passage retrieval and reranking, 3) passage summarization and reranking. For the UWCmanual22 run, we used the manual rewritten utterances provided as reformulated queries. For the UWCauto22 run, we used the automatic rewritten utterances provided as reformulated queries. For the UWCcano22 run, we augmented the automatic rewritten utterances provided with keywords extracted from the previous responses' titles, and used them as reformulated queries. We continued last year's strategy <ref type="bibr" coords="1,416.40,404.07,10.52,8.74" target="#b5">[6]</ref> of performing passage reranking with both MonoT5 and DuoT5 <ref type="bibr" coords="1,271.52,416.02,10.52,8.74" target="#b1">[2]</ref> on the pooled retrieved passages from both sparse and dense retrievers <ref type="bibr" coords="1,157.06,427.98,9.96,8.74" target="#b0">[1]</ref>. This year we focused on experimenting with the following changes: 1) Incorporating generative summarization techniques to make the produced answers more conversational. 2) Incorporating pseudo-relevance feedback into the passage retrieval stage to improve performance. In the next section, we will present the details of our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Reformulation</head><p>Similar to previous years, raw utterances are informal and often omit details that need to be inferred from the context. Therefore, query reformulation is a crucial step, in which essential information from the context is added to the raw utterances. This is the only part of the pipeline where each run uses a different approach.</p><p>The UWCmanual22 run uses provided manual rewritten utterances, where human manually expand the raw utterances with all the necessary information from the conversation history.</p><p>The UWCauto22 run uses provided automatically rewritten utterances, where a finetuned T5 model ChattyGoose <ref type="bibr" coords="1,144.16,622.68,10.52,8.74" target="#b7">[8]</ref> is used to automatically rewrite the query using conversation history. The conversation history consists of all the raw utterances and responses before the current query that is to be rewritten. Let u i denote the i th raw utterance, and r i denote the response to u i , the conversation history for</p><formula xml:id="formula_1" coords="1,85.04,646.59,441.42,21.61">u i is [u 1 , r 1 , ..., u i-1 , r i-1 ].</formula><p>The UWCcano22 run augments provided automatically rewritten utterances, by appending keywords from the previous automatically rewritten utterance, response passage, and title of the response passage. Let a i denote the automatically rewritten output of u i , a i is augmented with keywords of a i-1 , r i-1 , and the title of the raw passage corresponds to r i-1 . To extract the keywords, the RAKE algorithm <ref type="bibr" coords="1,130.45,718.32,10.52,8.74" target="#b3">[4]</ref> is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval and Reranking</head><p>Similar to last year, we used the BERT-based dense retriever ANCE <ref type="bibr" coords="2,390.50,177.93,10.52,8.74" target="#b4">[5]</ref> and a BM25-based retriever finetuned to maximize recall@1000 over previous years' TREC CAsT data. Also similar to last year, we pooled the retrieved results from both retrievers, and then applied MonoT5 and DuoT5 <ref type="bibr" coords="2,503.06,201.84,10.52,8.74" target="#b1">[2]</ref> to rerank the retrieved pool of passages.</p><p>In addition, we incorporated pseudo-relevance feedback (PRF) for the dense retriever ANCE as well. Recent study <ref type="bibr" coords="2,172.44,237.70,10.52,8.74" target="#b6">[7]</ref> demonstrates that ANCE-PRF can lead to significant accuracy improvements. We combine the original query with the top three reranked passages to obtain a new query embedding, and then used ANCE to do another pass of passage retrieval with the new query embedding. The newly retrieved passages are merged into the previous pool of passages, and another pass of passage reranking with MonoT5 and DuoT5 is performed to obtain the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Summarization and Reranking</head><p>This year we experimented with generative passage summarization using T5 <ref type="bibr" coords="2,433.12,331.80,9.96,8.74" target="#b2">[3]</ref>. Passage summarization can reduce the amount of extraneous information in the returned passages, which makes the outputs more like a real answer compared to a raw passage. We take the top twenty reranked passages from the previous step, summarize them and feed the summarized passages into MonoT5 and DuoT5 to obtain the final ranked list of responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We compared our results with the released performance benchmarks this year. The benchmark including the median and maximum score across all participating teams this year. Our performance is summarized in Table <ref type="table" coords="2,180.68,458.30,3.87,8.74" target="#tab_0">1</ref>. As expected, using the manually rewritten utterances yield the best performance, which implies more work could be done to improve the query reformulation stage. Through comparing the results for UWCauto22 and UWCcano22, it seems augmenting automatically rewritten utterances with extracted keywords help improving the performance for those originally performing badly, but also brings down the performance of some that originally performs very well. This suggests that our attempt of augmenting automatically rewritten utterances is successful in general, but sometimes the extracted keywords may introduce extraneous or irrelevant information. This suggests we can experiment with better topic detection methods to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In TREC CAsT 2022, our team further improved our passage retrieval by performing ANCE-PRF on top reranked passages. We also experimented with incorporating generative passage summarization into our pipeline. In the future, we can experiment with query-based summarization methods, finetuning the contextual query rewriting model, and augment automatically rewritten queries using better topic detection methods. We look forward to participating in TREC CAsT 2023.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,67.10,58.04,477.81,78.41"><head>Table 1 :</head><label>1</label><figDesc>TREC CAsT 2022 Preliminary Results.</figDesc><table coords="2,67.10,58.04,477.81,57.17"><row><cell></cell><cell></cell><cell cols="2">UWCmanual22</cell><cell></cell><cell></cell><cell></cell><cell>UWCauto22</cell><cell></cell><cell></cell><cell></cell><cell>UWCcano22</cell><cell></cell></row><row><cell></cell><cell></cell><cell>lenient</cell><cell></cell><cell>strict</cell><cell></cell><cell>lenient</cell><cell></cell><cell>strict</cell><cell></cell><cell>lenient</cell><cell></cell><cell>strict</cell></row><row><cell></cell><cell cols="12">map ndcg cut 20 map ndcg cut 20 map ndcg cut 20 map ndcg cut 20 map ndcg cut 20 map ndcg cut 20</cell></row><row><cell># &lt;median</cell><cell>20</cell><cell>21</cell><cell>20</cell><cell>21</cell><cell>60</cell><cell>61</cell><cell>64</cell><cell>61</cell><cell>45</cell><cell>47</cell><cell>45</cell><cell>47</cell></row><row><cell cols="2"># = median 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell># &gt;median</cell><cell>122</cell><cell>120</cell><cell>114</cell><cell>120</cell><cell>92</cell><cell>93</cell><cell>90</cell><cell>93</cell><cell>107</cell><cell>109</cell><cell>110</cell><cell>109</cell></row><row><cell># = max</cell><cell>21</cell><cell>22</cell><cell>29</cell><cell>22</cell><cell>11</cell><cell>8</cell><cell>9</cell><cell>8</cell><cell>10</cell><cell>7</cell><cell>8</cell><cell>7</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="2,105.52,699.34,421.44,8.74;2,105.52,711.29,421.44,8.74;2,105.52,723.25,282.31,9.30" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="2,374.16,699.34,152.80,8.74;2,105.52,711.29,261.56,8.74">Predicting Efficiency/Effectiveness Trade-offs for Dense vs. Sparse Retrieval Strategy Selection</title>
		<author>
			<persName coords=""><forename type="first">Negar</forename><surname>Arabzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10739</idno>
		<ptr target="https://arxiv.org/abs/2109.10739" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.52,59.85,421.45,8.74;3,105.52,71.81,421.44,8.74;3,105.52,83.76,282.31,9.30" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="3,336.74,59.85,190.22,8.74;3,105.52,71.81,262.05,8.74">The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models</title>
		<author>
			<persName coords=""><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05667</idno>
		<ptr target="https://arxiv.org/abs/2101.05667" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.52,99.70,421.44,8.74;3,105.52,111.66,421.44,9.30;3,105.52,124.33,55.07,8.30" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="3,190.05,99.70,336.91,8.74;3,105.52,111.66,27.23,8.74">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.52,139.55,421.44,8.74;3,105.52,151.51,294.74,9.30" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="3,191.71,139.55,259.22,8.74">Automatic Keyword Extraction from Individual Documents</title>
		<author>
			<persName coords=""><forename type="first">Stuart</forename><surname>Rose</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470689646.ch1</idno>
		<imprint>
			<date type="published" when="2010-03">Mar. 2010</date>
			<biblScope unit="page">9780470689646</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.52,167.45,421.44,8.74;3,105.52,179.40,421.44,9.30;3,105.52,192.07,75.99,8.30" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="3,184.71,167.45,342.25,8.74;3,105.52,179.40,38.40,8.74">Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00808</idno>
		<ptr target="https://arxiv.org/abs/2007.00808" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.52,207.30,421.45,8.74;3,105.52,219.25,436.73,9.30;3,105.52,231.93,44.61,8.30" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="3,351.24,207.30,175.72,8.74;3,105.52,219.25,114.88,8.74">WaterlooClarke at the TREC 2021 Conversational Assistant Track</title>
		<author>
			<persName coords=""><forename type="first">Xinyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Negar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Arabzadeh</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/pubs/trec30/papers/WaterlooClarke-CAsT.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.52,247.15,421.44,8.74;3,105.52,259.10,421.44,9.02;3,105.52,271.78,193.45,8.58" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="3,335.52,247.15,191.44,8.74;3,105.52,259.10,179.84,8.74">Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">Hongchien</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13454</idno>
		<ptr target="https://arxiv.org/abs/2108.13454" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,105.52,287.00,125.02,8.74;3,267.66,287.00,259.31,8.74;3,105.52,298.95,421.44,8.74;3,105.52,310.91,421.45,8.74;3,105.52,322.86,421.45,9.30;3,105.52,335.54,175.37,8.30" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="3,200.18,287.00,30.36,8.74;3,267.66,287.00,211.30,8.74">Chatty A Python Framework for Conversational Search</title>
		<author>
			<persName coords=""><forename type="first">Edwin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462782</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462782" />
	</analytic>
	<monogr>
		<title level="m" coord="3,507.86,287.00,19.11,8.74;3,105.52,298.95,421.44,8.74;3,105.52,310.91,421.45,8.74;3,105.52,322.86,11.65,8.74">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR &apos;21. Virtual Event, Canada: Association for Computing Machinery</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR &apos;21. Virtual Event, Canada: Association for Computing Machinery</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">9781450380379</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
