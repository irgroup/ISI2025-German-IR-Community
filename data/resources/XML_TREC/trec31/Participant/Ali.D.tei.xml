<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.70,97.29,404.61,14.93;1,189.17,119.21,233.66,14.93">Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,91.02,156.51,63.86,10.37"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName coords="1,163.73,156.51,78.21,10.37"><forename type="first">Yangzhao</forename><surname>Zhang</surname></persName>
							<email>zhangyanzhao.zyz@alibaba-inc.com</email>
						</author>
						<author>
							<persName coords="1,250.51,156.51,71.45,10.37"><forename type="first">Longhui</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName coords="1,330.54,156.51,67.41,10.37"><forename type="first">Dingkun</forename><surname>Long</surname></persName>
							<email>dingkun.ldk@alibaba-inc.com</email>
						</author>
						<author>
							<persName coords="1,406.59,156.51,57.37,10.37"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName coords="1,472.01,156.51,52.81,10.37"><forename type="first">Ruijie</forename><surname>Guo</surname></persName>
							<email>ruijie.guo@alibaba-inc.com</email>
						</author>
						<title level="a" type="main" coord="1,103.70,97.29,404.61,14.93;1,189.17,119.21,233.66,14.93">Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E72C09AE484B327785215AB68E2DEE03</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale text retrieval technology has been widely used in various practical business scenarios. This paper presents our systems for the TREC 2022 Deep Learning Track. We explain the hybrid text retrieval and multi-stage text ranking method adopted in our solution. The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval. In the ranking stage, in addition to the full interactionbased ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 1st and 4th rank on the test set of passage ranking and document ranking respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The text retrieval task is usually divided into two sub tasks according to the length of the text: passage retrieval and document retrieval. Whether it is a Document ranking or Passage ranking task, under the setting of full ranking, it generally needs to be processed in two stages: retrieval and ranking. Among them, the retrieval stage needs to quickly find the most relevant top-k candidates in the entire corpus set given the input query, and then a more complex and accurate ranking stage will be performed over the top-k candidates thus producing the final ranked result.</p><p>For the retrieval stage, BM25 [RZ + 09] is a bag-of-words retrieval method that ranks a set of documents based on the query terms appearing in each document and is one of the best retrieval algorithms. Recently, various improved methods [NLE19, FLPC21] based on BM25 have been proposed in the sparse retrieval research field. With the development of deep neural network models, the performance of dense retrieval models such as DPR [KOM + 20], and coCondenser <ref type="bibr" coords="1,241.42,534.39,29.61,8.64" target="#b3">[GC21,</ref><ref type="bibr" coords="1,273.75,534.39,27.12,8.64" target="#b4">GC22]</ref> have surpassed traditional methods with a large margin. For the ranking stage, since the amount of processed data is greatly reduced, a more complex model structure can be adopted. Benefit from the excellent performance of large-scale pre-trained language model (e,g. BERT <ref type="bibr" coords="1,113.30,570.26,40.01,8.64" target="#b1">[DCLT19]</ref>) on various natural language processing tasks, the ranking model is also gradually turning to the BERT model as the relevance scoring model. Specifically, the query and doc will be concatenated as the input of BERT or other pre-trained language models like RoBerta [LOG + 19], ERNIE [LLL + 22], ELEC-TRA <ref type="bibr" coords="1,107.45,606.12,41.57,8.64" target="#b0">[CLLM20]</ref>, and Deberta <ref type="bibr" coords="1,207.61,606.12,41.07,8.64" target="#b5">[HLGC20]</ref>.</p><p>Based on this multi-stage processing method, our main optimization methods in this evaluation are: 1. In the retrieval stage, a hybrid retrieval method that combines traditional sparse retrieval and dense retrieval model is adopted, and the ROM [LZXX22] model we proposed for retrieval tasks is used on the dense retrieval pre-training model.</p><p>2. In the ranking stage, multiple backbone networks are used to train the ranking models, and the re-ranking HLATR <ref type="bibr" coords="1,119.54,692.80,43.16,8.64" target="#b11">[ZLXX22]</ref> model we proposed before is used to further improve the final ensemble performance when merging these ranking models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>From the query to the final retrieval and ranking results, our method is divided into three stages: retrieval, ranking, and HLATR re-ranking. The specific process is shown in Figure <ref type="figure" coords="2,362.83,99.37,3.74,8.64" target="#fig_0">1</ref>. The difference between the Document ranking and the Passage ranking is that document is first divided into passages to participate in the retrieval and ranking, and finally the passage with the highest score is taken as the representative of the document. The following will introduce the specific practices of these three stages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval stage</head><p>Sparse retrieval BM25 has a good performance as a classic efficient scoring algorithm in the retrieval stage, but it is limited by the text matching between query and doc, and has no semantic expansion ability. Doc2query and SPLADE have extended the semantics from different perspectives, which can bring significant improvement in the sparse retrieval mode. Here we use BM25, Doc2query and SPLADE at the same time to perform weighted ensemble on the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense retrieval</head><p>The retrieval stage is limited by computational efficiency. At present, the two-tower structure is basically used. The key is how to obtain a good twin-tower text semantic representation model. As a pretraining model, BERT can make full use of unlabeled corpus and can easily do fine-tuning downstream tasks, and is widely used here. For retrieval tasks, there are also many works to optimize the model structure or training loss function. The ROM model we use is optimized for the random mask in the MLM task to a weighted mask based on term weighting, which reduces the probability of the stop words that have a weak role in retrieval being masked. Such adjustments can make the final trained model more suitable for retrieval tasks.</p><p>Hybrid retrieval Although the performance of the dense retrieval method has far exceeded that of the sparse retrieval method, we found that the results of the two methods can be further improved by weighted ensemble. It may be that the focus of the two methods is different.. The sparse retrieval method pays more attention to the text literal matching information, and the dense retrieval method pays more attention to the semantic matching information, and the two can complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking stage</head><p>Single-tower interaction model Compared with the retrieval stage, the amount of data processed in this stage is greatly reduced, and a single-tower model structure in which query and doc interact together can be used to model more accurately. The pre-training model of the BERT structure is also used here, and negative sampling is performed based on the results of the retrieval stage, and the rdrop method is used for the loss function.</p><p>Multi-model ensemble Here we tried a variety of backbone models for training, including BERT, RoBERTa, ERNIE, ELECTRA, and DeBERTa. In the last model trained under different parameter settings, ten models with better performance were selected, while keeping the diversity of the backbone as much as possible. </p><formula xml:id="formula_0" coords="3,185.92,60.51,114.20,97.59">ùë£ !,# ! $ +‚Ä¶ ùë£ !,# ! % ùë£ !,# " $ +‚Ä¶ ùë£ !,# " % ùë£ !,# # $ +‚Ä¶ ùë£ !,# # % + + + ùëí $ ùëí &amp; ùëí ' ùëùùëí ! ùëùùëí " ùëùùëí # Multi Reranker</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">HLATR re-ranking stage</head><p>When training the ranking model, we found that as the candidate set participating in the ranking stage increases, the final ranking performance will decrease. In the training stage, each query only interacts with a small number of negative samples, but in the testing stage, a larger candidate set needs to be interacted with. In addition, we made a simple weighted ensemble between the ranking model stage and the retrieval stage, and found that it can bring significant improvement. Based on these two findings, we introduce the HLATR re-ranking stage, using the Transformer structure to fuse the first two stages together. The model structure is shown in Figure <ref type="figure" coords="3,491.69,409.61,3.74,8.64">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main results</head><p>The running results we finally submitted, the Passage ranking task is in Table <ref type="table" coords="3,393.20,501.12,3.74,8.64" target="#tab_0">1</ref>, and the Document ranking task is in Table <ref type="table" coords="3,128.79,513.07,3.74,8.64" target="#tab_1">2</ref>.</p><p>The three sets of results we submitted for the Passage ranking task were produced by different model ensemble, and finally pass3 got the best results on the test set.</p><p>For the Document ranking task, the first two sets of results we submitted tried to train the model directly based on the document data, and doc3 obtained the highest score after training the model based on the passage data as the document score. It may be due to the difference in the distribution of Dev and Eval data, the results of the first two groups dropped a lot, and only doc3 finally got normal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation study</head><p>Since there are too many model combinations involved in the ranking model stage and the HLATR re-ranking stage, including a lot of parameter tuning work. Different data sampling and training result selection will have different performance, so the ablation experiments of these two stages are not listed here. In contrast, the retrieval stage is more valuable. We list the experimental results on the Passage ranking task in Table <ref type="table" coords="3,488.07,687.79,3.74,8.64" target="#tab_2">3</ref>, and the experimental results on the Document ranking task in Table <ref type="table" coords="3,325.10,699.75,3.74,8.64" target="#tab_3">4</ref>.</p><p>Whether it is Passage ranking task or Document ranking task, BM25 is a strong baseline, and the Doc2query can further strengthen the baseline. The condenser model based on the dense retrieval architecture is similar </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this TREC 2022 Deep Learning Tack evaluation, we participated in the full ranking setting of both the Passage ranking and Document ranking tasks. Through the technology of hybrid retrieval and multi-stage ranking model, the ensemble of multiple current mainstream algorithms has achieved relatively good results. But there is still a lot of room for improvement in the ranking stage and Document ranking task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,227.26,282.14,157.48,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model Architecture Diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,177.89,60.51,254.42,238.31"><head>Table 1 :</head><label>1</label><figDesc>Final results on TREC 2022 passage dataset.</figDesc><table coords="3,177.89,60.51,254.42,238.31"><row><cell cols="2">Logists Retrieve order</cell><cell></cell><cell></cell></row><row><cell>ùëë $</cell><cell></cell><cell></cell><cell>ùë† $</cell><cell>ùëë &amp;</cell></row><row><cell>ùëë &amp;</cell><cell></cell><cell>Transformer</cell><cell>ùë† &amp;</cell><cell>ùëë $</cell></row><row><cell>‚Ä¶</cell><cell>‚Ä¶</cell><cell>Blocks</cell><cell></cell></row><row><cell>ùëë (</cell><cell></cell><cell></cell><cell>ùë† (</cell><cell>ùëë (</cell></row><row><cell>Initial Rank List</cell><cell>Input Layer</cell><cell>Encoder Layer</cell><cell cols="2">Re-rankded List</cell></row><row><cell cols="3">Figure 2: HLATR Re-ranking Model.</cell><cell></cell></row><row><cell cols="4">Run NDCG@10(Dev) NDCG@10(Eval)</cell></row><row><cell>pass1</cell><cell>0.7602</cell><cell>0.7050</cell><cell></cell></row><row><cell>pass2</cell><cell>0.7568</cell><cell>0.7105</cell><cell></cell></row><row><cell>pass3</cell><cell>0.7572</cell><cell>0.7184</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,195.27,66.92,221.46,62.55"><head>Table 2 :</head><label>2</label><figDesc>Final results on TREC 2022 document dataset.</figDesc><table coords="4,213.18,79.97,185.64,49.50"><row><cell cols="3">Run NDCG@10(Dev) NDCG@10(Eval)</cell></row><row><cell>doc1</cell><cell>0.7643</cell><cell>0.4908</cell></row><row><cell>doc2</cell><cell>0.7631</cell><cell>0.4555</cell></row><row><cell>doc3</cell><cell>-</cell><cell>0.7533</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,167.40,155.61,277.21,112.43"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments result on TREC 2021 passage dataset.</figDesc><table coords="4,167.40,170.72,277.21,97.33"><row><cell>Model</cell><cell>AP</cell><cell cols="3">NDCG@10 R@1000 MRR@100</cell></row><row><cell>BM25</cell><cell>0.0977</cell><cell>0.3977</cell><cell>0.5835</cell><cell>0.5303</cell></row><row><cell>Doc2query</cell><cell>0.1358</cell><cell>0.4463</cell><cell>0.6147</cell><cell>0.5076</cell></row><row><cell>ROM</cell><cell>0.2965</cell><cell>0.6479</cell><cell>0.8251</cell><cell>0.7865</cell></row><row><cell>coCondenser</cell><cell>0.2857</cell><cell>0.6520</cell><cell>0.8095</cell><cell>0.7601</cell></row><row><cell cols="2">BERT-large-ROM 0.2919</cell><cell>0.6471</cell><cell>0.8235</cell><cell>0.7799</cell></row><row><cell>SPLADE</cell><cell>0.3329</cell><cell>0.6844</cell><cell>0.8588</cell><cell>0.8289</cell></row><row><cell>ensemble</cell><cell>0.3792</cell><cell>0.7426</cell><cell>0.8185</cell><cell>0.8841</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,85.04,294.18,441.92,196.24"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments result on TREC 2021 document dataset.to the ROM model, and SPLADE performed very well on this evaluation data set. The results of the ensemble of multiple models in the final retrieval stage are very close to the final submitted results, which shows the importance of this stage.</figDesc><table coords="4,204.82,309.29,202.35,121.24"><row><cell>Model</cell><cell>NDCG@10</cell></row><row><cell>BM25</cell><cell>0.4370</cell></row><row><cell>Doc2query</cell><cell>0.5703</cell></row><row><cell>Condenser</cell><cell>0.5778</cell></row><row><cell>ROM</cell><cell>0.6115</cell></row><row><cell>SPLADE</cell><cell>0.6202</cell></row><row><cell>coROM (trained on document)</cell><cell>0.5497</cell></row><row><cell>coCondenser (trained on document)</cell><cell>0.5871</cell></row><row><cell>coROM (trained on passage)</cell><cell>0.5815</cell></row><row><cell>ensemble</cell><cell>0.7401</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,136.18,86.42,390.78,8.64;5,136.18,98.20,377.99,8.82" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="5,479.37,86.42,47.59,8.64;5,136.18,98.38,210.82,8.64">Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,136.18,118.30,390.78,8.64;5,136.18,130.08,390.78,8.82;5,136.18,142.03,390.78,8.59;5,136.18,153.99,390.78,8.59;5,136.18,165.94,157.11,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,418.14,118.30,108.82,8.64;5,136.18,130.26,214.27,8.64">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,370.38,130.08,156.57,8.59;5,136.18,142.03,390.78,8.59;5,136.18,153.99,131.24,8.59">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s" coord="5,489.58,153.99,37.38,8.59;5,136.18,165.94,48.93,8.59">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,136.18,186.05,390.78,8.64;5,136.18,197.82,390.78,8.82;5,136.18,209.96,22.42,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,483.45,186.05,43.51,8.64;5,136.18,198.00,245.29,8.64">Splade v2: Sparse lexical and expansion model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Thibault</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">St√©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10086</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,136.18,229.70,390.78,8.82;5,136.18,241.66,390.78,8.59;5,136.18,253.62,390.78,8.82;5,136.18,265.75,22.42,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,258.34,229.88,230.08,8.64">Condenser: a pre-training architecture for dense retrieval</title>
		<author>
			<persName coords=""><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,509.15,229.70,17.81,8.59;5,136.18,241.66,390.78,8.59;5,136.18,253.62,135.85,8.59">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11-11">7-11 November, 2021. 2021</date>
			<biblScope unit="page" from="981" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,136.18,285.67,390.78,8.64;5,136.18,297.45,390.78,8.82;5,136.18,309.41,390.78,8.82;5,136.18,321.54,47.32,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,258.03,285.67,268.93,8.64;5,136.18,297.63,64.90,8.64">Unsupervised corpus aware language model pre-training for dense passage retrieval</title>
		<author>
			<persName coords=""><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,218.21,297.45,308.75,8.59;5,136.18,309.41,43.74,8.59;5,229.97,309.41,97.84,8.59">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">May 22-27, 2022. 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2843" to="2853" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2022</note>
</biblStruct>

<biblStruct coords="5,136.18,341.47,390.78,8.64;5,136.18,353.24,370.62,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,392.21,341.47,134.75,8.64;5,136.18,353.42,106.68,8.64">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName coords=""><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,260.97,353.24,216.77,8.59">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,85.04,373.35,26.26,8.64;5,111.30,371.45,6.12,6.12;5,117.92,373.35,409.04,8.64;5,136.18,385.30,390.78,8.64;5,136.18,397.08,390.79,8.82;5,136.18,409.03,285.93,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,273.27,385.30,249.34,8.64">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Barlas</forename><surname>Kom + 20] Vladimir Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sewon</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ledell</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Tau</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,147.74,397.08,375.02,8.59">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,85.04,429.14,21.58,8.64;5,106.62,427.24,6.12,6.12;5,113.23,429.14,413.73,8.64;5,136.18,441.09,390.78,8.64;5,136.18,453.05,360.02,8.64;5,85.04,472.97,23.79,8.64;5,108.83,471.08,6.12,6.12;5,115.44,472.97,411.51,8.64;5,136.18,484.93,390.78,8.64;5,136.18,496.70,202.17,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,407.49,441.09,119.47,8.64;5,136.18,453.05,331.59,8.64">Ernie-search: Bridging crossencoder with dual-encoder via self on-the-fly distillation for dense passage retrieval</title>
		<author>
			<persName coords=""><surname>Lll + 22 ; Yuxiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiding</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunsheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengjie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shikun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuaiqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haifeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wang ; Yinhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,376.10,484.93,150.86,8.64;5,136.18,496.88,34.67,8.64">A robustly optimized bert pretraining approach</title>
		<meeting><address><addrLine>Roberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2022. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>LOG + 19</note>
</biblStruct>

<biblStruct coords="5,136.18,516.81,390.78,8.64;5,136.18,528.58,370.45,8.82" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="5,400.80,516.81,126.16,8.64;5,136.18,528.76,203.76,8.64">Retrieval oriented masking pretraining language model for dense passage retrieval</title>
		<author>
			<persName coords=""><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.15133</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,136.18,548.51,390.78,8.82;5,136.18,560.46,69.65,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,354.10,548.69,132.95,8.64">From doc2query to doctttttquery</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename><surname>Epistemic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,499.84,548.51,27.12,8.59;5,136.18,560.46,30.92,8.59">Online preprint</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,136.18,580.57,390.78,8.64;5,136.18,592.34,324.65,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,318.01,580.57,208.95,8.64;5,136.18,592.52,27.15,8.64">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,171.43,592.34,201.38,8.59">Foundations and Trends¬Æ in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,136.18,612.45,390.78,8.64;5,136.18,624.23,377.34,8.82" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yanzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hlatr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10569</idno>
		<title level="m" coord="5,427.83,612.45,99.13,8.64;5,136.18,624.40,210.11,8.64">Enhance multi-stage text retrieval with hybrid list aware transformer reranking</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
