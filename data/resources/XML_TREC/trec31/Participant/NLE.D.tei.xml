<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.41,99.67,269.18,14.93;1,197.51,119.60,216.98,14.93">NAVER LABS EUROPE (SPLADE) @ TREC DEEP LEARNING 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,228.09,184.61,67.37,8.96"><forename type="first">Carlos</forename><surname>Lassance</surname></persName>
							<email>carlos.lassance@naverlabs.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Naver Labs Europe</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.53,184.61,83.87,8.96"><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
							<email>stephane.clinchant@naverlabs.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Naver Labs Europe</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.41,99.67,269.18,14.93;1,197.51,119.60,216.98,14.93">NAVER LABS EUROPE (SPLADE) @ TREC DEEP LEARNING 2022</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB4B8CA33597EAD83D160C601884C519</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation to the 2022 TREC Deep Learning challenge. We submitted runs to all four tasks, with a focus on the full retrieval passage task. The strategy is almost the same as 2021, with first stage retrieval being based around SPLADE, with some added ensembling with ColBERTv2 and DocT5. We also use the same strategy of last year for the second stage, with an ensemble of re-rankers trained using hard negatives selected by SPLADE. Initial result analysis show that the strategy is still strong, but is still unclear to us what next steps should we take.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we detail our TREC 2022 Deep Learning track submission, based on the latest improvements of the SPLADE model <ref type="bibr" coords="1,265.18,429.23,56.71,8.64" target="#b1">[Lassance and</ref><ref type="bibr" coords="1,324.38,429.23,146.55,8.64" target="#b1">Clinchant, 2022, Formal et al., 2022]</ref>. Compared to last year, there were only three changes: i) The use of Rocchio <ref type="bibr" coords="1,413.90,440.14,68.62,8.64" target="#b3">[Joachims, 1996]</ref> with SPLADE to improve first stage ranking, ii) A first stage ensemble of different SPLADE models, ColBERTv2 <ref type="bibr" coords="1,160.20,461.96,97.67,8.64" target="#b4">[Santhanam et al., 2021]</ref> and DocT5 <ref type="bibr" coords="1,308.87,461.96,100.02,8.64" target="#b4">[Nogueira and Lin, 2019]</ref>, and iii) Inclusion of a MonoT5 based reranker using T0pp. In total, we submitted more than 30 runs, most of them baselines using the models from <ref type="bibr" coords="1,224.61,483.78,56.71,8.64" target="#b1">[Lassance and</ref><ref type="bibr" coords="1,283.81,483.78,148.03,8.64" target="#b1">Clinchant, 2022, Formal et al., 2022]</ref> that are available at https://huggingface.co/naver. As per last year, we focus on passage retrieval and for the document task, we simply score a document by taking the maximum score over its passages. This year we decided to make this notebook more streamlined compared to last year. For a more thorough introduction of the models used here, we invite the reader to check the following articles: SPLADE training <ref type="bibr" coords="1,291.67,543.80,140.38,8.64" target="#b1">[Formal et al., 2022, Lassance and</ref><ref type="bibr" coords="1,434.54,543.80,65.17,8.64" target="#b1">Clinchant, 2022]</ref>, <ref type="bibr" coords="1,108.00,554.71,145.10,8.64">ColBERTv2[Santhanam et al., 2021]</ref>, DocT5 <ref type="bibr" coords="1,290.91,554.71,100.02,8.64" target="#b4">[Nogueira and Lin, 2019]</ref>, Training style we used for our rerankers <ref type="bibr" coords="1,163.22,565.62,70.00,8.64" target="#b2">[Gao et al., 2021]</ref> and T5 based reranking <ref type="bibr" coords="1,331.99,565.62,88.67,8.64" target="#b4">[Nogueira et al., 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In the following, we introduce the models we consider for both candidate generation as well as re-ranking. We also describe our training procedure, and detail the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">First Stage</head><p>For the first stage this year, we only have one addition which is the use of Rocchio on SPLADE via Anserini.</p><p>Notably: we use the following methods i) SPLADE++ EnsembleDistil <ref type="bibr" coords="1,182.81,702.61,80.23,8.64" target="#b1">[Formal et al., 2022]</ref>,</p><p>ii) SPLADE++ SelfDistil <ref type="bibr" coords="1,419.48,702.61,80.23,8.64" target="#b1">[Formal et al., 2022]</ref>, iii) ColBERTv2 <ref type="bibr" coords="1,167.98,713.51,99.24,8.64" target="#b4">[Santhanam et al., 2021]</ref>, and iv) DocT5 <ref type="bibr" coords="1,334.34,713.51,101.82,8.64" target="#b4">[Nogueira and Lin, 2019]</ref> As per last year competition we use a mix of different PLMs as rerankers for which training is inspired by<ref type="foot" coords="2,157.72,106.23,3.49,6.05" target="#foot_0">1</ref>  <ref type="bibr" coords="2,165.53,107.90,70.00,8.64" target="#b2">[Gao et al., 2021]</ref> and using negatives from SPLADE. Namely we use the following PLMs: Deberta-v2 xxlarge, Deberta-v3 large, Electra-large, T0pp, Albert-v2-xxlarge, Roberta-Large. We make the rerankers available at https://huggingface.co/naver/\{name\} with the models being named: trecdl22-crossencoder-{debertav2,debertav3,electra,albert,roberta}.</p><p>One main difference from last year is that we added to the mix a reranker based on MonoT5 <ref type="bibr" coords="2,145.84,167.93,88.67,8.64" target="#b4">[Nogueira et al., 2020]</ref>. For that reranker we start from the T0pp model which is an 11B PLM. We train it both with the MonoT5 loss and the InfoLCE from <ref type="bibr" coords="2,397.38,178.83,68.20,8.64" target="#b2">[Gao et al., 2021]</ref>. Unfortunately, this model did not worked as well as it would be expected from its size, and in hindsight would be better served to go fully on the InfoLCE loss, as demonstrated by a post-TREC paper <ref type="bibr" coords="2,132.62,211.56,82.04,8.64" target="#b5">[Zhuang et al., 2022]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ensembling</head><p>We also have applied ensembling in order to improve our results. This year we used ranx <ref type="bibr" coords="2,129.42,272.29,114.01,8.64" target="#b0">[Bassani and Romelli, 2022]</ref> to generate all our ensembles, using average normalized score over the ensembles, unless explicitly noted. The normalized score uses the min and max values of the query so that, for each model, the best score is 1 and the lowest one 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Runs submitted to TREC</head><p>We submitted a total of 32 runs, 16 for the passage task and 16 for the document task. All of the document runs are the same as the passage ones, just with the added max pooling over the passages over the same document. For the 16 runs, we have the 3 official full ranking runs, 3 official rerank runs and 10 baselines runs (5 with rocchio, 5 without).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Official full ranking runs</head><p>For our three full-ranking runs, we always use the same 6 rerankers for the second stage, while we vary the first stage by adding a new model for each new run:</p><p>• NLE SPLADE RR: First stage ensemble of naver/splade-cocondenser-ensembledistil and naver/splade-cocondenser-selfdistil, both models using Rocchio, followed by the ensemble reranking of 6 models.</p><p>• NLE SPLADE CBERT RR: Same as before, but adding Col-BERTv2 <ref type="bibr" coords="2,181.18,509.67,97.67,8.64" target="#b4">[Santhanam et al., 2021]</ref> <ref type="foot" coords="2,281.35,508.00,3.49,6.05" target="#foot_1">2</ref> to the first stage ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• NLE SPLADE CBERT DT5 RR:</head><p>Same as the previous one, but adding DocT5 <ref type="bibr" coords="2,174.02,538.89,101.82,8.64" target="#b4">[Nogueira and Lin, 2019]</ref> to the first stage ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Official rerank runs</head><p>For our three official reranking runs, we submit the ensemble of the 6 rerankers using either the average normalized score or average normalized Condorcet score and a run based on T0pp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Baseline runs</head><p>For our baseline runs, the idea was to simply run the four models from our SIGIR 2022 contributions <ref type="bibr" coords="2,129.55,654.97,136.77,8.64" target="#b1">[Formal et al., 2022, Lassance and</ref><ref type="bibr" coords="2,268.81,654.97,66.97,8.64" target="#b1">Clinchant, 2022]</ref> that have been made available in huggingface and an ensemble of the two best Splade++ models. To those 5 runs, we also add the Rocchio versions of each run, totaling 10 baselines.</p><p>In order to generate our final runs, we used as development scores the dev set of MSMARCO v1 and the last three TREC competitions. Results for our final runs are available in Table <ref type="table" coords="3,450.06,110.11,3.74,8.64" target="#tab_2">3</ref>. From it we drew some conclusions (passage track):</p><p>1. Rocchio <ref type="bibr" coords="3,181.40,143.07,68.62,8.64" target="#b3">[Joachims, 1996]</ref> improves the performance of SPLADE when labels are not sparse (i.e. outside of MSMARCO v1 dev)</p><p>2. As expected, TREC19, 20, and 21 are biased to techniques that participated in the competition, we notice that the ensemble of SPLADE models is way more competitive in 21 where it was first applied, while the effectiveness of DT5 greatly diminishes over time (as more techniques are added to the fold)</p><p>3. Moreover, we are able to "beat" the best nDCG@10 results for TREC2019 and 21, but not for 2020, while we are able to increase mAP in all years. The TREC2020 reranker that got the best results seems very impressive, especially because it gets those results reranking BM25 directly.</p><p>4. There is not so much correlation between results on the MSMARCO dev and TREC sets. For example, SPLADE EFFICIENT V performs the best out of all baselines on MS-MARCO dev but is not on the TOP5 when we consider the average of TREC results.</p><p>5. More-so, while our efficient models get very good results on MSMARCO, they struggle on TREC (something we already had seen). We are still not sure of the reason for this decrease in performance 6. Statistically significant results are hard to get on TREC. We do not report on the Table, but most first-stage models are considered "the same" when we apply statistical significance testing, maybe there would be a way of joining all years in order to get more queries and thus more significant results? The average as we use here does not seem like a good idea (see point 1)</p><p>7. Over the rerankers it seems that Electra-large took the cake as the best model we trained, while T0pp struggled on MSMARCO, but was surprisingly good on TREC21. However, adding the models on an ensemble almost always helped.</p><p>8. While the rerankers improve over the first stage models, it is slightly deceiving as they add a lot more cost for inference. However, this is something that seems to be changing (the gap is larger on TREC21)</p><p>Table <ref type="table" coords="3,132.54,488.39,3.88,8.64">1</ref>: Results for the developmental process of our runs. *:SPLADE models considered without rocchio for ensembles evaluated on MSMARCO dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reranker</head><p>Dev MSMARCO v1 TREC 2019 TREC 2020 TREC 2021 Average over TREC MRR@10 nDCG@10 mAP@1000 nDCG@10 mAP@1000 nDCG@10 mAP@100 nDCG@10 mAP As per the previous year, we mostly focused on the passage track, and thus report only results and analysis for it. Results are made available on Table <ref type="table" coords="4,319.62,109.85,4.98,8.64">4</ref> . We drew some initial conclusions and are still analyzing results:</p><p>1. The gap between the first stage and reranked runs increased compared to previous years.</p><p>2. The gap between our best run and the best possible run was stable compared to last year (0.102 difference last year, 0.106 this year), however our best run increased the gap to the "median" run<ref type="foot" coords="4,213.26,178.08,3.49,6.05" target="#foot_2">3</ref> (0.135 last year, and 0.177 this year). While the former shows that we maintained the goodness of our runs, the latter is probably a consequence of point 1 (larger gap between first stage and reranking).</p><p>3. Our three runs performed almost the same, which is expected given the results we had seen on the development set, but is kinda frustrating as improving the first stage ranking seems to plateau after reranking.</p><p>4. As it had happened on the development set, Rocchio improved the results from SPLADE.</p><p>5. There seems to be a lack of consistency on our ensembling process, making that various of our runs got "the best" result for different queries. What is more worrying is the case of query 2002533, where our first stage models got the best nDCG@10 over all submitted runs, but our rerankers got a more average score. More intelligent ensembling seems to be needed. More details on Table <ref type="table" coords="4,267.06,313.25,4.98,8.64" target="#tab_2">3</ref> 6. Over all our trained networks we only used the training set of MSMARCO v1.</p><p>7. Complementing the last point, there were not so many new things we added this year, and currently, it seems that we would do the same for next year as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>For the TREC DL 22 competition, we submitted runs based on our recent SPLADE advancements for first-stage ranking, followed by an ensemble of re-rankers trained with hard negatives selected by SPLADE. While this is a very rough draft, and we are still analyzing the results, it seems that this year there is a larger gap between the first stage runs and the reranked ones. Also, compared to the best results possible per query, our submitted runs get results that are inline with last year. Has two distinct wh propositions (when and what). It was the query with the highest distance between median nDCG@10 (0.07) and best nDCG@10 (1.0) SPLADE and BM25 are completely lost in this query (0 nDCG@10), but rerankers saved the day.</p><p>2002533 how much average cost to plan a 8' tree?</p><p>model has to understand that 8' is 8 feet. Has the worst best nDCG@10 at 0.42 and median is pretty low as well (0.07)</p><p>Differently from the previous one, here our first stage models shine (best nDCG@10 at 0.42) and the rerankers suffered (0.24) 2003157 how to cook frozen ham steak on nuwave oven not so sure why it was so hard, maybe nuwave got badly tokenized? Had the worst PREC@10 (best model got only 3 positives in the top10).</p><p>Our models perform well, very close to the best nDCG@10 and we get the best PREC@10. 2028378 when is trial by jury used also not sure why it is so hard, but had the worst best mAP (only 0.06). It has too many positives and many duplicates</p><p>Average results for our models, but the number of duplicates makes our mAP suffer</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,161.23,381.78,287.04,213.63"><head>Table 2 :</head><label>2</label><figDesc>Results for the TREC 2022 passage track.</figDesc><table coords="4,161.23,403.13,287.04,192.28"><row><cell>Method</cell><cell cols="2">ndcg@10 mAP (100)</cell></row><row><cell>Runs sent as baselines</cell><cell></cell><cell></cell></row><row><cell>SPLADE PP SDISTIL</cell><cell>57.05</cell><cell>18.46</cell></row><row><cell>SPLADE PP EDISTIL</cell><cell>57.86</cell><cell>18.01</cell></row><row><cell>SPLADE EFFICIENT V</cell><cell>55.09</cell><cell>16.31</cell></row><row><cell>SPLADE EFFICIENT VI-BT</cell><cell>52.71</cell><cell>14.52</cell></row><row><cell>SPLADE PP SDISTIL ROCCHIO</cell><cell>58.97</cell><cell>19.68</cell></row><row><cell>SPLADE PP EDISTIL ROCCHIO</cell><cell>59.17</cell><cell>19.23</cell></row><row><cell>SPLADE EFFICIENT V ROCCHIO</cell><cell>54.52</cell><cell>17.25</cell></row><row><cell>SPLADE EFFICIENT VI-BT ROCCHIO</cell><cell>50.84</cell><cell>14.52</cell></row><row><cell>SPLADE ENSEMBLE PP</cell><cell>57.89</cell><cell>18.62</cell></row><row><cell>SPLADE ENSEMBLE PP ROCCHIO</cell><cell>59.91</cell><cell>20.05</cell></row><row><cell>Runs</cell><cell></cell><cell></cell></row><row><cell>Run 1 -NLE SPLADE RR</cell><cell>70.92</cell><cell>29.77</cell></row><row><cell>Run 2 -NLE SPLADE COLBERT RR</cell><cell>71.41</cell><cell>29.63</cell></row><row><cell>Run 3 -NLE SPLADE COLBERT DT5 RR</cell><cell>71.45</cell><cell>29.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,128.90,72.31,282.60,100.44"><head>Table 3 :</head><label>3</label><figDesc>Some query examples and comments</figDesc><table coords="5,128.90,93.66,282.60,79.09"><row><cell>query id query</cell><cell>overall results</cell><cell>our results</cell></row><row><cell cols="2">2053884 when a house</cell><cell></cell></row><row><cell>goes</cell><cell>into</cell><cell></cell></row><row><cell cols="2">foreclosure</cell><cell></cell></row><row><cell cols="2">what happens</cell><cell></cell></row><row><cell cols="2">to items on</cell><cell></cell></row><row><cell cols="2">the premises</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,124.14,703.29,217.43,7.83"><p>code available at https://github.com/luyug/Reranker</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,124.14,714.52,371.87,7.47"><p>https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,124.14,714.16,347.67,7.77"><p>the run that got the median score on all queries, which is different from the "median" submission</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,112.29,397.45,391.71,8.64;5,119.62,408.18,384.39,8.82;5,119.62,419.09,150.49,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,375.70,397.45,128.31,8.64;5,119.62,408.36,43.00,8.64">ranx. fuse: A python library for metasearch</title>
		<author>
			<persName coords=""><forename type="first">Romelli ;</forename><surname>Bassani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bassani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Romelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,180.91,408.18,323.10,8.59;5,119.62,419.09,70.57,8.59">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="4808" to="4812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,434.16,396.00,8.64;5,119.62,444.89,384.38,8.82;5,119.62,455.80,110.05,8.59" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><surname>Formal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.04733</idno>
		<title level="m" coord="5,482.41,434.16,21.59,8.64;5,119.62,445.07,351.35,8.64">From distillation to hard negative sampling: Making sparse neural ir models more effective</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,108.00,470.87,396.00,8.64;5,119.62,481.78,117.10,8.64" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><surname>Gao</surname></persName>
		</author>
		<title level="m" coord="5,354.40,470.87,149.61,8.64;5,119.62,481.78,113.26,8.64">Rethink training of bert rerankers in multi-stage retrieval pipeline</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,496.67,396.00,8.64;5,119.62,507.58,384.39,8.64;5,119.62,518.49,31.81,8.64;5,108.00,533.39,396.00,8.64;5,119.62,544.12,384.39,8.82;5,119.62,555.03,246.52,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,268.63,496.67,235.38,8.64;5,119.62,507.58,87.60,8.64;5,288.23,507.58,215.77,8.64;5,119.62,518.49,27.83,8.64;5,112.43,533.39,174.52,8.64;5,408.10,533.39,95.90,8.64;5,119.62,544.30,55.53,8.64">A probabilistic analysis of the rocchio algorithm with tfidf for text categorization</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims ; Joachims</surname></persName>
			<affiliation>
				<orgName type="collaboration">. and Clinchant, S.</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,197.27,544.12,306.73,8.59;5,119.62,555.03,167.65,8.59">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996. 1996. 2022</date>
			<biblScope unit="page" from="2220" to="2226" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>An efficiency study for splade models</note>
</biblStruct>

<biblStruct coords="5,108.00,570.10,396.00,8.64;5,119.62,580.83,285.72,8.82;5,108.00,595.90,369.29,8.64;5,108.00,610.80,396.00,8.64;5,119.62,621.53,384.38,8.82;5,119.62,632.43,75.27,8.59" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,381.97,570.10,122.03,8.64;5,119.62,581.01,142.89,8.64;5,345.67,595.90,127.86,8.64;5,151.53,621.70,289.27,8.64">Colbertv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName coords=""><surname>Nogueira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06713</idno>
		<idno>arXiv:2112.01488</idno>
		<imprint>
			<date type="published" when="2019">2020. 2020. 2019. 2019. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Document ranking with a pretrained sequence-to-sequence model</note>
</biblStruct>

<biblStruct coords="5,108.00,647.51,396.00,8.64;5,119.62,658.42,347.75,8.64" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><surname>Zhuang</surname></persName>
		</author>
		<title level="m" coord="5,229.76,658.42,233.85,8.64">Rankt5: Fine-tuning t5 for text ranking with ranking losses</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
