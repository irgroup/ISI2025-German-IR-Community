<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,60.30,82.42,491.32,17.30;1,219.18,103.78,173.72,17.30">Collection Selection Based on Historical Performance for Efficient Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.90,163.90,109.87,11.48"><forename type="first">Christopher</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
							<email>fallen@arsc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center</orgName>
								<orgName type="institution">University of Alaska</orgName>
								<address>
									<settlement>Fairbanks Fairbanks, Alaska</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.74,163.90,96.48,11.48"><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
							<email>newby@arsc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center</orgName>
								<orgName type="institution">University of Alaska</orgName>
								<address>
									<settlement>Fairbanks Fairbanks, Alaska</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,60.30,82.42,491.32,17.30;1,219.18,103.78,173.72,17.30">Collection Selection Based on Historical Performance for Efficient Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DABA878B51EE872067242D3A9F6D4D18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Grid Information Retrieval (GIR) simulation was used to process the TREC Million Query Track queries. The GOV2 collection was partitioned by hostname and the aggregate performance of each host, as measured by qrel counts from the past TREC Terabyte Tracks, was used to rank the hosts in order of quality. Only the 100 highest quality hosts were included in the Grid IR simulation, representing less than 20% of all GOV2 documents. The IR performance of the GIR simulation, as measured by the topic-averaged AP, b-pref, and Rel@10 over the TREC Terabyte-Track topics is within one standard deviation of the respective topic-averaged TREC Million Query participant median scores. Estimated AP of the Million Query topic results is comparable to the topic-averaged AP of the Terabyte topic results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>One goal of the ARSC multisearch experiment for the 2007 TREC Million Query Track is to estimate a practical upper bound of the number of document collections that can be independently searched in a distributed information retrieval application, while simultaneously completing Track requirements. The specific question is: if a set of internet hosts, each providing its own local search service, are to be chosen nearly at random, how many hosts can be chosen if 10,000 queries are to be processed in about 100 hours? A secondary goal is to answer or at least reduce the scope of the question: how should those hosts be chosen so as to maximize the IR performance while simultaneously minimizing the number of hosts searched?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Grid Information Retrieval</head><p>One of the main inspirations for this work is our continued interest in distributed information retrieval systems. The authors and colleagues are involved with the Open Grid Forum's standards working group on Grid Information Retrieval. The WG is striving to develop standards for distributed search in grid computing environments <ref type="bibr" coords="1,105.18,687.71,13.68,7.87" target="#b10">[10]</ref>.</p><p>GIR spans several major themes: distributed indexing, transport methods for queries and result sets, human interface, and methods for query persistence. For TREC purposes, though, the emphasis is on result set merging. The issue is that different IR systems, with different collections or subcollections, each produce their own results for a given query in a distributed IR system. How can these different sets of results (each ordered by relevance, as produced by the independent IR systems), be effectively unified into one set? This issue was investigated for TREC 2006 <ref type="bibr" coords="1,545.10,382.91,9.40,7.87" target="#b9">[9]</ref>. Rather than ranking arbitrarily, GIR seeks to produce a single relevance-ranked set, with ordering that indicates the relative relevance of each document, regardless of which IR system it came from. In recognition of the vast number of potential collections (every Web host; every database server; etc.), the current work examines the potential for automatically selecting a sestet of collections to query. This investigation is highly relevant to GIR, where collection selection is viewed as a necessary early phase of any search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Host Quality and QREL Density</head><p>One criterion that may help to automatically decide which of many hosts to search for a particular query is past performance of each host for similar queries. The qrels from the TREC Terabyte Tracks are an available data set of past host relevance and were used to pick a subset of presumably high-quality hosts over which to process the 10,000 queries in the Million Query Track. The analysis in the remainder of this section also appears in <ref type="bibr" coords="1,524.22,706.67,10.40,7.87">[7]</ref> but is included here for completeness. Each TREC Terabyte topic consists of an identifying number, a title, a description, and a narrative that encapsulates possible search keywords, an idea of the information desired, and a criterion for determining the relevance of a document to the topic. Participants could submit several runs of ranked result sets for the annual set of 50 topics. About 100 of the top documents from each of up to two runs were added to the judgment pool. The method of pooling to estimate the performance of an IR system on a large test collection is described in <ref type="bibr" coords="2,100.14,469.07,13.68,7.87" target="#b11">[11]</ref>. Assessors assign topic,document ( ) pairs in the judgment pool to relevance scores of 0, 1, or 2 to not relevant, relevant, or highly relevant pairs. The information contained in the topic is used by the assessor to determine relevance <ref type="bibr" coords="2,262.62,504.11,9.58,7.87" target="#b5">[5]</ref>. For the purpose of discussion, a qrel or relevance judgment will be defined as the triplet topic,document,relevance score ( ) . The distribution of relevance judgments according to relevance score is summarized in Table <ref type="table" coords="2,140.22,551.87,3.39,7.87" target="#tab_0">1</ref>.</p><p>From the perspective of distributed information retrieval, the set of relevance judgments for a topic can be partitioned by the document field into disjoint subsets corresponding to Web domain names. By assuming that the set of the top documents retrieved by the monolithic IR systems used in the TREC Terabyte track is about the same set of documents that would be returned by local IR search nodes at each Web host, the number of relevance judgments binned by Web domain name and topic can be used to estimate the number of domains that would respond to a given topic. Table <ref type="table" coords="2,105.42,665.63,4.44,7.87" target="#tab_1">2</ref> lists the number of hosts that contain the set of relevance judgments partitioned by relevance score. Many hosts that return relevant documents also return non-relevant documents so the total number of hosts responding to a query is about the same as the number of hosts retrieving non-relevant documents.</p><p>Hundreds of hosts in GOV2 contain non-relevant qrels for each topic and generally only tens of hosts contain relevant qrels <ref type="bibr" coords="2,545.10,84.83,9.40,7.87">[7]</ref>.</p><p>Using a priori statistical or experience-based knowledge like term-document frequencies or past performance of similar queries to identify the Web domain names most likely to retrieve documents relevant to a particular topic before transmitting the search query to remote search nodes is one way to reduce the bandwidth used during the distributed search. However, this is not viable in practice because a priori knowledge of which hosts contain documents that are relevant to a topic is not easily available and most hosts do not contain relevant documents for any particular topic.</p><p>Over the union of all the TREC Terabyte Track topics, only 2704 hosts out of the 6353 hosts represented in the judgment pool retrieved at least one relevant or very relevant document. As with the distribution of documents among hosts described in <ref type="bibr" coords="2,530.46,239.63,9.40,7.87">[7]</ref>, the distribution of relevant or very relevant qrels among hosts in the judgment pool appears to follow a Zipfian distribution as illustrated in Figure <ref type="figure" coords="2,395.58,270.83,3.39,7.87" target="#fig_0">0</ref>. Quantitatively, 20% of the hosts in the judgment pool that retrieved a relevant document for any topic contain more than 78% of all the relevant and very relevant qrels.</p><p>And more than 80% of the hosts that contain relevant (or very relevant) qrels for any TREC topic contain 10 or fewer relevant qrels for all topics. The qualitative similarity between the distribution of topic-aggregated relevant qrels and the distribution of documents among hosts is the premise of a model of relevance, described in <ref type="bibr" coords="2,373.50,353.63,9.58,7.87">[7]</ref>, postulating that the number of relevant documents retrieved from a host that contains relevant documents for a topic is approximately, over many topics and hosts, proportional to the total number of documents contained in the host. This model may then be used to improve the efficiency of a distributed search. In the 2007 TREC Million Query Track where it was not practical to search every sub-collection for every query, only the sub-collections with the most relevant qrels in the previous TREC Terabyte Tracks were chosen to search. This decision is approximately equivalent to deciding to search only the largest GOV2 hosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT DESCRIPTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search Software</head><p>Simple index and search applications were constructed from the demo classes of the Apache Lucene 2.1 toolkit. The Lucene framework has been used by the ARSC distributed IR group in a previous TREC Terabyte Track as described in <ref type="bibr" coords="2,489.42,685.07,9.40,7.87" target="#b8">[8]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search Hardware</head><p>ARSC's Cray XD1 ("Nelchina") was used to index the GOV2 collection and process the queries. This system was used in the 2006 TREC Terabyte Track <ref type="bibr" coords="3,157.74,390.35,10.40,7.87" target="#b9">[9]</ref> with Amberfish software <ref type="bibr" coords="3,263.58,390.35,14.96,7.87" target="#b13">[13]</ref> and this is the first year that the ARSC distributed IR group has used the Lucene toolkit on the XD1 to complete a TREC Track.</p><p>Nelchina features 108 2.6Ghz Opteron processor cores with 4GB of memory each, the PBS Pro scheduler, and Cray's variation on the SuSE Linux operating system. A disk subsystem, provided by Direct Data Networks (DDN), provides 18TB of high performance disk space for temporary storage via the Lustre cluster file system. Results from informal experiments at ARSC have revealed that five to ten simultaneous indexing or searching threads on dedicated nodes can operate simultaneously before the overall aggregate performance decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Host Collection and Selection</head><p>The GOV2 collection was stored on the shared file system on the DDN disk and then partitioned into disjoint sub-collections at index-time according to hostname as described above and in <ref type="bibr" coords="3,283.50,567.23,10.40,7.87" target="#b8">[8]</ref> and <ref type="bibr" coords="3,70.62,577.55,9.58,7.87" target="#b9">[9]</ref>. The indexes were constructed using a simple Lucenebased index application and stored on the shared file system. For performance considerations, GOV2 source text was placed on a different physical disk than the target index.</p><p>Only a small fraction of the indexes were used to process all 10,000 queries. About 100 hosts or indexes could be independently searched within TREC time constraints using the software and hardware described above. The indexes chosen corresponded to the GOV2 hosts that contained the most relevant and very relevant qrels over the TREC Terabyte Tracks.</p><p>First the relevant and very relevant qrel counts were binned according to topic and containing host, so let N r (t, H ) be the total number of relevant and very relevant qrels for TREC Terabyte Track topic number t and host H , defined as the set of documents in the GOV2 collection contained in the host. Then the qrel counts were summed over the topics</p><formula xml:id="formula_0" coords="3,394.22,152.83,78.26,23.64">! N r ( H ) = N r (t, H ) t=701 850 !</formula><p>The hosts were then sorted in descending order of the topicsummed qrel counts creating a sort permutation ! so that ! N r ( H ! i ( ) ) is a monotonically decreasing function of i . Each topic was processed using the Lucene-based search application for every host in the set</p><formula xml:id="formula_1" coords="3,395.34,236.76,52.15,26.07">H ! i ( ) { } i" 1,…,100 { }</formula><p>. The cumulative proportions of relevant and non-relevant qrels</p><formula xml:id="formula_2" coords="3,377.14,285.99,121.38,26.09">! N r,nr { } H ! i ( ) ( ) i=1 m " ! N r,nr { } H ! i ( ) ( ) i=1 6353 "</formula><p>are plotted in Figure <ref type="figure" coords="3,399.42,325.79,4.44,7.87">1</ref> against the number m of (relevant-qrel sorted) hosts. Similarly, the cumulative proportion of documents contained  </p><formula xml:id="formula_3" coords="3,402.10,367.39,68.66,23.64">H ! i ( ) i=1 m " H ! i ( )<label>i=1</label></formula><formula xml:id="formula_4" coords="4,121.30,418.23,104.83,26.09">! N r H ! i ( ) ( ) i=1 m " ! N nr H ! i ( ) ( ) i=1 m "</formula><p>as a function of m . The steady downward trend following the initial noise in the function of qrel-sorted hosts may indicate that only the first 300 or so most relevant hosts, just under 5% of all the hosts represented in the TREC Terabyte Track judgment pool or 27% of all GOV2 documents, can be included in distributed searches before the number of non-relevant documents likely to be found in the remaining hosts increases faster than the number of relevant documents likely to be found. Or in other words, the judgment pool sample of the aggregate performance of many systems over many topics begins to decrease as more hosts are included in the pool, where performance is defined by the total number of relevant documents found relative to the non-relevant documents. Therefore, it may be possible to increase the efficiency of a distributed, or even monolithic, search by restricting the search to only those hosts that performed well, i.e. to those hosts that contained the most relevant documents in the TREC Terabyte Track judgment pool drawn from the top documents from each system <ref type="bibr" coords="4,433.50,126.35,9.58,7.87" target="#b4">[4]</ref>, over a sufficient number of topics without sacrificing IR performance of the search or perhaps even improving it.</p><p>Note that the results and inferences made here are based on general observations of judgment pool results sampled and contributed over many topics from a wide variety of systems and may not hold true for a single system or topic applied to a restricted domain. The number of topics in the TREC Terabyte Track is small relative to the number of likely topics a typical IR system may process. Results from the Million Query Track may help to distinguish how much of the aggregate trends can be used to improve the performance of a single system over most topics and how stable the trends are over many more topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Batch Query Processing Method</head><p>Between six to twelve search processes were started on three to six, respectively, dual processor-core nodes on Nelchina. A 48hour job wall-clock limit is enforced on Nelchina, a shared resource at the Arctic Region Supercomputing Center, so several jobs were needed to process the entire 10,000 query Million Query set. The number of nodes used for any particular job depended strongly on the number of nodes available at the start of the job. Each search process selected queries from a query queue and ran the queries against the 100-host collection described in Section 4.3, saving the results to disk. After processing the entire set of queries, the TREC-formatted results from the jobs were concatenated to the final set of results and submitted to NIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IR PERFORMANCE RESULTS</head><p>Aggregate topic-averaged IR performance measures of the results submitted during this experiment, labeled ffind07d, and those submitted by MQ participants are presented in Table <ref type="table" coords="4,517.98,470.03,3.39,7.87" target="#tab_2">3</ref>. The IR performance was calculated relative to the TREC Terabyte topics and qrels. Note that the performance of ffind07d is within one standard deviation of the MQ participant median even though nearly 81% of the GOV2 collection was discarded at search time. While it is not very surprising when commercial-quality IR software performs reasonably well at searching a fraction of the GOV2 collection already known to contain many documents relevant to the topics, it might be surprising if searching the same fraction of GOV2 over many new MQ topics yielded comparable IR performance results. A judgment pool constructed by MQ  Track organizers using the expected AP method <ref type="bibr" coords="5,248.94,74.51,10.40,7.87" target="#b3">[3]</ref> and the statistical evaluation method <ref type="bibr" coords="5,166.62,84.83,10.40,7.87" target="#b1">[1]</ref> will be used to evaluate MQ system performance. The mean average precision of ffind07d estimated by the statistical evaluation method was 0.1633 over 1083 valid MQ topics and is within one standard deviation of the MAP calculated over the 149 Terabyte Track Topics. This is at least a promising indication that a distributed GIR search application could currently be constructed with IR performance comparable to that currently obtained by monolithic search applications provided that a relatively small number of likely relevant document collections can be pre-selected at search time.</p><p>It is of additional interest to note that the estimated MAP of the system on the 1083 valid MQ topics is near the calculated MAP of the system on the 149 Terabyte topics even though the document collection searched over all topics corresponded to just 100 out of over 17,000 hosts in GOV2 that contained many relevant documents for the Terabyte topics. A stronger inference may be possible after comparing the statistical evaluation method estimated MAP here to the estimated MAP of other MQ participant systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">TOWARD A GOV2 HOST SPACE</head><p>Due to bandwidth and other IR performance constraints inherent in any loosely coupled distributed information retrieval task like multisearch, an Internet-scale Grid IR application that effectively and dynamically relevance-ranks entire collections relative to a user query will tend to provide superior performance to the user in both search quality and bandwidth cost relative to a Grid IR application that searches all available collections for every query <ref type="bibr" coords="5,54.06,384.59,13.68,7.87" target="#b12">[12]</ref>. The vector space or document configuration space model <ref type="bibr" coords="5,54.06,394.91,14.22,7.87" target="#b15">[15,</ref><ref type="bibr" coords="5,71.10,394.91,12.08,7.87" target="#b16">16]</ref> for relevance-ranking documents with respect to a query has been enormously successful, and variations of it are used to some extent in nearly every IR application available today.</p><p>Conceptually, there are many seemingly reasonable and straightforward ways to extend a vector space model for automatic indexing of documents to the automatic indexing of entire hosts or other subsets of documents such as relevanceranked result-sets for the purpose of collection ranking and resultset merging. Practical considerations ranging from the cooperativeness of the distributed document collections <ref type="bibr" coords="5,267.90,497.87,14.96,7.87" target="#b17">[17]</ref> to user expectations and requirements, however, will inevitably narrow the range of models that may be considered reasonable and possibly even preclude straightforward extensions of a typical document vector space model entirely. Vector space models for the automatic indexing and query relevance ranking of entire subcollections include the "big document" approach briefly summarized in <ref type="bibr" coords="5,110.22,570.35,14.96,7.87" target="#b17">[17]</ref> where a union of documents in a collection is used to represent the collection as a single document, but may also include host vector spaces that are constructed dynamically at query time. One possible method may be to extract cluster descriptor terms from each of the collection subsets <ref type="bibr" coords="5,251.58,611.87,10.40,7.87" target="#b6">[6]</ref> and use those to define the basis of a coarse sub-collection configuration space. A simple similarity measure based on the cardinality of the intersection of descriptor sets may allow the Cluster Hypothesis <ref type="bibr" coords="5,97.74,653.15,14.96,7.87" target="#b14">[14]</ref> to be tested on collection subsets analogous to how it has been tested on documents. Broad but shallow judgment pools as constructed in the TREC Million Query Track will help to test this hypothesis as any subset similarity measure implemented in an efficient Grid IR application will likely need to operate on collection result-sets comparable in size to the single page of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Host Space Configuration Model</head><p>Consider the set G of GOV2 documents and let H be a partition of G defined by the equivalence relation "is contained in the same web host as". An element H !H is a host or subset of G containing the documents available to the local search service of the host. Define the host space V H as a subspace of R H where H is the number of documents or elements in the document set H . Put the documents of H in correspondence with the elements of the natural or canonical basis V H of V H . Weighted sums of the documents or basis elements of V H can be associated with ranked result-sets or other arbitrary subsets of H .</p><p>Similarly, define the document space V T H as a subspace of R T H , where T H is set of stemmed and stop-filtered index terms of H and T H is the number of terms. Initially assume that the index terms are conceptually independent or orthogonal so each term can be put in correspondence with an element in the canonical or natural basis V T H of V T H . A multi-term query submitted by a user can be represented as a point in V T H by summing the associated basis terms or elements in V T H . The document space V T H as defined above is similar in concept to the classic document space configuration in <ref type="bibr" coords="5,468.06,415.79,13.68,7.87" target="#b15">[15]</ref>. When considering problems of distributed information retrieval it will be convenient to treat document subsets such as result-sets from hosts or other collections as vectors in the host space V H so that measures of similarity, akin to vector space similarity measures between documents, between result-sets from a host collection can be defined as functions of points in a vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Example: Local result-set optimization</head><p>For instance, a local search service of host H can be represented as a map s H :V T H ! V H that sends a query, say of two terms q H = êT H 1 + êT H 2 from the basis V T H to a result set of, say three documents d H = êH 1 + êH 2 + êH 3 from the basis V H . A user that wishes to search multiple hosts H ! { } ! simultaneously for a query will submit the query to a query processor or search portal that sends queries q H ! { } ! to the respective search services s H ! { } ! and then will be presented with multiple result sets d H ! { } ! that will likely be merged into a single ranked list of results. Each result set d H ! will typically contain many more non-relevant documents than relevant documents so truncating or renormalizing the result set before transmitting it to the user is often desirable for efficiency <ref type="bibr" coords="5,433.26,711.23,10.40,7.87">[7]</ref> or search quality <ref type="bibr" coords="5,517.50,711.23,9.40,7.87" target="#b2">[2]</ref>. Then finding result sets in V H ! near to but shorter than d H ! or perhaps to a set of host documents known a priori to be relevant could be accomplished with a suitable similarity measure of result sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,67.02,74.75,239.86,7.87;2,67.02,85.07,239.69,7.87;2,67.02,95.15,22.41,7.87;2,58.14,117.18,240.55,254.23"><head>Figure 0 :</head><label>0</label><figDesc>Figure 0: Zipf plot of the host frequency vs. the total number of contained relevant and very relevant qrels over all TREC topics</figDesc><graphic coords="2,58.14,117.18,240.55,254.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,441.73,367.39,10.41,5.28;3,441.95,370.82,9.64,16.12;3,317.82,405.77,240.10,9.26;3,317.82,420.46,239.85,9.20;3,317.82,432.35,239.83,7.87"><head>6353 "</head><label>6353</label><figDesc>is plotted in Figure2against m where H denotes the cardinality or size of the set or host H . Note that the 100 hosts chosen in this experiment contain 19% of the documents in the set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,61.26,78.11,205.29,7.87;3,61.26,88.19,145.53,7.87;3,61.26,100.14,240.72,243.60"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Cumulative relevant and non-relevant qrels contained vs. qrel-sorted GOV2 Hosts</figDesc><graphic coords="3,61.26,100.14,240.72,243.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,56.22,78.35,234.08,7.87;4,56.22,88.43,205.05,7.87;4,56.22,100.38,249.12,249.12"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ratio of cumulative sums of relevant to nonrelevant qrels contained vs. relevant-qrel sorted hosts</figDesc><graphic coords="4,56.22,100.38,249.12,249.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,325.02,493.79,224.28,102.67"><head>Table 1 : Total count, mean and standard deviation over 149 topics of GOV2 Terabyte Track qrels</head><label>1</label><figDesc></figDesc><table coords="1,325.02,518.99,224.28,77.47"><row><cell>Relevance</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Score</cell><cell>∑</cell><cell>µ</cell><cell>σ</cell></row><row><cell>Not Relevant</cell><cell>108,434</cell><cell>728</cell><cell>335</cell></row><row><cell>Relevant</cell><cell>22,566</cell><cell>151</cell><cell>136</cell></row><row><cell>Very Relevant</cell><cell>4351</cell><cell>29</cell><cell>49</cell></row><row><cell>Total</cell><cell>135,351</cell><cell>908</cell><cell>342</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,325.02,475.07,229.54,92.11"><head>Table 2 : Total count of Web hosts containing the Terabyte Track qrels out of the 17,186 GOV2 hosts</head><label>2</label><figDesc></figDesc><table coords="2,325.02,500.03,224.04,67.15"><row><cell>Relevance score</cell><cell>∑</cell></row><row><cell>Not relevant</cell><cell>6109</cell></row><row><cell>Relevant</cell><cell>2553</cell></row><row><cell>Very relevant</cell><cell>970</cell></row><row><cell>Any</cell><cell>6353</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,93.66,594.35,411.00,102.43"><head>Table 3 : Mean and standard deviation of AP, bpref, and REL@10 over the 149 TREC Terabyte Topics</head><label>3</label><figDesc></figDesc><table coords="4,93.66,611.15,411.00,85.63"><row><cell></cell><cell></cell><cell></cell><cell cols="2">MQ participant</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>worst</cell><cell></cell><cell cols="2">median</cell><cell>best</cell><cell></cell><cell cols="2">ffind07d</cell></row><row><cell></cell><cell>µ</cell><cell>σ</cell><cell>µ</cell><cell>σ</cell><cell>µ</cell><cell>σ</cell><cell>µ</cell><cell>σ</cell></row><row><cell>AP</cell><cell>0.0101</cell><cell>0.0302</cell><cell>0.2219</cell><cell>0.1529</cell><cell>0.3956</cell><cell>0.1787</cell><cell>0.1360</cell><cell>0.1298</cell></row><row><cell>bpref</cell><cell>0.0230</cell><cell>0.0477</cell><cell>0.2958</cell><cell>0.1845</cell><cell>0.4697</cell><cell>0.1854</cell><cell>0.2253</cell><cell>0.1760</cell></row><row><cell>REL@10</cell><cell>0.3423</cell><cell>0.9283</cell><cell>4.7718</cell><cell>2.6256</cell><cell>7.9262</cell><cell>2.4192</cell><cell>2.6510</cell><cell>2.5809</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,58.56,123.78,91.93,10.63" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,136.91,203.88,7.87;6,90.06,147.47,203.88,7.87;6,90.06,157.79,62.70,7.87" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,208.86,136.91,85.08,7.87;6,90.06,147.47,151.30,7.87">A Practical Sampling Strategy for Efficient Retrieval Evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Northeastern University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,168.11,203.86,7.87;6,90.06,178.43,203.88,7.87;6,90.06,188.75,152.22,7.87" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,193.02,168.11,100.90,7.87;6,90.06,178.43,108.23,7.87">Database merging strategy based on logistic regression</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Calvé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,206.94,178.43,87.00,7.87;6,90.06,188.75,59.90,7.87">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,199.07,203.83,7.87;6,90.06,209.39,204.31,7.87;6,90.06,219.71,203.88,7.87;6,90.06,230.03,203.82,7.87;6,90.06,240.59,119.58,7.87" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sitaraman</surname></persName>
		</author>
		<title level="m" coord="6,245.10,199.07,48.79,7.87;6,90.06,209.39,204.31,7.87;6,90.06,219.71,203.88,7.87;6,90.06,230.03,145.09,7.87">Minimal Test Collections for Retrieval Evaluation, 19th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,250.91,203.68,7.87;6,90.06,261.23,203.83,7.87;6,90.06,271.55,203.82,7.87;6,90.06,281.87,133.74,7.87" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,259.50,250.91,34.24,7.87;6,90.06,261.23,124.37,7.87">Overview of the TREC 2004 Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,221.34,261.23,72.55,7.87;6,90.06,271.55,82.24,7.87">The Thirteenth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publications</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,292.19,204.12,7.87;6,90.06,302.51,203.83,7.87;6,90.06,312.83,203.82,7.87;6,90.06,323.39,82.14,7.87" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<title level="m" coord="6,228.78,292.19,65.40,7.87;6,90.06,302.51,203.83,7.87;6,90.06,312.83,39.55,7.87">The TREC 2005 Terabyte Track, The Fourteenth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publications</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,333.71,203.82,7.87;6,90.06,344.03,153.18,7.87" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dubin</surname></persName>
		</author>
		<title level="m" coord="6,133.98,333.71,156.05,7.87">Structure in Document Browsing Spaces</title>
		<meeting><address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,354.35,203.88,7.87;6,90.06,364.67,35.86,7.87;6,225.42,364.67,68.70,7.87;6,90.06,374.99,162.30,7.87" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,206.46,354.35,87.48,7.87;6,90.06,364.67,35.86,7.87;6,225.42,364.67,37.07,7.87">Distributed Web Search Efficiency JCDL &apos;07</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
			<pubPlace>Vancouver, British Columbia, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.06,385.31,203.88,7.87;6,90.06,395.63,203.82,7.87;6,90.06,406.19,203.91,7.87;6,353.82,74.51,177.66,7.87;6,353.82,84.83,20.46,7.87" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,220.14,385.31,73.80,7.87;6,90.06,395.63,200.30,7.87">Logistic Regression Merging of Amberfish and Lucene Multisearch Results</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,90.06,406.19,172.48,7.87">The Fourteenth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publications</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,95.15,203.88,7.87;6,353.82,105.47,203.83,7.87;6,353.82,115.79,203.83,7.87;6,353.82,126.35,203.82,7.87;6,353.82,136.67,82.14,7.87" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,475.98,95.15,81.72,7.87;6,353.82,105.47,203.83,7.87;6,353.82,115.79,203.83,7.87;6,353.82,126.35,40.85,7.87">Partitioning the Gov2 Corpus by Internet Domain Name: A Result-set Merging Experiment, The Fifteenth Text REtrieval Converence</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>NIST Special Publications</publisher>
			<pubPlace>Gaithersburg, Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,146.99,203.88,7.87;6,353.82,157.31,204.07,7.87;6,353.82,167.63,140.94,7.87" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gamiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nassar</surname></persName>
		</author>
		<title level="m" coord="6,540.78,146.99,16.92,7.87;6,353.82,157.31,204.07,7.87;6,353.82,167.63,41.41,7.87">Grid Information Retrieval Requirements (GFD.27), Global Grid Forum</title>
		<meeting><address><addrLine>Lamont, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,177.95,203.85,7.87;6,353.82,188.27,203.83,7.87;6,353.82,198.59,203.82,7.87;6,353.82,209.15,114.78,7.87" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="6,476.70,177.95,80.97,7.87;6,353.82,188.27,203.83,7.87;6,353.82,198.59,34.09,7.87">Report on the need for and provision of an &quot;ideal&quot; information retrieval test collection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">V</forename><surname>Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="6,353.82,219.47,203.83,7.87;6,353.82,229.79,203.88,7.87;6,353.82,240.11,109.74,7.87" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,493.74,219.47,63.91,7.87;6,353.82,229.79,127.88,7.87">Building efficient and effective metasearch engines</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,491.10,229.79,66.60,7.87;6,353.82,240.11,27.14,7.87">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="48" to="49" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,250.43,203.82,7.87;6,353.82,260.75,203.91,7.87;6,353.82,271.07,203.82,7.87;6,353.82,281.39,20.46,7.87" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,393.66,250.43,160.13,7.87">Amberfish at the TREC 2004 Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nassar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,353.82,260.75,172.00,7.87">The Thirteenth Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publications</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,291.95,203.82,7.87;6,353.82,302.27,73.74,7.87" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="6,392.70,291.95,96.05,7.87">Automatic Text Processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,312.59,203.86,7.87;6,353.82,322.91,203.86,7.87;6,353.82,333.23,105.90,7.87" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,499.02,312.59,58.66,7.87;6,353.82,322.91,110.32,7.87">A Vector Space Model for Automatic Indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,471.18,322.91,86.50,7.87;6,353.82,333.23,16.97,7.87">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,343.55,203.88,7.87;6,353.82,353.87,203.69,7.87;6,353.82,364.19,43.26,7.87" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="6,422.94,343.55,134.76,7.87;6,353.82,353.87,82.71,7.87">Some Mathematics of Information Storage and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Sammon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Griffis Air Force Base</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,353.82,374.75,203.88,7.87;6,353.82,385.07,203.86,7.87;6,353.82,395.39,159.66,7.87" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<title level="m" coord="6,381.42,374.75,176.28,7.87;6,353.82,385.07,108.35,7.87">Federated Search of Text Search Engines in Uncooperative Environments</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Language Technology Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
