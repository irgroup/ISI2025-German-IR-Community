<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.67,83.76,398.37,15.48">Parsimonious Language Models for a Terabyte of Text</title>
				<funder ref="#_ERSeXA6 #_fpMBKyg">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder>
					<orgName type="full">Effective Focused Retrieval Techniques (EfFoRT)</orgName>
				</funder>
				<funder ref="#_UV5hAas">
					<orgName type="full">E.U</orgName>
				</funder>
				<funder ref="#_93MnZ5r">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,100.22,116.28,86.65,10.75"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Database Group</orgName>
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.39,116.28,63.92,10.75"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.42,116.28,80.71,10.75"><forename type="first">Rianne</forename><surname>Kaptein</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.65,116.28,60.11,10.75"><forename type="first">Rongmei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Database Group</orgName>
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.67,83.76,398.37,15.48">Parsimonious Language Models for a Terabyte of Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AA320F52726421D3317983B088F729B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aims of this paper are twofold. Our first aim is to compare results of the earlier Terabyte tracks to the Million Query track. We submitted a number of runs using different document representations (such as full-text, title-fields, or incoming anchor-texts) to increase pool diversity. The initial results show broad agreement in system rankings over various measures on topic sets judged at both Terabyte and Million Query tracks, with runs using the full-text index giving superior results on all measures, but also some noteworthy upsets. Our second aim is to explore the use of parsimonious language models for retrieval on terabytescale collections. These models are smaller thus more efficient than the standard language models when used at indexing time, and they may also improve retrieval performance. We have conducted initial experiments using parsimonious models in combination with pseudo-relevance feedback, for both the Terabyte and Million Query track topic sets, and obtained promising initial results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Amsterdam, in collaboration with the University of Twente, participated in the Million Query track with the twofold aim to compare results of the earlier Terabyte tracks to the Million Query track, and to explore the use of parsimonious language models for large-scale Web retrieval. When comparing results we specifically look at the impact of shallow pooling methods on the (apparent) effectiveness of retrieval techniques? And what is the impact of substantially larger numbers of topics?</p><p>We examine the use of parsimonious language models in combination with pseudo-relevance feedback. Since their introduction by Ponte and Croft <ref type="bibr" coords="1,172.37,663.00,11.62,8.64" target="#b8">[9]</ref> in 1998 statistical language models have become a major research area in information retrieval. The parsimonious language model overcomes some of the weaknesses of the standard language modeling approach where a mixture of the document model with a gen-eral collection model is used as follows:</p><formula xml:id="formula_0" coords="1,331.12,226.98,210.49,30.32">P (t 1 , ..., t n |D) = n i=1 (λP (t i |D) + (1 -λ)P (t i |C))</formula><p>Instead of blindly modeling language use in a (relevant) document, we should model what language use distinguishes a document from other documents. The exclusion of words that are common in general English, and words that occur only occasionally in documents, can improve the performance of language models and decrease the size of the models. This so-called parsimonious model was introduced by Sparck-Jones et al. <ref type="bibr" coords="1,416.25,353.16,16.59,8.64" target="#b10">[11]</ref> and practically implemented by Hiemstra et al. <ref type="bibr" coords="1,390.14,365.12,10.58,8.64" target="#b0">[1]</ref>.</p><p>The rest of this paper is organized as follows. We detail the experimental set-up for the tasks in the Terabyte track in Section 2. In Section 3, we discuss the details of the parsimonious language model. In Section 4, we discuss our official submissions and results as well as the results of our parsimonious language model experiments. Finally, we summarize our findings in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Set-up</head><p>Our basic retrieval system is based on the Lucene engine with a number of home-grown extensions <ref type="bibr" coords="1,485.20,517.75,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="1,498.48,517.75,7.19,8.64" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexes</head><p>The Million Query track uses the GOV2 test collection, containing 25,205,178 documents (426 Gb uncompressed). The indexing approach is similar to our earlier experiments in the TREC Web and Terabyte tracks <ref type="bibr" coords="1,443.84,601.79,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="1,456.83,601.79,7.47,8.64" target="#b3">4,</ref><ref type="bibr" coords="1,466.51,601.79,7.47,8.64" target="#b4">5,</ref><ref type="bibr" coords="1,476.18,601.79,7.19,8.64" target="#b5">6]</ref>. We created three separate indexes for Full-text the full textual content of the documents (covering the whole collection);</p><p>Titles the text in the title tags of each document, if present (covering 86% of the collection);</p><p>Anchors another anchor-texts index in which we unfold all relative links (covering 49% of the collection).</p><p>For the anchor text index, we normalized the URLs, and did not index repeated occurrences of the same anchor-text.</p><p>As to tokenization, we removed HTML-tags, punctuation marks, applied case-folding, and mapped marked characters into the unmarked tokens. We used the Snowball stemming algorithm <ref type="bibr" coords="2,95.22,117.05,15.27,8.64" target="#b9">[10]</ref>. The main full document text index was created as a single, non-distributed index. The size of our fulltext index is 61 Gb. Building the full-text index (including all further processing) took a massive 15 days, 6 hours, and 21 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Basic Retrieval model</head><p>For our ranking, we use either a vector-space retrieval model or a language model. Our vector space model is the default similarity measure in Lucene <ref type="bibr" coords="2,178.04,241.08,10.58,8.64" target="#b6">[7]</ref>, i.e., for a collection C, document D and query q:</p><formula xml:id="formula_1" coords="2,66.38,276.02,213.94,40.75">sim(q, D) = t∈q tf t,q • idf t norm q • tf t,D • idf t norm D • coord q,D • weight t ,</formula><p>where tf t,X = freq(t, X)</p><formula xml:id="formula_2" coords="2,104.07,369.68,137.36,103.43">idf t = 1 + log |C| freq(t, C) norm q = t∈q tf t,q • idf t 2 norm D = |D| coord q,D = |q ∩ D| |q|</formula><p>Our language model is an extension to Lucene <ref type="bibr" coords="2,240.77,487.32,10.58,8.64" target="#b1">[2]</ref>, i.e., for a collection C, document D and query q:</p><formula xml:id="formula_3" coords="2,67.56,524.91,211.58,19.61">P (D|q) = P (D) • t∈q (λP (t|D) + (1 -λ)P (t|C)) ,</formula><p>where</p><formula xml:id="formula_4" coords="2,97.21,579.24,151.08,82.78">P (t|D) = tf t,D |D| P (t|C) = doc freq(t, C) t ∈C doc freq(t , C) P (D) = |D| D ∈C |D |</formula><p>The standard value for the smoothing parameter λ is 0.15.</p><p>In previous years of the TREC Terabyte track, we found out that the GOV2 collection requires substantially less smoothing <ref type="bibr" coords="2,69.02,710.82,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="2,82.30,710.82,7.19,8.64" target="#b3">4]</ref>. That is, we use a value of λ close to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parsimonious Retrieval Model</head><p>Besides the official runs using the basic retrieval models, we also do a range of experiments with parsimonious language models. For efficiency reasons, we rerank in this case the top 1,000 results as produced by the official Lucene language model run UAmsT07MTeLM (using a full-text index, the Snowball stemming algorithm, standard multinomial language model with Jelinek-Mercer smoothing, λ = 0.9)</p><p>We compare a standard language model run using maximum likelihood estimation with a parsimonious retrieval model. A description of the parsimonious model follows below in a separate section. Pseudo-relevance feedback will be applied to both models. When a standard language model is used, we remove stopwords according to a standard stopwords list. To further improve performance we also apply Porter stemming in some of the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Parsimonious Language Model</head><p>The parsimonious model concentrates the probability mass on fewer terms than a standard language model. Only terms that occur relatively more frequent in the document as in the whole collection will be assigned a non-zero probability making the parsimonious language model smaller than a standard language model. The model automatically removes stopwords, and words that are mentioned occasionally in the document <ref type="bibr" coords="2,358.60,382.43,10.58,8.64" target="#b0">[1]</ref>.</p><p>The model is estimated using Expectation-Maximization: In the M-step the terms that receive a probability below a certain threshold or pruning factor are removed from the model. In the next iteration the probabilities of the remaining terms are again normalized. The iteration process stops after a fixed number of iterations or when the probability distribution does not change significantly anymore. From a selection of health care pages from the GOV2 corpus, we built a standard and a parsimonious language model. In Table <ref type="table" coords="2,353.30,567.36,4.98,8.64" target="#tab_0">1</ref> the top ranked terms of both models are shown. The standard language model still contains some words that should be considered as stopwords, like 'shall'. When a standard stopword list is used there is always a trade-off between being complete and being too aggressive. When the parsimonious model is used, the document is compared to the background corpus to remove all words that do not occur relatively more frequently in the document than in the background corpus. In this way not only all standard stopwords are removed, but also the corpus specific stopwords. For example, in the GOV2 corpus the word 'information' can be considered a stopword because it occurs in almost half of all documents. Another advantage of the parsimonious model is that the probabilities are normalized. Frequently occurring words, such as stopwords, are removed from the model, and then the probabilities are redistributed over the remaining terms.</p><p>Pseudo-Relevance Feedback Since the parsimonious model concentrates on the most differentiating terms in a document, this model is a good candidate to use for pseudorelevance feedback.</p><p>There are several possibilities for integrating pseudorelevance feedback into the language model approach. We will use maximum likelihood ratios as described by Ng <ref type="bibr" coords="3,278.79,452.99,10.58,8.64" target="#b7">[8]</ref>. We calculate scores separately for the query and the relevance feedback likelihood ratio, and combine the scores at the end. The query and the feedback likelihood ratios differ substantially in length, therefore normalized log likelihood ratios should be used. For the query likelihood ratio, we then get:</p><formula xml:id="formula_5" coords="3,60.41,550.54,217.35,57.55">N LLR query = log P (Q|D, r) P (Q|D, r) = n i=1 P (q i |Q) • log λP (q i |D) + (1 -λ)P (q i |C) P (q i |C)</formula><p>To create the relevance feedback model P (t 1 , t 2 , ..., t n |R) we simply use the web-pages from the top scoring documents from our basic retrieval run. Here we use the top 10 results. The full-text of these web-pages is added together, and a (parsimonious) language model is created from this text. The relevance feedback model can be seen as an expanded weighted query. To estimate the feedback likelihood ratio, instead of summing over the query terms, we now sum over all terms contained in the feedback model as follows:</p><formula xml:id="formula_6" coords="3,324.82,75.64,214.57,44.19">N LLR f eedback = n i=1 P (t i |R) • log λP (t i |D) + (1 -λ)P (t i |C) P (t i |C)</formula><p>The document model, i.e., P (q i |D) and P (t i |D), and the relevance feedback language model, P (t i |R), can be estimated according to the parsimonious model or according to the standard language model using maximum likelihood estimation. Since the two likelihood ratios are normalized, they can be easily combined. We have chosen the following formula to combine the query likelihood ratio and the relevance feedback likelihood ratio:</p><formula xml:id="formula_7" coords="3,316.81,231.24,240.10,9.65">N LLR comb = (1 -α) • N LLR query + α • N LLR f eedback</formula><p>The α can depend on the quality of the initial run. If there P@10 is estimated to be low, the top 10 results are mostly not on topic, so it can be dangerous to use these results for pseudo-relevance feedback. In this case a smaller α should be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We will first discuss the official runs, and analyse their results. Then, in a separate subsection, we discuss our initial experiments with parsimonious language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Official runs</head><p>We submitted five runs before, and three runs after the official deadline. Two further runs were used to construct the official submissions. Only the five official submissions have been part of the pooling process.</p><p>We submitted two runs on the full-text index run, using the vector space model (UAmsT07MTeVS) and using the language model (UAmsT07MTeLM, not pooled).</p><p>Next, we submitted a plain title index run (UAmsT07MTiLM) and a plain anchor-text index run (UAmsT07MAnLM) both using the language model. We also have the similar runs using vector-space model, using the title index (UAmsT07MTiVS, not submitted) and the anchor-text index (UAmsT07MAnVS, not submitted).</p><p>These separate indexes can provide additional retrieval cues, for example, the anchor-texts provide a document representation completely disjoint from the document's text. Hence, we also submitted four runs that combine different sources of evidence. First, a weighted CombSUM with relative weights of 0.6 (text), 0.2 (anchors), and 0.2 (titles) using the vector space model (UAmsT07MSum6) and the language model (UAmsT07MSm6L, not pooled). Second, a similar combination with relative weights of 0.8 (text), 0.1 (anchors), and 0.1 (titles), again using using the vector space model (UAmsT07MSum8) and the language model (UAmsT07MSm8L, not pooled). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Official Run Results</head><p>The topic set contains 10,000 topics numbered 1 to 10000. Table <ref type="table" coords="4,78.89,456.32,4.98,8.64" target="#tab_1">2</ref> (top half) shows statistics of the number of judged and relevant documents, based on the final "prels" files. In total 1,778 different topics have been assessed. The number of relevant documents per topic varies from 1 to 52, with a mean of 12 and a median of 10. For no less than 253 topics, no relevant document has been found. The topic set also includes the adhoc topics of the Terabyte (TB) tracks at TREC 2004-2006. For comparison, we also show their statistics in Table <ref type="table" coords="4,77.89,551.96,4.98,8.64" target="#tab_1">2</ref> (bottom half). During the three years of the Terabyte track 149 topics have been assessed, with 4 to 617 relevant documents (mean 181 and median 130). There are striking differences between the two sets of judgments: First, the number of topics assessed at the MQ track is roughly ten times larger than the three year of TB track together. Second, the number of judged documents, as well as the number of relevant documents per topic is over ten times larger for the TB topics.</p><p>Table <ref type="table" coords="4,87.48,663.00,4.98,8.64" target="#tab_2">3</ref> shows the results for the MQ track. The first two scores are based on the MQ judgments: NEU stands for the estimated MAP (statMAP) as produced by the Northeastern University's method, UMass stands for the expected MAP as produced by the University of Massachusetts Amherst's method. <ref type="foot" coords="4,349.19,264.10,3.49,6.05" target="#foot_0">1</ref> Comparing the scores over the five runs, we see that they are in complete agreement about the ranking. Both NEU and UMass methods agree on the best of the five runs: the vector-space combination (Sum8). Over all runs, the NEU method gives the highest statMAP score to the fulltext language model run (TeLM). The next three scores in Table <ref type="table" coords="4,342.45,337.50,4.98,8.64" target="#tab_2">3</ref> are based on the TB assessments. The best scoring run on all measures is the full-text language model run (TeLM). The order of the five official submissions is different: now the full-text vector-space run (TeVS) scores best on MAP. What if we treat the MQ judgments as as normal qrels (so assuming for most measures that non-judged documents are non-relevant)? Table <ref type="table" coords="4,404.31,421.53,4.98,8.64" target="#tab_3">4</ref> shows the results. The best scoring run, again on all measures, is the full-text language model run (TeLM). The best official submission is the vector-space combination (Sum8), in agreement with both the NEU and UMass methods. In fact, the five official submission get the same order by MAP and by the NEU and UMass methods. More generally, we see that map and precision at 10 are resulting in the same system ranking, and that the precision at 10 scores are much lower than for the TB topics in Table <ref type="table" coords="4,548.44,517.17,3.74,8.64" target="#tab_2">3</ref>. This is a clear indication that we have only unearthed a small sample of the relevant documents.</p><p>We have now shown three "qrels" and eight measures, how do these agree? Table <ref type="table" coords="4,435.68,565.34,4.98,8.64">5</ref> shows Kendall's tau of the system rank correlation. Some observations present themselves: First, we see that there is reasonable correlation between all pairs of measures, with correlations ranging from 0.6 to 1.0, with the 0.6 for the agreement between UMass and TB map, and UMass and TB bpref. Second, the agreement between NEU and Terabyte MAP (0.911 over 10 systems) seems higher than that of UMass (0.600 over 5 systems), however this may be misleading since the NEU measure ranks the 5 official runs in the exact same order as UMass.</p><p>Table <ref type="table" coords="5,78.58,64.07,3.87,8.64">5</ref>: Rank correlations of the resulting system rankings (columns and rows are in the same order). Million Query Terabyte Million Query NEU UMass map bpref P@10 map bpref P@10 -1.000 0.911 0.867 0.867 1.000 0.867 1.000 --0.600 0.600 0.800 1.000 1.000 What is the impact of low pooling depth? We look at the number of relevant, nonrelevant, and unjudged documents in runs both inside and outside of the judgment pools. The results are shown in Table <ref type="table" coords="5,152.05,555.41,3.74,8.64" target="#tab_4">6</ref>. Looking at the 1,778 MQ topics, over 25% of the top 1 results have not been judged. At rank 10, the percentage of unjudged documents is 43% (full-text, not pooled) and 62% (anchor-texts, pooled). The relative precision over judged documents is still 41% (full-text) and 25% (anchor-texts) suggesting strongly that the judgments are merely a sample. A clear call for caution to use the MQ judgments as traditional qrels (as we did in Table <ref type="table" coords="5,250.84,639.09,3.60,8.64" target="#tab_3">4</ref>). For the MQ topics we see no significant difference between the coverages of runs in and outside the pools. In a sense this may make the comparison of official and post-submission runs less unfair. Looking at the 149 TB topics, we see clearly the difference in the percentage of judged documents for the pooled run (full-text, very similar runs were in the top 50 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parsimonious Language Model Runs</head><p>We have not implemented the parsimonious language model in Lucene, but instead use our own scripts for a standard language model, and for the parsimonious language model. As a result, these runs have some limitations that affect the performance (as we will see below). For efficiency reasons, the background corpus does not consist of the whole GOV2 corpus, but of a random 1% sample of the GOV2 corpus.</p><p>Another difference with the Lucene language model run is that we do not apply document length normalization in our runs. Moreover, since we rerank only the top 1,000 documents of each topic, possibly relevant documents outside these top 1,000 results are not being considered. Finally, our scripts used a more simple tokenization and stemming than was used in the original runs. Before applying our models on the topics of the Million Query track, we first apply them on the 149 Terabyte track topics. As mentioned before, in previous years of the TREC Terabyte track we already noticed the GOV2 collection requires little smoothing. Initial experiments show in our setting even less smoothing is needed than in the Lucene language model runs, i.e. we set λ = 0.99. For the parsimonious model we also have to set the parameter λ D p . In previous research <ref type="bibr" coords="5,389.81,530.03,11.62,8.64" target="#b0">[1]</ref> values from 0.01 to 0.2 for λ p lead to optimum performance depending on the exact task. We take the results of the query likelihood ratio as a baseline, and optimize on this baseline the parameter λ D p for our document model. Also we examine the influence of stemming on the baseline run. For the relevance feedback model we will optimize the parameter λ R p separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parsimonious Model Results</head><p>The baseline provides us with a first indicator of the quality of the parsimonious model without the influence of the pseudo-relevance feedback. Table <ref type="table" coords="5,455.08,674.96,4.98,8.64" target="#tab_5">7</ref> (top half) shows the results for the non-stemmed runs. First we see that the score of the reranking are somewhat lower than for the official runs, because of the differences listed above. However, we also see that the parsimonious models score better on all three evaluation measures than the standard language model with maximum likelihood estimation. With λ D p = 0.1 or 0.2, MAP and Bpref improve significantly (bootstrap test, onetailed, p &lt; 0.05) with around 5% and P@10 improves significantly with almost 12%. These initial experiments further show we get the best results with λ D p = 0.1. Although P@10 is better for λ D p = 0.01, MAP and Bpref are significantly worse. The results for λ D p = 0.1 are somewhat better than for λ D p = 0.2, but the difference is not significant. In the next experiments we use λ D p = 0.1 for the document model.</p><p>When we apply Porter stemming to the best baseline runs, shown in the bottom half of Table <ref type="table" coords="6,195.08,223.84,3.74,8.64" target="#tab_5">7</ref>, the results do not improve much or not at all depending on the evaluation measure and the model. The standard language model with stemming gets a lower MAP and P@10, but a slightly higher Bpref. The parsimonious model achieves the largest improvement on P@10; MAP and Bpref only improve slightly. Considering that our adhoc implementation is faster when stemming is not used, and stemming does not result in a clear improvement on the baseline run, we will not apply stemming to the subsequent runs.</p><p>In the next experiment we again run the 149 Terabyte topics, and this time we do apply the pseudo-relevance feedback. The experiment is run with values for α ranging from 0, only the query likelihood ratio, to 1, only the relevance feedback ratio, with steps of 0.1. The parameter λ p has a different optimal value for the relevance feedback model than for the document model. Experiments show that λ R p is best set at 0.01 for the relevance feedback model.</p><p>The optimal value for α depends on the evaluation measure that you want to optimize, MAP, Bpref or P@10. The parsimonious language model produces the best results for all three evaluation measures, however the measures are optimal at three different values for α. MAP peaks at α = 0.5 with a value of 0.2953; Bpref at α = 0.6 with a value of 0.3716; and P@10 is optimal at α = 0.4 with a value of 0.5530. The MAP scores of all three models on the different values for α are plotted in Figure <ref type="figure" coords="6,190.72,539.06,3.74,8.64" target="#fig_1">1</ref>. When we compare the best run of the standard language model with the best run of the parsimonious model (both with α = 0.5), Map and Bpref of the parsimonious model run are significantly better, with improvements of 5.2% and 7.3% respectively (see Table <ref type="table" coords="6,282.11,586.88,3.60,8.64" target="#tab_6">8</ref>). P@10 is slightly better, improving 2.3%, but the difference is not significant.</p><p>We also experiment with a mixed model using maximum likelihood estimation for the document model and parsimonious estimation for the pseudo-relevance feedback model. The scores are also shown in Figure <ref type="figure" coords="6,195.77,660.81,3.74,8.64" target="#fig_1">1</ref>, and are in between the two other runs, i.e better than the standard language model, and not as good as the parsimonious model.</p><p>We have also tested our models on the Million Query topics (using the initial set of 1,692 topics). The results  For example, the sample used as collection model does not contain all query terms. Despite these limitations, we see again that the parsimonious language is superior to the standard MLE model. When we treat the MQ judgments as as normal qrels, the results are consistent with the NEU statMAP scores, i.e., the parsimonious run is better than the standard language model run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results Analysis</head><p>When we compare the results of the official runs with our runs with the standard and the parsimonious language model, the unofficial runs, we have to take into account that we are reranking the results of the official run. Stemming for example does not improve the unofficial runs much, possibly because stemming is already applied in the basic retrieval run, so documents with different terms but with the same stem are retrieved anyway. However, despite the limitations mentioned in the beginning of the previous section, the best unofficial run (using parsimonious language models and α = 0.5) on the Terabyte topics is better than the best official run, UAmsT07MTeLM that is also used as the run to rerank (see Table <ref type="table" coords="6,514.97,710.82,3.60,8.64" target="#tab_6">8</ref>). While the improvement in MAP and P@10 is small, under the 2%, there is a significant improvement in Bpref of 8.8%. In the unofficial runs it is the application of pseudo-relevance feedback that leads to the biggest improvement. In the official runs no feedback is applied.</p><p>The performance of our standard and parsimonious model on the Million Query topic is of much poorer quality than the performance on the Terabyte topic set. The topic set is the major difference between the Million Query runs and the Terabyte runs, along with the less complete judgments. The Million Query topics are more specific and contain quite some words that are not contained in our 1% sample of the background corpus. This is a possible explanation for the poorer results. Furthermore, since P@10 for the Million Query topics is roughly half of that of the Terabyte topics, 0.2703 and 0.5376 respectively, in combination with the much lower number of relevant documents, the quality of our pseudo-relevance feedback is lower, which is reflected in the lower values of α that give the best results. We are currently implementing our models in a standard search engine, which will overcome the current limitations such as the use of a sample as collection model, to further investigate the utility of parsimonious language models for Web retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>During the TREC 2007 Million Query track, we submitted a number of runs using different document representations (such as full-text, title-fields, or incoming anchor-texts), and compared results of the earlier Terabyte tracks to the Million Query track. The initial results show broad agreement in system rankings over various measures on topic sets judged at both Terabyte and Million Query tracks, with runs using the full-text index giving superior results on all measures, but also some noteworthy upsets.</p><p>We also conducted initial experiments with parsimonious language models. We found that the parsimonious language model is to be preferred over the standard language model using maximum likelihood estimation. It leads to superior retrieval results, while at the same time using smaller document models (and hence reducing the index) and obliviate the need for stopword lists.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,330.79,419.76,99.83,9.65;2,472.64,413.02,40.84,9.65;2,434.03,426.60,118.06,9.65;2,329.41,445.46,81.90,8.96;2,423.10,438.72,7.65,9.65;2,425.77,457.32,3.01,6.12;2,430.94,452.29,7.65,9.65;2,440.28,445.46,103.49,8.96"><head>E</head><label></label><figDesc>-step: e t = tf (t, D) • λ p P (t|D) λ p P (t|D) + (1 -λ p )P (t|C) M-step: P (t|D) = e t t e t , i.e., normalize the model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,316.81,227.40,239.10,8.64;6,316.81,239.35,30.72,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Blind feedback results on the Terabyte topics (MAP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,53.80,64.07,239.10,213.24"><head>Table 1 :</head><label>1</label><figDesc>Top ranked terms in the standard and the parsimonious language model.</figDesc><table coords="3,59.78,88.94,205.46,188.36"><row><cell cols="2">Standard LM</cell><cell cols="2">Parsimonious LM</cell></row><row><cell>Term</cell><cell>P (t|D)</cell><cell>Term</cell><cell>P (t|D)</cell></row><row><cell>cancer</cell><cell>0.0071</cell><cell>pharmacy</cell><cell>0.1186</cell></row><row><cell>pharmacy</cell><cell>0.0045</cell><cell>cancer</cell><cell>0.0673</cell></row><row><cell>board</cell><cell>0.0044</cell><cell>pharmacist</cell><cell>0.0650</cell></row><row><cell>health</cell><cell>0.0044</cell><cell>prostate</cell><cell>0.0460</cell></row><row><cell>shall</cell><cell>0.0040</cell><cell>diabetes</cell><cell>0.0336</cell></row><row><cell>care</cell><cell>0.0035</cell><cell>prescription</cell><cell>0.0332</cell></row><row><cell>patients</cell><cell>0.0033</cell><cell>patients</cell><cell>0.0213</cell></row><row><cell>research</cell><cell>0.0032</cell><cell>pharmacists</cell><cell>0.0174</cell></row><row><cell>drug</cell><cell>0.0030</cell><cell>ovarian</cell><cell>0.0158</cell></row><row><cell>state</cell><cell>0.0029</cell><cell>cancers</cell><cell>0.0143</cell></row><row><cell>treatment</cell><cell>0.0028</cell><cell>dispensing</cell><cell>0.0122</cell></row><row><cell>disease</cell><cell>0.0028</cell><cell cols="2">chemotherapy 0.0109</cell></row><row><cell>new</cell><cell>0.0027</cell><cell>prescriber</cell><cell>0.0104</cell></row><row><cell>information</cell><cell>0.0027</cell><cell>tumors</cell><cell>0.0099</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,64.07,239.10,129.56"><head>Table 2 :</head><label>2</label><figDesc>Statistics over judged and relevant documents per topic for million query track (top) and terabyte tracks (bot-</figDesc><table coords="4,53.80,87.98,233.13,105.65"><row><cell>tom).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>nr. of</cell><cell></cell><cell></cell><cell>per topic</cell></row><row><cell></cell><cell cols="4">topics min max median mean st.dev</cell></row><row><cell>judged</cell><cell>1,778</cell><cell cols="2">6 147</cell><cell>40 41.07 6.58</cell></row><row><cell>relevant</cell><cell>1,524</cell><cell>1</cell><cell>52</cell><cell>10 12.23 9.62</cell></row><row><cell>high. rel.</cell><cell>745</cell><cell>1</cell><cell>44</cell><cell>3 5.36 6.17</cell></row><row><cell>judged</cell><cell cols="3">149 317 1,876</cell><cell>870 908.40 342.44</cell></row><row><cell>relevant</cell><cell>149</cell><cell cols="2">4 617</cell><cell>130 180.65 149.16</cell></row><row><cell>high. rel.</cell><cell>125</cell><cell cols="2">1 331</cell><cell>14 34.81 51.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,63.70,223.58,219.31,165.42"><head>Table 3 :</head><label>3</label><figDesc>Results for the MQ track.</figDesc><table coords="4,63.70,236.11,219.31,152.90"><row><cell></cell><cell cols="2">Million Query</cell><cell cols="3">Terabyte 2004-2006</cell></row><row><cell>UAmsT07</cell><cell cols="2">NEU UMass</cell><cell cols="3">map bpref P@10</cell></row><row><cell cols="6">. . . MTeVS 0.1805 0.0500 0.1654 0.2527 0.3047</cell></row><row><cell cols="2">. . . MTeLM 0.2908</cell><cell>-</cell><cell cols="3">0.2921 0.3410 0.5376</cell></row><row><cell cols="2">. . . MTiVS 0.0884</cell><cell>-</cell><cell cols="3">0.0369 0.0939 0.2168</cell></row><row><cell cols="6">. . . MTiLM 0.0938 0.0281 0.0392 0.0977 0.2154</cell></row><row><cell cols="2">. . . MAnVS 0.0561</cell><cell>-</cell><cell cols="3">0.0274 0.0763 0.2081</cell></row><row><cell cols="6">. . . MAnLM 0.0650 0.0205 0.0278 0.0742 0.2034</cell></row><row><cell cols="6">. . . MSum6 0.1816 0.0557 0.1398 0.2348 0.2953</cell></row><row><cell cols="2">. . . MSm6L 0.2255</cell><cell>-</cell><cell cols="3">0.2347 0.3069 0.3738</cell></row><row><cell cols="6">. . . MSum8 0.1995 0.0579 0.1621 0.2482 0.3094</cell></row><row><cell cols="2">. . . MSm8L 0.2867</cell><cell>-</cell><cell cols="3">0.2696 0.3273 0.4711</cell></row><row><cell>Topics</cell><cell cols="2">1,153 1,778</cell><cell>149</cell><cell>149</cell><cell>149</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,316.81,64.07,239.10,177.77"><head>Table 4 :</head><label>4</label><figDesc>Results for the MQ track using the shallow judgments as qrels.</figDesc><table coords="4,356.21,88.94,160.30,152.90"><row><cell></cell><cell cols="3">Million Query</cell></row><row><cell>UAmsT07</cell><cell>map</cell><cell>bpref</cell><cell>P@10</cell></row><row><cell cols="4">. . . MTeVS 0.1684 0.2991 0.1644</cell></row><row><cell cols="4">. . . MTeLM 0.2818 0.3987 0.2703</cell></row><row><cell cols="4">. . . MTiVS 0.0841 0.1839 0.1108</cell></row><row><cell cols="4">. . . MTiLM 0.0924 0.1789 0.1211</cell></row><row><cell cols="4">. . . MAnVS 0.0604 0.1438 0.0940</cell></row><row><cell cols="4">. . . MAnLM 0.0695 0.1408 0.1072</cell></row><row><cell cols="4">. . . MSum6 0.1759 0.3006 0.1844</cell></row><row><cell cols="4">. . . MSm6L 0.2164 0.3663 0.2089</cell></row><row><cell cols="4">. . . MSum8 0.1905 0.3052 0.1899</cell></row><row><cell cols="4">. . . MSm8L 0.2788 0.4006 0.2638</cell></row><row><cell>Topics</cell><cell>1,524</cell><cell>1,524</cell><cell>1,524</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,53.80,124.82,242.59,368.58"><head>Table 6 :</head><label>6</label><figDesc>Relevant, nonrelevant, and unjudged documents for MQ judged topics (top) and TB judged topics (bottom).</figDesc><table coords="5,273.97,124.82,22.42,8.64"><row><cell>1.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,316.81,64.07,239.10,162.01"><head>Table 7 :</head><label>7</label><figDesc>Results of the parsimonious models on the Terabyte topics for non-stemmed (top half) and stemmed (bottom half) documents.</figDesc><table coords="5,316.81,100.19,239.10,125.89"><row><cell>Model</cell><cell>λ p</cell><cell>map</cell><cell>bpref</cell><cell>P@10</cell></row><row><cell>MLE</cell><cell></cell><cell cols="3">0.2189 0.2985 0.3463</cell></row><row><cell>Parsimonious</cell><cell cols="4">0.01 0.2199 0.2994 0.3926</cell></row><row><cell>Parsimonious</cell><cell>0.1</cell><cell cols="3">0.2311 0.3125 0.3866</cell></row><row><cell>Parsimonious</cell><cell>0.2</cell><cell cols="3">0.2295 0.3104 0.3805</cell></row><row><cell>MLE (stemmed)</cell><cell></cell><cell cols="3">0.2183 0.3041 0.3342</cell></row><row><cell cols="2">Pars. (stemmed) 0.1</cell><cell cols="3">0.2359 0.3190 0.4040</cell></row><row><cell cols="5">pools at TREC 2004-2006), and outside the pool (anchor-</cell></row><row><cell>texts).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,316.81,269.30,239.10,185.81"><head>Table 8 :</head><label>8</label><figDesc>Results of the parsimonious models on the Terabyte and MQ topics (using the initial set of 1,692 topics).</figDesc><table coords="6,316.81,294.17,239.10,160.94"><row><cell></cell><cell>Terabyte 2004-2006</cell><cell>MQ track</cell></row><row><cell>Model</cell><cell>map bpref P@10</cell><cell>NEU</cell></row><row><cell>MLE</cell><cell>0.2807 0.3458 0.5349</cell><cell>0.1824</cell></row><row><cell>Parsimonious</cell><cell>0.2953 0.3711 0.5470</cell><cell>0.1850</cell></row><row><cell cols="3">of the best runs can be found in Table 8 and are achieved</cell></row><row><cell cols="3">with α = 0.4 for the standard model, and α = 0.2 for</cell></row><row><cell cols="3">the parsimonious model. The NEU statMAP scores are lag-</cell></row><row><cell cols="3">ging far behind the original score (UAmsT07MTeLM scores</cell></row><row><cell cols="3">a statMAP of 0.2986 on these 1,692 topics). The limita-</cell></row><row><cell cols="3">tions seem to affect the MQ topics (selected from a search</cell></row><row><cell cols="3">engine's query log) far more severe than the Terabyte top-</cell></row><row><cell>ics.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,331.16,702.66,224.75,6.91;4,316.81,712.12,95.11,6.91"><p>We failed to reproduce the "official" scores, and hence only include these for the five official runs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is part of the <rs type="funder">Effective Focused Retrieval Techniques (EfFoRT)</rs> project, funding by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grant # <rs type="grantNumber">612.066.-513</rs>). We further acknowledge support by <rs type="funder">NWO</rs> (grants # <rs type="grantNumber">639.072.601</rs>, and <rs type="grantNumber">640.001.501</rs>), and by the <rs type="funder">E.U</rs>.'s 6th <rs type="programName">FP for RTD</rs> (project <rs type="projectName">MultiMATCH</rs> contract <rs type="grantNumber">IST-033104</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_93MnZ5r">
					<idno type="grant-number">612.066.-513</idno>
				</org>
				<org type="funding" xml:id="_ERSeXA6">
					<idno type="grant-number">639.072.601</idno>
				</org>
				<org type="funding" xml:id="_fpMBKyg">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funded-project" xml:id="_UV5hAas">
					<idno type="grant-number">IST-033104</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
					<orgName type="program" subtype="full">FP for RTD</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,338.39,79.54,217.52,8.64;7,338.39,91.50,217.52,8.64;7,338.39,103.28,217.52,8.58;7,338.39,115.24,217.52,8.58;7,338.39,127.19,217.53,8.81;7,338.39,139.32,62.37,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,532.82,79.54,23.09,8.64;7,338.39,91.50,202.02,8.64">Parsimonious language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,338.39,103.28,217.52,8.58;7,338.39,115.24,217.52,8.58;7,338.39,127.19,76.77,8.58">Proceedings of the 27th Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SI-GIR Conference on Research and Development in Information Retrieval<address><addrLine>New York NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,159.24,217.52,8.64;7,338.39,171.20,217.52,8.64;7,338.39,184.09,62.27,7.01" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,368.58,159.24,187.33,8.64;7,338.39,171.20,15.72,8.64">The ILPS extension of the Lucene search engine</title>
		<author>
			<persName coords=""><surname>Ilps</surname></persName>
		</author>
		<ptr target="http://ilps.science.uva.nl/Resources/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,203.08,217.52,8.64;7,338.39,214.86,217.52,8.81;7,338.39,226.82,217.52,8.81;7,338.39,238.94,129.23,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,384.75,203.08,167.71,8.64">Effective smoothing for a terabyte of text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,350.65,214.86,205.27,8.58;7,338.39,226.82,17.16,8.58">The Fourteenth Text REtrieval Conference (TREC 2005</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,258.87,217.52,8.64;7,338.39,270.65,217.52,8.81;7,338.39,282.61,217.53,8.81;7,338.39,294.73,217.52,8.64;7,338.39,306.69,42.90,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,382.21,258.87,173.70,8.64;7,338.39,270.82,129.27,8.64">Experiments with document and query representations for a terabyte of text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,484.84,270.65,71.08,8.58;7,338.39,282.61,116.70,8.58">The Fifteenth Text REtrieval Conference (TREC</title>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2006">2006. 2007</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,326.62,217.52,8.64;7,338.39,338.57,49.54,8.64;7,403.17,338.57,152.74,8.64;7,338.39,350.36,217.52,8.81;7,338.39,362.31,217.52,8.81;7,338.39,374.44,217.52,8.64;7,338.39,386.39,60.60,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,403.17,338.57,152.74,8.64;7,338.39,350.53,24.02,8.64">Approaches to robust and web retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,391.48,350.36,164.43,8.58;7,338.39,362.31,53.83,8.58">The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,406.32,217.52,8.64;7,338.39,418.10,217.53,8.81;7,338.39,430.06,217.52,8.81;7,338.39,442.18,217.52,8.64;7,338.39,454.14,140.30,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,516.63,406.32,39.28,8.64;7,338.39,418.27,153.30,8.64">Language models for searching in Web corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,516.66,418.10,39.26,8.58;7,338.39,430.06,136.58,8.58;7,540.99,430.23,14.92,8.64;7,338.39,442.18,186.07,8.64">National Institute of Standards and Technology</title>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
	<note>The Thirteenth Text REtrieval Conference</note>
</biblStruct>

<biblStruct coords="7,338.39,474.06,217.53,8.64;7,338.39,486.95,110.09,7.01" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,375.29,474.06,104.04,8.64">The Lucene search engine</title>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,505.94,217.52,8.64;7,338.39,517.73,217.52,8.81;7,338.39,529.68,143.09,8.81" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,373.55,505.94,182.36,8.64;7,338.39,517.90,51.11,8.64">A maximum likelihood ratio information retrieval model</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,411.93,517.73,143.98,8.58;7,338.39,529.68,113.54,8.58">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,549.78,217.52,8.64;7,338.39,561.56,217.53,8.81;7,338.39,573.52,217.52,8.58;7,338.39,585.47,217.53,8.81" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,430.77,549.78,125.15,8.64;7,338.39,561.73,94.67,8.64">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,457.72,561.56,98.20,8.58;7,338.39,573.52,217.52,8.58;7,338.39,585.47,123.50,8.58">Proceedings of the 21st ACM Conference on Research and Development in Information Retrieval (SIGIR&apos;98)</title>
		<meeting>the 21st ACM Conference on Research and Development in Information Retrieval (SIGIR&apos;98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,605.57,217.52,8.64;7,338.39,617.52,217.53,8.64;7,338.39,630.42,80.20,7.01" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,390.79,605.57,165.13,8.64;7,338.39,617.52,62.60,8.64">Stemming algorithms for use in information retrieval</title>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,649.40,217.52,8.64;7,338.39,661.36,217.52,8.64;7,338.39,673.15,217.52,8.81;7,338.39,685.10,217.53,8.81;7,338.39,697.23,95.75,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,397.18,661.36,140.18,8.64">Language modelling and relevance</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sparck-Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,474.83,673.15,81.08,8.58;7,338.39,685.10,100.84,8.58">Language Modeling for Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
