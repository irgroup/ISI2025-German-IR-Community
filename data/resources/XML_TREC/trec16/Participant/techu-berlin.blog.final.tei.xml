<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.50,154.89,348.24,15.11">Feed Distillation Using AdaBoost and Topic Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-02-07">February 7, 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.96,187.37,69.86,10.48"><forename type="first">Wai-Lung</forename><surname>Lee</surname></persName>
							<email>wai-lung.lee@dai-labor.de</email>
						</author>
						<author>
							<persName coords="1,244.97,187.37,107.30,10.48"><forename type="first">Andreas</forename><surname>Lommatzsch</surname></persName>
							<email>andreas.lommatzsch@dai-labor.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Christian Scheel DAI-Labor</orgName>
								<orgName type="institution">Technical University</orgName>
								<address>
									<addrLine>Berlin Ernst-Reuter-Platz</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.50,154.89,348.24,15.11">Feed Distillation Using AdaBoost and Topic Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-02-07">February 7, 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">9F7763AE5E93517954988CDCDA910922</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper retains the experiences by participating in TREC 2007 Blog Track 'Feed Distillation'. To perform the run various classifiers are combined, which analyze title-, content-and splog-specific features to predict the relevance of a feed related to a topic, based on the idea of AdaBoost. The implemented classifiers utilize keywords retrieved from different thesauri such as Wordnet and Wortschatz, as well as from websites providing hierarchical organized 'ontology' such as the 'Open Directory Project' and Yahoo Directory. To structure the keywords, Topic Maps are utilized according to ISO/IEC 13250:2000.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>Nowadays blogs are the most used media to express individual's experiences, emotions and opinions related to a topic. Both the rapid growth 1 and the underlying influence of bloggers on different areas of our society and online community are fascinating <ref type="bibr" coords="1,271.44,576.15,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,285.15,576.15,11.62,8.74" target="#b13">14]</ref>. Commonly, the blogs are aggregated to a feed, which provides a clean, fine-grained structure and improves the machine readability. However, since bloggers not only compose blogs related to the same topic, it is time-consuming for interested readers to judge the topic-related relevance of a feed.</p><p>1 http://www.sifry.com/alerts/archives/000436.html</p><p>To face this problem, a novel approach using Ada-Boost <ref type="bibr" coords="1,339.43,331.07,15.50,8.74" target="#b15">[16]</ref> and Topic Map <ref type="bibr" coords="1,429.59,331.07,10.52,8.74" target="#b1">[2]</ref> is utilized to estimate a feed's relevance concerning a certain topic.</p><p>According to the ISO/IEC 13250:2000, a Topic Map is used to represent knowledge and concurrent views related to a topic and contains main parts such as association, occurrence and topic. Topics represent subjects in the real world and are identified by names. Occurrences refer to relevant resources of a topic. The relations between the topics are depicted by associations <ref type="bibr" coords="1,379.13,439.65,9.96,8.74" target="#b1">[2]</ref>.</p><p>AdaBoost is a learning algorithm and frequently used in classification issues. AdaBoost has been introduced by Freund and Schapire <ref type="bibr" coords="1,455.85,476.50,14.62,8.74" target="#b15">[16]</ref>, and belongs to ensemble-based systems in decision making <ref type="bibr" coords="1,499.57,488.46,14.61,8.74" target="#b39">[40]</ref>. The idea of AdaBoost is to build a strong learner using a set of weak learners, which individually are slightly better than a random guessing algorithm <ref type="bibr" coords="1,501.16,524.32,14.62,8.74" target="#b15">[16]</ref>. As previous investigations have shown that boosting algorithms commonly outperformed other learners such as SVM (Support Vector Machine) <ref type="bibr" coords="1,462.61,560.19,15.50,8.74" target="#b34">[35]</ref> or neural networks <ref type="bibr" coords="1,339.48,572.14,14.62,8.74" target="#b54">[55]</ref>, AdaBoost is combined with Topic Maps implementing a set of 16 weak classifiers analyzing title, content and spam blog (splog)<ref type="foot" coords="1,461.99,594.48,3.97,6.12" target="#foot_0">2</ref> specific features of blogs within a feed. The work in this paper is also inspired by investigation efforts in regard with 'data fusion' and analyses of multiple evidence combination <ref type="bibr" coords="1,349.41,643.87,14.62,8.74" target="#b23">[24]</ref>. Some efforts reveal that combining dif-ferent sources of evidence or strategies can improve retrieval performance. In this context, Lee additionally figures out that rank works better than using similarity in some circumstances <ref type="bibr" coords="2,217.14,163.83,14.61,8.74" target="#b23">[24]</ref>. Moreover, the analysis of Scheel et. al. on detecting redundant and avoiding strategies, can also improve retrieval effectiveness. They suggest that strategies have to be diverse as possible while maximizing their individual quality <ref type="bibr" coords="2,105.79,223.60,14.62,8.74" target="#b48">[49]</ref>. In other words, strategies complementing one another can leverage the retrieval results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Paper</head><p>The remaining part of this paper is structured as follows. Related work is referred in Section 3. Section 4 points out the disadvantages and advantages of Ada-Boost by comparing neural networks and SVMs relating to performance and evaluation issues, followed by Section 5 depicting the conceptual framework for participating in this annual TREC Blog Track 'Feed Distillation'. In doing so, the basic assumptions using AdaBoost and Topic Maps as well as diversity measures between the implemented classifiers are presented. In Section 6, the implemented architecture is briefly overviewed. In Section 7, some training and experimental settings are highlighted. Section 8 deals with the evaluation results judged by the participating groups and reflects the proposed approach for reperforming an inoffiicial blog track run. This paper concludes with a discussion and an outlook on further research and performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Blogs have different facets related to a topic, analyzed by <ref type="bibr" coords="2,85.83,570.30,14.62,8.74" target="#b32">[33]</ref>. In this context, Paradis already reveals that the notion of topic includes three aspects from the view of linguistic and discourse representation theories: theme (statutory analysis), given information (topic-focus articulation), and intentions (discourse representation) <ref type="bibr" coords="2,142.34,630.08,14.61,8.74" target="#b35">[36]</ref>. In doing so, Paradis suggests that using linguistic and discourse structures to derive topics can improve the subjective relevance of documents towards user's information need. Indeed, selecting the right amount of topic-related keywords and concepts is a great challenge and many mining techniques exist. While some mining and retrieval techniques refer to proven remedy such as identification of finite mixtures, latent semantic indexing, independent component analysis <ref type="bibr" coords="2,445.80,187.74,9.97,8.74" target="#b2">[3]</ref>. Other researchers devote to topic identification from external resources <ref type="bibr" coords="2,310.61,211.65,15.50,8.74" target="#b57">[58,</ref><ref type="bibr" coords="2,330.01,211.65,11.63,8.74" target="#b51">52]</ref>. These approaches base on query-based expansion <ref type="bibr" coords="2,349.69,223.60,15.50,8.74" target="#b55">[56]</ref> theory, and consider lexical-semantic meanings of certain topics using different thesauri <ref type="bibr" coords="2,310.61,247.51,15.50,8.74" target="#b28">[29]</ref> or hierarchical structured ontology provided by 'Open Project Directory'. Additionally, relating to the blog sphere there is a modern approach to mining theme patterns, which incorporate spatiotemporal properties of blogs to detect topics within the blog sphere <ref type="bibr" coords="2,341.33,307.29,14.62,8.74" target="#b30">[31]</ref>. However, the present approach in regard with topical features extraction is aligned to the topic retrieval from external resources.</p><p>Determining topical relevance of feeds is something new to research in terms of classification and ranking problems. Previous work refers to topic distillation of authoritative web sites. These techniques base on the hyperlink analysis, and use DOM (Document Object Models) <ref type="bibr" coords="2,366.20,402.93,10.51,8.74" target="#b6">[7]</ref> or exploit the correlation between the outgoing and incoming hyperlinks to predict the topical relevance of documents. <ref type="bibr" coords="2,442.87,426.84,14.62,8.74" target="#b38">[39]</ref>. A more modern approach, patent-registered by Google, assesses the topical relevance of blog using author's reputation score, which is secured by digital signature system of Google<ref type="foot" coords="2,353.16,473.09,3.97,6.12" target="#foot_1">3</ref> . Apart from these, boosting algorithms on text classification have been proved as excellent ranking and categorization techniques, e.g. Rank-Boost <ref type="bibr" coords="2,339.80,510.53,15.50,8.74" target="#b20">[21]</ref> and AdaBoost.MHKR <ref type="bibr" coords="2,461.39,510.53,14.62,8.74" target="#b49">[50]</ref>. Since SVMs result an excellent precision but poor recall, boosting SVMs is recently a very active area of machine learning research <ref type="bibr" coords="2,387.40,546.39,14.62,8.74" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminaries</head><p>In this section, similarities and distinctions between commonly used learning algorithms are briefly overviewed. In the research literature, many comparisons exist between SVM and AdaBoost or AdaBoost and Feed-forward Neural Network (FNN). However, the most comparative study on the three learning algorithms is conducted by <ref type="bibr" coords="3,185.30,139.92,14.62,8.74" target="#b45">[46]</ref>. Inspired by their empirical work, using AdaBoost as appropriate learning algorithm is justified in the context of classification issue in the following sections.</p><p>For those who are familiar SVM, FNN and Ada-Boost, it may be advisable to continue reading in Section 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of Feed-forward Neural Network, SVM and AdaBoost</head><p>Nowadays, learning algorithms such as AdaBoost, Support Vector Machine (SVM) and Feed-forward Neural Network (FNN) have recently attracted popularity in different domains such as handwritten character recognition, face detection, and especially in text classification <ref type="bibr" coords="3,150.99,333.19,15.49,8.74" target="#b12">[13,</ref><ref type="bibr" coords="3,169.80,333.19,11.63,8.74" target="#b50">51]</ref>. Commonly, all these algorithms are used to learn boundaries between positive and negative examples. The functions of the learners for determining the decision boundaries are different from the view of geometrical and mathematical basics. Principally, they all base on the marginal-theory <ref type="bibr" coords="3,211.32,405.11,15.50,8.74" target="#b14">[15]</ref> to gather a suitable function which can maximize the margin for separating classes by avoiding the overfitting problem. In this regard, the measures such as generalization problem, error rate, the size of training data sets are most used concepts to evaluate the different performance output of such learning and classification systems <ref type="bibr" coords="3,96.26,488.80,14.61,8.74" target="#b33">[34]</ref>. To obtain a better understanding for operating modi of these learning algorithms, literatures and researches in the domain of machine learning are recommended <ref type="bibr" coords="3,135.13,524.66,15.50,8.74" target="#b54">[55,</ref><ref type="bibr" coords="3,153.95,524.66,11.62,8.74" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">AdaBoost</head><p>The idea of AdaBoost is to produce a highly accurate classification rule by combining a set of classifiers (or weak hypotheses), each of which may be only moderately inaccurate <ref type="bibr" coords="3,152.45,606.17,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="3,170.93,606.17,11.62,8.74" target="#b47">48]</ref>. In the context of learning process, the weak learners are trained sequentially, one at a time. Principally, at each iteration a weak classifier is inaccurate to classify the examples, which were most difficult to classify by the previous weak hypotheses. After a certain number of iterations, the resulting weak hypotheses are linearly combined into a single prediction rule, so-called combined hypothesis.</p><p>The generalized AdaBoost algorithm for binary classification <ref type="bibr" coords="3,369.29,176.31,15.49,8.74" target="#b47">[48]</ref> maintains a vector of weights as a distribution D t over training data set. At round t, the objective of AdaBoost is to estimate a weak hypothesis h t : X → R with moderately low error in regard with to the weights D t . In this setting, weak hypotheses h t (x) make real-valued confidence-rated predictions <ref type="bibr" coords="3,361.78,248.04,14.62,8.74" target="#b45">[46]</ref>. Initially, the distribution D t is uniformly initialized, which means that all instances are equally weighted. The boosting algorithm increases (or decreases) the weights D t (i) when h t (x i ) making a bad (or good) prediction of instances, with a variation proportional to the confidence h t (x i ). The final hypothesis, f T : X → R, calculates its prediction using a weighted vote of the weak hypothesis f T (x) = T j=1 α j h j (x) . The updating rule can be expressed as</p><formula xml:id="formula_0" coords="3,352.75,387.42,186.50,121.31">D t+1 (i) = D t (i) * e -αtyiht(xi) Z t = e -t j=1 αj yihj (xi) M * t j=1 Z j = e yift(xi) M * t j=1 Z j = e -mrg(xi,yi,ft) M * t j=1 Z j<label>(1)</label></formula><p>Schapire and Singer <ref type="bibr" coords="3,415.87,521.95,15.50,8.74" target="#b47">[48]</ref> already prove that the training error of the AdaBoost algorithm exponentially decreases with the normalization factor Z t computed at round t. This property is utilized in designing of the weak learner, which attempts to figure out a weak hypothesis h t that minimizes Z t = M i=1 D t (i) * exp (-αtyiht(xi)) . From 1 and the previous expression of Z t , it can be interpreted that AdaBoost is a stage-wise procedure for minimizing a certain error function, which depends on the functional margin -mrg(x i , y i , f ). Particularly, AdaBoost attempts to minimize i exp (-yi t αtht(xi)) , which illustrates the negative exponential of the margin of the final classifier. According to Schapire and Singer, the learning bias of Adaboost is proven to be very aggressive at maximizing the margin of the training examples and this makes a clear connection to the SVM learning paradigm <ref type="bibr" coords="4,154.52,187.74,14.62,8.74" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Neural Network</head><p>One of well-known Neural Networks is the FNN, socalled Feed-forward Neural Network. The architecture is organized by layers of units, with connections between units from different layers in forward direction <ref type="bibr" coords="4,91.81,280.44,9.96,8.74" target="#b3">[4]</ref>. A fully connected FNN with one output unit and one hidden layer of N h utilizes the computation function:</p><formula xml:id="formula_1" coords="4,88.86,335.32,211.79,30.50">f F N N (x) = ϕ 0 N h i=1 λ i ϕ i (ω i , b i , x) + b 0 ,<label>(2)</label></formula><p>where λ i , b i , b 0 ∈ and x, ω ∈ N . To simplify, the weights can be separated in coef f icients</p><formula xml:id="formula_2" coords="4,72.00,386.29,228.64,26.41">(ϕ i ) N h i=1 , f requencies (ω N h i=1 ) and biases (b i ) N h i=0 .</formula><p>The most used activation functions ϕ i (ω, b, x) in the hidden units are sigmoidal, in particular, for Multi-layer Perceptrons (MLP) and radially symmetric for Radial Basis Function Networks (RBFN). Of course, there are many other functions <ref type="bibr" coords="4,186.86,461.54,15.50,8.74" target="#b36">[37,</ref><ref type="bibr" coords="4,206.21,461.54,11.63,8.74" target="#b18">19]</ref>. However, output activation functions ϕ 0 (u) are commonly sigmoidal or linear.</p><p>The objective of the training process is to determine adequate parameters such as coefficients, frequencies and biases for minimizing a pre-estimated cost function. The most usual sum-of-squares error function is illustrated as follows:</p><formula xml:id="formula_3" coords="4,106.78,576.28,189.62,30.32">E(X) = L i=1 1 2 (f c F N N (x i ) -y i ) 2 . (<label>3</label></formula><formula xml:id="formula_4" coords="4,296.40,586.69,4.24,8.74">)</formula><p>The sum-of-squares error function E(X), illustrated as <ref type="bibr" coords="4,117.66,630.08,11.62,8.74" target="#b2">(3)</ref>, is an approximation to the squared norm of the error function f F N N (x) -y(x) in the Hilbert space L 2 of squared integrable functions, where the integral refers to probability measure of the problem by X. For C-class problems, architectures with C output units are utilized <ref type="bibr" coords="4,482.99,139.92,14.62,8.74" target="#b45">[46]</ref>, and the goal is a transform minimizing</p><formula xml:id="formula_5" coords="4,336.81,180.21,202.44,30.32">E(X) = L i=1 C c=1 1 2 (f c F N N (x i ) -y c i ) 2 ,<label>(4)</label></formula><p>where f c F N N is c th component of the output function. The architecture of the network related to the connections, numbers of hidden units and output activation functions is commonly fixed in advance, whereas the weights are trained during the learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">SVM</head><p>As SVM is described by <ref type="bibr" coords="4,417.49,322.52,15.49,8.74" target="#b54">[55,</ref><ref type="bibr" coords="4,436.06,322.52,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="4,446.88,322.52,11.62,8.74" target="#b45">46]</ref>, the input vectors of SVM are mapped into a high-dimensional space (inner product) through non-linear mapping φ, which is chosen in advance. In this space, the so-called feature space, an optimal hyperplane is designed. The mapping using a kernel function K(u, v) can be implicit, since the inner product expressing the hyperplane can be defined as φ(u), φ(v) = K(u, v) for every two vectors u, v ∈ N . In the context of the SVM framework, an optimal hyperplane can be described a maximal normalized margin for separating data set. The functional margin of a point (x i , y i ) with regard to a a data set X is the minimum of the margins of the points in the data set. If f is hyperplane, the normalized (or geometric) margin can be considered as the margin divided by the norm of the orthogonal vector to hyperplane. Thus, the absolute value of the geometric margin is the distance to the hyperplane. Based on Lagrangian and Kuhn-Tucker theory, the maximal margin hyperplane for a binary classification problem given by X can be expressed as:</p><formula xml:id="formula_6" coords="4,343.03,601.91,196.22,30.32">f SV M (x) = M i=1 y i α i K(x i , y i ) + b,<label>(5)</label></formula><p>where the vector (α) M i=1 is the solution of the following constrained optimalization problem in the dual space:</p><formula xml:id="formula_7" coords="5,78.30,147.44,222.35,80.91">M aximize W (X) = - 1 2 M i,j=1 y i α i α j K(x i , y j ) + M i=1 α i subject to M i=1 y i α i = 0 (bias constraints),<label>(6)</label></formula><formula xml:id="formula_8" coords="5,158.58,231.98,104.77,9.65">0 ≤ α i ≤ C, i = 1, ..., M.</formula><p>To avoid the bias constraints, b is attended apart and fixed a priori in some implementations. A point is well classified if and only if its margin with regard to f SV M is positive signed. The points x i with α i ≤ 0 (active constraints) are support vectors. Relating to their margin value, non-bounded support vectors have margin 1. Conversely, the margin of bounded support vectors are less than 1. The parameter C is utilized to trade off the margin and the number of training errors. One obtains the hard margin hyperplane when setting C = ∞. However, the cost function -W (X) including a constant is the squared norm of the error function f SV M -y(x) in the Reproducing Kernel Hilbert Space, which is associated to K(u, v) <ref type="bibr" coords="5,121.17,418.84,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="5,135.73,418.84,11.63,8.74" target="#b45">46]</ref>. The most usual kernel functions K(u, v) are polynomial, Gaussian-like or some particular sigmoids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">AdaBoost vs. SVM</head><p>SVM has been emerged as a good technique in classification and categorization issues <ref type="bibr" coords="5,218.77,498.57,15.49,8.74" target="#b12">[13,</ref><ref type="bibr" coords="5,237.56,498.57,11.63,8.74" target="#b50">51]</ref>. SVM provides a good upper bound to generalization of error <ref type="bibr" coords="5,88.54,522.48,15.50,8.74" target="#b54">[55,</ref><ref type="bibr" coords="5,107.79,522.48,11.63,8.74" target="#b12">13]</ref>. While some SVMs achieve an excellent precision, the recall is poor when applying SVMs in text classification. However, SVM algorithms focus on finding the hyperplane as kernel function for maximizing the decision boundary <ref type="bibr" coords="5,201.68,570.30,14.62,8.74" target="#b50">[51]</ref>. Since the training kernel matrix grows quadratically, training SVM on a large data set is resource-consuming. In this context, there are improvements relating to this problem <ref type="bibr" coords="5,282.38,606.17,14.61,8.74" target="#b12">[13]</ref>. But to prove high effectiveness, SVMs strictly require a large training data set, which does not always exist. In contrast to this, AdaBoost can be used to address this problem using resampling techniques and small sets of training data <ref type="bibr" coords="5,168.62,665.94,14.61,8.74" target="#b39">[40]</ref>. Additionally, SVMs are initially designed for binary classification problems, whereas AdaBoost.M1, an extension of AdaBoost, can tackle multiclass problems <ref type="bibr" coords="5,450.31,151.87,14.62,8.74" target="#b15">[16]</ref>. In the context of 'Feed Distillation', the proposed approach has to deal with different topic classes and spam-infected blogs within a feed. This problem is addressed in Section 5.1.</p><p>From the mathematical perspective, SVM and AdaBoost are different regarding the approach searching the dimensional space and the underlying computation. While SVM refers to quadratic programming, AdaBoost corresponds only to linear programming. Of course, quadratic programming is more computationally consuming than linear one. In doing so, SVM has to deal with the estimation problem of kernel function allowing low dimensional calculations, that are mathematically equivalent to inner products in a high dimensional feature space <ref type="bibr" coords="5,505.44,337.49,15.49,8.74" target="#b15">[16,</ref><ref type="bibr" coords="5,523.75,337.49,11.62,8.74" target="#b54">55]</ref>. More details about the relation between AdaBoost and SVM can be found in <ref type="bibr" coords="5,426.83,361.40,14.62,8.74" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">AdaBoost vs. Neural Networks</head><p>However, the root of both neural network and Ada-Boost can be found in the probably approximately correct (PAC) model, which was introduced by <ref type="bibr" coords="5,310.61,486.62,14.62,8.74" target="#b53">[54]</ref>. Neural networks, once introduced by <ref type="bibr" coords="5,501.92,486.62,14.62,8.74" target="#b54">[55]</ref>, can tackle multiclass and multi-labeled problems as Ada-Boost.M1 does. The major disadvantages of neural networks are related to the generalization performance from training to test data, determining the architecture of layers, as well as overfitting problem <ref type="bibr" coords="5,310.61,558.35,14.62,8.74" target="#b31">[32]</ref>. In contrast to this, some researchers have empirically observed that AdaBoost does not overfit, even when running for thousands of rounds <ref type="bibr" coords="5,489.31,582.26,14.62,8.74" target="#b15">[16]</ref>. However, the fascinating generalization properties outperformed neural network relating to this issue <ref type="bibr" coords="5,520.98,606.17,14.61,8.74" target="#b33">[34]</ref>. Additionally, from the computational complexity, it is time-consuming to train neural networks. Conversely, AdaBoost is simple to implement and a stagewise procedure for minimizing a certain error function by focusing on misclassified training items <ref type="bibr" coords="5,497.99,665.94,14.61,8.74" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">AdaBoost.M1</head><p>In this subsection, a pseudo-code of AdaBoost.M1 is illustrated in Algorithm 1. AdaBoost.M1 is an extension of AdaBoost and highly robust against multiclass and regression problems <ref type="bibr" coords="6,180.18,183.23,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="6,199.00,183.23,11.63,8.74" target="#b40">41]</ref>.</p><formula xml:id="formula_9" coords="6,72.00,208.44,204.03,45.83">Algorithm 1 A Pseudo-code of AdaBoost.M1 Input: -Sequence of N examples S = [(X i , Y i )], i = 1, .</formula><p>.., N with labels y i ∈ Ω, Ω = ω 1 , ..., ω c -Weak learning algorithm WeakLearn (WL) -Integer T specifying number of learning iterations </p><formula xml:id="formula_10" coords="6,81.96,291.48,147.22,34.53">Intialize D t (i) = 1 N , i = 1, • • • , N for t = 1, 2, • • • , T : do 1.</formula><formula xml:id="formula_11" coords="6,91.93,363.21,172.33,83.77">t = i:ht(xi)=yi D t (i) • If t &gt; 1 2 abort 4. Set β i = t (1-t) . 5. Update distribution D t : D t+1 (i) = D t (i) Z t × β t if h t (x i ) = y i , 1, Otherwise</formula><p>where Z t = iD t (i) is a normalization constant chosen so that D t+1 becomes a proper distribution function. end for Test -Weighted Majority Voting: Given an unlabeled instance x i 1. Obtain total vote received by each weak learner</p><formula xml:id="formula_12" coords="6,81.96,555.13,161.31,22.57">V j = t:ht(x)=ωj log 1 βt , j = 1, • • • , C. 2.</formula><p>Choose the class that receives the highest total vote as the final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conceptual Framework</head><p>In this section, problems are precisely depicted when performing the Blog track task. In doing so, the as-sumptions relating to the suitability of AdaBoost and Topic Maps are presented. Particularly, the keyword aspects and the implemented AdaBoost algorithm is described by depicting the applied classifiers and its operation modus. Additionally, the dis-similarities between the keyword scources and the diversity of implmented classifiers are measured. This section concludes with the results related to diversity measures and an initial recommendation for designing the set of classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Problem Definition and Basic Assumption</head><p>This annual Blog Track task is defined as 'Find me a blog with a principle, recurring interest in X'. As some research efforts show that there is not one universally appropriate definition of relevance, it is important to deal with a definition of topic-related relevance of a feed. Generally, there is a distinction between objective and subjective relevance <ref type="bibr" coords="6,489.18,367.06,15.49,8.74" target="#b17">[18,</ref><ref type="bibr" coords="6,507.77,367.06,7.01,8.74" target="#b0">1]</ref>. Objective relevance can be achieved if a document covers the notion of topicality and aboutness, defined by <ref type="bibr" coords="6,310.61,402.93,14.62,8.74" target="#b29">[30]</ref>. But this notion is not the only the crucial factor that contributes to the usefulness of a document <ref type="bibr" coords="6,520.98,414.88,14.61,8.74" target="#b29">[30]</ref>.</p><p>In this context, Cooper and Bookstein reveal that a document is objectively relevant to a query if they both are related to a common topic. Subjective relevance is more aligned with user's information need.</p><p>A document is subjectively relevant to a query if it covers the information need of the user who issued the query <ref type="bibr" coords="6,355.65,498.57,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="6,369.59,498.57,7.01,8.74" target="#b5">6]</ref>. However, in regard with the TREC Blog Track task, the subjective kind of relevance of a feed is taken into account. Thus, a feed is relevant if this is principally devoted to a topic. In other words, a feed is relevant if this consists of a majority of blog entries which predominately deal with a topic. The major problem is not only to identify various facets <ref type="bibr" coords="6,357.78,582.26,15.49,8.74" target="#b10">[11]</ref> of a topic within a blog and the underlying topical relevance, but also to determine the quantity of topic-related blog entries within a feed. Since bloggers compose blogs related to different topics or seasonal themes, the variety of topical facets is increasingly intensified within a feed. Thus, a feed can be interpreted as a multiclass problem. An additional burden, as the spam detection task of TREC 2006 figures out, is that the TREC blog collection is 'infected' by splogs <ref type="bibr" coords="7,161.20,139.92,14.61,8.74" target="#b26">[27]</ref>. In doing so, this problem impedes the topical distillation procedure and has to be considered in the proposed approach. To tackle the problem mentioned above, different hypothesis are combined to predict the topical relevance of feeds by removing splogs simultaneously.</p><p>The general idea combining different hypotheses is to predict the relevance of feeds related to a topic. Intuitively, such a hypothesis can be interpreted as an expert related to the certain topic, which can be synonymously called classifier. The goal of using diverse experts is to obtain a high degree of objective relevance by combining the subjective view of each expert. Since the xml-based structure of a feed is fine-grained and provide information about the title, content, etc., these classifiers can make decision based on investigating these features, whether a feed is relevant to the certain topic. However, the approach consists of two parts. First, topic-related keywords are necessary to classify the blogs within a feed. Therefore, the research concerning query expansion <ref type="bibr" coords="7,72.00,391.04,15.50,8.74" target="#b55">[56]</ref> and extracting keywords from different keyword sources <ref type="bibr" coords="7,107.94,403.00,15.50,8.74" target="#b28">[29]</ref> are followed. In doing so, research of 'data fusion' and evidences combination are referred to cover the concurrent views of relevance and facets of a blog related to a topic. Additionally, to structure these keywords the concept of Topic Map is utilized. Secondly, the importance or weight has to be estimated for each classifier. Since a manual estimation of weights is laborious, AdaBoost is an appropriate learning algorithm to automatically determine such weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Topic Maps and Keyword Aspects</head><p>The genesis of Topic Map can be found in the 1990's, developed by Davenport Group, which was discussing ways of interchange of computer documentation <ref type="bibr" coords="7,282.38,581.27,14.61,8.74" target="#b37">[38]</ref>. In the past, Davenport Group developed DocBook DTD<ref type="foot" coords="7,94.42,603.61,3.97,6.12" target="#foot_2">4</ref> , which is one of most widely used DTDs for authoring SGML<ref type="foot" coords="7,147.51,615.56,3.97,6.12" target="#foot_3">5</ref> and XML<ref type="foot" coords="7,200.28,615.56,3.97,6.12" target="#foot_4">6</ref> . Indeed, using Topic Maps to organize, visualize and navigate knowledge and metadata is not a novel approach, but has recently attracted the interest of some researchers and practitioners <ref type="bibr" coords="7,368.83,151.87,14.62,8.74" target="#b27">[28]</ref>.</p><p>Nowadays, there are different variations, standard formats and query languages alongside the topic map research landscape. In doing so, they are used in different research areas such as E-learning environment, Knowledge Management, Artificial Intelligence, etc <ref type="bibr" coords="7,310.61,231.19,14.62,8.74" target="#b43">[44]</ref>. However, the main focus using Topic Map is to incorporate the different facets which are associated to the certain topic. The most important advantage of Topic Map lies in the generalization of this model, that one can describe and structure everything related to a topic and unify all into a single model. In this context, the associations are used to connect to the collections of named entities and concepts retrieved by the different thesauri and websites, as illustrated in Figure <ref type="figure" coords="7,395.82,338.79,3.88,8.74" target="#fig_0">1</ref>. To gather topic-related keywords in a Topic Map, the topic word is remitted as a query to existing data sources such as Wikipedia, Dmoz, Yahoo, Google, Wortschatz and Wordnet. In doing so, available APIs (Application Programming Interfaces) of these sources are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Wordnet</head><p>Wordnet<ref type="foot" coords="8,115.77,145.42,3.97,6.12" target="#foot_5">7</ref> is a well-known resource for lexical and semantic keywords. In this context, terms considering hypernyms, hyponyms and synonyms are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Wortschatz</head><p>Wortschatz<ref type="foot" coords="8,128.57,228.04,3.97,6.12" target="#foot_6">8</ref> is a German online webservice providing access to a large set of corpora in different languages. Additionally, this webservice offers statistical information about co-occurrences related to a topic <ref type="bibr" coords="8,72.00,277.43,14.62,8.74" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Google Suggest</head><p>Google suggest<ref type="foot" coords="8,147.77,322.61,3.97,6.12" target="#foot_7">9</ref> provides n-grams, based on most searched query phrases related to a topic. Frequently, this API offers 2-, 3-, 4-or 5-grams with information in terms of query frequencies associated with the keywords combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Yahoo</head><p>By querying Yahoo<ref type="foot" coords="8,161.19,417.19,7.94,6.12" target="#foot_8">10</ref> every topic is combined with the word 'shop' to gather product-or topic-related features from the perspective of shop-providers. In doing so, TF-IDF to select those terms is utilized, which are used by twenty shop sites offering topicrelated products. In this regard, the Yahoo API is utilized to extract additional n-grams. Yahoo n-gram API varies from 'Google suggest' in the missing information about the frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Open Directory Project</head><p>Dmoz<ref type="foot" coords="8,101.15,559.58,7.94,6.12" target="#foot_9">11</ref> , so-called 'Open Directory Project', is a human-edited and -maintained category-based search engine. Since Dmoz provides a hierarchical organized 'ontology', keywords of hierarchical related categories are used, e.g. relating to 'Solaris' category labels within the categories 'administration' and 'software' are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Wikipedia</head><p>The terms of the topic are adressed as query to Wikipedia<ref type="foot" coords="8,361.91,195.09,7.94,6.12" target="#foot_10">12</ref> for matching site titles via its underlying API. In this context, links related to categories or themes concerning the certain topic are retrieved. The problem is sometimes that there is no title which can be precisely matched by the given query word(s). In this case, the first hit of this query is taken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.7">Intersection</head><p>Apart from the black-listed keywords, a Topic Map also contains an intersection of all keywords, which occur in all the data sources mentioned above. In doing so, these keywords are removed from the remaining classifiers to avoid redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.8">Blacklist</head><p>In addition to the (semi-)automatic retrieved keywords, a hard-coded blacklist of spam-specific and query-independent keywords is deployed, which frequently are used for spam blogs. In doing so, the keywords are weighted with a numerical scoring system, as illustrated in Figure <ref type="figure" coords="8,436.28,453.63,4.98,8.74">2</ref> Figure <ref type="figure" coords="8,342.21,529.43,7.75,8.74">2:</ref> A query-indepedent blacklist for removing splogs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">AdaBoost Classifiers</head><p>In this section, the implemented classifiers and their purpose for completing the 'Feed Distillation' task are described.</p><p>Based on the idea of ensemble-based systems, diverse AdaBoost classifiers are sequentially developed, at one time. Diversity, more detailed in Section 5.4.2, is an essential requirement by developing ensemblebased systems. In doing so, the order, in which the classifiers are trained, has impacts on the weights to estimate. However, these classifiers can be simple or complex ones and coarsely categorized in title-, content-based and splog-specific classifiers. As optimalization tricks for blogs reveal, that a blog with key phrase including topic word(s) in the title can better placed in the search engine hits, a simple classifier has to be able, e.g. to analyze the existence or absence of the topic word in the title. In contrast to a simple classifier, a complex classifier is based on topic-related keywords and exceeding a threshold to predict the relevance of a blog. If the similarity score between the keywords and the blog entry exceeds a threshold, the analyzing blog document is predicted as relevant by the classifier of these keywords.</p><p>Since the topical similarity has to be estimated based on a committee of different classifiers by removing splogs simultaneously, the AdaBoost.M1 scoring is modified. Originally, the relevance estimation of AdaBoost.M1 is based on the linear combination of hypotheses which are positive signed. In the case of feed distillation, the splog-specific and titleand content-based classifiers are different. While the splog-related classifiers are negative signed, contentand title-based ones are positive signed. Thus, estimating topical relevance is based on the sum of negative and positive weighted classifiers. In the following subsections, each implemented classifiers based on title-, content-, as well as splogs-specific retrieval strategies are depicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Title-based classifiers</head><p>The IntersectionTitleFilter contains the intersection mentioned above and is used to check the existence of those keywords in the blog's title. Moreover, the assumption is that the intersection is a minimal evidence for the relevance of a blog relating to a topic.</p><p>The CategoryTitleFilter predicts the blog's relevance analyzing the occurrence of the topic word(s) in the title of a blog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Content-based classfiers</head><p>The IntersectionBodyFilter is applied to the content of a blog and determines the blog's topical relevance depending on the existence of all intersection keywords.</p><p>The CategoryBodyFilter works in the same manner as CategoryTitleFilter does and refers to the content of a blog.</p><p>The RelevanceWikipediaFilter is threshold-based classifier and contains a set of keywords retrieved from links. This classifier is applied on the content of the blog by analyzing the similarity between the blog's content and topic-related keywords retrieved by Wikipedia. Since both Weblog and Wikipedia are emerging technologies of Web 2.0, one can assume that there are similar relations between the keywords in both media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RelevanceDmoz1Filter,</head><p>RelevanceDmoz2Filter, and RelevanceDmoz3Filter are also threshold-based classifiers.</p><p>Since these classifiers are based on category labels, the intention is to observe the similarity between the blog's content and hierarchical organized 'ontology' related keywords which are biased by human-beings.</p><p>The RelevanceWordnetFilter is also a thresholdbased classifier and consists of synonyms, hypernyms, hyponyms, etc. retrieved by Wordnet. The idea using these keywords to consider topical facets of blog from the view of lexical aspects, especially, to bridge the gap of concept problems, e.g. if various name entities refer to the same concept.</p><p>The RelevanceWortschatzFilter is also a thresholdbased classifier and works with co-occurrences retrieved by the webservice of Wortschatz. In doing so, the topical facets can be covered from the statistical point of view.</p><p>The RelevanceYahooFilter also uses a threshold to separate classes and contains keywords retrieved by Yahoo. Particularly, features are interesting from the commercial-intended view of properties relating to a topic, e.g. concerning the topic 'iPod' features such as battery, cables and other accessory items are expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Splog-specific classifiers</head><p>SimilarityPatternFilter. Previous work on detecting splogs has shown <ref type="bibr" coords="10,154.69,159.03,14.61,8.74" target="#b25">[26]</ref>, that spam blogs frequently have similar structures. In doing so, the link-based and temporal aspects are discarded. Based on the basic idea, this classifier can predict a blog non-relevant if the similarity score between title and content does not exceed the threshold.</p><p>The NGramTitleFilter scrutinizes the occurrence of one of Google Suggest's keywords and is used for splog detecting.</p><p>NGramBodyFilter. Historically, commercial websites use most frequently searched phrases of search engines to be better ranked by the search results. Based on this fact, the commercial-intended bloggers can also exploit this knowledge to better place their blogs in the search hits. Thus, the NGramBodyFilter classifier judges a blog as non-relevant when exceeding the threshold by the similarity score between the n-grams and the blog's content. On the one hand, this classifier can be used for removing splogs. On the other hand, it can also be applied for the contentbased retrieval. For instance, while 'mobile phone ring tones' as 4-gram relating to the topic 'mobile phone' is more commercial-intended, 'machine learning approaches' are highly topic-specific related to the topic 'machine learning'. However, first one is utilized.</p><p>The BlacklistTitleFilter is a simple classifier that has a set of hard-coded features to detect splogs, and analyzes the title on splog-related keywords.</p><p>The BlackBodyFilter is similar to BlacklistTitle-Filter, but is threshold-based and complex one. As mentioned in Section 5.2, the blacklist utilizes the numerical points as a weighting system. If the threshold is exceeded by respective sum of points, the classifier assesses the analyzing blog as non-relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Diversity of Classifers and Dissimilarity of Keyword Sources</head><p>The present section deals with diversity measures for designing a ensemble of classifiers and prompts the question, whether dis-similarity of the used key sources is crucial factor or whether diversity mea-sure of classifiers is final criteria for improving the retrieval performance. Based on the extensive findings of Kunchewa and Whitaker <ref type="bibr" coords="10,457.01,151.87,14.61,8.74" target="#b22">[23]</ref>, a general recommendation is derived for specifying the present implementation with regard to learning procedure and applying parameters. Since the architecture has been already implemented when running the TREC track task, this section provides information which is attended after submitting the results to TREC. Thus, the architecture and its underlying procedure is reflected in Section 8.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Dis-similarity of the Keywords References</head><p>This subsection is devoted to an overview of the mutual dis-similarities between the lists structured in the Topic Maps. The objective of this overview serves to underline the desirable effects of the keyword references in terms of complementing each another. To do so, the cosinus-similarity is mutually measured between all lists pairwise within the Topic Map for a certain topic. However, the similarity of a list itself is not considered. Also the hand-coded and intersection are not taken into account. To compare the similarities between lists over all topics suggested in TREC, a standardization is utilized as follows:</p><formula xml:id="formula_13" coords="10,330.34,469.05,204.67,27.27">sim ti = 1 T 1 N -1 t∈T j∈L\{i} sim(l i , l j ), (<label>7</label></formula><formula xml:id="formula_14" coords="10,535.01,475.79,4.24,8.74">)</formula><p>where N is number of set of lists L within a Topic Map, T is number of topics suggested in TREC, l with indices i and j are elements of set L.</p><p>As illustrated in Figure <ref type="figure" coords="10,424.44,546.39,3.88,8.74">3</ref>, the average mutual similarities are relative low. Also there is not much of difference between the similarities with and without Porter's stemming algorithm (see Section 6.1 for more details). Since the keyword resources are dis-similar to each other, one could conjecture that these keywords from various sources could complement each another. However, that illustration does not finally testify to leveraging the retrieval effectiveness and requires keywords that are adequate topical relevant to match the various facets of a blogs and to predict Figure <ref type="figure" coords="11,103.76,288.97,3.88,8.74">3</ref>: An Overview of Average Mutual Similarities between the Keyword References for All Topics Suggested in TREC the relevance of a feed. In this context, the diversity required by ensemble systems cannot be justified yet, based on the computed dis-similarities illustrated above. The dis-similarity can be interpreted as dependency between keyword sources. To address the question whether the dis-similarity between the keyword sources is a crucial factor for the retrieval effectiveness and classification performance, measuring the output performances by adaptive addition of classifiers can be helpful. Additionally, it is also interesting to find out, how the diversity differs from a set of dependent and independent classifiers. However, these questions can be answered with regard to the diversity of AdaBoost classifiers in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Diversity of the AdaBoost Classifiers</head><p>Diversity has been recognized as a very important characteristic in the research area of combining classifiers <ref type="bibr" coords="11,99.69,582.26,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="11,118.38,582.26,12.73,8.74" target="#b46">47,</ref><ref type="bibr" coords="11,134.31,582.26,11.63,8.74" target="#b20">21]</ref>. In this context, the key success of ensemble systems, as AdaBoost belongs to, is to build a set of diverse classifiers. However, as the study of Kuncheva and Whitaker reveals <ref type="bibr" coords="11,210.42,618.12,14.62,8.74" target="#b22">[23]</ref>, there is no strict separation between diversity, dependence, orthogonality or complementarity of classifiers, the focus of this subsection is to deal with some measures of diversity. Since many metrics exist, which can be catego-rized in pair-wise and non-pairwise ones, the present goal is not to extensively describe and compare all of these measures. Even if Kuncheva concludes, that there is no diversity measures that consistently correlates with higher accuracy, Entropy of the Votes and Q-statistic can be used for reflecting and arranging the present approach and system.</p><p>First of all, as many authors use the concept of diversity in terms of correct/incorrect (oracle) outputs, a denotation for the most simple pair-wise measure such as Q-statistic is utilized and illustrated in Table <ref type="table" coords="11,310.61,259.47,3.88,8.74" target="#tab_1">1</ref>. For T classifiers, T (T -1)/2 pairwise diversity measures can be computed and an overall diversity of a set of classifiers can be calculated by averaging these pairwise measures. Based on given hypotheses h i and h j , Q-statistic can be expressed as</p><formula xml:id="formula_15" coords="11,325.32,279.85,199.21,34.36">h i is correct h i is incorrect h j is correct a b h j is incorrect c d</formula><formula xml:id="formula_16" coords="11,358.24,442.78,181.01,9.65">Q i,j = (ad -bc)/(ad + bc)<label>(8)</label></formula><p>Q results positive (negative) values if the same instances are correctly (incorrectly) classified by both classifiers. Maximum diversity is obtained if Q is equal to 0.</p><p>Conversely to pairwise measuring, Entropy of the Votes (Entropy Measure) assumes that the diversity is highest if half of the classifiers are correct, and the remaining ones are incorrect. This measure is defined as</p><formula xml:id="formula_17" coords="11,325.16,589.34,214.09,30.32">E = 1 N N i=1 1 T -T /2 min{(ζ i , (T -ζ i )},<label>(9)</label></formula><p>where • • • is ceiling operator, N is the dataset cardinality, ζ is number of classifiers, which incorrectly classifies unlabeled instance x i . The Entropy Measure varies between 0 and 1, where 0 indicates no difference between the classifiers and highest diversity in the team of classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Measuring Diversity of Implemented Classifiers</head><p>In this section, both the diversity of a set of independent and dependent classifiers are separately measured. To do so, this subsection hat its focus on the comparison and initial recommendation for redesigning of the present setting in terms of applying the learning parameters of the classifiers and its underlying set of keywords. To declare the meaning of dependency and independency of classifiers, a set of dependent classifiers has low similarity between the sources of keywords, as illustrated in Figure <ref type="figure" coords="12,268.07,303.31,3.88,8.74">3</ref>. Otherwise they are independent and the order, in which they are reduced can be found in subsection 6.6. To do so, classifiers based on the sources such as Intersections, Blacklist and Similarity are not taken into account, since they are query-independent and have similar settings in terms of F 1 -measure or precision, etc. for all topics suggested by TREC's participating groups.</p><p>Figure <ref type="figure" coords="12,102.60,571.33,3.88,8.74">4</ref>: Diversity Measuring set of independent and dependent Classifiers</p><p>As illustrated in Figure <ref type="figure" coords="12,184.40,606.17,3.88,8.74">4</ref>, there are remarkable differences between both the non-pair wise and pair wise measures, which should be more individually detailed as follows.</p><p>First, relating to the dependent and independent set of classifiers, both measures are relative inconsis-tent in terms of diversity. While the Q-statistic refers to that dependent classifiers are more diverse than independent ones on an average, the Entropy disagrees with Q-statistic in this issue. However, the common agreement of both diversity measures is that the differences between independent and dependent set of classifiers are proportional relative similar over the six topics when not considering the algebraic sign.</p><p>Second, both the diversity measures reveal very contradictory results. While the Q-statistic arguments that the weak learner are low diverse and they incorrectly and correctly classify the same instances on an average, the Entropy Measure reveals that the classifiers are relative diverse and half of those are incorrect and correct when considering the value 0 as criteria of highest diversity in both cases. In particular, the extreme values can be found in the topics 'mobile phone' and 'photography'. The result returned for 'photography' can be biased by characteristics related to photography. On the one hand, since photography related blogs capture both blogs describing the techniques and methods of photography and photos with low content. On the other hand, the sources using Dmoz's categories are manually chosen and the resulting keywords are not retrieved by query using the terms contained in the topic's name entity, as described in Subsection 5.2.5.</p><p>Relating to the topic 'mobile phone', another assumption is taken up. Since the topic 'mobile phone' consists of two self-contained terms, which commonly can be independently used and refer to concepts within a concept group. However, they not only have a great intersection of common keywords, but also use separate terms for querying the feed collections, which results relative highly diverse segments of blogs. For instance, while querying 'phone' results blogs related to address-and phonebook resources, using 'mobile' to query the blog collections returns blogs relating to interest areas such as 'mobile phone', 'wireless computing', 'downloads of tone', 'movie for mobile', etc. Moreover, the sources such as Wordnet or Wortschatz, as described in subsections 5.2.1 and 5.2.2, are vulenerable to queries of combined concepts and terms. However, these are just possible assumption for the conflicting findings resulted by both the diversity measures, in particular, for the extreme val-ues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Initial Recommendation for (Re-</head><p>)Designing the Architecture</p><p>The inconsistent findings measuring the diversity of implemented classifiers and the dis-similarities between sources cannot be utilized as the final criteria correlated to the crucial factor of the present approach. However, the Q-statistic measure can be initially used to detect common blogs for resampling training data sets and to imagine, how well these can complement each another and to combine these classifiers. Moreover, the Q-statistic has been proposed as measure of (dis-)similarity in the numerical taxonomy literature <ref type="bibr" coords="13,138.61,309.00,14.62,8.74" target="#b52">[53]</ref>. In contrast to that, Entropy of the Votes can be utilized for an overall performance evaluation, based on the overall correctness and incorrectness of the classifiers. However, an important review of the present setting of measurement is advisable, based on the disagreements of both the diversity measures. The classifiers have not similar setting of accuracy parameter values, as Kuncheva and Whitaker utilize and recommend in their extensive analysis of diversity measures. In this context, they use classifiers with high similarity in terms of accuracy characteristics, e.g. related to precision or recall, for spanning the largest possible interval for the diversity measures. Based on this idea, classifiers should be used considering similar accuracy peculiarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Architecture</head><p>To imagine how the proposed approach is implemented at the time of performing the TREC Blog run, the architecture and workflow is illustrated in Figure <ref type="figure" coords="13,104.45,584.94,3.88,8.74" target="#fig_1">5</ref>. The general recommendation derived, as mentioned above, is taken up in Section 8.2.1</p><p>First, as Mishne reveals that indexing RSS content is better than the entire HTML content <ref type="bibr" coords="13,242.65,621.54,14.61,8.74" target="#b32">[33]</ref>, feeds are indexed with Lucene<ref type="foot" coords="13,161.39,631.92,7.94,6.12" target="#foot_11">13</ref> for proposing topics and storing blog contents. Second, to gather and to structure keywords from various sources in a Topic Map, keywords are extracted from different sources via their underlying API. Third, to determine the thresholds of complex classifiers, the threshold estimation procedure is iterated by trading off precision and recall. Thus, the threshold is taken when the F 1 -measure cannot be improved.</p><p>The weighting and testing of these classifiers is the most labour-intensive part of the implemented architecture. The weighting procedure can be described as the sequential training of all classifiers, whereas the weight of one classifier depends on the weight distribution of previous ones. Therefore, the order, in which the classifiers are trained, influences the combined hypothesis and results. In this regard, some efforts have to be invested in experimenting the order in the weighting and testing process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Prepossessing</head><p>To improve Information Retrieval (IR) performance, stemming and filtering methods are used for the implemented prepossessing steps. The filtering step removes stop words using hand-coded stop-list for English. Among the others, Stop words' removal is used for analyzing the blog entries within the Lucene In-dexing Tool (Luke) <ref type="foot" coords="14,156.87,126.39,7.94,6.12" target="#foot_12">14</ref> and similarity measuring process between the keywords of the Topic Maps and the content of each blog within a feed.</p><p>Relating to the stemming step, a Java<ref type="foot" coords="14,258.20,162.84,7.94,6.12" target="#foot_13">15</ref> library 'Snowball English Stemmer' is utilized, which is based on Porter's stemming algorithm <ref type="bibr" coords="14,253.76,188.32,14.62,8.74" target="#b41">[42]</ref>. The Porter's stemming method belongs to conflation algorithms, in particular, to the category of suffix removal. It is intuitive and the most used stemming algorithm, since it is simple with regard to implementation and compactness <ref type="bibr" coords="14,197.83,248.10,14.62,8.74" target="#b56">[57]</ref>. Additionally, Hull points out in a detailed evaluation that the Porter's stemmer is the one of best stemming algorithms with regard to average precision and recall metrics <ref type="bibr" coords="14,282.38,283.97,14.61,8.74" target="#b19">[20]</ref>. Lennon et al. also reveal, that there are relatively small differences among the conflation methods in terms of compression percentage and retrieval effectiveness <ref type="bibr" coords="14,109.19,331.79,14.62,8.74" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Indexing and Querying Blogs with Lucene</head><p>The TREC collection of documents (feeds) amounts to over 25 GB, as described in <ref type="bibr" coords="14,215.60,408.19,14.62,8.74" target="#b26">[27]</ref>. To effectively search and query feeds, in particular, blog entities within feeds, the Lucene text search engine is used. Lucene is implemented in Java and consists of a set API providing high-performance, full-featured ranked searching functionality. Because it is high efficiency and usable on cross-platforms, it has been widely used in many applications to provide full text search functionality <ref type="bibr" coords="14,160.64,503.83,14.61,8.74" target="#b16">[17]</ref>. One of the most important advantages refers to the various ways to index a feed. For instance, the content fields such as 'title', 'description', etc. associating to a document can be specified for indexing and storing, while other adjunctive fields such as 'feedno' and 'feedurl', which are irrelevant to the content, can be just stored without being indexed. The indexing process is accompanied by a set of filters such as stemming, removal of stop words and tokenization when it is required.</p><p>Another most important advantage is the field search. For example, using 'title:music' as query, one can simply search documents with the title containing the named-entity 'music'. Lucene also provides usual Boolean operands such as 'AND', 'OR' and 'NOT' to make complex queries for matching documents. The querying process of fields also goes through the same set of filters as they have been indexed before. Additionally, the searching speed only amounts to relative few milliseconds by matching 1000 of blog entries. Moreover, there are additional ranking features using default similarity measure and TF-IDF model <ref type="bibr" coords="14,310.61,259.47,14.62,8.74" target="#b21">[22]</ref>. Also there are boosting factors for subsequently manipulating the relevance of documents. However, apart from the feed number and the feed URL, indexing feeds is limited to the level of blogs, additional properties of a feed such as permalinks, etc. are not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generating Topic Maps Using TopicMapBuilder</head><p>The TopicMapBuilder is responsible for querying and extracting keywords from various sources using their APIs. To do so, the keywords related to a topic are structured in the lists of a Topic Map. Via the Java Architecture for XML Binding (JAXB), it also manages the Topic Maps to serialize Java Objects to XML data (storing) and to deserialize XML data to Java Objects (loading). An additional feature of JAXB is that it can enable to talk to XSLT, DOM, dom4j, XML-aware database, and many existing libraries <ref type="foot" coords="14,528.04,493.68,7.94,6.12" target="#foot_14">16</ref> . Thus, the stored Topic Maps can be re-used for other applications and system in terms of IR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Managing Classification</head><p>The implemented Class ManageClassification has interfaces to Prepossessing, TopicMapBuilder and Lucene querying API, and unifies their functionalities. ManageClassification can retrieval the indexed blog documents from the TREC collection and the keywords from certain Topic Maps. To do so, the similarity score between terms of a document and the keywords can be measured by using prepossessing steps such as filtering and stemming. Moreover, via XStream API <ref type="foot" coords="15,149.79,150.30,7.94,6.12" target="#foot_15">17</ref> , a simple JAVA library to serialize and back again, ManageClassification can load and store the information of the AdaBoost Classifiers. These information refer to the weights and thresholds of each classifier as well as the evaluation result of feeds ranked by the implemented AdaBoost classification algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Estimating Appropriate Thresholds of Classifiers</head><p>To fix the denotation, the threshold t r of a classifier, chosen via iterative trading off recall and precision, can be interpreted as a fixed parameter determining the relevance f r ∈ [0, 1] of a blog document b, based on its similarity to the keywords structured in a certain list l of a Topic Map sim(b, l). Thus, it can expresses as the following formula:</p><formula xml:id="formula_18" coords="15,115.67,388.10,184.98,23.08">f r = 1, if sim(b, l) &gt; t r 0, others<label>(10)</label></formula><p>Since thresholds are not only utilized for the estimation of relevant blog documents, but also for spam detection of blogs, it is important to deal with another denotation for this issue. The threshold t s of a spam-specific classifier is defined as a fixed parameter (chosen as mentioned above) determining the spamrelatedness f s ∈ [0, 1] of a blog document b, based on its similarity to the spam-specific keywords structured in Blacklist's or Google Suggest's keywords sources l of a Topic Map sim(b, l). Thus, it can denoted as follows:</p><formula xml:id="formula_19" coords="15,115.82,576.60,184.82,23.08">f s = 1, if sim(b, l) &gt; t s 0, others<label>(11)</label></formula><p>However, this subsection has its focus on the determining procedure of appropriate thresholds. In fact, the process exploits the optimum performance between the precision and recall using F 1 -measure. An algorithm optimizing parameters of each thresholdbased classifiers is illustrated in Algorithm 2.</p><p>Algorithm 2 An Algorithm for Determining the Threshold of Classifier Input : Set of classifiers C and blogs D Init : curF 1 c = 0.0, prevF 1 c = 0.0, curT hreshold c = 0.6, prevT hreshold c = 0.0, round = 1</p><formula xml:id="formula_20" coords="15,320.57,265.09,218.68,153.14">while round &gt; 0 OR curF 1 c -prevF 1 c &gt; 0.0002 do prevT hreshold c = curT hreshold c prevF 1 c = curF 1 c curF 1 c = CalculateF 1(c, D) if prevF 1 c &gt; curF 1 c then curT hreshold c = curT hreshold c * 1.15 else curT hreshold c = curT hreshold c * 0.97 end if round -1 end while curT hreshold c = prevT hreshold c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Weighting AdaBoost Classfiers</head><p>This module is responsible for learning weights of the implemented classifiers and is closely connected with iterative changing and arranging parameters in terms of the thresholds of classifiers, training iterations, the size and resampling of training data set. In particular, the focus lies on balancing the appropriate settings for the learning process of the implemented AdaBoost classifiers. Also the variation of the order of weak learner is focal point of this part of the architecture. For more details to the weighting algorithm, see Section 4.2</p><p>The order, as mentioned above, in which a committee of classifiers are trained, has impacts on the output of the final classifier. Analyzing weighting output on the set of training data in terms of six topics ('music', 'Christmas', 'movie', 'food', 'mobile phone' and 'photography'), it is observed that using spam-detection, followed by title-based and contentbased, performs a appropriate weighting result. The ascending order of classifiers is illustrated in iii. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Testing AdaBoost Classfiers</head><p>The testing unit utilizes the scores of blogs assigned by the learned AdaBoost classifiers, as illustrated in Section 4.2, to rank the feeds. To do so, the scores of all blogs within a feed is summarized and averaged. Thus, the average score can be used as the criteria for ranking feeds in the test data set and is illustrated as follows:</p><formula xml:id="formula_21" coords="16,115.47,647.76,185.17,30.32">AverageScore j = 1 N N i V j ,<label>(12)</label></formula><p>where N is number of blogs within a feed, j is an unlabeled instance (feed) of test data and V is weighted majority voting score V = m l log 1 β l . As derived from AdaBoost.M1 in Algorithm 1, m is number of classifiers deployed for the current testing and β is the learned weight of a classifier.</p><p>Moreover, the testing unit serves to validate the prediction performance of the trained classifiers. As in all classification issues, the most important rule for creating a great predictor is that examples of test data may not exist in training data set. Thus, only applying of trained classifiers to unseen data can assure the generalization of hypotheses and prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Training and Experimental Settings</head><p>This section deals with the issue in terms of training and test data set. The size of the training and test data set varies from topic to topic, since some topics are preferred by the bloggers and depend on seasonal issues, e.g. 'Christmas'. Additionally, the size of the data set is different between the TRECrelated and improved training and testing settings, which are more detailed in the following subsections. However, the iteration for weight learning process for all topics and classifiers is limited to 20 rounds, since AdaBoost's error rate rapidly decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">TREC-related Training and Testing Settings</head><p>Since the TREC document collection already provides categorized feeds, the blogs within the feeds are indexed with Lucene. Thus, a new collection of data set consists of blogs with categories provided by the feed and is created for six topics such as 'music', 'Christmas', 'food', 'movie', 'mobile phone' and 'photography'. However, as <ref type="bibr" coords="16,414.36,606.17,15.50,8.74" target="#b26">[27]</ref> reveal that the blogs are infected by splogs, manual analyzing as well as separating splogs and non-relevant blogs are mandatory. Splogs are also important for learning splog-related classifiers, therefore the collection is additionally labeled with a new category 'blacklist' and can be iden-tified by 'true' and 'false'. Depending on desired setting of the training and test data, the splog-specific and relevance determining classifiers such as titleand content-based ones can be separately trained and tested.</p><p>By participating in TREC Blog Track, the competition with other participating groups not only plays a important role, but also the underestimated time. In addition, performing the run requires more than one person. Therefore only one result and first test is submitted when the Blog Track run is due. In this regard, weights and thresholds determined by six topics such as 'Christmas', 'music', 'photography', 'food' and 'mobile phone', are utilized for the remaining 40 topics. Based on the idea of generalizing the proposed approach, the classifiers are initialized with these weights and thresholds mentioned above for the topical classification of the remaining topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Improved Training and Testing Settings</head><p>Exploiting the advantage of Lucene's TF-IDF scoring, the training data set varies from 40% to 60%.</p><p>The assumption is based on the fact, that the classifiers should learn from relative good example set. Moreover, since there is a test data set judged by the participating groups of this annual Blog Track, the test data set should have more evaluative accuracy than the manual labeled one. Also orientation of performance output of the implemented approach towards the average precision of testing output is a great chance for detecting bugs and errors in learning and improving process of both the classifiers and keyword sources.</p><p>The thresholds and weights of the classifiers estimated for one of six topics mentioned above are used for all remaining 44 topics. In this context, the best one result of one topic is chosen from all six test series.</p><p>Figure <ref type="figure" coords="17,114.91,606.17,4.98,8.74">6</ref> points out, how many percent of feeds has been be considered at the time of performing the TREC Blog Track run and submitting the results. Additionally, re-extracting docs feeds does not result in existing complete number of feeds in the present testing environment. In particular, there are many Figure <ref type="figure" coords="17,380.49,276.92,3.88,8.74">6</ref>: Missing Feeds in Percent relevant documents, which are still missing. Hence, evaluation of performance of the present prediction system has be taken this into account and has to be modified with regard to evaluation metric such as the number of relative documents for calculating the average precision. This modification is explained more detailed in Section 8.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation Results</head><p>In this section, both the TREC-related and inofficial evaluation results are depicted. Since implementing the proposed approach is accompanied by bugs in terms of quality of the keyword references and implementing AdaBoost classifiers, the differential dealing of both results is advisable. Particularly, qualitative arguments with regard to the evaluation results are illustrated. Also the essential improvements to increase the retrieval and classification results are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">TREC-related Evaluation</head><p>Since the implemented system was in beta-version when the TREC was due, the evaluation results were not convincing, as shown in Figure <ref type="figure" coords="17,461.91,630.08,3.88,8.74" target="#fig_3">7</ref>. In this context, mistakes by performing the run have been identified and removed in regard with the keyword aspects and weighting AdaBoost classifiers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Inofficial Evaluation and Reconsidering of the Proposed Approach</head><p>This subsection points out, which essential arrangements of the implementation are necessary to improve the prediction performance of the AdaBoost classifiers and judge the proposed approach. In the second subsection, performance results are separately illustrated for dependent and independent set of classifiers considering results submitted by other participating groups of the TREC Blog run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Reflecting Keyword Resources and Implemented Classifiers</head><p>This subsection reflects the configuration of classifiers in terms of weighting and testing environments, based on the findings of diversity measures, as depicted in subsections 5.4. One of the most important finding refers to the original belief that dis-similarity of keyword sources could be equal diversity of classifiers. In this regard, the classifiers utilize the relative dis-similar lists of keywords to compute the similarity and to learn the weights. To do so, the possibility, that the thresholds of certain classifiers are exceeded by the same intersection of similar keywords, is just discarded. In particular, this intersection is greater when taken the stemming process into account. However, even if the similarity of a keyword list to the remaining lists of a Topic Map is relative low, as exhibited in Figure <ref type="figure" coords="18,531.49,139.92,3.88,8.74">3</ref>, but is greater than the thresholds used by the underlying classifiers. Hence, the question arises, whether the dis-similarity can completely replace the diversity of implemented classifiers or in worst case defeat the boosting effects of AdaBoost. To face this problems, a new set of independent classifiers is created. In this context, all lists of each Topic Map have to be revised in terms of redundancy of retrieved keywords. Therefore, the revised condition is: 'Each keyword of a list must not exist in another remaining list of a Topic Map'. Additionally, since some topics are more vulnerable to spam, e.g. 'mobile phone' or 'music', etc., the hand-coded blacklist's keywords can be used for removing splog-specific keywords from the remaining lists of a Topic Map, which are utilized for the relevance determination. The order, in which a list's keywords are removed, has influences on the selecting and estimating the thresholds and at least on the weights, but it does not matter, if this order is kept in the threshold determining and weight learning process.</p><p>However, in Section 8, the results of prediction performance are separately analyzed and illustrated for both dependent and independent set of classifiers.</p><p>Relating to the findings of diversity measures the conditions in terms of the size of classifiers' team is arranged and depends on the setting recommended by <ref type="bibr" coords="18,310.61,475.43,14.62,8.74" target="#b22">[23]</ref>. In this context, weak learners with similar accuracy characteristics produce the final set of classifiers. Based on experiments and observations related to the parameters of six topics, the conditions of precision and F 1 -measure differ from topic to topic, as illustrated in Table <ref type="table" coords="18,379.42,535.21,3.88,8.74" target="#tab_2">2</ref>, where c i ∈ C and C is the created set of weak learners. To do so, the final condition is the unification of both ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Inofficial Performance Results</head><p>Since the comparison of the present prediction performance with the 'Best'-result is not appropriate, the following evaluations refer to the Median of performance results judged by the participating groups of TREC. Additionally, the results are presented in un-revised and revised version of performance out- As illustrated in Figure <ref type="figure" coords="19,185.06,324.29,4.98,8.74" target="#fig_4">8</ref> and<ref type="figure" coords="19,211.55,324.29,3.88,8.74" target="#fig_5">9</ref>, both the set of dependent and independent classifiers outperforms the 'Median' in over 14 topics. Also the hard-to-classify topics are better predicted by the implemented classification system. Interestingly, relating to the topics, which have high average precision resulted by Median, the proposed approach only results poor performance.   <ref type="figure" coords="19,442.23,346.52,8.49,8.74" target="#fig_6">10</ref>, there are not much of difference between the revised and un-revised version of average precision. The system of independent classifiers achieves more poor performance of average precision than the system of dependent ones, as exhibited in Figure <ref type="figure" coords="19,398.03,406.29,8.49,8.74" target="#fig_7">11</ref>. However, the differences between dependent and independent versions are critical in terms of performance output in the range between 0.04925 and 0.24625 when comparing both the figures 10 and 11.  Apart from 8 topics, the remaining topics are better predicted by the committee of dependent classifiers, as shown in Figure <ref type="figure" coords="20,187.02,358.40,8.49,8.74" target="#fig_8">12</ref>. However, high differences between both the sets can be found in 9 topics.</p><p>Based on the view of separated bins of average precision, as illustrated in Figure <ref type="figure" coords="20,206.13,395.52,9.96,8.74" target="#fig_8">12</ref> and Figure <ref type="figure" coords="20,270.60,395.52,8.49,8.74" target="#fig_7">11</ref>, the dependent prediction system achieves a better performance output when considering the last three bins. However, the disadvantage is that this illustration discards the topic-specific characteristics and splogspecific vulnerability.  Definitely, Figure <ref type="figure" coords="20,404.23,325.70,9.96,8.74" target="#fig_10">14</ref> and Figure <ref type="figure" coords="20,474.67,325.70,9.96,8.74" target="#fig_11">15</ref> reveal that there is no correlation between Median and the implemented system for both the independent and dependent sets. To address the question how the independent and dependent set of classifiers are related to each another, the interpretation of Figure <ref type="figure" coords="20,497.04,385.48,9.96,8.74" target="#fig_12">16</ref> can be helpful. Given the average precision of a dependent set of classifiers one can conjecture that the average precision of independent classifiers has a similar characteristic. In other words, both the systems archieve more or less similar performance results when discarding the question to which parts of the topics they generally match and better perform.   Although the presented evaluation results are not convincing, one can assume that the present approach using APIs of emerging technologies of Web 2.0 and different thesauri is an appropriate method to perform this annual Blog Track task 'Feed Distillation'. Apart from the infinite definition of diversity, the diversity of classifiers could be fulfilled by combining the sources considering both the semantic and statistical aspects related to a certain topic. However, the title-and content-based classifiers refer to the terms used in the blogs within a feed. In fact, the diversity required by ensemble-based systems has not only to base on the content-based facets, but also has to consider the structural properties of blogs such as incoming and outgoing hyperlinks, as proposed by <ref type="bibr" coords="21,512.00,318.16,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="21,526.52,318.16,12.72,8.74" target="#b38">39]</ref> for topic distillation of web sites.</p><p>Additionally, the performance of the implemented algorithm varies from topic to topic. This issue can be caused by the diversity of the topics suggested by the participating groups. In this context, the 45 topics of TREC do not only contain general concepts such as music, food, etc., but also specific concepts such as 'Buffy', 'Lost TV', which can only be extracted by search engines such as Yahoo, Dmoz or Google Suggest. However, extracting terms of specific concepts leads to the problem that the semantic resources such as Wordnet and Wortschatz cannot provide queries based on the fusion of terms. Separate extracting terms from parts of the topic generates an alienation of origin concept.</p><p>Moreover, blogs deal with individual diaries and stories using narrative terms, from the individual perspective of bloggers. This prompts to the question, how high is the similarity between the commonly used keyword sources and the emerging technologies of Web 2.0, which blogs and feeds belong to. However, relating to a general applicability of topicrelated concepts and terms for the feed distillation, further research should not only focus on evaluating the quality of commonly used keyword sources such as Wortschatz, Yahoo, Google and Dmoz, but also blog search engines such as Technorati, Blogger, Bloglines, etc, which could provide keywords discovering the narrative aspects of bloggers.</p><p>Another one of the most important findings refers to the diversity measures and performance results provided by a set of dependent and independent predictors. In this context, pairwise and non-pairwise diversity measures such as Q-statistic and Entropy Measure have contradictory results in terms of the diversity of the overall system. Additionally, the correlation between the prediction success and diversity of classifiers cannot be consistently reproduced for all topics. While some hard-to-classify topics are better predicted by dependent predictors, other topics are better classified by independent ones. In addition, splogs impede the determining of relevant blogs. This non-straightness of prediction success over all topics concludes that a pre-clustering of blogs, which are more or less vulnerable to splog, is necessary, e.g. while the topic 'machine learning' is less vulnerable to this issue, the topic 'music' or 'mobile phone' are highly infected by splogs using terms such as 'download', 'free', etc. However, the set of dependent classifiers results a better performance than the committee of independent classifiers on an average. Therefore, in the context of blog distillation, designing a set of dependent, content-based classifiers with low similarity is recommended. To do so, classifiers with similar accuracy characteristics can be used as an initial prototype system before scrutinizing the diversity of classifiers extensively.</p><p>Finally, the implemented AdaBoost.M1 cannot adequately consider the proportionality of relevant and non-relevant blogs as well as splogs and non-splogs within a feed. Since the final score of AdaBoost applying to a feed is limited to the blog's level and is the result of averaging summarized scores of blogs within a feed, it is also interesting to know whether there is another scoring method, which alternatively achieves the same or better accuracy. Based on the findings by <ref type="bibr" coords="22,120.98,570.30,14.62,8.74" target="#b11">[12]</ref>, that the boosting effects of AdaBoost can be defeated by classification noise and the error rates of the individual classifiers become very high, experiments considering the diversity measures can be used for adjusting this noise and ranking the feeds. Therefore, further work on improving the proposed approach has to consider an appropriate normalization factor for stabilizing the error variances biased by the various facets of a topic and its underlying splog-specific characteristics, which could reduce the boosting effects a la major votes within a feed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,310.61,557.06,228.64,8.74;7,310.61,569.02,30.22,8.74;7,310.61,374.66,228.95,167.29"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of Topic Map for the topic 'music'</figDesc><graphic coords="7,310.61,374.66,228.95,167.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="13,310.61,519.30,228.65,8.74;13,310.61,531.26,39.60,8.74;13,310.61,346.07,228.94,158.12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Workflow and architecture of the proposed approach</figDesc><graphic coords="13,310.61,346.07,228.94,158.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="18,72.00,275.01,228.65,8.74;18,72.00,286.97,196.01,8.74;18,72.00,124.80,228.95,135.10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Topics Submitted by TREC and Sorted in Descending Order of Average Precision (AP)</figDesc><graphic coords="18,72.00,124.80,228.95,135.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="19,72.00,579.80,228.64,8.74;19,72.00,591.76,228.65,8.74;19,72.00,603.71,103.34,8.74;19,310.61,475.97,228.94,136.47"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Topics judged by the Committee of Dependent Classiferis and Sorted in Descending Order of Average Precision (AP)</figDesc><graphic coords="19,310.61,475.97,228.94,136.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="19,310.61,277.11,228.64,8.74;19,310.61,289.06,228.65,8.74;19,310.61,301.02,114.68,8.74;19,310.61,124.80,228.96,137.20"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Topics Judged by the Committee of Independent Classifiers and Sorted in Descending Order of Average Precision (AP)</figDesc><graphic coords="19,310.61,124.80,228.96,137.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="19,310.61,627.56,228.65,8.74;19,310.61,639.51,228.64,8.74;19,310.61,651.47,189.30,8.74;19,72.00,426.70,228.94,137.99"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Prediction Performance Results Divided in AP Bins for Comparing the Dependent and Independent Classifiers Considering the Median</figDesc><graphic coords="19,72.00,426.70,228.94,137.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="20,72.00,276.26,228.64,8.74;20,72.00,288.21,228.65,8.74;20,72.00,300.17,201.18,8.74;20,72.00,124.80,228.94,136.34"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Difference between Un-revised and Revised Performance Results Related to the Median Considering the Set of Independent Classifiers</figDesc><graphic coords="20,72.00,124.80,228.94,136.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="20,72.00,630.08,228.64,8.74;20,72.00,642.03,228.64,8.74;20,72.00,653.99,228.64,8.74;20,72.00,665.94,42.79,8.74;20,72.00,478.49,228.95,136.47"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Difference of Revised Prediction Performance Results between the Set of Dependent and Independent Classifiers, Based on the Descending Order of AP</figDesc><graphic coords="20,72.00,478.49,228.95,136.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="20,310.61,277.47,228.64,8.74;20,310.61,289.43,228.64,8.74;20,310.61,301.38,228.64,8.74;20,310.61,124.80,228.95,137.55"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Difference of Revised Prediction Performance Result between the Set of Dependent and Independent Classifiers, Based on the View of AP Bins</figDesc><graphic coords="20,310.61,124.80,228.95,137.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="20,310.61,642.03,228.64,8.74;20,310.61,653.99,228.65,8.74;20,310.61,665.94,32.38,8.74;20,310.61,490.27,228.94,136.65"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Correlation of Prediction Performance Results between the Set of Dependent Classifiers and Median</figDesc><graphic coords="20,310.61,490.27,228.94,136.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="21,72.00,277.47,228.64,8.74;21,72.00,289.42,228.65,8.74;21,72.00,301.38,32.38,8.74;21,72.00,124.80,228.96,137.55"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Correlation of Prediction Performance Results between the Set of Independent Classifiers and Median</figDesc><graphic coords="21,72.00,124.80,228.96,137.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="21,72.00,593.57,228.64,8.74;21,72.00,605.52,228.64,8.74;21,72.00,617.48,17.79,8.74;21,72.00,441.63,228.94,136.82"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Correlation of Prediction Performance between the Set of Dependent and Independent Classifiers</figDesc><graphic coords="21,72.00,441.63,228.94,136.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,310.61,326.42,228.64,20.69"><head>Table 1 :</head><label>1</label><figDesc>Denotation for Pairwise Measuring Diversity of Classifiers</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="19,72.00,126.76,228.65,194.31"><head>Table 2 :</head><label>2</label><figDesc>Topics c i &gt; F 1 -measure c i &gt;Precision Conditions for Set Formation of Weak Learners</figDesc><table coords="19,80.08,139.12,192.51,70.51"><row><cell>Christmas</cell><cell>0.20</cell><cell>0.20</cell></row><row><cell>Food</cell><cell>0.45</cell><cell>0.50</cell></row><row><cell>Movie</cell><cell>0.20</cell><cell>0.45</cell></row><row><cell>Mobile phone</cell><cell>0.20</cell><cell>0.50</cell></row><row><cell>Music</cell><cell>0.20</cell><cell>0.50</cell></row><row><cell>Photography</cell><cell>0.20</cell><cell>0.45</cell></row></table><note coords="19,72.00,264.52,228.65,8.74;19,72.00,276.47,228.65,8.74;19,72.00,288.43,228.65,8.74;19,72.00,300.38,228.64,8.74;19,72.00,312.34,143.55,8.74"><p>puts. They differ from each other in terms of reduced number of relevant feeds, based on the subtraction of missing number of feeds. Moreover, the evaluation deals with results performed by both the set of indepedent and dependent classifiers.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="1,325.85,667.30,149.84,6.99"><p>http://en.wikipedia.org/wiki/Spam blog</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,325.85,667.30,180.71,6.99"><p>http://searchengineland.com/070209-164512.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,87.24,648.29,138.89,6.99"><p>http://www.oasis-open.org/docbook/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="7,87.24,657.80,139.65,6.99"><p>http://www.w3.org/MarkUp/SGML/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="7,87.24,667.30,99.60,6.99"><p>http://www.w3.org/XML/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="8,87.24,629.29,113.40,6.99"><p>http://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="8,87.24,638.79,121.67,6.99"><p>http://wortschatz.uni-leipzig.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="8,87.24,648.29,193.66,6.99"><p>http://google.com/complete/search?output=toolbar</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="8,87.24,657.80,90.79,6.99"><p>http://www.yahoo.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="8,87.24,667.30,85.39,6.99"><p>http://www.dmoz.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="8,325.85,667.30,129.15,6.99"><p>http://en.wikipedia.org/w/api.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="13,87.24,667.30,91.27,6.99"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12" coords="14,87.24,657.80,104.21,6.99"><p>http://www.getopt.org/luke</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13" coords="14,87.24,667.30,104.56,6.99"><p>http://snowball.tartarus.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14" coords="14,325.85,667.30,205.83,6.99"><p>http://www.xml.com/pub/a/2003/01/08/jaxb-api.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_15" coords="15,87.24,667.30,106.19,6.99"><p>http://xstream.codehaus.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="22,329.92,191.75,209.32,7.86;22,329.92,202.71,209.33,7.86;22,329.92,213.67,209.33,7.86;22,329.92,224.63,21.00,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="22,374.77,191.75,164.47,7.86;22,329.92,202.71,205.50,7.86">Optimizing Ranking Functions a Connectionist Approach to Adaptive Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bartell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>University of California, San Diego</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="22,329.92,239.24,209.32,7.86;22,329.92,250.20,209.33,7.86;22,329.92,261.15,209.32,7.86;22,329.92,272.11,209.33,7.86;22,329.92,283.07,209.33,7.86;22,329.92,294.03,51.81,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="22,502.78,239.24,36.46,7.86;22,329.92,250.20,209.33,7.86;22,329.92,261.15,209.32,7.86;22,329.92,272.11,209.33,7.86;22,329.92,283.07,204.87,7.86">ISO/IEC 13250: 2000 Topic Maps: Information Technology-Document Description and Markup Language. International Organization for Standarization (ISO) and International Electrotechnical Commission (IEC)</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Biezunski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Newcomb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-12-01">Dec, 1, 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,308.64,209.33,7.86;22,329.92,319.60,209.33,7.86;22,329.92,330.56,209.33,7.86;22,329.92,341.52,137.87,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="22,503.27,308.64,35.98,7.86;22,329.92,319.60,33.75,7.86">Topics in 0-1 data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seppänen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,373.23,319.60,166.02,7.86;22,329.92,330.56,209.33,7.86;22,329.92,341.52,46.45,7.86">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="450" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,356.13,209.33,7.86;22,329.92,367.09,148.06,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m" coord="22,374.76,356.13,160.44,7.86">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,381.70,209.32,7.86;22,329.92,392.66,209.33,7.86;22,329.92,403.62,77.82,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="22,384.02,381.70,155.22,7.86;22,329.92,392.66,70.68,7.86">How blogging software reshapes the online community</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Blood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,414.13,392.66,119.12,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="53" to="55" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,418.23,209.32,7.86;22,329.92,429.19,209.33,7.86;22,329.92,440.15,21.00,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="22,395.30,418.23,38.14,7.86">Relevance</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,446.50,418.23,92.74,7.86;22,329.92,429.19,143.59,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="269" to="273" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,454.76,209.33,7.86;22,329.92,465.72,209.33,7.86;22,329.92,476.67,209.32,7.86;22,329.92,487.63,209.33,7.86;22,329.92,498.59,84.02,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="22,403.18,454.76,136.07,7.86;22,329.92,465.72,209.33,7.86;22,329.92,476.67,127.37,7.86">Integrating the document object model with hyperlinks for enhanced topic distillation and information extraction</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,466.51,476.67,72.73,7.86;22,329.92,487.63,204.27,7.86">Proceedings of the tenth international conference on World Wide Web</title>
		<meeting>the tenth international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,513.20,209.33,7.86;22,329.92,524.16,209.32,7.86;22,329.92,535.12,155.79,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="22,383.01,513.20,156.24,7.86;22,329.92,524.16,41.17,7.86">On selecting a measure of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,378.95,524.16,160.29,7.86;22,329.92,535.12,70.87,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="100" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,549.73,209.33,7.86;22,329.92,560.69,141.13,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="22,474.86,549.73,64.39,7.86;22,329.92,560.69,111.62,7.86">An introduction to Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,575.30,209.32,7.86;22,329.92,586.26,209.33,7.86;22,329.92,597.22,209.33,7.86;22,329.92,608.18,209.33,7.86;22,329.92,619.14,209.33,7.86;22,329.92,630.10,21.00,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="22,473.14,575.30,66.10,7.86;22,329.92,586.26,209.33,7.86;22,329.92,597.22,56.14,7.86">Diversity versus Quality in Classification Ensembles Based on Feature Selection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,400.23,597.22,139.03,7.86;22,329.92,608.18,205.12,7.86">Machine Learning: ECML 2000: 11th European Conference on Machine Learning</title>
		<meeting><address><addrLine>Barcelona, Catalonia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06-02">May 31-June 2, 2000, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,329.92,644.71,209.32,7.86;22,329.92,655.66,209.33,7.86;22,329.92,666.62,99.73,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="22,466.68,644.71,72.56,7.86;22,329.92,655.66,142.01,7.86">Identifying Facets in Query-Biased Sets of Blog Posts</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,481.40,655.66,32.75,7.86">ICWSM</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<pubPlace>Boulder, CO USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,128.64,209.33,7.86;23,91.32,139.60,209.33,7.86;23,91.32,150.56,209.33,7.86;23,91.32,161.52,160.54,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="23,161.32,128.64,139.32,7.86;23,91.32,139.60,209.33,7.86;23,91.32,150.56,204.78,7.86">An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,91.32,161.52,70.87,7.86">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="139" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,175.56,209.33,7.86;23,91.32,186.52,209.33,7.86;23,91.32,197.47,209.33,7.86;23,91.32,208.43,171.14,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="23,234.85,175.56,65.79,7.86;23,91.32,186.52,209.33,7.86;23,91.32,197.47,14.02,7.86">Fast SVM training algorithm with decomposition on very large data sets</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,116.50,197.47,184.14,7.86;23,91.32,208.43,81.93,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="618" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,222.47,209.32,7.86;23,91.32,233.43,209.32,7.86;23,91.32,244.39,159.66,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="23,206.16,222.47,94.48,7.86;23,91.32,233.43,30.80,7.86">The power and politics of blogs</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Drezner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Farrell</surname></persName>
		</author>
		<ptr target="http://www.danieldrezner.com/research/blogpaperfinal.pdf" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,258.43,209.33,7.86;23,91.32,269.39,209.33,7.86;23,91.32,280.34,209.33,7.86;23,91.32,291.30,82.43,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="23,201.99,258.43,98.65,7.86;23,91.32,269.39,209.33,7.86;23,91.32,280.34,32.81,7.86">A decision-theoretic generalization of online learning and an application to boosting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,131.09,280.34,165.56,7.86">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,305.34,209.33,7.86;23,91.32,316.30,209.32,7.86;23,91.32,327.26,133.72,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="23,206.16,305.34,94.48,7.86;23,91.32,316.30,32.81,7.86">A short introduction to boosting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,133.33,316.30,167.31,7.86;23,91.32,327.26,44.51,7.86">Journal of Japanese Society for Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="771" to="780" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,341.30,209.33,7.86;23,91.32,352.26,209.33,7.86;23,91.32,363.22,209.33,7.86;23,91.32,374.17,209.32,7.86;23,91.32,385.13,136.01,7.86" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Harkema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<title level="m" coord="23,266.84,341.30,33.80,7.86;23,91.32,352.26,209.33,7.86;23,91.32,363.22,209.33,7.86;23,91.32,374.17,209.32,7.86;23,91.32,385.13,44.03,7.86">Sheffield University and the TREC 2004 Genomics Track: Query Expansion Using Synonymous Terms. Proceedings of the Thirteenth Text REtrieval Conference (TREC-13)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="753" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,399.17,209.33,7.86;23,91.32,410.13,209.32,7.86;23,91.32,421.09,148.99,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="23,137.42,399.17,163.23,7.86;23,91.32,410.13,26.49,7.86">Psychological relevance and information science</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,126.83,410.13,173.81,7.86;23,91.32,421.09,59.46,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="602" to="615" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,435.13,209.33,7.86;23,91.32,446.09,209.32,7.86;23,91.32,457.05,209.33,7.86;23,91.32,468.00,150.07,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="23,270.67,435.13,29.97,7.86;23,91.32,446.09,209.32,7.86;23,91.32,457.05,205.08,7.86">Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,91.32,468.00,64.80,7.86">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="551" to="560" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,482.04,209.33,7.86;23,91.32,493.00,209.32,7.86;23,91.32,503.96,174.61,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="23,133.45,482.04,167.20,7.86;23,91.32,493.00,74.05,7.86">Stemming algorithms: A case study for detailed evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,172.50,493.00,128.13,7.86;23,91.32,503.96,94.30,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="84" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,518.00,209.33,7.86;23,91.32,528.96,209.33,7.86;23,91.32,539.92,209.32,7.86;23,91.32,550.88,209.33,7.86;23,91.32,561.83,21.00,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="23,144.91,528.96,126.51,7.86">Boosting for document routing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,282.99,528.96,17.65,7.86;23,91.32,539.92,209.32,7.86;23,91.32,550.88,151.27,7.86">Proceedings of the ninth international conference on Information and knowledge management</title>
		<meeting>the ninth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,575.87,209.33,7.86;23,91.32,586.83,209.33,7.86;23,91.32,597.79,209.33,7.86;23,91.32,608.75,209.33,7.86;23,91.32,619.71,209.33,7.86;23,91.32,630.67,21.00,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="23,135.46,597.79,165.19,7.86;23,91.32,608.75,131.67,7.86">Integrating web-based and corpus-based techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,230.05,608.75,70.59,7.86;23,91.32,619.71,205.14,7.86">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,91.32,644.71,209.32,7.86;23,91.32,655.66,209.32,7.86;23,91.32,666.62,40.45,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="23,216.57,644.71,84.06,7.86;23,91.32,655.66,87.77,7.86">Measures of diversity in classifier ensembles</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,190.25,655.66,71.74,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,128.64,209.33,7.86;23,329.92,139.60,156.77,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="23,362.97,128.64,171.95,7.86">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,329.92,139.60,78.24,7.86">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,153.64,209.33,7.86;23,329.92,164.60,209.33,7.86;23,329.92,175.56,209.33,7.86;23,329.92,186.52,59.39,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="23,527.22,153.64,12.03,7.86;23,329.92,164.60,209.33,7.86;23,329.92,175.56,64.79,7.86">An evaluation of some conflation algorithms for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lennon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Peirce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tarry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,406.81,175.56,128.42,7.86">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">177</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,200.55,209.33,7.86;23,329.92,211.51,209.33,7.86;23,329.92,222.47,209.32,7.86;23,329.92,233.43,209.32,7.86;23,329.92,244.39,169.55,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="23,372.89,211.51,166.36,7.86;23,329.92,222.47,126.99,7.86">Splog detection using self-similarity analysis on blog temporal dynamics</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tatemura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,466.57,222.47,72.67,7.86;23,329.92,233.43,209.32,7.86;23,329.92,244.39,96.60,7.86">Proceedings of the 3rd international workshop on Adversarial information retrieval on the web</title>
		<meeting>the 3rd international workshop on Adversarial information retrieval on the web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,258.43,209.33,7.86;23,329.92,269.39,209.33,7.86;23,329.92,280.34,209.32,7.86;23,329.92,291.30,178.08,7.86" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="23,452.29,258.43,86.96,7.86;23,329.92,269.39,209.33,7.86;23,329.92,280.34,14.75,7.86">The trec blogs06 collection: Creating and analysing a blog test collection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct coords="23,329.92,305.34,209.32,7.86;23,329.92,316.30,209.33,7.86;23,329.92,327.26,209.33,7.86;23,329.92,338.22,209.32,7.86;23,329.92,349.18,152.13,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="23,435.31,305.34,103.93,7.86;23,329.92,316.30,209.33,7.86;23,329.92,327.26,209.33,7.86;23,329.92,338.22,37.28,7.86">Charting the Topic Maps Research and Applications Landscape: First International Workshop on Topic Map Research and Applications</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maicher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,374.58,338.22,28.80,7.86">TMRA</title>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
			<pubPlace>Leipzig, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,363.22,209.33,7.86;23,329.92,374.17,209.33,7.86;23,329.92,385.13,209.32,7.86;23,329.92,396.09,209.32,7.86;23,329.92,407.05,209.33,7.86;23,329.92,418.01,84.02,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="23,517.23,363.22,22.02,7.86;23,329.92,374.17,209.33,7.86;23,329.92,385.13,106.53,7.86">Combining multiple evidence from different types of thesaurus for query expansion</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mandala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,444.73,385.13,94.52,7.86;23,329.92,396.09,209.32,7.86;23,329.92,407.05,205.78,7.86">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="191" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,432.05,209.33,7.86;23,329.92,443.01,209.32,7.86;23,329.92,453.97,139.77,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="23,378.52,432.05,160.73,7.86;23,329.92,443.01,31.71,7.86">On indexing, retrieval and the meaning of about</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,369.47,443.01,169.78,7.86;23,329.92,453.97,59.46,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="43" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,468.00,209.33,7.86;23,329.92,478.96,209.33,7.86;23,329.92,489.92,209.33,7.86;23,329.92,500.88,204.03,7.86" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="23,479.41,468.00,59.84,7.86;23,329.92,478.96,209.33,7.86;23,329.92,489.92,29.40,7.86">A probabilistic approach to spatiotemporal theme pattern mining on weblogs</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,369.09,489.92,170.16,7.86;23,329.92,500.88,111.89,7.86">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,514.92,209.33,7.86;23,329.92,525.88,209.33,7.86;23,329.92,536.84,209.33,7.86;23,329.92,547.80,21.00,7.86" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="23,349.86,525.88,189.39,7.86;23,329.92,536.84,22.96,7.86">Machine learning, neural and statistical classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Ellis Horwood Upper</publisher>
			<pubPlace>Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,561.83,209.33,7.86;23,329.92,572.79,209.33,7.86;23,329.92,583.75,21.00,7.86" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="23,381.94,561.83,157.31,7.86;23,329.92,572.79,66.70,7.86">Using Blog Properties to Improve Retrieval. ICWSM</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<pubPlace>colorado, USA, Boulder</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,597.79,209.33,7.86;23,329.92,608.75,209.32,7.86;23,329.92,619.71,209.33,7.86;23,329.92,630.67,31.24,7.86" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="23,476.87,597.79,62.39,7.86;23,329.92,608.75,62.95,7.86">Neural learning using AdaBoost</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Murphey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,506.07,608.75,33.17,7.86;23,329.92,619.71,205.18,7.86">Proceedings. IJCNN&apos;01. International Joint Conference on</title>
		<meeting>IJCNN&apos;01. International Joint Conference on</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,329.92,644.71,209.33,7.86;23,329.92,655.66,209.33,7.86;23,329.92,666.62,158.18,7.86" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="23,506.84,644.71,32.41,7.86;23,329.92,655.66,205.55,7.86">Predicting protein structural class with AdaBoost learner</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="23,329.92,666.62,80.81,7.86">Protein Peptide Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="489" to="492" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,128.64,209.32,7.86;24,91.32,139.60,209.32,7.86;24,91.32,150.56,209.32,7.86;24,91.32,161.52,131.01,7.86" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="24,138.95,128.64,161.68,7.86;24,91.32,139.60,64.43,7.86">Using linguistic and discourse structures to derive topics</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Paradis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,168.91,139.60,131.73,7.86;24,91.32,150.56,209.32,7.86;24,91.32,161.52,48.31,7.86">Proceedings of the fourth international conference on Information and knowledge management</title>
		<meeting>the fourth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,176.55,209.33,7.86;24,91.32,187.51,209.33,7.86;24,91.32,198.47,77.82,7.86" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="24,218.65,176.55,81.99,7.86;24,91.32,187.51,118.22,7.86">Approximation and radial-basis-function networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,216.88,187.51,79.22,7.86">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,213.50,209.33,7.86;24,91.32,224.46,78.56,7.86" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="24,137.33,213.50,97.36,7.86">The TAO of Topic Maps</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,243.87,213.50,56.77,7.86;24,91.32,224.46,50.16,7.86">Proceedings of XML Europe</title>
		<meeting>XML Europe</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,239.50,209.33,7.86;24,91.32,250.46,209.33,7.86;24,91.32,261.42,209.33,7.86;24,91.32,272.37,209.33,7.86;24,91.32,283.33,117.29,7.86" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="24,206.69,239.50,93.95,7.86;24,91.32,250.46,209.33,7.86;24,91.32,261.42,30.87,7.86">Distribution of relevant documents in domain-level aggregates for topic distillation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,128.44,261.42,172.20,7.86;24,91.32,272.37,209.33,7.86;24,91.32,283.33,26.43,7.86">Proceedings of the 13th international World Wide Web conference on Alternate track papers &amp; posters</title>
		<meeting>the 13th international World Wide Web conference on Alternate track papers &amp; posters</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="372" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,298.37,209.33,7.86;24,91.32,309.33,209.33,7.86;24,91.32,320.29,90.15,7.86" xml:id="b39">
	<monogr>
		<title level="m" type="main" coord="24,136.91,298.37,163.74,7.86;24,91.32,309.33,11.14,7.86">Ensemble based systems in decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="21" to="45" />
		</imprint>
		<respStmt>
			<orgName>IEEE Circuits and Systems Magazine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,335.32,209.33,7.86;24,91.32,346.28,209.33,7.86;24,91.32,357.24,209.32,7.86;24,91.32,368.20,209.33,7.86;24,91.32,379.16,40.45,7.86" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="24,91.32,346.28,209.33,7.86;24,91.32,357.24,209.32,7.86;24,91.32,368.20,58.11,7.86">Learn++: an incremental learning algorithm for supervised neuralnetworks. Systems, Man and Cybernetics, Part C</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Upda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Upda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,158.68,368.20,91.06,7.86">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="508" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,394.19,209.32,7.86;24,91.32,405.15,149.93,7.86" xml:id="b41">
	<monogr>
		<title level="m" type="main" coord="24,138.38,394.19,162.26,7.86;24,91.32,405.15,60.29,7.86">An algorithm for suffix stripping. information systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,420.18,209.32,7.86;24,91.32,431.14,209.32,7.86;24,91.32,442.10,209.32,7.86;24,91.32,453.06,209.32,7.86;24,91.32,464.02,45.05,7.86" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="24,271.90,420.18,28.74,7.86;24,91.32,431.14,167.21,7.86">Corpus Portal for Search in Monolingual Corpora</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Quasthoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,267.47,431.14,33.17,7.86;24,91.32,442.10,209.32,7.86;24,91.32,453.06,154.21,7.86">Proceedings of the fifth international conference on Language Resources and Evaluation, LREC 2006</title>
		<meeting>the fifth international conference on Language Resources and Evaluation, LREC 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1799" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,479.05,46.09,7.86;24,173.96,479.05,126.69,7.86;24,91.32,490.01,22.27,7.86;24,129.85,490.01,73.39,7.86;24,238.76,490.01,61.88,7.86;24,91.32,500.97,184.98,7.86;24,91.32,511.93,177.65,7.86" xml:id="b43">
	<monogr>
		<title level="m" type="main" coord="24,173.96,479.05,126.69,7.86;24,91.32,490.01,22.27,7.86;24,129.85,490.01,67.54,7.86">The Topic Maps Handbook, empolis GmbH</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rath</surname></persName>
		</author>
		<ptr target="http://www.empolis.com/downloads/empolisTopicMapsWhitepaper20030206.pdf" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,526.97,209.33,7.86;24,91.32,537.92,209.33,7.86;24,91.32,548.88,209.33,7.86;24,91.32,559.84,209.33,7.86;24,91.32,570.80,107.16,7.86" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="24,91.32,537.92,209.33,7.86;24,91.32,548.88,209.33,7.86;24,91.32,559.84,122.26,7.86">Constructing boosting algorithms from SVMs: an application toone-class classification. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,222.31,559.84,78.34,7.86;24,91.32,570.80,8.30,7.86">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1184" to="1199" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,585.84,209.33,7.86;24,91.32,596.79,209.33,7.86;24,91.32,607.75,209.32,7.86;24,91.32,618.71,124.86,7.86" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="24,271.69,585.84,28.95,7.86;24,91.32,596.79,209.33,7.86;24,91.32,607.75,179.94,7.86">Margin maximization with feed-forward neural networks: a comparative study with SVM and AdaBoost</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,281.33,607.75,19.30,7.86;24,91.32,618.71,46.88,7.86">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="313" to="344" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,91.32,633.75,209.33,7.86;24,91.32,644.71,209.32,7.86;24,91.32,655.66,209.32,7.86;24,91.32,666.62,147.90,7.86" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="24,143.93,633.75,156.72,7.86;24,91.32,644.71,37.28,7.86">Theoretical Views of Boosting and Applications</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,139.55,644.71,161.08,7.86;24,91.32,655.66,126.57,7.86">Algorithmic Learning Theory: 10th International Conference, ALT&apos;99</title>
		<meeting><address><addrLine>Tokyo, Japan, December 6-8</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,128.64,209.33,7.86;24,329.92,139.60,209.32,7.86;24,329.92,150.56,123.12,7.86" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="24,438.92,128.64,100.33,7.86;24,329.92,139.60,168.21,7.86">Improved Boosting Algorithms Using Confidence-rated Predictions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,505.12,139.60,34.12,7.86;24,329.92,150.56,33.44,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,165.50,209.33,7.86;24,329.92,176.46,209.33,7.86;24,329.92,187.42,209.32,7.86;24,329.92,198.38,209.32,7.86;24,329.92,209.34,93.68,7.86" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="24,439.15,176.46,100.11,7.86;24,329.92,187.42,209.32,7.86;24,329.92,198.38,37.87,7.86">Effiencient query delegation by detecting redundant retrieval strategies. SI-GIR 2007</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Scheel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Neubauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Albayarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,375.30,198.38,163.94,7.86;24,329.92,209.34,65.78,7.86">Workshop on Learning to rank for Information Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,224.28,209.33,7.86;24,329.92,235.24,209.33,7.86;24,329.92,246.20,209.33,7.86;24,329.92,257.16,209.32,7.86;24,329.92,268.12,131.01,7.86" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="24,527.22,224.28,12.03,7.86;24,329.92,235.24,209.33,7.86;24,329.92,246.20,74.86,7.86">An improved boosting algorithm and its application to text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Valdambrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,414.79,246.20,124.46,7.86;24,329.92,257.16,209.32,7.86;24,329.92,268.12,48.31,7.86">Proceedings of the ninth international conference on Information and knowledge management</title>
		<meeting>the ninth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,283.06,209.33,7.86;24,329.92,294.02,209.33,7.86;24,329.92,304.98,209.33,7.86;24,329.92,315.94,209.32,7.86;24,329.92,326.90,209.32,7.86;24,329.92,337.86,181.76,7.86" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="24,480.69,283.06,58.57,7.86;24,329.92,294.02,209.33,7.86;24,329.92,304.98,142.58,7.86">Boosting support vector machines for text classification through parameter-free threshold relaxation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Roma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,494.79,304.98,44.46,7.86;24,329.92,315.94,209.32,7.86;24,329.92,326.90,159.05,7.86">CIKM &apos;03: Proceedings of the twelfth international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,352.80,209.33,7.86;24,329.92,363.76,209.33,7.86;24,329.92,374.72,209.32,7.86;24,329.92,385.68,117.96,7.86" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="24,372.91,363.76,166.35,7.86;24,329.92,374.72,23.26,7.86">Query enrichment for web-query classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,362.08,374.72,177.17,7.86;24,329.92,385.68,27.82,7.86">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="320" to="352" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,400.62,209.33,7.86;24,329.92,411.58,60.46,7.86" xml:id="b52">
	<monogr>
		<title level="m" type="main" coord="24,450.97,400.62,83.71,7.86">Numerical taxonomy</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sneath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sokal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,426.52,209.32,7.86;24,329.92,437.48,172.26,7.86" xml:id="b53">
	<analytic>
		<title level="a" type="main" coord="24,377.97,426.52,101.44,7.86">A theory of the learnable</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,488.20,426.52,51.04,7.86;24,329.92,437.48,66.93,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,452.43,209.32,7.86;24,329.92,463.39,80.17,7.86" xml:id="b54">
	<monogr>
		<title level="m" type="main" coord="24,378.38,452.43,160.86,7.86;24,329.92,463.39,11.71,7.86">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,478.33,209.33,7.86;24,329.92,489.29,209.32,7.86;24,329.92,500.25,209.33,7.86;24,329.92,511.21,74.80,7.86" xml:id="b55">
	<analytic>
		<title level="a" type="main" coord="24,453.42,478.33,85.83,7.86;24,329.92,489.29,119.43,7.86">Query expansion using lexical-semantic relations</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,463.39,489.29,75.85,7.86;24,329.92,500.25,205.07,7.86">Proceedings of the 17th Annual International ACM-SIGIR Conference</title>
		<meeting>the 17th Annual International ACM-SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,526.15,209.33,7.86;24,329.92,537.11,209.33,7.86;24,329.92,548.07,209.33,7.86;24,329.92,559.03,21.00,7.86" xml:id="b56">
	<analytic>
		<title level="a" type="main" coord="24,444.33,526.15,94.92,7.86;24,329.92,537.11,209.33,7.86;24,329.92,548.07,18.03,7.86">A Case Study of Using Domain Analysis for the Conflation Algorithms Domain</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Frakes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,357.18,548.07,177.88,7.86">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,329.92,573.97,209.32,7.86;24,329.92,584.93,209.33,7.86;24,329.92,595.89,209.33,7.86;24,329.92,606.85,209.33,7.86;24,329.92,617.81,62.81,7.86" xml:id="b57">
	<monogr>
		<title level="m" type="main" coord="24,404.91,573.97,134.33,7.86;24,329.92,584.93,60.31,7.86">On Information Need and Categorizing Search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Zu Eißen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Electrical, Computer Science, and Mathematics Department of Computer Science of the University of Paderborn</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
