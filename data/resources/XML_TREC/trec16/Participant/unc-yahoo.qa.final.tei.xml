<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.72,75.94,424.51,14.42;1,246.41,94.36,119.11,14.42">Using Interactions to Improve Translation Dictionaries: UNC, Yahoo! and ciQA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.30,125.42,58.41,10.80"><forename type="first">Diane</forename><surname>Kelly</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3360</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.61,125.42,86.96,10.80"><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
							<email>vmurdock@yahoo-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Research Barcelona Ocata</orgName>
								<address>
									<postCode>08003</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.96,125.42,33.66,10.80"><forename type="first">Xin</forename><surname>Fu</surname></persName>
							<email>fu@email.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599-3360</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.72,75.94,424.51,14.42;1,246.41,94.36,119.11,14.42">Using Interactions to Improve Translation Dictionaries: UNC, Yahoo! and ciQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CFE74809EA51E2072E9CAF9523770C7E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentence retrieval is an important step in many question-answering (QA) technologies. However, characteristics of sentences and of the question-answering task itself often make it difficult to apply document retrieval techniques to sentence retrieval. The use of translation dictionaries offers one potentially useful approach to sentence retrieval, but training such dictionaries using QA corpora often introduces noise that can negatively impact retrieval performance. In this study, we experiment with using data elicited from assessors during interactions as training data for a translation dictionary. We employ two different interactions that elicit two types of data: data about assessors' topics and data about retrieved sentences. Results show that using sentence-level relevance feedback to adjust the translation dictionary improved retrieval for about half the topics, but harmed it for the other half.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Many question answering systems employ sentence or passage retrieval as a first step to finding answers to questions. This is a critical first step and if retrieval results are not good, then it is unlikely that useful answers will be found. Sentence retrieval is challenging for several reasons. Sentence retrieval is especially sensitive to vocabulary mismatch between question terms and sentence terms because sentences have so few terms to begin with, and most sentences are unlikely to contain both a word and its synonym. Furthermore, because the question usually does not contain the answer, the application of traditional document retrieval approaches to sentence retrieval is often ineffective. Such approaches may retrieve topically related sentences, but not sentences that contain actual answers.</p><p>To address these problems we propose the use of a translation dictionary to associate a sentence to a question containing related words. A translation dictionary learns related words from a parallel corpus of answered questions. Ideally, questions would be associated with answer terms in a translation table, which should improve the precision of the retrieval system. However, a parallel corpus of answered questions can produce a very noisy translation dictionary. In this study, we investigate the potential effectiveness of using data generated by users as an additional source of evidence for improving the quality of the translation dictionary. The data generated by users is done so through two different types of interactions: one that probes users' about their information problems and another that elicits sentence-level relevance feedback.</p><p>Previous research has shown that techniques employed by reference librarians to discover more information about users' information needs are also useful for eliciting additional information from users of online information systems. It has further been shown that this information can be used to improve retrieval performance for a document retrieval task <ref type="bibr" coords="2,155.33,185.24,131.79,10.80" target="#b3">(Kelly, Dollu and Fu, 2005)</ref>. As part of our participation in the TREC ciQA task we explored the effectiveness of an interaction technique based on the standard reference interview for the complex, question-answering task where sentences were being retrieved rather than documents. Specifically, we propose to use the feedback from this interaction technique to refine the translation dictionary.</p><p>In the area of information retrieval, it is well documented that relevance feedback is an effective technique for improving retrieval performance (c.f., <ref type="bibr" coords="2,411.85,268.04,102.38,10.80;2,90.00,281.84,25.82,10.80">Ruthven and Llamas, 2003)</ref>. However, in last year's ciQA task <ref type="bibr" coords="2,292.52,281.84,104.74,10.80" target="#b2">(Kelly and Lin, 2007)</ref> traditional relevance feedback approaches were not particularly effective for question-answering tasks. We suspect that one reason for this is because techniques that work well for document retrieval do not necessarily work well for sentence retrieval. Specific reasons for this were noted above: the item that is being retrieved is much smaller than a document and the question and the answer may share few (if any) of the same words. In this study, we were interested in investigating an alternative application of data generated through relevance feedback by applying it to the development of a translation dictionary instead of using it in a traditional way (i.e., for query expansion).</p><p>Although there are questions about whether real users would actually provide relevance feedback in operational settings, we thought we would take advantage of this experimental situation where assessors are asked to provide feedback, to investigate whether this is a potentially effective way to use relevance feedback for sentence retrieval. We use the relevance feedback to modify the translation dictionary, both by adding vocabulary to the dictionary from the relevant sentences, and by improving the translation probabilities for terms related to relevant sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Interaction Techniques</head><p>We studied two types of interactions both of which were embodied in static forms; thus, our approach to interaction was similar to that studied in previous versions of the ciQA task and in the HARD Track in that it was a single-cycle interaction. That is, the user presents an information need and the system asks the user to respond to a set of questions. The feedback provided by the user is then used to produce new results. We experimented with two types of interactions: open and closed. In the open interaction, assessors responded to a predefined set of questions that were designed to elicit more information from them about their questions. In the closed interaction, assessors provided relevance feedback about sets of sentences.</p><p>Eight assessors participated in this study. Each assessor was matched with 3-4 questions. Thus, assessors completed our interactions several times -once for each of their questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Open Interaction Technique</head><p>In the first interaction, we asked assessors to respond to a set of open questions which has been demonstrated in previous research to be effective at eliciting additional information from users beyond a simple query <ref type="bibr" coords="3,315.61,143.84,123.78,10.80" target="#b3">(Kelly, Dollu &amp; Fu, 2005)</ref>. These questions were inspired by questions typically asked by reference librarians when attempting to get a better understanding of a user's information need. Questioning techniques such as these are commonly employed by reference librarians and are a standard part of reference instruction <ref type="bibr" coords="3,270.24,199.04,58.47,10.80" target="#b1">(Katz, 2002)</ref>.</p><p>The format of the interaction is displayed in Figure <ref type="figure" coords="3,374.28,212.84,4.50,10.80">1</ref>. The first question asked assessors to describe what they already know about their topics, the second question asked them to describe what else they need to know about their topics, and the third question asked them to describe things that are not relevant to their topic. Of course, responses to this last question could be numerous, but in the context of this study, it was expected that assessors would interpret this question in relation to their information problem. We asked this last question in hopes of getting some information that would allow us to experiment with negative feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Open Interaction Technique</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Closed Interaction Technique</head><p>Our second interaction technique consisted of sentence-level feedback (Figure <ref type="figure" coords="3,505.00,589.58,4.34,10.80">2</ref>). Assessors were presented with a list of 20 top-ranking sentences and asked to indicate whether sentences were useful or not useful, or if they couldn't tell. Our original intention was to present a limited number of sentences from a varying number of documents. Because some documents contain many relevant sentences, there is a possibility that top ranking sentences will be from the same document. We hoped our presentation method would potentially provide assessors with more diverse sentence sets from a larger number of documents. Unfortunately, we were not able to organize the sentences in this way and did not change the interface to reflect this. We used the Lemur IR toolkit (http://www.lemurproject.org) to conduct our experiments. We did the retrieval in a two-step process: a document retrieval step, followed by a sentence retrieval step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Retrieval</head><p>We built the document index using Lemur's basic defaults for indexing (BuildIndex function), with the Krovetz stemmer <ref type="bibr" coords="4,328.95,438.62,74.51,10.80" target="#b4">(Krovetz, 1993)</ref>. Our baseline queries consisted of the template and narrative for each topic. Stopwords from the Inquery stoplist were removed from the text, and the default parameters for Jelinek-Mercer smoothing were used. We retrieved the top 1000 documents, sentence-segmented them with a rule-based segmenter, and indexed the sentences for each query separately.</p><p>Unfortunately, only documents from one source were indexed. We were not aware of this until close to the deadline for submitting the baseline results and despite our trouble-shooting efforts, we were unable to identify and fix the problem. Therefore, our official TREC baseline results (UNCYABL30) were only from AFP (&gt; 99%) and APW sources (&lt; 1%)</p><p>After the deadline passed, we gave up on the latest version of Lemur, reinstalled an earlier version (4.3.2) and re-indexed the corpus without any problem. This seems to indicate that the latest version of Lemur has some problem indexing this corpus, but we were unable to identify the problem. The new retrieval run, UNCYAEX1, was later submitted as one of our experimental runs, but will be analyzed hereafter as a baseline. It is important to note that the sentences we used to populate our closed interaction form were from UNCYAEX1 and not from our official baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Retrieval</head><p>We used Model S for sentence retrieval. Model S is described in <ref type="bibr" coords="5,439.92,102.44,78.96,10.80" target="#b4">Murdock (2006)</ref> and uses translation probabilities learned from parallel corpora of answered questions to weight the language modeling probability in a query-likelihood retrieval framework. In addition, as the context of a sentence is a good indicator of its relevance, the sentences were smoothed from the documents they came from, as described in <ref type="bibr" coords="5,420.60,157.64,92.74,10.80;5,90.00,171.44,30.01,10.80" target="#b4">Murdock and Croft (2005)</ref>. Model S used Jelinek-Mercer smoothing with the document language model, giving as little probability as possible to the corpus language model. Sentences were stemmed using the Krovetz stemmer, but only single-characters were removed as stopwords.</p><p>Model S relies on a translation table, which is trained from a parallel corpus of aligned sentences. In our case, we combined dictionaries from two different sources. The first source consisted of factoid questions and their known answer sentences from the TREC QA Track, questions 1 through 1893. The second source consisted of a parallel corpus of ciQA task questions, and their answer nuggets from the 2006 TREC ciQA task <ref type="bibr" coords="5,90.00,295.64,102.48,10.80" target="#b2">(Kelly and Lin, 2007)</ref>. Both translation dictionaries were learned using GIZA++ <ref type="bibr" coords="5,481.56,295.64,20.00,10.80;5,90.00,309.44,89.90,10.80">(Al-Onaizan et al. 1999</ref>), an open-source implementation of the IBM translation models <ref type="bibr" coords="5,90.00,323.24,86.63,10.80" target="#b0">(Brown et al. 1990</ref>). The translation tables were learned using IBM Model 4, with the default parameter settings. Prior to learning the translation tables, sentences and questions were stemmed using the Krovetz stemmer, but only single-character stopwords were removed.</p><p>The two dictionaries were combined such that if a term appeared in both dictionaries, its probabilities were averaged otherwise the terms were simply added to the dictionary. Both runs UNCYABL30 and UNCYAEX1 were produced using this model, the difference in performance accounted for entirely by the faulty indexing of the baseline run. For our experimental run (UNCYAEX2), we used the sentences assessors indicated as useful or somewhat useful paired with the questions for which the sentences were retrieved to create a parallel corpus. The sentences were processed as described above. That is, single characters were removed and the sentences were stemmed using the Krovetz stemmer. A translation table was learned using GIZA++ as described above, and the dictionary was merged with the existing dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Interaction form responses</head><p>We obtained quite a bit of feedback from assessors from our open-ended interaction. This feedback was illuminating in several ways. While the majority of it appears useful (we have not analyzed it thoroughly because we have not decided on a method for handling some of the data), some assessors' replies to these questions were unexpected and very different from the type of responses we received in our previous studies of this technique. For instance, at least one assessor provided what we consider spam -the responses entered were obviously and purposely not about the assessor's topic. In response to the first question asking assessors what more they'd like to know about their topics, several assessors responded by saying that they already knew everything about their topics and that providing a response to this question wouldn't be fair. Clearly, our understanding of how much assessors know about their topics a priori was not accurate. To be fair, we used this technique in a previous TREC study <ref type="bibr" coords="6,471.91,102.44,33.65,10.80;6,90.00,116.24,100.31,10.80" target="#b3">(Kelly, Dollu, and Fu, 2005)</ref> and found it to be effective. However, we note that the assessors in this previous Track were recruited by the Linguistic Data Consortium and were engaged in a document retrieval task. It may be the case that assessors for QA tasks necessarily have to know more about their topics, in which case the open interaction (or any interaction for that matter) may not have been a good fit. We'd like to emphasize that these types of responses were in the minority, but they definitely made us think about the potential limitations of the TREC framework for studying interactions of this type. Overall, some data was elicited by this technique; however, we have yet to determine whether it is useful. Furthermore, we need to identify a technique for automatically removing responses that were written to us as researchers (such as those described above), rather than in response to the system's questions and for dealing with spam.</p><p>With respect to sentence feedback, about 45% (274) of the 600 sentences shown to all assessors (30 topics * 20 sentences) were marked as useful, 36% (215) were marked as not useful, 16% (99) were marked as can't tell, and 3% (18) were not marked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retrieval results</head><p>Table <ref type="table" coords="6,156.37,350.84,6.00,10.80" target="#tab_0">1</ref> shows the mean F-scores and standard deviations for our official baseline run (UNCYACL30), unofficial baseline run (UNCYAEX1) and experimental run (UNCYAEX2). As expected, our official baseline run performed very poorly. However, our unofficial baseline run (which was the same technique employed in our unofficial run, expect with the entire corpus indexed) was on par with the median average baseline run score of 0.359. We conducted a paired sample t-test to investigate whether the difference between UNCYAEX1 and UNCYAEX2 was statistically significant. Results of this test indicated that UNCYAEX2 did not significantly improve performance over <ref type="bibr" coords="6,409.25,560.30,64.68,10.80;6,90.00,574.10,64.08,10.80">UNCYAEX1 [t(29)=0.892,</ref><ref type="bibr" coords="6,157.08,574.10,41.60,10.80">p=0.380]</ref>. Table <ref type="table" coords="6,237.23,574.10,6.00,10.80" target="#tab_1">2</ref> shows the comparison of UNCYAEX2 to the unofficial baseline run (UNCYAEX1) at the topic level. UNCYAEX2 improved the performance of about 53% of the topics, but harmed it for 40% of the topics. We note that results remained unchanged for very few topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross site comparison</head><p>In this subsection, we report a comparison between our results and results of other sites. Figures <ref type="figure" coords="7,156.32,116.24,6.00,10.80" target="#fig_1">3</ref> and<ref type="figure" coords="7,185.67,116.24,6.00,10.80">4</ref> show the performance of our official and unofficial baseline runs relative to the best, median and worst baselines from other sites. The best and worst scores appear at the top and bottom of each line, respectively, while the median score for each topic is set to 0.000. As expected, our official baseline run was the worst performing run for most topics. Our unofficial baseline run fared better (Figure <ref type="figure" coords="7,472.77,171.44,5.00,10.80">4</ref>) and even produced the best run for a couple of topics.  Figure <ref type="figure" coords="7,160.37,506.00,6.00,10.80">5</ref> shows the performance of our experimental run (UNCYAEX2) relative to the best, median and worst final runs from other sites. For 19 topics (more than 60%), the performance of UNCYAEX2 was equal to or above the ciQA07 medians. The average F-score of UNCYAEX2 was 0.374, which was also higher than the average final run performance of all sites 0.351, but the difference was not significant [t(29)=0.704, p=0.487]. In this study, we used sentence-level relevance feedback to modify a translation dictionary, both by adding vocabulary to the dictionary from the relevant sentences, and by adjusting the translation probabilities for terms related to relevant sentences. Our initial expectation was that the quality of the translation dictionary would improve slightly as term pairs not previously in the dictionary were added, and terms that existed already were adjusted by the translation probabilities from the feedback dictionary. We did not expect a drastic improvement because the original terms still existed in the dictionary. However, we showed that with even a small amount of feedback data, we can improve the quality of the translation table sufficiently to improve retrieval for about 50% of the topics. That the performance improved at all suggests that a better dictionary can improve performance, and that constructing this dictionary with feedback from users is a potentially useful way to construct a high-quality dictionary of related terms.</p><p>We experienced numerous problems in conducting this study, including problems with our initial baseline run. The experiment did not quite turn out as we had hoped, but we learned a lot about QA assessors and about the types of interactions that are or are not possible in this experimental setting. Despite not finding any substantial results, we believe there is some potential value in using user-generated data as evidence for improving the quality of translation dictionaries. We plan to continue to investigate how the data generated in this experiment can be used to improve sentence retrieval and the retrieval of answers for complex questions, and to investigate how we might collect data from other types of users for similar tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,211.86,286.82,188.34,10.80;4,131.40,86.28,349.20,197.22"><head>Figure</head><label></label><figDesc>Figure 2. Closed Interaction Technique</figDesc><graphic coords="4,131.40,86.28,349.20,197.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,116.34,478.40,379.32,10.80"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of official baseline (UNCYABL30) to ciQA07 baselines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,112.98,335.90,386.03,10.80"><head></head><label></label><figDesc>Figure 4. Comparison of unofficial baseline (UNCYAEX1) to ciQA07 baselines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,159.54,334.74,292.92,196.32"><head></head><label></label><figDesc></figDesc><graphic coords="3,159.54,334.74,292.92,196.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,101.16,447.38,401.77,67.98"><head>Table 1 .</head><label>1</label><figDesc>Means and standard deviations for F-scores of different runs</figDesc><table coords="6,334.38,461.66,168.55,10.80"><row><cell>Mean</cell><cell>Std. Deviation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,94.98,643.10,421.98,53.16"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="6,106.56,643.10,410.40,53.16"><row><cell cols="4">. Number of topics for which the experimental run (UNCYAEX2) outperformed</cell></row><row><cell></cell><cell cols="2">the unofficial baseline (UNCYAEX1)</cell><cell></cell></row><row><cell></cell><cell>Better</cell><cell>Same</cell><cell>Worse</cell></row><row><cell>UNCYAEX2</cell><cell>16</cell><cell>2</cell><cell>12</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,126.00,350.84,369.70,10.80;9,90.00,364.64,424.75,10.80;9,90.00,378.44,421.21,10.80;9,90.00,392.24,73.33,10.80" xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Della</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fredrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,507.75,364.64,7.00,10.80;9,90.00,378.44,359.26,10.80">A Statistical Approach to Machine Translation&quot; in Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,126.00,406.04,392.32,10.80;9,90.00,419.84,157.56,10.80;9,247.56,417.07,6.21,7.18;9,256.80,419.84,140.38,10.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,252.60,406.04,265.72,10.80;9,90.00,419.84,92.98,10.80">Introduction to reference work: reference services and reference processes</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">A</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>McGraw-Hill</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
	<note>th Edition</note>
</biblStruct>

<biblStruct coords="9,126.00,433.58,377.94,10.80;9,90.00,447.38,146.68,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,306.91,433.58,191.76,10.80">Overview of the TREC 2006 ciQA Task</title>
		<author>
			<persName coords=""><forename type="first">Diane</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,90.00,447.38,62.70,10.80">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,126.00,461.18,348.71,10.80;9,90.00,474.98,419.28,10.80;9,90.00,488.84,410.25,10.80;9,90.00,502.58,235.56,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,363.03,461.18,111.68,10.80;9,90.00,474.98,283.77,10.80">The loquacious user: A document-independent source of terms for query expansion</title>
		<author>
			<persName coords=""><forename type="first">Diane</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><forename type="middle">J</forename><surname>Dollu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,394.98,474.98,114.30,10.80;9,90.00,488.84,410.25,10.80;9,90.00,502.58,101.18,10.80">Proceedings of the 28th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;05)</title>
		<meeting>the 28th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;05)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,126.00,516.38,364.05,10.80;9,90.00,530.24,105.05,10.80;9,195.06,527.47,6.21,7.18;9,204.26,530.24,277.87,10.80;9,90.00,543.98,107.40,10.80;9,126.00,557.78,393.98,10.80;9,90.00,571.58,85.37,10.80;9,126.00,585.38,357.67,10.80;9,90.00,599.18,430.69,10.80;9,90.00,613.04,276.45,10.80;9,126.00,626.78,370.66,10.80;9,90.00,640.58,423.14,10.80;9,90.00,654.38,21.00,10.80;9,126.00,668.18,376.15,10.80;9,90.00,681.98,418.61,10.80;9,90.00,695.78,420.54,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,253.14,516.38,219.32,10.80;9,257.04,557.78,139.95,10.80;9,367.65,585.38,116.02,10.80;9,90.00,599.18,90.75,10.80;9,341.02,626.78,155.64,10.80;9,90.00,640.58,194.49,10.80">A survey on the use of relevance feedback for information access systems</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">;</forename><surname>Krovetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,90.00,530.24,105.05,10.80;9,195.06,527.47,6.21,7.18;9,204.26,530.24,277.87,10.80;9,90.00,543.98,102.70,10.80;9,201.00,599.18,319.69,10.80;9,90.00,613.04,276.45,10.80;9,126.00,626.78,118.22,10.80">Proceedings of the Conference on Human Language Technologies and Empirical Methods in Natural Language Processing. Ruthven, Ian and Lalmas</title>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Curin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Jahr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><surname>Knight</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Franz-Josef</forename><surname>Melamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Och</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Purdy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><surname>Yarowsky</surname></persName>
		</editor>
		<meeting>the Conference on Human Language Technologies and Empirical Methods in Natural Language Processing. Ruthven, Ian and Lalmas<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Yaser Al-Onaizan</publisher>
			<date type="published" when="1993">1993. 2006. 2005. 2003. Jan. 1999</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="95" to="145" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts ; JHU Workshop</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
	<note>Mounia. Statistical Machine Translation, Final Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
