<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,41.27,87.39,512.44,22.52;1,56.72,115.29,481.59,22.52;1,123.17,143.18,348.70,22.52">Evaluation of Query Formulations in the Negotiated Query Refinement Process of Legal e-Discovery: UMKC at TREC 2007 Legal Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.13,179.10,76.17,13.14"><forename type="first">Feng</forename><forename type="middle">C</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Engineering</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<settlement>Kansas City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.61,179.10,73.54,13.14"><forename type="first">Yugyung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Engineering</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<settlement>Kansas City</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,348.14,179.10,70.73,13.14"><forename type="first">Deep</forename><surname>Medhi</surname></persName>
							<email>dmedhi@umkc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Engineering</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<settlement>Kansas City</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,41.27,87.39,512.44,22.52;1,56.72,115.29,481.59,22.52;1,123.17,143.18,348.70,22.52">Evaluation of Query Formulations in the Negotiated Query Refinement Process of Legal e-Discovery: UMKC at TREC 2007 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E265BF77DA245639934ABF59EED5E33C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>UMKC participated in the 2007 legal track. Our experiments focused mainly on evaluating the different query formulations in the negotiated query refinement process of legal e-discovery. For our study, we considered three sets of paired runs in vector space model and language model respectively. Our experiments indicated that although the Boolean query negotiating process was successful for the standard Boolean retrieval model, it did not make statistically significant query improvements in our ranked systems. This result provided us an insight into the query negotiation process and a new direction to refine queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Electronically stored information has gained substantial standing during trial-preparation and litigation in recent years. However, the development of the corresponding legal e-discovery methodology and underlying engineering has not gained the same momentum. The efficacy of the legal e-discovery process fundamentally started from the initial query formulation and negotiated query refinement known as the development of a search protocol, even well before applying any information retrieval strategies and techniques.</p><p>There are four main stages in the negotiated query refinement process of legal e-discovery. First, the plaintiff states the objectives of the request for the production of documents as the legal evidences in the requested text (RequestText). Second, the defendant devises the initial query (ProposalByDefendant) from the requested text. Third, the plaintiff presents a counterproposal (RejoinderByPlaintiff) with usually more complex queries. Finally, both parties negotiate an agreement on the final query string (FinalQuery). In this escalating process, broad query terms (such as synonyms) and Boolean constrains (such as proximity) are added to the query.</p><p>The essential purpose of query refinement through negotiations in legal e-discovery is to improve the recall, which is a measure of the ability of a system to present all relevant items. Therefore, the final negotiated query is supposed to find more relevant documents than what was initially proposed by the defendant. Unfortunately, the introduced ambiguity or over-broad scope in queries may lead to the contrary. This has motivated our investigation of the dynamics of the search protocols development by comparing the performance among different query formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Measures</head><p>We are interested in the relative performance improvement among different query formulations during the negotiation process. Traditionally, the mean average precision (MAP) and many other robust measures are common performance measures for information retrieval systems. For recall-oriented measures, the 2007 legal track further extended the concept of the inferred average precision (infAP) <ref type="bibr" coords="1,540.59,666.52,13.94,11.26" target="#b3">[4]</ref> by incorporating the deep pooling in order to obtain estimated recalls (est R) and estimated precisions (est P). In our experiments, we focused on the MAP and the estimated recall at B (est RB) and estimated precision at B (est PB), where B is the number of documents matching the final negotiated boolean query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Methods</head><p>The most resources demanding component of evaluation is to obtain the relevance judgment. Pooling is the status quo in TREC to obtain the relevance judgment in which only a small percentage of documents in the pool is judged. But the traditional pooling method has its biases and limits <ref type="bibr" coords="2,243.78,171.31,13.94,11.26" target="#b0">[1]</ref> and is insufficient for the challenge of large corpus in the 2006 legal track <ref type="bibr" coords="2,121.45,199.05,13.93,11.26" target="#b1">[2]</ref>  <ref type="bibr" coords="2,138.77,199.05,12.69,11.26" target="#b2">[3]</ref>. Hence, the 2007 legal track uses a deep pooling method to obtain estimated recalls and estimated precisions.</p><p>However, the above evaluation methods are impractical to evaluate query formulation in a production environment because the true relevance judgment can not be a prerequisite at the query formulation stage. Since our investigation is only interested in the relative superiority of query formulations and the absolute performance measurements are a secondary issue, this emphasis shift made it possible to set up a contingent run as the pseudo relevance judgment. The relative standing of different query formulations can be estimated if this relevance proxy is close enough to the true relevance judgement.</p><p>Our experiments compare the relative performance of different query formulations before the actual relevance assessment by using the reference Boolean run in the 2007 legal track as the pseudo relevance judgment. We hypothesize a well performed Boolean run can serve the similar role of the human relevance judgment to a certain extent based on the following observations. First, the result of the 2006 legal track showed that the reference Boolean run found 57% of the known relevant documents <ref type="bibr" coords="2,40.46,559.77,12.70,11.26" target="#b1">[2]</ref>, and it is one of the top performing runs. Second, although the Boolean run is well known to contain many irrelevant documents, it can still be expected to produce a reasonable est RB and est PB as these measures are designed to cope with incomplete and imperfect relevance judgment. Finally, we will partially verify this hypothesis by comparing the query performances obtained from the pseudo relevance judgment to the human relevance judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Systems</head><p>The information retrieval system that we use in this experiment is a modified Lucene search engine <ref type="bibr" coords="2,76.06,749.73,13.94,11.26" target="#b4">[5]</ref> on a Cray XD1 system in the Arctic Region Supercomputing Center. Both the vector space model (VSM) and the language model (LM) were implemented, which represent algebraic models and probabilistic models respectively <ref type="bibr" coords="2,470.72,111.80,12.70,11.26" target="#b8">[9]</ref>. The primary advantage of utilizing two representative retrieval models is to mitigate the potential bias in any one particular model during the evaluation. The key relevance measurement in the VSM of query q for document d correlates to the cosine-distance or the dot-product between document and query vectors; its formula is explained in the Lucene book <ref type="bibr" coords="2,537.60,208.87,12.70,11.26" target="#b4">[5]</ref>.</p><p>In language modeling approaches for information retrieval <ref type="bibr" coords="2,349.68,236.61,12.70,11.26" target="#b5">[6]</ref>, we estimate a language model for each document and then rank documents according to their likelihood of generating the query. For a collection C, document d, query q, term t, term frequency of t in d (tf t,d ) and document frequency of t in C (df t,C ), the language modeling formula in this system is given by the equation 1. The Jelinek-Mercer smoothing parameter λ holds the responsibility for a linear interpolation of the maximum likelihood estimation in document language model and the collection language model.</p><formula xml:id="formula_0" coords="2,309.84,410.60,244.69,24.71">P (d|q) = P (d) t∈q (λP (t|d) + (1 -λ)P (t|C)) (1)</formula><p>where</p><formula xml:id="formula_1" coords="2,303.47,454.50,105.16,50.98">P (d) = |d| d ∈C |d | P (t|d) = tf t,d P (t|C) = df t,C t ∈C df t ,C .</formula><p>The most significant technique we utilized is the query expansion model based on the conceptual relevance framework <ref type="bibr" coords="2,434.60,546.80,12.70,11.26" target="#b6">[7]</ref>. Conceptual relevant concepts are expanded into a query based on its query centroid. The query is expanded before the initial search, so there is no relevance feedback required. But since this query expansion process does not observe the full Boolean syntax and simply concatenates every query term with an OR operator, information of phrase and proximity is lost from original Boolean queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Indexes</head><p>We indexed only the OCR text portion and its document number from the IIT CDIP test collection. Porter stemmer was invoked, but its potential was hindered by the numerous OCR errors. A customized stop words list of 1,236 items was used to reduce the index size and to clean the OCR error. We then crafted the most of the two-letter permutations into this stop words list, and they are counted as almost half of the list. All of the above efforts are mounted at a common goal to create a centralized index with a manageable size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Runs</head><p>There are six runs designed as stated in Table II-E; essentially, three paired runs utilize the various query fields in two different retrieval models. They are labeled UMKC 1 , UMKC 2 , and so on. Although the RejoinderByPlaintiff is different with the FinalQuery, they do not significantly diverge. Therefore, we do not show the RejoinderByPlaintiff as a separate query genre in this table. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Queries</head><p>The final query strings of paired runs are the same for both retrieval models if they share the same query source. The actual query string generated from the query expansion model is quite distinguishable from its query source text. As an example, for request number 56, the query source of UMKC 2 run is:</p><p>RequestText: Please produce any and all documents concerning soil water management as it pertains to commercial irrigation.</p><p>And its final query string contains a list of weighted relevant terms: irrig (0.3084472), soil (0.25898176), water (0.2516427), pertain (0.20087002), commerci (0.1465618), tobacco (0.08702624), cigarett (0.03782016), plant (0.037246022), product (0.031287868), smoke (0.029167147) ... In the above final query string, those functional terms in the requested text are automatically eliminated due to their marginal weight and other relevant terms have been expanded into the query. The connotation of a term being relevant to the query is local to this particular IIT CDIP test collection and may not strictly correspond to our common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Results</head><p>The six runs were evaluated through the l07 eval program where both the traditional measures and the 2007 legal track specific measures were produced. As every document in the reference Boolean run is assumed to be selected and judged as relevant, the probability of including document d in the judging sample is one. Hence, p(d) = 1.0 was added to the reference Boolean run to accommodate the l07 eval program.</p><p>In the TREC environment, it has been suggested that the t-Test significance coupled with at least a 10% relative difference in MAP between two runs is significant <ref type="bibr" coords="3,357.15,394.96,12.70,11.26" target="#b7">[8]</ref>. Therefore, the absolute performance measures, including MAP, est RB, and est PB, are shown in Table <ref type="table" coords="3,385.05,422.70,7.30,11.26">II</ref>. Both the relative difference on MAP and the two-tail P-values from the paired t-Test of MAP, est RB, and est PB are shown in Table <ref type="table" coords="3,337.09,464.30,11.20,11.26">III</ref>, where the notion of (m, n) indicates a comparison between UMKC m and UMKC n . A particular query formulation is compared with not only the other query formulations using the same retrieval model but also its corresponding run in the other retrieval model.</p><p>There are two observations from the above results:</p><p>• Observation-1: there is no statistical significant improvement at the 0.05 level among different runs regardless of the query sources. • Observation-2: the language model generally outperforms the vector space model. The corresponding performance evaluation tables after the human relevance judgment being obtained are shown in Tables IV and V. In the case of using the human relevance judgment, observation-1 still holds except for the P-value of est RB between UMKC 1 and UMKC 2 . But the observation-2 is reversed in that the vector space model actually outperforms the language model, especially when queries are derived from the FinalQuery field.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DISCUSSIONS</head><p>There is generally no statistically significant performance difference among different query formulations, regardless of whether we latch on to the pseudo relevance judgment or the human relevance judgement. Therefore, the pseudo relevance judgement can be justified to practitioners as an economical mean to compare the query formulations. However, such justification is largely dependent on the actual performance of the reference Boolean run. In other words, we have to choose a sound reference Boolean run to evaluate query formulations. On the other side, the available performance measures between the vector space model and the language model do not yield any significant conclusion in this experiment. They simply indicate that the language model performs more closely to the reference Boolean run, whereas the vector space model performs more closely to the human judgement. Hence, we need additional and different retrieval systems in order to verify whether the pseudo relevance judgement can also be used to compare different retrieval systems.</p><p>The results indicate the deficiency in the current Boolean query negotiation process as the negotiation has not improved retrieval performance, at least in our ranked systems. The utility of using the reference Boolean run as the pseudo relevance judgement also supports the particular finding in the 2006 legal track that the effectiveness of the Boolean query system is compatible with the effectiveness of the best ranked retrieval systems <ref type="bibr" coords="4,453.59,595.01,12.70,11.26" target="#b1">[2]</ref>. But we have to realize that the Boolean query refinement process is specifically intended for Boolean retrieval systems, and that may be the chief advantage of the reference Boolean run. Table <ref type="table" coords="4,407.71,650.48,12.61,11.26">VI</ref> shows the definite statistically significant improvement when the Boolean queries move from ProposalByDefendant to either RejoinderByPlaintiff or FinalQuery. But generally, there is no further improvement from RejoinderBy-Plaintiff to FinalQuery and this is consistent with our previous observation that there is only little difference between these two queries.</p><p>Thus, what made the current query refinement )) The intention of performing the above two techniques is to increase the recall; as a matter of fact, this objective is well achieved in the standard Boolean query system. But in our ranked systems, we discarded the Boolean constraints and leveled query coverage through query expansion; hence, both techniques lost their thrust. From another point of view, the infertility of the Boolean query negotiation revealed the fact that the negotiation is ineffective to discover and inject semantically independent terms into queries. In other words, the negotiated final query essentially has the same semantic coverage as what was initially proposed by the defendant after we drop all the Boolean syntax.</p><p>Interestingly, among all 70 submitted main task runs in the 2007 legal track, the UMKC 5 run has the highest estimated precision at depth of 25,000 (est P25000), and the UMKC 2 run has the highest estimated relevance retrieved measure (est rel ret). Both UMKC 5 and UMKC 2 are using the Request-Text only, and they are paired runs in the vector space model and the language model respectively. If we choose to view the Boolean negotiation process as a kind of manual query expansion, then the above results indicate that the unsupervised query expansion model we deployed is more effective than the manual query refinement in terms of retrieving more relevant documents at the 25,000 level-the designated depth of the 2007 legal track.</p><p>In order for the ranked systems to take further advantage of the query refinement process, we suggest enriching the legal query with new concepts which are pertinent to the overall query intention in the RequestText but located in some other semantic dimensions. From the perspective of information retrieval, it will be more helpful to our ranked systems if the query negotiators simply identify a solid list of core concepts. From the perspective of e-discovery, it is desirable to limit the query scope to avoid unduly burdensome or expensive discovery requests <ref type="bibr" coords="5,303.47,361.90,18.32,11.26" target="#b9">[10]</ref>. Therefore, we should investigate the criteria which qualifies a term to be considered for query refinement and the effect of a term on the overall query performance. Furthermore, in the absence of insights of underlying information retrieval engines, the emphasis of the query negotiators should focus on identifying those basic core concepts, rather than expanding the query according to the needs of the standard Boolean retrieval system. Additionally, from a broader perspective of e-discovery, we need to negotiate the information retrieval system beyond the e-discovery queries.</p><p>Finally, in the review of our system design, there are still several techniques that may be added to improve its performance. For example, sentence boundary detection and phrase identification can be utilized during conceptual relevance building, and full Boolean syntax can be observed during query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>Our experiments indicate that although the Boolean query negotiating process was successful for the standard Boolean retrieval model, it did not make statistically significant query improvements in our ranked systems. This implies a new challenge to the legal query negotiator, who has to discover new semantically independent query terms during negotiation. We also found that the utility of the Boolean run as the pseudo relevance judgment can serve as a potential economical mean to evaluate and direct the query refinement process without the expensive human relevance judgment. As the query negotiators usually are uncertain with the nature of the underlying information retrieval system, we further propose a direction for the query refinement in legal e-discovery, which is to shift from amplifying a particular query term to identifying the core concepts pertaining to the overall query intention.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGEMENT We thank <rs type="person">Christopher Fallen</rs> of the <rs type="affiliation">Arctic Region Supercomputing Center</rs> for generously facilitating the set up of the modified Lucene search engine. The Boolean run results listed in Table VI are provided by <rs type="person">Dr. Stephen Tomlinson</rs> of <rs type="affiliation">Open Text Corporation</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,61.38,375.34,230.13,8.45;6,61.38,385.80,230.14,8.45;6,61.38,396.26,230.14,8.45;6,61.38,406.72,230.14,8.45;6,61.38,417.18,44.48,8.45" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,275.57,375.34,15.94,8.45;6,61.38,385.80,91.97,8.45">Bias and the limits of pooling</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,174.91,385.80,116.60,8.45;6,61.38,396.26,230.14,8.45;6,61.38,406.72,105.88,8.45">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,61.38,427.64,230.15,8.45;6,61.38,438.10,177.94,8.45" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,224.04,427.64,67.49,8.45;6,61.38,438.10,56.31,8.45">TREC-2006 Legal Track Overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,135.51,438.10,80.50,8.45">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,61.38,448.56,230.14,8.45;6,61.38,459.03,230.15,8.45;6,61.38,469.49,91.35,8.45" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,125.50,448.56,166.01,8.45;6,61.38,459.03,188.85,8.45">Experiments with the Negotiated Boolean Queries of the TREC 2006 Legal Discovery Track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,276.09,459.03,15.44,8.45;6,61.38,469.49,68.03,8.45">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,61.38,479.95,230.14,8.45;6,61.38,490.41,230.13,8.45;6,61.38,500.87,230.14,8.45;6,61.38,511.33,230.14,8.45;6,61.38,521.79,20.17,8.45" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,182.09,479.95,109.43,8.45;6,61.38,490.41,152.99,8.45">Estimating average precision with incomplete and imperfect judgments</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,236.53,490.41,54.98,8.45;6,61.38,500.87,230.14,8.45;6,61.38,511.33,87.44,8.45">Proceedings of the 15th ACM international conference on Information and knowledge management</title>
		<meeting>the 15th ACM international conference on Information and knowledge management<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,61.38,532.25,230.14,8.45;6,61.38,542.71,122.04,8.45" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<title level="m" coord="6,182.64,532.25,60.58,8.45">Lucene in action</title>
		<meeting><address><addrLine>Greenwich, CT</addrLine></address></meeting>
		<imprint>
			<publisher>Manning Publications</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,61.38,553.17,230.13,8.45;6,61.38,563.63,230.13,8.45;6,61.38,574.09,145.68,8.45" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,118.70,553.17,172.81,8.45;6,61.38,563.63,21.82,8.45">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,102.81,563.63,184.71,8.45">Center for Telematics and Information Technology</title>
		<imprint>
			<publisher>University of Twente</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Ph.D.</note>
</biblStruct>

<biblStruct coords="6,61.38,584.55,230.14,8.45;6,61.38,595.01,230.14,8.45;6,61.38,605.47,46.23,8.45" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,200.22,584.55,91.30,8.45;6,61.38,595.01,149.06,8.45">Experiments with Query Expansion at TREC 2006 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Medhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,235.35,595.01,56.17,8.45;6,61.38,605.47,22.92,8.45">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,61.38,615.93,230.14,8.45;6,61.38,626.39,230.14,8.45;6,61.38,636.85,230.14,8.45;6,61.38,647.31,230.13,8.45;6,61.38,657.77,95.23,8.45" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,182.96,615.93,108.56,8.45;6,61.38,626.39,164.25,8.45">Information retrieval system evaluation: effort, sensitivity, and reliability</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,247.69,626.39,43.83,8.45;6,61.38,636.85,230.14,8.45;6,61.38,647.31,226.40,8.45">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval Salvador</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval Salvador<address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,61.38,668.23,230.14,8.45;6,61.38,678.69,230.13,8.45;6,61.38,689.15,56.07,8.45" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<title level="m" coord="6,193.07,668.23,98.45,8.45;6,61.38,678.69,87.71,8.45">Information retrieval : algorithms and heuristics</title>
		<meeting><address><addrLine>Great Britain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed. Dordrecht</note>
</biblStruct>

<biblStruct coords="6,61.38,699.62,230.13,8.45;6,61.38,710.08,230.14,8.45;6,61.38,720.53,177.50,8.45" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,235.29,699.62,56.22,8.45;6,61.38,710.08,226.34,8.45">Electronic Discovery and the Challenge Posed by the Sarbanes-Oxley Act</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Garrie</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,61.38,720.53,142.58,8.45">UCLA Journal of Law and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
