<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.63,97.62,410.16,10.75">SEMANTIC EXTENSIONS OF THE EPHYRA QA SYSTEM FOR TREC 2007</title>
				<funder ref="#_uaRbAX7">
					<orgName type="full">Carnegie Mellon University and Universität Karlsruhe possible</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,63.51,126.77,70.55,10.31"><forename type="first">Nico</forename><surname>Schlaefer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,144.76,126.77,64.64,10.31"><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.11,126.77,80.82,10.31"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.64,126.77,67.41,10.31"><forename type="first">Guido</forename><surname>Sautter</surname></persName>
							<email>sautter@ira.uka.de</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Program Structures School of Computer Science and Data Organization Carnegie Mellon University Fakultät für Informatik</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.76,126.77,68.44,10.31"><forename type="first">Manas</forename><surname>Pathak</surname></persName>
							<email>manasp@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,486.85,126.77,58.33,10.31"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.63,97.62,410.16,10.75">SEMANTIC EXTENSIONS OF THE EPHYRA QA SYSTEM FOR TREC 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">961E1845DCFDB8B5E4C00ECA4D197D54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe recent extensions to the Ephyra question answering (QA) system and their evaluation in the TREC 2007 QA track. Existing syntactic answer extraction approaches for factoid and list questions have been complemented with a high-accuracy semantic approach that generates a semantic representation of the question and extracts answer candidates from similar semantic structures in the corpus. Candidates found by different answer extractors are combined and ranked by a statistical framework that integrates a variety of answer validation techniques and similarity measures to estimate a probability for each candidate. A novel answer type classifier combines a statistical model and hand-coded rules to predict the answer type based on syntactic and semantic features of the question. Our approach for the 'other' questions uses Wikipedia and Google to judge the relevance of answer candidates found in the corpora.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper we describe the Ephyra question answering system that has been evaluated in the TREC 2007 QA main track. The system extends the approach we reported on at last year's TREC evaluation <ref type="bibr" coords="1,126.99,535.14,11.62,8.64" target="#b1">[1]</ref> and the overall architecture and design goals have been described in past papers <ref type="bibr" coords="1,221.02,547.09,10.79,8.64" target="#b2">[2,</ref><ref type="bibr" coords="1,234.71,547.09,7.19,8.64" target="#b3">3]</ref>. Here we focus on recent improvements and new techniques that proved effective in this year's evaluation.</p><p>We augmented our answer extraction approaches for factoid and list questions with a high-accuracy semantic answer extractor that is based on semantic role labeling. The question is transformed into a semantic representation and answer candidates are extracted from phrases which match this representation. Different query generation techniques are used to retrieve relevant text passages, ranging from simple keyword queries to specific query strings such as reformulations of the question into answer patterns. WordNet is used to expand query terms with semantically related concepts. As full semantic role labeling of all retrieved sentences is a prohibitively time-intensive task, we carefully select candidates that are further analyzed and transformed into semantic representations. A fuzzy similarity metric is then used to compare these representations to the question representation to identify semantic structures that potentially contain an answer. The similarity measure was designed to be flexible and robust in order to maximize the recall of the answer extractor.</p><p>A statistical framework for answer selection combines the answer candidates produced by this semantic approach and previously developed answer type based and pattern based extractors <ref type="bibr" coords="1,358.21,360.48,10.58,8.64" target="#b4">[4]</ref>. The framework estimates the probability of an answer candidate based on a set of answer validation and similarity features. Validation features use external semantic resources to verify an answer, while similarity features measure the syntactic and semantic similarity to other candidates.</p><p>One of the crucial steps in answering factoid and list questions is the classification of the questions with respected to the expected answer type. For this purpose, we have developed a classifier that uses both manually encoded rules and a statistical model to predict the answer type given a set of syntactic and semantic features of the question. This hybrid approach outperforms our previous pattern-based classifier.</p><p>To answer the 'other' questions, we use Wikipedia and Google searches to identify keywords that frequently occur in the proximity of the target. Assuming that these keywords provide relevant information on the target, we favor those answer candidates that contain the frequent terms. Additional filtering techniques are used to drop redundant and non-informative answers to improve the precision of our responses.</p><p>In this year's TREC evaluation, answers were extracted from both the AQUAINT2 newswire corpus and the Blog06 corpus, a crawl of a large volume of Web logs. We submitted runs using only the newswire corpus and a combination of both corpora, which gave us an insight into the benefits and also the challenges arising from the use of a large corpus containing a significant proportion of noisy text as an additional source.</p><p>The remainder of this paper is organized as follows. Section 2 gives an overview of related approaches. Section 3 discusses recent improvements and extensions of our pipeline for factoid and list questions, while Section 4 deals with the 'other' questions. Section 5 describes our TREC runs and summarizes the evaluation results; Section 6 outlines open issues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Moldovan and Novischi <ref type="bibr" coords="2,155.33,163.71,11.62,8.64" target="#b5">[5]</ref> use relations in WordNet <ref type="bibr" coords="2,275.45,163.71,11.62,8.64" target="#b6">[6]</ref> to derive topically related terms for query expansion. Terms are considered semantically similar if they are linked through a lexical chain, a sequence of related WordNet synsets. For each of the WordNet relations, a weight has been determined empirically that reflects the degree of similarity between related synsets. We adopt the idea of lexical chains and reuse the proposed weights to calculate confidence scores for semantically related concepts.</p><p>Nyberg et al. <ref type="bibr" coords="2,125.97,271.64,11.61,8.64">[7]</ref> describe how the JAVELIN QA system has been extended with domain semantics to answer questions in a restricted domain. A manually created ontology covers frequent concepts in this domain and English expressions with domain-specific meanings. <ref type="bibr" coords="2,186.57,319.46,11.62,8.64" target="#b8">[8]</ref> introduces a lightweight knowledge-based reasoning framework for JAVELIN. Questions and text passages are transformed into uniform semantic representations and a flexible unification framework matches questions with relevant passages, using weighted semantic relations between terms.</p><p>The QA system from the National University of Singapore <ref type="bibr" coords="2,74.38,403.48,11.62,8.64" target="#b9">[9]</ref> used the semantic role labeling system ASSERT <ref type="bibr" coords="2,281.61,403.48,16.60,8.64" target="#b10">[10]</ref> to answer factoid and list questions in past TREC evaluations. Predicate-argument structures are extracted from the question and answer sentences. The predicates are then compared using a similarity metric composed of the similarity of the predicate verbs and the similarity of the arguments. We refined the similarity measure for predicates to make it more flexible and robust to parsing errors in order to improve the recall of this approach. The terms in the arguments are expanded with WordNet, and the semantic similarity of the arguments is measured in addition to their syntactic similarity.</p><p>To select the most probable answer(s) from an answer candidate list, QA systems have applied several different answer selection approaches. One of the most common approaches relies on external resources such as WordNet, CYC and gazetteers for answer validation <ref type="bibr" coords="2,204.87,583.14,15.77,8.64" target="#b11">[11,</ref><ref type="bibr" coords="2,224.02,583.14,12.45,8.64" target="#b12">12,</ref><ref type="bibr" coords="2,239.84,583.14,11.83,8.64" target="#b13">13]</ref>. The Web has also been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords <ref type="bibr" coords="2,175.11,619.01,15.27,8.64" target="#b14">[14]</ref>. Collecting evidence from similar answer candidates to boost the rank of redundant answers is also important for answer selection. One popular approach is to cluster identical or complementary answers <ref type="bibr" coords="2,282.44,654.87,15.77,8.64" target="#b15">[15,</ref><ref type="bibr" coords="2,54.43,666.83,11.83,8.64" target="#b16">16]</ref>. Our recent work <ref type="bibr" coords="2,142.06,666.83,11.62,8.64" target="#b4">[4]</ref> proposed a unified probabilistic answer ranking framework which combines different techniques to validate answers and exploit answer redundancy for the answer selection task. The results in this year's TREC evaluation demonstrate the effectiveness of this framework.</p><p>The evolution of answer type classification approaches mirrors that of natural language understanding algorithms in general: initial approaches consisting of handcoded rules <ref type="bibr" coords="2,542.40,99.39,16.60,8.64" target="#b16">[16]</ref> or patterns <ref type="bibr" coords="2,360.06,111.34,16.60,8.64" target="#b17">[17]</ref> were followed by a variety of data-driven approaches based on simple <ref type="bibr" coords="2,419.51,123.30,16.60,8.64" target="#b18">[18]</ref> or complex <ref type="bibr" coords="2,485.81,123.30,16.60,8.64" target="#b19">[19]</ref> features, with the occasional emergence of a hybrid algorithm <ref type="bibr" coords="2,510.91,135.25,15.27,8.64" target="#b20">[20]</ref>. While a machine learning technique based on syntactic and semantic features achieves one of the highest reported accuracies (89.3%) for classifying English questions with fine granularity (50 types) using a standard data set <ref type="bibr" coords="2,476.99,183.07,15.27,8.64" target="#b19">[19]</ref>, it is also interesting to note that Day et al. <ref type="bibr" coords="2,441.64,195.03,16.60,8.64" target="#b20">[20]</ref> report that combining a rule-based classifier with a data-driven classifier yields higher accuracy on Chinese questions than either approach used in isolation. Notably absent is an empirical study comparing different global strategies for combining manually-encoded and automatically-acquired linguistic knowledge for answer type classification.</p><p>A simple but very effective approach for answering the 'other' questions has been introduced by Kaisser et al. <ref type="bibr" coords="2,539.91,291.81,15.27,8.64" target="#b21">[21]</ref>. Google is used to search for text snippets that contain the target of the question and the frequencies of the terms in the snippets are counted. Assuming that frequent terms provide important information on the target, answer candidates containing these terms are assigned higher confidence scores. We adopt a similar approach but use Wikipedia in conjunction with Google to determine the term frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FACTOID AND LIST QUESTIONS</head><p>Our approach for factoid and list questions is based on a pipeline architecture consisting of components for question analysis, query generation, search, and answer extraction and selection. While the pipeline layout has been described in some detail in previous papers <ref type="bibr" coords="2,415.34,481.67,10.79,8.64" target="#b2">[2,</ref><ref type="bibr" coords="2,428.67,481.67,7.19,8.64" target="#b3">3]</ref>, this section focuses on recent extensions that were deployed for this year's TREC evaluation, including an improved answer type classifier (Section 3.1), a high-precision semantic answer extraction approach that is based on semantic role labeling (Sections 3.2 -3.5), and a probabilistic framework for the combination and selection of answer candidates (Section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Answer Type Classification</head><p>Classifying questions in terms of their expected answer type is a crucial first step in automatic question answering. Some QA systems rely solely on handcoded rules or heuristics to determine the expected type of the answer, while other systems rely solely on automatically-learned patterns or probabilistic models based on features to classify questions. For this year's TREC competition, we adopted a hybrid approach that combines manually-encoded rules with a learned model.</p><p>The result of answer type classification is used downstream in the Ephyra system to aid answer extraction and selection. Top-level answer types used in Ephyra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Answer Type Hierarchy</head><p>A crucial but often variable aspect of answer type classification is the answer (or question) typology used. Early QA systems used a small set of coarse-grained types, but modern systems, including Ephyra, usually use a larger set of finegrained types. Because of the underspecified nature of many questions (e.g. Where is the conference? as opposed to What city is the conference being held in?), the typology used is usually a hierarchy, allowing for varying levels of granularity in the classification. Ephyra uses a set of 154 answer types arranged in a hierarchy with 44 top-level categories, which are shown in Table <ref type="table" coords="3,119.60,456.05,3.74,8.64" target="#tab_0">1</ref>. The hierarchy was designed to cover answer types frequently found in past TREC questions. Named entity recognizers have been devised to extract candidate answers of all of these types except for a few high-level categories such as food or event. We use the Stanford NE Recognizer <ref type="bibr" coords="3,76.43,515.83,16.60,8.64" target="#b22">[22]</ref> to extract answers of the types person, organization and location and rule-and list-based taggers we built for the remaining types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Features</head><p>The features we use for answer type classification fall into three categories: lexical, syntactic, and semantic. They are described in detail below. All of our features actually appear as binary features when input to the learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical Features</head><p>• UNIGRAM : Individual tokens present in the question.</p><p>• BIGRAM : Pairs of adjacent tokens in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic Features</head><p>• FOCUS ADJ : The focus adjective or adverb of the question; only applicable for how questions that ask about the degree of some property, such as How fast can whales travel?</p><p>• MAIN VERB : The main verb of the question, determined from the syntactic parse of the question using Collins-style head rules.</p><p>• WH WORD : The question word.</p><p>• WH DETERMINER : Indicates whether the question word serves as a determiner to the focus word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Features</head><p>• FOCUS TYPE : The semantic type of the question focus word (e.g. city in Which city hosted the 2002 Winter Olympics?); only applicable for questions with a whword of what or which. The focus word is identified using a manually-compiled set of syntactic patterns which are matched against a syntactic parse of the question.</p><p>The semantic type of the focus word is determined by traversing the WordNet hypernym tree for each sense of the focus word,</p><p>looking up each hypernym synset in a manuallyconstructed, many-to-one mapping<ref type="foot" coords="3,503.39,396.95,3.49,6.05" target="#foot_0">1</ref> from Word-Net synsets to a set of general answer types,</p><p>adding any successful mappings to a set of candidate types, and selecting the candidate type which corresponds to the hypernym synset with the shortest hypernym tree traversal.</p><p>Consequently, an incorrect disambiguation of the focus word sense is possible, although not common. If the focus word's semantic type cannot be determined using this approach, the focus word itself is used as the value of this feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Classification Approach</head><p>Initially, the above features were meant to be used for training a model-based classifier. However, given information as predictive as the focus word type, focus adjective, and the question word, the answer type classification task becomes fairly straightforward. This motivated us to construct a simple rule-based classifier that tests the same features (including BIGRAM and UNIGRAM). We also built a hybrid classifier that combines the outputs of the model-based and rule-based classifiers using their associated confidence scores. To compare these three approaches, we measured their classification accuracy on TREC 13 questions, with questions from TREC 8-12, 14 and 15 serving as training data for the model-based classifier and as development data for the rulebased classifier. The results are shown in Table <ref type="table" coords="4,240.44,224.74,3.74,8.64" target="#tab_1">2</ref>. Because the hybrid classifier outperformed both the model-based classifier and the rule-based classifier, this is the approach we adopted. In the final system used for this year's evaluation, the modelbased component of our approach was trained on questions from TREC 8-15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic Parsing of Questions</head><p>We augmented our answer type based and pattern based answer extraction techniques <ref type="bibr" coords="4,163.53,343.77,10.79,8.64" target="#b2">[2,</ref><ref type="bibr" coords="4,176.98,343.77,8.30,8.64" target="#b3">3]</ref> with a semantic parsing approach that generates a semantic representation of the question and extracts answer candidates from phrases in the corpus that match this representation.</p><p>The semantic role labeling (SRL) system ASSERT was used to label semantic structures in questions and the corpus, using the PropBank inventory of semantic roles <ref type="bibr" coords="4,254.29,415.78,15.27,8.64" target="#b10">[10]</ref>. SRL is a form of shallow semantic parsing that labels predicateargument structures consisting of a target verb, usually describing an event, and a set of arguments along with their semantic roles in the event. For instance, the sentence The CMU campus at the west coast was founded in 2002. contains the predicate-argument structure Since SRL systems often fail to correctly label the semantic roles in questions, we first transform the question string into a statement, using a number of simple syntactic transformation rules. An appropriate rule is selected based on the interrogative pronoun of the question, adjacent prepositions and noun phrases, and the expected answer type. Examples of transformation rules are given in Table <ref type="table" coords="4,209.05,678.78,3.74,8.64" target="#tab_2">3</ref>. For instance, applying the second rule, the question In what year was the CMU campus at the west coast established? can be transformed into the statement The CMU campus at the west coast was established now. The phrase now is a placeholder for an argument that specifies the time and represents the answer to the question. ASSERT is used to extract predicate-argument structures from the resulting statement. The placeholder argument is dropped and the corresponding semantic role is marked as missing, indicating that this is the information the question is seeking. The arguments are further split into terms, which are units of meaning consisting of one or more tokens (e.g. "Carnegie Mellon", "west coast"). Terms are used for query generation and expansion (Section 3.3) and to measure the similarity between predicates in questions and corpus sentences (Section 3.4).</p><p>We use our NE recognizers and WordNet lookups to extract compound terms from the predicate arguments. Word-Net is also used to expand terms with semantically similar concepts, following an approach similar to <ref type="bibr" coords="4,495.75,505.56,10.58,8.64" target="#b9">[9]</ref>. Each term is mapped to a synset in WordNet and a breadth-first search along WordNet relations identifies related synsets. We make use of relations such as synonym, hypernym, hyponym, holonym and meronym and restrict the search depth to a maximum of two relations. The related terms are assigned confidence values based on the relations on the path from the original term, adopting the weights suggested in <ref type="bibr" coords="4,475.99,589.24,10.58,8.64" target="#b5">[5]</ref>.</p><p>Figure <ref type="figure" coords="4,359.91,602.21,4.98,8.64" target="#fig_1">1</ref> shows the semantic representation that is generated for the sample question In what year was the CMU campus at the west coast established?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Query Generation and Expansion</head><p>We generate various types of queries, ranging from recalloriented queries such as bags of keywords to specific query strings which retrieve text passages that closely match the structure of the question: The above queries are used to retrieve text passages from both the Web and the corpora used in this year's TREC evaluation (AQUAINT2 and Blog06), and answer candidates are extracted from both sources. The Web answers are matched with the answers found in the corpora to identify supporting documents, following the answer extraction approach described in <ref type="bibr" coords="5,95.93,478.42,10.58,8.64" target="#b2">[2]</ref>.</p><p>We use Google to search the Web and the Indri search engine, which is part of the Lemur toolkit <ref type="bibr" coords="5,233.48,502.33,15.27,8.64" target="#b24">[24]</ref>, to retrieve passages from the TREC corpora. For the top 100 Google snippets, we fetch the entire Web documents and convert them to plain text. Both the Web documents and the passages from the corpora are segmented into sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extraction of Candidate Answers</head><p>Semantic parsing is a time-intensive task and not all of the retrieved sentences are equally likely to contain an answer. Thus we first narrow down the number of candidate sentences before we parse them with ASSERT. A sentence is considered relevant if it meets the following constraints:</p><p>• The number of tokens in the sentence falls between upper and lower thresholds that are typically satisfied by a natural language sentence.</p><p>• The sentence contains the predicate verb from the question or a semantically related verb.</p><p>• If the answer type of the question is known, the sentence must contain an entity of that type.</p><p>• The sentence contains at least one additional term that is semantically similar to a term in the question.</p><p>Sentences that pass the above tests are parsed and transformed into a semantic representation similar to the one given for the question in Figure <ref type="figure" coords="5,421.97,277.37,3.74,8.64" target="#fig_1">1</ref>. The semantic structure of each sentence is compared to the question and a similarity score is calculated as described in the following.</p><p>The term similarity of two terms t 1 and t 2 is defined as the Jaccard coefficient of the sets of content words W 1 and W 2 in the terms:</p><formula xml:id="formula_0" coords="5,355.50,368.69,162.01,23.23">S T (t 1 , t 2 ) := J(W 1 , W 2 ) = |W 1 ∩ W 2 | |W 1 ∪ W 2 |</formula><p>The expanded term similarity between an answer term t a and a question term t q takes into account that each question term t has related concepts R = {r 1 , ..., r n } with weights w(r 1 ), ..., w(r n ):</p><formula xml:id="formula_1" coords="5,349.56,467.02,175.09,15.05">S ET (t a , t q ) := max t∈{tq}∪R (w(t) × S T (t a , t))</formula><p>where w(t q ) := 1</p><p>The verb similarity of an answer predicate p a and a question predicate p q is the expanded term similarity of the predicate verbs v a and v q :</p><formula xml:id="formula_2" coords="5,382.02,566.47,110.17,9.65">S V (p a , p q ) := S ET (v a , v q )</formula><p>The argument similarity of an answer predicate p a and a question predicate p q is determined by comparing the sets of terms within the arguments of the predicates, denoted T a and T q . We have extended the concept of the Jaccard coefficient to take the semantic similarity of terms into account, rather than just distinguishing between common terms and terms that appear exclusively in one set:</p><formula xml:id="formula_3" coords="5,324.73,681.70,216.21,42.84">S A (p a , p q ) := ta∈Ta max tq∈Tq (S ET (t a , t q )) |T q | + count ta∈Ta max tq∈Tq (S ET (t a , t q )) = 0</formula><p>Each term in T a is compared to all terms in T q and the maximum of the similarity scores is computed. If the maximum is larger than 0, then the term is assumed to be covered by both predicates and the numerator of the coefficient is incremented by this score, else the denominator is incremented by 1.</p><p>Finally, the predicate similarity of an answer predicate p a and a question predicate p q is the product of their verb and argument similarity scores: S P (p a , p q ) = S V (p a , p q ) × S A (p a , p q ) This scoring mechanism has been designed to be flexible and robust to parsing errors in order to maximize the recall of the answer extraction. The idea of using a Jaccard coefficient to measure the similarity of all arguments as a whole was introduced in <ref type="bibr" coords="6,131.70,263.21,10.58,8.64" target="#b9">[9]</ref>. It takes into account that SRL systems often fail to assign the correct semantic roles to the arguments, which makes a per-argument comparison infeasible. We extended this idea to perform a fuzzy matching not only for arguments but also at the level of terms.</p><p>If the confidence score of a predicate is larger than 0, it is considered semantically similar to the question and one of the following strategies is used to extract factoid answers:</p><p>• If the answer type of the question is known, entities of the expected type are extracted from all arguments of the answer predicate. This takes into account that SRL systems often mislabel the arguments.</p><p>• If the answer type is unknown and the answer predicate has an argument with the role that is missing in the question, this argument is extracted as an answer.</p><p>The confidence score of an answer is the sum of the confidence scores of all the predicates it was extracted from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Score Normalization and Combination</head><p>The answer candidates retrieved with this semantic approach are combined with candidates from our answer type based and pattern based extractors <ref type="bibr" coords="6,174.46,550.74,10.79,8.64" target="#b2">[2,</ref><ref type="bibr" coords="6,189.29,550.74,7.19,8.64" target="#b3">3]</ref>. Since these extraction techniques use different underlying scoring mechanisms that produce incomparable scores, it is necessary to normalize the confidence scores before merging the answers. We trained an AdaBoost classifier <ref type="bibr" coords="6,133.89,598.56,16.60,8.64" target="#b25">[25]</ref> that uses a decision tree as the underlying weak learner to classify answer candidates into correct and incorrect ones. The classifier is applied to unseen answer candidates and the probability of the positive class is used as a normalized confidence score. The following features were used to train the classifier:</p><p>• Score assigned to the candidate by the extractor.</p><p>• Answer extractor that found the candidate.</p><p>• Predicted answer type(s) of the question.</p><p>• Number of candidates from the same extractor.</p><p>• Minimum and maximum score over all candidates.</p><p>For one of our runs, we used a simple score combination scheme (known as CombMNZ <ref type="bibr" coords="6,439.84,126.58,15.93,8.64" target="#b26">[26]</ref>) to merge the normalized scores of a candidate found by multiple extractors: The combined score is the sum of the (normalized) scores from all extractors, multiplied by the number of extractors that found the answer. However, this technique was outperformed by the more general answer selection approach described in the following section, which was used for the remaining runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Answer Selection</head><p>The Answer Generator (AG) is responsible for selecting the correct answers from the candidates produced by our answer extractors. A statistical framework estimates the probability of an individual answer candidate given a set of validation features that predict its relevance according to external resources, and a number of similarity features that exploit redundancy among the answer candidates. The AG has been described in detail in our previous work <ref type="bibr" coords="6,476.18,330.37,10.58,8.64" target="#b4">[4]</ref>.</p><p>To estimate the relevance of an answer candidate, we use four external resources. Gazetteers and WordNet are used in a knowledge-based approach (e.g. to check whether a candidate satisfies the relationship described in the question such as IS-A(Shanghai, city) or IS-IN(Shanghai, China)). The Web and Wikipedia are used in a data-driven approach. For instance, if there is a Wikipedia document whose title matches the answer candidate, the document is analyzed to obtain a tf-idf score, which is used as a relevance feature. Web snippets are used to calculate a word distance between an answer candidate and question keywords.</p><p>To identify similar answer candidates found by different extractors, we combine various syntactic and semantic similarity features. String distance metrics such as the Levenshtein distance and the cosine similarity are used to measure the syntactic similarity of answer candidates. A database of synonyms was generated from WordNet, the CIA World Factbook and Wikipedia.</p><p>As we use three different approaches to extract answer candidates, and each extractor sometimes produces more than 100 candidates, the AG only estimates probabilities for the top 50 answer candidates from each extractor. The answer candidates are then reranked according to these probabilities. For factoid questions, the highest ranked candidate is chosen as the final answer.</p><p>For list questions, we return all candidates with estimated probabilities of at least 25% of the probability of the top answer.</p><p>In our experiments on previous TREC questions, we first attempted to maximize the F1 score by returning all candidates with probabilities of at least 0.5. However, the probabilities assigned by the AG turned out to be rather unreliable estimates of the correctness of an answer candidate because of the high variance of the original scores from our answer extractors. For instance, the score of the top answer was sometimes very low, which resulted in estimated probabilities below the threshold of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">'OTHER' QUESTIONS</head><p>Our approach for the 'other' questions uses answer projection from the Web onto the corpora, picking up the idea of Kaisser et al. <ref type="bibr" coords="7,76.95,440.90,15.27,8.64" target="#b21">[21]</ref>. However, we use Wikipedia in addition to Google to retrieve terms that are important in the context of the target. A Google query can be ambiguous and the results may be unrelated to the target, while we can retrieve information from Wikipedia unambiguously by searching for an article on the target. We furthermore assume that in the online encyclopedia, users make sure the most relevant information on the subject (target) is given in a concise and concentrated fashion with little noise. For targets not found in Wikipedia, we extract terms from Google snippets as a fallback solution.</p><p>For selecting the relevant snippets from the corpus, we further deploy several of the filtering techniques introduced last year <ref type="bibr" coords="7,90.79,584.53,11.61,8.64" target="#b2">[2]</ref> to eliminate nonsensical, redundant or irrelevant information. The modular architecture of Ephyra enabled us to evaluate various combinations of filters, parameterizations and orders. In the following we describe the key components of our pipeline, illustrated in Figure <ref type="figure" coords="7,199.15,632.35,3.74,8.64" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Snippet Retrieval</head><p>We first retrieve whole paragraphs from the corpora, using the Indri IR engine from the Lemur toolkit <ref type="bibr" coords="7,211.27,690.74,15.27,8.64" target="#b24">[24]</ref>. As the initial set of passages can be quite large, we run them through a number of filtering mechanisms before selecting answer candidates:</p><p>Reduction of Snippet Size. In order to allow more finegrained filtering operations and to increase the precision of our answers, we reduce the size of the passages as much as possible. Previously reported results <ref type="bibr" coords="7,464.89,111.34,15.77,8.64" target="#b27">[27,</ref><ref type="bibr" coords="7,483.55,111.34,7.19,8.64" target="#b2">2]</ref>, and an analysis of the snippets that were judged vital or OK in past TREC evaluations, led us to the conclusion that the most effective answers are sentences or sentence fragments. Thus we split paragraphs into sentences, and we further segment long sentences into their atomic clauses (Figure <ref type="figure" coords="7,471.27,171.12,3.74,8.64" target="#fig_2">2</ref>, Filter 1). This helps us to deal with additional material outside these clauses, such as explanatory prefixes (e.g. "PARIS (France) AFP ...") and indirect speech (e.g. "Secretary Gates said that ..."). By retaining only clauses that contribute relevant information we further increase the precision (Figure <ref type="figure" coords="7,465.44,230.89,3.74,8.64" target="#fig_2">2</ref>, Filters 2 &amp; 3).</p><p>Elimination of Redundancy. In order to minimize the time required for the Web-based scoring process, duplicate snippets are eliminated. Semantic duplicates, i.e. snippets providing the same information with slightly different wording, are detected with a bag-of-words comparison mechanism, which stems content words and ignores stop words in the similarity calculation (Figure <ref type="figure" coords="7,393.24,315.22,3.74,8.64" target="#fig_2">2</ref>, Filter 4).</p><p>Elimination of Useless Snippets. The snippets from the AQUAINT2 corpus turned out to include two special types of likely useless snippets, which mainly appear if the target is a person's name. In particular, these include long lists of proper names without any further information, and snippets that consist of a person's statement in direct speech. We filter out lists of proper names based on the observation that most of their tokens are either stop words, or parts of a proper name and thus capitalized (Figure <ref type="figure" coords="7,412.25,423.46,3.74,8.64" target="#fig_2">2</ref>, Filter 5). Our experiments on previous TREC questions proved that the risk of useful snippets being lost to this filter is very low. Direct speech formulations citing a person's statement sometimes contain useful information, but in general they deliver opinions rather than facts, so we decided to also filter them out (Figure <ref type="figure" coords="7,482.34,483.24,3.74,8.64" target="#fig_2">2</ref>, Filter 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Snippet Selection</head><p>To select the actual answer snippets from the ones returned by the retrieval and filtering step, we adopted the Web-based approach proposed in <ref type="bibr" coords="7,407.26,557.10,16.60,8.64" target="#b21">[21]</ref> with Wikipedia as an additional information source. We assume retrieving an article from Wikipedia is considerably less ambiguous than retrieving text snippets from a Web search engine using the target as a keyword query. Only if we do not find a Wikipedia article, we use Google as a fallback. Experimental results on past TREC data encouraged us to use this combined approach. The complete scoring process consists of several steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Query Generation</head><p>In order to become less vulnerable to different possible ways of naming the target, we generate several different queries from the target string, as proposed in <ref type="bibr" coords="7,468.30,714.65,15.27,8.64" target="#b21">[21]</ref>. In particular, the generation process comprises two steps: First, we identify the type of the target, using the Stanford NE Recognizer <ref type="bibr" coords="8,279.12,87.43,15.27,8.64" target="#b22">[22]</ref>. Second, we vary the target string using several modification rules that depend on the target type:</p><p>• If the target contains a part enclosed in brackets, we use the part inside and the part(s) outside as individual queries.</p><p>• If the target contains a proper name, we use it as an individual query.</p><p>• If the target is an organization, we produce variations with and without determiner, and we produce an acronym query from the organization's name.</p><p>• For the Google fallback, we also generate quoted versions of all queries that consist of a proper name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Web Term Retrieval</head><p>Since Wikipedia is our primary source of Web terms, we first try to retrieve a Wikipedia article on the target. If that succeeds, we do not retrieve any further Web documents. Otherwise, we try to retrieve articles on the parts we extracted from the target during the query generation. If this fails as well, we obtain the top 100 snippets from Google for each of our generated queries. Once we have obtained all necessary Web documents according to the policy above, we extract all the terms from them and count their frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Snippet Scoring</head><p>Our scoring mechanism for the corpus snippets uses the general method of score computation reported by Kaisser et al.</p><p>[21] (Figure <ref type="figure" coords="8,106.53,496.77,3.74,8.64" target="#fig_2">2</ref>, Filter 7), with the following refinements: (a) A count decrease parameter indicates how the counter of a term is decreased after it has contributed to the score of a snippet that is actually selected. (b) We do not simply score the snippets using the terms they contain, because this would favor longer snippets over shorter ones. Instead we normalize the score of a snippet using the logarithm of the number of terms it contains. (c) We also found that using the plain count of Web terms for computing the scores of the corpus snippets over-weights common terms, while under-weighting more specific terms. To compensate for this effect, we normalize the term counters by the logarithm of the term's global frequency, which we obtain from a dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Answer Length</head><p>To make sure our answer snippets do not exceed the maximum of 7000 characters, we finally apply a filter that drops all snippets after the top ones have a combined length of some threshold ≤ 7000 (Figure <ref type="figure" coords="8,420.26,75.48,3.74,8.64" target="#fig_2">2</ref>, Filter 8). The relatively low impact of precision might encourage to return the maximum allowed number of characters, but our experiments with previous TREC targets revealed that with almost every parameter combination for the scoring, the optimal total length of the answer was 3000 characters. Consequently, we used 3000 as the cutoff length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION RESULTS</head><p>We submitted 3 runs, differing in the answer selection approach and the corpora used for answer projection (factoid and list questions) and to extract information nuggets ('other' questions). Ephyra1 exclusively used the AQUAINT2 corpus, while Ephyra3 combined the AQUAINT2 and Blog06 corpora and treated them as a single knowledge source. Ephyra2 deployed both corpora to find supporting documents for factoid and list questions, but restricted the nugget extraction for the 'other' questions to the AQUAINT2 corpus. In the runs Ephyra1 and Ephyra3 the Answer Generator (AG, cf. Section 3.6) was used to select and combine candidate answers. Ephyra2 applied a simple score combination approach to merge candidates from different extractors (cf. Section 3.5). Figure <ref type="figure" coords="8,371.66,358.40,4.98,8.64">4</ref> shows our evaluation results and compares them to the median over all 51 runs. The best setup for factoid and list questions (Ephyra3) projected the candidate answers found in the Web onto both the AQUAINT2 and the Blog06 corpus and deployed the AG to generate the final list of ranked answers. A comparison to the runs Ephyra1 and Ephyra2 shows to what extend the Blog06 corpus and the AG improved the overall performance.</p><p>For the 'other' questions, it proved most effective to extract information nuggets from the AQUAINT2 corpus only. Ephyra1 and Ephyra2 were identical runs that restricted the search to the newswire text. The nuggets extracted from the Blog06 corpus in the run Ephyra3 rarely contained relevant information on the target, but often they were meaningless sentence fragments or not even natural language phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">FUTURE WORK</head><p>A major bottleneck of the previously described semantic approach for question analysis and answer extraction is the coverage and reliability of the semantic parser. It has been shown that by integrating multiple semantic role labeling (SRL) systems, the robustness can be improved significantly <ref type="bibr" coords="8,526.33,630.96,15.27,8.64" target="#b28">[28]</ref>. A combination of SRL systems is particularly beneficial if the systems use different syntactic parsers <ref type="bibr" coords="8,468.01,654.87,15.27,8.64" target="#b29">[29]</ref>. Yet there remains a significant portion of questions and answer sentences with semantic structures that do not fit into the schema of predicate verbs and arguments. It would therefore be desirable to cover a wider range of semantic structures such as the semantic frames used in FrameNet <ref type="bibr" coords="8,430.58,714.65,15.27,8.64" target="#b30">[30]</ref> Currently we do not pre-annotate the TREC corpora but we build a full-text index and use simple boolean queries for both the Web search and to retrieve text passages from the corpora. By annotating the corpora with semantic information and integrating these annotations in the index, we could (a) improve the runtime performance and (b) formulate structured queries that combine evidence from different documents. For instance, one could search for all organizations X in the corpora that satisfy the constraints imposed by the predicate-argument structures based(ARG1: X, ARGM-LOC: Japan) and manufacture(ARG0: X, ARG1: SUV) to obtain a list of Japanese car makers that offer SUVs.</p><p>The Answer Generator can be further improved by incorporating additional features to validate individual answer candidates and to identify semantically similar answers among the candidates. We will also need to conduct additional experiments to determine a more flexible cutoff strategy for list questions. Furthermore, we consider extending the AG to perform answer selection not only for factoid answers but also for more complex answers such as the definitional phrases retrieved for the 'other' questions.</p><p>Extensions to our answer type classifier would include analyzing precisely how the knowledge encoded by the rules augments the knowledge contained in the training data, in the hope of discovering whether it helps to resolve noise, cover gaps, or do both. It also seems important to address questions such as: How can we most effectively cover gaps in the data with hand-crafted rules? How can we most effectively cover gaps in the hand-crafted rules by learning from data sets that target specific, variable aspects of language? Which of these strategies yields the best performance? Are there general linguistic principles that can guide the decision of whether to treat a particular phenomenon using rules or data?</p><p>Experiments on past TREC data revealed that the effectiveness of our answers to an 'other' question highly depends on the choice of (a) the source of Web terms, (b) the parameter values for the scoring process, and (c) the cutoff length. Therefore we need to thoroughly investigate characteristics of the individual targets and find categories of targets for which an individual combination of term source and parameter values yields the optimal result. We then need to find criteria for assigning each target to a category, so that we can choose a setup for our scoring mechanism accordingly. Furthermore, additional filtering techniques and more robust parsing and sentence segmentation approaches will be required to extract useful information nuggets from resources with a high content of noise, such as the Blog06 corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,69.37,507.97,85.74,8.96;4,69.37,525.90,115.44,8.96;4,69.37,543.83,134.20,8.96;4,69.37,561.77,99.35,8.96;4,54.43,582.68,243.78,8.82;4,54.43,594.63,167.69,8.82"><head>•</head><label></label><figDesc>TARGET: founded • ARG1: The CMU campus • ARGM LOC: at the west coast • ARGM TMP: in 2002 where ARG1 refers to the patient or theme, ARGM LOC to the location and ARGM TMP to the time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,327.03,277.81,220.15,9.03"><head>•Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Semantic representation of the sample question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,81.18,297.22,190.27,9.03;7,54.43,72.00,243.78,210.18"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pipeline layout for the 'other' questions.</figDesc><graphic coords="7,54.43,72.00,243.78,210.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,75.28,74.28,202.08,210.10"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table coords="3,75.28,74.28,202.08,188.36"><row><cell></cell><cell>Answer Types</cell><cell></cell></row><row><cell>acronym</cell><cell>language</cell><cell>rate</cell></row><row><cell>angle</cell><cell>legalSentence</cell><cell>relation</cell></row><row><cell>birthstone</cell><cell>location</cell><cell>religion</cell></row><row><cell>bodyPart</cell><cell>material</cell><cell>score</cell></row><row><cell cols="2">causeOfDeath medicalTreatment</cell><cell>size</cell></row><row><cell>color</cell><cell>money</cell><cell>socialTitle</cell></row><row><cell>creature</cell><cell>musicalInstrument</cell><cell>sport</cell></row><row><cell>crime</cell><cell>musicType</cell><cell>style</cell></row><row><cell>date</cell><cell>nationality</cell><cell>temperature</cell></row><row><cell>disease</cell><cell>number</cell><cell>time</cell></row><row><cell>dramaType</cell><cell>pathogen</cell><cell>timezone</cell></row><row><cell>drug</cell><cell>percentage</cell><cell>unit</cell></row><row><cell>duration</cell><cell>profession</cell><cell>url</cell></row><row><cell>food</cell><cell>properName</cell><cell>zodiacSign</cell></row><row><cell>frequency</cell><cell>range</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,54.43,74.28,243.78,78.66"><head>Table 2 .</head><label>2</label><figDesc>Performance of different answer type classification strategies.</figDesc><table coords="4,126.05,74.28,100.52,44.90"><row><cell>Approach</cell><cell>Acc. (%)</cell></row><row><cell>Model-based</cell><cell>72.38</cell></row><row><cell>Rule-based</cell><cell>68.18</cell></row><row><cell>Hybrid</cell><cell>79.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,69.37,74.28,435.74,321.98"><head>Table 3 .</head><label>3</label><figDesc>Question transformation rules. [PP] refers to an optional preposition, NP to a noun phrase.</figDesc><table coords="5,69.37,74.28,434.26,243.74"><row><cell cols="3">Interrogative Pronoun Answer Type Transformation Rule</cell></row><row><cell>+ Adjacent Phrases</cell><cell></cell><cell></cell></row><row><cell>where</cell><cell>any</cell><cell>drop where, move auxiliary verb to main verb,</cell></row><row><cell></cell><cell></cell><cell>append placeholder argument here</cell></row><row><cell>[PP] + what + NP</cell><cell>date, time</cell><cell>drop [PP] + what + NP, move auxiliary verb to main verb,</cell></row><row><cell></cell><cell>or subtype</cell><cell>append placeholder argument now</cell></row><row><cell cols="3">• Keyword queries are duplicate-free sets of the content</cell></row><row><cell>words in the question.</cell><cell></cell><cell></cell></row><row><cell cols="2">Example: CMU campus west coast established</cell><cell></cell></row><row><cell cols="3">• Term queries consist of the question terms (single to-</cell></row><row><cell cols="3">kens or compound expressions), expanded with seman-</cell></row><row><cell>tically related concepts.</cell><cell></cell><cell></cell></row><row><cell cols="3">Example: (CMU OR "Carnegie Mellon") campus</cell></row><row><cell cols="3">"west coast" (established OR founded OR launched)</cell></row><row><cell cols="3">• Predicate queries are formed from the predicate verb</cell></row><row><cell>and arguments.</cell><cell></cell><cell></cell></row></table><note coords="5,79.33,321.16,201.79,8.82;5,79.33,333.11,44.83,8.59;5,69.37,351.60,228.84,9.03;5,79.33,363.94,127.00,8.64;5,79.33,375.72,203.16,8.82;5,79.33,387.67,60.61,8.59"><p><p><p>Example: "the CMU campus" "at the west coast" established</p>• Reformulation queries are obtained by rephrasing the question into an answer pattern.</p>Example: "the CMU campus at the west coast was established in"</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,445.86,714.65,3.82,8.64"><head>Table 4 .</head><label>4</label><figDesc>. TREC 16 evaluation results.</figDesc><table coords="9,121.63,74.28,370.16,142.14"><row><cell></cell><cell>Ephyra1</cell><cell>Ephyra2</cell><cell>Ephyra3</cell><cell>Median</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(51 runs)</cell></row><row><cell>Corpora</cell><cell>AQUAINT2</cell><cell>AQUAINT2</cell><cell>AQUAINT2,</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Blog06 (factoid, list)</cell><cell>Blog06</cell><cell></cell></row><row><cell>Answer Generator</cell><cell>yes</cell><cell>no</cell><cell>yes</cell><cell>-</cell></row><row><cell>Unsupported (U)</cell><cell>28</cell><cell>21</cell><cell>23</cell><cell>-</cell></row><row><cell>Inexact (X)</cell><cell>18</cell><cell>18</cell><cell>23</cell><cell>-</cell></row><row><cell>Locally correct (L)</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>-</cell></row><row><cell>Factoid accuracy</cell><cell>0.206</cell><cell>0.203</cell><cell>0.208</cell><cell>0.131</cell></row><row><cell>List F 1</cell><cell>0.140</cell><cell>0.123</cell><cell>0.144</cell><cell>0.085</cell></row><row><cell>Other F 3 (pyramid score)</cell><cell>0.189</cell><cell>0.188</cell><cell>0.156</cell><cell>0.118</cell></row><row><cell>Average per-series score</cell><cell>0.181</cell><cell>0.171</cell><cell>0.172</cell><cell>0.108</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,329.56,706.48,229.43,6.91;3,315.21,715.95,15.27,6.91"><p>Similar in spirit to the use of WordNet for answer type classification in<ref type="bibr" coords="3,315.21,715.95,12.22,6.91" target="#b23">[23]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the interACT <rs type="institution">Exchange Program</rs> for making this collaboration between <rs type="funder">Carnegie Mellon University and Universität Karlsruhe possible</rs>. This work was supported in part by the <rs type="programName">AQUAINT program</rs> under grant <rs type="grantNumber">NBCHC040164</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uaRbAX7">
					<idno type="grant-number">NBCHC040164</idno>
					<orgName type="program" subtype="full">AQUAINT program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,400.62,473.27,76.70,8.96" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.79,495.93,222.20,8.64;9,336.79,507.70,222.20,8.82;9,336.79,519.66,158.85,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,468.54,495.93,90.45,8.64;9,336.79,507.88,120.45,8.64">Overview of the TREC 2006 question answering track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,468.07,507.70,90.92,8.59;9,336.79,519.66,129.58,8.59">Proceedings of the Fifteenth Text REtrieval Conference</title>
		<meeting>the Fifteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.79,538.65,222.21,8.64;9,336.79,550.43,222.20,8.82;9,336.79,562.38,170.03,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,544.06,538.65,14.94,8.64;9,336.79,550.61,134.43,8.64">The Ephyra QA system at TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gieselmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sautter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,484.29,550.43,74.70,8.59;9,336.79,562.38,140.76,8.59">Proceedings of the Fifteenth Text REtrieval Conference</title>
		<meeting>the Fifteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.79,581.38,222.20,8.64;9,336.79,593.33,222.20,8.64;9,336.79,605.11,222.20,8.82;9,336.79,617.06,222.20,8.59;9,336.79,629.02,60.60,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,342.20,593.33,216.79,8.64;9,336.79,605.29,113.00,8.64">A pattern learning approach to question answering within the Ephyra framework</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gieselmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schaaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,461.48,605.11,97.51,8.59;9,336.79,617.06,136.68,8.59">Proceedings of the Ninth International Conference on TEXT</title>
		<meeting>the Ninth International Conference on TEXT</meeting>
		<imprint>
			<publisher>SPEECH and DIA-LOGUE</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.79,648.01,222.21,8.64;9,336.79,659.79,222.20,8.82;9,336.79,671.74,108.56,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,455.54,648.01,103.46,8.64;9,336.79,659.97,171.46,8.64">A probabilistic framework for answer selection in question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,522.93,659.79,36.07,8.59;9,336.79,671.74,78.22,8.59">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,336.79,690.74,222.20,8.64;9,336.79,702.51,222.20,8.82;9,336.79,714.47,42.34,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,464.41,690.74,94.58,8.64;9,336.79,702.69,56.09,8.64">Lexical chains for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,404.10,702.51,117.59,8.59">Proceedings of COLING 2002</title>
		<meeting>COLING 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="674" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.01,75.48,222.20,8.64;10,76.00,87.25,113.75,8.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,143.26,75.48,154.95,8.64;10,76.00,87.43,15.95,8.64">WordNet: An electronic lexical database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.01,105.83,222.20,8.64;10,76.00,117.78,222.20,8.64;10,76.00,129.56,222.20,8.82;10,76.00,141.51,222.20,8.59;10,76.00,153.47,101.19,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,241.85,117.78,56.36,8.64;10,76.00,129.74,175.42,8.64">Extending the JAVELIN QA system with domain semantics</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Frederking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schlaikjer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,262.14,129.56,36.07,8.59;10,76.00,141.51,222.20,8.59;10,76.00,153.47,71.47,8.59">Proceedings of the Question Answering in Restricted Domains Workshop at AAAI</title>
		<meeting>the Question Answering in Restricted Domains Workshop at AAAI</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.01,172.04,222.20,8.64;10,76.00,184.00,222.20,8.64;10,76.00,195.77,181.92,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,80.77,184.00,217.44,8.64;10,76.00,195.95,29.37,8.64">Towards light semantic processing for question answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kupsc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,117.35,195.77,110.31,8.59">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.01,214.35,222.20,8.64;10,76.00,226.30,222.20,8.64;10,76.00,238.08,222.20,8.82;10,76.00,250.03,132.01,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,114.37,226.30,183.84,8.64;10,76.00,238.26,86.26,8.64">Using syntactic and semantic relation analysis in question answering</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,176.46,238.08,121.75,8.59;10,76.00,250.03,102.74,8.59">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,268.61,222.20,8.64;10,76.00,280.56,222.20,8.64;10,76.00,292.34,188.00,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,111.50,280.56,186.70,8.64;10,76.00,292.52,35.08,8.64">Shallow semantic parsing using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,123.44,292.34,110.31,8.59">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,310.91,222.20,8.64;10,76.00,322.87,222.20,8.64;10,76.00,334.64,222.20,8.82;10,76.00,346.60,132.01,8.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,121.68,322.87,176.52,8.64;10,76.00,334.82,101.41,8.64">TREC 2002 QA at BBN: Answer selection and confidence estimation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Licuanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,188.08,334.64,110.12,8.59;10,76.00,346.60,102.74,8.59">Proceedings of the Eleventh Text REtrieval Conference</title>
		<meeting>the Eleventh Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,365.17,222.20,8.64;10,76.00,377.13,222.20,8.64;10,76.00,388.91,222.20,8.82;10,76.00,400.86,169.21,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,141.68,377.13,156.53,8.64;10,76.00,389.09,122.17,8.64">A multi-strategy and multi-source approach to question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,218.95,388.91,79.25,8.59;10,76.00,400.86,139.94,8.59">Proceedings of the Eleventh Text REtrieval Conference</title>
		<meeting>the Eleventh Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,419.44,222.20,8.64;10,76.00,431.21,222.20,8.82;10,76.00,443.17,126.08,8.82" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,80.73,431.39,185.24,8.64">Cogex: A logic prover for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,280.39,431.21,17.81,8.59;10,76.00,443.17,95.81,8.59">Proceedings HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,461.74,222.20,8.64;10,76.00,473.70,222.20,8.64;10,76.00,485.47,222.20,8.82;10,76.00,497.43,93.28,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,276.51,461.74,21.70,8.64;10,76.00,473.70,222.20,8.64;10,76.00,485.65,109.47,8.64">Comparing statistical and content-based techniques for answer validation on the Web</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pervete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,202.46,485.47,95.74,8.59;10,76.00,497.43,63.99,8.59">Proceedings of the VIII Convegno AI*IA</title>
		<meeting>the VIII Convegno AI*IA</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,516.00,222.20,8.64;10,76.00,527.78,222.20,8.82;10,76.00,539.73,222.20,8.59;10,76.00,551.69,105.56,8.82" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,241.98,516.00,56.23,8.64;10,76.00,527.96,126.53,8.64">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,215.14,527.78,83.07,8.59;10,76.00,539.73,222.20,8.59;10,76.00,551.69,76.77,8.59">Proceedings of ACM SIGIR Conference on Research and Development on Information Retrieval</title>
		<meeting>ACM SIGIR Conference on Research and Development on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,570.26,222.20,8.64;10,76.00,582.04,222.20,8.82;10,76.00,593.99,113.48,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,231.11,570.26,67.10,8.64;10,76.00,582.22,85.71,8.64">Scaling question answering to the Web</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,178.01,582.04,120.19,8.59;10,76.00,593.99,84.21,8.59">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,612.57,222.20,8.64;10,76.00,624.52,222.20,8.64;10,76.00,636.30,222.20,8.59;10,76.00,648.43,22.42,8.64" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,238.29,612.57,59.92,8.64;10,76.00,624.52,213.70,8.64">Patterns of potential answer expressions as clues to the right answer</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,76.00,636.30,217.83,8.59">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,76.00,666.83,222.20,8.64;10,76.00,678.78,222.20,8.64;10,76.00,690.56,222.20,8.82;10,76.00,702.51,222.20,8.59;10,76.00,714.47,173.53,8.82" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="10,254.71,678.78,43.50,8.64;10,76.00,690.74,189.52,8.64">A language independent method for question classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perez-Coutino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes Y Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villasenor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez-Lopez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 20th International Conference on Computational Linguistics (COLING-04</note>
</biblStruct>

<biblStruct coords="10,336.79,75.48,222.20,8.64;10,336.79,87.25,222.20,8.82;10,336.79,99.21,57.83,8.82" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,424.33,75.48,134.67,8.64;10,336.79,87.43,112.12,8.64">Learning question classifiers: the role of semantic information</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,461.12,87.25,97.88,8.59;10,336.79,99.21,28.81,8.59">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,118.53,222.21,8.64;10,336.79,130.49,222.20,8.64;10,336.79,142.26,222.20,8.82;10,336.79,154.22,222.20,8.59;10,336.79,166.17,222.20,8.59;10,336.79,178.13,65.04,8.82" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,342.06,130.49,216.94,8.64;10,336.79,142.44,184.91,8.64">An integrated knowledge-based and machine learning approach for Chinese question classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">L</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,541.18,142.26,17.81,8.59;10,336.79,154.22,222.20,8.59;10,336.79,166.17,222.20,8.59;10,336.79,178.13,35.11,8.59">Proceedings of IEEE International Conference on Natural Language Processing and Knowledge Engineering (NLPKE)</title>
		<meeting>IEEE International Conference on Natural Language Processing and Knowledge Engineering (NLPKE)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,197.45,222.21,8.64;10,336.79,209.41,222.20,8.64;10,336.79,221.18,222.20,8.82;10,336.79,233.14,56.35,8.82" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,508.78,197.45,50.22,8.64;10,336.79,209.41,222.20,8.64;10,336.79,221.36,18.67,8.64">Experiments at the University of Edinburgh for the TREC 2006 QA track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kaisser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,366.20,221.18,192.79,8.59;10,336.79,233.14,27.51,8.59">Proceedings of the Fifteenth Text REtrieval Conference</title>
		<meeting>the Fifteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,252.46,222.21,8.64;10,336.79,264.41,222.20,8.64;10,336.79,276.19,222.20,8.82;10,336.79,288.14,222.20,8.59;10,336.79,300.10,71.13,8.82" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,524.57,252.46,34.42,8.64;10,336.79,264.41,222.20,8.64;10,336.79,276.37,109.49,8.64">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,461.29,276.19,97.71,8.59;10,336.79,288.14,222.20,8.59;10,336.79,300.10,42.37,8.59">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,319.42,222.21,8.64;10,336.79,331.38,222.20,8.64;10,336.79,343.33,222.20,8.64;10,336.79,355.11,222.20,8.82;10,336.79,367.06,72.96,8.82" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,402.81,343.33,156.19,8.64;10,336.79,355.29,50.10,8.64">FALCON: Boosting knowledge for answer engines</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Buneascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,399.60,355.11,159.39,8.59;10,336.79,367.06,43.68,8.59">Proceedings of the Ninth Text REtrieval Conference</title>
		<meeting>the Ninth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,386.39,222.21,8.64;10,336.79,398.34,160.39,8.64" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="10,341.96,386.39,217.04,8.64;10,336.79,398.34,31.84,8.64">Lemur toolkit for language modeling and information retrieval</title>
		<ptr target="http://www.lemurproject.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,417.49,222.21,8.64;10,336.79,429.26,222.20,8.82;10,336.79,441.22,145.30,8.82" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,412.84,417.49,146.16,8.64;10,336.79,429.44,87.90,8.64">The boosting approach to machine learning: An overview</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,437.92,429.26,121.07,8.59;10,336.79,441.22,116.58,8.59">MSRI Workshop on Nonlinear Estimation and Classification</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,460.54,222.21,8.64;10,336.79,472.31,222.20,8.59;10,336.79,484.45,22.42,8.64" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,420.06,460.54,130.95,8.64">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.79,472.31,217.83,8.59">Proceedings of the Second Text REtrieval Conference</title>
		<meeting>the Second Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,503.59,222.21,8.64;10,336.79,515.55,222.20,8.64;10,336.79,527.32,222.20,8.82;10,336.79,539.28,221.74,8.82" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,485.30,515.55,73.70,8.64;10,336.79,527.50,170.45,8.64">The University of Sheffield&apos;s TREC 2005 Q&amp;A experiments</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Harkema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,522.93,527.32,36.07,8.59;10,336.79,539.28,192.46,8.59">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,558.60,222.20,8.64;10,336.79,570.56,222.20,8.64;10,336.79,582.33,222.20,8.82;10,336.79,594.29,222.20,8.59;10,336.79,606.24,113.36,8.82" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="10,342.20,570.56,216.79,8.64;10,336.79,582.51,11.37,8.64">A robust combination strategy for semantic role labeling</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,363.62,582.33,195.36,8.59;10,336.79,594.29,222.20,8.59;10,336.79,606.24,84.25,8.59">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,625.57,222.21,8.64;10,336.79,637.34,222.20,8.82;10,336.79,649.30,222.20,8.59;10,336.79,661.25,112.25,8.82" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="10,494.22,625.57,64.77,8.64;10,336.79,637.52,173.05,8.64">The necessity of syntactic parsing for semantic role labeling</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,522.93,637.34,36.07,8.59;10,336.79,649.30,222.20,8.59;10,336.79,661.25,83.54,8.59">Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Nineteenth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.79,680.57,222.21,8.64;10,336.79,692.53,222.20,8.64;10,336.79,704.48,218.64,8.64" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="10,464.89,692.53,94.10,8.64;10,336.79,704.48,75.02,8.64">FrameNet II: Extended theory and practice</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Scheffczyk</surname></persName>
		</author>
		<ptr target="http://framenet.icsi.berkeley.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
