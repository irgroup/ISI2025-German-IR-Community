<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.58,102.09,364.85,15.12">The Hedge Algorithm for Metasearch at TREC 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.37,134.57,81.19,10.48;1,258.56,132.95,1.41,6.99"><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.21,134.57,61.94,10.48"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.81,134.57,86.83,10.48"><forename type="first">Olena</forename><surname>Zubaryeva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.58,102.09,364.85,15.12">The Hedge Algorithm for Metasearch at TREC 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6E0CDE5050DA5781D9C0BF7E6C2ECDC2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> Aslam, Pavlu, and Savell [3]  <p>introduced the Hedge algorithm for metasearch which effectively combines the ranked lists of documents returned by multiple retrieval systems in response to a given query. It has been demonstrated that the Hedge algorithm is an effective technique for metasearch, often significantly exceeding the performance of standard metasearch and IR techniques over small TREC collections. In this work, we explore the effectiveness of Hedge as an automatic metasearch engine over the much larger GOV2 collection on about 1700 topics as part of the Million Query Track of TREC 2007. * We gratefully acknowledge the support provided by NSF grants CCF-0418390 and IIS-0534482.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aslam, Pavlu, and Savell introduced a unified framework for simultaneously solving the problems of metasearch, pooling, and system evaluation based on the Hedge algorithm for on-line learning <ref type="bibr" coords="1,255.47,462.45,9.97,8.74">[3]</ref>. Given the ranked lists of documents returned by a collection of IR systems in response to a given query, Hedge is capable of matching and often exceeding the performance of the best underlying retrieval system; given relevance feedback, Hedge is capable of "learning" how to optimally combine the input systems, yielding a level of performance which often significantly exceeds that of the best underlying system.</p><p>In previous experiments with smaller TREC collections <ref type="bibr" coords="1,96.24,582.52,9.96,8.74">[3]</ref>, it has been shown that after only a handful of judged feedback documents, Hedge is able to significantly outperform the CombMNZ and Condorcet metasearch techniques. It has also been shown that Hedge is able to efficiently construct pools containing significant numbers of relevant documents and that these pools are highly effective at evaluating the underlying systems <ref type="bibr" coords="1,416.42,213.84,9.97,8.74">[3]</ref>. Although the Hedge algorithm has been shown to be a strong technique for metasearch, pooling, and system evaluation using the relatively small or moderate TREC collections <ref type="bibr" coords="1,310.98,261.66,47.44,8.74">(TRECs 3,</ref><ref type="bibr" coords="1,361.82,261.66,7.75,8.74">5,</ref><ref type="bibr" coords="1,372.98,261.66,7.75,8.74">6,</ref><ref type="bibr" coords="1,384.13,261.66,7.75,8.74">7,</ref><ref type="bibr" coords="1,395.27,261.66,7.75,8.74">8)</ref>, it has yet to be demonstrated that the technique is scalable to corpora whose data size is at the terabyte level. In this work, we assess the performance of Hedge on a terabyte scale, presenting testing results using the Million Query Track topics and data.</p><p>Finally, we note that in the context of this TREC, in the absence of feedback, Hedge is a fully automatic metasearch algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Results</head><p>In the Million Query Track, Hedge was run with no feedback (ergo the name of the submission "hedge0") as an automatic metasearch engine. We indexed the collection using the Lemur Toolkit; that process took about 3 days using a 2-processor dual-core Opteron machine (2.4 GHz/core).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Underlying IR systems</head><p>The underlying systems include: (1) two tf-idf retrieval systems; (2) three KL-divergence retrieval models, one with Dirichlet prior smoothing, one with Jelinek-Mercer smoothing, and the last with absolute discounting; (3) a cosine similarity model; (4) the OKAPI retrieval model; (5) and the INQUERY retrieval method. All of the above retrieval models are provided as standard IR systems by the Lemur Toolkit <ref type="bibr" coords="1,345.88,623.53,9.96,8.74">[1]</ref>. For each query and retrieval system, we considered the top 1,000 scored documents for that retrieval system. Once all retrieval systems were run against all queries, we ran the Hedge algorithm <ref type="bibr" coords="1,529.48,659.40,10.52,8.74">[3]</ref> to perform metasearch on the ranked lists obtained. These models were run on 10,000 topics using the GOV2 collection.</p><p>For reference, Table <ref type="table" coords="2,171.77,495.09,4.98,8.74">1</ref> illustrates that both hedge0 and CombMNZ are able to exceed the performance of the best underlying system (Terabyte05 data). This demonstrates that Hedge alone, even without any relevance feedback, is a successful metasearch technique, exceeding the metasearch performance of Condorcet and rivaling the performance of CombMNZ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results for Million Query 07</head><p>Tables <ref type="table" coords="2,103.92,623.53,4.98,8.74">3</ref> &amp;<ref type="table" coords="2,124.47,623.53,4.98,8.74" target="#tab_1">4</ref> and Figure <ref type="figure" coords="2,185.36,623.53,4.98,8.74">1</ref> present the performance of hedge0 on the 2007 Million Query Track collection and topics, separately for the MQ evaluation methods and for the 149 Terabyte topics using traditional evaluation methods and metrics. This performance was in line with expectations and previous results.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,366.61,103.74,23.39,8.74"><head>MAP</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,310.98,103.74,229.02,558.54"><head>Table 4 :</head><label>4</label><figDesc>Performance over the 149 Terabyte06 topics, where the evaluation was performed using traditional methods and metrics.</figDesc><table coords="2,310.98,103.74,229.02,502.80"><row><cell></cell><cell cols="2">0.1708</cell></row><row><cell>R-prec</cell><cell cols="2">0.2411</cell></row><row><cell>bpref</cell><cell cols="2">0.2414</cell></row><row><cell>recip-rank</cell><cell cols="2">0.6039</cell></row><row><cell>retrieved</cell><cell cols="2">135075</cell></row><row><cell>relevant</cell><cell></cell><cell>26917</cell></row><row><cell cols="2">relevant retrieved</cell><cell>13944</cell></row><row><cell cols="3">Table 3: Performance over the 149 Terabyte06 topics,</cell></row><row><cell cols="3">where the evaluation was performed using traditional</cell></row><row><cell>methods and metrics.</cell><cell></cell></row><row><cell cols="3">Precision at Recall (149 Terabyte06 topics)</cell></row><row><cell>recall</cell><cell cols="2">precision</cell></row><row><cell>.00</cell><cell>0.6611</cell></row><row><cell>.10</cell><cell>0.3748</cell></row><row><cell>.20</cell><cell>0.3096</cell></row><row><cell>.30</cell><cell>0.2499</cell></row><row><cell>.40</cell><cell>0.2015</cell></row><row><cell>.50</cell><cell>0.1558</cell></row><row><cell>.60</cell><cell>0.1019</cell></row><row><cell>.70</cell><cell>0.0592</cell></row><row><cell>.80</cell><cell>0.0351</cell></row><row><cell>.90</cell><cell>0.0086</cell></row><row><cell>1.00</cell><cell>0.0035</cell></row><row><cell cols="3">Precision at Rank (149 Terabyte06 topics)</cell></row><row><cell>rank</cell><cell cols="2">precision</cell></row><row><cell>at 5 docs</cell><cell cols="2">0.4174</cell></row><row><cell>at 10 docs</cell><cell cols="2">0.3826</cell></row><row><cell>at 15 docs</cell><cell cols="2">0.3575</cell></row><row><cell>at 20 docs</cell><cell cols="2">0.3453</cell></row><row><cell>at 30 docs</cell><cell cols="2">0.3327</cell></row><row><cell>at 100 docs</cell><cell cols="2">0.2644</cell></row><row><cell>at 200 docs</cell><cell cols="2">0.2209</cell></row><row><cell>at 500 docs</cell><cell cols="2">0.1473</cell></row><row><cell>at 1000 docs</cell><cell cols="2">0.0936</cell></row><row><cell>R-precision</cell><cell cols="2">0.2411</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>[1] The lemur toolkit for language modeling and information retrieval. http://www.cs.cmu.edu/~lemur.</p><p>[2] James Allan, Ben carterette, Javed A. </p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
