<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,202.31,83.76,205.10,15.48;1,139.64,105.68,330.42,15.48">Access to Legal Documents: Exact Match, Best Match, and Combinations</title>
				<funder ref="#_fWUkfQg #_aUnUfX3 #_WJNdhF3">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
				<funder ref="#_vqvj2f5">
					<orgName type="full">E.U</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,99.49,138.20,79.49,10.75"><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.38,138.20,63.92,10.75"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.28,138.20,75.75,10.75"><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.43,138.20,74.06,10.75"><forename type="first">Nir</forename><surname>Nussbaum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,202.31,83.76,205.10,15.48;1,139.64,105.68,330.42,15.48">Access to Legal Documents: Exact Match, Best Match, and Combinations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D01B48D94B89A12D23334788F6D537FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we document our efforts in participating to the TREC 2007 Legal track. We had multiple aims: First, to experiment with using different query formulations, trying to exploit the verbose topic statements. Second, to analyse how ranked retrieval methods can be fruitfully combined with traditional Boolean queries. Our main findings can be summarized as follows: First, we got mixed results trying to combine the original search request with terms extracted from the verbose topic statement. Second, by combining the Boolean reference run wit our ranked retrieval run allows us to get the high recall of the Boolean retrieval, whilst precision scores show an improvement over both the Boolean and the ranked retrieval runs. Third, we found out that if we treat the Boolean query as free text with varying degrees of interpretation of the original operator, we get competitive results. Moreover, both types of queries seem to capture different relevant documents, and the combination between the request text and the Boolean query leads to substantial gain in precision and recall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our first participation in the Legal track, we experimented with different query formulations and run combinations. Since the focus in the Legal track ad hoc evaluation is on recall oriented measures, we investigated methods to increase recall by combining result lists based on different query representations. We also analyzed differences between our ranked retrieval runs and the Boolean reference run, and investigated ways to fruitfully combine the strengths of both approaches. Finally, we also checked different methods of exploiting the Boolean topic statements and combinations thereof with the request text.</p><p>The rest of this paper is organized as follows. In Section 2, we describe the experimental set-up. In Section 4, we discuss our official submission, results, and additional experiments. Finally, we summarize our findings in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Set-up</head><p>Our retrieval system is based on the LUCENE engine version 1.9 <ref type="bibr" coords="1,331.76,253.40,10.58,8.64" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Index</head><p>the Legal track uses a test collection, containing 6,910,192 documents (∼58Gb uncompressed). The documents are from the legal domain, on issues of the tobacco industry. The documents are all in XML format, containing limited meta-data.</p><p>We created two separate indexes as follows.</p><p>Full-text: the full textual content of the documents, including the meta-data tags, as is (∼37GB).</p><p>Text-only: the text inside the tags, not including the tags (∼33GB).</p><p>During indexing, we captured the document ID and stored it in the index. In tokenization, we removed the common stop-words and stemmed using the Snowball stemming algorithm <ref type="bibr" coords="1,350.86,482.93,10.58,8.64" target="#b4">[5]</ref>.</p><p>The original corpus contains almost 7 million documents in 650 files and is over 58Gb uncompressed data. The file-name convention is iitcdip.x.y.xml, where x is a letter a-z (excluding s) and y is a letter a-z. As a first step we decided to create 25 partial indexes for each of the two indexing choices mentioned above, for example: index a indexed the files iitcdip.a.a.xml to iitcdip.a.z.xml. We chose to index in chunks because a single indexing process would have taken very long time. Indexing in chunks enabled us to use seven computers concurrently, each indexing different chunks. Since accessing multiple indexes is substantially slower, we eventually merged the 25 indexes into one index, an action that LUCENE is handling straightforwardly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval Model</head><p>For ranking, we used a vector-space retrieval model. Our vector space model is the default similarity measure in LUCENE, i.e., for a collection D, document d, and query q:</p><formula xml:id="formula_0" coords="2,61.66,89.83,223.39,40.74">sim(q, d) = = t∈q tf t,q • idf t norm q • tf t,d • idf t norm d • coord q,d • weight t ,</formula><p>where tf t,X = freq(t, X) ,</p><formula xml:id="formula_1" coords="2,120.61,178.75,116.61,22.31">idf t = 1 + log |D| freq(t, D)</formula><p>,</p><formula xml:id="formula_2" coords="2,101.09,211.98,137.70,70.20">norm q = t∈q tf t,q • idf t 2 , norm d = |d| , coord q,d = |q ∩ d| |q| .</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Runs</head><p>This was our first participation in the TREC Legal Track. Some of the runs described in this paper are post-submission experiments and have not been included in the pooling process. We made runs using the search request as stated in the RequestedText tag:</p><p>Full-text: runs on the full-text index. In this run, we used the Snowball stemming algorithm on the RequestedText meta-data in the query.</p><p>Text-only: similar to full-text, but using the text-only</p><p>The Legal Track topics have very lengthy topic descriptions providing a range of background information on the topic of request. Hence, we tried to extract potentially useful terms from them. Specifically, we decided to select only those terms that are most characteristic for a single topic, with reference to the whole topic set. That is, the terms that best distinguish the topic at hand from the other topics in the topic set. For this we used a variant of the parsimonious language modeling techniques <ref type="bibr" coords="2,141.25,613.29,10.58,8.64" target="#b2">[3]</ref>, and created a query by selecting the 25 terms that are most characteristic for the topic. For example, topic 52 reads:</p><p>Please produce any and all documents that discuss the use or introduction of high-phosphate fertilizers (HPF) for the specific purpose of boosting crop yield in commercial agriculture.</p><p>and the 25 selected terms are:</p><p>hpf sugar mh vsf gcc valhalla candy phosphate plaintiffs its fertilizers crop high gladsheim beet yield community groundwater health death use fertiliz contamination phosphat cause</p><p>We used the selected terms in the following ways:</p><p>SelectedTerms: the query string fed into LUCENE is the original query (the RequestedText tag) appended by the most significant 25 terms in the background information supplied in the file f ullL07 v1.xml. Duplicate terms were removed.</p><p>CombiTextSelectedTerms: a combination of Text-only and SelectedTerms. We used the standard combination method CombSUM <ref type="bibr" coords="2,438.28,225.73,11.61,8.64" target="#b1">[2]</ref> and combined full length runs without normalizing the scores.</p><p>One of the main differences between our ranked-retrieval approach and the Boolean reference run is the query: we used the RequestedText field whereas for the reference run the FinalQuery was used. To study the effect of the different topic statements, we performed additional runs, trying to exploit the terms and operators stated in the topic's Boolean query fields. Specifically, the tags FinalQuery, Proposal-ByDefendant and RejoinderByPlaintiff, the former one representing the query agreed by the sides and the two latter ones representing the negotiation history between the defendant and the plaintiff. Our experiments showed that using FinalQuery gives better results than ProposalByDefendant and RejoinderByPlaintiff, and we will only discuss experiments using FinalQuery in this paper.</p><p>The Boolean queries, including FinalQuery that we use, consist of terms and operators which represent a query syntax, explained in <ref type="bibr" coords="2,388.02,448.54,10.58,8.64" target="#b0">[1]</ref>. Since this syntax does not correspond to Lucene's query syntax and is actually more expressive than Lucene's syntax, we did some translation of the queries to a Lucene-readable syntax. Unfortunately, Lucene's ability to deal with multiple wildcards is very limited in such a big index (using too many wildcards would lead to a crash with a 'too many clauses' error message) and we had to translate them as well.</p><p>The runs are as following:</p><p>BoolTermsOnly: The terms alone are used; all the other characters, such as parentheses or proximity operators are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BoolLuceneTrans:</head><p>The original syntax is translated into Lucene's query language. Proximity operator (w/k) is translated into AND, wildcards symbol (!) are removed and BUT NOT is replaced by AND NOT.</p><p>BoolTermsWildcardExp: For this run we used the terms alone and removed all the other characters except the wildcard symbol (!). Wildcards are expanded to all the possible variations that appear in the complete topic text, with the OR operator between them. In this BoolWildcardExpLuceneTrans: Here we expanded the wildcards in the original Boolean query as we did in the BoolTermsWildcardExp run and then translated the result as we did in the BoolLuceneTrans run.</p><p>We were especially interested in combinations of both types: the request text and the Boolean query text, in various interpretations. We combined the TextOnly run with each of the four Boolean runs, using the CombSUM method, creating CombiTextTerms, CombiTextLuceneTrans, Com-biTextTermsWildcard and CombiTextWildcardLucene-Trans runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topics and Judgments</head><p>The results are based on the qrels over 43 topics. Statistics of the assessments are shown in Table <ref type="table" coords="3,216.41,445.43,3.74,8.64" target="#tab_0">1</ref>. We include the number of results of the negotiated Boolean query ("B" for short), since it plays an important role in the recall oriented measures used. It is striking that B is orders of magnitude larger than the number of known relevant documents. Moreover, there is no significant correlation between B and the number of relevant documents (Pearson r = 0.059). There is a significant correlation (0.55) between the number of judged and number of found relevant documents, which is not unexpected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Runs</head><p>Table <ref type="table" coords="3,79.37,602.87,4.98,8.64" target="#tab_1">2</ref> shows the results (all scores based on l07 eval v1.0). First, we look at Full-text, and compare it to the similar Text-only run. The Full-text run scores marginally better on bpref, but the Text-Only run scores marginally better on all other measures including the main measure, i.e. estimated recall at B. Second, we look at the SelectedTerms run. We see a drop in performance for all measures. This is not unexpected: the selected terms from the complete topic statement are less focused on the topic. Nevertheless, the run may have picked up documents that are missed by the original query or improve the ranking of retrieved relevant documents. Can we use these results to improve recall in our original run? We combine the results from the two runs Text-only and Se-lectedTerms using standard CombSUM, i.e. we just add the scores of the individual runs per document and re-rank. The results are mixed. For the CombiTextSelectedTerms run, we see a drop in MAP and Precision@10 when compared to the best individual run, but an increase in bpref. We see a minimal gain in recallB, but a loss of estimated recall at B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combining Ranked &amp; Boolean Retrieval</head><p>We conducted further experiments trying to shed light on the relative strength and weaknesses of our ranked retrieval methods versus the Boolean reference run.</p><p>First, we looked at the reference Boolean run refL07B which has unranked set results (all selected documents have a RSV of 1). As Table <ref type="table" coords="3,412.90,276.19,4.98,8.64" target="#tab_1">2</ref> shows, this results indeed in very poor MAP and P10 scores. The bpref score is comparable to the scores of ranked retrieval (this is also clearly signalling that bpref should not be treated as an approximation of traditional MAP). In terms of recall, the reference run is retrieving fewer relevant documents overall (but has only B results per topic whereas our runs have up to 25,000). It has slightly better recall at B, and much better estimated recall at B. Summarizing, the reference run has unimpressive precision but very good recall.</p><p>Is there a way to combine the strength of ranked and Boolean approaches? What we did is the following. Recall that refL07B run assigns a score of 1 to every document. Our runs score in the range [0, 1]. Now, consider what happens if we combine the reference run with one of our runs: it will first have all documents of the reference run, but ranked by our retrieval score, and then have the remaining documents from our run, again ranked by our retrieval score. The run CombiTextRef in Table <ref type="table" coords="3,434.58,491.91,4.98,8.64" target="#tab_1">2</ref> combines the Text-only run with the refL07B run. What we see is that, indeed, the recall at B and estimated recall at B of the Boolean run are preserved. However, the MAP, bpref, and P10 scores even improve over the scores from the ranked retrieval run alone. Summarizing, combining Boolean and ranked retrieval allows us to get the best of both worlds: the high recall scores of the Boolean run are maintained, while the MAP and precision scores show an improvement over both the Boolean and the ranked retrieval run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Boolean Queries Runs</head><p>We now look at the relative strength and weaknesses of the different query statements, by making various runs based on the Boolean FinalQuery field.</p><p>We first look at the four variants to derive a query from the FinalQuery field (as discussed above). As Table <ref type="table" coords="3,550.93,698.87,4.98,8.64" target="#tab_1">2</ref> shows, extracting just the keywords from the Boolean query, We added the wildcard expansion in order to compensate somewhat for Lucene's inability to cope with a too long query expression. In this way we could look into the variations of the terms as they appear in the entire topic statements. The whole index contains too many variations for Lucene's built-in equivalent, especially because of the often poor graphic quality of the original scanned documents, which results in many mistakes during the OCR process, creating an astounding number of unique (and often wrong) terms. A Boolean query that has a few wildcards could be expanded internally into thousands of clauses or even more. The runs that incorporate the wildcard expansion (BoolTermsWildcardExp and BoolWildcardExpLucene-Trans) did perform better in general. The general conclusion of the runs based on the Boolean query is that their performance meets and exceeds that of the keyword query based on the RequestText field.</p><p>Finally, we look at how complementary the runs based on the different topic fields are, by looking at their combination. As Table <ref type="table" coords="4,94.75,615.18,4.98,8.64" target="#tab_1">2</ref> shows, this leads to improvement throughout. The stricter interpretation of the FinalQuery leads to better results (since it is more different from the original Request-Text run) and we get close to the reference run's score on estimated recall at B. The best scoring run is CombiTextWild-cardLuceneTrans with a score of 0.1976 against 0.2158 for the reference run. Our general conclusion is that Request-Text and FinalQuery complete each other nicely: combining them leads to substantial gains in precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our first participation in the Legal Track was driven by two main aims. First, we experimented with using different query formulations trying to exploit the verbose topic statements. Second, we analysed how ranked retrieval methods can be fruitfully combined with traditional Boolean queries. Our main findings can be summarized as follows: First, we got mixed results trying to combine the original search request with terms extracted from the verbose topic statement. Second, by combining the Boolean reference run with our ranked retrieval run allows us to get the high recall of the Boolean retrieval, whilst precision scores show an improvement over both the Boolean and the ranked retrieval runs. Third, we found that if we treat the Boolean query as free text with varying degrees of interpretation of the original operator, we get competitive results. Moreover, both types of queries seem to capture different relevant documents, and the combination between the request text and the Boolean query leads to substantial gain in precision and recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,53.80,64.07,239.10,147.51"><head>Table 1 :</head><label>1</label><figDesc>Statistics over judged and relevant documents per topic.</figDesc><table coords="3,59.78,98.90,233.13,112.67"><row><cell></cell><cell># of</cell><cell></cell><cell cols="2">per topic</cell></row><row><cell></cell><cell cols="3">topics min max median</cell><cell>mean st.dev</cell></row><row><cell>judged</cell><cell cols="2">43 488 1,000</cell><cell cols="2">499 567.53 164.84</cell></row><row><cell>relevant</cell><cell>43</cell><cell>10 391</cell><cell cols="2">72 101.02 97.76</cell></row><row><cell>B</cell><cell cols="4">43 103 22,518 2,665 5,004.02 6,156.75</cell></row><row><cell cols="5">way, the keyword produc! in topic 52 was replaced</cell></row><row><cell cols="5">by produc OR production OR product OR</cell></row><row><cell cols="2">products.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,64.07,445.77,280.98"><head>Table 2 :</head><label>2</label><figDesc>Results for the Legal Track 2007 (using l07 eval v1.0). Best scores are in bold-face.BoolTermsOnly performed slightly worse than the Text-Only run. Adding a basic translation of the operators into Lucene syntax as in BoolLuceneTrans gives mixed results in terms of performance: we see a small in estimated Recall at B, but bpref and known recall at B drop.</figDesc><table coords="4,110.15,86.95,389.42,178.00"><row><cell>Run</cell><cell>MAP</cell><cell>bpref</cell><cell>P10</cell><cell cols="2">num rel ret recallB est RB</cell></row><row><cell>Full-text</cell><cell cols="3">0.0878 0.3266 0.2837</cell><cell>3,338</cell><cell>0.4792 0.1448</cell></row><row><cell>Text-only</cell><cell cols="3">0.0880 0.3255 0.2860</cell><cell>3,339</cell><cell>0.4835 0.1548</cell></row><row><cell>SelectedTerms</cell><cell cols="3">0.0355 0.2619 0.1070</cell><cell>2,522</cell><cell>0.3173 0.0772</cell></row><row><cell>CombiTextSelectedTerms</cell><cell cols="3">0.0846 0.3302 0.2698</cell><cell>3,306</cell><cell>0.4841 0.1447</cell></row><row><cell>refL07B</cell><cell cols="3">0.0167 0.2902 0.0209</cell><cell>2,145</cell><cell>0.4864 0.2158</cell></row><row><cell>CombiTextRef</cell><cell cols="3">0.1181 0.3842 0.3209</cell><cell>3,553</cell><cell>0.4864 0.2158</cell></row><row><cell>BoolTermsOnly</cell><cell cols="3">0.0878 0.3274 0.2535</cell><cell>3,016</cell><cell>0.4846 0.1417</cell></row><row><cell>BoolLuceneTrans</cell><cell cols="3">0.0880 0.3039 0.2535</cell><cell>2,200</cell><cell>0.4172 0.1526</cell></row><row><cell>BoolTermsWildcardExp</cell><cell cols="3">0.1021 0.3321 0.3140</cell><cell>3,352</cell><cell>0.5122 0.1335</cell></row><row><cell>BoolWildcardExpLuceneTrans</cell><cell cols="3">0.0915 0.3250 0.2488</cell><cell>2,758</cell><cell>0.4369 0.1555</cell></row><row><cell>CombiTextTerms</cell><cell cols="3">0.1220 0.3615 0.3326</cell><cell>3,473</cell><cell>0.5673 0.1843</cell></row><row><cell>CombiTextLuceneTrans</cell><cell cols="3">0.1191 0.3683 0.3047</cell><cell>3,426</cell><cell>0.5520 0.1908</cell></row><row><cell>CombiTextTermsWildcard</cell><cell cols="3">0.1264 0.3592 0.3465</cell><cell>3,490</cell><cell>0.5627 0.1644</cell></row><row><cell cols="4">CombiTextWildcardLuceneTrans 0.1191 0.3665 0.3256</cell><cell>3,431</cell><cell>0.5479 0.1976</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grant # <rs type="grantNumber">612.066.513</rs>, <rs type="grantNumber">639.072.601</rs>, and <rs type="grantNumber">640.001.501</rs>), and by the <rs type="funder">E.U</rs>.'s 6th <rs type="programName">FP for RTD</rs> (project <rs type="projectName">MultiMATCH</rs> contract <rs type="grantNumber">IST-033104</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fWUkfQg">
					<idno type="grant-number">612.066.513</idno>
				</org>
				<org type="funding" xml:id="_aUnUfX3">
					<idno type="grant-number">639.072.601</idno>
				</org>
				<org type="funding" xml:id="_WJNdhF3">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funded-project" xml:id="_vqvj2f5">
					<idno type="grant-number">IST-033104</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
					<orgName type="program" subtype="full">FP for RTD</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,333.41,643.74,222.51,8.64;4,333.41,656.63,167.37,7.01;4,333.41,668.59,95.74,7.01" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,333.41,643.74,79.97,8.64">Trec legal README</title>
		<ptr target="http://trec-legal.umiacs.umd.edu/trec07topics/readmeL07v1.txt" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.41,686.91,222.50,8.64;4,333.41,698.70,222.51,8.81;4,333.41,710.65,222.51,8.81;5,70.40,57.28,222.50,8.64;5,70.40,69.23,60.60,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,417.73,686.91,134.16,8.64">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,424.96,698.70,130.96,8.58;4,333.41,710.65,68.27,8.58">The Second Text REtrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="215" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,89.16,222.50,8.64;5,70.40,100.94,222.51,8.81;5,70.40,112.90,222.50,8.58;5,70.40,124.85,222.50,8.58;5,70.40,136.81,222.51,8.81;5,70.40,148.93,40.50,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,257.07,89.16,35.83,8.64;5,70.40,101.11,187.70,8.64">Parsimonious language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,275.09,100.94,17.81,8.58;5,70.40,112.90,222.50,8.58;5,70.40,124.85,222.50,8.58;5,70.40,136.81,53.89,8.58">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,168.86,222.51,8.64;5,70.40,181.75,110.09,7.01" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,108.79,168.86,105.44,8.64">The Lucene search engine</title>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,200.74,222.50,8.64;5,70.40,212.69,222.51,8.64;5,70.40,225.58,80.20,7.01" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,124.73,200.74,168.17,8.64;5,70.40,212.69,63.50,8.64">Stemming algorithms for use in information retrieval</title>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org/" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
