<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,121.80,99.04,368.27,12.91">Question Answering with LCC&apos;s CHAUCER-2 at TREC 2007</title>
				<funder>
					<orgName type="full">U.S. Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,42.00,131.27,69.66,10.76"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,119.69,131.27,65.54,10.76"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.78,131.27,57.31,10.76"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.72,131.27,76.68,10.76"><forename type="first">Jeremy</forename><surname>Bensley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.59,131.27,70.66,10.76"><forename type="first">Tobias</forename><surname>Jungen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,423.95,131.27,41.76,10.76"><forename type="first">Ying</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,495.91,131.27,73.87,10.76"><forename type="first">John</forename><surname>Williams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation</orgName>
								<address>
									<addrLine>1701 North Collins Boulevard Richardson</addrLine>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,121.80,99.04,368.27,12.91">Question Answering with LCC&apos;s CHAUCER-2 at TREC 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ED5E8B6F5BD6F2E8D7ACD7DE077D0925</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2007, Language Computer Corporation explored how a new, semantically-rich framework for information retrieval could be used to boost the overall performance of the answer extraction and answer selection components featured in its CHAUCER-2 automatic question-answering (Q/A) system. By replacing the traditional keyword-based retrieval system used in (?) with a new indexing and retrieval engine capable of retrieving documents or passages based on the distribution of named entities or semantic dependencies, we were able to dramatically enhance CHAUCER-2's overall accuracy, while significantly reducing the number of of candidate answers that were considered by its Answer Ranking and Answer Validation modules.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In TREC 2007, Language Computer Corporation explored how a new, semantically-rich framework for information retrieval could be used to boost the overall performance of the answer extraction and answer selection components featured in its CHAUCER-2 automatic question-answering (Q/A) system.</p><p>Unlike the keyword-based retrieval systems traditionally used by Q/A systems, CHAUCER-2 leverages a novel indexing and retrieval engine which makes it possible to retrieve documents or passages using queries that look for instances of (1) entity types recognized by a named entity recognition (NER) system, (2) semantic dependencies identified by PropBank-or NomBank-based semantic parsers, (3) semantic frames (or frame-denoting elements) recognized by a FrameNet parser, or even (4) the normalized versions of temporal or spatial expressions. Support for these new types of queries dramatically enhanced the performance of CHAUCER-2's Document and Passage Retrieval components while significantly reducing the number of candidate answers that had to be considered by its Answer Ranking and Answer Validation modules.</p><p>CHAUCER-2 also leverages a variant of the Bindings Engine (BE) first proposed by (?; ?) in order to retrieve of all of the text snippets matched by a pattern-based (or variabilized) query without having to retrieve documents using a keyword-based query. We found that use of this framework greatly both enhanced the efficiency and the recall of tradi-tional pattern-based approaches to Q/A and allowed for the development of new libraries of precise patterns for specific question types asked in previous TREC QA evaluations.</p><p>Finally, CHAUCER-2 incorporates a new, multi-tiered Answer Type Detection (ATD) module which reduces the number of expected answer types (EATs) considered by the system for factoid or list questions, while maintaining the same high levels of precision exhibited by previous systems. Since LCC's previous ATD systems often identified a large number of spurious answer types along with the most correct answer type for a question, we developed a new backoff mechanism which forces the Q/A system to consider the most specific EATs identified for a question first; other, more general EATs were then included as search terms only when insufficient evidence was retrieved using the more specific EAT.</p><p>Taken together, we believe these three enhancements to CHAUCER-2's core retrieval capabilities allowed us to develop a battery of high-precision, low-recall Answer Retrieval strategies which could be run independently of the traditional entity-based Q/A strategies currently being used by LCC's FERRET (?) and CHAUCER-1 (?) questionanswering systems. In order to maximize the value of these individual strategies, we re-cast the new CHAUCER-2 Q/A pipeline developed for the TREC 2007 evaluations as a cascade of end-to-end Q/A systems which were tasked with answering questions in order of their expected precision for a particular question type.</p><p>The rest of this paper is organized in the following way. Section 2 presents a brief overview of the architecture of the CHAUCER-2 system. Section 3 presents details of CHAUCER-2's core factoid Q/As system, while Section 4 describes the system for answering list questions, and Section 5 describes the techniques used to answer "other" questions. Results from this year's official evaluation are discussed in Section 6, while Section 7 summarizes our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The CHAUCER-2 Question-Answering System</head><p>This section describes the architecture of the CHAUCER-2 question-answering system used to answer factoid and list questions for the TREC 2007 QA Track Main Task. The </p><formula xml:id="formula_0" coords="2,62.20,50.85,466.10,216.30">H PI H RQ S T 5 ¨¥ ¨ E § % &amp; 3 &amp; ¨&amp; ¨% § ¡ ) § % &amp; 3 ¢B 3 ¢B 3 ¢B 3 ¢B 9 ¥ )U V¡ ¢&amp; § " W ¡ ¢£ E &amp; ¨¤ T 5 ¨¥ ¨£ $ X@ 1% 2£ $U P5 ¨ ¡ ) § % &amp; 9 § £ $¡ ) § ¥ ¢¤ ¢ ¥ ) Y ¥ ¨ ¤ ¦8 4 % 2£ $8 ¨% % A `&amp; ¨A ¨¥ ¨0 B G% 2" #5 ¨U V¥ )&amp; 2 § 7 ¥ ) § £ 6 ¥ )( ¦¡ 1 W ¨¡ ¢ ¡ ¢¤ 1¥ 7 ¥ ) § £ 6 ¥ )( ¡ ¢ 3 &amp; ¨ a b¥ )£ c' ¢0 ¦ § £ 6¡ ¢" § % &amp; 3 &amp; ) Ea b¥ ¢£ c7 ¡ ¢&amp; ¨d ¦ &amp; ¨¤ e ¥ )4 W ¡ ¢¤ ¢¥ 7 2¥ ¨ § £ $ ¥ ¨( ¡ ¢ W ¡ ¢ f¡ ¢¤ 1¥ 7 2¥ ) § £ 6 ¥ )( ¦¡ ¢ 9 § $£ 5 ¨" ¦ § 5 2£ ¥ )A B D¡ 1 § ¡ 3 &amp; ¨ a g¥ ¢£ 9 ¥ ) ¥ ¨" # § $ % &amp; C¡ ¢&amp; A CF ©¡ ) A )¡ ¨ § % &amp; h Q S h pi h Q S h pi T Xq D3 cr Rs ¥ )&amp; ¨¥ ¨£ ¡ ) § % &amp; `&amp; A ¨¥ )0 `&amp; ¨A ¨¥ )0 t ( ¡ bu Dv ¨v w x y ¢ W £ % 2 r ¡ ¢&amp; ¨d Y % 2U r ¡ ¢&amp; )d @ ¦£ $¡ )U V¥ Y ¥ § ¢¥ ¨0 # § 5 ¨¡ ) % &amp; ¨ § $£ $¡ ¢A " § $ % 2&amp; ¢¥ )0 # § 5 ¡ ¢ ' ¢&amp; ¨ § ¡ ) U R¥ &amp; ¨ § b ) E y ¢ ¢ ¦ ¨y g Gy ¢ y ¢ E v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Processing</head><p>Targets in CHAUCER-2 are initially submitted to a Target Type Classification module which uses a version of the Maximum Entropy-based classifier introduced in (?) in order to categorize series targets into one of a set of semantic categories taken from the large ontology of semantic types recognized by LCC's CICEROLITE named entity recognition system. As with our TREC 2006 work, targets were classified into one of six target types, including: (1) PEO-PLE (e.g. Warren Moon), (2) ORGANIZATIONS (American Enterprise Institute), (3) LOCATIONS (Amazon River), (4) EVENTS (1991 eruption of Mount Pinatubo), (5) AU-THORED WORK (The Daily Show), or (6) GENERIC NOUNS (avocados).</p><p>Classified targets were then sent to a Discovery of Essential Information module which leveraged sets of event, attribute, and relationship extractors created prior to the TREC 2007 evaluations for each individual target type category using LCC's CICEROCUSTOM open-domain, customizable information extraction system. In addition to these sets of custom extractors, CHAUCER-2 also used sets of heuristics in order to extract information related to targets from a number of "authoritative sources" available on the WWW,  including imdb.com, nndb.com, iplpotus.com, s9.com, and  wikipedia.org. Once each of these four extraction strategies were run for an individual target, output was then cast in a structured form and stored in a database (referred to as the Factoid Database).</p><p>In order to ensure that the Factoid Database contained a minimum of contradictory and/or redundant information, all new information added to the database was first sent to a Content Validation module, which followed (?) in using the output of systems for recognizing textual entailment (?) and textual contradiction (?) in order to determine when newly-discovered facts could be either inferred from -or were directly contradicted by -knowledge already stored in the database.</p><p>Once the Factoid Database was populated for each target, a select set database fields were then sent to a QUAB Generation module in order to generate sets of question-answer pairs which could be used by CHAUCER-2's other downstream Answer Selection and Answer Validation modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing</head><p>Following Target Processing, each question in a series was then sent to a series of Question Processing modules which annotated individual questions with the lexico-semantic information needed to generate queries for each of the individual Answer Retrieval strategies employed by CHAUCER-2.</p><p>As with our TREC 2006 system, questions were initially sent to a Question Annotation module which (1) identified token and collocation boundaries, performed (2) partof-speech tagging and (3) named entity annotation (using LCC's CICEROLITE named entity recognition system), and (4) resolved instances of pronominal and nominal coreference (using the knowledge-lean, heuristic approach first introduced in (?)). Questions were then sent to a Semantic Parsing module, which identified semantic dependencies using LCC's own PropBank-, NomBank-, and FrameNetbased semantic parsers.</p><p>Factoid and list questions were then sent to a new Answer Type Detection module which followed (?) and (?) in using a multiple Maximum Entropy-based classifiers in order to identify the expected answer type (EAT) of the question from LCC's answer type hierarchy.</p><p>Once annotation and answer type detection were complete, CHAUCER-2 sent questions to a Query Formulation module responsible for (1) extracting keywords and phrases, (2) identifying synonymous terms that could be used to augment a query, and (3) transforming questions into the partic-ular kinds of queries required by each of the system's Answer Retrieval strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Preprocessing and Retrieval</head><p>CHAUCER-2 employed the same document preprocessing framework introduced in (?). As with our TREC 2006 submission, we preprocessed the AQUAINT corpus with four types of information. First, we used an in-house implementation of the Collins parser to provide a full syntactic parse of every document in the AQUAINT-2 corpus; documents in the larger (and "noisier") BLOG-06 corpus were parsed using an in-house chunk parser. Second, we used three different semantic parsers in order to identify semantic dependencies imposed by both verbal and nominalized predicates. In addition to LCC's PropBank and NomBank parsers, we also used LCC's FrameNet-based semantic parser to identify instances FrameNet frames in natural language texts; a separate role classifier was used to identify roles associated each FrameNet frame.<ref type="foot" coords="3,146.28,260.32,3.48,6.28" target="#foot_0">1</ref> Third, we used LCC's CICERO-LITE named entity recognition system in order to classify more than 300 different types of names found in the corpus. We also used more than 500 lexicons and gazetteers derived from web-based resources in order to tag additional name types not covered by CICEROLITE. Finally, we used LCC's PINPOINT temporal normalization system (?) in order to map temporal expressions found in documents to a standardized (ISO 8601) format.</p><p>Following annotation, we indexed the AQUAINT-2 and BLOG-06 corpora using a customized version of the Lucene information retrieval engine. In addition to retrieving documents (and passages) based on literal strings and stemmed words, CHAUCER-2 was able also retrieve documents based on a wide range of semantic annotations made available by LCC's annotation components, including (1) entity types from LCC's CICEROLITE, (2) predicate-argument relationships from LCC's PropBank and NomBank parsers, (3) semantic frames, frame roles, and frame-denoting elements recognized by LCC's FrameNet parser, or (4) any of the events, attributes, or relations extracted by LCC's CICERO-CUSTOM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Retrieval and Extraction</head><p>CHAUCER-2 leverages a cascade of three separate factoid Answer Retrieval and Extraction strategies in order to identify the best answer to each question in a series. Strategies developed for this year's TREC include: (1) a Structured Data strategy which identifies answers from the information stored in the Factoid Database, (2) a Pattern-based strategy which leverages a variant of the Bindings Engine (BE) first proposed by (?; ?) in order to retrieve all of the text snippets matched by a pattern-based query, and (3) a traditional Entity-based strategy identifies candidate answers based on the distribution of entity types associated with an expected answer type. (Details of each of these three strategies are presented in Section 3.)</p><p>Although previous versions of CHAUCER (?; ?) have sought to identify likely candidate answers by combining the output of multiple Q/A strategies, CHAUCER-2 considers answers returned by its four Q/A engines in a fixed order. If no answers are returned by the Structured Data strategy, then questions are sent to the Neighborhood-based strategy; likewise, if no answers above a fixed confidence threshold are returned by the Neighborhood-based strategy, CHAUCER-2 defaults to using the Entity-based strategy in order to find answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Validation</head><p>As with our TREC 2006 submission, CHAUCER-2 employs a Answer Selection and Validation module in order to identify the best answer when multiple candidate answers are returned by one (or more) Answer Extraction strategies. Following Answer Extraction, the top five candidate answers identified by each strategy are then sent to a Candidate Answer Re-ranking module which uses a Maximum Entropybased re-ranker (based on (?) in order to provide a single ranked list of candidate answers for a particular question. The re-ranked list of answers were then sent to a final Answer Selection module which uses the state-of-the-art textual entailment system described in (?) in order to identify the single answer passage whose meaning is most likely to be entailed by the meaning of the original question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Answering Factoid Questions</head><p>In this section, we describe the CHAUCER-2 system for answering factoid questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing</head><p>This section describes how questions were processed in CHAUCER-2.</p><p>Keyword Expansion As with the TREC 2006 version of CHAUCER, keywords extracted from each question were processed by a Keyword Expansion module that was designed to identify additional synonymous keywords that could be used to augment the query CHAUCER-2 used to retrieve documents. This module used a set of heuristics in order to append synonyms and alternate keywords from a database of similar terms developed by LCC for previous TREC QA evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Coreference We incorporated a heuristic-based</head><p>Question Coreference module in order to resolve referring expressions found in the question series to antecedents mentioned in previous questions or in the target description. First, we used heuristics for performing name aliasing and nominal coreference from CICEROLITE in order to identify the full referent for each partial name mention found in the question series. Next, we constructed an antecedent list from all of the named entities that occurred in the question series prior to the current question. Each potential antecedent and referring expression found in the series were then annotated with name class, gender, and number information available from CICEROLITE. We then used the Hobbs Algorithm (Hobbs 1978) in order to match referring expressions to candidate antecedents. When no compatible antecedent could be identified from the antecedent list, we made no further attempt to resolve the referring expression found in the question.</p><p>Answer Type Detection CHAUCER-2 follows recent work in Answer Type Detection (?; ?; ?) (ATD) in using a multi-tiered classification approach to the recognition of the Expected Answer Type (EAT) of both factoid and list questions. Under our current approach, we use a three-stage approach to identifying the expected answer type of a question. First, questions are submitted to a coarse type ATD classifier which uses an variant of the Maximum Entropybased classifier first introduced in (?) in order to associate each question with one of a set of 11 coarse types. (The complete list of coarse types that were used in TREC 2007 are listed in Table <ref type="table" coords="4,128.81,261.04,3.63,8.97" target="#tab_0">1</ref>.) Second, questions of certain selected answer types are submitted to a second, expanded coarse type classifier which identifies a second coarse-grained answer type which can be used to further describe the type of answer sought by the question. (The set of expanded coarse types we considered in our TREC 2007 work are presented in Table <ref type="table" coords="4,89.73,326.80,3.63,8.97" target="#tab_1">2</ref>.) Finally, questions of each coarse type (or subtype) are then submitted to a third set of fine type classifiers which map each question to one of the set of fine answer types associated with each coarse type. In our work, we have used a hierarchy of over 275 different fine entity types derivable from the more than 300 different entity types recognized by LCC's CICEROLITE. (Table <ref type="table" coords="4,215.12,392.56,5.03,8.97" target="#tab_2">3</ref> presents a sample of some of the fine types that were used in CHAUCER-2.)   In our TREC 2007 work, we have found that CHAUCER-2's approach to Answer Type Detection hinges on the recognition of three core elements from each question: (1) the question stem, (2) the predicate answer type term, and ( <ref type="formula" coords="4,550.01,204.16,3.92,8.97">3</ref>) the nominal answer type term.</p><p>We define a question stem as the word (or phrase) which signals the broadest type of information sought by the question. With most interrogatives, the question stem is equivalent to a WH-word (e.g. who, what how) or a WH-phrase (e.g. how many, what book and can be extracted heuristically from the text of a question. <ref type="bibr" coords="4,427.80,280.00,3.48,6.28">2</ref> We consider a question's predicate answer type term (or predicate ATT) to be any verbal predicate (or predicate nominal) which exhibits a semantic dependency with a question stem. For example, in a question like "What civilization built the pyramids that towered over the Nile River?", the words built and towered are both predicates, but only the predicate built has selects the question stem What nation as an argument. In contrast, we define the nominal answer type term (or nominal ATT) as the noun phrase (NP) in a question that can lead to the inference of a question's expected answer type (EAT). For example, in the questions What country is Ahmadinejad president of? and What is Jon Bon Jovi's profession?, we assume that words such as country and profession can be used to infer an the most appropriate answer type for these questions.</p><p>In CHAUCER-2, recognition of the question stem, predicate ATT, and nominal ATT were performed using a heuristic based method that was tuned on a collection of more than 6000 factoid questions which had been annotated with these three core elements.</p><p>Evaluation results for nominal ATT detection are listed in Table <ref type="table" coords="4,345.10,513.04,3.77,8.97" target="#tab_3">4</ref>. CHAUCER-2 is least accurate on question stems that need no nominal ATT, such as who, when, and where. However, since these questions already derive much meaning from their stems, the downstream performance is not significantly damaged. On what questions, however, missing the nominal ATT will almost always cause the final answers to be incorrect. We found that the most common cause for missing the nominal ATT occurs in syntactic parsing or while interpreting the syntactic parse tree. For example, syntactic parsers will often mis-parse question, "What state-of-the-art technique is being used for the newest TMNT movie?" without the use of high performance chunking or collocation detection. In this question, CHAUCER-2 incorrectly annotates state as the nominal ATT instead of technique.  Document Retrieval CHAUCER-2 takes advantage of the same two-tiered approach to document retrieval first introduced in (?). Under this approach, output from a conservative entity-based answer extraction strategy was used in order to re-rank the top 200 documents retrieved from CHAUCER-2's standard retrieval engine.</p><p>Our TREC 2007 approach follows the same four-step approach that was implemented for our TREC 2006 system. First, we used a standard (expanded) keyword query to retrieve a total of 200 documents from the AQUAINT-2 and BLOG-06 corpora. The top 50 passages were then identified using a passage retrieval engine and submitted to CHAUCER-2's traditional entity-based answer extraction system. Passages were then re-ranked based on both (1) the density keywords extracted from the question found in each passage and (2) the distribution of entity types corresponding to the expected answer type of the question. The original set of 200 retrieved documents were then re-ranked based on the distribution of the top-ranked passages. As with our TREC 2006 system, only candidate answers that were extracted from the top 50 re-ranked documents were considered by downstream Answer Selection and Answer Validation modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Retrieval and Extraction</head><p>In this section, we the three different answer retrieval strategies that CHAUCER-2 leverages in order provide answers to factoid questions.</p><p>Extracting Answers from the Factoid Database CHAUCER-2's first factoid Q/A strategy takes advantage of the large repository of factual information stored in its Factoid Database in order to find answers to a fixed set of question types. Under this approach, a series of heuristics are used to transform specific types of questions into database queries designed to retrieve specific information from the Factoid Database. For example, given a question like (Q282.2) What is Pamuk's year of birth?, heuristics employed by CHAUCER-2 will retrieve the BIRTH-YEAR field associated with a record with a NAME label of Pamuk. While we were encouraged by the precision of this approach, this strategy ultimately was limited in terms of the coverage and precision of the mapping heuristics we employed to convert questions into database queries. In future work, we plan to explore a multi-tiered classification approach -similar to the one we have employed for Answer Type Detection -in order to directly map between questions and individual fields stored in the Factoid Database.</p><p>Pattern-based Answer Extraction Previous versions of LCC's question-answering systems (?; ?) have successfully used libraries of hand-crafted patterns in order to retrieveand extract -candidate answers from collections of texts. Despite their promise (and their precision), pattern-based approaches have faced three significant challenges which have ultimately limited their recall. First, in order to be effective, pattern-based systems must include large libraries of patterns which account for a significant portion of the different types of questions that users will ask. Second, pattern-based systems also need to have access to accurate heuristics which will map different types of questions to the classes of patterns which can be used to extract answers. Finally, pattern-based systems need to be used in conjunction with high-recall document retrieval engines: if the relevant text snippets aren't retrieved, pattern-based systems will not be able to return answers. In order to counter this third challenge, CHAUCER-2 leverages a new index annotation framework which makes it possible to retrieve all of the text snippets matched by a pattern-based (or variabilized) query -without having to retrieve documents using a traditional keyword-based query. CHAUCER-2's index annotation framework (based on work first done by (?) for an information extraction application) which makes it possible to extract all of the text passages matching an extraction pattern in a text collection without having to retrieve documents through an information retrieval engine. Following (?), we developed our own retrieval engine -which we refer to as theneighborhood retrieval engine -which can return short text snippets in response to variabilized queries. For example, given a query like TYPE PERSON NAME such as ProperNoun(Head(NP)), our engine will return the set of entities marked as TYPE PERSON NAME which are followed by the sequence of the string such as and any proper noun which also heads an noun phrase (NP).</p><p>CHAUCER-2's neighborhood retrieval engine processes variables like TYPE PERSON NAME or ProperNoun(Head(NP)) by returning every possible string in the corpus that has a matching type and that can be substituted for the variable and still satisfy the user's query. In order to retrieve the extensions of these variables quickly and without having to post-process documents, we again followed (Cafarella et al. 2005) in creating a new type of augmented inverted index, known as a neighborhood index, which allows for the processing of these queries with O(k) random disk seeks and O(k) disk reads, where k is defined as the number of non-variable terms in a query. In addition to keeping a list of the documents in which a term occurs -and a list of positions where the term occurs, the neighborhood index also stores a list of left-hand and right-hand neighbors at each position. The neighborhood contains the tokens token the left and right of the center token as well as any named entities and phrase chunks that end just before the token or start just after the token. Neighborhoods are additionally constrained to avoid crossing sentence boundaries.</p><p>Neighborhood indices are built by loading the documents from a normal Lucene index in order to produce a separate index just to represent neighborhoods. Most stop words are indexed because they can be important for certain query types, although queries involving stop words take much longer to execute than other queries. To reduce the size of the index, common words are stored in a dictionary and the index contains 1-or 2-byte pointer into the dictionary. Less common words are stored verbatim in the index. When determining which entities and phrase chunks are adjacent to a given token, some tokens are skipped. These tokens include articles, the word "who", quotation marks, and parenthesis. Skipping over these tokens dramatically increases the recall of some queries. These "noise" tokens are not stop words in the traditional sense; it is possibly to include these tokens in a query, but their presence in a document does not prevent neighborhoods from being found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity-based Answer Extraction</head><p>As with the TREC 2006 version of CHAUCER, CHAUCER-2's entity-based answer extraction strategy uses the distribution of named entities (recognized by LCC's CICEROLITE named entity recognition system) in order to identify candidate answers to individual questions. Under this approach, passages containing entity types associated with the question's expected answer type are first retrieved from the set of documents retrieved by the system. Candidate answers found within each passage are then extracted and re-ranked based on the distribution and density of question keywords discovered in each passage.</p><p>While traditional entity-based Q/A strategies have shown much promise in previous TREC QA evaluations (?), they often retrieve many spurious answers which can greatly complicate the tasks of Answer Ranking and Answer Selection. In our TREC 2007 work, we hypothesized that if we could retrieve candidate answers not just based on the distribution of entity types -but in terms of specific conjunctions of semantic features extracted from a question -we could constrain the total number of candidate answers that are retrieved for a question without experiencing any reduction in overall precision.</p><p>In our experiments, we investigated how five different se-mantic features -based on the distribution of semantic dependencies in an candidate answer (as recognized by LCC's PropBank and NomBank parsers) -could be used in order to enhance the precision of a traditional entity-based answer extraction strategy. These five features included the presence of a semantic dependency found (1) between an entity in the answer (Ent ans ) corresponding to the question's expected answer type and a predicate (Pred ans ) corresponding to a the question's predicate answer type term, (2) between the Ent ans and any other predicate in the candidate answer, (3) between the Pred ans and any other argument in the candidate answer, (4) between an argument in the candidate answer (Arg ans ) corresponding to an argument from the question and any other predicate in the answer, and ( <ref type="formula" coords="6,514.04,199.60,3.92,8.97">5</ref>) between the Arg ans and the Pred ans . (A summary of the 10 different strategies are presented in Table <ref type="table" coords="6,449.25,221.56,3.77,8.97">6</ref>. <ref type="bibr" coords="6,456.96,220.00,3.48,6.28">3</ref> )</p><formula xml:id="formula_1" coords="6,333.96,245.20,209.27,111.88">Strategy Ent Ent-Pred Ent- * * -Pred Arg- * Arg-Pred 1 × × × × × 2 × × × × 3 × × × × 4 × × × × 5 × × × × 6 × × × × 7 × × × 8 × × × 9 × × × 10 × × ×</formula><p>Table <ref type="table" coords="6,345.45,369.88,3.90,8.97">6</ref>: Query Strategies used by CHAUCER-2's Entity-Based Q/A Strategy.</p><p>While Strategy 1 in (Table <ref type="table" coords="6,442.82,403.84,5.03,8.97">6</ref> corresponds to the default entity-based Q/A strategy, Strategies 2 through 10 represent contexts in which the retrieved candidate answers are subject to additional constraints. For example, Strategy 8 requires that all retrieved candidate answers must meet two conditions. First, any valid candidate answer must include an entity that corresponds to the expected answer type of the question that also participates in a predicate-argument relation with a predicate. In addition, the answer must also include an instance of an argument from the question which participates in a predicate-argument relationship with a predicate as well.</p><p>In our early work, we found that most of the query strategies listed in Table <ref type="table" coords="6,395.20,546.40,5.03,8.97">6</ref> returned few (if any) candidate answers for most questions; however, their precision (when parser errors where taken into account) in many cases approached 100%. In order to capitalize on these high-precision, lowrecall strategies, we impelmented these 10 strategies as another cascade, which ranged from the most restrictive strategies (i.e. the ones which included the most constraints) to the least restrictive (i.e. Strategy 1, the traditional entitybased strategy). Although we considered candidate answers retrieved by all 10 strategies during Answer Ranking and Answer Validation, candidate answers were assigned a weight corresponding to the query strategy (or strategies) which was responsible for retrieving them; answers retrieved by more restrictive strategies received higher weights than those retrieved by less restrictive strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Ranking</head><p>Following Answer Extraction, CHAUCER uses a Maximum Entropy-based re-ranker (similar to (?)) in order to compile answers from each of the six answer extraction strategies into a single ranked list. This re-ranker was trained on the top ten answers returned by each of CHAUCER's answer extraction strategies for each of the questions taken from the TREC 2004 and TREC 2005 datasets. (Answers were keyed automatically using "gold" answer patterns made available by the TREC organizers and other participating teams.) Five sets of features were used in this re-ranker: (1) the strategy used to extract the answer, (2) the EAT of the original question, (3) the entity type associated with the exact answer, (4) the redundancy of the answer across the top-ranked answers, and (5) the confidence assigned to the answer by each answer extraction strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Selection</head><p>Once a ranking of candidate answers is performed, the top 25 answers were then sent to an Answer Selection module which leverages LCC's state-of-the-art textual entailment system in order to identify the answer which best approximates the semantic content of the original question. Popularized by the recent PASCAL Recognizing Textual Entailment (RTE) Challenges (?), textual entailment systems seek to identify whether the meaning of a hypothesis can be reasonably inferred from the meaning of a corresponding text. While the RTE Challenges have focused to-date only on the computation of entailment relationships between sentencelength texts and hypotheses, our recent work (?) has shown that current systems for recognizing TE can be leveraged to accurately identify entailment relationships between questions and answers -or even questions and other questions.</p><p>CHAUCER uses the entailment system described in (?) in order to estimate the likelihood that a question entails either (1) a candidate answer extracted by one of CHAUCER's six answer extraction strategies or (2) a predictive question generated by the Predictive Question Generation module. Following (?), we first filtered all candidate answers that were not entailed by the original questions. The remaining candidate answers (including any remaining predictive questionanswer pairs) were re-ranked based on the entailment confidence output by the RTE system. The top-ranked answer was then returned as our submitted answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">List Questions</head><p>This section describes the multiple strategies that CHAUCER-2 uses in order to provide answers to list questions. In order to maximize both precision and recall of the list answers CHAUCER-2 returns, we developed two distinct types of answer-finding strategies: (i) strategies that find all globally correct answers from an external knowledge source, then choose the supported answers that actually exist in the text, and (ii) strategies that find possible answers in the text and retain only those that pass some form of validation. The Type (i) strategies we have investigated in our TREC 2007 work include an (1) Authoritative Source strategy, a (2) Wikipedia list strategy, and a (3) Lexicon strategy. We only investigated one Type (ii) strategies this year, however: a Web Count strategy first introduced in (?). Authoritative Source List Strategy Similar to the Structured Data strategy implemented for factoid questions, CHAUCER-2's Authoritative Source strategy uses the sources of semi-structured data stored in the Factoid Database in order to provide answers to list questions. Although the Factoid Database was used primarily used for answering factoid questions, some list fields (such as the types of lists found on sites like imdb.org or stored in the HTML table "infoboxes" found on many wikipedia.org pages) were extracted heuristically and stored in the Factoid Database prior to the TREC 2007 evaluation. As with factoid questions, heuristics are used to map common question types to the particular fields (and sources) which would be most likely contain a correct answer. In addition, we found that the the the nominal ATT (as recognized by the Answer Type Detection module could often be used to identify a field which could contain a relevant set of answers. For example, given a question like (278.5) What architects were involved in building St. Peter's?, we found that searching the Fac-toid Database for the term architect returned a pointer to the "infobox" included on the wikipedia.org page for St. Peter's Basilica which mentions the four architects who worked on the basilica (e.g. Donato Bramante, Antonio da Sangallo the Younger, Michelangelo, and Giacomo della Porta. (See Figure <ref type="figure" coords="8,82.53,112.00,5.03,8.97" target="#fig_1">2</ref> for an example of a "infobox".) <ref type="table" coords="8,166.01,128.92,36.19,8.97">Strategy</ref> Our second strategy sought to leverage lists and tables mentioned on relevant Wikipedia<ref type="foot" coords="8,93.96,149.20,3.48,6.28" target="#foot_3">4</ref> pages in order to identify candidate answers for list questions. Under this strategy, keywords extracted from both the question and the series target were used to retrieve a set of relevant pages from Wikipedia. Heuristics used to extract lists (and to "unroll" HTML) tables were then used -in conjunction with entity information available from CICERO-LITE in order to identify sets of multiple candidate answers. For example, the question (217.6) What are titles of albums featuring Jay-Z? has answers that can be found in the table on Wikipedia's "Jay-Z discography" page.<ref type="foot" coords="8,223.20,247.84,3.48,6.28" target="#foot_4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia List and Table</head><p>Lexicon List Strategy In a third strategy, we used the collection of more than 800 different lexicons included in LCC's CICEROLITE in order to provide answers to list questions. As with the previous two strategies, heuristics were used again to map between selected types of question types (and or question keywords) and each of the lexicons available to CHAUCER-2.CHAUCER-2 utilizes the lexicon list strategy if three conditions are met: (1) a lexicon exists that matches the nominal ATT, (2) the nominal ATT is sufficiently far down the answer type hierarchy (by default, 2 nodes) from a coarse type, and (3) there is a proper noun (which becomes a mandatory keyword) after the nominal ATT in the question. This means that a questions like "What Republican senators supported the nomination?" and "What persons has Krugman criticized in his op-ed columns?" will not use the lexicon strategy (due to conditions (3) and (2), respectively), while a question like "What musicals did Kurt Weill write?" will.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web Count</head><p>List Strategy As with our TREC 2006 system, CHAUCER-2 also utilizes a method based on term frequency counts (obtained from search engines like Google) in order to determine how much of an associate there was between a candidate answer and both the series target and answer type term. These two scores were then combined in order to rank each individual candidate answer; answers above a threshold were included in the set of candidate answers considered by the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy Selection</head><p>As with factoid questions, we again cast the problem of selecting answers from multiple strategies as a cascade: list answers were considered first from the (1) Authoritative Source strategy, followed by answers from the (2) Wikipedia List and Table strategy, the (3) Lexicon List strategy, and (4) the Web Count strategy. Answers were added to the list until there were a maximum number of answers -or until there were no answers with a confidence level above a fixed threshold to return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Answering "Other" Questions</head><p>In this section, we describe our approach to answering the "other" questions included with each question in the TREC 2007 QA Main Task.</p><p>As with our TREC 2006 submission, CHAUCER-2 begins the process of finding answers to "other" questions by first computing two types of automatic topic representations, including: (1) weighted lists of topic relevant terms known as Topic Signatures (?) (TS 1 ) and (2) a corresponding weighted list of topic relevant relations, known as Enhanced Topic Signatures (?) (TS 2 ). (As described in (?), both topic representations are computed from the top 100 documents retrieved from the AQUAINT-2/BLOG-06 corpus using keywords extracted from the series target -and all of the previous questions contained in the question series.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nugget Extraction</head><p>Once sets of TS 1 terms and TS 2 relations have been computed, CHAUCER-2 retrieves the top 500 documents from the AQUAINT-2/BLOG-06 corpus which contain at least one keyword from the series target. Passages are then extracted and ranked based on the top 25 most topical terms and relations. The top 500 passages retrieved using this method are then split into individual clauses using the sentence decomposition techniques introduced in (?) and then were made available to the following four nugget extraction techniques.</p><p>"Web Words" Nugget Extraction Following an approach proposed by (?), we used the top 50 most frequentlyoccurring non-stop words found in the first 100 pages retrieved from Google containing the series target in order to rank sentences retrieved from the AQUAINT-2/BLOG-06 corpus. Top-scoring sentences were then sent to an Answer Selection module to be combined with output from the other nugget extraction techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic-Based Nugget Extraction</head><p>Following work done by (?) for question-focused summarization, we used weights associated with TS 1 terms and TS 2 relations to compute a composite topic score for each sentence in the set of documents retrieved for a target. Sentences were re-ranked based on their topic score before being submitted to the Answer Selection module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Pattern-Based Nugget Extraction</head><p>As with our TREC 2006 submission, we again experimented with using the probabilistic soft matching techniques first described in (?) in order to identify additional patterns that could be used to extract nuggets for a particular target type. we followed (?) in developing a bigram soft pattern model in order to identify potential matches between a set of training sentences and each of the sentences extracted for a particular target. Training sentences were derived for each target type from two different sources: (1) the collection of "gold" nuggets identified for the TREC 2005 "other questions" and a collection of 5,000 biographies, descriptions, and encyclopedia articles that were downloaded from the collection of "authoritative sources" used to populate CHAUCER-2's factoid database.</p><p>Headline Extraction In addition to nuggets retrieved using the previous three strategies, CHAUCER-2 also retrieves all of the document headlines which contain both the series target and at least one TS 1 term or TS 2 relation. While not every series target appeared in a headline of a document contained in the AQUAINT-2 collection, we found that headlines often contained a succinct, topical statement that was not unlike the "gold standard" nuggets reported as the keys for "other" questions. Since headlines appeared to provide consistently good information for "other" questions, they were not submitted to the Answer Selection module, but appended to the top of each submitted set of nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Combination</head><p>In a departure from the content modeling approach introduced in (?), we used a simple combination method to combine (and rank) candidate nuggets for submission. Following work done by (?) for the DUC multi-document summarization evaluations, candidate nuggets were assigned a composite score based on the density of TS 1 terms and TS 2 relations as well as the individual rank that they were assigned by each individual nugget extraction technique. All nuggets which received a score above a fixed threshold were returned as part of our official submission.   TREC 2007 marked the first year where we made a concentrated effort to develop a coherent strategy for answering list questions. We believe our results to be encouraging: our TREC 2007 results more than doubled our TREC 2006 results in terms of recall, precision, and F-measure (Fβ1).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper describes CHAUCER-2, the most recent version of Language Computer Corporation's CHAUCER line of automatic question-answering systems. Developed for the 2007 TREC QA Track Main Task, CHAUCER-2 was designed to explore how a new, semantically-rich framework for information retrieval could be used to boost the overall performance of the answer extraction and answer selection components of an end-to-end question-answering system. First, unlike the keyword-based retrieval systems used by LCC's previous Q/A systems (?; ?), CHAUCER-2 employed a novel indexing and retrieval engine which supported a wide range of semantically-rich queries, including queries based on semantic types recognized by LCC's CICEROLITE named entity recognition system as well as semantic dependencies identified by LCC's PropBank-, NomBank-, and FrameNet-based semantic parsers. We found that support for these new types of queries dramatically enhanced the performance of the retrieval components used in CHAUCER-2 while significantly reducing the number of candidate answers that had to be considered by CHAUCER-2s Answer Ranking and Answer Validation modules.</p><p>In addition to supporting multiple query types, CHAUCER-2 also leveraged a variant of the Bindings Engine (BE) first proposed by (?; ?) in order to retrieve of all of the text snippets matched by a query without having to retrieve documents using a keyword-based query. We found that use of this framework greatly both enhanced the efficiency and the recall of traditional pattern-based approaches to Q/A.</p><p>Finally, CHAUCER-2 incorporated a new, multi-tiered Answer Type Detection (ATD) module which reduced the number of expected answer types (EATs) considered by the system for factoid or list questions, while maintaining the same high levels of precision exhibited by previous systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,162.96,289.36,285.90,8.97;2,54.00,310.60,210.37,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the CHAUCER-2 Question-Answering System architecture of CHAUCER-2 is presented in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,330.96,479.80,215.30,8.97;7,366.72,146.95,144.00,313.85"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Wikipedia Infobox for "St. Peter's Basilica"</figDesc><graphic coords="7,366.72,146.95,144.00,313.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,68.40,425.80,209.17,232.84"><head>Table 1 :</head><label>1</label><figDesc>Coarse Answer Types used by CHAUCER-2</figDesc><table coords="4,72.60,425.80,201.28,232.84"><row><cell cols="2">Coarse Type</cell><cell>Example(s)</cell></row><row><cell>HUMAN</cell><cell></cell><cell cols="2">George W. Bush, Texans, State Department</cell></row><row><cell cols="2">LOCATION</cell><cell cols="2">Tajikistan, Grand Canyon, Sears Tower</cell></row><row><cell cols="2">ABBREVIATION</cell><cell>AARP, Dr.</cell></row><row><cell>WORK</cell><cell></cell><cell>Hamlet, Guernica</cell></row><row><cell cols="2">NUMERIC</cell><cell>55mph, £124</cell></row><row><cell cols="2">TEMPORAL</cell><cell>1945, 8 years ago</cell></row><row><cell>TITLE</cell><cell></cell><cell>Physician, Israeli</cell></row><row><cell cols="2">CONTACT-INFO</cell><cell cols="2">andy@languagecomputer.com</cell></row><row><cell cols="2">OTHER-ENTITY</cell><cell cols="2">Hurricane Andrew, Budweiser</cell></row><row><cell cols="2">OTHER-VALUE</cell><cell>purple, guilty</cell></row><row><cell cols="2">COMPLEX</cell><cell>-</cell></row><row><cell>Coarse Type</cell><cell cols="2">Expanded Coarse Type</cell><cell>Example(s)</cell></row><row><cell></cell><cell></cell><cell>INDIVIDUAL</cell><cell>Bill Clinton, Paul McStay</cell></row><row><cell>HUMAN</cell><cell></cell><cell>GROUP</cell><cell>journalists, Floridians</cell></row><row><cell></cell><cell cols="2">ORGANIZATION</cell><cell>FBI, The White Stripes</cell></row><row><cell></cell><cell></cell><cell>FACILITY</cell><cell>MacDill AFB, Hoover Dam</cell></row><row><cell>LOCATION</cell><cell></cell><cell>GPE</cell><cell>India, Los Angeles</cell></row><row><cell></cell><cell cols="2">PHYSICAL LOCATION</cell><cell>Great Plains, Blue Nile</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,54.00,56.56,493.56,634.76"><head>Table 2 :</head><label>2</label><figDesc>Breakdown of HUMAN and LOCATION Coarse Answer Types into Expanded Coarse Types</figDesc><table coords="4,329.04,56.56,218.52,68.44"><row><cell>Coarse Type</cell><cell>Fine Types</cell></row><row><cell>FACILITY</cell><cell>CASINO, MUSEUM</cell></row><row><cell>GPE</cell><cell>CITY, COUNTRY, STATE</cell></row><row><cell>INDIVIDUAL</cell><cell>ACTOR, BASEBALL-PLAYER, MILITARY-PERSON</cell></row><row><cell>ORGANIZATION</cell><cell>COMPANY, UNIVERSITY, BASEBALL-TEAM</cell></row><row><cell>PHYSICAL LOCATION</cell><cell>ISLAND, PLANET, RIVER</cell></row><row><cell>WORK</cell><cell>ALBUM, SONG, BOOK</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,324.60,137.92,228.24,8.97"><head>Table 3 :</head><label>3</label><figDesc>Examples of CHAUCER-2's Fine Answer Types.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,54.00,103.24,238.46,236.92"><head>Table 4 :</head><label>4</label><figDesc>Nominal Answer Type Term Detection Results, by question stem, on TREC 2007 Questions.The overall answer type detection accuracy scores for CHAUCER-2 are listed in Table5. The final score is primarily due to the combined error of the Coarse, Human, and Location classifiers.</figDesc><table coords="5,100.56,103.24,145.36,236.92"><row><cell>Question Stem</cell><cell cols="3">Total Questions</cell><cell>Accuracy</cell></row><row><cell cols="2">who/whom/whose</cell><cell>58</cell><cell>89.7</cell></row><row><cell>what/which</cell><cell></cell><cell>278</cell><cell>97.8</cell></row><row><cell>when</cell><cell></cell><cell>13</cell><cell>92.3</cell></row><row><cell>where</cell><cell></cell><cell>13</cell><cell>92.3</cell></row><row><cell>how</cell><cell></cell><cell>65</cell><cell>100</cell></row><row><cell>list</cell><cell></cell><cell>8</cell><cell>100</cell></row><row><cell>name</cell><cell></cell><cell>10</cell><cell>100</cell></row><row><cell>Total</cell><cell></cell><cell>445</cell><cell>96.9</cell></row><row><cell>Type</cell><cell cols="2">Total Questions</cell><cell>Accuracy</cell></row><row><cell>Coarse</cell><cell>445</cell><cell></cell><cell>90.6%</cell></row><row><cell>Human</cell><cell>154</cell><cell></cell><cell>90.3%</cell></row><row><cell>Location</cell><cell>59</cell><cell></cell><cell>88.1%</cell></row><row><cell>Fine</cell><cell>445</cell><cell></cell><cell>79.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,54.00,352.96,238.44,19.89"><head>Table 5 :</head><label>5</label><figDesc>Answer Type Detection Results on TREC 2007 Questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,54.00,355.36,238.65,82.96"><head></head><label></label><figDesc>Table 7 presents a summary of CHAUCER-2's performance on the TREC 2007 QA Main Task.</figDesc><table coords="9,100.08,390.64,145.37,47.68"><row><cell>Task</cell><cell>Evaluation Metric</cell><cell>CHAUCER-2</cell></row><row><cell>Factoid Q/A</cell><cell>Accuracy</cell><cell>56.1%</cell></row><row><cell>List Q/A</cell><cell>Fβ1</cell><cell>32.4%</cell></row><row><cell>"Other" Q/A</cell><cell>Fβ3</cell><cell>26.1%</cell></row><row><cell>Series Score</cell><cell>Aggregate</cell><cell>35.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,54.00,451.24,238.31,116.56"><head>Table 7 :</head><label>7</label><figDesc>Summary of TREC 2007 QA Main Track Results.   A detailed breakdown of the results from the Factoid Q/A task is presented in Table8.</figDesc><table coords="9,133.92,511.36,76.90,56.44"><row><cell>Judgment</cell><cell>Percent</cell></row><row><cell>Wrong</cell><cell>37.5%</cell></row><row><cell>Unsupported</cell><cell>2.7%</cell></row><row><cell>Inexact</cell><cell>4.7%</cell></row><row><cell>Locally Correct</cell><cell>1.2%</cell></row><row><cell>Globally Right</cell><cell>53.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,89.88,573.40,166.52,8.97"><head>Table 8 :</head><label>8</label><figDesc>TREC 2007 Factoid Q/A Results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,96.96,57.16,460.88,652.41"><head>Table 9 :</head><label>9</label><figDesc>TREC 2007 List Q/A ResultsFinally, Table10shows our precision, recall, and F-Score for "other" questions.</figDesc><table coords="9,402.12,92.68,71.37,36.52"><row><cell>Metric</cell><cell>TREC 2007</cell></row><row><cell>Recall</cell><cell>0.288</cell></row><row><cell>Precision</cell><cell>0.2501</cell></row><row><cell>F(β=3)</cell><cell>0.261</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,356.40,134.80,164.48,8.97"><head>Table 10 :</head><label>10</label><figDesc>TREC 2006 Other Q/A Results</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,70.08,666.00,222.18,8.07;3,54.00,675.96,238.30,8.07;3,54.00,685.92,238.28,8.07;3,54.00,695.88,218.01,8.07"><p>The AQUAINT-2 corpus was processed using semantic parsers which had been previously trained on data that had been fully parsed; documents in the BLOG-06 corpus were processed using parsers that had been trained on the output of a chunk parser.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,335.64,656.04,222.23,8.07;4,319.56,666.00,238.44,8.07;4,319.56,675.96,238.29,8.07;4,319.56,685.92,238.05,8.07;4,319.56,695.88,21.58,8.07"><p>We assume that question stem of an imperative questions like Name books that Pamuk has written. corresponds to the initial predicate which signals both that the statement is a request for information and the type of information that the speaker presumably seeks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,335.64,675.96,222.10,8.07;6,319.56,685.92,238.17,8.07;6,319.56,695.88,144.14,8.07"><p>We selected these 10 strategies to experiment with during our preparations for TREC 2007. We plan to explore the other possible combinations of features in future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,70.08,679.56,91.86,8.07"><p>http://www.wikipedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,70.08,690.48,169.57,8.07"><p>http://en.wikipedia.org/wiki/Jay-Z discography</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Sanda Harabagiu</rs>, <rs type="person">Paul Aarseth</rs>, <rs type="person">John Lehmann</rs>, and <rs type="person">Luke Nezda</rs> for their assistance with this work. This material is based upon work funded in whole or in part by the <rs type="funder">U.S. Government</rs> and any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,324.48,675.96,233.34,8.07;9,324.48,685.92,233.25,8.07;9,324.48,695.88,129.38,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,456.03,675.96,101.79,8.07;9,324.48,685.92,82.17,8.07">A Search Engine for Natural Language Applications</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,424.73,685.92,133.00,8.07;9,324.48,695.88,76.26,8.07">Proceedings of the Fourteenth World Wide Web conference</title>
		<meeting>the Fourteenth World Wide Web conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,57.84,233.12,8.07;10,59.04,67.80,233.24,8.07;10,59.04,77.76,139.35,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,59.04,67.80,233.24,8.07;10,59.04,77.76,13.58,8.07">KnowItNow: Fast, Scalable, Information Extraction from the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,89.98,77.76,103.64,8.07">Proceedings of EMNLP-2005</title>
		<meeting>EMNLP-2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,90.12,233.24,8.07;10,59.04,100.08,233.25,8.07;10,59.04,110.04,87.54,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,242.77,90.12,49.52,8.07;10,59.04,100.08,216.65,8.07">Enhanced answer type inference from questions using sequential models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,59.04,110.04,82.25,8.07">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,122.40,233.24,8.07;10,59.04,132.36,233.37,8.07;10,59.04,142.32,233.24,8.07;10,59.04,152.28,50.39,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,216.98,122.40,75.30,8.07;10,59.04,132.36,200.44,8.07">Unsupervised Learning of Soft Patterns for Definitional Question Answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,276.35,132.36,16.07,8.07;10,59.04,142.32,204.23,8.07">Proceedings of the Thirteenth World Wide Web conference</title>
		<meeting>the Thirteenth World Wide Web conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,164.52,233.26,8.07;10,59.04,174.48,233.25,8.07;10,59.04,184.56,113.21,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,252.61,164.52,39.69,8.07;10,59.04,174.48,145.77,8.07">The pascal recognizing textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.29,174.48,68.00,8.07;10,59.04,184.56,109.04,8.07">Proceedings of the PASCAL Challenges Workshop</title>
		<meeting>the PASCAL Challenges Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,196.80,233.37,8.07;10,59.04,206.76,233.24,8.07;10,59.04,216.72,80.96,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,194.07,196.80,98.34,8.07;10,59.04,206.76,179.81,8.07">Methods for Using Textual Entailment in Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,259.92,206.76,32.37,8.07;10,59.04,216.72,76.00,8.07">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,229.08,233.37,8.07;10,59.04,239.04,233.40,8.07;10,59.04,249.00,233.38,8.07;10,59.04,258.96,68.47,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,137.85,239.04,154.59,8.07;10,59.04,249.00,70.96,8.07">Employing Two Question Answering Systems in TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,149.12,249.00,143.30,8.07;10,59.04,258.96,64.56,8.07">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,271.32,233.25,8.07;10,59.04,281.28,233.14,8.07;10,59.04,291.24,42.80,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,257.40,271.32,34.89,8.07;10,59.04,281.28,168.37,8.07">Negation, Contrast and Contradiction in Text Processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,247.94,281.28,44.25,8.07;10,59.04,291.24,38.59,8.07">Proceedings of AAAI-06</title>
		<meeting>AAAI-06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,303.60,233.35,8.07;10,59.04,313.56,152.30,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,135.67,303.60,123.27,8.07">Incremental Topic Representations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,276.33,303.60,16.07,8.07;10,59.04,313.56,148.39,8.07">Proceedings of the 20th COLING Conference</title>
		<meeting>the 20th COLING Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,325.92,233.13,8.07;10,59.04,335.88,233.12,8.07;10,59.04,345.84,233.41,8.07;10,59.04,355.80,19.66,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,175.25,325.92,116.93,8.07;10,59.04,335.88,171.17,8.07">A Discourse Commitment-based Framework for Recognizing Textual Entailment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,247.91,335.88,44.25,8.07;10,59.04,345.84,233.41,8.07;10,59.04,355.80,15.73,8.07">Proceedings of the ACL 2007 Workshop on Paraphrasing and Textual Entailment</title>
		<meeting>the ACL 2007 Workshop on Paraphrasing and Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,368.04,233.14,8.07;10,59.04,378.00,233.13,8.07;10,59.04,387.96,167.29,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,197.07,368.04,95.10,8.07;10,59.04,378.00,126.45,8.07">Machine Reading through Textual and Knowledge Entailment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,204.03,378.00,88.15,8.07;10,59.04,387.96,163.31,8.07">Proceedings of the 2007 AAAI Spring Symposium on Machine Reading</title>
		<meeting>the 2007 AAAI Spring Symposium on Machine Reading</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,400.32,233.14,8.07;10,59.04,410.28,233.21,8.07;10,59.04,420.24,233.38,8.07;10,59.04,430.20,103.74,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,59.04,410.28,233.21,8.07;10,59.04,420.24,48.04,8.07">Ferret: Interactive Question-Answering for Real-World Research Environments</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehamnn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,126.20,420.24,166.22,8.07;10,59.04,430.20,100.11,8.07">Proceedings of the 2006 COLING-ACL Interactive Presentations Session</title>
		<meeting>the 2006 COLING-ACL Interactive Presentations Session</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,442.56,233.28,8.07;10,59.04,452.52,233.40,8.07;10,59.04,462.48,233.26,8.07;10,59.04,472.44,105.41,8.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,126.22,452.52,166.22,8.07;10,59.04,462.48,67.39,8.07">Recognizing Textual Entailment with LCC&apos;s Groundhog System</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,142.64,462.48,149.67,8.07;10,59.04,472.44,60.20,8.07">Proceedings of the Second PASCAL Challenges Workshop</title>
		<meeting>the Second PASCAL Challenges Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="10,59.04,484.80,233.27,8.07;10,59.04,494.76,233.50,8.07;10,59.04,504.72,232.41,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,103.42,494.76,189.12,8.07;10,59.04,504.72,16.02,8.07">Qusetion Answering with LCC&apos;s Chaucer at TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,92.03,504.72,195.51,8.07">Proceedings of the Fifteenth Text REtrieval Conference</title>
		<meeting>the Fifteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,517.08,233.37,8.07;10,59.04,527.04,233.25,8.07;10,59.04,537.00,199.30,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,238.09,517.08,54.32,8.07;10,59.04,527.04,215.90,8.07">Experiments at the University of Edinburgh for the TREC 2006 QA Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kaisser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,59.04,537.00,195.39,8.07">Proceedings of the Fifteenth Text REtrieval Conference</title>
		<meeting>the Fifteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,549.24,233.26,8.07;10,59.04,559.20,233.14,8.07;10,59.04,569.28,233.26,8.07;10,59.04,579.24,41.14,8.07" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,191.45,559.20,100.73,8.07;10,59.04,569.28,163.82,8.07">Lcc&apos;s gistexter at duc 2006: Multi-strategy multi-document summarization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,239.05,569.28,53.24,8.07;10,59.04,579.24,18.95,8.07">Proceedings of DUC</title>
		<meeting>DUC</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,591.48,233.36,8.07;10,59.04,601.44,233.14,8.07;10,59.04,611.40,233.38,8.07;10,59.04,621.36,113.57,8.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,83.87,601.44,208.31,8.07;10,59.04,611.40,93.04,8.07">TASER: A Temporal and Spatial Expression Recognition and Normalization System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Aarseth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nezda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deligonul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,167.93,611.40,124.49,8.07;10,59.04,621.36,109.65,8.07">Proceedings of the 2005 Automatic Content Extraction Conference</title>
		<meeting>the 2005 Automatic Content Extraction Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,633.72,233.27,8.07;10,59.04,643.68,233.28,8.07;10,59.04,653.64,41.25,8.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,170.59,633.72,104.19,8.07">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,59.04,643.68,233.28,8.07;10,59.04,653.64,36.66,8.07">Proc. the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,59.04,666.00,233.25,8.07;10,59.04,675.96,233.26,8.07;10,59.04,685.92,233.90,8.07;10,59.04,695.88,191.89,8.07" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,169.13,666.00,123.16,8.07;10,59.04,675.96,122.08,8.07">The automated acquisition of topic signatures for text summarization</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,203.55,675.96,88.75,8.07;10,59.04,685.92,145.22,8.07">Proceedings of the 18th conference on Computational linguistics</title>
		<meeting>the 18th conference on Computational linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="495" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,324.48,57.84,233.37,8.07;10,324.48,67.80,233.33,8.07;10,324.48,77.76,233.29,8.07;10,324.48,87.72,40.54,8.07" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,505.21,57.84,52.65,8.07;10,324.48,67.80,164.38,8.07">Statistical qaclassifier vs re-ranker: What&apos;s the difference?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,503.97,67.80,53.85,8.07;10,324.48,77.76,233.29,8.07;10,324.48,87.72,36.48,8.07">Proceedings of the ACL Workshop on Multilingual Summarization and Question Answering</title>
		<meeting>the ACL Workshop on Multilingual Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
