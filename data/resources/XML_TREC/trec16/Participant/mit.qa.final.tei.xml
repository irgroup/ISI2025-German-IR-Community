<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.88,94.61,304.15,12.62">CSAIL at TREC 2007 Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.35,133.86,60.92,10.52"><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,186.15,133.86,64.88,10.52"><forename type="first">Sue</forename><surname>Felshin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.12,133.86,93.83,10.52"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.29,133.86,81.69,10.52"><forename type="first">Federico</forename><surname>Mora</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,459.52,133.86,79.58,10.52"><forename type="first">Yuan</forename><forename type="middle">K</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,100.27,147.81,85.96,10.52"><forename type="first">Gabriel</forename><surname>Zaccak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,197.01,147.81,89.37,10.52"><forename type="first">Ammar</forename><surname>Ammar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.92,147.81,63.41,10.52"><forename type="first">Eric</forename><surname>Eisner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.62,147.81,65.45,10.52"><forename type="first">Asli</forename><surname>Turgut</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,448.88,147.81,110.76,10.52"><forename type="first">L</forename><surname>Brown Westrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.88,94.61,304.15,12.62">CSAIL at TREC 2007 Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E66325590AF28A5EE05B624435D59FE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction MIT CSAIL's entries for the TREC 2007 question answering track built on our systems of previous years, updating them for the new corpora. Our greatest efforts went into the system that handles the 'other' questions, looking for new descriptive information about the topic. We noticed in our experiments with Nuggeteer <ref type="bibr" coords="1,278.86,355.32,42.58,10.48;1,103.18,369.76,87.67,10.48" target="#b5">(Marton and Radul, 2006)</ref> 1 that some of the parameters made a big difference in the results, and decided to restructure our scoring to be able to tune its parameters. This represents the first such use of the Nuggeteer software that we are aware of, and yielded excellent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach and Results</head><p>We discuss our approaches and results for each system component, from preliminary steps including document retrieval and question analysis, to the final systems that answered each kind of question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis</head><p>We used the START natural language question answering system <ref type="bibr" coords="1,223.60,622.06,64.40,10.48" target="#b3">(Katz, 1988;</ref><ref type="bibr" coords="1,293.49,622.06,27.95,10.48;1,103.18,636.50,27.96,10.48" target="#b4">Katz, 1997)</ref> for question analysis. For each question, START identifies a question focus or answer type, and transforms the question into an assertion. In our evaluations last year, these had accuracies near 52% and 63% respectively, with correct resolution 1 http://csail.mit.edu/âˆ¼gremio/code/Nuggeteer of all referent phrases occurring in 32% of questions. Our system this year added only bugfixes.</p><p>We notice that some of the question sets this year used entirely or almost entirely explicit references to the topic. When the exact list of such questions is published, we look forward to comparing our overall performance on the resolved and unresolved subsets.</p><p>Question analysis affects list questions primarily by specifying the expected answer type. The list processor then decides whether each candidate string is of that type, as described in Section 2.4. For factoid questions, the second output of question analysis, the question transformed into assertion, is used to match the environments that candidate answers appear in. Question analysis can affect 'other' questions by identifying the portions of the topic as being synonyms (as in the case of abbreviations) or else by separating nominal parts from actions for events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Retrieval</head><p>We used Lucene<ref type="foot" coords="1,420.30,606.33,4.23,6.99" target="#foot_0">2</ref> for indexing and retrieval on the AQUAINT2 corpus, and BLOG06 corpus, though we chose not to incorporate results from the BLOG corpus in most of our systems. Unlike in previous years, question analysis did not affect document retrieval, because we did not try any query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Factoid Question Answering</head><p>Our factoid question answering component, Aranea, remained much the same system as has been used in previous years. The minimum changes made, reflected in our csail1 entry, included adapting specialized extractors to recent changes in the web site layouts from which they gathered data, and updating the IDFs to reflect the new AQUAINT2 corpus. We did not use the BLOG corpus for factoid answering.</p><p>We had also noticed that we would have benefitted in previous years from disregarding one of the Aranea components, and we ran this ablated system as csail2. For this data set, the component turned out to help by 2% (p &lt; 0.05).</p><p>In the third entry, we made the csail2 scoring modification, and we also experimented with an aggressive filtering mechanism based on answer type. We got six of the 16 NIL answers correct, compared to zero in the other runs. While this raised our score, the final score was not different from either the immediate csail2 baseline or the cross-year csail1 baseline (both p &gt; 0.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">List Question Answering</head><p>Our baseline list question answering strategy was the same as in previous years: we look for phrases which match the expected answer type, and are near question keywords or their synonyms <ref type="bibr" coords="2,205.00,576.47,65.42,10.48;2,52.16,590.92,26.01,10.48" target="#b2">(Katz et al., 2006)</ref>. The csail2 list system was the same system as last year's, with new question analysis and bugfixes. For our csail1 entry this year, we added Wikipedia categories<ref type="foot" coords="2,81.49,647.09,4.23,6.99" target="#foot_1">3</ref> to the set of answer types with lists of known members.</p><p>The csail1 entry, using Wikipedia, performed better than the baseline by 1.4%, but not statistically significantly. It influ-enced 18 questions, and certainly enabled us to answer some questions we would otherwise not have been able to answer, such as about names of ships. In a pairwise ttest on those 18 questions, however, the score improvement remained shy of statistical significance (0.050 &lt; p &lt; 0.053).</p><p>We did not use the BLOG corpus in answering list questions, but excluding BLOG data did not significantly affect our score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">"Other" Question Answering</head><p>Our system for answering "Other" questions, seeking new information about the topic of the question series, closely follows previous year's systems, but adds major infrastructural improvements to response ranking, and redefines and improves our ability to project descriptions from trusted sources onto the collection of interest.</p><p>The system has a candidate generation and a candidate ranking phase. Candidate generation aims to include all relevant nuggets among the available response snippets, and candidate ranking is responsible for selecting the most salient snippets and for minimizing redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Candidate Generation</head><p>In candidate generation, response snippets can come from two sources: direct keyword search and pre-extracted definitional contexts. Documents retrieved through direct keyword search are segmented into paragraphs and sentences, and both paragraphs and sentences become candidate snippets.</p><p>Pre-extracted definitions were generated using our ColForbin <ref type="bibr" coords="2,392.13,642.61,90.86,10.48" target="#b1">(Fernandes, 2004)</ref> system, which uses manually defined surface syntactic patterns to extract definitional text snippets from documents annotated with BBN IdentiFinder named entity tags <ref type="bibr" coords="2,342.52,714.84,106.82,10.48" target="#b0">(Bikel et al., 1999)</ref>. Examples of extraction patterns include: copula that describe is-relationships: "[Named-Entity: Ur] is [Definition: the home of capital of Sumeria]"; appositives that describe additional facts about the target entity: "[Named-Entity: Archimedes], [Definition: a philosopher and a mathematician], was celebrated in Ancient Greece". We identified approximately 1.25 million definitional snippets from the AQUAINT2 corpus, and did not run ColForbin on the BLOG06 corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Candidate Ranking:</head><p>Projection Projection is the process of finding an answer in one source and then locating support for it in another. In the case of 'other' questions, the intuition is that others have put effort into compiling the most 'vital' nuggets of information about many topics in sources like Wikipedia and Google Timelines 4 , so we would like to find the same nuggets in our corpus of interest. We looked for similarity between the trusted sources and the candidate snippets using the BLEU similarity metric <ref type="bibr" coords="3,256.44,448.53,65.01,10.48;3,103.18,462.98,45.52,10.48" target="#b6">(Papineni et al., 2001)</ref>.</p><p>For Wikipedia, we used as "trusted text" the contents of the top Wikipedia page in a Google search on the topic (as opposed to just the first paragraph, as in previous years). For Google Timelines, we used the text from the first page of responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Candidate Ranking:</head><p>Redundancy For redundancy elimination, we have experimented with a word-based editdistance metric to prune near-identical responses, and with complete-link hierarchical agglomerative clustering (both described in <ref type="bibr" coords="3,156.95,678.17,91.98,10.48" target="#b2">(Katz et al., 2006)</ref>). These were 4 Google Timelines is a recent Google Labs' product that returns a set of chronological facts for a given topic. See http://www.google.com/views? q=thomas+jefferson%20view%3Atimeline incorporated as the novelty feature, available to the final scorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Candidate Ranking: Parameters</head><p>Our ranking system uses machine learning to train parameters for the five features described in Table <ref type="table" coords="3,441.15,213.85,4.55,10.48">1</ref>. The first four features, topic match, informativeness, source, and novelty, are carried over from the old system, whereas the projection feature is new-projection had been applied by modifying the keyword weights to be used in other parts of scoring. The features are real-valued functions over the query and response snippet, and the score function combined these features into a boolean relevance judgement.</p><p>We trained scoring function weights on data from previous years using the Nuggeteer program to assign likely nuggets to each of our generated responses. Nuggeteer memorizes all human-judged responses exactly, and uses a keyword-based similarity metric with the known answers to estimate nugget judgements for previouslyunseen responses. Unlike other systems, it provides a likelihood for the presence of each nugget in each proposed response. We used TREC2006 as the training set, but did not use a validation set. We used the Weka toolkit <ref type="bibr" coords="3,492.18,570.38,64.55,10.48;3,338.46,584.83,67.99,10.48" target="#b10">(Witten and Frank, 2005)</ref> to create classifiers in several families, including support vector machines, logistic regression, radial basis function, and decision trees. We used the output of a logistic function over the binary classifiers to obtain a score between zero and one, which we then used to rank the responses. We simultaneously also computed the score using the method in our TREC2006 system, and used that score to break ties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F topic</head><p>The topic should be mentioned in each response, but may not match exactly. F topic approximates the overlap between the actual topic (or synonyms) and words in the response. If we consider the set of unique terms in the topic Q (or a synonym), and the terms in the best named entity and keyword matches in the response R, and the exactly matching set between them M , then F topic is an F-measure: F (p, r, 2), of the topic precision and recall p = wâˆˆM t(w) wâˆˆR t(w) r = wâˆˆM t(w) wâˆˆQ t(w)</p><p>Here t(w) is the term weighing function, in this case IDF.</p><p>As an example, if the topic were "Warren Moon retires" and the candidate: "Fred Moon is 52.", then F topic would be close to F(0.5,0.33,2) (modulo term weights). The third value is the beta, weighting recall as more important than precision. In a future version, we will try separating the precision and recall portions of this into two features. F inf orm IDF captures the observation that words that occur less frequently in the corpus are generally more informative. F inf orm approximates the "informativeness" of a response by comparing their combined idf score with the corpus average idf, idf avg . For a response R,</p><formula xml:id="formula_0" coords="4,85.75,458.40,298.97,61.40">F inf orm = wâˆˆR idf (w) idf avg â€¢ |w âˆˆ R| F source</formula><p>While we have only two main response sources, the responses from pre-extracted definition source can vary in quality depending on the pattern type. F source is an enumeration of the possible sources, 15 sources are from the definition extraction source e.g. def:appositive (appositive pattern), and 2 are from the lucene source (whether it was a paragraph or sentence.) F novelty</p><p>If an answer is similar to another one already given, then it is unlikely to yield previously unseen nuggets. Our approaches to novelty scoring are described in <ref type="bibr" coords="4,353.23,638.71,88.71,10.48" target="#b2">(Katz et al., 2006</ref>)</p><formula xml:id="formula_1" coords="4,85.75,653.55,46.65,11.51">F projection</formula><p>This feature quantifies the ngram distributional similarity between a response d and a set of sentences from a trusted source about the topic T (q). The trusted source could be the Wikipedia article for the given topic, or the Google Timelines result. The similarity metric we used was BLEU <ref type="bibr" coords="4,450.16,711.34,16.45,10.48;4,169.71,725.78,94.30,10.48" target="#b6">(Papineni et al., 2001)</ref>.</p><p>Table <ref type="table" coords="4,146.43,749.78,4.55,10.48">1</ref>: A summary of features for ranking candidate responses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5">Results</head><p>Our submitted responses to 'Other' questions consisted of two successful runs: csail2 included projection (on both Wikipedia and Google Timelines) but not the new scoring, and csail3 included projection and a ranking function trained using logistic regression. Our csail1 submission, meant as a baseline without either the new projection or the new scoring, unfortunately had errors that made it unusable for comparison.</p><p>In addition to the two successfully submitted runs, we also conducted several internal experiments on previous years' data.</p><p>In our experiments, we varied: 1. projection sources: Google Timelines (gtl) or Wikipedia (wiki) or both; 2. sources of candidate responses: search (luc) or preextracted definitions (db) or both. 3. methods for tuning the score function. The baseline runs consisted of best settings for TREC 2006. Evaluation was done using Nuggeteer <ref type="bibr" coords="5,159.03,425.02,134.00,10.48" target="#b5">(Marton and Radul, 2006)</ref>. Table 2 summarizes our experiments, and includes the F-measures for previous years and pyramid scores for the current year.</p><p>The projection feature alone improved performance significantly. Experiments on previous years' data showed a remarkable gain of 3-5% in F-measure (and a gain of 7-10% in recall) as compared to our baseline. Experiments varying the source of projection showed that using Wikipedia alone resulted in a better performance (17.0%) versus using Google Timelines alone (14.7%).</p><p>In some examples, we noticed that Google Timelines mix references to various items with the same name, whereas the top Wikipedia page is either on topic, or badly off topic, so the projected data will be consistent.</p><p>Experiments in varying the algorithm for optimizing the ranking function showed that simple methods such as logistic regres-sion gave significant improvements, a 2% Fmeasure gain for 2004, and a 3.7% improvement in this year's submissions (csail2 vs. csail3). The more sophisticated methods did worse, but this may be due to the decision tree's overfitting. For example decision trees are known to often overfit the training data, and indeed, examination of the generated decision trees (for TREC 2006) pointed to an overly complex (multi-level) rule, that clearly indicated overfitting as a problem.</p><p>Lastly, experiments comparing our two response sources, pre-extracted definitions (db) and Lucene search engine results (luc), showed that the Lucene source was predominant in returning valid responses. Lucene-only results provided the system with much needed answer recall. The combination of the two sources (db+luc) perform slightly better than Lucene-alone but not by any large margin. While preextracted definitions were of higher quality, its low recall, greatly diminished its usefulness as a source of descriptive responses. This confirms our analyses from last year <ref type="bibr" coords="5,385.99,465.03,87.15,10.48" target="#b2">(Katz et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Contributions</head><p>Our TREC-2007 baselines for factoid and list made minimal modifications from last year, but one cannot immediately conclude that the questions were harder, because we did not use the BLOG06 corpus. In closer analysis for list questions, excluding answers that were listed with BLOG supporting documents, our systems gained about 0.6% across the board, indicating that at least the newspaper part of the list question answering is more difficult than last year, where our scores with the same system were some 6% higher.</p><p>A common thread was that using Wikipedia in both list and 'other' questions seemed to improve our performance, and  ** the first "other" run used failed document retrieval, so the score should not be considered as a true baseline.</p><p>learning to make the best use of trusted outside sources is a future priority.</p><p>Our greatest effort and best results this year were in the 'other' questions, where a more systematic approach to parameter estimation and better use of projection from trusted descriptive sources raised our performance significantly in a traditionally difficult task.</p><p>Our effort on 'other' questions was fueled by our success with using Nuggeteer as a tool for system performance estimation and tuning. Our experiments using Nuggeteer correlated with the eventual human judgements, validating the tool and the approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,52.16,559.51,453.55,53.82"><head>Table 2 :</head><label>2</label><figDesc>Summary of TREC 2007 results for 'other' questions and experimental results on TREC 2004-2006 data. Reported are f-measure, precision, recall of responses from Nuggeteer under differing system settings. baseline used only settings from previous year.</figDesc><table coords="7,109.16,89.82,460.48,144.48"><row><cell></cell><cell>Factoid</cell><cell>Lists</cell><cell>'Other'</cell><cell>Per-Series</cell></row><row><cell cols="2">csail1 baseline</cell><cell>wiki</cell><cell>baseline</cell><cell></cell></row><row><cell></cell><cell>0.131</cell><cell>0.068</cell><cell>(0.076)**</cell><cell>0.093</cell></row><row><cell cols="2">csail2 scoring variant</cell><cell>no-wiki</cell><cell>projection</cell><cell></cell></row><row><cell></cell><cell>0.111</cell><cell>0.054</cell><cell>0.198</cell><cell>0.122</cell></row><row><cell cols="2">csail3 scoring variant and</cell><cell>wiki  â€ </cell><cell>projection and logis-</cell><cell></cell></row><row><cell></cell><cell>filtering</cell><cell></cell><cell>tic regression</cell><cell></cell></row><row><cell></cell><cell>0.119</cell><cell>0.067</cell><cell>0.235</cell><cell>0.142</cell></row><row><cell>Best</cell><cell>0.706</cell><cell>0.479</cell><cell>0.329</cell><cell>0.484</cell></row><row><cell cols="2">Median 0.131</cell><cell>0.085</cell><cell>0.118</cell><cell>0.108</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,103.18,249.81,453.54,38.80"><head>Table 3 :</head><label>3</label><figDesc>A summary of our TREC 2007 results, as compared with Best and Median values for all participants. â€  the second wiki run used a different question analysis input.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="1,355.04,731.33,70.43,7.86"><p>lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,68.75,731.33,156.97,7.86"><p>http://en.wikipedia.org/wiki/Category</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2" coords="6,59.38,619.33,416.62,8.11"><p>Runs that varied projection sources, where gtl is using Google Timelines and wiki is using Wikipedia.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3" coords="6,59.91,633.78,445.79,8.11;6,52.16,648.23,30.85,7.86"><p>Runs that varied candidate generation strategies, where db is using the ColForbin database and luc is using Lucene.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4" coords="6,59.62,662.67,446.08,7.86;6,52.16,677.12,453.54,8.11;6,52.16,691.56,274.01,7.86;6,52.16,704.24,3.43,5.24;6,59.16,706.01,192.85,8.11"><p>Runs that varied ranking methods. Training data was derived from TREC 2006 questions. Experiments used gtl and wiki as projection sources, and both db and luc as candidate generation methods. Classifiers were trained using default settings from Weka<ref type="bibr" coords="6,219.52,691.56,102.55,7.86" target="#b10">(Witten and Frank, 2005)</ref>. â€  Used both db and luc for candidate generation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,103.18,600.02,218.27,8.74;7,114.89,610.97,206.56,8.74;7,114.89,621.93,206.56,8.74;7,114.89,632.89,71.40,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,241.41,610.97,80.04,8.74;7,114.89,621.93,108.55,8.74">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,238.54,621.93,78.39,8.74">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,103.18,654.37,218.27,8.74;7,114.89,665.33,206.56,8.74;7,114.89,676.29,172.19,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,224.32,654.37,97.13,8.74;7,114.89,665.33,202.67,8.74">Answering definitional questions before they are asked. Master&apos;s thesis</title>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><forename type="middle">D</forename><surname>Fernandes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Massachussetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,103.18,697.77,218.27,8.74;7,114.89,708.73,206.56,8.74;7,114.89,719.69,206.56,8.74;7,114.89,730.65,206.56,8.74;7,350.16,328.13,206.56,8.74;7,350.16,339.09,206.57,8.74;7,350.16,350.05,72.93,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,454.15,328.13,102.58,8.74;7,350.16,339.09,101.85,8.74">Question answering experiments and resources</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mcgraw-Herdeg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalie</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Radul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Zaccak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>In Text Retrieval Conference (TREC</note>
</biblStruct>

<biblStruct coords="7,338.46,374.67,218.27,8.74;7,350.16,385.63,206.56,8.74;7,350.16,396.59,206.56,8.74;7,350.16,407.55,152.96,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,420.97,374.67,135.76,8.74;7,350.16,385.63,40.56,8.74">Using English for indexing and retrieving</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,410.89,385.63,145.84,8.74;7,350.16,396.59,206.56,8.74;7,350.16,407.55,88.64,8.74">Proceedings of the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</title>
		<meeting>the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</meeting>
		<imprint>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.46,432.18,218.26,8.74;7,350.16,443.13,206.56,8.74;7,350.16,454.09,206.56,8.74;7,350.16,465.05,178.14,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,429.58,432.18,127.14,8.74;7,350.16,443.13,126.69,8.74">Annotating the World Wide Web using natural language</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,507.07,443.13,49.66,8.74;7,350.16,454.09,206.56,8.74;7,350.16,465.05,109.59,8.74">Proceedings of the Conference on the Computer-Assisted Searching on the Internet</title>
		<meeting>the Conference on the Computer-Assisted Searching on the Internet</meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.46,489.68,218.27,8.74;7,350.16,500.64,206.56,8.74;7,350.16,511.60,204.95,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,526.01,489.68,30.72,8.74;7,350.16,500.64,206.56,8.74;7,350.16,511.60,121.66,8.74">Nuggeteer: Automatic nugget-based evaluation using descriptions and judgements</title>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Radul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,493.42,511.60,55.52,8.74">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.46,536.22,218.27,8.74;7,350.16,547.18,206.56,8.74;7,350.16,558.14,206.56,8.74;7,350.16,569.10,206.57,8.74;7,350.16,580.06,206.56,8.74;7,350.16,591.02,75.75,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,459.50,547.18,97.23,8.74;7,350.16,558.14,202.40,8.74">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,364.11,569.10,192.62,8.74;7,350.16,580.06,206.56,8.74;7,350.16,591.02,46.63,8.74">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002)</meeting>
		<imprint>
			<date type="published" when="2001-07">2001. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.46,615.64,218.27,8.74;7,350.16,626.60,206.56,8.74;7,350.16,637.56,157.28,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,350.16,626.60,206.56,8.74;7,350.16,637.56,20.78,8.74">Overview of the trec 2005 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,391.97,637.56,110.86,8.74">Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.46,662.19,218.26,8.74;7,350.16,673.15,206.56,8.74;7,350.16,684.11,50.76,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,463.48,662.19,93.23,8.74;7,350.16,673.15,126.80,8.74">Overview of the trec 2004 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,496.54,673.15,60.19,8.74;7,350.16,684.11,46.15,8.74">Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.46,708.73,218.27,8.74;7,350.16,719.69,206.56,8.74;7,350.16,730.65,39.20,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,449.92,708.73,106.80,8.74;7,350.16,719.69,71.83,8.74">Trec 2006 question answering overview</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Vorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,443.81,719.69,112.92,8.74;7,350.16,730.65,33.60,8.74">Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,52.16,90.64,218.28,8.74;8,63.86,101.59,206.56,8.74" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,212.98,90.64,57.46,8.74;8,63.86,101.59,202.29,8.74">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,63.86,112.55,206.56,8.74;8,63.86,123.51,32.66,8.74" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">San</forename><surname>Francisco</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2nd edition edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
