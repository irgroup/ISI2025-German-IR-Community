<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.42,100.95,318.96,19.61">FDUQA on TREC2007 QA Track</title>
				<funder ref="#_HJEct8f">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.98,147.05,49.62,10.04"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.19,147.05,23.92,10.04"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.02,147.05,45.86,10.04"><forename type="first">Chao</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.61,147.05,35.33,10.04"><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.91,147.05,70.28,10.04"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.27,147.05,57.82,10.04"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.42,100.95,318.96,19.61">FDUQA on TREC2007 QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C2CD89FBD23CE06E04E191B52EC260A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this year's QA track, we only participant in the main task <ref type="bibr" coords="1,411.66,251.12,47.24,10.04">[Dang 2006</ref>]. There are two changes in this year. One change is that time dependent questions are added, and the other is that the corpus is consisted by two collections with different qualities. Therefore, we need add some time limitation in answer filter and merge the answers from two different datasets.</p><p>The preprocess step is same as our system in TREC QA 2006 <ref type="bibr" coords="1,430.80,344.72,37.37,10.04;1,144.02,360.32,33.01,10.04">[Zhou et al. 2006</ref>]. We firstly index the documents for fast retrieval. The search engine used in our system is Lucene, an open source document retrieval system. We build four different indexing files. The first two are indexed based on the whole document and the single paragraph of original articles respectively. The rest two are indexed based on the whole document and single paragraph of the morphed articles. Before analyzing question, we process the questions with our question series anaphora resolution.</p><p>Our modifications mainly are done for factoid questions and definition questions. For list questions, we used the system in TREC 2006 <ref type="bibr" coords="1,165.78,516.34,75.58,10.04" target="#b7">[Zhou et al. 2006</ref>]. The only modification is that we used a natural paragraph as a unit to index instead of three sentences.</p><p>For factoid questions, we added query expansion and time filter to our system.</p><p>For definition questions, we integrate the language model and syntactic features to rank the candidate sentences, and remove the redundancies on sub-sentence level.</p><p>The rest of the paper is arranged as follows. Section 2, 3 describe our system of factoid and definition questions respectively. Section 4 presents our results in TREC 2007. At last, we give our conclusions in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Factoid Questions</head><p>The framework of our factoid component remains the same as previous years. We resort to the web as the main knowledge resource to find the answer of the question, and then project them to the new Aquaint2 corpus.</p><p>Our factoid component includes four main modifications this year: time constraint, query expansion module, a new answer ranking module and answer projecting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Time Constraint</head><p>Since Trec2007 introduced the concept of time dependency, there may be multiple answers to be extracted for one question without time constraint. If the tense of the question is present, it indicates that the event, which includes the correct answer, should occur recently. So we need find the newest answer for the question.</p><p>We analyzed the tense of the question and roughly divided question into two categories.</p><p>The first one includes the questions with present tense. Although its time constraint is not stated explicitly, the question in this category seeks for the newest answer. For example, the question is about the present chairman of some organization. For this problem, we use Google to find correct answers because Google tends to prefer the new materials or documents. For the questions like "Who is the chairman of WWF?", the name of recent chairman of WWF will appear more times than his predecessor in the return list of Google.</p><p>The second one includes the questions with time constraint. The time is stated like "in 1993". For the questions of this category, we assume that the publishing date of the support documents should not be earlier than the time stated in question sentence. Besides, it is not allowed that the time appear in support sentence is different with that in question.</p><p>For the rest questions which do not fall into the above two categories, we don't perform any additional operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Module</head><p>In the searching phase, a sequence of queries is generated from strict to loose. This strategy was also used in previous years and performed well. In this year, query expansion is added to this phase.</p><p>We use the automatic feedback relevance method in the procedure of query expansion. First, we retrieve some relevant documents from the Web via Google. Second, we extract the terms which are highly relevant to original question. The relevance is calculated by</p><formula xml:id="formula_0" coords="3,376.63,96.51,85.02,30.13">  ( &amp; ) () || i i tT i C t t rt C t t   </formula><p>, where term t is the keyword in original question, T is the collection of relevant terms of t, which is consisted of the terms around t in returned snippets by Google. We constrain that the distance between t and the relevant terms is no more than 3. The expanded queries are added to the query sequences and are used as the first query to search the web. The expanded queries can not only improve the recall of the answer, but also increase the average occurrences of the correct answers.</p><formula xml:id="formula_1" coords="3,296.46,191.49,40.24,12.22">( &amp; ) i C t t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ranking Module</head><p>In answer ranking, we use a new method to evaluate the answers candidates which are extracted from web. The score of each answer candidate is calculated as follow: s_doc is the score of documents, which is calculated by overlap of keyword and target.</p><p>s_sentence is the score of the sentence. s_path is the score to measure the distance between answer and keyword in the parsing tree. The dependency parse tree is generated by miniparser. </p><formula xml:id="formula_2" coords="3,170.16,595.56,293.31,34.55">_ ( * ) i ii w s path avg w dist    , (<label>2</label></formula><formula xml:id="formula_3" coords="3,463.47,606.61,4.26,10.04">)</formula><p>where i w  is the sum of weight of the keyword occurred in the sentence, and i dist is the distance from the candidate to keyword in the parsing tree.</p><p>The distance means how many nodes we need go through to reach the candidate from one specified keyword.</p><p>Because the answer candidate may be extracted in several sentences, the final s_path is got by calculating the average. i w is calculated from original question and</p><formula xml:id="formula_4" coords="4,244.06,143.22,41.93,16.30">1 i w  </formula><p>. Generally, the noun is given the largest weight, then verb, number and adjective. And the head word of the phrase is given larger weight than the modifiers. We tried some machine learning method to tune the parameter in equation ( <ref type="formula" coords="4,189.47,216.41,3.90,10.04">1</ref>), such as logistic regression and SVM classifier. But neither methods yield better result than empiric parameter. In our system , parameters are finally set to 0.5, 0.1, 0.1, 0.3. The occurrence score made the biggest impact and the s_path score is second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Projecting Module</head><p>Trec2007 includes two big corpuses to answer question: Aquaint2 and Blog. The projection module in our system is changed a little to adapt to the integration of these two corpuses.</p><p>Blog corpus is cleaned by simple strategy. First, we try to remove spam and advertisement links with some features. For example, these advertisement often appear as list of hyperlinks, and their html codes are such as &lt;tr&gt;&lt;a&gt;spam here&lt;/a&gt;&lt;/tr&gt;. Second, all html tags are removed. The remaining text is indexed using Lucene.</p><p>We first try to project the answer to aquaint2 corpus. Basically, we believe aquaint2 is a better resource than Blog. If one or more good support documents are found, the projection is done. Otherwise, we turn to Blog for support documents. If no support documents are found in this step, nil will be output for this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Definition Questions</head><p>For definition questions, we first obtain the candidate sentences, and then we integrate the language model and syntactic features to rank the candidate sentences, and remove the redundancies on sub-sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Sentences Generation</head><p>We use the question target as query and submit it to retrieval engine. Then we get at most 200 related documents. For each document, we check all sentences in the document with two simple rules. If no noun word of the sentence appears in the target, or the sentences have more than 70% overlap words with one of the sentences we have extracted, we abandon the sentence. In training phase, the sentences retrieved are used as train samples. In test phase, the sentences retrieved are spitted into short snippets according to the splitting regular expression "(,|-|) " and all snippets length should be more than 40. Then, we take all combination of continuous snippets as candidate answer sentences. After applying the learned ranking model, candidate answer sentences are ranked. Then we check redundancies of the candidate answer sentences in turn, and take those as the final answer if they pass the check of the redundancies conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>We use features of 3 categories, the first category is based on language models <ref type="bibr" coords="5,174.37,325.64,50.75,10.04" target="#b6">[Zhai 2004</ref><ref type="bibr" coords="5,225.12,325.64,51.59,10.04" target="#b1">, Cui 2004</ref><ref type="bibr" coords="5,276.71,325.64,51.77,10.04" target="#b2">, Cui 2005</ref><ref type="bibr" coords="5,328.49,325.64,54.10,10.04" target="#b4">, Han 2006</ref><ref type="bibr" coords="5,382.58,325.64,58.25,10.04" target="#b3">, Chen 2006</ref>], the second is based on syntax of the sentence, and the last contains only one feature, the score of the document returned from IR engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Features based on Language Models</head><p>To a candidate sentence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AQUAINT</head><p>We train the language model on the collection AQUAINT+AQUAINT2, and calculate the probability P(s| AQUAINT) of sentence s, to measure the complexity of s。</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AQUAINT*</head><p>We find that the named entities and numbers in sentence are often related to target, so we replace the person name, location name, organization name with (PRN, LCN, ORG) in the collection AQUAINT+AQUAINT2.</p><p>We also replace the number with label CD. We calculate P(s| AQUAINT*) after the same replacement process with s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition Corpus (DC)</head><p>We collect the corpus related to target from wikipedia to train the language model. We also process the named entities and numbers in sentence like AQUAINT*. Sine this corpus is small, we do Dirichlet smoothing on AQUAINT*. P(s| DC) is the probability that s is a definitional sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Corpus (TC)</head><p>We use target as queries and submit it to Google，we collect the first 100 returned snippet as target corpus. Similarly，we do Dirichlet smoothing on AQUAINT. P(s| TC) is measuring the relatedness between s and target.</p><p>Thus, we get the four features of the sentence s, log P(s| AQUAINT), log P(s| AQUAINT*), log P(s|DC), and log P(s|TC) based on language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Features based on Syntax of a Sentence</head><p>We use Minipar to analyze each sentence, and get a set of triples {w1, rel, w2}. For any relation rel-a, if there is a triple (w1, rel-a, w2), where one of w1, w2 is not stop word and appears in target, another is not in target, we define rel-a(s)=1, else rel-a(s)=0, and the relation rel-a is used as a feature. However, All relations do not help to find the correct answer. We use chi-square test to select four features, which are the punctuation "punc", the appositive "appo", the complement clause of prepositional phrase "pcomp-n" and the grammatical subject "s". To the ranked candidate answers, we check the redundancy from the top. Algorithm 1 shows the detailed process, where FA is final answer set and WP is a word pool maintained in the process, R(x,WP) is used to indicate whether x is a redundant and is calculated as R(x,WP)=1 if 70% of the words of one of the snippet of x are in the WP, and 0 else.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Removing Redundancy and Getting Final Answer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Differences among the 3 submitted results</head><p>In the run1, sentences of trec2007 targets are retrieved from Aquaint2. The difference of run1 and run2 is that, in the run2, sentence selection is based on the whole sentence and the step of removing redundancy is not used. In the run3, sentences are retrieved from Aquaint2 and BlogCorpus and the Topic Corpus (TC) in feature extraction is defined according to the target types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We submitted three runs for the main task of TREC15 QA Track: FDUQAT16A, FDUQAT16B and FDUQAT16C. From this table, we can see that we get some improvements of our factoid and other question answering systems. Moreover, the algorithm we use to answer definition questions is quite promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this year, we focus our attentions on factoid and other question, and get some improvements which mainly are derived by adding the syntactical features. However, there're still a lot of things to be improved in our question answering systems. Some more sophistic methods can be used to improve the performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,189.62,409.94,12.66,11.08;3,247.03,409.94,28.05,11.08;3,310.24,409.94,27.66,11.08;3,397.28,409.94,28.05,11.08;3,146.06,418.39,304.56,1.00;3,176.10,409.94,7.12,11.08;3,232.78,409.94,7.12,11.08;3,296.40,409.94,7.12,11.08;3,383.10,409.94,7.12,11.08;3,452.14,410.48,2.76,10.04;3,455.26,434.86,12.91,10.04;3,144.02,450.46,26.97,10.04;3,165.02,473.71,25.16,1.00;3,194.45,466.06,273.73,10.04;3,144.02,481.66,102.75,10.04"><head></head><label></label><figDesc>occur is the count of occurrences of the candidate in the returned snippets of web search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,310.85,613.32,3.52,6.34"><head></head><label></label><figDesc>2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,286.41,431.70,8.93,6.42;5,262.56,425.06,25.49,11.01;5,299.35,425.74,168.87,10.04;5,145.67,456.74,78.60,11.08;5,163.05,456.73,17.18,11.09;5,229.25,457.30,238.92,10.04;5,144.02,481.78,324.11,10.04;5,144.02,495.55,149.79,11.87"><head></head><label></label><figDesc>corpus. Here we use four corpuses: AQUAINT、processed AQUAINT(AQUAINT*)、 definition corpus (DC)和 Target corpus(TC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,144.02,544.03,61.25,10.36;6,221.09,544.03,176.07,10.36;6,144.02,561.45,169.99,10.05;6,164.54,576.61,6.37,10.98;6,145.48,576.60,18.68,10.99;6,144.02,592.80,313.40,10.05;6,146.45,607.33,150.48,11.00;6,144.02,624.00,59.90,10.05;6,144.02,639.48,120.42,10.05;6,144.02,655.08,160.49,10.05;6,144.02,671.11,23.84,9.86;6,144.02,686.82,42.19,9.86"><head>Algorithm 1</head><label>1</label><figDesc>Algorithm of Removing RedundancyInitialize a word pool WP as empty set 1 i  while length(FA ) &lt; threshold and i &lt; number of candidate sentences do x ith in the candidate sentences if R(x,WP)=0 Add all words of x into WP Take x as part of the final answer FA endif endwhile</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,145.34,382.73,320.64,135.67"><head>Table 1</head><label>1</label><figDesc>Evaluation Results of FDUQA Runs in TREC QA 2007</figDesc><table coords="7,145.34,400.78,320.64,117.62"><row><cell></cell><cell></cell><cell>FDUQAT16A</cell><cell>FDUQAT16B</cell><cell>FDUQAT16C</cell><cell>Best</cell><cell>Mean</cell><cell>Worst</cell></row><row><cell>Factoid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>0.236</cell><cell>0.228</cell><cell>0.228</cell><cell>0.706</cell><cell>0.131</cell><cell>0.019</cell></row><row><cell>Question</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>List</cell><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.107</cell><cell>0.131</cell><cell>0.101</cell><cell>0.479</cell><cell>0.085</cell><cell>0.000</cell></row><row><cell>Question</cell><cell>F score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Other</cell><cell>Average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.291</cell><cell>0.329</cell><cell>0.309</cell><cell>0.329</cell><cell>0.118</cell><cell>0.000</cell></row><row><cell>Question</cell><cell>F score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Final Score</cell><cell>0.213</cell><cell>0.231</cell><cell>0.215</cell><cell>0.484</cell><cell>0.108</cell><cell>0.015</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>This research was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">60435020</rs>. We are very thankful to <rs type="person">Zhongchao Fei</rs>, <rs type="person">Feng Ji</rs>, <rs type="person">Xiaofeng Yuan</rs>, <rs type="person">Yindong Chen</rs> and <rs type="person">Wei Pan</rs> for their contributions in our works.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HJEct8f">
					<idno type="grant-number">60435020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,150.68,201.40,91.62,14.97" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Reference</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,239.81,324.16,10.04;8,144.02,255.44,324.06,10.04;8,144.02,271.04,324.04,10.04;8,144.02,286.64,22.59,10.04" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,399.91,239.81,68.27,10.04;8,144.02,255.44,304.39,10.04">A comparative Study o Sentence Retrieval for Definitional Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,144.02,271.04,319.05,10.04">Proceedings of the 27th Annual International ACM SIGIR Conference</title>
		<meeting>the 27th Annual International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,302.24,323.98,10.04;8,144.02,317.84,324.19,10.04;8,144.02,333.44,323.89,10.04;8,144.02,349.04,323.69,10.04;8,144.02,364.64,64.23,10.04" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,322.26,302.24,145.74,10.04;8,144.02,317.84,141.21,10.04">Generic soft pattern models for definitional question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.08,317.84,159.13,10.04;8,144.02,333.44,323.89,10.04;8,144.02,349.04,166.80,10.04">SIGIR05: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="384" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,380.24,323.92,10.04;8,144.02,395.84,324.07,10.04;8,144.02,411.44,323.96,10.04;8,144.02,427.06,280.13,10.04" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,359.33,380.24,108.61,10.04;8,144.02,395.84,189.37,10.04">Reranking answers for definitional QA using language modeling</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,357.60,395.84,110.49,10.04;8,144.02,411.44,323.96,10.04;8,144.02,427.06,120.68,10.04">Proceedings of the 21st international Conference on Computational Linguistics and the 44th Annual Meeting of the ACL</title>
		<meeting>the 21st international Conference on Computational Linguistics and the 44th Annual Meeting of the ACL<address><addrLine>Morristown, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,442.66,323.84,10.04;8,144.02,458.26,324.11,10.04;8,144.02,473.86,323.89,10.04;8,144.02,489.46,323.88,10.04;8,144.02,505.06,165.99,10.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,312.51,442.66,155.34,10.04;8,144.02,458.26,82.78,10.04">Probabilistic model for definitional question answering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,245.38,458.26,222.75,10.04;8,144.02,473.86,323.89,10.04;8,144.02,489.46,40.35,10.04;8,419.80,489.46,44.20,10.04">Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval</title>
		<meeting>the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval<address><addrLine>Seattle, Washington, USA; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006-08-06">2006. August 06 -11, 2006</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;06</note>
</biblStruct>

<biblStruct coords="8,144.02,520.66,326.96,10.04;8,144.02,536.26,324.07,10.04;8,144.02,551.86,81.03,10.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,222.05,520.66,244.25,10.04">Overview of the TREC 2006 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Trang</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,158.65,536.26,304.11,10.04">Proceedings of the 15th Text Retrieval Conference (TREC-2006)</title>
		<meeting>the 15th Text Retrieval Conference (TREC-2006)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="54" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,567.46,323.80,10.04;8,144.02,583.09,324.14,10.04;8,144.02,598.69,67.83,10.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,262.15,567.46,205.67,10.04;8,144.02,583.09,170.56,10.04">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,320.57,583.09,97.69,10.04">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.02,614.29,323.92,10.04;8,144.02,629.89,323.89,10.04;8,144.02,645.49,168.51,10.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,144.02,629.89,149.45,10.04">FDUQA on TREC2006 QA Track</title>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaofeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junkuo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,301.26,629.89,166.65,10.04;8,144.02,645.49,48.63,10.04">Proceeding of the 15th Text Retrieval Conference</title>
		<meeting>eeding of the 15th Text Retrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
