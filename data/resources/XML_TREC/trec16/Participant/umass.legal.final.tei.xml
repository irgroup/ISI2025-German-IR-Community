<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,165.77,82.89,280.47,15.11">CIIR Experiments for TREC Legal 2007</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_tXG5QH7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,190.00,115.37,74.78,10.48"><forename type="first">Howard</forename><surname>Turtle</surname></persName>
							<email>turtle@cogitech.com</email>
							<affiliation key="aff0">
								<orgName type="department">Yahoo! Research</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.65,129.32,47.47,10.48;1,193.46,143.27,67.86,10.48"><roleName>WY</roleName><forename type="first">Cogitech</forename><surname>Jackson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Yahoo! Research</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.41,115.37,79.50,10.48;1,415.91,113.75,1.41,6.99"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
							<email>metzler@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Yahoo! Research</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,165.77,82.89,280.47,15.11">CIIR Experiments for TREC Legal 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AFD7D5B5AC7540492C684A9478407F48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Four baseline experiments using standard Indri retrieval facilities and simple query formulation techniques and two experiments using more advanced formulations (dependence models and pseudo-relevance feedback) are described. All of the experiments perform substantially better than the median performance of automatic runs but exhibit lower estimated precision and recall at B than the reference Boolean run.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information retrieval techniques have been widely embraced by the legal community. Large scale commercial IR systems (Lexis and Westlaw) have been in use since the 1970's. Legal retrieval systems were among the first to adopt advanced natural language and ranking techniques -West Publishing incorporated the inference network models developed at the University of Massachusetts in the early 1990's <ref type="bibr" coords="1,237.96,464.98,29.89,8.74" target="#b9">[TC90,</ref><ref type="bibr" coords="1,271.04,464.98,25.70,8.74" target="#b3">Gri92]</ref>. Lexis incorporated vector space techniques in the mid 1990's <ref type="bibr" coords="1,139.86,488.89,26.22,8.74" target="#b7">[PS95]</ref>.</p><p>Early commercial systems focused largely on case law, statutes, and other materials that were generated as part of a print publication process. Legal discovery was slower to develop because most of the historical materials of interest were in paper files and the cost of document conversion was high. By the early 1980's, though, research was underway to evaluate the effectiveness of discovery using then extant tools <ref type="bibr" coords="1,231.15,596.48,31.68,8.74" target="#b1">[BM85,</ref><ref type="bibr" coords="1,267.39,596.48,28.83,8.74" target="#b2">Dab86]</ref>. Today the historical materials of interest are increasingly electronic. As the 2006 Legal Track organizers point out the "importance of doing well at e-discovery is hard to overstate" <ref type="bibr" coords="1,246.05,644.31,34.39,8.74" target="#b0">[BLO06]</ref>.</p><p>Recall is important for legal retrieval. Not citing an important precedent or statute can seri-ously weaken an attorney's argument. Not finding a needed document in discovery can seriously weaken a case. The tools necessary to evaluate high recall retrieval are not well developed or understood. The Legal Track evaluation measures represent an attempt to better quantify recall for large test collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment Description</head><p>The TREC Legal collection used for the CIIR experiments was built using standard Indri [SMTC04] tools. A small program was written to extract text from the XML source and convert it to normal TREC format for input to the Indri build utility (IndriBuildIndex). The size of the text extracted from the 61.25 Gb XML source was 52.5 Gb. Using the extracted file the build of 6,910,192 documents took 22.3 hours on an Intel Xeon 3.0GHz box (four processors but the build software is single threaded). The collection was indexed with a three word stoplist (a, of, the) and the Krovetz stemmer.</p><p>We did not participate in the 2006 TREC Legal Track so the bulk of our work was to establish baseline performance numbers for future work. We submitted four runs using standard Indri retrieval facilities and basic query formulation and two runs to test advanced techniques (Table <ref type="table" coords="1,507.10,539.72,3.88,8.74">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline retrieval experiments</head><p>The four baseline runs submitted all use standard Indri retrieval facilities (using IndriRunQuery) and differ only in the query formulation techniques used. With the exception of UMass10 all of the runs were produced automatically from the XML topic source.</p><p>The first run (UMass11) consists of queries formed from words and phrases extracted from the RequestText topic element. Phrases were recognized using two phrase dictionaries (one con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description UMass10 Manual</head><p>Manually edited version of UMass11 queries. UMass11 Automatic Terms and phrases extracted from RequestText. UMass12 Automatic Queries generated from FinalBooleanQuery. UMass13 Automatic UMass11 and UMass12 combined. UMass14 Automatic Dependence model features extracted from RequestText. UMass15 Automatic Pseudo-relevance feedback using UMass14 for initial retrieval.</p><p>Table <ref type="table" coords="2,273.22,148.76,3.88,8.74">1</ref>: Experimental runs taining Wordnet colocations, the other a legal dictionary). Phrases were implemented using the Indri unordered word operator. Simple synonym expansion was done to expand hyphenated terms (e.g., high-phosphate expands to #syn(highphosphate #2(high phosphate))) and using a small thesaurus containing common expansions (e.g., United States expands to #syn(#1(united states) america usa)). Stop structures and common stop prefixes (e.g., "Please produce all documents") were removed. This query set also includes one date filter (topic 89). Date filters (e.g., #dateafter(12/31/1980)) were useful with the training topics (four queries used them) but were not useful with the test topics.</p><p>The second run (UMass10) is a hand-edited version of the UMass11 queries. Modifications to the queries were made to add phrases that were not recognized using the phrase dictionaries, to drop noise terms that were not recognized as stop structures, and to expand the range synonym classes. No documents were reviewed during the revision process.</p><p>UMass12 consists of queries automatically generated from the FinalBoolean query contained in the XML topic using simple syntactic translations. Quoted strings were converted to phrases. OR groupings were converted to synonym classes. Proximity operators were retained. Truncation and wildcard operations were converted to the most plausible stem form. The objective here was not to replicate the Boolean query evaluation but, rather, to capture some of the value added when human searchers created the Boolean queries (phrases, synonyms, term variations).</p><p>UMass13 consists of a weighted combination of the UMass11 queries (RequestText) and the UMass12 queries (FinalBoolean). Weights were set based on training set performance.</p><p>The four baseline query sets were run on the same four processor Xeon box (3.0 GHz) used to build the collections. Query sizes and evaluation times are shown in Table <ref type="table" coords="2,429.72,180.58,4.98,8.74" target="#tab_0">2</ref> for the case where 25,000 documents are ranked (as required by the track) and where 25 documents are ranked (closer to interactive use). Note that the query evaluation code is multithreaded and can use all four processors effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Term dependence and feedback experiments</head><p>In addition to the four baseline retrieval experiments, we performed two advanced runs that focused on term dependence and pseudo-relevance feedback. These runs are intended to improve both precision (via modeling term dependencies) and recall (via pseudo-relevance feedback). The UMass14 run uses Metzler and Croft's dependence model, which is based on a Markov Random Field (MRF) model for information retrieval <ref type="bibr" coords="2,344.65,416.89,29.65,8.74" target="#b5">[MC05]</ref>. The model captures various types of dependencies by modeling features over sets of query terms. These dependence features include single term features, phrase features, and general term proximity feature such as ordered and unordered window matching. The model has been shown to significantly improve precisionoriented measures, especially on large data sets.</p><p>The model was applied to the RequestText in the following manner. First, a set of stop words, stop prefixes, and stop phrases were applied to the text in order to remove non-informative terms. The remaining terms were then treated as the query and the sequential dependence variant of Metzler's dependence model was applied <ref type="bibr" coords="2,334.79,597.29,29.66,8.74" target="#b5">[MC05]</ref>. Using this variant, only dependencies between adjacent query terms are modeled.</p><p>For example, for topic 63, we first distill the query exclusivity clause sugar contract from the RequestText. Next, dependence model features, such as #1(exclusivity clause) (exact phrase) and #uw8(sugar contract) (unordered window of size 8), are extracted and used for ranking documents. No parameter tuning was done on the MRF model. Instead, we used parameter settings that were known to be effective in the past. Previous experiments have shown that the model is relatively insensitive across collections. Thus, our parameters are likely to be reasonable.</p><p>The UMass15 run builds upon the UMass14 run by adding an additional pseudo-relevance feedback (i.e., query expansion) step to the process. Our pseudo-relevance feedback approach, called Latent Concept Expansion (LCE), is designed to complement the MRF model and has been shown to improve recall <ref type="bibr" coords="3,219.39,325.11,31.24,8.74" target="#b6">[Met07]</ref>. The technique is similar in nature to Lavrenko and Croft's relevance models <ref type="bibr" coords="3,168.32,349.02,27.16,8.74" target="#b4">[LC01]</ref>, except the underlying probability distribution over query terms and documents is no longer treated as a bag of words multinomial model.</p><p>Although the details of the technique are beyond the scope of this paper, we provide a high level overview of how the approach works. First, an initial retrieval is done using the approach described for our UMass14 run. Then, a set of k concepts (such as terms, phrases, etc.) are extracted from the top N ranked documents. These concepts are then added to the original query. The augmented query is then evaluated to produce the final ranked list of results.</p><p>We use N = 5, k = 5, and only consider single term concepts for expansion. A more rigorous exploration of parameter values and possible expansion entities was not done due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussion</head><p>Experimental results are shown in Table <ref type="table" coords="3,272.67,622.27,3.88,8.74" target="#tab_1">3</ref>. For each of the six experimental runs we show estimated recall (Est R@B) and estimated precision (Est P@B) at the cutoff determined by the reference Boolean run (B), precision at rank 5, precision at rank 20, and bpref. Precision at rank 20 is not a reliable measure (judging was only done to a depth of 5) but it is included since it gives a rough indication of what an interactive user might see on the initial result screen.</p><p>We also show the results for the reference Boolean run (refL07B) and the median value for the 25 runs that used only the Request-Text element. Note, however that UMass12 and UMass13 are not directly comparable -UMass12 uses only the FinalBoolean query, and UMass13 uses both RequestText and FinalBoolean.</p><p>All of the UMass runs perform substantially better than the corresponding medians, they outperform the reference Boolean on normal precision and bpref measures, but they do not perform as well as the reference Boolean on estimated precision and recall at B.</p><p>The manually reviewed queries (UMass10) exhibit higher precision than the automatically produced version based on RequestText (UMass11) but slightly worse estimated recall at B. Of the baseline runs, the queries based on the combination of RequestText and FinalBoolean exhibited the best overall performance.</p><p>The two advanced runs (UMass14 and UMass15) using only RequestText performed significantly better than the corresponding baseline (UMass11) on all measures. They also performed better than the best baseline (UMass13) that used both the RequestText and FinalBoolean elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Overall, the UMass experimental runs performed well on all of the traditional IR measures. None of our runs (and, it would appear, none of the automatic runs) performed as well as the reference Boolean on estimated precision and recall at B. We don't really know how to interpret this result as yet -this marks the first use of these estimated measures.</p><p>One issue of concern with this test collection is the shallow pooling depth. It will be difficult to Est R@B Est P@B P@5 P@20 bpref The Legal Track is an important step toward a better understanding of high recall retrieval but much work remains. We need to understand and validate the new measures developed for this year. We also need to figure out how these results might find their way into a set of real world discovery tools that operate with diverse electronic collections in addition to historical OCR materials.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,193.01,54.37,225.99,103.28"><head>Table 2 :</head><label>2</label><figDesc>Query evaluation performance (46 queries)</figDesc><table coords="3,206.98,54.37,198.04,80.87"><row><cell></cell><cell cols="2">depth 25000</cell><cell cols="2">depth 25</cell></row><row><cell></cell><cell cols="4">CPU Elapsed CPU Elapsed</cell></row><row><cell></cell><cell>sec</cell><cell>sec</cell><cell>sec</cell><cell>sec</cell></row><row><cell>UMass10</cell><cell>648</cell><cell>163</cell><cell>530</cell><cell>133</cell></row><row><cell cols="2">UMass11 1211</cell><cell>305</cell><cell>979</cell><cell>246</cell></row><row><cell cols="2">UMass12 1487</cell><cell cols="2">374 1424</cell><cell>358</cell></row><row><cell cols="2">UMass13 3084</cell><cell cols="2">774 2915</cell><cell>732</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,66.72,356.38,183.48"><head>Table 3 :</head><label>3</label><figDesc>Retrieval Effectiveness (43 queries) compare results using traditional measures between this topic set (pooling depth of 5) and other TREC results (e.g., pooling depth of 100 for the 2006 Legal Track topics).</figDesc><table coords="4,165.62,66.72,280.76,92.82"><row><cell>UMass10</cell><cell>0.1310</cell><cell>0.2068 0.4884 0.2779 0.3168</cell></row><row><cell>UMass11</cell><cell>0.1367</cell><cell>0.1911 0.3767 0.2453 0.2962</cell></row><row><cell>UMass12</cell><cell>0.1502</cell><cell>0.1971 0.3442 0.2093 0.2853</cell></row><row><cell>UMass13</cell><cell>0.1618</cell><cell>0.1945 0.4372 0.2872 0.3163</cell></row><row><cell>UMass14</cell><cell>0.1472</cell><cell>0.2078 0.4605 0.3198 0.3271</cell></row><row><cell>UMass15</cell><cell>0.1650</cell><cell>0.2286 0.4605 0.3279 0.3446</cell></row><row><cell>req25 median</cell><cell>0.1078</cell><cell>0.1796 0.3535 0.2314 0.2839</cell></row><row><cell>refL07B</cell><cell>0.2158</cell><cell>0.2921 0.0326 0.0326 0.2598</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Center for Intelligent Information Retrieval</rs> and <rs type="funder">NSF</rs> grant number <rs type="grantNumber">CCF-0205575</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tXG5QH7">
					<idno type="grant-number">CCF-0205575</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,139.54,485.31,161.49,8.74;4,139.54,497.27,161.48,8.74;4,139.54,509.22,161.48,8.74;4,139.54,521.18,161.49,8.74;4,139.54,533.13,161.48,8.74;4,139.54,545.09,161.48,8.74;4,139.54,557.04,161.49,8.74;4,139.54,569.00,161.48,8.74;4,139.54,580.95,161.48,8.74;4,139.54,592.91,17.71,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,233.39,497.27,67.63,8.74;4,139.54,509.22,61.65,8.74">Trec-2006 legal track overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,284.22,521.18,16.81,8.74;4,139.54,533.13,161.48,8.74;4,139.54,545.09,113.92,8.74;4,222.18,569.00,78.84,8.74;4,139.54,580.95,140.80,8.74">The Fifteenth Text Retrieval Conference Proceedings (TREC 2006)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Elen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="500" to="272" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings available as NIST Special Publication</note>
</biblStruct>

<biblStruct coords="4,139.54,613.56,161.49,8.74;4,139.54,625.52,161.48,8.74;4,139.54,637.47,161.48,8.74;4,139.54,649.43,161.48,8.74;4,139.54,661.39,89.11,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,288.01,613.56,13.01,8.74;4,139.54,625.52,161.48,8.74;4,139.54,637.47,161.48,8.74;4,139.54,649.43,14.53,8.74">An evaluation of retrieval effectiveness for a full-text document retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,167.00,649.43,127.53,8.74">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="290" to="299" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,139.54,682.04,161.49,8.74;4,139.54,694.00,161.48,8.74;4,360.52,205.60,161.48,8.74;4,360.52,217.55,95.23,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,240.01,682.04,61.01,8.74;4,139.54,694.00,161.48,8.74;4,360.52,205.60,64.42,8.74">The curse of Thamus: An analysis of full-text doc-ument retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,431.62,205.60,85.89,8.74">Law Library Journal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="5" to="40" />
			<date type="published" when="1986">1986</date>
			<pubPlace>Winter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,360.52,236.09,161.49,8.74;4,360.52,248.04,161.48,8.74;4,360.52,260.00,161.49,8.74;4,360.52,271.95,22.70,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,433.17,236.09,88.84,8.74;4,360.52,248.04,120.82,8.74">WESTLAW&apos;s WIN: Not only natural, but new</title>
		<author>
			<persName coords=""><forename type="first">Cary</forename><surname>Griffith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,496.65,248.04,25.36,8.74;4,360.52,260.00,59.31,8.74">Information Today</title>
		<imprint>
			<date type="published" when="1992-10">October 1992</date>
			<biblScope unit="page" from="9" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,360.52,290.49,161.49,8.74;4,360.52,302.44,161.49,8.74;4,360.52,314.40,161.48,8.74;4,360.52,326.35,63.65,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,360.52,302.44,143.52,8.74">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,360.52,314.40,125.18,8.74">Proceedings of SIGIR 2001</title>
		<meeting>SIGIR 2001</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,360.52,344.89,161.49,8.74;4,360.52,356.84,161.49,8.74;4,360.52,368.80,161.48,8.74;4,360.52,380.76,149.11,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,360.52,356.84,161.49,8.74;4,360.52,368.80,78.77,8.74">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,460.66,368.80,61.34,8.74;4,360.52,380.76,50.33,8.74">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,360.52,399.29,161.48,8.74;4,360.52,411.25,161.48,8.74;4,360.52,423.20,161.48,8.74;4,360.52,435.16,73.09,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,438.80,399.29,83.20,8.74;4,360.52,411.25,157.91,8.74">Latent concept expansion using Markov random fields</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,374.29,423.20,118.20,8.74">Proceedings of CIKM 2007</title>
		<meeting>CIKM 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>page To appear</note>
</biblStruct>

<biblStruct coords="4,360.52,453.69,161.48,8.74;4,360.52,465.65,161.48,8.74;4,360.52,477.60,161.48,8.74;4,360.52,489.56,60.74,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,473.82,453.69,48.18,8.74;4,360.52,465.65,161.48,8.74;4,360.52,477.60,57.04,8.74">Comparing natural language retrieval: WIN &amp; FREESTYLE</title>
		<author>
			<persName coords=""><forename type="first">Teresa</forename><surname>Pritchard-Schoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,434.23,477.60,36.06,8.74">ONLINE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="83" to="87" />
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,360.52,508.09,161.49,8.74;4,360.52,520.05,161.48,8.74;4,360.52,532.00,161.49,8.74;4,360.52,543.96,161.49,8.74;4,360.52,555.91,161.48,8.74;4,360.52,567.87,154.85,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,509.54,520.05,12.45,8.74;4,360.52,532.00,161.49,8.74;4,360.52,543.96,117.26,8.74">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,502.89,543.96,19.11,8.74;4,360.52,555.91,161.48,8.74;4,360.52,567.87,124.48,8.74">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,360.52,586.40,161.49,8.74;4,360.52,598.36,161.48,8.74;4,360.52,610.31,30.20,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="4,360.52,598.36,161.48,8.74;4,360.52,610.31,26.42,8.74">Inference networks for document retrieval</title>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,406.71,610.31,115.29,8.74;4,360.52,622.27,132.36,8.74;4,492.88,620.69,7.68,6.12;4,508.99,622.27,13.01,8.74;4,360.52,634.22,161.48,8.74;4,360.52,646.18,161.48,8.74;4,360.52,658.13,161.48,8.74;4,360.52,670.09,161.48,8.74;4,360.52,682.04,161.47,8.74;4,360.52,694.00,133.06,8.74" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">In Jean-Luc</forename><surname>Vidick</surname></persName>
		</author>
		<title level="m" coord="4,388.08,622.27,104.80,8.74;4,492.88,620.69,7.68,6.12;4,508.99,622.27,13.01,8.74;4,360.52,634.22,161.48,8.74;4,360.52,646.18,161.48,8.74;4,360.52,658.13,26.37,8.74;4,443.58,670.09,78.43,8.74;4,360.52,682.04,70.26,8.74">Proceedings of the 13 th International Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jones</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Willett</surname></persName>
		</editor>
		<meeting>the 13 th International Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990-09">September 1990. 1997</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
	<note>Readings in Information Retrieval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
