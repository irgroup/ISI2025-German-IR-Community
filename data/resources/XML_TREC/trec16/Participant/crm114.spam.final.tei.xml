<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.10,56.76,323.87,22.21;1,230.70,78.96,154.84,22.22">Three NonBayesian Methods of Spam Filtration: CRM114 at TREC 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,259.80,117.45,77.73,19.25"><forename type="first">Mamoru</forename><surname>Kato</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mitsubishi Electric Information Technology Center</orgName>
								<address>
									<settlement>Ofuna</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.60,136.65,104.10,19.25"><forename type="first">Joseph</forename><surname>Langeway</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Southern Connecticut State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.00,155.95,55.33,19.25"><forename type="first">Yimin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Massachusetts at Amherst</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.90,175.15,150.49,19.25"><roleName>PhD</roleName><forename type="first">William</forename><forename type="middle">S</forename><surname>Yerazunis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.10,56.76,323.87,22.21;1,230.70,78.96,154.84,22.22">Three NonBayesian Methods of Spam Filtration: CRM114 at TREC 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F366155E59305580ADBFE8FEC530AD22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the TREC 2007 conference, the CRM114 team considered three nonBayesian methods of spam filtration in the CRM114 framework -an SVM based on the "hyperspace" feature==document paradigm, a bitentropy matcher, and substring compression based on LZ77. As a calibration yardstick, we used the welltested and widely used CRM114 OSB markov random field system (basically unchanged since 2003). The results show that the SVM has a spamfiltering accuracy of about a factor of two to three better accuracy than the OSB system, that substring compression is somewhat more accurate than OSB, and that bit entropy is somewhat less accurate for the TREC 2007 test sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction:</head><p>CRM114 is an opensource, GPLed language framework that makes it easy to construct arbitrary text modification and classification engines. The current version of CRM114 contains standard Bayesian classifiers, classifiers based on word parsing to create a Markov Random Field, orthogonal sparse bigram random fields (OSB) optionally enhanced with the EDDC confidence factor), Littlestone's "Winnow" algorithm (a linear perceptron), a Knearestneighbor ("Hyperspace") nonlinear classifier, a linear SVM operating in the KNN space, a bitentropy classifier, a classifier based on LempelZiv compression, and a threelayer neural network classifier with backpropagation training.</p><p>The TREC Spam Track allows teams to enter up to four spam classifiers for formal testing against the secret NIST database. For TREC 2007, we chose to formally test three of the new classifiers and retain our "standard yardstick" of the 2003 OSB classifier as the fourth test. The three new classifiers selected for testing were the SVM operating in a onefeatureperdocument space, an enhanced bit entropy classifier (with a shortrange "precognition" system enabled in the codec), and a classifier based on the LZ77 text compression algorithm (Lempel and <ref type="bibr" coords="2,356.50,105.64,45.05,16.29" target="#b4">Ziv, 1977)</ref>.</p><p>Additionally, we requested an informal run of a fourth experimental classifier based on a threelayer neural network with backpropagation, however the results of that run were not available at press time for this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Corpora:</head><p>The filters were tested against a battery of spam/nonspam corpora. For 2007, there were two basic corpora -a "public" test set with nonspam email from the ENRON corpus and recent spam, and a private (secret) corpus based on the 2007 emails of a Mr. X (believed to be a college professor but otherwise Mr. X's identity is a secret; the emails in the Mr. X corpus are never released). The public corpus was submitted in four different forms:</p><p>1. an immediate form, which is an idealized case where each of the documents is classified in time sequence, and then the filter is given the correct answer and allowed to retrain itself on that document before going on to the next document; 2. a delayed form, where the filter is not given the correct answer until some other messages are classified (and possibly trained) first; 3. a partial form, where some subset of the messages are not allowed to be trained; 4. an active form, where the filter is given access to all of the message texts, and is allowed to ask for the correct types of a small subset of the messages and must then classify the other messages based on just those subset</p><p>The MrX corpus was tested with only the "immediate" and "delayed" form. We did not build a version for "active" testing, so we have no results in that area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Classifiers:</head><p>We tested four classifiers in the "official" runs. All four classifiers were tested in a SSTTT format (singlesided thick threshold training). In SSTTT training, the classifier runs against the text and returns a result. If the result is incorrect, the text is trained in. Additionally, if the result is correct, but not judged correct by a sufficiently large margin, then the text is trained in as well.</p><p>All four classifiers were trained in this way. No classifier received "negative" or "doublesided" training (although it might be considered that the twoclass C SVM implementation obtained negative training because calculation of the ideal hyperplane and support vector set requires knowledge of both classes. The CRM114 SVM implementation is a single twoclass SVM, not a series of 1class SVMs.)</p><p>Classifier Details OSB: Our benchmark "control" classifier is the CRM114 OSB classifier which was introduced in 2003 and is substantially unchanged since 2004. We use the OSB classifier as a well known benchmark to make an interyear comparison of the difficulty of each new test set.</p><p>The basic concepts of Markov OSB classification is to break the text down into words (where a word is defined as a set of printing characters separated by whitespace characters) then to assemble short pseudophrases, by skipping zero to three words, from consecutive wordsets. For example, the text "Gentlemen! You can't fight in here! This is the War Room!"</p><p>will break down into the feature pseudophrases: and so on (the &lt;skip&gt; tokens are significant, as is case and word order; the features "You can't" and "can't You" are distinct. There is no stop word removal, and no preloaded dictionary; everything is learned from zero.</p><formula xml:id="formula_0" coords="3,92.20,291.48,92.40,21.15">Gentlemen! You Gentlemen!</formula><p>Once these features are created, they are hashed and a lookup is done; from the ratio of times each feature is seen in the spam and nonspam corpora, a probability is calculated (heavily biased towards 0.5 especially when the number of seen occurrences is small) and the probabilities are combined with a normalization rule such as Bayes law. Overall, the mathematical structure formed from the known feature pseudophrases is a Markov random field and the text stream steps across that manifold; the combining rule measures how well the given text is modeled by the field. In prior work by other researchers, the typical "feature" in the SVM was an individual word -typically this meant that the most common words only were used in the SVM. Instead of this, we performed the SVM using entire documents as single features; thus the output hyperplane of the CRM114 SVM is a weighting of total documents rather than single words. <ref type="bibr" coords="4,322.20,56.74,12.26,16.29" target="#b1">[1]</ref> In more detail, the CRM114 SVM classifier performs the OSB feature creation as seen above, and then keeps the entire document's hash set as a single very large feature; this is the same behavior and code as the CRM114 "Hyperspace" Knearestneighbor classifier. To calculate the distance between any two such documents, we simply count the number of shared pseudophrase hashes, which is the number of shared pseudophrases as described above under OSB. This is the "dot" function we use in the CSVM.</p><p>Solving for the optimal SVM weighting of these features is done on a pairwise basis; pairs of points are selected and the hyperplane weights are adjusted to maximise the error margin between them. The process repeats until convergence. The result is a set of a few documents in each class whose dot product weights are nonzero; these define the separation hyperplane.</p><p>Unlike the prior "Hyperspace" KNN code that is completely nonlinear, the current SVM code produces a linear classifier. Because the SVM calculation determines which support vectors (that is, documents) are significant and which are not significant, the SVM code is capable of forgetting documents that are insignificant and likely to remain insignificant.</p><p>In more detail, we implemented one type of SVMs called CSupport Vector Classification (CSVC). The dual formulation of CSVC is to find min 0.5 ( T Q ) -e T subject to y T = 0 y i = +1 or 1 0 &lt;= i &lt;= C, i=1,...,sizeof(corpus).</p><p>where "e" is the vector of all ones, Q is the sizeof(corpus) by sizeof(corpus) matrix containing the calculated distances between any two documents (that is, Qij = yi * yj * kernel(xi, xj) which may be intractably large and so we only calculate part of it at any one time ( xi is the feature vector of document i). The decision function is sgn (sum(yi * i * kernel(xi, xj) + b)</p><p>In order to solve the above quadratic optimization problem, we used SMOtype decomposition method proposed by Fan et al. <ref type="bibr" coords="4,168.60,664.94,12.26,16.29" target="#b3">[2]</ref> in which each iteration chooses two elements in the vector based on second order information and only updates these two elements instead of the whole vector. But for some email corpuses, this method will choose the same element in several times and then converge very slowly. So in our implementation, for every element in we set a upper bound to stop it from being chosen for update too many times.</p><p>In our experiments, we tried different kernel functions on TREC 2006 : linear kernels, polynomial kernels, and a radial basis function (RBF). Our testing shows that linear kernel performs better than the other two.</p><p>Compared with other classification methods, one advantage of SVM is that the hyperplane is only determined by a small number of support vectors ( only a few elements in vector aren't zero). In another words, for a new email we can quickly determine whether it is spam or not by calculating the decision function. The second advantage of SVM is that it can handle unbalanced datasets in which different classes have very different training sizes because the algorithm does not directly try to minimize the error rate within classes but try to separate data in a higher dimension space.</p><p>Classifier Details: BitEntropy: The bitentropy classifier is a bitatatime compression scheme.</p><p>This classifier is rather unique in that not only doesn't it have any concept of a text string; it doesn't even have any concept of a byte. During training, the serialized bits of incoming known text are used to generate a transition lattice that is later used to optimally compress the unknown text. Because the compressed text is not actually used, this classifier only calculates the absolute entropy of the result; the transition lattice that produces the best compression is the one that represents the unknown text most closely and hence was produced by members of the unknown text's class.</p><p>To produce a compression lattice rather than a simple directed tree, we provide the ability to crosslink during the construction of the transition lattice. Specifically, if we are about to add a node, but a node with a similar prior transition history already exists, we do not allocate a new node, but instead route the current transition path to the old node. This allows our compression lattice to have arbitrary loops and cycles as required to describe the text documents.</p><p>As an improvement to this, we also check the tentative new node's future paths; our current system looks a minimum at 20 bits of past history and three bits of future (hence one could consider the compression coder is "three bits precognitive").</p><p>One advantage of this method of generating the compression lattice "on the fly" is that the amount of memory needed is quite small compared to other methods; we typically run with only one million bit nodes total and find that more than adequate for the TREC public corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier Details: Substring Compression:</head><p>The fourth CRM114 classifier tested in TREC2007 is the Fast Substring Compression Match ( FSCM ) classifier. This classifier is based on the LempelZiv compression algorithm <ref type="bibr" coords="5,170.40,677.44,12.16,16.29" target="#b4">[3]</ref> commonly known as LZ77 . In the CRM114 implementation, the priortext window is greatly expanded (with a default of one megabyte for the TREC runs), a minimum compressible string length of three characters in succession, and some additional data structures are used to accellerate the processing, but in general the algorithm is unchanged.</p><p>For scoring, the document is deflated and for each substring match that would cause LZ77 to record a repeated substring, the number of bytes that would be compressed out is raised to the 1.5 th power and added to the score (the 1.5 value is empirically determined). Futher, we do not do any Huffman coding on the unrepeated substrings, as that would reflect only individual character frequencies rather than stringtostring similarities in the texts. Greater overall score implies greater overall similarity between the known and unknown texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The main TREC results can be summarized in the following table giving overall error rate and 1 ROCA% value and confidence inter(1ROCA% is the area not under the ROC curve expressed as a percentage.: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,56.70,609.31,496.04,16.57;3,56.70,625.84,489.16,16.29;3,56.70,642.14,120.86,16.29"><head>Classifier</head><label></label><figDesc>Details: SVM : The SVM classifier is a CSVM based on the wellknown work of Boser et al 1992 and Cortes and Vapnik 1995). We use an SMO type decomposition [ Fan, Chen, and Lin 2005] to select the working set.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion:</head><p>We were pleasantly surprised at the good showing by the SVM classifier; by any measure we see it performs better than any other CRM114 classifier on the TREC corpora. ( 1ROCAs of 0.05 and 0.01 versus OSB at 0.12 and 0.13, and 1/3 fewer total errors. Interestingly, the SVM is working against the very same data representation and format as the weakperforming Hyperspace <ref type="bibr" coords="7,443.10,154.74,23.24,16.29">KNN</ref>  Bit Entropy was a losing proposition at TREC 2007. The addition of "precognition" did not seem to improve performance in either ROC terms or in absolute error rate over the nonprecognitive version in TREC 2006 (performance actually decreased from the TREC 2006 numbers, but not by a statistically significant amount). We will continue to consider algorithmic improvements to bit entropy in the future, and for problem corpora that do not contain blankdelimited words.</p><p>Finally, the FSCM (Fast Substring Compression Matching, via LZ77) did surprisingly well. In the Public Full corpus, it beat OSB by a strongly statistically significant amount (the error bars do not overlap); in the private Mr. X corpus, it did better than OSB but not by a statistically significant amount.</p><p>The results also as some surprises, such as some inversions between raw error rates and 1ROCA (the likecolored pairs above. In particular, note the results of the MrX Full corpus for bit entropy and OSB versus substring compression: if one measures by 1ROCA, compression is eight times better, but measuring by actual error count shows that it's almost twice worse; their confidence intervals don't even overlap.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,56.70,677.54,495.32,16.29;6,56.70,693.84,396.16,16.29;7,56.70,562.28,62.96,16.52" xml:id="b0">
	<monogr>
		<title level="m" coord="6,56.70,677.54,495.32,16.29;6,56.70,693.84,31.08,16.29">To our mild surprise, we found that the SVM did the best overall in all testing regimes (bold in the table above)</title>
		<imprint/>
	</monogr>
	<note>in two cases, well within the error bars to be &quot;best overall&quot; for TREC 2007. References</note>
</biblStruct>

<biblStruct coords="7,72.00,595.14,151.36,16.29" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,226.40,595.14,299.72,16.29" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m" coord="7,335.60,595.20,101.04,16.23">Pattern Classification</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="259" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.40,627.75,453.50,17.77;7,56.70,645.55,434.74,17.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,308.20,627.81,218.70,17.70;7,56.70,645.61,143.65,17.70">Working set selection using second order information for training SVM</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,210.80,645.55,203.28,17.77">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18891918</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.40,681.15,474.64,17.77;7,56.70,698.95,378.57,17.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,249.60,681.15,292.27,17.77">A Universal Algorithm for Sequential Data Compression</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,56.70,698.95,215.99,17.77">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">337343</biblScope>
			<date type="published" when="1977-05">May 1977</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
