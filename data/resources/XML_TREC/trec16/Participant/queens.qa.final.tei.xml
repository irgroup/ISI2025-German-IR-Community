<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.04,75.94,409.83,14.36">Testing an Entity Ranking Function for English Factoid QA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.16,111.65,56.96,10.80"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Queens College</orgName>
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.53,111.65,45.32,10.80"><forename type="first">N</forename><surname>Dinstl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Queens College</orgName>
								<orgName type="institution">City University of New York</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.04,75.94,409.83,14.36">Testing an Entity Ranking Function for English Factoid QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AECB5F871AFED34700B1FBBA5079BD7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Much progress has been made in the English question-answering task since it was initiated in TREC-8 and our last participation in TREC-2001. QA is a complex semantics-oriented task, and it is necessary that much linguistic processing, auxiliary resources, and learning steps are needed to come up with adequate performance to the task <ref type="bibr" coords="1,112.90,277.01,12.78,10.80" target="#b0">[1]</ref>. Chinese language factoid QA was first introduced during NTCIR-5 and -6 <ref type="bibr" coords="1,499.15,277.01,13.76,10.80" target="#b2">[2,</ref><ref type="bibr" coords="1,512.91,277.01,9.18,10.80" target="#b3">3]</ref> and in which we participated. Because Chinese QA was new with little training data and Chinese linguistic tools and resources were not as readily available as in English, we employed a simple answer ranking technique that depends only on surface term usage statistics in questions and document sentences <ref type="bibr" coords="1,320.16,332.09,13.13,10.80" target="#b4">[4,</ref><ref type="bibr" coords="1,333.29,332.09,8.75,10.80" target="#b5">5]</ref>. The outcome has been surprisingly satisfactory, achieving results ~80% of the best <ref type="bibr" coords="1,320.34,345.89,13.01,10.80" target="#b2">[2,</ref><ref type="bibr" coords="1,333.35,345.89,8.67,10.80" target="#b3">3]</ref>. We were curious how this approach may perform for English factoid questions, comparing with median result for example. The program was modified to support English in this TREC-2007 QA task. As in Chinese, we made little use of linguistic tools or training data, and no auxiliary resources. It can form a basis result from which more sophisticated tools may enhance.</p><p>The Sections 2-5 describes our question classification, document processing and retrieval, entity extraction and answer ranker respectively. Section 6 shows our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Classification</head><p>Each question was analyzed via a rule-based pattern-matching procedure as to what it wants as an answer, and assigned a question type. This procedure has been used in our English-Chinese CLQA system <ref type="bibr" coords="1,272.17,525.29,12.87,10.80" target="#b4">[4]</ref>, and further updated based on questions from TREC 2006. This classification procedure provides 8 basic types (person, location, organization, date, time, money, percent and number). Questions needing acronym definition (acrn), or entities with titles (art) (such as artistic entities: titles of books, films, operas, shows, etc.) as answers were also identified. We do not have sub-categories within each class. Questions that are not successful for the above 10 categories are assigned an 'unknown' type, and these include all the 70 'Other' questions. The 85 'List' questions got assigned types but are not considered here. Table <ref type="table" coords="2,352.76,74.57,6.00,10.80" target="#tab_0">1</ref> shows that out of 515-155 = 360 factoid questions, 69 (~19%) failed to be assigned a type. We have not analyzed how accurate the classification is for the other ~81% that a type was assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Processing &amp; Retrieval</head><p>There were two different document streams from which to discover answers for TREC 2007 QA: one from the newswire-oriented AQUAINT-2 collections, and the other from blog data. Only the AQUAINT collection was processed in-house via our PIRCS retrieval system to return the top 100 documents for each question. This set of AQUAINT documents were merged with the NIST-supplied top 50 blog pages for each question. These documents were split into sentence units. A second retrieval was done to rank the top d sentences for each question. We have submitted results with retrieval depth d = 30, 50 and 70 sentences.</p><p>QA questions came as sets, each with a 'target' (topic) phrase. We added the topic phrase to each question within a set to form queries for retrieval. Only initial retrieval lists were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entity Extraction</head><p>The d top document sentences were processed by BBN's IdentiFinder <ref type="bibr" coords="2,488.72,364.37,14.08,10.80" target="#b6">[6]</ref> for entity extraction. This system can only provide entities of the first seven types (Section 2). We wrote some pattern-based routines to extract the other three types: numbers, acronym, and entities with titles. We consider the last category as those strings that are enclosed within special symbol pairs such as: {``,''}, {`,'}, {","}, etc. (Care is taken to remove those that occur close to utterance wordings like: 'said', 'saying', etc.). Extracted entities are tagged with their types and sentence source, and form an entity pool. Frequencies of repeated entries are also captured. From this pool, we attempt to extract the one correct answer for each question using a ranking function described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ranker for Answer Extraction</head><p>Typically, out of the top n (e.g. 50) sentences for each question, there can be a hundred or more different candidate entities. If a question has a type, the number of candidate entities for that type is substantially reduced, perhaps to half a dozen or less. It is still a challenge to pick one candidate as the correct answer. Most QA approaches face this problem, and they may use various evidential clues from syntax, semantics or logic, and sources such as the Wikipedia or WordNet, etc. Our simple ranker employs only surface term usage statistics of the question and top-ranked document sentences.</p><p>For each question, all extracted entities are collected into a pool. They are then ranked by the following formula where each factor (except W w ) defaults to 1 when it has no influence <ref type="bibr" coords="2,152.49,667.97,13.12,10.80" target="#b5">[5]</ref>:</p><formula xml:id="formula_0" coords="2,126.00,681.77,147.97,11.50">W = W c * W w * W f * W p * W s</formula><p>W c is a category score. When the question type and answer type agrees (except {art} which is less certain), the highest score Wc1 = 200 is assigned. Lesser values Wc2 = 50 are given when they do not agree but both belong to one of the entity groups: {person, location, organization}, {time, date}, {money, percent, number}; Wc3 = 10 when they belong to {art, unk}; and Wc4 = 2 if answer type is {person, location, organization} but question type is {unk} (i.e. these three types are more prevalent even in the failed types).</p><p>W w concerns whether an answer string appears in the question text or not and has Boolean value 0 or 1. We assume that normally answers do not appear in a question.</p><p>W f is the frequency (f) evidence of a candidate. If it repeatedly occurs and confirms itself in the top-ranked sentences, we give it a higher score of the form: W f = 1 + a f * log f, where a f is a constant (1/3).</p><p>W p is a proximity score for an answer candidate based on the matching question content terms and their positions in a document sentence. For each question content term (word or sequence of adjacent words) present in the sentence, it contributes a score: n/distance_to_candidate where n denotes the number of words in a term: W p =1+a p *Σ jε{matching terms} n j /distance j , a p is a constant (1/4), and distances are measured both before or after a candidate.</p><p>W s = is a similarity score based on matching terms between question and sentence as well as the sentence retrieval rank r: W s =1+a s * [Σ i=1..5 m i *log(n i ] ] / log(sentence-length) / r 2 . m i counts the number of term matching of length n i .</p><p>When evaluating W w , W p , W s scores, one counts matching between words. In English, although words are well delimited by blanks, there are issues of whether one should keep stop-words or not, use original word spelling or stems, keep their case or use one case. For the purpose of raising matching instances at the expense of precision, we discard stop-words, (Porter) stem all words, and generally use only one case.</p><p>We did not attempt to identify questions with NIL answers. Since our ranker provides candidates in order, we answer 'List' questions by providing candidates as answers from maximum score (Wmax) to low until a cut-off W &lt; 0.75*Wmax. For 'Other' questions, we provide additional candidates beside the top ranked answer. These additional ones come from all questions of a set and satisfy the following conditions: they are ranked 2 nd or 3 rd , has W &gt;= 0.75*Wmax in their respective question, and are unique among answers of a question set. Uniqueness is measured by surface term matching only; we did not use WordNet for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>We submitted 3 runs with the following characteristics: pircT07qa1: retrieval depth d = 50, one case matching, 0.75 cut-off pircT07qa2: retrieval depth d = 30, one case matching, 0.75 cut-off pircT07qa3: retrieval depth d = 70, mixed case matching, 0.75 cut-off.</p><p>Their factoid only results are tabulated in Table <ref type="table" coords="4,337.21,74.57,6.00,10.80" target="#tab_1">2</ref> below. They all have quite similar effectiveness. In general, there is a substantial number of inexact answers (X); the number of locally relevant (L) and unsupported (U) answers are fewer. On closer analysis of pircT07qa2 results, it showed that slightly more than ½ of the inexact answers are due to the return of incomplete names rather than full names that have been extracted successfully (such as only the surname of a person, or a popular shortened name of an entity). This we believe can be corrected. It turns out that for the parameters of this run, d=30 provides the best result in the range of 10 to 70. Figs.1 shows the pircT07qa2 effectiveness of our ranking function with single factors, and cumulatively. It shows that, with our current set-up, proximity is most effective followed by question classification when used singly. Our question classification accuracy is probably low (81% at best). The cumulative plot shows the synergistic nature of employing multiple factors simultaneously. These effects have been observed previously <ref type="bibr" coords="4,189.83,366.29,13.91,10.80" target="#b4">[4,</ref><ref type="bibr" coords="4,203.73,366.29,9.27,10.80" target="#b5">5]</ref> for Chinese QA. Fig. <ref type="figure" coords="4,145.90,672.05,4.97,10.80" target="#fig_1">2</ref> plots the effectiveness when one evidence factor is missing for ranking. It shows how one evidence source contributes in the presence of other factors. Proximity is still the most important and leads to a decrease from 47 right answers to 27 when it is not used. Surprisingly, the in-question-text factor W w is as important as the question classification W c . Apparently, many entities extracted appear also in the question, and this W w factor helps to eliminate many non-candidates that are ranked high because of other factors. For TREC-2006 QA task as comparison, our procedure produces the factoid ^results of Table <ref type="table" coords="5,175.30,453.29,4.50,10.80" target="#tab_2">3</ref>. TREC-2006 median factoid accuracy was .181 from all participants. Our simple approach returns a factoid accuracy of only .1315 which is ~27% worse than median. For TREC-2007, the median is at .131 much worse than that of TREC-2006. Our TREC-2006 experiments perform similarly to TREC-2007 and blog data does not seem to have much influence on our simple approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,232.08,398.81,149.14,9.93;4,182.40,412.13,248.21,9.93"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: TREC-2007 Factoid QA Effect of Different/Cummulative Ranking Factors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,238.92,139.04,134.08,8.97;5,215.88,150.92,180.01,8.97"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: TREC-2007 Factoid QA Effect of Missing 1 Feature for Ranking</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,90.00,622.37,431.92,53.52"><head>Table 1 : Question Classification</head><label>1</label><figDesc></figDesc><table coords="1,90.00,622.37,431.92,25.08"><row><cell cols="11">Type: per loc org dat tim mony prct num acrn Art unk List Other</cell></row><row><cell>#</cell><cell>71</cell><cell>39 32 61 1</cell><cell>8</cell><cell>4</cell><cell>65</cell><cell>2</cell><cell>8</cell><cell>69</cell><cell>85</cell><cell>70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,199.49,427.06,81.36"><head>Table 2 : TREC-2007 Factoid Result</head><label>2</label><figDesc></figDesc><table coords="4,90.00,199.49,427.06,53.16"><row><cell>-2007</cell><cell cols="3">pircT07qa1 (d=50) pircT07qa2 (d=30) pircT07qa3 (d=70,</cell></row><row><cell>(360 questions)</cell><cell></cell><cell></cell><cell>mixed case)</cell></row><row><cell>Factoid Accuracy</cell><cell>.128</cell><cell>.131</cell><cell>.128</cell></row><row><cell>R/L/X/U/W</cell><cell>46/5/19/7/283</cell><cell>47/4/17/7/285</cell><cell>46/5/21/8/280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.00,536.81,427.06,81.36"><head>Table 3 : TREC-2006 Factoid Result</head><label>3</label><figDesc></figDesc><table coords="5,90.00,536.81,427.06,53.16"><row><cell>-2006</cell><cell cols="3">pircT06qa1 (d=50) pircT06qa2 (d=30) pircT06qa3 (d=70,</cell></row><row><cell>(403 questions)</cell><cell></cell><cell></cell><cell>mixed case)</cell></row><row><cell>Factoid Accuracy</cell><cell>.132</cell><cell>.127</cell><cell>.124</cell></row><row><cell>R/L/X/U/W</cell><cell>53/0/12/5/333</cell><cell>51/0/11/5/336</cell><cell>50/0/11/5/337</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,112.50,676.01,409.66,10.80;5,126.00,689.81,396.08,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,277.56,676.01,244.61,10.80;5,126.00,689.81,25.78,10.80">Overview of the TREC 2006 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J &amp;</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,185.07,689.81,200.88,10.80">The Fifteenth Text REtrieval Conference</title>
		<meeting><address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct coords="6,126.00,74.57,395.96,10.80;6,126.00,88.37,108.27,10.80" xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://trec.nist.gov/pubs/trec15/t15_proceedings.html" />
	</analytic>
	<monogr>
		<title level="j" coord="6,126.00,74.57,138.48,10.80">NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="issue">272</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.50,102.17,409.65,10.80;6,126.00,115.97,395.86,10.80;6,126.00,129.77,395.98,10.80;6,126.00,143.57,229.49,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,358.52,102.17,163.63,10.80;6,126.00,115.97,229.27,10.80">Overview of the NTCIR-5 Cross-Lingual Question Answering Task (CLQA1)</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K-H &amp;</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings5/toc.html#CLQA" />
	</analytic>
	<monogr>
		<title level="m" coord="6,392.39,115.97,129.47,10.80;6,126.00,129.77,95.65,10.80">Proc of the Fifth NTCIR Workshop Meeting</title>
		<meeting>of the Fifth NTCIR Workshop Meeting<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.50,157.37,409.67,10.80;6,126.00,171.17,396.01,10.80;6,126.00,184.97,396.04,10.80;6,126.00,198.77,237.29,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,355.32,157.37,166.85,10.80;6,126.00,171.17,203.90,10.80">Overview of the NTCIR-6 Crosslingual Question Answering (CLQA) Task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C-J</forename></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings6/NTCIR/index.html#CLQA" />
	</analytic>
	<monogr>
		<title level="m" coord="6,354.95,171.17,167.06,10.80;6,126.00,184.97,37.48,10.80">Proc. of the 6th NTCIR Workshop Meeting</title>
		<meeting>of the 6th NTCIR Workshop Meeting<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.50,212.57,409.29,10.80;6,126.00,226.37,396.14,10.80;6,126.00,240.17,249.96,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,253.77,212.57,268.02,10.80;6,126.00,226.37,220.52,10.80">Chinese Question-Answering: Comparing Monolingual with English-Chinese Cross-Lingual Results</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,357.18,226.37,164.96,10.80;6,126.00,240.17,101.22,10.80">Proc. of Third Asia Information Retrieval Symposium</title>
		<meeting>of Third Asia Information Retrieval Symposium<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="244" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.50,253.97,409.37,10.80;6,126.00,267.77,395.88,10.80;6,126.00,281.57,396.19,10.80;6,126.00,295.37,399.97,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,302.29,253.97,219.58,10.80;6,126.00,267.77,343.67,10.80">NTCIR-6 Monolingual Chinese and English-Chinese Cross-Lingual Question Answering Experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings6/NTCIR/index.html#CLQA" />
	</analytic>
	<monogr>
		<title level="m" coord="6,496.84,267.77,25.04,10.80;6,126.00,281.57,202.76,10.80">Proc. of the Sixth NTCIR Workshop Meeting</title>
		<meeting>of the Sixth NTCIR Workshop Meeting<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="191" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.50,309.17,409.42,10.80;6,126.00,322.97,395.95,10.80;6,126.00,336.77,84.84,10.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,417.66,309.17,104.27,10.80;6,126.00,322.97,112.23,10.80">A High-Performance Learning Name-Finder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,272.22,322.97,249.73,10.80;6,126.00,336.77,49.87,10.80">Proc. Conference of Applied Natural Language Processing</title>
		<meeting>Conference of Applied Natural Language essing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
