<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,192.30,110.02,210.67,13.59">UAlbany&apos;s ILQUA at TREC2007</title>
				<funder ref="#_aaBFwzs">
					<orgName type="full">Disruptive Technology Office</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,86.52,245.29,48.05,10.50"><surname>Overview</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILS</orgName>
								<orgName type="institution" key="instit2">University at Albany SUNY</orgName>
								<address>
									<addrLine>1400 Washington Ave. Albany</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,154.50,153.79,37.77,10.50"><forename type="first">Min</forename><surname>Wu</surname></persName>
							<email>minwu@cs.albany.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILS</orgName>
								<orgName type="institution" key="instit2">University at Albany SUNY</orgName>
								<address>
									<addrLine>1400 Washington Ave. Albany</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.18,153.79,51.61,10.50"><forename type="first">Chen</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILS</orgName>
								<orgName type="institution" key="instit2">University at Albany SUNY</orgName>
								<address>
									<addrLine>1400 Washington Ave. Albany</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.13,153.79,41.47,10.50"><forename type="first">Yu</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILS</orgName>
								<orgName type="institution" key="instit2">University at Albany SUNY</orgName>
								<address>
									<addrLine>1400 Washington Ave. Albany</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.20,153.79,102.77,10.50;1,436.92,151.06,1.94,6.98"><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILS</orgName>
								<orgName type="institution" key="instit2">University at Albany SUNY</orgName>
								<address>
									<addrLine>1400 Washington Ave. Albany</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,192.30,110.02,210.67,13.59">UAlbany&apos;s ILQUA at TREC2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">144263BD2B39770F783988EA8B1A33C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC2007 QA track introduced a combined collection of 175GB BLOG data and 2.5GB newswire data. This introduced an additional challenge for an automatic QA system to processes data in different formats without sacrificing the accuracy.</p><p>In ILQUA we added a data preprocessing component to filter out noisy blog data.</p><p>ILQUA has been built as an IE-driven QA system; it extracts answers from documents annotated with named entity tags. Answer extraction methods applied are surface text pattern matching, n-gram proximity search and syntactic dependency matching. The answer patterns used in ILQUA are automatically summarized by a supervised learning system and represented in form of regular expressions which contain multiple question terms. In addition to surface text pattern matching, we also adopt N-gram proximity search and syntactic dependency matching. N-grams of question terms are matched around every named entity in the candidate passages and a list of named entities are extracted as answer candidate. These named entities then go through a multi-level syntactic dependency matching component until a final answer is chosen. This year, we modified the component that tackles "Other" questions and applied different method in the two runs we submitted. One method utilized representative words and syntactic patterns, while the other method utilized representative words from TREC data and web data.</p><p>Figure <ref type="figure" coords="1,115.11,637.09,5.34,9.31">1</ref> gives an illustration of components, data flow and control flow of ILQUA. The following sections give detailed discussion of each component of the system, evaluation results, conclusion and future work. â€  Also affiliated with the Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland 2 Data Preprocessing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BLOG Data Cleaning</head><p>The TREC2007 QA BLOG data contain three types of files: permalinks, RSS feeds and blog homepages. Indexing, searching and analysis are performed on permalink documents collection because permalinks contain the actual content of the posts. As is well known, blog-data can be often noisy and messy due to the diversity of the post sources and many formats used; therefore, some cleaning and filtering is necessary in order to obtain a reduced representation, which is much smaller in volume, yet maintains the overall integrity of the original data.</p><p>Data cleaning is performed on two levels: document-level cleaning and corpus-level cleaning. At the document-level cleaning markup tags such as HTML tags, XML tags and stylesheets which don't contribute to the actual content at all, are removed. We used HTMLParser to perform this task. At the corpus-level cleaning non-English BLOG documents are removed from the corpus. We used LUCENE built-in language analyzer to classify BLOG pages into two sets (English and non-English), and then removed the non-English set from the corpus.</p><p>After the cleaning and filtering, the BLOG data is reduced to approximately 13 GBytes (or less that 10% of the original size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Named Entity Tagging</head><p>As we mentioned above, ILQUA is an IE-driven QA system and answer extraction is performed on named entity tagged text snippets. The data corpus is first tagged with NE tagger (we use BBN's Iden-tiFinder). It was a very time-consuming task to annotate the whole 15 GB TREC2007 data corpus.</p><p>Question analysis component consists of four modules: syntactic chunking, answer target classification, question type categorization and query generation.</p><p>Syntactic chunking splits question into a list of question terms with syntactic tags. For example, the question "When were the first postage stamps issued in the United States" will be chunked with the syntactic structure of "When_Be_NP_VP_NP". However, some questions with special answer patterns do not follow thispattern, e.g., "Born_When", "Born_Where", "Die_When", "Die_Where", "Abbreviation" etc.</p><p>The answer targets for questions are classified into named entity types that are expected in the answer. The number of named entity types that ILQUA can process is 27. The following lists all the named entity types that our system can currently process. ILQUA classifies answer target with the aid of pre-defined rules and dictionary. For example, if questions begin with "When", "Where" and "Who", the answer targets are simply assigned as "Date", "Location" and "Person"; if questions begin with pattern "How+Adj.", answer targets are assigned according to the adjectives; if questions begin with "What_Be", "What_NP", "Which_Be", "Which_NP", the key term of noun phrase is mapped to appropriate answer target type. We built a dictionary with around 8000 entries to map noun phrase patterns to named entity types which can be processed by ILQUA. The mapped named entity type is set as the major answer target type and the specific noun phrase is set as the minor answer target type. For example, if the major answer target is "Quantity", the minor answer target could be "age", "distance", "height", "speed" etc. This twolevel answer target categorization is helpful to answer validation.</p><p>Query expansion is done with the aid of Word-Net to find the morphological forms and synonyms of verbs. We didn't use the noun synonyms to expand the query because the noise introduced by some noun synonyms would reduce the precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Passage Retrieval and Filtering</head><p>We used two different IR engines this year. For BLOG document indexing and searching, we used Lucene. For newswire data indexing and searching, we used Inquery (by University of Massachusetts at Amherst). The top 100 BLOG documents retrieved by Lucene and the top 100 newswire documents retrieved by Inquery are segmented into passages. Then these passages are filtered by answer target type. Passages without named entity of answer target type are filtered out. All the NE tags in the passages except the tags of answer target are filtered out for later processing. The remaining passages are again filtered by question terms and topic terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answer Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Surface Text Pattern Matching</head><p>Surface text pattern matching has been adopted by some researchers <ref type="bibr" coords="3,161.35,574.76,124.37,9.31" target="#b13">(Ravichandran &amp; Hovy 2002</ref><ref type="bibr" coords="3,285.72,574.76,4.82,9.31;3,70.02,587.00,67.04,9.31">, Soubbotin 2002</ref>) in building QA system during the last few years. Although surface text pattern matching is a simple method, it is very effective and accurate to answering specific types of questions.</p><p>Patterns used in ILQUA are represented as regular expressions with terms of "NP", "VP", "VPN", "ADVP", "be", "in", "of", "on", "by", "at", "which", "when", "where", "who", ",", "-", "(" etc. Some questions contains more than one noun phrase, we number these noun phrases according to their orders in the questions. The following regular expression list is a sample of answer patterns to question type "when_do_np1_vp_np2". These patterns were automatically mined from web and organized by question type. We used previous TREC QA questions and answers as sample question-answer pairs. The details of answer pattern mining process have been explained in our previous TREC QA reports.</p><p>When applying these patterns to specific question, the terms such as "NP", "VP", "VPN", "ADVP" and "be" should be replaced with the corresponding question terms. The replaced patterns can be matched directly to the candidate passages and answer candidate be extracted quickly with Java tools. The number of patterns varies by specific question type. Some question type has up to several hundred patterns. Only patterns with score greater than some empirically determined threshold are applied in pattern matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">N-gram Syntactic Dependency Matching</head><p>Proximity search as an IR method has been used in QA before <ref type="bibr" coords="3,370.65,554.83,130.04,9.31" target="#b7">(Han, Chung and Kim 2004)</ref>. We applied a combined method of n-gram proximity search and syntactic dependency matching to process questions whose answer cannot be extracted by surface text pattern matching.</p><p>Around every named entity in the candidate passages, question terms as well as topic terms are matched as n-grams. Each term is tokenized by word. We matched the longest possible sequence of tokenized word within the 100-word sliding window around the named entity. Once a sequence is matched, the corresponding tokens are removed from the token list and the same searching and matching is repeated until the token list is empty or no sequence can be matched. The candidate named entity is scored by the average weighted distance score of matched question terms and topic terms.</p><p>Let Num(t i ...t j ) denote the number of all matched n-grams, d(E, t i ...t j ) denote the word distance between the named entity and the matched n-gram, W1(t i ...t j ) denote the topic weight of the matched ngram, and W2(t i ...t j ) denote the length weight of the matched n-gram. If t i ...t j contains topic terms or question verb phrase, 0.5 is assigned to W1, otherwise 1.0 is assigned to W1. The value assigned to length weight W2 is determined by Î», the ratio value of matched n-gram length to question term length. How to assign the value of W2 is illustrated as follows. </p><formula xml:id="formula_0" coords="4,106.38,413.07,122.76,100.59">âˆ‘ Ã— = N QTerm E D E S N i i âˆ‘ = ) , ( ) (</formula><p>After the n-gram proximity search generates a list of named entities as answer candidates, the syntactic dependency matching component takes the top 20 ones as input and set the final answer.</p><p>Here we use a simple example to illustrate how syntactic dependency works. Figure <ref type="figure" coords="4,235.66,590.23,5.34,9.31">5</ref> shows the top 5 answer candidates (underlined dates) of question "When was the first Burger King restaurant opened?" after the n-gram proximity search. We used MINIPAR (DeKang Lin) to parse the question and the sentences containing the answer candidates. Then, we matched the syntactic relation triples of the question one by one against the triples of parsed sentences. With the aid of dependency-based word similarity list (developed by DeKang Lin), we can match synonyms or highly related words between question and candidate sentence. To improve the matching accuracy, we introduced the forward matching propagation and the backward matching propagation. If there are two syntactic relations A:R1:B and B:R2:C in the question, suppose A:R:B is not matched against any relations in the parsed sentence, the forward propagation will consider the relation A:R1:C or A:R2:C. Suppose A:R1:B is matched with the parsed sentence and B:R2:C is not matched with any relations in the parsed sentences, the matching score of A:R1:B will be adjusted according to the rule of backward propagation.</p><p>As for the example question mentioned above, the parsed dependency relation triples are listed as follows:</p><p>When was the first Burger King restaurant opened?</p><p>1. The number of Burger King fast-food restaurants have reached 100 throughout Turkey since first was opened in 1995, reported the Anatolia News Agency on Sunday. 2. Coke has supplied Burger King for most of the restaurant chain's history, starting with the first Burger King that opened in Miami in 1954.</p><p>3. ``The company chose an available Pillsbury pancake mix brand name, Hungry Jack's, in its place. . . . Burger King opened its first four company-owned restaurants (under the Burger King name) in Sydney, New South Wales . . . in December 1997.'' 4. why some BK look-alike restaurants in Australia are named Hungry Jack's while others bear the Burger King moniker. ``The Burger King brand name was not available for use by Burger King Corp. in 1971,'' BK says. ``The company chose an available Pillsbury pancake mix brand name, Hungry Jack's, in its place. . . . Burger King opened its first four company-owned restaurants ..... 5. When the recall was first announced Dec. 27, Burger King placed an ad in USA Today, posted signs in its restaurants and sent out notices to 56,000 pediatricians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5 Answer Candidates</head><p>There are two main syntactic dependency relations: the first relation is about "when" and "open" and the second relation is about "open" and "the first Burger King restaurant". These two relations are then matched with the relations in parsed sentences in the Figure <ref type="figure" coords="5,159.17,286.99,4.03,9.31">5</ref>. In the first round of syntactic matching, answer candidates "1995", "1954" and "December 1997" are matched. In the second round of matching, answer candidate "1954" get higher score because "the first Burger King" is more close to the question term "the first Burger King restaurant". So the final answer will be "1954".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Answer Validation</head><p>The goal of answer validation is to make sure the answer is returned as correct format and the answer matches both the major answer target type and minor answer target type. For example, for questions asking for "What year", the answer will be returned as year format; for questions asking for "distance", the answer should be a quantity value and contain terms such as "miles", "meters" etc.; for questions asking for "how much money", the answer should be a quantity value and contain terms of currency names such as "dollar", "pound" etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">"Other" Questions</head><p>In previous TREC QA, ILQUA used syntactic patterns with semantic features to extract answer nuggets to definition ("Other") questions. This year, we integrated relative words analysis in order to improve the precision of the answers.</p><p>Firstly, candidate sentences selection is very necessary to reduce the huge volume of relevant information about each topic. We set a threshold of 100 sentences. We utilized the relevant passages retrieved from previous factoid and list questions. For each factoid/list question, we collect passages from the top 30 relevant newswire documents and top 30 relevant BLOG documents. These passages are split into sentences and sentences are thus scored by their relevance to topic. Among the final ranked sentence list, the top 100 ones are saved as later use.</p><p>Next we retrieved representative words from the top 100 sentences. In each sentence, we set a 60word sliding window around topic phrases and select the most frequently occurring words among that frame. In addition, we also collect representative words from the web.</p><p>The candidate sentences are parsed and the parse trees are traversed bottom-up. The content at each level in the parsed tree are evaluated and assigned a score according to the following formula: Here S* is the sum score of every syntactic unit at each parsed tree level. For example, if at some level the syntactic parsed tree pattern is "NP JJS NN NNS", then S* is the sum of scores for "NP", "JJS", "NN" and "NNS". To be more specific, S topic is calculated if a topic exists in the text content of the syntactic unit; S digit is calculated if digits are contained in the text content; S rep is calculated if a representative word occurred in text; and S adj is calculated if there are adjective phrases present in text. L pattern is the number of syntactic units and L content is the number of words in text snippets. L opt is a constant value of 64 and Î± is an optional value that is adjusted by experiments.</p><p>All the scored nuggets are sorted and ranked. Finally we chose the top 30 nuggets as the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments and Evaluation Results</head><p>Table <ref type="table" coords="5,343.52,617.06,5.34,9.31" target="#tab_3">1</ref> shows the evaluation results of two submitted runs. The factoid accuracy of ILQUA1 and ILQUA2 are the same. The F-scores for list questions in ILQUA1 and ILQUA2 differ a little because the two runs use different thresholds to control number of returned answers. The pyramid scores for other questions in ILQUA1 and ILQUA2 differ in the number of representative words. ILQUA is an IE-driven QA system and has participated TREC QA task since 2003. It kept a stable performance in answering factoid, list and other questions from a data collection with mixed formats.</p><p>To improve the system performance, future development could include: increasing and refining the named entity categories; integrating semantic similarity into the proximity search; improving temporal context analysis techniques. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,102.54,131.60,399.41,89.50"><head>Table 1 ILQUA Evaluation Result</head><label>1</label><figDesc></figDesc><table coords="6,102.54,152.25,399.41,68.86"><row><cell>RUN ID</cell><cell>Factoid</cell><cell>List</cell><cell>Other</cell><cell>Average Per Series</cell></row><row><cell>ILQUA1</cell><cell>0.222</cell><cell>0.147</cell><cell>0.242</cell><cell>0.203</cell></row><row><cell>ILQUA2</cell><cell>0.222</cell><cell>0.144</cell><cell>0.216</cell><cell>0.193</cell></row><row><cell>Best</cell><cell>0.706</cell><cell>0.479</cell><cell>0.329</cell><cell>0.484</cell></row><row><cell>Median</cell><cell>0.131</cell><cell>0.085</cell><cell>0.118</cell><cell>0.108</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The development of ILQUA and our more advanced interactive QA system (HITIQA) is supported by the <rs type="funder">Disruptive Technology Office</rs>'s <rs type="programName">Advanced Question Answering for Intelligence (AQUAINT) program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aaBFwzs">
					<orgName type="program" subtype="full">Advanced Question Answering for Intelligence (AQUAINT) program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,334.50,147.33,146.90,8.50;3,314.82,158.49,168.96,8.50" xml:id="b0">
	<monogr>
		<title level="m" coord="3,334.50,147.33,71.27,8.50;3,445.81,147.33,35.59,8.50;3,314.82,158.49,93.25,8.50">&lt;\/Date&gt; NP1 VP NP2 on &lt;Date&gt;</title>
		<imprint/>
	</monogr>
	<note>VP NP2 in &lt;Date&gt;</note>
</biblStruct>

<biblStruct coords="3,329.45,169.71,178.51,8.50;3,314.82,180.87,178.23,8.50;3,314.82,192.03,183.15,8.50" xml:id="b1">
	<monogr>
		<idno>NP1 VP.{1</idno>
		<title level="m" coord="3,334.33,169.71,97.96,8.50;3,472.36,169.71,35.61,8.50;3,314.82,180.87,56.72,8.50;3,412.04,180.87,81.01,8.50;3,314.82,192.03,26.71,8.50">&lt;\/Date&gt;.{1,15}NP1 &lt;Date&gt;</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>{1,15}VP NP2 in &lt;Date&gt;</note>
</biblStruct>

<biblStruct coords="3,314.82,203.19,204.19,8.50;3,314.82,214.41,180.40,8.50" xml:id="b2">
	<monogr>
		<idno>NP1.{1</idno>
		<title level="m" coord="3,346.97,203.19,96.40,8.50;3,483.41,203.19,35.60,8.50;3,314.82,214.41,104.69,8.50">&lt;\/Date&gt; NP1.{1,15}NP2 on &lt;Date&gt;</title>
		<imprint/>
	</monogr>
	<note>15}VP.{1,30} on &lt;Date&gt;</note>
</biblStruct>

<biblStruct coords="3,329.45,225.57,165.77,8.50;3,314.82,236.73,189.64,8.50;3,314.82,247.89,157.35,8.50;3,314.82,259.11,183.02,8.50;6,70.02,505.21,54.47,10.50" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,382.55,236.73,121.91,8.50;3,314.82,247.89,81.69,8.50">&lt;\/Date&gt;.{1,15}NP1.{1,50}VP NP1&apos;s NP2 in &lt;Date&gt;</title>
		<idno>{1,30}NP2 on &lt;Date&gt;</idno>
	</analytic>
	<monogr>
		<title level="m" coord="3,459.59,225.57,35.63,8.50;3,314.82,236.73,27.09,8.50;3,436.57,247.89,35.60,8.50;3,314.82,259.11,26.98,8.50;3,382.27,259.11,115.57,8.50;6,70.02,505.21,54.47,10.50">&lt;\/Date&gt;.{1,15}NP1 VP NP2 References</title>
		<imprint/>
	</monogr>
	<note>&lt;\/Date&gt; &lt;Date&gt;</note>
</biblStruct>

<biblStruct coords="6,70.02,536.43,220.52,8.50;6,81.06,547.59,209.47,8.50;6,81.06,558.75,209.63,8.50;6,81.06,569.91,24.02,8.50;6,105.06,567.79,4.89,5.50;6,112.38,569.91,51.71,8.50" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,126.64,547.59,163.89,8.50;6,81.06,558.75,123.20,8.50">A Multi-Strategy and Multi-Source Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carrol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,230.42,558.75,60.27,8.50;6,81.06,569.91,24.02,8.50;6,105.06,567.79,4.89,5.50;6,112.38,569.91,21.86,8.50">Proceedings of the 11 th TREC</title>
		<meeting>the 11 th TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.02,586.40,220.57,9.10;6,81.06,598.17,209.54,8.50;6,81.06,609.39,209.49,8.50;6,81.06,620.49,9.73,8.50;6,90.72,618.37,4.89,5.50;6,98.10,620.49,51.67,8.50" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,276.00,586.40,14.58,9.01;6,81.06,598.17,209.54,8.50;6,81.06,609.39,108.31,8.50">National University of Singapore at the TREC 13 Question Answering Main Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,214.59,609.39,75.96,8.50;6,81.06,620.49,9.73,8.50;6,90.72,618.37,4.89,5.50;6,98.10,620.49,21.85,8.50">Proceedings of the 13 th TREC</title>
		<meeting>the 13 th TREC</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.02,637.53,220.69,8.50;6,81.06,648.69,209.59,8.50;6,81.06,659.91,209.50,8.50;6,81.06,671.07,9.73,8.50;6,90.72,668.95,4.89,5.50;6,98.10,671.07,51.67,8.50" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,199.53,648.69,91.12,8.50;6,81.06,659.91,109.59,8.50">Multiple-Engine Question Answering in TextMap</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Melz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,215.64,659.91,74.93,8.50;6,81.06,671.07,9.73,8.50;6,90.72,668.95,4.89,5.50;6,98.10,671.07,21.85,8.50">Proceedings of the 12 th TREC</title>
		<meeting>the 12 th TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.02,688.11,220.57,8.50;6,81.06,699.21,209.63,8.50;6,81.06,710.43,209.50,8.50;6,81.06,721.59,9.73,8.50;6,90.72,719.47,4.89,5.50;6,98.10,721.59,51.69,8.50" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,149.11,699.21,141.58,8.50;6,81.06,710.43,86.04,8.50">Korea University Question Answering System at TREC</title>
		<author>
			<persName coords=""><forename type="first">K.-S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-C</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,214.40,710.43,76.16,8.50;6,81.06,721.59,9.73,8.50;6,90.72,719.47,4.89,5.50;6,98.10,721.59,21.86,8.50">Proceedings of the 13 th TREC</title>
		<meeting>the 13 th TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,237.99,220.50,8.50;6,315.60,249.15,209.46,8.50;6,315.60,260.31,209.63,8.50;6,315.60,271.53,115.93,8.50;6,431.46,269.41,4.95,5.50;6,438.84,271.53,51.65,8.50" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,424.39,249.15,100.68,8.50;6,315.60,260.31,209.63,8.50;6,315.60,271.53,24.49,8.50">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,361.27,271.53,70.26,8.50;6,431.46,269.41,4.95,5.50;6,438.84,271.53,21.85,8.50">Proceedings of 12 th TREC</title>
		<meeting>12 th TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,288.51,220.63,8.50;6,315.60,299.73,209.55,8.50;6,315.60,310.83,66.21,8.50;6,381.78,308.71,4.89,5.50;6,389.16,310.83,51.59,8.50" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,338.92,299.73,146.46,8.50">Question Answering in Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In Proceedings of the 9 th TREC</note>
</biblStruct>

<biblStruct coords="6,304.56,327.87,220.55,8.50;6,315.60,339.03,209.47,8.50;6,315.60,350.25,209.53,8.50;6,315.60,361.41,209.53,8.50;6,315.60,372.57,21.85,8.50" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,388.67,327.87,136.44,8.50;6,315.60,339.03,152.32,8.50">Selectively Using Relations to Improve Precision in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,490.50,339.03,34.57,8.50;6,315.60,350.25,209.53,8.50;6,315.60,361.41,181.14,8.50">Proceedings of the EACL-2003 Workshop on Natural Language Processing for Question Answering</title>
		<meeting>the EACL-2003 Workshop on Natural Language Processing for Question Answering</meeting>
		<imprint>
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,389.55,220.49,8.50;6,315.60,400.77,209.57,8.50;6,315.60,411.93,183.45,8.50" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,418.91,389.55,106.14,8.50;6,315.60,400.77,209.57,8.50;6,315.60,411.93,40.43,8.50">Logical Form Transformation of WordNet and its Applicability to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,378.04,411.93,91.20,8.50">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,428.97,220.56,8.50;6,315.60,440.07,209.57,8.50;6,315.60,451.29,172.85,8.50" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,502.21,428.97,22.91,8.50;6,315.60,440.07,166.67,8.50">Question-Answering by Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,508.41,440.07,16.76,8.50;6,315.60,451.29,92.36,8.50">Proceedings of SIGIR 2000</title>
		<meeting>SIGIR 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,468.27,220.61,8.50;6,315.60,479.49,209.66,8.50;6,315.60,490.65,70.26,8.50;6,385.80,488.53,4.89,5.50;6,393.18,490.65,46.50,8.50" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,454.84,468.27,70.32,8.50;6,315.60,479.49,189.29,8.50">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,315.60,490.65,70.26,8.50;6,385.80,488.53,4.89,5.50;6,393.18,490.65,16.41,8.50">Proceedings of 40 th ACL</title>
		<meeting>40 th ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,507.69,220.55,8.50;6,315.60,518.79,209.61,8.50;6,315.60,530.01,80.85,8.50;6,396.30,527.89,4.89,5.50;6,403.68,530.01,51.59,8.50" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,443.77,507.69,81.34,8.50;6,315.60,518.79,200.55,8.50">Patterns of Potential Answer Expressions as Clues to the Right Answers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,326.16,530.01,70.29,8.50;6,396.30,527.89,4.89,5.50;6,403.68,530.01,21.85,8.50">Proceedings of 11 th TREC</title>
		<meeting>11 th TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,546.99,220.60,8.50;6,315.60,558.21,209.55,8.50;6,315.60,569.37,96.76,8.50" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,363.32,546.99,161.84,8.50;6,315.60,558.21,150.47,8.50">Using Question Series to Evaluate Question Answering System Effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,490.57,558.21,34.58,8.50;6,315.60,569.37,68.03,8.50">Proceedings of HLT 2005</title>
		<meeting>HLT 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,586.35,220.64,8.50;6,315.60,597.51,209.56,8.50;6,315.60,608.73,114.54,8.50" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,487.12,586.35,38.08,8.50;6,315.60,597.51,191.15,8.50">ILQUA -An IE-Driven Questioning Answering System</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,315.60,608.73,86.10,8.50">TREC-14 Proceedings</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,304.56,625.71,209.74,8.50;6,304.56,636.93,200.21,8.50;6,304.56,648.03,137.24,8.50" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="6,419.91,625.71,94.39,8.50;6,304.56,636.93,135.23,8.50">Utilizing Co-occurrence of Answers in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,457.33,636.93,47.45,8.50;6,304.56,648.03,70.00,8.50">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006-07">2006. July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
