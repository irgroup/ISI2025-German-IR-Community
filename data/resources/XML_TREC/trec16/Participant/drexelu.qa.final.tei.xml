<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.56,87.70,284.85,14.04">Drexel at TREC 2007: Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,129.96,143.36,82.30,10.80"><forename type="first">Protima</forename><surname>Banerjee</surname></persName>
							<email>protima.banerjee@drexel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Technology Drexel University Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.28,143.36,49.45,10.80"><forename type="first">Hyoil</forename><surname>Han</surname></persName>
							<email>hyoil.han@acm.org</email>
							<affiliation key="aff1">
								<orgName type="department">College of Information Science and Technology Drexel University Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.56,87.70,284.85,14.04">Drexel at TREC 2007: Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">41BE5E885D1BBA5E36BEE43AA16BF657</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Question Answering Track presented several distinct challenges to participants in 2007.</p><p>Participants were asked to create a system which discovers the answers to factoid and list questions about people, entities, organizations and events, given both blog and newswire text data sources. In addition, participants were asked to expose interesting information nuggets which exist in the data collection, which were not uncovered by the factoid or list questions. This year is the first time the Intelligent Information Processing group at Drexel has participated in the TREC Question Answering Track. As such, our goal was the development of a Question Answering system framework to which future enhancements could be made, and the construction of simple components to populate the framework. The results of our system this year were not significant; our primary accomplishment was the establishment of a baseline system which can be improved upon in 2008 and going forward.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The TREC Question Answering Track presented several distinct challenges to participants in 2007.</p><p>Participants were asked to create a system which discovers the answers to factoid and list questions, given both blog and newswire data sources. The targets for each question series could be people, entities, events or organizations, and the questions themselves varied in the types of information required to construct a correct response. Each question set ended with an "Other" question; the answer to the "Other" question is an interesting information nugget which was not previously uncovered by any of the other questions in the series. Each group was permitted to submit three runs, and no manual intervention was allowed for any results submitted.</p><p>All participating systems were required to freeze their code and configuration once the 2007 question set had been downloaded from the TREC Question Answering website. This paper describes the first time participation of Drexel University's Intelligent Information Processing group in the TREC Question Answering Track. Our primary goal this year was the development of a Question Answering system framework to which future enhancements can be applied. As a result, our results this year were not significant; our primary accomplishment was the establishment of a baseline system which can be improved upon in 2008 and going forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>Hirschman <ref type="bibr" coords="1,376.79,575.84,13.99,10.80" target="#b1">[1]</ref> describes a generic architecture for a Question Answering system, which is depicted (with slight variation) in Figure <ref type="figure" coords="1,548.83,603.44,4.50,10.80" target="#fig_0">1</ref>. According to Hirschman, five core steps exist in the QA process. They are:</p><p>1. Question Analysis, during which a question is interpreted and decomposed.  This architecture can be thought of as a two-stage model, represented in Figure <ref type="figure" coords="2,203.72,570.08,4.50,10.80" target="#fig_1">2</ref>. Here, the first stage is an Information Retrieval stage, which outputs a set of candidate (answer-bearing) documents. The second stage is then primarily an Information Extraction stage, in which answers are mined from the candidate documents. The final step in the second stage of the Question Answering process is then to formulate a response which can be returned back to the user. The remainder of this paper will describe our Question Answering architecture and framework in the context of this two-stage model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM ARCHITECTURE</head><p>In this section we will describe the components of our system architecture. We will explain these components in a generic sense, as well as their specific implementations in our system this year.</p><p>Our system architecture is shown in Figure <ref type="figure" coords="2,551.82,413.48,6.00,10.80" target="#fig_2">3</ref> below.</p><p>As previously stated, the first stage of our architecture is primarily an Information Retrieval (IR) stage. In this stage, the AQUAINT2 documents were pre-processed using stop-word removal, stemming, and indexing using tools provided as a part of the LEMUR toolkit <ref type="bibr" coords="2,544.06,514.04,13.99,10.80" target="#b2">[2]</ref> (http://www.lemurproject.org). Once the corpus pre-processing was completed, we used the Indri <ref type="bibr" coords="2,317.88,555.44,13.99,10.80" target="#b3">[3]</ref> search engine to perform document retrieval. Indri queries were created from the TREC questions and targets using Indri's querylikelihood retrieval method which is based on the Ponte and Croft language modeling approach <ref type="bibr" coords="2,543.92,610.64,13.99,10.80" target="#b4">[4]</ref> using Dirichlet priors for smoothing <ref type="bibr" coords="2,496.51,624.44,12.74,10.80" target="#b5">[5]</ref>. The top documents returned from Indri were then considered to be the set of Candidate (answerbearing) Documents. These documents were then sent on to the second Question Answering stage for further processing. We considered Candidate Documents sets in sizes of 5, 10 and year. It should be noted here that due to time and budgetary constraints we utilized only the AQUAINT2 corpus for our experiments. No Blog data was included as a part of our submission this year.  The set of Candidate Documents were sent to a Candidate Answer Extraction module. Our current implementation of this module made use of the Information Extraction (IE) resources provided by the GATE ANNIE Toolkit <ref type="bibr" coords="3,279.91,598.64,13.99,10.80">[6]</ref> (http://gate.ac.uk/ie/annie.html). ANNIE was used to extract all entities of type "Person," "Location," "Organization," "Date," "Currency," and "Percentage" from the Candidate Documents. These entities, which we considered in a generic sense to be the Raw List of Candidate Answers, were stored in an Oracle database for further processing. We then performed two types of basic processing on the Raw List of Candidate Answers. First, we eliminated duplicates using simple string matching functions provided by Oracle. Then, we grouped the answers by entity type (eg. "Person," "Location," "Organization") and ranked the entities according to their frequency of occurrence in the set of Candidate Documents. At this point, we returned to the original query to try to understand what type of entity would best answer the question that was posed. We created a simple set of heuristics based on the existence of keywords within the query such as "who," "what," "when," "where," "why," "how many" and "how much." Having reviewed the query classification scheme proposed by Lehnert <ref type="bibr" coords="3,540.94,317.24,12.74,10.80" target="#b7">[7]</ref>, we recognize that this is an area where our approach can use significant improvement. The output of our simple classification scheme was an Answer Type, which is represents the entity type that will correctly answer the question. Knowing the Answer Type and the set of Candidate Answers allowed us to implement a simple matching algorithm. Our final response to the question was the top ranked Candidate Answer of the proper Answer Type. We then went back to our Raw Answer List to determine the document from which the top-ranked Candidate Answer came. Our final response to the question consisted of the top-ranked Candidate Answer and its associated Document Number. The above approach was designed primarily with factoid questions in mind. We were able to re-use the same approach for list questions. In the case of list questions, however, we selected a set Candidate Answers to be returned as the query response. We selected this set by considering answers which ranked above a fixed frequency threshold. Our approach does not yet incorporate a mechanism to respond to "Other" questions. We are currently evaluating a methodology for modeling Question Answering Context that will solidify our approach to answering "Other" questions in 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>As might be expected, our results this year were not significant. We submitted three runs, using Candidate Document sets of 5, 10 and 25 documents, and achieved accuracy results of 0.016, 0.019, and 0.022 for these runs respectively. The accuracy results of our system were thus directly influenced by the quantity of data returned by the IR stage of our system. However, our results are far below the accuracy results for state-of-the-art systems. We expect significant improvements in these results in 2008 as we continue to experiment and enhance the components within our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In conclusion, our primary goal this year was to establish a framework for our future work in Question Answering. We started with a holistic approach to the Question Answering process, using the generic Question Answering architecture proposed by Hirschman <ref type="bibr" coords="4,249.46,390.20,13.99,10.80" target="#b1">[1]</ref> as a foundation. Over the next year, our work in Question Answering will focus on several key areas:</p><p>development and management of a Question Answering Context using statistical language modeling methods, development of robust algorithms for Candidate Answer Selection and Response Generation, and the incorporation of knowledge sources into the Question Answering process.</p><p>We plan to participate in the 2008 TREC Question Answering Track to evaluate our research against other systems and approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,104.04,516.81,139.98,8.10;2,119.40,527.25,109.22,8.10"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hirschman's Architecture for a Generic QA System [1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,324.96,282.33,226.00,8.10"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two-stage Model for a Generic QA Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,114.84,510.93,118.47,8.10"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: System Architecture</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,322.38,74.84,92.36,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,337.87,94.40,204.85,10.80;4,317.88,108.20,212.96,10.80;4,317.88,122.00,224.88,10.80;4,317.88,135.80,43.00,10.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,506.95,94.40,35.77,10.80;4,317.88,108.20,212.96,10.80;4,317.88,122.00,19.03,10.80">Natural language question answering: the view from here</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Cambridge Univ Press</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="275" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,337.87,155.60,198.23,10.80;4,317.88,169.40,201.55,10.80;4,317.88,183.20,88.31,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,476.01,155.60,60.09,10.80;4,317.88,169.40,117.95,10.80">Experiments Using the Lemur Toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,448.44,169.40,70.99,10.80;4,317.88,183.20,53.01,10.80">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,337.87,203.00,208.63,10.80;4,317.88,216.80,232.85,10.80;4,317.88,230.60,213.52,10.80;4,317.88,244.40,217.62,10.80;4,317.88,258.20,134.65,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,367.72,216.80,183.00,10.80;4,317.88,230.60,129.41,10.80">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,459.74,230.60,71.65,10.80;4,317.88,244.40,217.62,10.80;4,317.88,258.20,99.65,10.80">Proceedings of International Conference on New Methods in Intelligence Analysis</title>
		<meeting>International Conference on New Methods in Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,337.87,278.00,195.65,10.80;4,317.88,291.80,210.64,10.80;4,317.88,305.60,195.31,10.80" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,478.54,278.00,54.98,10.80;4,317.88,291.80,206.30,10.80">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>ACM Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,337.87,325.40,170.87,10.80;4,317.88,339.20,233.65,10.80;4,317.88,353.00,229.54,10.80;4,317.88,366.80,223.32,10.80;4,317.88,380.60,51.00,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,459.96,325.40,48.78,10.80;4,317.88,339.20,233.65,10.80;4,317.88,353.00,111.17,10.80">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,440.76,353.00,106.66,10.80;4,317.88,366.80,135.61,10.80">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,337.87,400.40,207.29,10.80;4,317.88,414.20,228.22,10.80;4,317.88,428.00,216.53,10.80;4,317.88,441.80,205.66,10.80" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="4,405.98,414.20,140.11,10.80;4,317.88,428.00,166.93,10.80">Evolving GATE to meet new challenges in language engineering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge Univ Press</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="349" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,337.87,461.60,178.30,10.80;4,317.88,475.40,237.95,10.80" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="4,398.52,461.60,117.65,10.80;4,317.88,475.40,50.39,10.80">The Process of Question Answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lehnert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
