<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,211.80,86.93,188.27,15.31">UALR at TREC-ENT 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,91.56,121.70,65.74,10.87"><forename type="first">Hemant</forename><surname>Joshi</surname></persName>
							<email>hemant.joshi@acxiom.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Acxiom Research</orgName>
								<address>
									<settlement>Little Rock</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,167.76,121.70,83.88,10.87;1,251.64,118.91,4.02,7.28"><forename type="first">Sithu</forename><forename type="middle">D</forename><surname>Sudarsan #</surname></persName>
							<email>sdsudarsan@ualr.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Applied Science</orgName>
								<orgName type="institution">University of Arkansas at Little Rock</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.72,121.70,138.48,10.87"><forename type="first">Subhashish</forename><surname>Duttachowdhury</surname></persName>
							<email>sxduttachowd@ualr.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Arkansas at Little Rock</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.92,121.70,77.16,10.87"><forename type="first">Chuanlei</forename><surname>Zhang</surname></persName>
							<email>cxzhang@ualr.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Applied Science</orgName>
								<orgName type="institution">University of Arkansas at Little Rock</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,497.04,121.70,23.38,10.87;1,273.00,135.50,62.16,10.87"><forename type="first">Srini</forename><surname>Ramaswamy</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Arkansas at Little Rock</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,211.80,86.93,188.27,15.31">UALR at TREC-ENT 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">60B2D1304B6DA0DB8B04968EDD832600</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the first year we participated in the enterprise track. This year's enterprise track offered completely new enterprise data and two new tasks. The data offered was the CSIRO Enterprise Research Collection corpus 1 . The two new tasks introduced this year are Expert search and Document search. We participated in both tasks, though Document Search was our primary focus this year. We also believe that the results in our document search task might have a direct impact on the expert search task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Expert search task was to identify experts or subject matter experts given a particular topic. The goal was to drive queries regarding a certain subject be diverted to a particular set of experts. Identifying experts from the document collection is a challenging problem. We have to assert if the document is informative enough for the given topic and shows the mark of an expert. We have to also find the author of the article or the relevant name or email address mentioned. The results were to be submitted as email addresses with proof of documents that we believe are expert information for the given topic. Fifty new topics were provided by NIST<ref type="foot" coords="1,236.88,422.51,4.02,7.28" target="#foot_1">2</ref> and evaluation for expert search task was conducted with help from real-world CSIRO personnel.</p><p>The document search task was to identify documents that are authoritative information about a given topic. Fifty topics were common among the document search and expert search tasks. The challenge was to determine if the document merely contained words associated with the given topic or the document was indeed the authoritative source on that topic. We had to analyze the documents relevant to the given topic and rank them according to how informative those documents are for that particular topic. We experimented with various approaches that can estimate authoritative information content contained within a document. We discuss these approaches and compare them later in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC-ENT Data</head><p>CSIRO document collection consists of 370,715 documents with unique IDs provided with the collection. Each individual document is in HTML and is in the TREC format<ref type="foot" coords="1,515.04,646.79,4.02,7.28" target="#foot_2">3</ref> .</p><p>From the CSIRO website, we gather that the enterprise is organized into flagships and divisions. There are 7 flagships and 18 divisions listed on the website that represent diverse areas of research. We identified a list of 19,073 email addresses that occur at least once in the collection and sort them in descending order of frequency of occurrence. We removed email addresses such as ento-webmaster@csiro.au which do not indicate an expert but a generic email address. We also removed email addresses that occur only once in the entire collection. Finally we have 6,754 email addresses that we believe uniquely represent an expert within the organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Search Task</head><p>We submitted 4 runs namely UALR07Ent1, UALR07Ent2, UALR07Ent3 and UALR07Ent4. UALR07Ent1 was our baseline run. We used the Indri search engine<ref type="foot" coords="2,517.92,227.03,4.02,7.28" target="#foot_3">4</ref> available as a part of Lemur Language Modeling Kit<ref type="foot" coords="2,361.08,240.83,4.02,7.28" target="#foot_4">5</ref> . We used the top 5 document pseudo feedback to boost the accuracy of the search results returned. The 50 topics were modified to follow Indri query syntax<ref type="foot" coords="2,271.80,268.43,4.02,7.28" target="#foot_5">6</ref> . For the base run, we did not use query expansion. Our objective was to experiment with various approaches to detect documents that are authoritative about the given topic and to rank them.</p><p>• UALR07Ent2: We used MMRSummApp<ref type="foot" coords="2,323.40,310.79,4.02,7.28" target="#foot_6">7</ref> (part of Lemur Applications) which is a complex summarizer that compares passages of the document with respect to the given query and summary length. We passed sample authoritative documents for each query as input to MMRSummApp. Query is important for summarization to know which sentences establish correct context of the query. We limited each summary to 20 words and added those words to the actual query (also known as topic) and re-ranked top 1,500 results of each topic from base run. Re-ranking helps us boost those results which contain most of the words from summary of sample authoritative pages provided. • UALR07Ent3: This run was driven by one question:</p><p>What is so special about documents that will make them authoritative or more informative about a particular topic?</p><p>In order to answer this question, we introduce the 'word difference' approach. We set out to find what words are in sample authoritative pages that are not in our top documents of the base run. This will tell us not to focus on common words but those special indicative words that indicate authoritative source about the particular topic. So we found words in sample authoritative pages that were not in top results for the topic. Then, we added those words to the query through manual query expansion and re-ranked 1,500 results obtained using Indri. Top 1,000 ranked documents for each query were submitted as run UALR07Ent3.</p><p>• UALR07Ent4: Unlike previous runs, this is a manual query expansion run. We manually selected and modified given topics to yield more informative documents as top results for each query. This run was submitted due to encouragement from track organizers to submit manual runs. We were also interested to see if our other two runs UALR07Ent2 and UALR07Ent3 can either match or perform better than the manual run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expert Search Task</head><p>Expert search task was not the primary task we participated in, and so, we decided to utilize our results and runs from the document search task to submit 3 runs for the expert search task.</p><p>• UALR07Exp1: We used the entire 50,000 results for the 50 topics from UALR07Ent3 run to identify potential experts for each query and filtered them against our manually created list of 6,754 email addresses. If the email address existed in our master list of expert emails, then we added and used the container document's rank to list up to 100 experts for each topic. The format also required us to submit documents that support the claim that a particular email address is an expert.</p><p>• UALR07Exp2: This run is different from UALR07Exp1 in one aspect. Instead of using all 50,000 results from all topics, we focused on each query individually and used a set of 1,000 results from UALR07Ent3 to identify experts and crossreference them against the master list of 6,754 email addresses. UALR07Exp1 run is aimed at identifying global experts but UALR07Exp2 run identifies topic specific experts. We limited the number of experts to 100 though it was suggested that potential number of experts for each run would be at best 2 or 3 for each topic.</p><p>• UALR07Exp3: This is manual run where we identified expert email addresses for each topic by analyzing top results from UALR07Ent3. We used UALR07Ent3 run as baseline for all 3 runs submitted in expert search task. Run UALR07Ent3 (discussed earlier) uses word difference approach and at the time we believed that UALR07Ent3 run would produce best results in identifying authoritative documents. So we decided to exploit that run to enhance our runs submitted for expert search task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>For document search task, only 42 topics have been completely judged and so at the time of writing this, we present results obtained in 42 out of the 50 topics. Figure <ref type="figure" coords="3,464.40,608.60,6.00,10.68">1</ref> shows the interpolated precision response comparison of 4 runs submitted for document search task. All 3 runs UALR07Ent2, UALR07Ent3 and UALR07Ent4 perform better than the baseline. Also we observe that runs UALR07Ent2 and UALR07Ent3 match or better the performance of manual run UALR07Ent4. Figure <ref type="figure" coords="3,330.84,663.68,6.00,10.68">2</ref> shows precision response comparison of the 4 runs submitted. Results from figure 2 also reiterate the same observation that word difference approach as well as summarization (MMRSummApp) approach can match human experts in identifying authoritative documents.</p><p>We did not perform well in expert search task. We anticipated poor results as we did not get enough time to focus on expert search task. We plan to improve on our expert search task results in near future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We performed extremely well in Document Search task and we were satisfied with our results. We performed better than the baseline run established and matched performance of a manual run and even performed better in early precision values. We plan to continue our research using approaches discussed earlier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,99.24,407.02,413.51,10.92"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Document Search: Comparison of interpolated precision response of 4 runs</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,98.28,687.71,88.21,9.02"><p>http://es.csiro.au/cerc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,98.28,699.11,79.86,9.02"><p>http://www.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,98.28,710.63,73.26,9.02"><p>http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,98.28,676.19,139.33,9.02"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,98.28,687.71,143.77,9.02"><p>http://www.lemurproject.org/lemur/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,98.28,699.11,206.29,9.02"><p>http://boston.lti.cs.cmu.edu/lti-search/help-qry.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="2,98.28,710.63,340.45,9.02"><p>http://www.cs.cmu.edu/~lemur/doxygen/lemur-2.0/html/MMRSummApp_8cpp.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
