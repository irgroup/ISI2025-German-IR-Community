<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.67,81.09,304.65,12.91">Indri at TREC 2007: Million Query (1MQ) Track</title>
				<funder ref="#_Jdzb7kb">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.54,131.16,39.08,10.76"><forename type="first">Xing</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Intelligent Information Retrieval</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003-4610</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.86,131.16,62.58,10.76"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Intelligent Information Retrieval</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003-4610</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.67,81.09,304.65,12.91">Indri at TREC 2007: Million Query (1MQ) Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E83923597E08E58037E863D4ADD0D7FE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work details the experiments carried out using the Indri search engine for the ad hoc retrieval task in the TREC 2007 Million Query Track. We investigate using proximity features for this task, and also explore whether using a simple spelling checker -Aspell to correct plausible spelling errors in the noisy queries could help retrieval. Results evaluated by three different approaches are presented. The strength and weakness of introducing Aspell for IR are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year a new track -Million Query (1MQ) Track was introduced for two purposes: (1) investigating which approach is better for system evaluationbuilding test collection from very many very incompletely judged topics or from traditional TREC pooling; and (2) exploring ad hoc retrieval on a large corpus. For the ad hoc retrieval task, each participant is required to submit results of running 10,000 given queries against the GOV2 corpus. Our search engine, Indri<ref type="foot" coords="1,118.36,561.90,3.99,7.17" target="#foot_0">1</ref>  <ref type="bibr" coords="1,122.85,563.85,103.27,9.82" target="#b2">(Strohman et al., 2005)</ref> was utilized for this task. As evidenced by previous Terabyte Track results <ref type="bibr" coords="1,102.91,590.95,89.59,9.82" target="#b1">(Metzler et al., 2006)</ref>, Indri is highly efficient and effective; we want to further investigate its performance with large number of queries.</p><p>In addition, because there is no quality control imposed on the 10,000 given queries, some may contain spelling errors; therefore we also utilized a simple Unix spelling checker -Aspell<ref type="foot" coords="1,223.49,670.35,3.99,7.17" target="#foot_1">2</ref> , in experiments to correct plausible spelling errors. We are interested in testing how this simple spelling check approach will work for large number of queries having typos and errors.</p><p>This paper describes our experiments in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ad Hoc Task</head><p>For the ad hoc retrieval task this year, we submitted results of four automatic official runs. Two of them utilized a spelling checker to find plausible spelling errors and give correction suggestions. We followed our previous successful approach of using proximity information in Terabyte Track <ref type="bibr" coords="1,313.20,430.41,93.11,9.82" target="#b1">(Metzler et al., 2006)</ref>, and preprocessed the GOV2 collection in a similar setting. First, we indexed the whole GOV2 collection with no special document or link structure indexed. Second, we stemmed all documents by using the Porter stemmer. Third, we did not stop documents at index time and did not stop query terms. Last, we used Bayesian smoothing and allowed single term and proximity features (i.e. #1, #uw8) to be smoothed differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline -Simple Query Likelihood</head><p>Our baseline run this year, IndriQL, is a simple titleonly query likelihood run. For example, topic 9101, "california department of motor and vechicles", is converted into the following Indri query: #combine( california department of motor and vechicles), which produces results rank-equivalent to a simple query likelihood language modeling run. We utilized Dirichlet smoothing and set µ = 1500 without tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Simple Query Likelihood + Simple Spelling check</head><p>In this run, IndriQLSC, we utilize the Unix spelling checker -Aspell to find plausible spelling errors for each topic, then combine Aspell's correction suggestions with the title to formulate a query. Given a topic's terms, if no errors are found, we formulated the same Indri query as in the IndriQL run; otherwise, the top three corrections suggestions by Aspell are weighted and combined with the original title terms to formulate a new Indri query by using the Indri operators "#weight" and "#syn". For example, given the topic 9101, Aspell finds a plausible spelling error "vechicles" and gives seven correction suggestions, vehicles, vehicle's, vesicles, chicle's, vehicle, vesicle's, versicle's. Then, the top three terms "vehicles, vehicle's, vesicles" are combined with the original title to formulate this Indri query: #weight(0.8#combine( california department of motor and vechicles) 0.2#syn( #1(vehicles) #1(vehicles) #1(vesicles))), where punctuation in suggested terms has been removed. The weight is fixed to be 0.2 for correction suggestions and 0.8 for the original title terms.</p><p>In experiments, plausible spelling errors have been found in 1865 of 10,000 topics. Dirichlet smoothing is used with µ = 1500 without tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dependence Model</head><p>In last year's Terabyte Track, we found term proximity features were very useful for the ad hoc retrieval task on large scale, noisy web collection <ref type="bibr" coords="2,261.24,535.55,37.56,9.82;2,72.00,549.10,53.70,9.82" target="#b1">(Metzler et al., 2006)</ref>. Therefore in this run, IndriDM, we keep using dependence model <ref type="bibr" coords="2,211.45,562.65,87.35,9.82;2,72.00,576.20,23.48,9.82">(Metzler and Croft, 2005)</ref>, which assumes query term order and proximity are very important for finding relevant documents. From three variants of dependence model <ref type="bibr" coords="2,72.00,616.85,117.90,9.82">(Metzler and Croft, 2005)</ref>, we have used the sequential dependence version instead of the full dependence one because some topics have too many terms (e.g. topic 653 has 23 terms), thus the full dependence model will obtain very long Indri queries which are hard to run in limited time.</p><p>To give an idea of how the sequential dependence model translates topic terms into Indri queries, we give the following example, again for topic 9101: #weight(0.8#combine( california department of motor and vechicles ) 0.1#combine( #1(and vechicles) #1(motor and) #1(of motor) #1(department of) #1(california department)) 0.1#combine( #uw8(and vechicles) #uw8(motor and) #uw8(of motor) #uw8(department of) #uw8(california department))).</p><p>In this run, Dirichlet smoothing is used with µ = 1500 for single term and µ = 4000 for proximity features without tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dependence Model + Simple Spelling check</head><p>In this run, IndriDMCSC, we utilize not only the spelling checker Aspell to find plausible spelling errors in each topic title, but also sequential dependence model to use proximity information.</p><p>Given a topic title, first use Aspell to check spelling errors. If no errors are found, use the sequential dependence model to transform the title to an Indri query same as in the IndriDM run; otherwise, each error term in Indri query obtained by the sequential model is replaced by an Indri operator "#wsyn()", which weights and combines the original error term and the top three correction suggestions by Aspell. We use topic 9101 again as the example. The error term "vechicles" and the top three suggestions (vehicles, vehicle's, vesicles) are combined to form: #wsyn(1.0 vechicles 0.2 vehicles 0.2 vehicles 0.2 vesicles), which is then used to replace every "vechicles" in the sequential dependence model Indri query, thus resulting in the final complicated Indri query: #weight(0.8#combine(california department of motor and #wsyn(1.0 vechicles 0.2 vehicles 0.2 vehicles 0.2 vesicles)) 0.1#combine(#1(and #wsyn(1.0 vechicles 0.2 vehicles 0.2 vehicles 0.2 vesicles)) #1(motor and) #1(of motor) #1(department of) #1(california department)) 0.1#combine(#uw8(and #wsyn(1.0 vechicles 0.2 vehicles 0.2 vehicles 0.2 vesicles)) #uw8(motor and) #uw8(of motor) #uw8(department of) #uw8(california department))).</p><p>In this run, Dirichlet smoothing is used with µ = 1500 for single term and µ = 4000 for proximity features without tuning. Again, plausible spelling errors are found in 1865 topics, thus IndriDMCSC and IndriDM are different in 1865 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The results from our four official runs are evaluated by three different approaches: NEU-style, UMassstyle, and using topics and relevance judgments in the previous Terabyte Track 3 (called TBTrack-style later). The corresponding mean average precision (MAP) results are given in Table <ref type="table" coords="3,216.27,284.72,4.09,9.82">1</ref>. The confidences of pairwise differences between four runs are calculated by the UMass-style evaluation, and given in Table <ref type="table" coords="3,98.69,325.37,4.09,9.82">2</ref>.</p><p>In Table <ref type="table" coords="3,125.24,339.20,4.09,9.82">1</ref>, IndriDM is the best, or the second best of four runs, by different evaluation approaches. This result shows that proximity features are useful for the ad hoc retrieval task on large scale, noisy web collection, which is consistent with our previous finding in Terabyte Track <ref type="bibr" coords="3,231.95,406.95,66.85,9.82;3,72.00,420.50,23.48,9.82" target="#b1">(Metzler et al., 2006)</ref>. However when evaluating using large number of topics, using proximity features are not significantly better than not using them: in Table <ref type="table" coords="3,290.62,447.60,4.09,9.82">2</ref>, P(IndriDM&lt;IndriQL)= 0.6104; in Table <ref type="table" coords="3,265.66,461.14,4.09,9.82">1</ref>, both the NEU-style and the UMass-style evaluations rank IndriQL&gt;IndriDM.</p><p>It can be observed in both Table <ref type="table" coords="3,228.97,502.07,5.45,9.82">1</ref> and 2 that average retrieval performances have been hurt a little when using a simple spelling checker for this task: IndriQL is better than IndriQLSC, IndriDM is better than IndriDMCSC. To show the bias of choosing topics for judging does not cause this happening, we present the number of topics that have been judged by NEU and UMass, and the number of judged topics that may contain spelling errors in Table <ref type="table" coords="3,261.63,610.47,4.09,9.82">3</ref>, which indicates Aspell did affect performances of about 16% judged topics.</p><p>To investigate the causes of this failure, we look into several specific topics listed below, in which Aspell found plausible spelling errors: The estimated average precisions by the NEUstyle evaluation of these three topics are shown in Table <ref type="table" coords="3,341.38,495.76,4.09,9.82" target="#tab_1">4</ref>. It can be seen that in Topic 169, by using spelling checker to correct hurricain to hurricane, we improve the AP drastically: IndriQLSC achieved 870% improvement, compared with In-driQL. However, as shown in Topic 133 and 863, if there are proper nouns that are very important to find relevant documents, Aspell would mistakenly attempt to correct these terms, thus decreasing the IR performance a lot, with respect to the absolute AP values. Another example is that Aspell always thinks Los Angeles misspelled, and suggests correct Los to be laos, leos, or lois. This happens in many noisy web queries, therefore the approach of simply applying Aspell on each query hurts the performance. Although Aspell works for some topics, it is an open issue how to avoid applying this spelling checker on some topics containing proper nouns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This year in the ad hoc retrieval task of Million Query Track we investigated how the Indri search engine performs with large number of queries in noisy web environments. We submitted four official runs to explore the effect of using proximity features and of using a simple spelling checker for this task. Positive results were obtained by using proximity features and dependence modeling, while the simple approach of using spelling checker to correct topic terms failed, at least in part because many topics contain proper nouns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,74.43,226.80,288.56"><head>Table 4 :</head><label>4</label><figDesc>Spelling Checkers' Impact on Estimated APs of Topics 169, 133 and 863 by the NEU-style evaluation. Bold figures show our best official run for each topic.</figDesc><table coords="4,72.00,74.43,226.80,225.05"><row><cell></cell><cell cols="3"># of Topics # of Topics</cell></row><row><cell></cell><cell></cell><cell></cell><cell>having errors</cell></row><row><cell>Overall</cell><cell cols="2">10,000</cell><cell>1865</cell></row><row><cell>only NEU Judged</cell><cell>548</cell><cell></cell><cell>80</cell></row><row><cell>only UMass Judged</cell><cell>429</cell><cell></cell><cell>73</cell></row><row><cell>Mix Judged</cell><cell>801</cell><cell></cell><cell>126</cell></row><row><cell cols="4">Table 3: Number of topics and topics having plausi-</cell></row><row><cell>ble spelling errors</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Estimated APs</cell></row><row><cell>TopicID</cell><cell>169</cell><cell>133</cell><cell>863</cell></row><row><cell>RunID</cell><cell></cell><cell></cell><cell></cell></row><row><cell>IndriQL</cell><cell cols="3">0.0072 0.2672 0.7962</cell></row><row><cell>IndriQLSC</cell><cell cols="3">0.0682 0.1253 0.4699</cell></row><row><cell>IndriDM</cell><cell cols="3">0.0012 0.0410 0.7249</cell></row><row><cell cols="4">IndriDMCSC 0.0539 0.0347 0.7238</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.14,693.04,201.23,8.07"><p>Available for download at: http://lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,88.14,703.91,210.66,8.07;1,72.00,713.88,57.53,8.07"><p>The version is Aspell 0.50.5α. Copyright is held by Kevin Atkinson, 2000.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Don Metzler</rs> and <rs type="person">Trevor Strohman</rs> for their help in producing the queries used in this evaluation. This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs> and in part by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> under contract number <rs type="grantNumber">HR0011-06-C-0023</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Jdzb7kb">
					<idno type="grant-number">HR0011-06-C-0023</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,313.20,93.84,226.81,8.97;4,324.11,104.80,215.89,8.97;4,324.11,115.76,114.83,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,464.67,93.84,75.34,8.97;4,324.11,104.80,135.74,8.97">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,480.05,104.80,59.96,8.97;4,324.11,115.76,45.32,8.97">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,313.20,135.68,226.81,8.97;4,324.11,146.64,215.89,8.97;4,324.11,157.60,195.66,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,324.11,146.64,145.43,8.97">Indri at TREC 2005: Terabyte track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,490.65,146.64,49.36,8.97;4,324.11,157.60,137.29,8.97">Proceedings of 2005 Text REtrieval Conference</title>
		<meeting>2005 Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2006. TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,313.20,177.52,226.81,8.97;4,324.11,188.48,215.90,8.97;4,324.11,199.44,215.89,8.97;4,324.11,210.40,171.92,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,353.11,188.48,186.90,8.97;4,324.11,199.44,80.31,8.97">Indri: A language model-based serach engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,427.31,199.44,112.69,8.97;4,324.11,210.40,167.90,8.97">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
