<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,232.44,67.57,130.40,13.50">WIM at TREC 2007</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,232.44,67.57,130.40,13.50">WIM at TREC 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8871E3D6D5EC4F9D7C4F930C83070667</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduced the four tracks that WIM-Lab Fudan University had taken part in at TREC 2007. For spam track, a multi-centre model was proposed considering the characteristics of spam mails in contrast of traditional 2-class classification methodology, and the incremental clustering and closeness-based classification methods were applied this year. For enterprise track, our research was mainly focused on ranking functions of experts and selecting correct supporting documents regarding to a given topic. For legal track, the effects of word distribution model in query expansion and various corpus pre-processing methods were mainly evaluated. For genomics track, three score methods were proposed to find the most relevant text snippets to a given topic. This paper gives an overview of the methods employed for each sub tasks, and compares the results of each track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For spam track, in real world there could be several different genres in the spam category. There was a great diversity within the single spam class, while at the same time they might be similar to some certain kinds of ham mails. For this reason, in addition to the traditional 2-class classification methodology, we proposed a multi-centre model and applied the incremental clustering and closeness-based classification method to the Spam Track tasks this year. We applied character-level model and online linear classifier as our basic classification method, then we adopted a multi-centre model to treat the mails that were equivocal to the basic classifier. This method was evolved from the idea once used in <ref type="bibr" coords="1,266.80,747.60,12.84,9.57" target="#b6">[7]</ref> for expanding a credible negative sample set from the unlabeled training corpus. We developed this idea to make it compatible to the spam-filtering task.</p><p>For enterprise track, we participated in both tasks for the track. A new enterprise corpus was introduced in the track this year, CSIRO repository instead of W3C repository, which makes great difference in the tasks and the topics for the tasks were provided by science communicators which are of great real meaning. However, chances always come from the challenges. Some new methods are used to accomplish the tasks.</p><p>For legal discovery track, our team submitted 5 runs finally (1 manual + 4 autos). The objective of Legal Track is to evaluate the efficacy of automated support for review and production of electronic records in the context of litigation, regulation and legislation. The corpus consists of 650 xml files, 60G+ after unzipped. Each file contains multiple documents. In the document, 61 types of xml leave nodes form all textual information. Since legal corpus is converted from OCR format by program automatically, corpus may contain lots of meaningless text blocks and latent data inconsistency, which makes it a challenge for lawyers to lookup related documents supporting their quoting. The query of each topic is give by xml format as well. Participating teams can build queries in any way they like, using materials provided in the complaint, the production request, the boolean query, and any external resources that they have available.</p><p>For genomics track, the system was required to extract the relevant passages of text that answers the topic questions <ref type="bibr" coords="1,393.85,685.21,16.79,9.57" target="#b9">[10]</ref>. It's similar to the task of Genomics Track 2006 except the question types. Our group submitted three runs based three different score models. Three methods had the same process which extracted the concepts for followed scoring process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message by message</head><p>Character-level matching Feature map updating First-layer results </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closeness based classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Spam Track</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>Many traditional spam filters regarded anti-spam challenge as a 2-class classification problem, yet there was a potential premise for us to use common 2-class classification methods to solve this problem. The premise was that samples in the same class were quite similar while samples in different classes had a distinct difference. Common 2-class classification methods could somehow extract some implicit features to predict whether a mail was ham or spam. But empirically the border between spam and ham was not so clear even for manual recognition. And there was also a great diversity within the single spam or ham class. For instance, there could be several different genres in spam, such as automated, list, newsletter, phishing, sex, virus and etc. Various kinds of spam mails were quite different to each other, while at the same time they might be similar to some certain kinds of ham mails.</p><p>For this reason, in addition to the traditional 2-class classification methodology, we proposed a multi-centre model and applied the incremental clustering and closeness-based classification method to the Spam Track tasks this year. And we tried to evaluate whether this method could make some further improvements to the precision of the spam filters.</p><p>We applied character-level model and online linear classifier as our basic classification methods, which were inspired by IJS <ref type="bibr" coords="2,171.04,560.41,12.89,9.57" target="#b4">[5]</ref> and Tufts <ref type="bibr" coords="2,234.91,560.41,12.89,9.57" target="#b5">[6]</ref> in last two years' tasks. Then we adopted a multi-centre model to treat the mails that were equivocal to the basic classifier. This method <ref type="bibr" coords="2,179.20,607.21,12.89,9.57" target="#b6">[7]</ref> was evolved from the idea once used for expanding a credible negative sample set from the unlabeled training corpus, and worked well on the TREC 2005 Genomics Corpus. We developed this idea to make it compatible to the spam-filtering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Methods</head><p>The "FDW" filter we submitted had a two-layer structure. We used character-level model and linear classifier as the first-layer filter. During the "classify" period, each mail processed through the first-layer filter would be given a spamminess to denote the likelihood to be a spam or not. Mails with a relatively extreme spamminess score would skip the second-layer filter and were put into the spam or ham class directly, while those with moderate scores would go to the next-layer filter. The second-layer filter used a multi-centre model. We applied a closeness-based text classification method on it to get an adjustment score of the mail. Finally, we integrated the scores of the two layer filters together to determine if the incoming mail was a spam.</p><p>During the "train" period, the parameters of the first-layer filter were adjusted according to the online linear classifier mechanism. We choose part of the mails by the spamminess score given by the first-level filter, and then use incremental clustering method to establish and update a multi-centre model for the second-layer filter. The details of these methods were described in the following subsections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.1.Character-Level Matching and Linear Classification</head><p>Character-level spam filters had the advantage to avoid the vulnerability of the key-word escape attacking by spammers compared with those bag-of-words filters. <ref type="bibr" coords="3,143.76,139.20,12.84,9.57" target="#b4">[5]</ref> and <ref type="bibr" coords="3,186.08,139.20,12.84,9.57" target="#b5">[6]</ref> had shown this advantage on the basis of the experimentation in last two years' Spam Track. We designed our first-layer filter referring to the inexact word matching idea by <ref type="bibr" coords="3,42.54,201.60,11.73,9.57" target="#b5">[6]</ref>, but made some changes.</p><p>The first difference was that we used a fuzzy weighting strategy to denote the similarity of the character-level matching instead of giving a binary score to each explicit string feature, since one certain string in the feature space might have tens of transformed forms by the inexact matching method, and from the empirical view most of them would have a relation with the target string but weaker than the exact matching. The related weighting method was once proposed in <ref type="bibr" coords="3,145.72,357.60,11.60,9.57" target="#b8">[9]</ref>. The method introduced a decay factorλ∈(0,1) to weight the presence of a certain feature in a text. The weight of each dimension is λn, where n is determined by the similarity between the incoming text and the string in the feature space. In our system, as we only considered one-character obfuscation as <ref type="bibr" coords="3,226.78,451.21,12.84,9.57" target="#b5">[6]</ref> did. We simplify our fuzzy weighting strategy as follows:</p><p>matching condition weighting score exact matching 1 inexact matching λ no matching 0 The second difference was that we used a fixed length of string feature space instead of limiting of the string length in an optional range. This would save some computational cost and empirically too short length of string would do little benefit but bring noise to the filter.</p><p>After the character-level feature mapping, we applied the Perceptron classifier to do the first-layer filtration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Incremental Clustering and Closeness Based Classification</head><p>There were two hypotheses for us to think of using the multi-centre model in our system.</p><p>The first hypothesis was that in one hyperspace, the spam and the ham were not distributed as two single clusters. By practical experience, we found that there were a variety of types of mails in the spam category in the real world. Each type of mails had their own characteristics and was quite different from each other type. So we suggested a model that spams were represented in scattered clusters while hams still in one single cluster. Later we would evaluate if this model would be more reasonable to represent the spam and ham distribution by experiments.</p><p>The second hypothesis was that for a practical spam filter, a low hm% misclassification rate was more important than sm%. So a misty mail would be judged spam only when it was a credibly spam sample in the multi-centre model.</p><p>For these two hypotheses, we applied incremental clustering and closeness based classification methods to our system.</p><p>The similar methods were originally proposed by <ref type="bibr" coords="3,332.31,342.01,11.64,9.57" target="#b6">[7]</ref>, and had produced effective results for the classification task on TREC 2005 Genomics Corpus. The scenario of the method was that there was a corpus with some pre-labeled credible positive samples and only a few credible negative samples extracted by the Rocchio classifier, and then it was needed to expand the negative sample set to facilitate further classification. The methods were overviewed as follows.</p><p>Firstly it used the k-means algorithm to divide the previously-produced credible negative sample sets into k clusters to generate a multi-centre model. Next a closeness-based classification method is applied to expand the negative sample sets. We denoted the k clusters of the credible negative set with 1 2   , ,..., k N N N , and denoted the centers of them as 1 2   , ,...</p><formula xml:id="formula_0" coords="3,326.28,738.98,61.31,16.57">, k C C C .</formula><p>The closeness of each cluster represents the average similarity of the samples within a cluster, it was represented as: 1 ( ) ( , )</p><formula xml:id="formula_1" coords="4,97.08,96.99,122.23,29.56">j i i j i d N i Cl N S d C N ∈ = ∑</formula><p>The average closeness of the negative sample set (including k clusters) was:</p><formula xml:id="formula_2" coords="4,99.00,175.37,123.40,29.85">1 1 ( ) ( ) k negative i i Cl N Cl N k = = ∑</formula><p>And the difference between the similarity within the negative set and the similarity between the positive and negative sets was defined as:</p><formula xml:id="formula_3" coords="4,51.78,267.47,219.04,31.22">1 1 1 ( ) ( , ) j i k negative j positive i d N i Df Cl N S d C k N = ∈ = -∑ ∑</formula><p>For an incoming sample text d, there were two requirements need to be filled to be regarded as a negative sample:</p><formula xml:id="formula_4" coords="4,65.94,359.11,187.79,49.14">1... max { ( , ) ( , )} i i k N positive S d C S d C Df = - &gt; 1.. max { ( , )} ( ) i i k N negative S d C Cl N = &gt;</formula><p>The closeness based method mainly solved two problems. One was that it was compatible to the situation that the samples in the hyperspace are not distributed centralized and well-proportioned. The second point was that it made the expanding negative samples credible and left those misty samples untreated.</p><p>These two points were actually suitable to the situation we met in the anti-spam scenario. While in our hypotheses proposed above, the spam mails were distributed in several scattered clusters, and moreover, we needed to be more careful to judge a mail as spam than ham in order to avoid the risk of losing important messages.</p><p>Here was an analogy between the scenarios in the Closeness based method to expand negative sample sets <ref type="bibr" coords="4,95.74,685.20,12.90,9.57" target="#b6">[7]</ref> and in the Spam Track.</p><p>As for the common benefits the idea of <ref type="bibr" coords="4,539.88,319.08,12.90,9.57" target="#b6">[7]</ref> would bring, we inherit the idea in our second-layer filter but made some changes to make it adaptive in our system. The main challenge we face was the computational cost of the system since an online spam filter has many limits on time and space requirements.</p><p>For the "train" aspect, the main task was to establish a multi-centre model and update the related information of each clusters when new mails coming. As the mails were coming in a stream form and considering the time limit for the system, it was unacceptable to apply a k-means or some other static clustering methods to the system. We adopt two measures to solve this problem. First we limit the number of mails to be trained in the second-layer filter. We set a score range r and only the mails whose first-layer filter score were accepted by the range were regarded as valuable to be trained in the next-layer filter. Secondly we applied an incremental clustering method to build and update the multi-center model.</p><p>The pseudo code was as follows.</p><p>Closeness based method to expand negative sample sets <ref type="bibr" coords="4,315.66,101.71,12.84,9.57" target="#b6">[7]</ref> Our Considering the computational cost, we changed the calculation of some attributes into an approximate form to make it adaptive for incremental computation.</p><p>[</p><p>]/</p><formula xml:id="formula_6" coords="5,43.98,406.81,239.96,95.17">new old C C N d N ≈ × -+ ( ) [ ( ) ( 1) ( , )]/ new i new i old i N i Cl N Cl N N S d C N ≈ × -+ 1 1 ( ) ( , ) k negative i positive i Df Cl N S C C k = ≈ -∑</formula><p>And the parameter t also played an important role to control the number of clusters and the effect of the second-layer filter.</p><p>During the experiments we found that the computation of closeness Cl and difference Df would bring little benefit to this filtering scenario, so a more simplified method was used in our submitted versions. We only computed and updated the cluster centre C of each cluster during the clustering. And the filtering results were determined by:</p><formula xml:id="formula_7" coords="5,79.44,671.51,164.17,18.08">1... max { ( , ) ( , )} i i k N positive S d C S d C = -</formula><p>Finally we integrated this score and the first-layer score together with certain weights to work out the final spamminess score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Systems submitted for Trec 2007</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Active Learning Mechanism</head><p>This year we didn't put much emphasis on the active learning mechanism. We proposed a naïve mechanism as follows: We tried to train the mails as early as possible, and we assumed that mails with extreme spamminess score were credible for training. To implement this mechanism under the Spam Track Framework, we simply gave those mails with moderate spamminess scores "Label N" labels, and gave those mails with extreme spamminess scores "Label B" labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">System Configuration</head><p>This year we submitted four filters to participate in the Spam Track. The configurations of the filters were as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">. Results and Future Works</head><p>Here is the (1-ROCA)% statistics of our submitted filters on this year's tasks:</p><p>From the comparison between Fdw2 and Fdw4, we found the fuzzy weighting method could make some improvements to the filter. From the comparison between Fdw1 and Fdw3, we found in intermediate tasks Fdw3 performs as well as or even better than Fdw1, while in delayed and active learning tasks, Fdw1 has some obvious advantages. Yet how the maximum length for each mail to be processed could affect the filter performance was still not determined by our experiments. And from the comparison between Fdw1 and Fdw2, which actually shows the filter performance with multi-centre model and without multi-centre model. They both perform well, but we haven't found obvious improvements by applying the model to the filter. So far we still have several problems to solve in our future work. As for the multi-centre model itself, we think it has some reasonable factors to solve the spam filtering scenario; this is an attempt during the beginning phase of our work and we would try to improve our methods more considerately. And another crucial point in our further work would be how to control the computational cost of the spam filtering method. As the mail streams arrived continuously, it is a realistic problem to control the expanding scale of temporal and spatial cost, especially for the multi-centre and clustering related methods. So we would focus the study on making our spam filtering methods more adaptable to huge volume stream data in real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Enterprise Track</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Candidate profiling and searching</head><p>For the expert search task, as no candidate list was provided, the first thing we did was to recognize emails using the pattern "first.last@csiro.au" as expert identifiers and found candidates' full names in the context of emails. However, we got almost 4000 candidates and the list seemed too big. As key people were CSIRO staff members who were the correct key contacts for this topic e.g. the project leader according to the guideline, we filtered the candidate list by the rule that the candidate should be contact on some project at least once. In addition, to evaluate the relationship between expert and page, we also gave higher weight if the candidate appeared in the document as a contact for he was more responsible for the document than other candidates. Since the tie between the email and the name can not be completely accurate. We separately calculated the score for each candidate's identifier on each topic, one for the candidate's name and another for his email. We added the two scores in some proportion to get the candidate's final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Document grouping and categorizing</head><p>The enterprise track had two big changes from last year: new corpus and no candidate list for the expert search task. However, some example pages were listed for each topic to simulate the pages which were often clicked according to the click log. We found that the most example key pages were of same kind: home project pages which were preferred for the task. So our system made feedback runs based on the page structure to find the same kind pages as example key pages. We analyzed the page, got out the id from html elements, for example &lt;table id="expertTable" ….&gt; ….&lt;/table&gt;, and made all ids stable and hash them up. Using the hash value as the key in the dictionary, every single page had the hash value as the recognizing of the "type" of it. When we found those pages, we gave those pages higher weight to improve the rank of the documents relevant to the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overview, strategies and runs</head><p>Document Search Task was the first enterprise track experiment involving document search over a full crawl. As our users were science communicators, retrieved documents should be those that help them create an overview page in the given topic area, which was quite different from the general document search task <ref type="bibr" coords="6,368.91,498.00,11.64,9.57" target="#b0">[1]</ref>. These "key" pages would tend to be authoritative pages such as project homepages and documents dedicated to the topic, rather than pages that made passing mention of the topic. For document search, we submitted four runs. First, FDUBase was query only run. We used Lemur as the search engine to index and query the topics. We added the score of the document by analyzing the pages which linked to this document. Second, we use auto term expansion mehod in FDUExpan run. By analyzing the narrative field, we automatically chose candidate expansion terms for topics and calculate the frequencies of the terms in the first five documents in the search result by Lemur. We used those high frequency terms as query terms for the second time search. Then we added the score of the document by analyzing the pages which linked to this document. Third, we used html tag as classification evidence in FDUFeedT run. We used FDUBase as the basic result. Then we analyzed the structure of each document and if the document had similar structure as the documents in the page field, we added the score of the document. At last, we took advantage of hits score as classification evidence for FDUFeedH run. We used HITS algorithm to judge the quality of the document and regard documents of the same quality as a category. We used FDUBase as the basic result. If the document was in the same category as the documents in the page field, we added the score of the document.</p><p>For expert search, we also submitted four runs. First, FDUn5e5 1, which gave the portion that name: 50% and email 50%. We detected email addresses and relevant full names automatically from the corpus. We also filtered the candidate list and remain those who were probably contacts on some projects. We calculated two scores for each candidate, one for the candidate's name and another for his email. We added the two scores by 50% and 50%. Second, FDUn3e7 3 which gave name 30% and email 70%. We detected email addresses and relevant full names automatically from the corpus. We also filtered the candidate list and remain those who were probably contacts on some projects. We calculated two scores for each candidate, one for the candidate's name and another for his email. We added the two scores by 30% and 70%. Third, FDUn7e3 4, which gave name 70% and email 30%. We detected email addresses and relevant full names automatically from the corpus. We also filtered the candidate list and remained those who were probably contacts on some projects. We calculated two scores for each candidate, one for the candidate's name and another for his email. We added the two scores by 70% and 30%. Finally, FDUGroup 2, which used group search. We detected email addresses and relevant full names automatically from the corpus. We also filtered the candidate list and remain those who were probably contacts on some projects. We divided the corpus to several groups according to the document structure. We gave different weights for the different kinds of the documents when calculating each candidate's score on the topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Conclusion</head><p>From above illustrations we can conclude that: to conquer the new task we were using several new approaches such as expert profiling and document categorizing. We mainly illustrated the technical issues we faced after an introduction of expert search. Also we explained where our final result was coming from and how we chose our model and organized the system. The results show that the "group" method did not improve the document search result effectively. It is mainly because the example pages given do not cover all kinds of pages required by users. FDUn7e3 run preformed best in all of the runs which shows the importance of name in expert finding. Although the email is more correct, names are more helpful in person's expertise judgment.</p><p>In the further work, we will pursue the study of finding similar-structure documents to improve the document search results. We will focus on the methods to extract similar-structure documents more accurately. Furthermore, we will pay great attention to the improvement of expert profiling model to get more information about the experts besides their expertise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Legal Track</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Task Introduction</head><p>WIM team participated in the main task of TREC 2007 Legal Discovery Track, and submitted 5 runs finally (1 manual + 4 autos). The objective of Legal Track is to evaluate the efficacy of automated support for review and production of electronic records in the context of litigation, regulation and legislation. The corpus consists of 650 xml files, 60G+ after unzipped. Each file contains multiple documents. In the document, 61 types of xml leave nodes form all textual information. Since legal corpus is converted from OCR format by program automatically, corpus may contain lots of meaningless text blocks and latent data inconsistency, which makes it a challenge for lawyers to lookup related documents supporting their quoting. The query of each topic is give by xml format as well. Participating teams can build queries in any way they like, using materials provided in the complaint, the production request, the boolean query, and any external resources that they have available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">System Overview</head><p>Before building search platform, we found there were many duplicated steps between pro-processing corpus and building query. So we built a unified framework to prepare the process of indexing and searching. The system framework lists as above.</p><p>In the framework, considering the efficiency, we employed Indri 2.3 (also known as next generation of lemur) as the main engine to index the whole corpus and returned the basic query results. The management module was called Yatata coded by java. Yatata was developed by WIM team for this year's legal track. It was an independent mini search engine fully implementing B-tree dictionary, core inverse table, query purser and candidate documents ranking. The reason why we took effect to develop a private search engine is that the experiment of many new methods could not be implemented in existed engine framework. But finally we only apply it as management module to do assistant work for Yatata's low performance when the scale of corpus extends to 50GB+. To make it clear, we made experiments between Indri and Lucene on the indexing efficiency using their default parameter settings. Indri took 48 hours to finish indexing the whole corpus while Lucene took 26 hours to finish 1/26 of them. For the reason that java is mush slower than native code in I/O operation we applied Yatata as the assistant of Indri.</p><p>In index step, we make two copies of depositories. First one was indexed by Indri directly without any pro-processing. The second one processed by Yatata, which removed "stop words" and "meaningless words", took word relevance statistics, constructed distribution model (abbreviated to DM, which will be described in next section) and finally input to Indri as another parallel depository.</p><p>In the step of searching, we generated different runs based on depository A or B. Required by the virtual court scenario, legal track preferred recall to precision in the production of documents. Most of teams in last year took the step of query expansion in their system. We tentatively handled the query expansion by applying DM built in the step of indexing by Yatata. Next section is the detail about DM application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Methodology</head><p>Similar to idea of scoring and term weighting <ref type="bibr" coords="8,514.04,763.20,18.59,9.57" target="#b10">[11]</ref>, we calculated the distribution of each word in whole corpus as the background model. In background model, each word had a mapped float value BM w representing how frequently it appeared in the whole corpus. This mapping information was maintained by Yatata. When one topic was submitted to Yatata, by measuring the distribution of words in fields of &lt;request text&gt;, &lt;instruction&gt;, &lt;definition&gt;, &lt;complaint&gt; related to each topic, Yatata generated topic-specified distribution models for each topic. In this model a float value TM w was mapped to each word appeared in query expression. Here is the expansion formulation:</p><p>In above formulation, avg(TM,BM) denotes for the average value of the division TM i /BM i for each word i in TM. Similarly σ(TM,BM) denotes for standard deviation of the value TM i /BM i . DM takes the factor into consideration that more frequently word appeared in specific topic relatively, the more important it related to current topic.</p><p>On opposite sides of expansion, we tentative carried out the method of query shrink as follow:</p><p>Among the five runs submitted by WIM, four of them were generated by applying DM. and the query of last one is built by manual as baseline. Here is the list of all runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Result Analysis</head><p>All runs submitted by WIM only took use of textual information. Since final scores were generated by pooling algorithm, we selected est_PB and est_RB as the main evaluation. Here is the illustration of the comparison among the five run iteratively by topics. The estimated recall @ B From the illustration, we can learn that different methods would cause great performance fluctuation. Since Legal Track 2007 gave the scores by topics, in order to make the comparison clear, we used the aggregate voting function. For each topic, if the run ranked at 1st, the bonus was 4 points; and the 2nd with bonus 3 points, and etc. We reached the result in evaluation est_PB that fdwim7rs &gt; fdwim7xj &gt; fdwim7sl &gt; fdwim7ss &gt; fdwim7ts, and the same as est_RB.</p><p>To conclude, only fdwim7rs was above the median score, while other three runs were under performance. This result leaded to the conclusion that: a) Using DM to query shrink could harvest better performance b) Query expansion implemented DM seems to reduce the original precision and recall. In another word, the text information existed in fields of &lt;instruction&gt;, &lt;definition&gt;, &lt;complaint&gt; may be useless in enhancing retrieval performance. c) Indri prefers raw materials to pre-processed material during the step of indexing.</p><p>The future work can be focused on handling data inconsistency in legal corpus. Perhaps many runs of our team suffered low recall scores by unsuited Passage2 MAP of each topic for three submitted runs Aspect MAP of each topic for three submitted runs For each measure, the overall performance of fdrun2 is better than other two runs. Two reasons support the success of the second method: Firstly, the concepts, instead of key words of questions, adopted in the method guarantee a better recall; Secondly, the contexts of each underlying answer sentence boost the precision of answers to each question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,315.66,92.40,261.16,236.87"><head></head><label></label><figDesc>The baseline fdwim7xj approached to median score of whole runs in Legal Track 2007.</figDesc><table coords="9,321.24,144.63,255.58,184.64"><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>4</cell><cell>7</cell><cell>10</cell><cell>13</cell><cell>16</cell><cell>19</cell><cell>22</cell><cell>25</cell><cell>28</cell><cell>31</cell><cell>34</cell><cell>37</cell><cell>40</cell><cell>43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>fdwim7rs</cell><cell cols="2">fdwim7sl</cell><cell>fdwim7ss</cell><cell></cell><cell>fdwim7ts</cell><cell></cell><cell>fdwim7xj</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">The estimated precision @ B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>4</cell><cell>7</cell><cell>10</cell><cell>13</cell><cell>16</cell><cell>19</cell><cell>22</cell><cell>25</cell><cell>28</cell><cell>31</cell><cell>34</cell><cell>37</cell><cell>40</cell><cell>43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>fdwim7rs</cell><cell cols="2">fdwim7sl</cell><cell>fdwim7ss</cell><cell></cell><cell>fdwim7ts</cell><cell></cell><cell>fdwim7xj</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Genomics Track</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview</head><p>For the TREC 2007 Genomics Track, the system was required to extract out the relevant passages of text that answers the topic questions <ref type="bibr" coords="10,192.30,178.20,16.82,9.57" target="#b9">[10]</ref>. It's similar to the task of Genomics Track 2006 except the question types. We group submitted three runs based on three different score models. Three methods have the common process which extracts the concepts for followed scoring process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relevant Concept Extraction</head><p>A topic question is an information need unit, in which some key biological entities can catch the leading need. The relevant concept extraction is based on the heuristic method that the frequent biological entities occurring around those key ones in the text should have some association with them. So we find the relevant concepts from the context of those key entities. Concretely, we retrieval top 1000 sentences for each topic question by language model ranking strategy, and remove these sentences that don't contain any of those key biological entities. We think of the top most frequent biological entities as relevant concepts according to the remaining sentence snippets. The extracted concepts are used to expand the corresponding topic question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Score Models</head><p>We employ three methods to score sentences: 1) sentence language model; 2) context language model; 3)boost co-occurring method.</p><p>The first one, sentence language model, is our baseline method, which looks at each sentence in the corpus as a document. Then the language model with a linear smoothing is used to score and rank them according to the expanded query.</p><p>The context language model definitely involves the context component. That's, the context text can increase the score of a sentence to some extent according to the text relevance with the given topic question. We score the sentence by employing following context language model:</p><p>The third method scores sentence according to two basic components: baseline score and boosted score derived from the co-occurring of different type of biological entities. The baseline score is calculated from the following formula:</p><p>The boosted score is calculated as follows: BoostedScore = 2 × min(rEntityNum, lEntityNum) × min(maxlweight,maxrweight)</p><p>The final score of a given sentence is the sum of these two parts. That is: SentenceScore = BScore + BoostedScore</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results</head><p>The following figures give the result of each run regard to each performance measure method: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,65.22,615.12,214.36,9.57;11,42.54,630.72,126.27,9.57" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,71.08,615.12,199.29,9.57">TREC-2007 Enterprise Track Guidelines</title>
		<ptr target="http://www.ins.cwi.nl/" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,58.80,646.32,220.82,9.57;11,59.04,661.92,220.62,9.57;11,59.04,677.52,143.90,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,166.55,646.32,113.07,9.57;11,59.04,661.92,23.77,9.57">WIM at TREC Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">Junyu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,115.29,661.92,164.37,9.57;11,59.04,677.52,111.74,9.57">Proceedings of 15th Text Retrieval Conference (TREC 2006)</title>
		<meeting>15th Text Retrieval Conference (TREC 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,59.52,693.12,220.19,9.57;11,59.04,708.72,220.64,9.57;11,59.04,724.32,220.52,9.57;11,59.04,739.92,220.56,9.57;11,59.04,755.52,220.67,9.57;11,59.04,771.12,82.80,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,259.17,693.12,20.55,9.57;11,59.04,708.72,220.64,9.57;11,59.04,724.32,101.07,9.57">Role Centralized Modeling for Expert Search in Enterprise Corporation</title>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,190.61,724.32,88.96,9.57;11,59.04,739.92,220.56,9.57;11,59.04,755.52,220.67,9.57;11,59.04,771.12,50.31,9.57">Proceedings of The 7th International Conference on Advanced Language Processing and Web Information Technology</title>
		<meeting>The 7th International Conference on Advanced Language Processing and Web Information Technology</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,336.12,61.20,216.67,9.57;11,332.16,76.80,220.61,9.57;11,332.16,92.40,220.62,9.57;11,332.16,108.00,220.64,9.57;11,332.16,123.60,24.75,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,337.38,76.80,215.39,9.57;11,332.16,92.40,32.08,9.57">Formal models for expert finding in enterprise corpora</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,390.50,92.40,162.28,9.57;11,332.16,108.00,216.00,9.57">SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,335.40,139.20,217.42,9.57;11,332.16,154.80,220.60,9.57;11,332.16,170.40,181.47,9.57" xml:id="b4">
	<monogr>
		<title level="m" coord="11,335.40,139.20,217.42,9.57;11,332.16,154.80,220.60,9.57;11,332.16,170.40,181.47,9.57">Spam Filtering using Character-level Markov Models: Experiments for the TREC 2005 Spam Track, Andrej Bratko and Bogdan Filipic</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,332.34,186.00,220.53,9.57;11,332.16,201.60,220.56,9.57;11,332.16,217.20,220.58,9.57;11,332.16,232.81,142.91,9.57" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Wachman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brodley</surname></persName>
		</author>
		<title level="m" coord="11,332.34,186.00,220.53,9.57;11,332.16,201.60,220.56,9.57;11,332.16,217.20,88.00,9.57">Spam Filtering using Inexact String Matching in Explicit in Explicit Feature Space with On-Line Linear Classifiers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,341.09,248.41,211.58,9.57;11,332.16,264.01,220.61,9.57;11,332.16,279.61,64.22,9.57" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,341.09,248.41,211.58,9.57;11,332.16,264.01,95.91,9.57">A Closeness-based Semi-supervised Text Classification Method</title>
		<editor>Zheng Haiqing, Lin Chen and Niu Junyu</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.54,295.21,219.20,9.57;11,336.66,310.81,144.46,9.57" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.24,326.41,219.48,9.57;11,332.16,342.01,220.67,9.57;11,332.16,357.61,131.09,9.57" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Huma</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
		<title level="m" coord="11,333.24,326.41,183.15,9.57">Text Classification using String Kernels</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,336.66,373.21,171.47,9.57;11,337.67,388.81,204.89,9.57" xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName coords=""><surname>Trec</surname></persName>
		</author>
		<ptr target="http://ir.ohsu.edu/genomics/2007protocol.html" />
	</analytic>
	<monogr>
		<title level="j" coord="11,392.32,373.21,111.36,9.57">Genomics Track Protocol</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,338.40,404.41,214.35,9.57;11,337.68,420.01,215.09,9.57;11,337.68,435.61,43.04,9.57;11,397.70,435.61,48.81,9.57;11,463.48,435.61,46.40,9.57;11,526.89,435.61,25.95,9.57;11,337.68,451.21,91.09,9.57" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,414.89,420.01,137.88,9.57;11,337.68,435.61,38.73,9.57">An Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
