<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,71.25,72.44,467.23,16.59;1,166.43,92.36,276.85,16.59">Enterprise Search: Identifying Relevant Sentences and using them for Query Expansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,146.94,137.94,89.89,11.06"><forename type="first">Maheedhar</forename><surname>Kolla</surname></persName>
							<email>mkolla@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo Canada</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.04,137.94,93.58,11.06"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo Canada</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,71.25,72.44,467.23,16.59;1,166.43,92.36,276.85,16.59">Enterprise Search: Identifying Relevant Sentences and using them for Query Expansion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B67CBA412FFEFEA47958C9E12F0EC242</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we discuss the experiments conducted in context of Document Search task of 2007 Enterprise Search track. Our method is based on selecting sentences from the given relevant documents and using those selected sentences for query expansion. We observed that our method of query expansion improves system's performance over baseline run, under various methods of comparison.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Enterprise Search track, organized by NIST, is aimed at studying information search problems faced by employees (mostly) in an organization. In 2007, two tasks were proposed in this track:</p><formula xml:id="formula_0" coords="1,67.12,376.74,105.65,26.69">• Document Search • Expert (People) Search.</formula><p>These tasks simulated problems faced by Science Communicators of the (CSIRO) organization. As their work task, communicators are required to create an overview page for a given topic. Systems' task is therefore defined as to retrieve documents that would help such science communicators in creating such an overview page. Sample topic, composed by the communicators, is as follows:</p><p>query: hairpin RNAi/gene silencing narr: Information to help scientists find out more about hairpin RNAi technology. Specific contacts to obtain vectors. page: CSIRO197-05231046 page: CSIRO139-13111797</p><p>As observed, along with query and narrative, each topic consists of list of documents (under page fields), tagged relevant by topic creators. These documents could be used for relevance feedback runs.</p><p>Our interest in this year's participation is to study the term expansion from the relevant documents provided. We propose a method to rank sentences in the relevant documents and then extract terms from top ranked sentences of each relevant document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TERM EXPANSION</head><p>We hypothesize that terms extracted from summary of a document would enable us to capture the intent of an user (who judged the relevance of the document). We propose a method of sentence selection that would identify sentences closer to topic model than general English model. We built topic relevance model using the top 20 documents retrieved for initial query, as proposed by Lavrenko and Croft <ref type="bibr" coords="1,530.64,355.46,9.22,7.86" target="#b3">[3]</ref>.</p><p>Using both, relevance and generic collection models, we rank the sentences within each document based on Kullback-Leibler Divergence (KLD) measure. KLD value of a sentence is computed as follows:</p><formula xml:id="formula_1" coords="1,356.16,413.82,199.76,23.60">KLD(rm, cm) = X t S prm(t) * log prm(t) pcmt<label>(1)</label></formula><p>where prm is the probability of a term in relevance model, pcm is the probability of the term in general collection model and t represents terms belonging to a sentence S. KLD measure in our method would indicate the "goodness" of a sentence towards topic model than collection model . Each sentence would obtain "goodness" value displaying its closeness towards language model built from relevant documents as supposed to general English language model. Büttcher.et.al <ref type="bibr" coords="1,316.81,529.09,9.73,7.86" target="#b1">[1]</ref> used similar measure to re-rank documents initially retrieved for a given query.</p><p>We then rank sentences based on their KLD values and extract top ranking sentence from each related article. We pool these top ranked sentence extracted from each related article and extract terms for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION</head><p>Documents retrieved were evaluated on a three point scale: non-relevant (0), relevant (1) and highly relevant <ref type="bibr" coords="1,515.18,630.86,10.75,7.86" target="#b2">(2)</ref>, where highly relevance are documents that would help the science communicators create an overview page. The following runs are were compared in our experiments:</p><p>• uwBase -baseline Okapi BM25 ranking</p><p>• uwKLd -pseudo-relevance feedback run, with term extracted based in KL Divergence <ref type="bibr" coords="1,470.40,711.19,9.73,7.86" target="#b2">[2]</ref> • uwRF -terms extracted from top ranked sentences, selected from documents under pages fields Run MAP P@5 P@10 ndcg1000 uwbase 0.3878 0.6520 0.5780 0.7071 uwKld 0.3877 0.6160 0.5380 0.7335 uwRF 0.4299 0.7040 0.6220 0.7557</p><p>Table <ref type="table" coords="2,84.12,148.98,4.12,7.89">1</ref>: Standard Measures: Comparison of systems (without altering any runs)</p><p>In Table <ref type="table" coords="2,102.83,180.66,3.59,7.86">1</ref>, we present the standard results of evaluation for all three runs. Only documents judged with relevance value of 2 are considered relevant in these evaluation. However, as mentioned in guidelines, it would be unfair to compare relevance feedback runs with pseudo-relevant feedback and/or baseline runs directly -relevance feedback runs might just rank those documents provided under pages fields than identifying different relevant documents. NIST proposed different means to compare relevant feedback runs directly with baseline and pseudo-relevant feedback runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Promotion Evaluation</head><p>In this mode of comparison, all documents provided by topic creators under pages fields are ranked at the top of each system's run. Through this method of comparison, we could investigate the extent to which any relevance feedback method would actually find relevant documents instead of just promoting already known relevant documents. Evaluation results are presented in Table <ref type="table" coords="2,194.82,368.59,3.59,7.86" target="#tab_0">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual Collection</head><p>In residual collection method of evaluation, all provided relevant documents(i.e documents from pages fields) are removed from evaluation process. Systems are then compared based on their ability on retrieving all possible relevant documents from residual collection. Table <ref type="table" coords="2,209.43,547.79,4.61,7.86" target="#tab_2">3</ref> presents evaluation results for all three systems under this approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION AND FUTURE WORK</head><p>As observed from evaluation results, relevance feedback run improves over baseline run and performs better than pseudo-relevant feedback runs in all three methods of comparison (although not statistically significant). We would like to now compare different sentence selection methods with our sentence selection method. We would also attempt to replicate similar experiments on previous Enterprise Search test data to test the efficiency of our term extraction method on tasks such as Email search, Known-item search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,368.59,239.11,103.97"><head>Table 2 :</head><label>2</label><figDesc>. Promotion Evaluation: Comparison of systems in which all documents provided in pages fields are placed at the top of each run.</figDesc><table coords="2,81.12,387.92,181.64,43.25"><row><cell>Run</cell><cell>MAP</cell><cell>P@5</cell><cell>P@10</cell><cell>ndcg1000</cell></row><row><cell cols="4">uwbase.p 0.4698 0.8720 0.7080</cell><cell>0.7506</cell></row><row><cell>uwKld.p</cell><cell cols="3">0.4663 0.8360 0.6860</cell><cell>0.7697</cell></row><row><cell>uwRF.p</cell><cell cols="3">0.5004 0.8920 0.7160</cell><cell>0.7871</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,53.80,633.41,239.10,28.81"><head>Table 3 :</head><label>3</label><figDesc>Residual Collection Evaluation: Comparison of systems in which documents provided under pages fields are not included in evaluation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,53.80,705.09,239.10,7.17;1,53.80,714.06,70.84,7.17"><p>To appear in 16th Text Retrieval Conference proceedings, Gaithersburg Maryland, USA,2007.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="2,321.30,152.73,96.81,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="2,331.02,167.92,201.64,7.86;2,331.02,178.38,215.09,7.86;2,331.02,188.85,187.95,7.86;2,331.02,199.31,199.17,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="2,331.02,178.38,215.09,7.86;2,331.02,188.85,131.70,7.86">Index pruning and result reranking: Effects on adhoc retrieval and named page finding</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="2,481.08,188.85,37.89,7.86;2,331.02,199.31,84.18,7.86">15th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithesburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="2,331.02,210.76,223.32,7.86;2,331.02,221.22,205.38,7.86;2,331.02,231.68,210.34,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="2,542.32,210.76,12.03,7.86;2,331.02,221.22,205.38,7.86;2,331.02,231.68,38.06,7.86">An information-theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="2,376.39,231.68,89.66,7.86">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="2,331.02,243.14,185.89,7.86;2,331.02,253.60,184.68,7.86;2,331.02,264.06,202.69,7.86;2,331.02,274.52,204.07,7.86;2,331.02,284.99,214.25,7.86;2,331.02,295.45,97.27,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="2,331.02,253.60,121.29,7.86">Cross-lingual relevance models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="2,471.25,253.60,44.45,7.86;2,331.02,264.06,202.69,7.86;2,331.02,274.52,204.07,7.86;2,331.02,284.99,82.64,7.86">SIGIR &apos;02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
