<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.12,80.99,411.77,12.90;1,124.77,96.93,362.46,12.90">The Pronto QA system at TREC-2007: harvesting hyponyms, using nominalisation patterns, and computing answer cardinality</title>
				<funder>
					<orgName type="full">&quot;Rientro dei Cervelli</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,177.37,137.25,52.64,10.75"><forename type="first">Johan</forename><surname>Bos</surname></persName>
							<email>bos@di.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.72,137.25,87.81,10.75"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of IT</orgName>
								<orgName type="institution">University of Rome &quot;La Sapienza&quot; University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.24,137.25,90.66,10.75"><forename type="first">Edoardo</forename><surname>Guzzetti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.12,80.99,411.77,12.90;1,124.77,96.93,362.46,12.90">The Pronto QA system at TREC-2007: harvesting hyponyms, using nominalisation patterns, and computing answer cardinality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C678DD9DEED27FD2CFA9D5D73E2AF492</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The backbone of the Pronto QA system is linguistically-principled: Combinatory Categorial Grammar is used to generate syntactic analyses of questions and potential answer snippets, and Discourse Representation Theory is employed as semantic formalism to match the meanings of questions and answers. The key idea of the Pronto system is to use semantics to prune answer candidates, thereby exploiting lexical resources such as WordNet and NomLex to facilitate the selection of answers.</p><p>The system performed well at TREC-2007 on factoid-questions with an answer accuracy of 22%, a score higher than the median accuracy score of all participating systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The QA evaluation exercise at TREC consists in automatically finding answers for a collection of questions arranged by different topics, or targets in TREC parlance. Questions can be either factoid-questions, asking for a unique short answer, or list-questions, asking for a set of answers. Each series of questions ends with an otherquestion, which is a request to provide all relevant information about the target which was not already asked in the previous questions. An example of a target and its questions is shown in Figure <ref type="figure" coords="1,188.26,593.34,3.74,8.64">1</ref>.</p><p>The answers must be presented with their sources. For TREC-2007, the relevant document collections were Aquaint-2 and Blog06. Aquaint-2 is collection of almost one million newswire articles (written in English) dating from <ref type="bibr" coords="1,93.27,653.74,47.32,8.64">2004-2006.</ref> Blog06 is a set of homepage documents from late 2005 and early 2006. A response is evaluated as correct only if it exactly answers the question (in an exhaustive but not overinformative way), if it is the most recent correct answer (i.e., globally correct rather than locally correct), and if it is accompanied by a document ID from one of the two previously mentioned corpora supporting the answer.</p><p>This paper contains a description and results of our QA system "Pronto" at TREC-2007. Probably the most interesting aspect of the Pronto system is that it uses a deep linguistic analysis, combining symbolic with statistical approaches, and its use of general background knowledge as found in resources such as WordNet and NomLex. The Pronto system is a reincarnation of the "La Sapienza" system (Bos06), which was in turn inspired by the QED system (LBD + 03; ABC + 04; ABC + 05).</p><p>The major modifications with respect to this earlier versions concern mainly question analysis (supporting multiple interpretations of the question), the use of large corpora to find relevant hyponyms, named entity recognition of creative works (Guz07), computing the answer cardinality for list-questions, and the resolution of indirect temporal expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Pronto QA system</head><p>As with any open-domain QA system, the input of Pronto is a question, and its output a set of answers. In between we have a cascaded architecture of components, consisting of question interpretation (parsing and boxing the question), computing relevant background knowledge, expected answer typing, query construction, document retrieval, answer extraction, and finally answer selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Interpretation</head><p>Each question in a serie is analysed together with its corresponding target. The steps of processing are: tokenisation, morphological analysis with Morpha (MP01), named entity recognition, part of speech tagging, parsing, and semantic construction.</p><p>Two parsers are employed to maximise coverage: the wide-coverage CCG-parser of <ref type="bibr" coords="1,440.27,677.65,99.73,8.64">Clark &amp; Curran (CC04)</ref> and the Pronto in-built question parser for CCG. On the basis of the output of the parsers a semantic representation is constructed with the help of Boxer (BCS + 04; For each question a set of expected answer types is produced. The result of question interpretation and answer type determination is a Question-DRS (Q-DRS for short), or in case of ambiguities, several Q-DRSs. An example of a Q-DRS is shown in Figure <ref type="figure" coords="2,198.22,360.90,3.74,8.64">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computing Background Knowledge</head><p>Many questions require additional background information in order to be able to answer it with a high level of confidence. Here we have in mind background knowledge derived from large databases such as WordNet (Fel98) and NomLex (MMY + 98). But to use all the knowledge encoded in WordNet or other large resources for each question and potential answer pair would obviously render the overall system inefficient. Instead, Pronto comes with a component that computes all background knowledge for a given question that is expected to be useful in later stages of the processing pipeline.</p><p>Put differently, the background knowledge for a question constitutes a list of axioms related to the question. It is gathered from lexical resources on the basis of the non-logical symbols that occur in the semantic representation of the question (the Question-DRS). Currently the following kinds of axioms are used:</p><p>• synonyms, plus direct and indirect hyponyms and hyperonyms for nouns and verbs derived from WordNet (Fel98);</p><p>• synonyms of names derived from WordNet (Fel98);</p><p>• hyponyms for nouns harvested from corpora (Aquaint-1, Aquaint-2, and the web) using lexical patterns using techniques similar as in (Hea92);</p><p>• nominalisation rules generated from NomLex (MMY + 98);</p><p>• specialised knowledge, such as attributes (colours, shapes), and geographical knowledge (continents, states, countries, capitals).</p><p>The background knowledge for a question is used for determining the expected answer type, for generating queries in the document retrieval stage, and in answer extraction and selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Cardinality</head><p>For each list-question the expect number of answers is computed. We refer to this as answer cardinality, which denotes a range expressed by an ordered pair of two numbers. The first of these numbers indicates the mininal number of answers expected (the lower bound), the second the maximal number of answers (the upper bound, which is set to 0 if unknown). For instance, the answer cardinality "3-3" indicates that exactly three answers are expected, and "2-0" means that the question requires at least two answers.</p><p>For the TREC-2007 exercise, Answer Cardinality was computed by reformulating the list-question into a question asking for a number. For instance, considering the question 268.7, "What were the settlements that were evacuated?", with target "Israel evacuation of the Gaza Strip", a new question was generated with the surface form "How many were the settlements that were evacuated?". This question was given to the Pronto system, and the returned answer regarded as answer cardinality. For this specific example, Pronto found the following (correct) answer:</p><p>[XIN ENG 20050630.0060] Israel is scheduled to evacuate 21 settlements in the Gaza Strip and four in northern West Bank from Aug. 17 in order to " disengage " from conflicts with the Palestinians. The questions were reformulated using simple rewrite rules with the help of regular expressions. To increase the chance of finding a correct answer, documents of both Aquaint-1 and Aquaint-2 were taken into account in the document retrieval stage. If no reliable answer was found, an answer-cardinality of 12 was returned, a number considered to be a good default based on previous TREC campaigns. If a number higher than 100 was found, answer cardinalty was "corrected down" to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Document Retrieval</head><p>All documents in the Aquaint-2 corpus were preprocessed: the XML was stripped off, sentence boundaries were detected using Punkt (KS06), and all text was tokenised. The documents were then rearranged into smaller documents of two sentences each (taking a sliding window, so each sentence appeared in two minidocuments). These mini-documents were indexed with the Indri information retrieval tools (MC04).</p><p>For each query, up to 5,000 mini-documents were retrieved, again with the help of Indri (MC04). At this stage of processing, the aim is high recall at the expense of precision. By selecting a high number of documents, the pool of potential answers can be narrowed down as late as possible in the processing pipeline. Processing a high number of documents is certainly time-consuming, but as there are no important time-constraints in the TREC exercise, this is no big concern and advantage is taken of the situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Document Analysis</head><p>With the help of the C&amp;C wide-coverage CCG parser, all retrieved mini-documents are parsed and for each of them a Discourse Representation Structure (DRS) is generated using Boxer. The parser also performs basic named entity recognition for locations, persons, and organisations. In addition, a named entity recogniser for creative works was used too (Guz07), as usually a substantial part of questions required a name of a creative work (book, film, play) as answer (Bos06).</p><p>Each mini-document is translated into a single DRS (the so-called A-DRS). A set of DRS normalisation rules are applied in a post-processing step, thereby dealing with active-passive alternations, inferred semantic information, normalisation of temporal expressions, and the disambiguation of noun-noun compounds. The resulting DRS is enriched with information about the original surface word-forms and parts of speech. An example of an A-DRS is shown in Figure <ref type="figure" coords="3,180.51,652.47,3.74,8.64">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Answer Extraction</head><p>Given the DRS of the question (the Q-DRS), and a set of DRSs of the retrieved documents (the A-DRSs), each A-DRS is matched with the Q-DRS to find a potential answer. This process proceeds as follows: if the A-DRS contains a discourse referent of the expected answer type matching will commence. The process of matching attempts to identify the semantic structure in the Q-DRS with that of the A-DRS. The result is a score between 0 and 1 indicating the amount of semantic material that could be matched. The generated background knowledge for the question (see Section 2.2) is used to assist in the matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Answer Selection</head><p>The Answer Extraction component yields a list of answers and a matching score. Answers that are semantically identical are grouped together. This process produces a new list of answers, ranked on matching score and frequency. A simple method of reranking was employed at the TREC-2007 exercise, namely by sorting on the matching score, using the highest answer frequency as tie-breaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Processing other-questions</head><p>Since other-questions do not appear as ordinarily formulated questions, but the QA system expects questions phrased in English as input, they are automatically transformed into definition questions. This is simply done by parsing the question "What did he also do?" and assigning it the answer type DEFINITION with answer cardinality 1-0. The answer extraction component deals with definition questions by finding statements that directly relate the target to numeral expressions, date expressions, adjectives, or proper names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Three runs were submitted, all with different parameters with respect to the treatment of factoid, list, and other-questions. The parameters for factoid-questions were the use of hypernyms, the use of hyponyms harvested from large corpora (i.e., not from WordNet), and whether documents from the Blog06 corpus were included in the search or not. The first run for list-questions selected the twelve best matching answers, whereas the second and third run used our answer cardinality method (Section 2.3), to select the N-best answers. For otherquestions, we increased the number of selected answers with each run. Table <ref type="table" coords="3,400.90,637.62,4.98,8.64" target="#tab_1">1</ref> summarises the runs and the parameters used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TREC-2007 Judgements</head><p>Factoid questions, as usual, formed the majority of the questions at the TREC-2007 QA evaluation exercise. The results of the Pronto system over 360 factoid questions Question:</p><p>When was Alan Greenspan born? Target:</p><p>Alan Greenspan ID:</p><p>264  <ref type="table" coords="5,149.74,204.94,3.74,8.64" target="#tab_2">2</ref>, where U is the number of unsupported (correct but without a supporting document), X the number of inexact, L the number of locally correct (a later document in the Acquint corpus contradicts the answer, so the response is judged locally rather than globally correct), and R the number of correct answers. The last two columns of Table <ref type="table" coords="5,213.77,387.79,4.98,8.64" target="#tab_2">2</ref> show the accuracy (calculated on the basis of R) and lenient accuracy (calculated on the basis of U+X+L+R). In addition, it shows the summed scores of all participating systems at TREC-2007, a total of 51 runs. As suspected, adding extra hyponyms improves accuracy, whereas adding hypernyms can actually have a negative influence. Our third run showed the best results, which only differed from the second run by inclusion of the Blog06 documents.</p><p>Table <ref type="table" coords="5,107.88,496.37,4.98,8.64" target="#tab_2">2</ref> also shows that, compared with the total of all runs, Pronto produced relatively few unsupported answers, and relatively many locally correct answers. The former is probably due to the deep linguistic analysis for matching answers to questions, and refraining from using the web as an additional corpus. The latter is likely caused by the fact that Pronto doesn't distinguish locally correct from globally correct answers.</p><p>There were 85 list-questions in total. These are evaluated by calculating the precision and recall for each question and then averaging their corresponding F-scores. For the third run, the Pronto system achieved an average Fscore slightly higher than the median of all participating systems (Table <ref type="table" coords="5,133.74,652.76,3.60,8.64" target="#tab_3">3</ref>).</p><p>The results of the other-questions were slightly disappointing compared to last year's results (Bos06). The perseries results, where the results of factoid, list and other questions are combined into one summarising score, were higher than the medium score of all participating systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Closer Look</head><p>Here we look at three specific aspects of the Pronto QA system that we believe contributed to its performance at TREC-2007: the use of nominalisation patterns in the background knowledge, harvesting hyponyms from large corpora, and the use of answer cardinality in listquestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Using NomLex</head><p>Although we didn't specifically evaluate the contribution of NomLex (MMY + 98), we found several examples where NomLex clearly contributed to finding a correct answer. A case in point is the factoid question in Figure <ref type="figure" coords="5,329.35,386.88,3.74,8.64">3</ref>. Here the background knowledge generated from the NomLex patterns facilitates the matching between question and answer. As a result, a higher matching score is obtained, and the correct answer is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Using Hyponyms found in Corpora</head><p>Hyponyms are probably the most important lexical relation required in question answering. Wordnet (Fel98) contains many hyponyms, but in the majority of cases the hyponym is a common noun, rather than a proper name. For many questions, proper name hyponyms (sometimes also referred to as instances) are crucial in selecting appropriate answers. This hypothesis was confirmed in TREC-2007, as is illustrated for a few selected examples in Table <ref type="table" coords="5,347.82,556.02,3.74,8.64" target="#tab_4">4</ref>. ∀x∀y(founder(x) ∧ of(x,y) → ∃e(found(e) ∧ agent(e,x) ∧ patient(e,y)))</p><p>Figure <ref type="figure" coords="6,198.27,206.01,3.88,8.64">3</ref>: Example of background knowledge generated by NomLex</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Computing Answer Cardinality</head><p>The computation of answer cardinality was put into action in the second and third run (Table <ref type="table" coords="6,245.14,264.93,4.15,8.64" target="#tab_1">1</ref>) but hardly seemed to make a difference in the overall results for listquestions, as Table <ref type="table" coords="6,151.37,288.84,4.98,8.64" target="#tab_3">3</ref> suggests. A good reason to have a closer look as to what happened. What we did is the following: we compared the number of answers returned by Pronto with the number of known answers as reported in the official judgement files distributed by the organisers of TREC.</p><p>It turned out that the baseline, guessing an answer cardinalty of 12 or less when fewer answers were found, had a precision of 5/48. The method described in Section 2.3 reached a precision of 7/48. This is of course only a rough indication of success and could just be a coincidence. A better way to measure accuracy of answer cardinality given AC k , the known number of answers, and ACc, the computed answer accuracy, is:</p><formula xml:id="formula_0" coords="6,91.93,465.06,91.67,15.00">Acc = 1 -|AC K -AC C | AC K +AC C</formula><p>Using this equation, the baseline yielded an average accuracy of 0.62, whereas our experimental method reached a lower average accuracy of 0.57. In other words, our novel method for computing answer cardinality failed to beat the baseline. It is interesting to note that the best default answer cardinality for the list-questions of TREC-2007 turned out to be 7, reaching an average accuracy of 0.72 (or 0.63 when taking 7 as an upper bound of selected answers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The Pronto QA system is based on a deep linguistic analysis of question and potential answers contexts and uses semantics to narrow down the number of answer candidates. Compared to other QA systems at TREC-2007, Pronto performed above par for factoid and listquestions. We have shown that the use of hyponyms obtained from large corpora, and patterns translated from NomLex increase the performance of the system. We tried to find a simple but effective method for computing answer cardinality when dealing with list-questions, but it turns out not to be straightforward to beat a baseline choosing the average number of answers. To compare different approaches to answer cardinality, we introduced a new metric for measuring the accuracy of answer cardinality for list-questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,127.36,230.23,351.34,362.52"><head>Table 1 :</head><label>1</label><figDesc>Description of the three runs of Pronto at TREC-2007, for factoid, list and other-questions.</figDesc><table coords="4,127.36,230.23,351.34,362.52"><row><cell></cell><cell>.1</cell><cell></cell></row><row><cell></cell><cell>X1</cell><cell></cell></row><row><cell></cell><cell>named(X0,alan,per)</cell><cell></cell></row><row><cell></cell><cell>named(X0,greenspan,per)</cell><cell></cell></row><row><cell>Q-DRS:</cell><cell>X1</cell><cell>X2</cell></row><row><cell></cell><cell>?</cell><cell>bear(X2)</cell></row><row><cell></cell><cell>unit-of-time(X1)</cell><cell>patient(X2,X0)</cell></row><row><cell></cell><cell></cell><cell>temp-rel(X2,X1)</cell></row><row><cell cols="2">Answer Type: [match:name:year,match:name:day]</cell><cell></cell></row><row><cell>Cardinality:</cell><cell>1-1</cell><cell></cell></row><row><cell>Query</cell><cell>greenspan</cell><cell></cell></row><row><cell>Context:</cell><cell cols="2">[AFP ENG 20060125.0276] Greenspan was born in New York in 1926</cell></row><row><cell></cell><cell>x0 x1 x2 x3</cell><cell></cell></row><row><cell></cell><cell>named(x0,greenspan,per)</cell><cell></cell></row><row><cell></cell><cell>bear(x1)</cell><cell></cell></row><row><cell>A-DRS:</cell><cell>patient(x1,x0)</cell><cell></cell></row><row><cell></cell><cell>named(x2,new york,loc)</cell><cell></cell></row><row><cell></cell><cell>in(x1,x2)</cell><cell></cell></row><row><cell></cell><cell>timex(x3,'+1926XXXX')</cell><cell></cell></row><row><cell>Answer:</cell><cell cols="2">264.1 pronto07run3 AFP ENG 20060125.0276 1926</cell></row><row><cell cols="3">Figure 2: System input and output for the factoid-question 264.1 at TREC-2007.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,78.70,297.39,213.41,71.35"><head>Table 2 :</head><label>2</label><figDesc>Resultsfor factoid-questions, TREC-2007.</figDesc><table coords="5,78.70,309.10,213.41,59.64"><row><cell>Run</cell><cell>U</cell><cell>X</cell><cell>L</cell><cell cols="2">R Acc. Len. Acc.</cell></row><row><cell>1</cell><cell>3</cell><cell>9</cell><cell>7</cell><cell>70 0.19</cell><cell>0.25</cell></row><row><cell>2</cell><cell>3</cell><cell>11</cell><cell>7</cell><cell>75 0.21</cell><cell>0.27</cell></row><row><cell>3</cell><cell>5</cell><cell>15</cell><cell>8</cell><cell>80 0.22</cell><cell>0.30</cell></row><row><cell>all</cell><cell cols="4">529 756 169 3055 0.17</cell><cell>0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,313.20,82.12,226.81,110.18"><head>Table 3 :</head><label>3</label><figDesc>Results: average F-score for list question, Pyramid F-score for other-questions, and per-series scores at TREC-2007.</figDesc><table coords="5,359.83,120.70,133.54,71.59"><row><cell>Run</cell><cell cols="3">List Other Series</cell></row><row><cell>1</cell><cell>0.09</cell><cell>0.04</cell><cell>0.11</cell></row><row><cell>2</cell><cell>0.09</cell><cell>0.06</cell><cell>0.12</cell></row><row><cell>3</cell><cell>0.10</cell><cell>0.07</cell><cell>0.13</cell></row><row><cell cols="2">median 0.09</cell><cell>0.12</cell><cell>0.11</cell></row><row><cell>best</cell><cell>0.48</cell><cell>0.33</cell><cell>0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,313.20,586.70,245.45,118.59"><head>Table 4 :</head><label>4</label><figDesc>Examples of hyponyms found in large corpora, thereby selecting a correctly judged answer.</figDesc><table coords="5,319.18,610.18,239.48,95.10"><row><cell>ID</cell><cell>BK</cell><cell>Source</cell></row><row><cell cols="3">216.1 ∀x(new york times(x) → newspaper(x)) Aquaint</cell></row><row><cell cols="2">220.7 ∀x(tiger(x) ∧ woods(x) → star(x))</cell><cell>Web</cell></row><row><cell cols="2">222.7 ∀x(scotchgard(x) → brand(x))</cell><cell>Web</cell></row><row><cell cols="2">222.7 ∀x(upn(x) → network(x))</cell><cell>Aquaint</cell></row><row><cell cols="2">232.6 ∀x(independence air(x) → airline(x))</cell><cell>Aquaint</cell></row><row><cell cols="2">232.6 ∀x(jetblue(x) → airline(x))</cell><cell>Aquaint</cell></row><row><cell cols="2">255.5 ∀x(imperial(x) → company(x))</cell><cell>Web</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Johan Bos is supported by a <rs type="funder">"Rientro dei Cervelli</rs>" grant (<rs type="person">Italian Ministry</rs> for Research).</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,313.20,410.13,226.80,8.64;6,323.16,421.09,216.84,8.64;6,323.16,432.05,216.84,8.64;6,323.16,442.83,216.84,8.82;6,323.16,453.79,203.09,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,467.38,421.09,72.62,8.64;6,323.16,432.05,157.22,8.64">Question Answering with QED and Wee at TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Smillie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
		<idno>TREC-2004</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,503.11,432.05,36.89,8.64;6,323.16,443.01,54.32,8.64;6,419.80,442.83,120.20,8.59;6,323.16,453.79,43.59,8.59">The Thirteenth Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Voorhees and Buckland</note>
</biblStruct>

<biblStruct coords="6,313.20,473.00,226.80,8.64;6,323.16,483.96,216.84,8.64;6,323.16,494.74,216.84,8.82;6,323.16,505.70,216.84,8.59;6,323.16,516.84,100.06,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,397.56,483.96,142.43,8.64;6,323.16,494.92,46.47,8.64">Question Answering with QED at TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,390.92,494.92,93.58,8.64;6,525.06,494.74,14.94,8.59;6,323.16,505.70,211.83,8.59">The Fourteenth Text Retrieval Conference, TREC-2005</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Voorhees and Buckland</note>
</biblStruct>

<biblStruct coords="6,313.20,535.87,226.80,8.64;6,323.16,546.83,216.84,8.64;6,323.16,557.61,216.83,8.82;6,323.16,568.57,216.84,8.59;6,323.16,579.53,173.82,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,370.55,546.83,169.44,8.64;6,323.16,557.79,79.09,8.64">Wide-Coverage Semantic Representations from a CCG Parser</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,426.10,557.61,113.89,8.59;6,323.16,568.57,216.84,8.59;6,323.16,579.53,57.61,8.59">Proceedings of the 20th International Conference on Computational Linguistics (COLING &apos;04)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING &apos;04)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,598.74,226.80,8.64;6,323.16,609.52,216.84,8.82;6,323.16,620.48,216.84,8.82;6,323.16,631.61,22.42,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,362.30,598.74,177.70,8.64;6,323.16,609.70,14.39,8.64">Towards wide-coverage semantic interpretation</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,358.30,609.52,181.70,8.59;6,323.16,620.48,152.57,8.59">Proceedings of Sixth International Workshop on Computational Semantics IWCS-6</title>
		<meeting>Sixth International Workshop on Computational Semantics IWCS-6</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,313.20,650.65,226.80,8.64;6,323.16,661.61,216.84,8.64;6,323.16,672.39,216.84,8.59;6,323.16,683.52,100.06,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,370.26,650.65,169.74,8.64;6,323.16,661.61,93.58,8.64">Question Answering System at TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,323.16,672.39,211.83,8.59">The Fifteenth Text REtrieval Conference, TREC-2006</title>
		<editor>
			<persName><surname>Voorhees</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>The &quot;La Sapienza</note>
</biblStruct>

<biblStruct coords="6,313.20,702.56,226.80,8.64;6,323.16,713.33,216.84,8.82;7,81.96,75.30,216.84,8.59;7,81.96,86.26,187.05,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,423.57,702.56,116.43,8.64;6,323.16,713.51,94.37,8.64">Parsing the WSJ using CCG and Log-Linear Models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,440.50,713.33,99.51,8.59;7,81.96,75.30,216.84,8.59;7,81.96,86.26,85.28,8.59">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,105.19,226.80,8.82;7,81.96,116.14,131.43,8.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,166.55,105.19,132.25,8.59;7,81.96,116.14,36.16,8.59">WordNet. An Electronic Lexical Database</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,135.25,226.80,8.64;7,81.96,146.21,216.84,8.64;7,81.96,157.17,216.84,8.64;7,81.96,168.13,22.42,8.64" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,129.09,135.25,169.71,8.64;7,81.96,146.21,212.81,8.64">Riconoscimento automatico di nomi propri: un&apos;applicazione al caso dei titoli cinematografici</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Guzzetti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Rome &quot;La Sapienza</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="7,72.00,187.06,226.80,8.64;7,81.96,197.84,216.84,8.82;7,81.96,208.80,216.84,8.59;7,81.96,219.76,105.97,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,126.19,187.06,172.61,8.64;7,81.96,198.02,70.50,8.64">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,175.54,197.84,123.26,8.59;7,81.96,208.80,216.84,8.59;7,81.96,219.76,13.06,8.59">Proceedings of the Fourteenth International Conference on Computational Linguistics</title>
		<meeting>the Fourteenth International Conference on Computational Linguistics<address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,238.68,226.79,8.82;7,81.96,249.64,216.84,8.59;7,81.96,260.60,216.84,8.82;7,81.96,271.74,52.29,8.64" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Reyle</surname></persName>
		</author>
		<title level="m" coord="7,177.50,238.68,121.29,8.59;7,81.96,249.64,216.84,8.59;7,81.96,260.60,144.37,8.59">From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT</title>
		<imprint>
			<publisher>Kluwer, Dordrecht</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,290.67,226.80,8.64;7,81.96,301.45,216.84,8.82;7,81.96,312.41,105.43,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,191.46,290.67,107.34,8.64;7,81.96,301.63,112.61,8.64">Unsupervised multilingual sentence boundary detection</title>
		<author>
			<persName coords=""><forename type="first">Tibor</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Strunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,203.01,301.45,95.79,8.59;7,81.96,312.41,13.06,8.59">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="525" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,331.52,226.80,8.64;7,81.96,342.48,216.84,8.64;7,81.96,353.43,216.84,8.64;7,81.96,364.21,216.84,8.82;7,81.96,375.17,216.84,8.82;7,81.96,386.31,43.45,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,260.53,342.48,38.27,8.64;7,81.96,353.43,216.84,8.64;7,81.96,364.39,17.93,8.64">The QED Open-Domain Answer Retrieval System for TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
		<idno>TREC-2004</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,119.59,364.39,92.81,8.64;7,252.01,364.21,46.79,8.59;7,81.96,375.17,102.00,8.59">The Twelfth Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Voorhees and Buckland</note>
</biblStruct>

<biblStruct coords="7,72.00,405.24,226.80,8.64;7,81.96,416.20,216.84,8.64;7,81.96,426.98,216.84,8.82;7,81.96,438.12,86.61,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,194.63,405.24,104.17,8.64;7,81.96,416.20,216.84,8.64;7,81.96,427.16,24.02,8.64">Combining the Language Model and Inference Network Approaches to Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,122.11,426.98,171.75,8.59">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,457.05,226.80,8.64;7,81.96,468.00,216.84,8.64;7,81.96,478.96,216.84,8.64;7,81.96,489.74,216.84,8.59;7,81.96,500.70,216.84,8.82;7,81.96,511.84,22.42,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,195.28,468.00,103.52,8.64;7,81.96,478.96,199.86,8.64">Using nomlex to produce nominalization patterns for information extraction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yangarber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,81.96,489.74,216.84,8.59;7,81.96,500.70,129.81,8.59">Coling-ACL98 workshop Proceedings, The Computational Treatment of Nominals</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,530.77,226.80,8.64;7,81.96,541.55,216.83,8.82;7,81.96,552.51,117.05,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,229.09,530.77,69.71,8.64;7,81.96,541.73,112.15,8.64">Applied morphological processing of english</title>
		<author>
			<persName coords=""><forename type="first">Carroll</forename><forename type="middle">J</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,201.65,541.55,97.15,8.59;7,81.96,552.51,28.81,8.59">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
