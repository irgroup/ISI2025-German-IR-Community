<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,67.04,72.06,474.34,12.90">On Retrieving Legal Files: Shortening Documents and Weeding Out Garbage</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,233.25,93.02,45.52,8.96"><forename type="first">Scott</forename><surname>Kulp</surname></persName>
							<email>sckulp@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Ursinus College Collegeville PA</orgName>
								<address>
									<postCode>19426</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.81,93.02,78.93,8.96"><forename type="first">April</forename><surname>Kontostathis</surname></persName>
							<email>akontostathis@ursinus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Ursinus College Collegeville PA</orgName>
								<address>
									<postCode>19426</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,67.04,72.06,474.34,12.90">On Retrieving Legal Files: Shortening Documents and Weeding Out Garbage</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A05CAF8408D62108620810678C81C530</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the TREC Legal experiments in 2007. We have applied novel normalization techniques that are designed to slightly favor longer documents instead of assuming that all documents should have equal weight. We have also developed a new method for reformulating query text when background information is provided with an information request. We have also experimented with using enhanced OCR error detection to reduce the size of the term list and remove noise in the data. In this article, we discuss the impact of these effects on the TREC 2007 data sets. We show that the use of simple normalization methods significantly outperforms cosine normalization in the legal domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the problems people in the legal profession face is the discovery of relevant documentation. When preparing cases for trial, law firms must search through hundreds of thousands of documents in order to find the most pertinent information in support of their case. In the legal domain, recall is considered to be more important than precision, but an increase in precision at top ranks would prevent time wasted on irrelevant materials.</p><p>In response to this need, the Text Retrieval Conference (TREC) started a new "Legal" track in 2006. The IIT Complex Document Information Processing test collection was used as the data set for the 2006 and 2007 competitions. This collection consists of roughly 7 million documents (approximately 57 GB of uncompressed text) taken from the Legacy Tobacco Document Library hosted by the University of California at San Francisco. These documents were made public during various legal cases involving US tobacco companies as part of the settlement agreement.</p><p>The documents in the TREC Legal corpus have widely disparate lengths, from a few sentences to hundreds of pages. Our primary experiments looked at the impact that document normalization has on this type of corpus. The normalization techniques we used are described in Section 2.3. We believe that the normalization studies we have done can be generalized to other collections with widely varying lengths. In addition to a standard query, the TREC Legal topics include background information which can be helpful for improving search. We have devised a method for using this background information to better identify which query terms serve as the best discriminators. These techniques are described in Section 2.4. We also experimented with using OCR error detection algorithms to reduce the size of the term list and reduce noise in the data, as described in Section 2.2. The results of our studies appear in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section we describe our search and retrieval platform and discuss the methodologies used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Information Retrieval System</head><p>The search and retrieval system we used for this project is custom-built. The TREC Legal data set is large, and generating and storing a single term-by-document matrix for the entire index was infeasible with the resources available. We developed a way to split up the data set so that our system could index and run queries on each piece separately, but produce results as if the system processed the entire set at once. The approach mirrors generalized vector space retrieval. We had intended to use the log-entropy weighting scheme <ref type="bibr" coords="1,342.26,654.38,11.62,8.64" target="#b5">[6]</ref> for term weighting. However, we discovered a programming error after our runs were submitted and we ended up using only local weighting (log tf) for all runs. We compared log tf to log entropy in subsequent experiments and the precision and recall numbers are almost identical, which is very interesting. More analysis is needed to determine the effect of term weighting with power normalization, but this is left as future work. We used the following algorithm to index TREC Legal:</p><p>• Split up the original data set into some number of smaller pieces; indexing TREC Legal on a machine with 8 GB of RAM required us to split it up into 81 subsets, each approximately 700-750 MB in size.</p><p>• Index each subset. For each subset, we removed terms that appear less than six times from the index. The documents were also processed using Optical Character Recognition (OCR) software. We applied the OCR error detection algorithm described in <ref type="bibr" coords="2,249.12,131.27,10.58,8.64" target="#b8">[9]</ref>, supplemented by some additional rules, to reduce the size of the term list. Table <ref type="table" coords="2,139.05,143.22,4.98,8.64" target="#tab_0">1</ref> shows the number of terms pruned with and without OCR error detection.</p><p>• Loop through each sub-index file, keeping a running total of all the global term frequencies. After all the global frequencies of the terms in each of the sub-indices are found, the global term weights could be calculated (but this step was inadvertently omitted in our experiments as explained above). Given enough memory resources on the computer, this step can be combined with the previous step.</p><p>• With the global term weight list loaded, each sub-index is again reloaded separately. The local weight (and global weight, if applicable) is then applied to each entry in the term-by-document matrix. The sub-indices are then re-saved.</p><p>After the data set is indexed, a list of queries is processed by running the queries separately against each sub-index and keeping a running list of the top-scoring documents for each query. In our experiments, the normalization of documents was done at query run time, because we changed it with each run, but normally normalization would be applied during the term weighting step above.</p><p>Our test computer has an Intel Core 2 Quad processor at 2.4 GHz with 8 GB of system RAM and is running Windows XP x64 Professional. Given some parallelization, it takes about 6-8 hours to index TREC Legal, 2 hours to process queries on the entire data set, and 45 minutes to run queries on just the documents that have been judged for relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">OCR Error Detection</head><p>The documents in the TREC Legal data set were scanned in using Optical Character Recognition (OCR) software. However, OCR technology is imperfect and often creates errors within the documents. For example, the word "wonderful" may be mistakingly read as "wonolerful" by the OCR software. Sometimes, such as when it comes across a graphic image, the OCR software generates garbage strings. Both mistakes can adversely affect the weighting of terms in the documents, as well as make the size of the index much larger than it should be.</p><p>To help alleviate this problem, our system can automatically detect OCR errors and remove them. We began by mimicking the garbage detection rules found in the rmgarbage system <ref type="bibr" coords="2,354.05,467.38,10.58,8.64" target="#b8">[9]</ref>, and then added additional rules in order to find more items. The changes we have made to rmgarbage were done in order to remove terms more liberally, thus shrinking the index even more. We used the following rules to decide if a string is garbage (an example follows each rule):</p><p>• If a string is more than 20 characters in length, it is garbage. This rule was taken from rmgarbage, but shortened from 40 to 20 characters.</p><p>Example: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii...</p><p>• If the number of punctuation characters in a string is greater than the number of alphanumeric characters, it is garbage. This rule was taken from rmgarbage.</p><p>Example: ?3//la'</p><p>• Ignoring the first and last characters in a string, if there are two or more different punctuation characters in the string, it is garbage. This rule was taken from rmgarbage.</p><p>Example: b?bl@bjk.1e.322</p><p>• If there are three or more identical characters in a row in a string, it is garbage. This rule was taken from rmgarbage, but shortened from four or more characters to three. • If the number of uppercase characters in a string is greater than the number of lowercase characters, and if the number of uppercase characters is less than the total number of characters in the string, it is garbage. This is a new rule we developed when we saw that OCR errors often created excessive numbers of uppercase characters, but normally, in English, there is usually no more than one uppercase character in a term. However, sometimes real English words appeared in all uppercase characters, which is acceptable, so words that contain only uppercase characters are not considered garbage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example: aaaaaBlE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example: BBEYaYYq</head><p>• If all the characters in a string are alphabetic, and if the number of consonants in the string is greater than 8 times the number of vowels in the string, or vice-versa, it is garbage. This rule was taken from rmgarbage, but the threshold was shortened from 10 to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example: jabwqbpP</head><p>• If there are four or more consecutive vowels in the string or five or more consecutive consonants in the string, it is garbage. This is a new rule we developed when we noticed that real English words with these traits are rare, but this property appeared often in OCR errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example: buauub</head><p>• If the first and last characters in a string are both lowercase and any other character is uppercase, it is garbage. This rule was taken from rmgarbage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example: awwgrapHic</head><p>Table <ref type="table" coords="3,111.86,427.76,4.98,8.64" target="#tab_0">1</ref> lists the number of terms pruned using three different methods. These numbers count repeated terms as well (for example, if the misspelled term "lavvyer" appears five times, it would be counted five times). The total number of terms in the entire collection is 7,358,393,244 before any pruning has been done. We estimate that an full index for TREC Legal without pruning would uses about 62 GB of hard drive space. Using OCR error detection saves about 8 GB of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document Normalization</head><p>One of the major problems in information retrieval is the way large documents have a natural advantage over shorter documents when processing queries, simply because there are more opportunities for term matches in longer documents. The goal of document normalization is to reduce this advantage so that small relevant documents have the same probability of being returned as large relevant documents. The most widely-used normalization method is cosine normalization, and we used it as the basis for comparison in our experiments. Some alternate normalization techniques include pivoted document length normalization <ref type="bibr" coords="3,312.36,582.01,10.58,8.64" target="#b6">[7]</ref>, maximum tf normalization <ref type="bibr" coords="3,441.44,582.01,10.58,8.64" target="#b5">[6]</ref>, BM25 <ref type="bibr" coords="3,487.69,582.01,10.58,8.64" target="#b4">[5]</ref>, and byte length normalization <ref type="bibr" coords="3,158.58,593.96,10.58,8.64" target="#b7">[8]</ref>. Pivoted document length normalization and BM25 have been shown to be more effective compared to cosine normalization on some collections; however, these techniques require extensive training, so they are not directly comparable to our approach (although our approach could be enhanced with pivoting). Maximum tf weighting schemes <ref type="bibr" coords="3,150.84,629.83,11.62,8.64" target="#b5">[6]</ref> use the largest term weight in a document as the normalization factor. This approach does not seem applicable to the TREC legal collection, where we have such widely differing document lengths and the risk of having outliers (terms with weights that are much higher than the norm), due to undetected OCR errors or uncommon proper names, appears to be particularly high. Byte length normalization has been shown to be effective on collections which have OCR errors corrected and on collections with simulated OCR errors <ref type="bibr" coords="3,394.30,677.65,10.58,8.64" target="#b7">[8]</ref>. This technique does not directly apply to our situation because a significant number of the OCR errors in TREC Legal were removed by our preprocessor and the remaining OCR errors are quite real, not simulated. We plan to compare our results to the byte length normalization, BM25, and pivoted document length normalization in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Cosine Normalization</head><p>The standard method of document normalization is cosine normalization. For every document, the normalization factor is calculated. The normalization factor is defined by the expression n i=1 w 2 i where w i is weight of the i th term in the document, and n is the number of unique terms in the document. The original term-document weight is divided by this normalization factor to get the final normalized term-document weight. When this is applied to every document in the collection, each document will have length of one.</p><p>The problem with cosine normalization is that it assumes that the probability of relevance is completely independent from document length. However, it is more likely that very long documents do have a slightly higher chance of being truly relevant to a query, since they have more content. To account for this, we developed document normalization schemes that bestow less of a penalty on the longest documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Log Normalization</head><p>We first use a log function to normalize documents. Let t be the total number of terms in a document. The log normalization factor is defined by the expression</p><formula xml:id="formula_0" coords="4,293.77,293.72,24.45,8.74">log(t)</formula><p>The original term-document weight is divided by the normalization factor to find the final normalized term-document weight. We chose the log function because of its slow growth as t becomes higher. This way, while all documents are shortened somewhat, very long documents are not penalized as much as shorter documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Power Normalization</head><p>We also experimented with using different powers of t as the normalization factor.</p><p>Let t be the total number of terms in a document. The square root normalization factor is defined by the expression √ t</p><p>The cube root normalization factor is defined by the expression</p><formula xml:id="formula_1" coords="4,297.94,451.96,15.62,10.31">t 1/3</formula><p>The fourth root normalization factor is defined by the expression</p><formula xml:id="formula_2" coords="4,297.94,491.35,15.62,10.31">t 1/4</formula><p>We used these powers of t for reasons similar to log normalization. These functions grow slowly with very large values of t, and so very large documents still have some advantage. The biggest difference between log normalization and power normalization is simply the rate of growth of the normalization factor functions. The power functions grow much faster than the log function, meaning the length advantage of extremely large documents is diminished more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Analysis</head><p>Early experiments showed that log normalization worked well on small datasets with homogeneous lengths, but gave too much advantage to longer documents when used for the TREC Legal experiments. Figure <ref type="figure" coords="4,444.57,605.92,4.98,8.64" target="#fig_1">1</ref> contains the data plots displaying the vector lengths of all the judged documents for the 2006 TREC Legal queries after normalization. A graph of document lengths after cosine normalization is applied would be a horizontal straight line at 1. The documents are sorted by vector length before normalization, in ascending order, so that document x in Figure <ref type="figure" coords="4,466.82,641.78,4.01,8.64" target="#fig_1">1</ref>(a) is referring to the same document as document x in Figure <ref type="figure" coords="4,253.48,653.74,3.82,8.64" target="#fig_1">1</ref>(b), as well as Figure <ref type="figure" coords="4,346.48,653.74,3.71,8.64" target="#fig_1">1</ref>(c), etc. In Figure <ref type="figure" coords="4,425.99,653.74,3.71,8.64" target="#fig_1">1</ref>(a), we can see the original document lengths, without normalization (sorted shortest document to longest document). On Figure <ref type="figure" coords="4,475.55,665.69,3.82,8.64" target="#fig_1">1</ref>(b), we can see the document lengths after log normalization is applied. The shape of the curve for log normalization is very similar to the original document length curve, meaning the log function had little effect. This prompted us to try power normalization techniques for TREC Legal. As we can see from Figures <ref type="figure" coords="4,521.47,701.56,11.12,8.64" target="#fig_1">1(c</ref>), 1(d) and 1(e), the power functions have a much greater effect on the curve. Figure <ref type="figure" coords="4,413.96,713.51,12.03,8.64" target="#fig_1">1(c</ref>   function is so heavy, the graph is actually somewhat downward-sloping. However, when using cube root or fourth root normalization, the graphs acts precisely as we intended with a very slight upward slope as original document sizes become longer. In Tables <ref type="table" coords="6,126.95,111.34,4.98,8.64" target="#tab_1">2</ref> and<ref type="table" coords="6,152.28,111.34,3.74,8.64">3</ref>, we can see the effect the normalization scheme has on the number of terms in the documents returned by our search system at top ranks for the 2006 and 2007 queries. As we expected, using no normalization results in too many large documents, which is not useful. Since log normalization does not have a very large effect on the long documents in TREC Legal, the average term length of the returned documents is within the same order of magnitude as using no normalization. Cosine normalization and square root normalization return very short documents. However, cube root normalization and fourth root normalization have average term counts that are between cosine/square root normalization and log normalization.</p><p>It is interesting to compare the 2006 numbers to the 2007 numbers. Although the trends are the same, the variation is much more pronounced for the 2006 queries. This leads us to believe that the 2007 queries are somehow different from the 2006 queries (collectively), but more analysis is needed to identify the differences. Subsequent experiments with the 2006 and 2007 queries separately have determined that retrieval performance also differs significantly <ref type="bibr" coords="6,514.98,230.89,10.58,8.64" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query Pruning</head><p>Another problem we faced in the TREC Legal project was deciding what to use as the queries. In the past, research has been done on expanding queries using such methods as thesaurus expansion <ref type="bibr" coords="6,373.67,290.05,11.62,8.64" target="#b2">[3]</ref> and automatic/manual query relevance feedback expansion <ref type="bibr" coords="6,154.46,302.00,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="6,168.38,302.00,7.19,8.64" target="#b0">1]</ref>. However, instead of expanding queries to make them longer, we wanted a way to make queries shorter and more specific.</p><p>In for each of the 46 topics in the 2006 set released by TREC legal, there is a short problem summary (the request text), and then a much longer overview of the problem. The problem summary is similar to a standard query, in that it is a simple one-sentence description of what the user wants to find. The following is an example:</p><p>All documents referencing the lobbying efforts of antismoking groups which specifically refer to their use of cartoons.</p><p>Using a standard English stop list, the terms all, the, of, which, to, their, and use are removed. However, the terms documents, referencing, specifically, and refer are jargon and are of no use when trying to do a document search. In fact, using these terms in the queries could have a negative effect on recall and precision, since documents that have matches to just those terms are probably not relevant.</p><p>To solve this problem, we needed a custom stop list to prune the legal jargon from the queries. However, given the very short length of the problem summaries, there was not enough information to automatically generate this stop list. We then decided to look at the longer problem descriptions. We used these descriptions directly as queries, but they were much too large (many were multiple pages long) and had too much extra information. They sharply lowered recall and precision rates when tested with the 2006 queries. They also are filled with technical jargon, and so we used them to automatically generate a stop list of high-frequency legal terms.</p><p>We developed a program which read all the topics. The system kept a record of the running total for each term's frequency throughout the query topics. We reviewed the list of terms that appeared more than sixty times in the query file to identify candidates for the legal term stop list, all but three of these terms were chosen for the stop list (the three that were eliminated were California, media and code -these were eliminated from the stop list because they did not appear to be legal jargon). Four additional terms were added to the legal stop list, as a result of manual analysis of the problem summary statements. The these four terms were expressly, discussing, referencing, relating. A legal term stop list consisting of 155 terms was created. In the next section we discuss our 2007 runs and see the effect of using this stop list on precision and recall rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We were restricted to eight runs for 2007. The runs are described in Table <ref type="table" coords="6,379.96,647.26,3.74,8.64" target="#tab_2">4</ref>. We selected runs that would test the effectiveness of each of the three strategies. For example, we could test the effectiveness of the various normalization strategies by comparing the results of runs 1-5. The effectiveness of automated query pruning can be gauged by comparing the manual runs 7 and 8 to query pruning runs 2 and 5. The OCR could be evaluated by comparing runs 6 and 2. The results provided by NIST are shown in Tables <ref type="table" coords="7,286.71,399.43,7.47,8.64">5,</ref><ref type="table" coords="7,296.42,399.43,3.74,8.64" target="#tab_3">6</ref>, and 7. The precision rates for power normalization runs are much higher than precision rates for the cosine normalization (baseline) run. At rank 10, there is a 415% improvement in precision when using fourth root normalization over cosine normalization, and there is a 365% improvement in precision when using cube root normalization over cosine normalization. Large improvements in precision performance can be seen in all other rankings as well, when using cube or fourth root normalization over cosine normalization. Figure <ref type="figure" coords="7,101.03,459.21,4.98,8.64" target="#fig_2">2</ref> shows the recall comparison between the cosine normalization baseline and the fourth root, cube root, and log normalization schemes. Interestingly, log normalization performed better on the 2007 queries than it did on comparable runs using the 2006 queries. We speculate that this may be related to the pooling techniques used in the evaluation process. Log normalization resulted in longer documents being retuned at top ranks, and therefore more gray documents appeared in top ranks. The estimated gray at B for log normalization was .1305 as compared with .0088 for cosine, .0157 for fourth root, and .0084 for cube root. The estimated gray at rank 5 for log was .3070, which was much larger than all other runs.</p><p>The results for Query Pruning were mixed. Generally the Query Pruning with cube root normalization (run 2) outperformed manual querying with cube root normalization (run 7), but the opposite was true when cosine normalization was used (see the data for runs 5 and 8 for cosine normalization with and without query pruning, resp.). This provides further evidence that there are subtle differences between the 2006 and 2007 queries, and they cannot be used interchangably to compare retrieval results.</p><p>As expected OCR detection had little impact on retrieval performance. A comparison of run 6 to run 2 shows that no OCR detection (run 2) has slightly higher retrieval performance in many cases. We speculate that the aggressive OCR error detection scheme may be removing true keywords unintentionally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have described experiments using several new algorithms that improve the performance of a search and retrieval system in the legal domain. Log and power normalization are promising new methods of document normalization that aim to retrieve longer, more relevant documents in the top ranks when the corpus has widely disparate lengths. Thus  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,425.99,713.51,114.02,8.64;5,97.53,165.57,2.70,5.32;5,94.83,155.49,5.39,5.32;5,92.13,145.39,8.09,5.33;5,92.13,135.31,8.09,5.33;5,92.13,125.22,8.08,5.32;5,92.13,115.14,8.08,5.32;5,92.13,105.05,8.08,5.32;5,103.84,174.08,171.92,5.32;5,82.68,141.70,5.32,8.44;5,82.68,125.51,5.32,15.03;5,176.87,184.61,23.12,5.32;5,129.06,92.08,92.78,9.61;5,151.04,203.45,71.90,6.91;5,327.32,165.60,2.70,5.32;5,324.62,155.52,5.39,5.32;5,324.62,145.43,5.39,5.32;5,324.62,135.34,5.39,5.32;5,324.62,125.25,5.39,5.32;5,324.62,115.17,5.39,5.32;5,324.62,105.08,5.39,5.32;5,333.63,174.11,174.63,5.32;5,315.18,141.72,5.32,8.44;5,315.18,125.52,5.32,15.03;5,408.03,184.63,23.10,5.33;5,362.50,92.11,91.21,9.61;5,380.50,203.45,75.44,6.91;5,96.20,304.23,2.70,5.32;5,92.18,296.66,6.74,5.32;5,92.18,289.10,6.74,5.32;5,92.18,281.53,6.74,5.32;5,92.18,273.97,6.74,5.32;5,92.18,266.41,6.74,5.32;5,92.18,258.84,6.74,5.32;5,92.18,251.27,6.74,5.32;5,92.18,243.70,6.74,5.32;5,102.51,312.74,173.28,5.32;5,82.71,280.37,5.32,8.43;5,82.71,264.19,5.32,15.01;5,176.22,323.26,23.11,5.32;5,128.71,230.72,93.92,9.62;5,134.85,342.09,101.79,6.91;5,319.64,304.24,2.70,5.32;5,319.64,292.13,2.70,5.32;5,319.64,280.02,2.70,5.32;5,319.64,267.92,2.70,5.32;5,319.64,255.82,2.70,5.32;5,319.64,243.71,2.70,5.32;5,325.94,312.75,2.70,5.32;5,343.71,312.75,159.58,5.32;5,310.20,280.36,5.32,8.44;5,310.20,264.17,5.32,15.03;5,401.70,323.27,23.08,5.32;5,342.43,230.74,118.77,9.61;5,364.78,342.09,96.92,6.91;5,94.81,442.92,2.70,5.32;5,94.81,434.26,2.70,5.32;5,94.81,425.61,2.70,5.32;5,94.81,416.95,2.70,5.33;5,94.81,408.30,2.70,5.32;5,92.11,399.65,5.39,5.32;5,92.11,390.99,5.39,5.32;5,92.11,382.34,5.39,5.32;5,101.13,451.43,174.63,5.32;5,82.68,419.04,5.32,8.44;5,82.68,402.82,5.32,15.04;5,175.52,461.96,23.12,5.32;5,118.74,369.37,112.42,9.61;5,135.35,480.73,100.79,6.91"><head></head><label></label><figDesc>) shows that the square root</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,186.30,503.58,239.41,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Effect of Normalization on Document Length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,202.86,92.33,206.29,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: TREC Legal Estimate Recall Comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,182.77,82.27,243.97,56.82"><head>Table 1 :</head><label>1</label><figDesc>Number of Terms Removed Using Pruning Methods</figDesc><table coords="3,193.09,93.80,225.83,45.29"><row><cell></cell><cell>Num Terms Removed</cell></row><row><cell>Prune &lt;= 5 (No OCR Detect)</cell><cell>454,892,023</cell></row><row><cell>Prune &lt;= 5, rmgarbage</cell><cell>3,160,618,688</cell></row><row><cell>Prune &lt;= 5, rmgarbage ext</cell><cell>3,195,632,736</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,99.00,541.86,414.01,160.65"><head>Table 2 :</head><label>2</label><figDesc>Average Number of Terms in Returned Documents -2006 queries No Norm Log Norm FrthRt Norm CubeRt Norm Sqrt Norm Cosine Norm</figDesc><table coords="5,99.00,567.73,414.01,134.78"><row><cell cols="3">Rank 10 194658.17 114085.49</cell><cell>11222.43</cell><cell>1709.70</cell><cell>362.09</cell><cell>299.68</cell></row><row><cell cols="3">Rank 20 174553.65 107315.34</cell><cell>12910.54</cell><cell>2011.27</cell><cell>434.03</cell><cell>348.11</cell></row><row><cell cols="3">Rank 30 162117.71 104196.73</cell><cell>14813.93</cell><cell>2118.35</cell><cell>458.15</cell><cell>398.22</cell></row><row><cell></cell><cell cols="5">Table 3: Average Number of Terms in Returned Documents -2007 queries</cell><cell></cell></row><row><cell></cell><cell cols="6">No Norm Log Norm FrthRt Norm CubeRt Norm Sqrt Norm Cosine Norm</cell></row><row><cell>Rank 10</cell><cell>36691.7</cell><cell>28742.4</cell><cell>15845.5</cell><cell>8186.0</cell><cell>2146.5</cell><cell>2829.7</cell></row><row><cell>Rank 20</cell><cell>29168.3</cell><cell>24734.4</cell><cell>16084.3</cell><cell>11068.2</cell><cell>3093.4</cell><cell>3797.5</cell></row><row><cell>Rank 30</cell><cell>23651.3</cell><cell>21442.8</cell><cell>16207.4</cell><cell>11049.5</cell><cell>3381.9</cell><cell>4434.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,111.18,82.27,389.64,281.96"><head>Table 4 :</head><label>4</label><figDesc>Description of submitted runs for TRECLegal 2007   </figDesc><table coords="7,111.18,93.80,389.64,270.43"><row><cell cols="9">Run Name Automatic? Normalization Query Pruning? OCR?</cell></row><row><cell></cell><cell>ursinus1</cell><cell>Yes</cell><cell></cell><cell cols="2">Fourth Root</cell><cell></cell><cell>Yes</cell><cell>No</cell></row><row><cell></cell><cell>ursinus2</cell><cell>Yes</cell><cell></cell><cell cols="2">Cube Root</cell><cell></cell><cell>Yes</cell><cell>No</cell></row><row><cell></cell><cell>ursinus3</cell><cell>Yes</cell><cell></cell><cell cols="2">Square Root</cell><cell></cell><cell>Yes</cell><cell>No</cell></row><row><cell></cell><cell>ursinus4</cell><cell>Yes</cell><cell></cell><cell>Log</cell><cell></cell><cell></cell><cell>Yes</cell><cell>No</cell></row><row><cell></cell><cell>ursinus5</cell><cell>Yes</cell><cell></cell><cell cols="2">Cosine</cell><cell></cell><cell>Yes</cell><cell>No</cell></row><row><cell></cell><cell>ursinus6</cell><cell>Yes</cell><cell></cell><cell cols="2">Cube Root</cell><cell></cell><cell>Yes</cell><cell>Yes</cell></row><row><cell></cell><cell>ursinus7</cell><cell>No</cell><cell></cell><cell cols="2">Cube Root</cell><cell></cell><cell>No</cell><cell>No</cell></row><row><cell></cell><cell>ursinus8</cell><cell>No</cell><cell></cell><cell cols="2">Cosine</cell><cell></cell><cell>No</cell><cell>No</cell></row><row><cell></cell><cell cols="8">Table 5: Actual Precision Comparisons for TREC Legal 2007</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rank</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run Name</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>100</cell><cell>200</cell><cell>500 1000 MAP</cell></row><row><cell>ursinus5</cell><cell cols="8">0.065 0.056 0.056 0.049 0.042 0.027 0.023 0.015 0.012 0.010</cell></row><row><cell>ursinus1</cell><cell cols="8">0.335 0.286 0.259 0.234 0.209 0.139 0.095 0.058 0.038 0.084</cell></row><row><cell>ursinus2</cell><cell cols="8">0.302 0.237 0.214 0.202 0.170 0.106 0.077 0.045 0.034 0.065</cell></row><row><cell>ursinus3</cell><cell cols="8">0.070 0.065 0.062 0.058 0.050 0.030 0.023 0.015 0.012 0.010</cell></row><row><cell>ursinus4</cell><cell cols="8">0.251 0.177 0.144 0.136 0.119 0.075 0.056 0.035 0.024 0.034</cell></row><row><cell>ursinus6</cell><cell cols="8">0.237 0.219 0.194 0.176 0.151 0.096 0.068 0.047 0.032 0.057</cell></row><row><cell>ursinus7</cell><cell cols="8">0.265 0.202 0.177 0.157 0.140 0.100 0.069 0.044 0.029 0.052</cell></row><row><cell>ursinus8</cell><cell cols="8">0.074 0.084 0.074 0.066 0.058 0.038 0.032 0.020 0.014 0.012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,82.61,385.30,446.78,131.34"><head>Table 6 :</head><label>6</label><figDesc>Estimated Precision Comparisons for TREC Legal 2007</figDesc><table coords="8,298.04,396.83,23.25,8.96"><row><cell>Rank</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,78.74,566.80,454.52,131.34"><head>Table 7 :</head><label>7</label><figDesc>Estimated Recall Comparisons for TREC Legal 2007 testing has been restricted to the legal domain, but we believe similar improvements would be seen in other applications.</figDesc><table coords="8,297.49,578.33,23.25,8.96"><row><cell>Rank</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,88.60,143.32,451.41,8.64;9,88.60,155.10,451.41,8.59;9,88.60,167.05,328.33,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,190.53,143.32,332.91,8.64">A User-Centred Evaluation of Ranking Algorithms for Interactive Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Efthimis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Efthimiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,88.60,155.10,451.41,8.59;9,88.60,167.05,95.31,8.59">SIGIR &apos;93: Proceedings of the 16th annual international ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="146" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.60,186.98,451.40,8.82;9,88.60,198.93,180.70,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,225.95,187.16,230.10,8.64">Search Improvement via Automatic Query Reformulation</title>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Gauch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,465.09,186.98,74.91,8.59;9,88.60,198.93,92.32,8.59">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="280" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.60,218.86,451.40,8.82;9,88.60,230.81,402.36,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,201.37,219.04,205.82,8.64">An Association Thesaurus for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,425.00,218.86,115.00,8.59;9,88.60,230.81,305.58,8.59">Proceedings of RIAO-94, 4th International Conference &quot;Recherche d&apos;Information Assistee par Ordinateur</title>
		<meeting>RIAO-94, 4th International Conference &quot;Recherche d&apos;Information Assistee par Ordinateur<address><addrLine>New York, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.60,250.92,50.09,8.64;9,157.81,250.92,382.20,8.64;9,88.60,262.87,178.52,8.64;9,282.81,262.87,257.19,8.64;9,88.60,274.83,287.55,8.64" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,157.81,250.92,382.20,8.64;9,88.60,262.87,174.40,8.64">Improving Search and Retrieval Performance through Shortening Documents, Detecting Garbage, and Throwing Out Jargon</title>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Kulp</surname></persName>
		</author>
		<ptr target="http://webpages.ursinus.edu/akontostathis/KulpHonorsThesis.pdf" />
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Ursinus College, Collegeville, PA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="9,88.60,294.75,451.41,8.64;9,88.60,306.53,229.41,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,505.10,294.75,34.90,8.64;9,88.60,306.71,22.36,8.64">Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aarron</forename><surname>Gull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marianna</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,130.93,306.53,102.74,8.59">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.60,326.64,451.40,8.64;9,88.60,338.59,173.10,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,230.69,326.64,231.85,8.64">Term Weighting Approaches in Automatic Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Ithaca, NY, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="9,88.60,358.34,451.40,8.82;9,88.60,370.29,234.07,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,296.44,358.52,165.86,8.64">Pivoted Document Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,485.04,358.34,54.95,8.59;9,88.60,370.29,150.21,8.59">Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.60,390.40,451.40,8.64;9,88.60,402.35,201.87,8.64" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,287.16,390.40,206.63,8.64">Length Normalization in Degraded Text Collections</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Ithaca, NY, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="9,88.60,422.28,451.41,8.64;9,88.60,434.05,447.71,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,340.88,422.28,199.12,8.64;9,88.60,434.23,99.85,8.64">Automatic Removal of &quot;Garbage Strings&quot; in OCR Text: An Implementation</title>
		<author>
			<persName coords=""><forename type="first">Kazem</forename><surname>Taghva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Nartker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allen</forename><surname>Condit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><surname>Borsack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.19,434.05,300.17,8.59">The 5th World Multi-Conference on Systemics, Cybernetics and Informatics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
