<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.34,70.05,307.05,18.83">Structured Queries for Legal Search</title>
				<funder ref="#_7s85QXC #_Wquf9X6 #_74HkKEa">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.71,116.40,63.44,12.55"><forename type="first">Yangbo</forename><surname>Zhu</surname></persName>
							<email>yangboz@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.16,116.40,43.86,12.55"><forename type="first">Le</forename><surname>Zhao</surname></persName>
							<email>lezhao@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.02,116.40,68.85,12.55"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.87,116.40,86.12,12.55"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.34,70.05,307.05,18.83">Structured Queries for Legal Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B577B8D2A5D92FAF5C00AB7A538FEB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports the experiments of using Indri for the main and routing (relevance feedback) tasks in the TREC 2007 Legal Track. For the main task, we analyze ranking algorithms using different fields, boolean constraints and structured operators. Evaluation results show that structured queries outperform bag-of-words ones. Boolean constraints improve both precision and recall. For the routing task, we train a linear SVM classifier for each topic. Terms with the largest weights are selected to form new queries. Both keywords and simple structured features (term.field) have been investigated. Named-Entity tags, LingPipe sentence breaker and metadata fields of the original documents are used to generate the field information. Results show that structured features and weighted queries improves retrieval, but only marginally. We also show which structures are more useful. It turns out metadata fields are not as important as we thought.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The goal of legal search is to retrieve all relevant documents for production requests. A production request describes a set of documents that the plaintiff forces the defendant to produce. The plaintiff usually forms comprehensive requests to cover large amount of documents that are potentially useful in the trial. Due to the high risk of missing important documents, legal search systems are usually recall-oriented.</p><p>Legal Track 2006 and 2007 both use the IIT CDIP collection 1 . It consists of 6,910,192 business records from US tobacco companies and research institutes. Each document contains OCR text and multiple fields of metadata, including title, authors, organizations, etc.</p><p>For the main task, 50 topics from Legal 2006 are used as 1 http://www.ir.iit.edu/projects/CDIP.html training data. For evaluation, 50 new topics are generated from four hypothetical complaints. Each topic contains detailed information about the background, instruction and negotiation history. Four fields are especially important for retrieval. First, the plaintiff describes the desired documents in RequestText (RT). Based on RT, the defendant proposes a boolean query called ProposalByDefendant (PD). Then the plaintiff modifies PD to form a new query RejoinderByPlaintiff (RP). Finally, the two parties agree on a FinalQuery (FQ), which is actually used to retrieve documents.</p><p>For the routing task, 10 topics from 2006 are adopted for evaluation. Systems take advantage of existing relevance judgements to retrieve more relevant documents. During evaluation, all previously judged documents are filtered out from the ranked lists, and performance metrics are calculated based on newly judged documents.</p><p>The rest of this paper is structured as follows. Section 2 and 3 describe our methods and experiments for the main and routing tasks. We conclude with section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MAIN TASK</head><p>This section introduces our experiments for the main task. First we describe the formation of Indri queries based on request topics. Then we compare the results of different runs using various boolean constraints and ranking functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Formulation</head><p>A typical Indri query in our experiments has two components: the boolean constraint and the ranking algorithm. For each topic, we convert FinalQuery to a boolean constraint, and combine terms from different fields for ranking. When the original query contains 'BUT NOT', the Indri query is of the general form</p><formula xml:id="formula_0" coords="1,348.25,615.83,176.22,9.41">#filrej(Z #filreq(#band(X) #combine(Y)))</formula><p>Otherwise, it's of the simpler form #filreq(#band(X) #combine(Y))</p><p>Here, X and Z are boolean constraints, and Y is the ranking component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Wildcards</head><p>Most queries in the data set contain lots of wildcards. For example, "boost!" matches all words with prefix "boost". Since the OCR texts contain errors, there are typically thousands of matches to one wildcard expression. If we directly translate these wildcards to Indri wildcards, the query execution will be very time consuming.</p><p>To speed up the retrieval, we expand the wildcards as query pre-processing. From all the terms matching a wildcard, top K words with the highest document frequencies are selected. For instance, given wildcard "multipl!", "multiple", "multiply" and "multiplicity" are selected. This method dramatically reduces the number of inverted lists to be merged at query time. Since top K frequent words cover the majority of matches, the potential loss in recall is low. However, queries expanded by this method does not perform well. One reason is that wildcards bring in noisy words. For example, in topic 52, "high!" is expanded to "highlight", "highway" and "highland". In topic 81, "bee!" is expanded to "beer", "beef", "beach", etc.</p><p>By examining the original queries, we find that most of the wildcards are unnecessary if we use stemmer during indexing. In this paper, wildcards are manually expanded to one or multiple lexically-related terms according to the topic description. A special case is for years: "198!" is expanded to "#syn <ref type="bibr" coords="2,75.59,339.08,83.85,9.41">(1980, 1981, ..., 1989</ref>)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Boolean Constraints</head><p>Each topic in the Legal Track provides three boolean queries: ProposalByDefendant, RejoinderByPlaintiff and FinalQuery. They contain boolean and proximity operators. We build a parser to parse original queries into trees, and then convert the trees to Indri queries. Table <ref type="table" coords="2,185.31,420.22,4.61,9.41" target="#tab_0">1</ref> shows the basic mapping rules, similar to those used in <ref type="bibr" coords="2,177.04,430.68,9.20,9.41" target="#b2">[2]</ref>. For example, an original query "((a OR "b c") AND d BUT NOT e)" is converted to Indri query #filrej(e #filreq(#band(#syn(a #1(b c)) d) #combine(. . . ))). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Ranking</head><p>Suppose the original query is "((a OR "b c") AND d)". The corresponding bag-of-words ranking component would be "#combine(a b c d)".</p><p>If the phrase operators are respected, the ranking component becomes "#combine(a #1(b c) d)". Since a phrase usually has much higher IDF than any of its composing terms, the phrase has high impact on ranking.</p><p>If we view terms connected by 'OR' as synonyms, the ranking component becomes "#combine(#syn(a #1(b c)) d)". For example, in topic 6, "TV", "television" and "cable" are synonyms. They may have different document frequencies, but they are treated as the same word after applying the #syn operator. The TF of a synonym set is the sum of TF of all its members, and the DF of a synonym set is the number of documents containing any of its members.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiments</head><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Submitted runs</head><p>Nine runs are produced for 2007 queries, eight of them are submitted for official pooling and evaluation. 25000 results are produced for each query.</p><p>CMUL07STD is the standard condition run required by Legal Track. It takes the keywords in RequestText to form a bag-of-words query using the "#combine" operator in Indri. Common query headers (e.g. "Please produce any and all documents that discuss") that are not meaningful to the topic are removed.</p><p>CMUL07O1 is an Okapi ranked list using terms in Final-Query. The parameters of BM25 function are the same with those used in <ref type="bibr" coords="2,374.58,491.66,9.20,9.41" target="#b4">[4]</ref>: k1 = 1.2, k2 = 0, k3 = 8 and b = 0.75. CMUL07O3 is the same with CMUL07O1 except that it combines terms from three fields: ProposalByDefendant, Re-joinderByPlaintiff and FinalQuery.</p><p>CMUL07IRT is the bag-of-words query using keywords from FinalQuery. It ignores boolean constraints. CMUL07IBT is the same with CMUL07IRT except that it uses boolean constraints to filter ranked lists. If the filtered list has less than 25000 results, top ranked results from CMUL07IRT are appended to the end of the list. Duplicate documents are removed.</p><p>CMUL07IRP is the same with CMUL07IRT except that it respects phrase operators in ranking. This run is not submitted because each group can submit up to eight runs. CMUL07IBP is the same with CMUL07IRP except that it uses boolean constraints. It appends list with results from CMUL07IRP.</p><p>CMUL07IRS is the same with CMUL07IRP except that it treats all terms connected by "OR" as synonyms. CMUL07IBS is the same with CMUL07IRS except that it uses boolean constraints. It appends list with results from CMUL07IRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Evaluation Results</head><p>The legal community is more interested in recall than precision. Legal 2007 takes a novel sampling method (the L07 method) to support deep pooling. Systems are required to return 25000 documents for each query. The sampling probability of a document is inversely proportional to its highest rank in all submitted runs.</p><p>Table <ref type="table" coords="3,78.63,197.27,4.61,9.41" target="#tab_2">2</ref> shows the evaluation results on 43 topics from Legal 2007. RefL07B is a reference boolean run provided by the organizers. Median is the median value over all 70 submitted runs, and Max is the maximum value. The reference run strictly follows the FinalQuery and provides a B value for each query. The B value is the number of documents matching FinalQuery. According to the sampling method, the organizers recommend estRB as the primary measure, which is the estimated Recall@B. We use estPB (estimated precision at B) as an auxiliary metric.</p><p>JudgedB is the judged documents at B. Okapi based runs have much more documents judged than Indri based ones. We suspect it is because most participating groups are using Okapi ranking, as the case in last year. If there are large amount of similar runs, the L07 method tends to sample more documents from those runs.</p><p>As last year, the reference run still outperforms all submitted runs. Among our nine runs, IBS is the best performing run in terms of both estRB and estPB. Comparing O1 and O3, using three fields does help. Since the FinalQuery usually covers all terms mentioned in ProposalByDefendant and Re-joinderByPlaintiff, the improvement primarily comes from better term weighting. Comparing IBT and IRT, boolean filters significantly improve performance. Comparing IBT and IBP, using phrase operators alone actually hurts performance a little bit. Comparing IBT and IBS, synonym operators improve both precision and recall. Since Legal 2006 evaluation method does not support estimated metrics, we use traditional metrics. Okapi ranking using three fields achieves the highest R@B and MAP. Since the sampling methods adopted by Legal 2006 and 2007 are vastly different <ref type="bibr" coords="3,380.63,108.44,9.20,9.41" target="#b1">[1]</ref>, the comparison should be taken with a grain of salt.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Error Analysis</head><p>Table <ref type="table" coords="3,342.03,646.92,4.61,9.41" target="#tab_4">4</ref> lists six topics on which RefL07B and CMUL07IBS behave most differently. There are three major reasons for the performance gap: chained proximity, wildcards and estimation error.</p><p>We manually expand wildcards with relevant words, and use Porter stemmer to aggregate words with the same root. This Intuitively, we expand "cry!" to "cry". Comparing the results of RefL07B and CMUL07IBS, the latter misses two relevant documents: "gar43d00" and "brq10e00", neither of which contains any form of "cry" or "tear". However, they contain non-relevant terms such as "crystalline" and "cryptococcus", and hit the relevant documents. Some final queries contain the "chained proximity" operators: "x W/k1 y W/k2 z", which requires the same occurrence of word "y" to satisfy "x W/k1 y" and "y W/k2 z". One possible approximation in Indri is #band(#uw(k1 + 2)(x y) #uw(k2 + 2)(y z) #uw(k1 + k2 + 3)(x y z))</p><p>However, the "y" in the Indri expressions could be different occurrences of the same word. Therefore, the Indri expression relaxes the original constraint. For topics 58 and 61, the relaxation hurts estRB, but for topic 57 and 59, it improves estRB.</p><p>In the L07 evaluation method <ref type="bibr" coords="4,181.78,586.18,9.20,9.41" target="#b3">[3]</ref>, the number of relevant documents is estimated as</p><formula xml:id="formula_1" coords="4,145.65,611.70,53.71,27.87">estR = n X i=1 1 pi</formula><p>where n is number of judged relevant documents, and pi is the sampling probability of document i. Although the estimator is unbiased, it has high variance when n is small. It is often dominated by relevant documents with low sampling probability. For topic 60, six documents are judged as relevant for both RefL07B and CMUL07IBS at B (1496). Between the two sets of relevant documents, only one is differ-ent. RefL07B gets "chg09d00" (p = 1.0), while CMUL07IBS gets "ake51c00" (p = 0.02). The latter gets much higher es-tRB because of a single document with low sampling probability. For topic 75, the number of judged relevant documents for RefL07B and CMUL07IBS at B (788) are three and eleven, respectively. One of the three documents by RefL07B ("twh67c00") has p = 0.03, while all eleven documents by CMUL07IBS has p = 1.0. Consequently, RefL07B achieves much higher estRB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ROUTING TASK</head><p>In the routing task, 39 topics from last year's (2006) main task with their judgments were used to simulate the routing task scenario where the system is given the information need and judgments of some of the documents returned from an initial retrieval. The routing task could be seen as a relevance feedback task, as true relevance information is known for some documents. In the experiments, at training stage, two thirds of the judged documents from 2006 were used as training while one third were used as validation data. For all 39 topics with judgments, 2 of them have only 2 judged relevant documents which were backed off to the original queries, instead of the feedback queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SVM Based Feedback</head><p>Given enough training documents, the relevance feedback task could be formulated as a supervised document classification problem, disregarding the original query completely.</p><p>In fact, each query has on average over 100 positive training examples within over 800 total training samples. Thus, treating the retrieval for each query as a binary classification task allows us to apply state of the art classification algorithms to solve the problem. As long as the document collection does not change, discarding the original query and using only the training documents would still give reliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Feature Selection for Classification</head><p>In text classification, it is well known that feature selection improves classification accuracy <ref type="bibr" coords="4,449.75,566.00,9.20,9.41" target="#b5">[5]</ref>. We try to investigate whether feature selection helps in this noisy OCR corpus for improving precision and recall. In our experiments, top 400 terms that have the highest correlations with relevance are the selected features for classification. The correlation is measured by Information Gain as in <ref type="bibr" coords="4,475.02,618.30,9.20,9.41" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Term Selection &amp; Expansion</head><p>Next, a linear SVM classifier<ref type="foot" coords="4,432.94,651.38,3.65,6.28" target="#foot_1">3</ref> with all default parameters is trained to obtain a linear classification model, which is just a weight vector for linear combination of feature values. The final list of words/features that corresponds to the highest SVM weights are selected and used to construct a new query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">SVM Classification using Indri Queries</head><p>Since the SVM term selection and feedback retrieval is just trying to use a query to approximate an SVM, we could approximate even better than just un-weighted keyword queries. When plugging in the weights learnt with SVM into the #weight operator of Indri query language<ref type="foot" coords="5,227.35,107.67,3.65,6.28" target="#foot_2">4</ref> , when the feature values of the training samples are just TFIDF scores similar to that of the retrieval model, the resulting weighted term query will be effectively classifying and ranking each document according to the linear SVM classifier.</p><p>Effectively, the retrieval system is using the inverted index to help classify and rank documents. How is this possible? Linear SVM classifier based on TFIDF features is:</p><formula xml:id="formula_2" coords="5,96.48,202.96,153.75,27.62">ScoreSVM(D| w) = X t i ∈V wi * tfidf(ti, D)</formula><p>where the w's are SVM weights learnt from training data, V is the set of all terms in the collection. The evaluation of a weighted sum operator of the Indri query language on the document D is just</p><formula xml:id="formula_3" coords="5,100.12,280.68,146.47,27.47">ScoreLM(q, D) = X t i ∈q wi * logP (ti|D)</formula><p>where q = #weight(w1t1, .., wN tN )</p><p>As long as the term feature values are similar during training and at retrieval time, the weights learnt from a SVM model should be helpful to other similar retrieval models as well.</p><p>In the experiments, for the term feature values at training time, we used</p><formula xml:id="formula_4" coords="5,72.38,406.62,203.14,21.30">tft,D * (1 + k1) tft,D + k1 * (1 -b + b * len(D)/avgLen) * log( N dft )</formula><p>which is a variant of the Okapi BM25 formula with a modified df component so that there are never negative weights. At retrieval time, the weighted retrieval in Indri uses Dirichlet smoothing to generate term tfidf scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simple Structured Features</head><p>Besides keywords, words that appeared in a particular metadata field or Named-Entity (NE) annotation could also be used to enlarge the feature space, so that the classifier could pick up features that predict relevance better than simple keywords. This new term consisting of the term + its field information can also be used in document retrieval (in Indri query language, simply "term.field", for example, "bush.person" where person is a named-entity tag which helps disambiguate "bush"). In the experiments BBN's Named-Entity tagger -Identifinder and the LingPipe<ref type="foot" coords="5,251.04,596.45,3.65,6.28" target="#foot_3">5</ref> sentence breaker are used for generating additional annotations. The metadata fields are also included in generating term.field features.</p><p>One interesting thing to know is whether annotations would help retrieval. We investigate whether and how many of these structured features have been weighted highly by the SVM learner (linear SVM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments</head><p>For different feedback retrieval algorithms, we report results comparing SVM term selection &amp; expansion method (denoted as ex), SVM approximation using weighted query with only positive weight features (wq) and weighted query of positive and negative weight features (wqn). For different feature sets, we compare unstructured keyword features (kw) with structured term.field features (f). For unweighted term query (ex) we used top 40 high weight terms from SVM model. For wq and wqn runs, we used top 70 positive weight terms and top 100 terms with highest absolute weights respectively. These parameters are trained on the judged documents from Legal 2006. As the vocabulary of the OCR collection is noisy and huge, words with DF smaller than 50 have been discarded. Also, because some of the training documents are huge, only the first 40,000 "term.field" features have been included for each document and further reduced to 400 such structured features that are highly correlated with relevance for each query, shared among the training documents.</p><p>To compare the effects of different evaluation metrics, MAP, RecallB and est RB are used in evaluation. Before evaluating these routing runs, all documents used in creating feedback queries, are excluded from the final result set. These documents are the judged relevant and non-relevant documents from TREC Legal Track 2006. The newly assessed ones have been used to evaluate the effectiveness of the routing task methods.</p><p>Table <ref type="table" coords="5,342.46,370.46,4.61,9.41" target="#tab_5">5</ref> shows the evaluation scores for the different methods as evaluated on the TREC 2006 Legal Track assessments. In TREC Legal Track 2006, 39 topics have been evaluated, and on average each topic has over 100 judged relevant documents within about 800 total judged documents. We randomly splitted the data and used 2/3 as training, 1/3 for evaluation. Results show that 1) same as in the main task, using boolean filter helps improve retrieval effectiveness, 2) using weight is better than unweighted term expansion and 3) although lots of the expansion terms are structured (see Table <ref type="table" coords="5,406.56,475.07,3.58,9.41" target="#tab_7">7</ref>), the increase in retrieval effectiveness is only marginal -maybe these "term.field" structures are not complex/precise enough to be more accurate than keywords as indications of relevance.   In table 7, we summarize on average the percentage of all types of features being selected within top 100 as given by the weights from SVM. Although "term.field" structured features constitute slightly more than half of the high weight terms, the increase in effectiveness of the structured feedback runs are only marginally better than keywords only. More accurate structures are yet to be found. Contradictory to our intuition, according to both table 5 and 6, the metadata fields only increased precision (MAP) a bit, but had no effect or even decreased recall (Recall@B or est RB). Maybe for human lawyers, it is simpler and more maintainable to use the metadata field in helping retrieval, but for the routing task, machine can do better with much more field information from Named-Entity fields etc..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>This paper reports our experiments using Indri structured queries to retrieve legal documents in TREC Legal 2007. Legal search is special in that it is more concerned with recall at deep cut-off point. This is because lawyers usually go through thousands of documents. Finding or missing an important document may have high impact on the result of the trial.</p><p>In the main task, we compare runs with or without boolean constraints, and runs using different fields of legal requests. We study the impact of phrase and proximity operators. We treat "OR" connected words as synonyms. Experimental results show that imposing boolean constraints improves both precision and recall. Combining multiple fields gets better term weights. Structured queries significantly outperform bag-of-words ones.</p><p>In the routing task, as compared to the baseline of simple queries of combined keywords, weighted term queries and simple structured queries help retrieval only marginally. Also, more than half of the SVM selected terms are structured. Named-Entity and sentence fields appear far more often in the SVM high weight features than do metadata fields. Given the performance, more accurate structured features need to be designed in order to show a more significant improvement over simple keyword feedback queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,316.81,602.04,239.11,9.41;3,316.81,612.50,83.66,9.41"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Main task: difference from median estRB of 39 manual runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,53.80,211.10,239.11,9.41;4,53.80,221.56,83.66,9.41"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Main task: difference from median estPB of 39 manual runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,494.13,239.10,107.71"><head>Table 1 : Mapping from original operators to Indri operators</head><label>1</label><figDesc></figDesc><table coords="2,91.12,516.61,163.33,85.23"><row><cell>Original expression</cell><cell>Indri expression</cell></row><row><cell>"x y"</cell><cell>#1(x y)</cell></row><row><cell>x W/k y</cell><cell>#uw(k+2)(x y)</cell></row><row><cell>x OR y</cell><cell>#syn(x y)</cell></row><row><cell>x AND y (inner)</cell><cell>#uw(x y)</cell></row><row><cell cols="2">x AND y (outermost) #band(x y)</cell></row><row><cell>x BUT NOT z</cell><cell>#filrej(z x)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.80,56.14,502.10,658.70"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="3,53.80,520.35,239.14,194.49"><row><cell cols="5">: Performance on 43 topics of Legal 2007,</cell></row><row><cell cols="5">with estRB as the primary measure. (*IRP is not</cell></row><row><cell>submitted)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="4">judgedB estRB estPB estR25K</cell></row><row><cell>RefL07B</cell><cell>108</cell><cell>0.216</cell><cell>0.292</cell><cell>-</cell></row><row><cell>Max</cell><cell>158</cell><cell>0.216</cell><cell>0.292</cell><cell>0.470</cell></row><row><cell>Median</cell><cell>122</cell><cell>0.132</cell><cell>0.207</cell><cell>0.317</cell></row><row><cell>STD</cell><cell>138</cell><cell>0.123</cell><cell>0.191</cell><cell>0.314</cell></row><row><cell>O1</cell><cell>145</cell><cell>0.152</cell><cell>0.204</cell><cell>0.361</cell></row><row><cell>O3</cell><cell>152</cell><cell>0.170</cell><cell>0.236</cell><cell>0.400</cell></row><row><cell>IRT</cell><cell>136</cell><cell>0.132</cell><cell>0.189</cell><cell>0.295</cell></row><row><cell>IRP*</cell><cell>130</cell><cell>0.138</cell><cell>0.188</cell><cell>0.291</cell></row><row><cell>IRS</cell><cell>126</cell><cell>0.194</cell><cell>0.242</cell><cell>0.395</cell></row><row><cell>IBT</cell><cell>118</cell><cell>0.187</cell><cell>0.261</cell><cell>0.391</cell></row><row><cell>IBP</cell><cell>114</cell><cell>0.183</cell><cell>0.252</cell><cell>0.392</cell></row><row><cell>IBS</cell><cell>117</cell><cell cols="2">0.208 0.267</cell><cell>0.392</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,316.81,148.68,239.13,284.22"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="3,316.81,148.68,239.13,197.08"><row><cell cols="5">: Performance in Legal 2006 and 2007, using</cell></row><row><cell cols="5">traditional metrics. (*IRP is not submitted)</cell></row><row><cell></cell><cell cols="2">2006</cell><cell cols="2">2007</cell></row><row><cell>Method</cell><cell>R@B</cell><cell>MAP</cell><cell>R@B</cell><cell>MAP</cell></row><row><cell cols="2">RefL07B 0.525</cell><cell>-</cell><cell>0.486</cell><cell>-</cell></row><row><cell>Max</cell><cell>-</cell><cell>-</cell><cell>0.609</cell><cell>0.172</cell></row><row><cell>Median</cell><cell>-</cell><cell>-</cell><cell>0.452</cell><cell>0.092</cell></row><row><cell>STD</cell><cell>0.469</cell><cell>0.084</cell><cell>0.485</cell><cell>0.115</cell></row><row><cell>O1</cell><cell>0.512</cell><cell>0.083</cell><cell>0.524</cell><cell>0.127</cell></row><row><cell>O3</cell><cell cols="4">0.580 0.098 0.568 0.142</cell></row><row><cell>IRT</cell><cell>0.487</cell><cell>0.074</cell><cell>0.480</cell><cell>0.115</cell></row><row><cell>IRP*</cell><cell>0.474</cell><cell>0.068</cell><cell>0.468</cell><cell>0.110</cell></row><row><cell>IRS</cell><cell>0.520</cell><cell>0.087</cell><cell>0.510</cell><cell>0.094</cell></row><row><cell>IBT</cell><cell>0.532</cell><cell>0.087</cell><cell>0.506</cell><cell>0.128</cell></row><row><cell>IBP</cell><cell>0.525</cell><cell>0.082</cell><cell>0.489</cell><cell>0.127</cell></row><row><cell>IBS</cell><cell>0.488</cell><cell>0.091</cell><cell>0.504</cell><cell>0.108</cell></row></table><note coords="3,316.81,360.72,239.10,9.41;3,316.81,371.18,239.07,9.41;3,316.81,381.64,239.08,9.41;3,316.81,392.10,239.10,9.41;3,316.81,402.57,239.07,9.41;3,316.81,413.03,239.09,9.41;3,316.81,423.49,122.05,9.41"><p><p><p><p><p>Figure</p>1</p>compares per-topic estRB between CMUL07IBS and median performance of 39 manual runs from all the groups. 30 out of 43 queries performs better than median, and four of them (60, 71, 84 and 97) achieve the highest estRB among all runs. Figure</p>2</p>compares per-topic estPB. 30 queries are above the median, and two of them (60 and 96) achieve the highest estPB.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,316.81,168.65,240.04,71.05"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="4,316.81,168.65,240.04,71.05"><row><cell></cell><cell cols="6">: Topics on which RefL07B and CMUL07IBS</cell></row><row><cell cols="6">perform most differently (measured in estRB)</cell><cell></cell></row><row><cell>Topics</cell><cell>58</cell><cell>59</cell><cell>60</cell><cell>61</cell><cell>65</cell><cell>72</cell></row><row><cell cols="7">RefL07B 0.940 0.009 0.072 0.439 0.672 0.779</cell></row><row><cell>IBS</cell><cell cols="6">0.424 0.410 0.706 0.048 0.962 0.304</cell></row><row><cell>Diff</cell><cell cols="6">-0.516 0.401 0.634 -0.391 0.290 -0.475</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,316.81,538.83,239.15,176.01"><head>Table 5 :</head><label>5</label><figDesc>Performance on 39 topics of Legal 2006Table6evaluates the routing task methods on the newly judged 10 topics out of all 39 topics of Legal 2006.</figDesc><table coords="5,316.81,549.29,239.15,165.55"><row><cell cols="5">Routing task, with 2/3 documents for training,</cell></row><row><cell cols="5">1/3 for validation. kw: keyword feature only, f:</cell></row><row><cell cols="5">NE+sentence+metadata field features, f w/o meta:</cell></row><row><cell cols="3">exclude metadata fields.</cell><cell></cell></row><row><cell></cell><cell>measures</cell><cell>ex</cell><cell>wq</cell><cell>wqn</cell></row><row><cell></cell><cell>MAP</cell><cell cols="3">0.1275 0.1396 0.1408</cell></row><row><cell>kw</cell><cell cols="4">Recall@B 0.1990 0.2013 0.2034</cell></row><row><cell></cell><cell>R-Prec</cell><cell cols="3">0.1658 0.1686 0.1684</cell></row><row><cell></cell><cell>MAP</cell><cell cols="3">0.1427 0.1480 0.1469</cell></row><row><cell>f</cell><cell cols="4">Recall@B 0.1976 0.2001 0.2011</cell></row><row><cell></cell><cell>R-Prec</cell><cell cols="3">0.1592 0.1663 0.1690</cell></row><row><cell>f</cell><cell>MAP</cell><cell cols="3">0.1423 0.1458 0.1464</cell></row><row><cell cols="5">w/o Recall@B 0.1976 0.2002 0.2011</cell></row><row><cell>meta</cell><cell>R-Prec</cell><cell cols="3">0.1623 0.1692 0.1703</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,53.80,95.29,239.14,315.52"><head>Table 6 :</head><label>6</label><figDesc>Performance on 10 topics of Legal 2007 Routing task. Topics and feedback documents are from Legal Track 2006. (bf: short for boolean filter. RBase is run CMU07RBase which is the boolean query run, SVME is the term expansionAs seen from Table6, the est RB measure is more correlated with MAP than with Recall@B. Consistent with the results on the development set, structured features and weighted queries help retrieval a bit, but not significant.</figDesc><table coords="6,53.80,147.60,239.12,208.54"><row><cell cols="5">run CMU07RFBSVME, SVMNP is the weighted</cell></row><row><cell cols="5">query run CMU07RSVMNP which includes nega-</cell></row><row><cell cols="3">tive weight keywords also)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>measures</cell><cell>ex</cell><cell>wq</cell><cell>wqn</cell></row><row><cell></cell><cell>MAP</cell><cell cols="2">0.0853 0.0901</cell><cell>0.0852</cell></row><row><cell>kw</cell><cell>Recall@B</cell><cell>0.5736</cell><cell>0.5752</cell><cell>0.5765</cell></row><row><cell></cell><cell>est RB</cell><cell cols="2">0.3614 0.3471</cell><cell>0.3476</cell></row><row><cell></cell><cell>MAP</cell><cell cols="2">0.0644 0.0722</cell><cell>0.0548</cell></row><row><cell>f</cell><cell>Recall@B</cell><cell cols="2">0.5738 0.5766</cell><cell>0.5667</cell></row><row><cell></cell><cell>est RB</cell><cell cols="2">0.3679 0.3601</cell><cell>0.3617</cell></row><row><cell>f</cell><cell>MAP</cell><cell cols="2">0.0637 0.0605</cell><cell>0.0454</cell></row><row><cell>w/o</cell><cell>Recall@B</cell><cell cols="2">0.5738 0.5766</cell><cell>0.5659</cell></row><row><cell>meta</cell><cell>est RB</cell><cell cols="2">0.3679 0.3601</cell><cell>0.3616</cell></row><row><cell></cell><cell cols="2">submitted runs RBase</cell><cell cols="2">SVME SVMNP</cell></row><row><cell></cell><cell>MAP</cell><cell>0.0976</cell><cell>0.1248</cell><cell>0.1386</cell></row><row><cell></cell><cell>Recall@B</cell><cell cols="2">0.5691 0.5779</cell><cell>0.5038</cell></row><row><cell></cell><cell>est RB</cell><cell>0.3530</cell><cell>0.3342</cell><cell>0.3803</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,53.80,430.09,239.12,74.59"><head>Table 7</head><label>7</label><figDesc></figDesc><table coords="6,53.80,430.09,239.12,74.59"><row><cell cols="3">: Percentage of structured features in the top</cell></row><row><cell cols="3">100 features selected out by SVM, averaged over 37</cell></row><row><cell cols="3">topics where there are enough training documents.</cell></row><row><cell cols="3">The rest are keywords which constitute less than</cell></row><row><cell>half of the feedback terms.</cell><cell></cell><cell></cell></row><row><cell>NE</cell><cell>sen</cell><cell>meta</cell></row><row><cell cols="3">percentage 26.56% 26.24% 5.291%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,321.42,700.72,234.49,9.41;2,316.81,709.69,30.98,9.41"><p>On average, each document contains 5 hyphens at the end of lines.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,321.42,709.69,120.51,9.41"><p>http://svmlight.joachims.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,58.40,699.36,122.54,9.41"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,58.40,709.69,134.98,9.41"><p>http://www.alias-i.com/lingpipe/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGEMENTS</head><p>This research was supported in part by <rs type="funder">National Science Foundation</rs> grant <rs type="grantNumber">IIS-0707801</rs>, <rs type="grantNumber">IIS-0534345</rs> and <rs type="grantNumber">CCR-0122581</rs>. Any opinions, findings, conclusions, or recommendations are the authors' and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7s85QXC">
					<idno type="grant-number">IIS-0707801</idno>
				</org>
				<org type="funding" xml:id="_Wquf9X6">
					<idno type="grant-number">IIS-0534345</idno>
				</org>
				<org type="funding" xml:id="_74HkKEa">
					<idno type="grant-number">CCR-0122581</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.30,384.36,96.80,12.55" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.01,397.36,216.57,9.41;6,331.01,407.82,219.15,9.41;6,331.01,418.28,130.61,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,521.24,397.36,26.34,9.41;6,331.01,407.82,107.70,9.41">TREC 2006 Legal Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,457.76,407.82,92.39,9.41;6,331.01,418.28,102.33,9.41">Proceedings of the 15th Text Retrieval Conference</title>
		<meeting>the 15th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.01,429.74,211.68,9.41;6,331.01,440.20,221.24,9.41;6,331.01,450.66,216.33,9.41;6,331.01,461.12,109.91,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,376.78,440.20,175.47,9.41;6,331.01,450.66,84.52,9.41">TREC 2006 at Maryland: Blog, Enterprise, Legal and QA Tracks</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,434.24,450.66,113.10,9.41;6,331.01,461.12,81.63,9.41">Proceedings of the 15th Text Retrieval Conference</title>
		<meeting>the 15th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.01,472.58,214.52,9.41;6,331.01,483.04,208.94,9.41;6,331.01,493.50,192.02,9.41;6,331.01,503.96,70.89,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,400.44,483.04,139.51,9.41;6,331.01,493.50,21.12,9.41">Overview of the TREC 2007 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,370.91,493.50,152.12,9.41;6,331.01,503.96,42.61,9.41">Proceedings of the 16th Text Retrieval Conference</title>
		<meeting>the 16th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.01,515.42,201.28,9.41;6,331.01,525.88,211.44,9.41;6,331.01,536.34,130.61,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,457.52,515.42,74.77,9.41;6,331.01,525.88,100.24,9.41">York University at TREC 2006: Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Miao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,450.05,525.88,92.39,9.41;6,331.01,536.34,102.33,9.41">Proceedings of the 15th Text Retrieval Conference</title>
		<meeting>the 15th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.01,547.80,217.19,9.41;6,331.01,558.26,181.34,9.41;6,331.01,568.72,185.29,9.41;6,331.01,579.18,159.76,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,463.75,547.80,84.45,9.41;6,331.01,558.26,165.91,9.41">A comparative study on feature selection in text categorization</title>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,331.01,568.72,185.29,9.41;6,331.01,579.18,131.56,9.41">Proceedings of ICML 1997, 14th International Conference on Machine Learning</title>
		<meeting>ICML 1997, 14th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
