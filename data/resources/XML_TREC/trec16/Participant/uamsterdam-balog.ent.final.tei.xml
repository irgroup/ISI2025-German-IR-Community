<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,115.01,83.76,379.70,15.48">Query and Document Models for Enterprise Search</title>
				<funder ref="#_C6H7gEb">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_THJv2qM #_FnSFAV8 #_M5mbvvK">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_DRd4ff9 #_VjfpPVx #_aPrspZa #_vdhQXbM #_zmU4NNa #_esTbUeG #_NyKgMpd #_Ra6MsZT">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_aD6hfvv">
					<orgName type="full">E.U. IST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,91.74,116.28,80.04,10.75"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.68,116.28,80.70,10.75"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.28,116.28,97.60,10.75"><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,430.78,116.28,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,115.01,83.76,379.70,15.48">Query and Document Models for Enterprise Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A302C12A820044D2CAF9677A3DB323F0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the TREC 2007 Enterprise track and detail our language modeling-based approaches. For document search, our focus was on estimating a mixture model using a standard web collection, and on constructing query models by employing blind relevance feedback and using the example documents provided with the topics. We found that settings performing well on a web collection do not carry over to the CSIRO collection, but the use of advanced query models resulted in significant improvements. In expert search, our experiments concerned document representation, identification of candidate experts, and combinations of expert search strategies. We find no significant difference in average precision but observe small overall positive effects of the advanced models, with large differences between individual topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our aim for the TREC 2007 Enterprise Track was to analyze the effect of query and document models on the performance of generative language models for enterprise search. The framework we use for our participation employs such models for document search and views expert search as an extension of these models. In addition, we describe how we address the new challenge of identifying candidate experts in the document collection.</p><p>The paper is organized as follows. We discuss our work on the document search task (Section 2) and on the expert search task (Section 3). We conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Search</head><p>The aim of the document search task is to retrieve documents that help a science communicator within an organization (in this case CSIRO) create an overview page for a given topical area. Relevant documents are therefore documents that discuss the given topic in detail and not the ones that only touch on the topic. Our aim for the document search task was to experiment with a document model based on a mixture of components and with different query models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling</head><p>We address the document search task using a language modeling approach. We rank documents according to the likelihood of the document being relevant given the query. Instead of calculating the probability p(d|q) directly, we apply Bayes' rule and rewrite it to p(d|q) = p(q|d)p(d)/p(q).</p><p>The probability of the query p(q) can be ignored for the purpose of ranking documents, which leaves us with</p><formula xml:id="formula_0" coords="1,316.81,355.86,162.07,8.96">p(d|q) ∝ p(d)p(q|d).<label>(1)</label></formula><p>Assuming that query terms are independent from each other, p(q|d) is estimated by taking the product across terms in the query. Substituting this into Eq. 1 we obtain</p><formula xml:id="formula_1" coords="1,316.81,422.10,181.19,21.69">p(d|q) ∝ p(d) t∈q p(t|θ d ) n(t,q) ,<label>(2)</label></formula><p>where n(t, q) denotes the number of times term t is present in query q. To prevent numerical underflows, we perform this computation in the log domain and rewrite our equation as log p(d|q) ∝ log p(d) + t∈q n(t, q) log p(t|θ d ).</p><p>Next, we generalize n(t, q) so that it can take not only integer but real values. This will allow more flexible weighting of query terms. We replace n(t, q) with p(t|q), which can be interpreted as the weight of the term t in query q. If all query terms are equally important p(t|q) = n(t, q)/|q|, but we will see different weightings of query terms below.</p><p>Our final formula for ranking documents is the following:</p><formula xml:id="formula_2" coords="1,316.81,596.79,217.52,11.15">log p(d|q) ∝ log p(d) + t∈q p(t|q) log p(t|θ d ).<label>(3)</label></formula><p>Next, we address the estimation of the components of our modeling: the document model p(t|θ d ) in Section 2.1.1 and the query model p(t|q) in Section 2.1.2. The document priors p(d) were assumed to be uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Document model</head><p>The document model is built up from a mixture of components, where each component corresponds to certain fields or segments of the document. Assuming that there are n components, our mixture model is constructed by taking their linear interpolation, smoothed with a background model:</p><formula xml:id="formula_3" coords="2,53.80,96.28,191.34,14.11">p(t|θ d ) = n i=1 λ i p(t|d i ) + λ c p(t),<label>(4)</label></formula><p>where the component d i is weighted with λ i , and p(t) is the maximum likelihood-estimate of the term t given the background (collection) model, weighted with λ c = 1i λ i . The components we consider are: title, meta, headings, body, and anchor text. The construction of the component indexes is described in Section 2.2. A key issue is the choice of component weights λ i (i = 1, . . . , n). To this end we tested different configurations on the TREC 2004 Web Track <ref type="bibr" coords="2,53.80,213.59,11.62,8.64" target="#b3">[4]</ref> test set; it emerged that the title and anchor components are preferred over other components and that the influence of the background model should be very modest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Query model</head><p>The core problem is to determine the probability p(t|q). In our baseline query model we use only terms from the topic field of the query, and set p(t|q) = n(t, q)/|q|. <ref type="bibr" coords="2,53.80,324.35,11.62,8.64" target="#b4">(5)</ref> For two of our runs we construct the query model using relevance models <ref type="bibr" coords="2,110.04,354.69,10.58,8.64" target="#b5">[6]</ref>. Given a set of feedback documents (along with relevance scores), relevance models return a number of feedback terms with an associated weight. We normalize the weights of the feedback terms so that they add up to 1 and use this weighted query to retrieve the final set of documents. We experimented with two ways of selecting feedback documents: one run (uams07bfb) uses blind relevance feedback by taking the top 10 documents returned by the original query; the second run (uams07bfbex) takes the examples of relevant documents provided by the topic descriptions. For both runs based on relevance models we combined the feedback terms and the original query terms with equal weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Collection Processing</head><p>Multiple indexes were created: the content of the &lt;title&gt; tag was extracted for the title index; the content of meta name="keywords" and meta name="description" tags were used to construct the metadata index. The header index used the content of &lt;h1&gt;, . . . , &lt;h6&gt; tags and for the anchor index anchors were normalized. The body index used all text between &lt;body&gt; tags, in which all HTML had been removed. The background index consisted of the body combined with metadata and title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Runs</head><p>We submitted the following runs, all of which were automatic. We determined the best mixture of weights based on experiments on the TREC 2004 Web test set (see Section 2.1.1). This mixture gives the following weights to the components: title 0.30, headings 0.10, metadata 0.05, anchor text 0.40, body text 0.10 and the background model 0.05. uams07bl the baseline run; uses the baseline query model (Eq. 5) and equal weights to all components in the mixture model (components: 0.18; background: 0.10). uams07pr baseline query model (Eq. <ref type="bibr" coords="2,477.92,152.26,7.19,8.64" target="#b4">5)</ref>, mixture weights based on TREC 2004 Web test set. uams07bfb same as uams07pr, but the query model is constructed using blind relevance feedback on the top 10 documents (returned for the original query). uams07bfbex identical to uams07bfb, except for the addition of the example documents to the document set that is used to construct the relevance models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>To compare our regular runs (i.e., uams07bl and uams07pr) to feedback runs using example documents (i.e., uams07bfbex) we need to remove the example documents from the results and qrels. To compare our regular runs to regular runs of other TREC 2007 participants, we should not do this. Therefore, results are split into two: with and without removal of example documents. The first three rows of Table <ref type="table" coords="2,372.31,370.50,4.98,8.64" target="#tab_0">1</ref> indicate the results without removal, the next four with removal. An extra feature in the assessments is the identification of "possibly relevant pages" and "key pages"; the basic metrics MAP, P@10 and MRR allow only the "key pages" as relevant, but the normalized discounted cumulative gain (nDCG) differentiates between the two by assigning higher gains to "key pages". We report on all metrics in Table <ref type="table" coords="2,503.70,454.19,3.74,8.64" target="#tab_0">1</ref>. Our findings are as follows. First, mixture component weights estimated on the TREC 2004 Web test set do not seem to be optimal for the CSIRO collection (see uams07bl vs. uams07pr). This suggests that the CSIRO collection is unlike a "standard" web collection. We leave the confirmation of this hypothesis and further explorations as future work. Second, relevance-based query models proved to be beneficial (uams07bfb). Combining example documents with the top relevant documents used for blind relevance feedback resulted in substantial improvements, and was our best performing run (uams07bfbex); this run was also best capable of ranking "key pages" higher than "possibly relevant pages" as indicated by the high nDCG score of this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Expert Search</head><p>The Expert Search task presents the following scenario: Given an organization's document repository, return (e-mail addresses of) people that are experts on a given topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting Candidate Information</head><p>A list of candidate experts had to be extracted from the document collection. Candidates are identified by their primary e-mail addresses which follow the format firstname.lastname@csiro.au. We also extract candidate names which are then used to form document-candidate associations. Challenges include spam protection measures, the use of alternate e-mail addresses, and of different forms of names, such as nicknames and abbreviations.</p><p>To extract candidate information we first parse out e-mail addresses from mailto link elements. We extract corresponding person names from the e-mail address, but also use heuristics (e.g., capitalization, number of words) to extract person names from anchor text and text elements neighboring the link element, thus extracting alternative forms of names. Second, we use a small set of regular expressions to extract both plain e-mail addresses and e-mail addreses that are obfuscated for spam protection. Third, we extract person names from the titles of personal pages and construct the corresponding e-mail addresses from these names.</p><p>We post-process the resulting candidate list by matching alternate e-mail addresses to the same person based on exact name matching. Finally, we remove duplicates and addresses that do not refer to person names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling</head><p>The approach we take does not require an explicit definition of the concept "being an expert," instead, we assume that people closely associated with a topic are experts on that area <ref type="bibr" coords="3,53.80,591.27,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="3,66.99,591.27,7.19,8.64" target="#b6">7]</ref>. Hence, the challenge is to estimate the strength of the association between candidate ca and topic q, which we express as p(ca|q), the probability of a candidate given a topic. Instead of computing this directly, we use Bayes' Theorem and estimate: p(ca|q) = p(q|ca)p(ca)/p(q), where p(ca) is the probability of a candidate, and p(q) is the probability of a query. Since p(q) is a constant, it can be ignored for the purpose of ranking. The apriori belief that candidate ca is an expert, p(ca), is assumed to be uniform. Thus, we rank candidates in proportion to p(q|ca), the probability of the query given the candidate.</p><p>Balog et al. <ref type="bibr" coords="3,376.89,57.28,11.61,8.64" target="#b1">[2]</ref> introduce two models for estimating the probability p(q|ca). Model 1 is candidate-based; the candidate is modeled as a distribution over words, based on documents she is associated with. Thus, we define p(q|θ ca ) as</p><formula xml:id="formula_4" coords="3,316.81,115.54,222.83,14.11">t∈q {(1 -λ) ( d p(t|d)p(d|ca)) + λp(t)} n(t,q) . (6)</formula><p>Here, the candidate language model is a linear interpolation between an empirical model and the background language model p(t); n(t, q) is the number of times term t occurs in query q, while p(d|ca) expresses the strength of the association between document d and candidate ca.</p><p>In contrast, Model 2 is a document-based approach; first, documents that are relevant to the query are located, then each candidate is scored by aggregating over all relevant documents associated with the candidate: p(q|ca) = d p(q|d)p(d|ca), where p(q|d), the probability of a query given the document model, is calculated using Eq. 3.</p><p>For further details concerning the models we refer to <ref type="bibr" coords="3,541.81,274.26,10.58,8.64" target="#b1">[2]</ref>. This year we also experimented with combining Model 1 and 2 in order to integrate focused information from personal pages of candidates with the document-based approach. We constructed candidate language models from personal pages, where we define a personal page as a document that contains the full name of a candidate in the title field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document-Candidate Associations</head><p>Document-candidate associations form a key part of the models presented in Section 3.2. We need to estimate p(d|ca), the probability which expresses the strength of the association between document d and candidate ca. First, association scores a(d, ca) are formed for document-candidate pairs: a(d, ca) = 0.55 • A n (d, ca) + 0.45 • A e (d, ca), where A n (d, ca) and A e (d, ca) are binary functions, taking either 0 or 1 as a value, depending on whether the name or e-mail address of candidate ca is recognized in document d. This particular choice of weights was acquired from previous years' experiments <ref type="bibr" coords="3,368.44,516.45,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="3,382.46,516.45,7.19,8.64" target="#b2">3]</ref>. We estimate p(d|ca) using the strength of the association scores a(d, ca) in two ways. Our naive approach uses the association scores as is: p(d|ca) = a(d, ca). Our candidate-centric approach <ref type="bibr" coords="3,446.87,552.32,11.62,8.64" target="#b1">[2]</ref> assumes that candidatedocument associations are stronger the more frequently the candidate is mentioned in the document compared to other candidates: p(d|ca) = a(d, ca)/( ca ∈C a(d, ca )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Supporting Documents</head><p>Runs submitted to the Expert Search task require a ranked list of experts and a ranked list of (up to 20) documents for each returned candidate that support the person's expertise on the topic at hand. For each topic q i we ranked documents according to p(q i |d). For each candidate ca, considered as an expert, the top (up to) 20 documents associated with the person (a(d, ca) &gt; 0) were returned as support. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Runs</head><p>We submitted the following 4 runs: uams07exbl the baseline run; uses a background LM and naive document-candidate associations. uams07exmm uses the same document model as the document search run uams07pr with a mixture model with experimentally derived weights as described in 2.1.1. Naive document-candidate associations are used. uams07expp identical to uams07exmm but a candidatebased document model is constructed from personal pages and combined with results of the previous run using equal weights. uams07exfr again identical to uams07exmm but uses candidate-centric document-candidate associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Results</head><p>Candidate extraction produced a list of 2556 candidates. Despite the large number, we missed some of the experts of the result set created by the assessors. The result set contained 150 unique experts (2 experts were listed twice). Our extraction method found 105 (70%) of these result experts. Personal pages were found for 1038 (40.6%) of all identified experts and 68 of the experts in the result set (45.3%).  The use of personal pages in uams07expp improves performance over uams07exmm but does not compensate for the drop in performance due to the document model used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Fig. <ref type="figure" coords="4,335.15,207.67,4.98,8.64" target="#fig_0">1</ref> points to a large per-topic difference in AP scores between these two runs. We attribute this to the fact that personal pages were found for less than half of the candidate experts. In cases where experts for a topic did have personal pages, this information could help to improve performance on this topic. Without such information, performance may have been hurt as scores of less relevant experts with personal pages would be ranked higher.</p><p>The use of candidate-centric instead of naive documentcandidate associations shows differences in the topic-wise comparison. However, differences are smaller than in the case where we use personal pages, possibly because only about 50% of the association scores differed. When only one candidate is mentioned in a document the association scores are the same as for naive document-candidate associations, and then results should equal those of uams07exmm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We described our participation in the TREC 2007 Enterprise track. Building on our earlier work <ref type="bibr" coords="4,465.38,491.29,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="4,479.71,491.29,7.19,8.64" target="#b2">3]</ref>, we employed a standard language modeling setting for both document and expert tasks. Our aim for the document search task was to experiment with the weights of our mixture model components using a standard web collection, and to construct query models by employing blind relevance feedback and using the example documents provided with the topics. Results show that weight settings optimized on a web collection do not carry over to an enterprise (CSIRO) collection. The use of advanced query models, on the other hand, results in significant improvements.</p><p>As to the expert search task, our experiments concerned document representation, identification of candidate experts, and combinations of expert search strategies. Advanced models display large differences in precision between individual topics, but no significant difference in average precision. These results suggest promising directions for future research, such as more sophisticated combinations of models incorporating additional information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,224.98,185.10,159.75,7.77"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Per-topic differences in AP scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,331.82,470.09,209.10,130.46"><head>Table 1 :</head><label>1</label><figDesc>Results for the document search task.</figDesc><table coords="2,331.82,470.09,209.10,116.96"><row><cell></cell><cell>MAP P@10 MRR nDCG</cell></row><row><cell cols="2">without removal of examples</cell></row><row><cell>uams07bl</cell><cell>.3336 .4840 .7587 .6467</cell></row><row><cell>uams07pr</cell><cell>.3257 .4660 .7971 .6466</cell></row><row><cell>uams07bfb</cell><cell>.3691 .5180 .6937 .6751</cell></row><row><cell cols="2">with removal of examples</cell></row><row><cell>uams07bl</cell><cell>.2921 .4100 .6717 .5740</cell></row><row><cell>uams07pr</cell><cell>.2855 .3960 .6739 .5729</cell></row><row><cell>uams07bfb</cell><cell>.3291 .4640 .6487 .5987</cell></row><row><cell cols="2">uams07bfbex .3587 .5120 .6849 .6395</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,516.83,239.11,73.47"><head>Table 2 :</head><label>2</label><figDesc>Results for the Expert Search task (supporting documents are not taken into account).</figDesc><table coords="4,61.14,516.83,224.42,49.32"><row><cell></cell><cell cols="2">#rel ret MAP</cell><cell>P@5</cell><cell>P@10</cell><cell>MRR</cell></row><row><cell>uams07exbl</cell><cell>94</cell><cell cols="3">0.3090 0.2080 0.1360 0.4776</cell></row><row><cell>uams07exmm</cell><cell>94</cell><cell cols="3">0.3011 0.1840 0.1280 0.4562</cell></row><row><cell>uams07expp</cell><cell>94</cell><cell cols="3">0.3066 0.1960 0.1260 0.4662</cell></row><row><cell>uams07exfr</cell><cell>95</cell><cell cols="3">0.3051 0.1960 0.1280 0.4608</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,53.80,603.23,239.10,80.37"><head>Table 2</head><label>2</label><figDesc>lists the number of relevant experts, MAP scores, early precision, and mean reciprocal rank for our submitted runs. The baseline, which only uses the background document model, performs best overall. The remaining three runs rely on the mixture model from the document search task. The drop in performance parallels the lower relevance scores of the corresponding document search run.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Balog and <rs type="person">De Rijke</rs> were supported by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">600.065.120</rs> and <rs type="grantNumber">612.000.106</rs>. Hofmann and <rs type="person">De Rijke</rs> were supported by <rs type="funder">NWO</rs> under project number <rs type="grantNumber">612.066.512</rs>. <rs type="person">De Rijke</rs> was also supported by <rs type="funder">NWO</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">354-20-005</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.001.501</rs>, <rs type="grantNumber">640.002.501</rs>, and by the <rs type="funder">E.U. IST</rs> programme of the 6th <rs type="programName">FP for RTD</rs> under project <rs type="projectName">MultiMATCH</rs> contract <rs type="grantNumber">IST-033104</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_C6H7gEb">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_DRd4ff9">
					<idno type="grant-number">600.065.120</idno>
				</org>
				<org type="funding" xml:id="_THJv2qM">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_FnSFAV8">
					<idno type="grant-number">612.066.512</idno>
				</org>
				<org type="funding" xml:id="_M5mbvvK">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_VjfpPVx">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_aPrspZa">
					<idno type="grant-number">354-20-005</idno>
				</org>
				<org type="funding" xml:id="_vdhQXbM">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_zmU4NNa">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_esTbUeG">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_NyKgMpd">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funded-project" xml:id="_aD6hfvv">
					<idno type="grant-number">640.002.501</idno>
					<orgName type="project" subtype="full">MultiMATCH</orgName>
					<orgName type="program" subtype="full">FP for RTD</orgName>
				</org>
				<org type="funding" xml:id="_Ra6MsZT">
					<idno type="grant-number">IST-033104</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,70.40,227.49,222.50,8.64;5,70.40,239.26,222.51,8.82;5,70.40,251.22,222.51,8.59;5,70.40,263.35,48.80,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,253.62,227.49,39.28,8.64;5,70.40,239.44,179.10,8.64">Language Modeling Approaches for Enterprise Tasks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,277.96,239.26,14.94,8.59;5,70.40,251.22,218.22,8.59">The Fourteenth Text Retrieval Conference (TREC 2005)</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,283.28,222.50,8.64;5,70.40,295.05,222.51,8.82;5,70.40,307.01,95.74,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,239.59,283.28,53.31,8.64;5,70.40,295.23,177.11,8.64">Formal Models for Expert Finding in Enterprise Corpora</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,268.01,295.05,24.90,8.59;5,70.40,307.01,11.83,8.59">SIGIR &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,327.11,222.50,8.64;5,70.40,339.07,222.50,8.64;5,70.40,350.84,222.50,8.82;5,70.40,362.80,138.63,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,220.83,327.11,72.07,8.64;5,70.40,339.07,222.50,8.64;5,70.40,351.02,65.32,8.64">Language Models for Enterprise Search: Query Expansion and Combination of Evidence</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,157.38,350.84,135.52,8.59;5,70.40,362.80,28.95,8.59">The Fifteenth Text Retrieval Conference</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2006">2006. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,382.90,222.50,8.64;5,70.40,394.68,222.51,8.82;5,70.40,406.81,189.41,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,195.73,382.90,97.17,8.64;5,70.40,394.86,70.81,8.64">Overview of the TREC-2004 Web Track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,173.98,394.68,113.92,8.59">Proceedings of TREC-2004</title>
		<meeting>TREC-2004<address><addrLine>Gaithersburg, Maryland USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,426.74,222.50,8.64;5,70.40,438.52,222.51,8.82;5,70.40,450.47,212.63,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,254.02,426.74,38.88,8.64;5,70.40,438.69,142.10,8.64">Overview of the TREC-2005 Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,232.02,438.52,60.88,8.59;5,70.40,450.47,157.05,8.59">The Fourteenth Text Retrieval Conference (TREC 2005)</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,470.58,222.50,8.64;5,70.40,482.35,178.74,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,186.43,470.58,106.47,8.64;5,70.40,482.53,26.81,8.64">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,116.04,482.35,39.22,8.59">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.40,502.46,222.50,8.64;5,70.40,514.23,222.51,8.82;5,70.40,526.19,194.10,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,242.96,502.46,49.94,8.64;5,70.40,514.41,130.71,8.64">Overview of the TREC 2006 Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,220.86,514.23,72.04,8.59;5,70.40,526.19,84.42,8.59">The Fifteenth Text Retrieval Conference</title>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2006">2006. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
