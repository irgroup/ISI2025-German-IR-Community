<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.44,120.22,233.25,10.51">TREC 2007 CiQA Track at RMIT and CSIRO</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.72,155.18,53.33,8.72"><forename type="first">Mingfang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.80,155.18,59.33,8.72"><forename type="first">Andrew</forename><surname>Turpin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,142.92,166.46,50.09,8.72"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.00,166.46,68.02,8.72"><forename type="first">Yohannes</forename><surname>Tsegay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.40,155.18,59.81,8.72"><forename type="first">Ross</forename><surname>Wilkinson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CSIRO ICT Centre Canberra</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.44,120.22,233.25,10.51">TREC 2007 CiQA Track at RMIT and CSIRO</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">519D4F93A05A9BA41241365BA1E955FF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Overview</head><p>As part of our participation in the 2007 CiQA track, the RMIT and CSIRO team investigated the following three research questions:</p><p>1. What contextual words are helpful in improving answer quality? 2. Given two answer lists of different quality, which list would a user prefer? 3. Would a user's preference choice be correlated with her own relevance judgement of an individual list?</p><p>To explore these questions, we submitted:</p><p>• Four system runs with various query formulation strategies;</p><p>• Two interactive runs, with one interface for the preference choice, and the other one for the relevance judgement of each answer sentence from an answer list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Two initial runs</head><p>We used the Indri index and search tools from the Lemur toolkit for all our system runs. When the collection was indexed, words were stemmed using the Krovetz stemmer, and words from the stoplist were removed. We chose a language model with Jelinek-Mercer smoothing (σ = 0.5) to weight and rank documents <ref type="bibr" coords="1,132.60,458.90,97.70,8.72" target="#b3">(Zhai and Lafferty, 2001)</ref>.</p><p>To get an answer list for a question, we extracted a query from the topic field, retrieved the set of top 20 documents, then parsed these documents into sentences. Sentences were ranked according to each sentence's score, calculated as a combination of the longest span of matched query words, the number of matched query words, and the number of matched distinct query words.</p><p>The only difference between all our system runs lies in the way in which the query was constructed. In our baseline system run, rmitrun1, we used those words within the brackets embedded in a question template as a query. In our second system run, rmitrun2, we added some additional words into the query; namely, those words form the narrative field of a question topic, except the introduction part (such as "The analyst would like to know of"). These words give elaborated information about what is counted as an answer. For the purpose of exploring the second research question, we would like to get an answer list from rmitrun2 that is sufficiently different from the one from rmitrun1, but not too dramatically dissimilar. We therefore experimented with giving different weights to the two sets of words from the title and narrative fields. Our submitted run used equal weight for the words from two fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Two interactive runs</head><p>To explore the second and third research questions, we set up two interfaces: one for preference choice between two lists, and the other for relevance judgement of an individual list. In the interface for the preference choice (submission run-id: rmitrun3), for each question, we took two answer lists each from rmitrun1 and rmitrun2 and then showed the two lists side by side, as in Figure <ref type="figure" coords="1,415.56,700.10,3.67,8.72" target="#fig_0">1</ref>. The two answer lists from rmitrun1 and rmitrun2 were randomly assigned to the left or the right panel in each question, but overall, the two systems have equal chance to be on either side for 30 questions. Users at NIST were required to browse the two answer lists, then select their preferred answer list by clicking the button located on top of each list, and finally fill in a questionnaire (as shown in Figure <ref type="figure" coords="2,408.00,119.42,3.49,8.72">2</ref>). Assessors were given five minutes for each question. In our systems, assessors/users were reminded to move on to the questionnaire by the end of four minutes. Because of this time limit, only the top ten answer sentences were displayed for each question.</p><p>This comparison of two systems through preference choice was first introduced and tested by Thomas and Hawking <ref type="bibr" coords="2,129.12,186.50,116.80,8.72" target="#b2">(Thomas and Hawking, 2006)</ref>. By using a similar interface with two panels, they evaluated two alternate search systems. Using both supplied queries and their own real queries, the participants found no discernable left-right bias, and subjects were able to reliably distinguish between high-and low-quality result sets. Therefore, the use of such a comparison study can avoid many of the costs and biases of familiar evaluation methods. We adopted this interface to test if this finding still holds true for the complex question answering task, and importantly we used the questionnaire instrument to gather information about why they chose one list over another.</p><p>Our third research question is: would this preference choice be correlated with a user's own relevance judgement of an individual list? In our system rmitrun4, the same lists that were used in rmitrun3 were displayed in a single panel (Figure <ref type="figure" coords="2,230.04,298.46,3.53,8.72">3</ref>), and assessors were asked to make a relevance judgement for each answer sentence. We also set up a time reminder, as in rmitrun3; assessors were then required to fill in the questionnaire (Figure <ref type="figure" coords="2,174.48,320.66,4.07,8.72">4</ref>) to give their overall assessment of a list. Ideally, for each question, both lists from these two systems should be judged, so we can compare the preference choice with the relevance judgement of individual lists on a question by question basis. However, in our second interface, a question could only appear once. Therefore, we had to make a compromise by displaying answer lists from each system for only half of the questions. When questions were assigned to systems, the question types were considered, so questions of the same type (such as "financial relationship") would have a roughly equal chance from both systems. In this way, we could only do the comparison on a system by system basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Two final runs</head><p>We submitted two runs for the final submission. The run rmitrun5 is a relevance feedback run based on the relevance judgments that were collected from the interface rmitrun4. In this run, a query includes the words inside brackets from question template (as in the run rmitrun1), as well as words from sentences (up to five) that were judged as "definitely an answer" by users of rmitrun4. If there isn't any sentence judged as "definitely an answer" for a question, sentences (up to five) that were judged "not sure, need to read original documents" would be chosen (question 72 and 77 from rmitrun2 and question 66 and 73 from rmitrun1 fall in this situation). If all sentences were judged as "definitely not an answer" (the question 68), then the top five ranked sentences would be taken.</p><p>In the run rmitrun6, we used the following steps to get an initial 20 candidate documents: 1. Take words from within each bracket, treat them as a phrase, and use Boolean "and" to connect each phrase. For example, for the question 56: "What evidence is there for transport of [illegal immigrants] from [Croatia] to [the European Union]?" The query is: "illegal immigrants" and "Croatia" and "European Union" 2. Relax the above queries by removing Boolean "and"; 3. Relax the above queries by removing quotation marks. A search will be stopped at a stage when a list of twenty candidate documents have been retrieved at that point, otherwise the top ranked documents form the next step search would be used to make up the list. As a result, there were 13/8/9 of questions got their lists of 20 documents at steps 1/2/3 respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,213.72,488.42,168.89,8.72;5,164.52,293.44,267.12,192.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The interface of the run rmitrun3</figDesc><graphic coords="5,164.52,293.44,267.12,192.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,130.92,508.84,334.56,155.40"><head></head><label></label><figDesc></figDesc><graphic coords="6,130.92,508.84,334.56,155.40" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">System runs</head><p>Overall, the Pyramid F scores for four system runs are very close to each other: there is no statistically significant difference between any runs. In fact, the baseline run rmitrun1 is slightly better than the other three runs. The three alternative querying strategies improved less topics (10/10/7 for rmitrun2/5/6) but worsen more topics (16/19/9 for rmitrun2/5/6). The run rmitrun2 was not expected to be better than rmitrun1, the weights of different query components were chosen to separate the corresponding lists of a question from two runs.  <ref type="table" coords="3,217.68,558.74,3.67,8.72">1</ref>. Pyramid F scores for our system runs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interactive Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Preference choice</head><p>There are 30 questions in total. No preference was made for questions 70 and 78. Among the remaining 28 questions, 16 of them were chosen with a convincing reason that one list was better than another -rmitrun1 and rmitrun2 each got 11 votes and 5 votes respectively. For the remaining 12 questions, assessors just made a random (but not side-biased) choice and commented later that in fact they thought there was not any difference between the two lists.</p><p>To explore if an assessor indeed choose a list that is of high quality, we compared an assessor's preference choice with the official nugget pyramid evaluation that was aggregated from nine assessors. Considering that the assessors/users of our interactive interfaces saw only the top ten answer sentences for each question, we adopt nugget precision to evaluate the quality of shown lists. As in <ref type="bibr" coords="3,423.72,722.18,84.89,8.72;4,87.96,108.26,24.40,8.72" target="#b1">Dang, Lin and Kelly (2006)</ref>, we approximate this nugget precision by a length allowance based on the number of both vital and okay nuggets, that is:</p><p>length: # of non-whitespace characters in the entire answer string okay: # of okay nuggets returned in a response vital: # of vital nuggets returned in a response allowance ( a ) = 100 * (okay + vital)</p><p>Thus, this measure would tell us how much useful information is contained in a string of certain length. However, similar to the precision in a document search task, this measure doesn't give information on whether the useful information is at the top or bottom of a list. We then calculated average nugget precision as in a document search task, but used the above precision formula to approximate the mean of the precision after each relevant answer sentence (a sentence that has either okay or vital nuggets). The correlations between nugget precision and average nugget precision are 0.57 for rmitrun1 and 0.83 for rmitrun2.</p><p>Figure <ref type="figure" coords="4,117.00,359.78,4.85,8.72">5</ref> shows the relationship between the average nugget precision and the preference choices of the assessors. An assessor's choice is marked after the question number. The choice labels A, B, E and X represent rmitrun1, rmitrun2, no difference, and no record of choices, respectively. The tick and cross symbols indicate if an assessor agreed or disagreed with the quality evaluation of the lists.</p><p>It can be seen that there is a big difference between a user's preference and the average nugget precision: the ratio for disagreement to agreement is 20:8. (A similar ratio exists between the preference and the nugget precision.) The average nugget difference between some paired lists may be too small to be recognised by the users, even through there still exists the disagreement for 10 and 6 questions, with each number corresponding to the average nugget difference at the 0.1 and 0.2 level. This result is quite different from Thomas and Hawking's work where users were asked to compare two lists for the same query but for a document relevance judgement task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Answer identification</head><p>In our second interactive interface (rmitrun4), users were asked to identify whether a presented answer sentence indeed contains an answer on a three-level semantic scale, namely: "definitely an answer", "not sure, need to read original document", and "definitely not an answer". We compared this judgement with the final judgement made by nine NIST assessors (it is believed this includes the users who interacted with our interfaces). We found that out of 300 answer sentences (30 questions x 10 sentences each), the assessors of our interfaces disagreed with the group judgement for 111 sentences. This difference may be one of the reasons that caused the big difference between the preference choice and the (average) nugget precision reported above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We tested various querying strategies for the system runs. Overall, we didn't see significant improvement by using any of these tested query formulation strategies. A more thorough analysis of questions and the performance of these runs on a question by question basis is required.</p><p>For the interactive runs, we observed a big difference between the preference choice and the comparison of two lists (using the average nugget precision measure), as well as a big difference in answer nugget identification between individual assessors of our interactive interfaces and a group of assessors as a whole. This again raises the classic questions about relevance: who make the relevance judgement, in which circumstances are the relevance judgements made, and how should relevance be measured? These issues are very important for the interactive evaluation of information retrieval systems, as we often don't see a similarity between system performance and user performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,87.96,164.30,172.49,8.72" xml:id="b0">
	<monogr>
		<ptr target="http://www.lemurproject.org" />
		<title level="m" coord="5,87.96,164.30,54.41,8.72">Lemur project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.96,179.30,420.53,8.72;5,104.52,190.58,378.98,8.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,233.40,179.30,208.98,8.72">Overview of the TREC 2006 Question Answer Track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,449.40,179.30,59.09,8.72;5,104.52,190.58,208.68,8.72">Proceedings of the Fifteenth Text REtrieval Conference (TREC 2006)</title>
		<meeting>the Fifteenth Text REtrieval Conference (TREC 2006)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.96,205.58,420.50,8.72;5,104.52,216.74,123.29,8.72" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<title level="m" coord="5,205.68,205.58,297.97,8.72">Evaluation by Comparing Result Sets in Context, Proceedings of CIKM&apos;06</title>
		<meeting><address><addrLine>Virginia USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.96,231.74,420.70,8.72;5,104.52,242.90,403.97,8.72;5,104.52,254.30,309.29,8.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,193.80,231.74,314.87,8.72;5,104.52,242.90,84.18,8.72">A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,207.24,242.90,301.25,8.72;5,104.52,254.30,232.46,8.72">Proceedings of the 24st International ACM-SIGIR Conference on Research and Development in Information Retrieval. Louisianna USA</title>
		<meeting>the 24st International ACM-SIGIR Conference on Research and Development in Information Retrieval. Louisianna USA</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
