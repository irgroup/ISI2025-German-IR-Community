<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.96,81.04,244.36,12.91">Lymba&apos;s PowerAnswer 4 in TREC 2007</title>
				<funder>
					<orgName type="full">Disruptive Technology Office&apos;s (DTO) Advanced Question Answering for Intelligence (AQUAINT) Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,178.44,134.03,72.75,10.76"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
							<email>moldovan@lymba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Lymba Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.27,134.03,78.15,10.76"><forename type="first">Christine</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lymba Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.91,134.03,86.46,10.76"><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lymba Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.96,81.04,244.36,12.91">Lymba&apos;s PowerAnswer 4 in TREC 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EA05D1683F38FCB0BC3CDA6863099E6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on Lymba Corporation's (a spinoff of Language Computer Corporation) participation in the TREC 2007 Question Answering track. An overview of the Power-Answer 4 question answering system and a discussion of new features added to meet the challenges of this year's evaluation are detailed. Special attention was given to methods for incorporating blogs into the searchable collection, methods for improving answer precision, both statistical and knowledge driven, new mechanisms for recognizing named entities, events, and time expressions, and updated pattern-driven approaches to answer definition questions. Lymba's results in the evaluation are presented at the end of the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Innovations for TREC 2007</head><p>New to TREC this year was a 175 GB collection of blog entries and an updated collection of 2.5 GB newswire articles. To meet the challenge of extracting answers from blogs PowerAnswer was prepared to search and process a collection of noisy data. For this reason mechanisms for filtering non-english text, filtering information-deficient articles and organizing blog entries into indexable documents were developed. To facilitate the required temporal indexing and processing of the AQUAINT 2 and Blog data, a new event and time detection system, called Concept Tagger, was introduced.</p><p>Perhaps the biggest change in PowerAnswer from the last few years consisted in the introduction of a new NER system, Rose that added new finer grained types for locations, quantities, and media.</p><p>Another innovation this year was the introduction of answer likelihood using question classes. By identifying question classes, improved language models can be built for these question types which in turn can be used during answer ranking.</p><p>Regarding list questions, COGEX logic prover was used for the first time as a re-ranker and candidate answer extractor during answer processing for list questions. New speciliazed types that consulted external resources were also integrated into the list question strategy. Both of these additions targeted the problem of increasing answer selection precision.</p><p>To maximize the coverage and robustness of the nugget extraction algorithms for "other" questions a hierarchy of nugget patterns and automatically derived generic answer patterns was developed.</p><p>What follows is a brief overview of PowerAnswer, followed by an indepth discussion of each of the innovations listed above and their impact on the results of the system. This paper will conclude with a summary of the overall results of PowerAnswer 4 at TREC 2007 and a discussion of future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PowerAnswer 4 Overview</head><p>Lymba, formerly Language Computer Corporation, developed PowerAnswer as a fully-modular and distributed multi-strategy question answering system that integrates semantic relations, advanced inferencing abilities, syntactically constrained lexical chains, and temporal contexts. This section presents an outline of the system that was used in TREC 2007.</p><p>PowerAnswer contains a set of strategies that are selected based on advanced question processing, and each strategy is developed to solve a specific class of questions either independently or together. A Strategy Selection module automatically analyzes the question and chooses a set of strategies with the algorithms and tools that are tailored to the class of the given question.</p><p>Each strategy is a collection of components, (1) Question Processing (QP), (2) Passage Retrieval (PR), and (3) Answer Processing (AP). Each of these components con-  stitute one or more modules, which interface to a library of generic NLP tools. These NLP tools are the building blocks of the PowerAnswer 4 system that, through a well-defined set of interfaces, allow for rapid integration and testing of new tools and third-party software such as IR systems, syntactic parsers, named entity recognizers, logic provers, semantic parsers, ontologies, word sense disambiguation modules, and more.</p><p>As illustrated in Figure <ref type="figure" coords="2,176.18,342.64,3.77,8.97" target="#fig_0">1</ref>, the role of the QP module is to (1) determine temporal constraints as defined by Concept Tagger, (2) detect the expected answer type, now extended with new types made available by Rose, (3) process any question semantics necessary such as roles and relations, (4) select the keywords used in retrieving relevant passages, (5) perform any preliminary questions as necessary for resolving question ambiguity, and (6) decide which question class to use when computing answer likelihood during answer ranking. The PR module ranks passages that are retrieved by the IR system, while the AP module extracts and scores the candidate answers based on a number of syntactic and semantic features such as keyword density, count, proximity, semantic ordering, roles, entity type, and, new to this year, an answer likelihood score, computed based on the language model associated with the class of the question. All modules have access to a syntactic parser, semantic parser, a named entity recognizer and a reference resolution system through Lymba's generic NLP tool libraries. To improve the answer selection, PowerAnswer takes advantage of redundancy in large corpora, specifically in this case, the Internet and Wikipedia. As the size of a document collection grows, a question answering system is more likely to pinpoint a candidate answer that closely resembles the surface structure of the question. These features have the role of correcting the errors in answer processing that are produced by the selection of keywords, by syntactic and semantic processing and by the absence of pragmatic information. The final decision for selecting answers is based on logical proofs from the inference engine COGEX <ref type="bibr" coords="2,137.88,713.20,112.03,8.97">(Moldovan, D. et al., 2006b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Answering over Blog Data</head><p>New for TREC2007 was the dataset -updated newswire articles and the blog06 collection of weblog entries from the University of Glasgow. The blog collection presented its own unique set of challenges. Lymba's PowerAnswer had to overcome the size of the data, the organization of the blog entries, and the noisy nature of the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Collection Preparation</head><p>The raw data size of the collection is 175GB. Much of this is surrounding HTML (or XML) and many of the actual entries are quite small when separated. After a pipeline of processing steps, the final data size used by Lymba for TREC2007 was 13.1GB, or a reduction of 92.5%. The final index used was 7GB.</p><p>To process the blog06 collection to be useful to Pow-erAnswer, we first parsed all the files to identify unique content, removing the duplicate entries. Secondly, we used an in-house language detection tool to identify and remove the non-English documents, spam documents and documents that had no retrievable text outside of the headline. This initial processing was done in a matter of 40 hours on 3 servers. Then we sent the remaining text through a limited set of Lymba's NLP toolset. The NLP tools used were distributed across 6 machines such that the processing of this subset of the data could be processed and indexed using Apache Lucene in 2 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Temporal Event Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of Concept Tagger</head><p>For 2007, we included the Concept Tagger module into our NLP processing pipeline. Concept Tagger detects the events present in a question or a candidate passage and labels them with their corresponding class, as seen in Table 1, and identifies a variety of temporal expressions, including absolute dates, times, durations and sets as well as resolving fuzzy temporal expressions, shown in Table 2. The Concept Tagger module uses a set of rules which operate on the full parse tree of the analyzed text.  The rich information extracted by Concept Tagger is used to detect event-event relations as well as time-event relations which are vital to the identification of relevant passages and extraction of accurate answers, as shown in Table <ref type="table" coords="3,108.24,593.68,3.77,8.97" target="#tab_5">3</ref>. For this temporal relation identification, we adopted a hybrid approach which combines machine learning algorithms and manually developed set of rules which achieves an F-measure of 55% to 73% for the three subtasks of the TempEval Temporal Relation Identification Task <ref type="bibr" coords="3,149.44,653.44,97.50,8.97">(Task 15, SemEval 2007</ref><ref type="bibr" coords="3,246.94,653.44,51.97,8.97;3,72.00,665.32,38.00,8.97" target="#b7">) (Min, C. et al., 2007)</ref>. The feature set used to learn SVM models for temporal relations include information about the event (class, stem, part-of-speech, polarity, tense, aspect, modality, syntactic roles, semantic roles, cereference relations, etc.), the time (type, value, relation to the document creation time), the temporal signal, and the linguistic context (reporting, belief, volitional contexts). For TREC 2007, we enhanced this module to recognize biographical information (prevelant in news documents) by augmenting the set of syntactic patterns it uses to extract information. These new patterns were useful for passages such as the following which contain biographical temporal information:</p><p>Today <ref type="bibr" coords="3,346.44,179.40,76.88,8.07">[February 23, 2006</ref><ref type="bibr" coords="3,423.32,179.40,97.40,8.07;3,319.20,189.36,140.15,8.07">] 's Birthdays: Bedrich Smetana, Bohemian composer (1824</ref><ref type="bibr" coords="3,459.35,189.36,27.71,8.07">-1884);</ref><ref type="bibr" coords="3,493.92,189.36,26.99,8.07;3,319.20,199.32,173.51,8.07">. . . Kurt Weill, German-born American composer (1900</ref><ref type="bibr" coords="3,492.71,199.32,23.75,8.07">-1950)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Metrics</head><p>We evaluated the impact that Concept Tagger had on PowerAnswer's performance by analyzing the 93 factoid questions with temporal aspects distributed across all targets (with seven out of the sixteen event targets having explicit temporal information). All these questions require temporal processing with four of them necessitating temporal relation identification within the question which should be matched to the passage. PowerAnswer returned correct answers in 64.36% of the remaining cases. The patterns for the biographical information were succesfully used for answering 3 temporal questions. The accurate resolution of fuzzy temporal expressions lead to the extraction of correct answers for 16 questions (two of them are shown in Table <ref type="table" coords="3,434.05,597.76,3.63,8.97" target="#tab_7">4</ref>). The answer passages for the remaining 40 questions contained absolute temporal expressions which when linked to their corresponding events matched the answer type of the input questions (one example is listed in Table <ref type="table" coords="3,437.28,645.52,3.63,8.97" target="#tab_7">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A New NER System: Rose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview of Rose</head><p>Lymba's named entity recognizer uses a three step process: (1) Pass the text through a pattern based grammar  Having text annotated by the grammars, the data is next input to a two pass system based on <ref type="bibr" coords="4,245.75,430.96,52.68,8.97;4,72.00,442.96,47.11,8.97" target="#b0">(Carreras, X. et al, 2003)</ref>. The process is separated into annotating the existance of named entities using the BIO labeling scheme and classifying the recognized named entities. The first task is, essentially, chunk annotation. Instead of using AdaBoost, per <ref type="bibr" coords="4,156.18,490.72,97.67,8.97" target="#b0">(Carreras, X. et al, 2003)</ref>, Rose employs Taku Kudo's YamCha <ref type="bibr" coords="4,190.42,502.72,109.09,8.97;4,72.00,514.72,34.64,8.97" target="#b5">(Kudo, T. and Matsumoto, Y. 2003)</ref> for the BIO task. What follows are the features extracted in a word window of ±2:</p><p>• The surface form of the token</p><formula xml:id="formula_0" coords="4,81.96,564.88,105.47,9.29">• The Part-of-Speech tag</formula><p>• Whether the word's first character is capitalized • 3 character prefixes and suffixes</p><p>• The output of the grammar system in BIO format, e.g. B-AWARD</p><p>A simple maximum entropy model is used to classify the identified named entities into PER, ORG, and LOC classes. The results are merged with the output of the grammar system. When the grammar system results in a subtype of the ML system, the more specific tag is preferred. When there is a conflict between the high level   type returned by the ML system and the grammar system, the ML system output takes precedence. Finally, we index all the named entities found in a document, create probable partial matches in the vein of <ref type="bibr" coords="4,324.48,402.76,100.93,8.97" target="#b7">(Mikheev, A. et al, 1998)</ref>, and determine whether the partial match is correct based on the output of a maximum entropy model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Metrics</head><p>The grammars were developed while looking over a large set of examples texts that included AQUAINT corpus data, WSJ, along with the data sets for MUC and ACE. Rose's lexicons were compiled from scratch from Wikipedia and other online resources, while the machinelearned components were trained on the MUC training data.</p><p>TREC answers of human entities usually require ranks and titles to be part of the returned answer. MUC annotations consider just the name itself to be the named entity. Our system returns "Secretary Ron Brown" instead of the desired "Ron Brown". "President Clinton" versus "Clinton". The results in Table <ref type="table" coords="4,442.93,615.88,4.98,8.97" target="#tab_9">5</ref> do not correct for this difference.</p><p>The impact of new types on the results was generally positive, as seen in Table <ref type="table" coords="4,418.92,653.44,3.77,8.97" target="#tab_10">6</ref>. 77.5% of the factoid questions where one of the new types was used was answered correctly. The average F mesaure for list questions with one of the new types were generally positive. Incorporating more specific question answering types helped the performance of PowerAnswer on these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answer Likelihood for Factoid Answer Selection</head><p>Answer ranking is a persistant challenge for any question answering system. PowerAnswer's primary semantic mechanism for extracting exact answers has been knowledge driven and enabled by COGEX <ref type="bibr" coords="5,252.37,146.20,46.83,8.97;5,72.00,158.08,63.85,8.97">(Moldovan, D. et al., 2006b)</ref>. Due to the time complexity of parsing, currently there is an upper limit of between 10-20 candidate answer passages that are evaluated by COGEX. This restriction makes it very important that the answer processing and ranking modules up to that point are able to push the most correct answers into at least the top 20.</p><p>For this reason Lymba experimented with clustering and language modelling techniques as part of the answer processing system similar to those reported by <ref type="bibr" coords="5,248.67,253.72,50.11,8.97;5,72.00,265.72,21.68,8.97" target="#b4">(Ko, J et al., 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Generating Question Class Models</head><p>The goal of the answer likelihood component was to group questions from previous TRECs into classes, and then build language models for each class based on features extracted from the questions in the class as well as the answers judged as correct for each of these. The automatic formation of the question classes proved challenging. Lymba experimented with three methods for building the classes: (1) Generate regular expression style paraphrases for the questions in the training set <ref type="bibr" coords="5,264.28,402.40,34.17,8.97;5,72.00,414.28,63.67,8.97" target="#b1">(Clifton, T. et al., 2004)</ref>, and then group them together based on their paraphrase identifiers, (2) Use hierarchical clustering <ref type="bibr" coords="5,98.77,438.16,151.28,8.97">(Manning, C. and Schutze, H. 1999)</ref> to organize the questions based on their expected answer type, most relevent keywords (as determined by the keyword selection algorithm), and named entity types, and (3) Group the questions together by their answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Building the Question Class Models</head><p>Regardless of the method used for determining the question classes, the same approach was used to build language models for the classes. For both the questions in the classes and for each of the correctly judged answers as supplied by NIST, the following features were extracted:</p><p>1. Stemmed keywords from the question and the answer 2. Morphological alternations for each of the keywords 3. Named entity tags from both the question and the answers From these features, a language model using goodturing smoothing was built with the SRILM <ref type="bibr" coords="5,250.72,665.32,48.09,8.97;5,72.00,677.32,23.48,8.97" target="#b11">(Stolcke, A. 2002)</ref> toolkit for each of the question classes. These models were then consulted during answer processing in order to compute the likelihood that an answer is in fact the correct response to a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Integration in the Answer Ranking Pipeline</head><p>The answer likelihood component was injected into Pow-erAnswer at two different stages: (1) During question processing a question was classified into one of the available question classes. In the case of the paraphrasing models, the paraphrase for the input question was generated, which was used to decide which model to consult later in the pipeline. (2) During answer processing, each of the semantic candidates retrieved after semantic matching are given a score to represent the probability or likelihood of the candidate sentence being in the language model for the question class chosen during question processing.</p><p>The log of the score of the answer likelihood was then added as a feature to the existing estimated relevance function embedded in PowerAnswer answer procesing <ref type="bibr" coords="5,313.20,270.27,111.29,8.97" target="#b10">(Moldovan, D. et al., 2004)</ref>. From this point the top N candidates are passed to COGEX to re-rank the candidates based on how well the question is entailed by the given candidate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Observations</head><p>For each of the three question class formation methods described above, Lymba ran a set of experiments on previous TREC test sets in order to determine which set of question classes performed the best. Leading into TREC 2007 it was empirically determined that the models based on grouping questions by answer types was most effective, and so was the configuration used for the TREC 2007 test set.</p><p>Our observations for this outcome include that for the models derived from the regular expression style paraphrases for the questions, the classes were too sparse as the software developed for this task was not able to generalize the patterns enough. The result was a large number of question classes with very few instances in them. For the models built from trying to cluster the classes using the named entities, n-gram features, the answer type, and the most relevant keywords of the questions, the clusters generated were noisy, and contained groups of questions that were not necessarily semantically similar. We hypothesize that the weighting schemes for the models needed more fine tuning. By simply grouping the questions by answer type, there is at least a guarantee that the expected focus of the questions in the class are the same, and so the classes generated tended to be more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Performance Metrics</head><p>Although for the test set evaluation of the TREC 2007 questions, Lymba used the answer type driven models at run time, during post-TREC analysis, we benchmarked the system using two of the question class methods, eliminating the paraphrase based models since they were not able to generalize well enough for use at the time of this writing. In the Table <ref type="table" coords="6,156.77,75.16,4.98,8.97" target="#tab_12">7</ref> below is a summary of the results over the TREC 2007 factoid questions. The row Clusters refers to the classes derived using hierarchical clustering. The row Answer Type refers to the classes derived by grouping questions by their answer types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Improving Precision for List Questions</head><p>Lymba PowerAnswer's list question answering strategy is to attempt to maximize recall by returning as many answer candidates as possible during passage retrieval using a set of lexico-semantic alternations and relaxing the query to include the target keywords and the most relevent keywords from the primary question text. One major challenge the system faces once this result set is returned is to effectively filter the candidates such that all false positives are removed from the set. In the past the answer selection component of this strategy relied heavily on statistical selection algorithms. This method proved to be overly greedy and resulted in suboptimal precision. In an attempt to overcome the precision issue, Lymba deployed two new extensions to the list answer ranking components. The first was to capitalize on the wide coverage of lists found in Wikipedia, and to consult this as an authoritative source. The second was to tightly integrate COGEX, Lymba's logic prover for textual inference, into the answer selection process for questions in highly confusable answer classes, specifically those seeking lists of humans. What follows is a description of each of these methods along with a summary of their performance impact on the overall TREC 2007 list question answering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Improving Precision Authoritative Resources</head><p>In previous years PowerAnswer successfully employed external resources for specialized answer types such as movies, songs, and books. Bots for searching amazon.com and imdb.com were executed whenever a question asking about a topic in the domain of books, songs, or movies was detected. The role of the bots was to identify candidate answers from these resources, and generate a dynamic lexicon to be consulted during the answer selection and ranking. Due to the increasing popularity and perceived authoritativeness of Wikipedia, Lymba chose to develop a similar bot framework for all questions whose expected answer type fell into the category of " media", a new native type introduced by Lymba's NER system, Rose. Leading up to TREC our team explored using the lists in Wikipedia for questions seeking humans and countries, but decided to only deploy the media feature during the actual evaluation.</p><p>Google's "I'm Feeling Lucky" search was employed to locate relevent Wikipedia articles. During question processing, if the expected answer type assigned to the question is " media", a bot named "searchWiki" is initialized with a query that includes the target keywords.In the case of the example for question Q282.5: What are titles of Pamuk's works?, the bot command is: search-Wiki.pl "Orhan Pamuk" media data/lists/wiki.</p><p>A results set containing the downloaded pages from Wikipedia that discuss Orhan Pamuk, are sent to a page parser that extracts double-quoted entities on lines that begin with * or -(unordered lists or tables) and recurses into pages that are linked with main-.. and see also-.. on the main page. The section names need to match one of Filmography, Films, Discography, Albums, Songs, Bibliography, Books, Novels, or Works Finally, a list of entities pulled from these pages and sections are placed in an in-memory dynamic lexicon that is used to filter and boost list answer candidates during answer selection. All candidates that at least 90% match any of the entries in the dynamic lexicon are considered sound and are added to the final list of results for the input question. In this case the resulting answers were all entities extracted from Wikipedia and included: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Exploiting Textual Entailment for Human</head><p>Seeking List Questions</p><p>One of the weakness of PowerAnswer's list question answering strategy involved questions seeking lists of humans. This can be explained by the fact that in any given candidate passages there are many references to humans.</p><p>Due to the greedy nature of the algorithms used to extract answer candidates from passages for list questions, the system was prone to select false positives as answers to human seeking questions. Because, in previous TRECs, it has been demonstrated that COGEX is an effective reranking tool <ref type="bibr" coords="6,364.69,653.44,106.65,8.97" target="#b2">(Harabagiu, S. et al, 2003;</ref><ref type="bibr" coords="6,474.09,653.44,65.87,8.97;6,313.20,665.32,37.69,8.97" target="#b10">Moldovan, D. et al., 2004;</ref><ref type="bibr" coords="6,353.28,665.32,104.49,8.97" target="#b3">Harabagiu, S. et al,, 2005;</ref><ref type="bibr" coords="6,460.16,665.32,79.99,8.97;6,313.20,677.32,27.92,8.97">Moldovan, D. et al., 2006a)</ref> for factoid seeking questions, Lymba integrated COGEX, the logic prover directly into the list answer extraction filtering process in order to increase the precision for human seeking questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">COGEX for LIST Questions</head><p>For any candidate passage returned by the passage retrieval engine that scores above a pre-defined threshold, all the human entities in that passage are returned as potential candidate answers for the question. Assertions are formed from the question such that each candidate is hypothesized to be the answer to the question. For questions of the form Name X or List Y, the reformulation into an assertion followed the pattern candidateAnswer is one of X/Y. For example, for question-answer pair: Q248.3: Name officers of CSPI, A: Michael Jacobson, the resuting assertion is: Michael Jacobson is one of the officers of CSPI.</p><p>Once each candidate answer is inserted in the question, COGEX checks if the newly derived assertions are entailed by the corresponding candidate answer passages. Only candidates that received an entailment score above the optimal threshold learned from past TREC evaluations are returned as valid answers to the list question. Consider the list question Q254.5 What women have worn Chanel clothing to award ceremonies? (House of Chanel), a candidate answer passage:  <ref type="table" coords="7,254.92,448.60,4.98,8.97">8</ref> the proof sketch for the assertions:</p><p>(1) Kirsten Dunst has worn Chanel clothing to award ceremonies (H<ref type="foot" coords="7,130.08,494.93,3.97,6.37" target="#foot_0">1</ref> i ) and</p><p>(2)Maureen Chiquet has worn Chanel clothing to award ceremonies (H 2 i ) is shown, along with their scores, thus demonstrating the process that lead to Kirsten Dunst's selection as an answer and the elimination of Maureen Chiquet.</p><p>Axiom A 1 (also shown in Table <ref type="table" coords="7,203.40,592.00,4.18,8.97">8</ref>) is used by the prover in both cases, but for the H 2 i assertion, the synactic and semantic dependencies between Maureen Chiquet and the verb wear cannot be inferred from the passage and, therefore, the prover relaxes one of the verb's arguments and drops the agent semantic relation. These score penalties cause the second assertion's score to drop below the threshold and Maureen Chiquet to be removed as an answer for the question Q254.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Performance Metrics</head><p>In the 2006 TREC evaluation, PowerAnswer's average precision was: 0.498 and this year jumped to 0.539. Ten out of the eighty-five questions in the list test set were classified as media and so utilized the Wikipedia as a resource for answer selection/confirmation. The average precision for these questions was 0.655, and so clearly contributed to the increase in precision this year. Additionally the approach, described in Section 7.2.1, proved to be effective as our f-score for humans increased from 0.372 to 0.458 and more importantly our average precision rose from 0.412 to 0.493.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">"Other" or Definition Questions</head><p>The inherent challenge of "other" questions in the TREC QA Track is the filtering and selection of interesting and novel nuggets from a large corpus. The passage recall for information about a target is typically overwhelming, and pruning these passages to pick the best nuggets is time consuming and difficult.</p><p>The traditional method employed by PowerAnswer to extract nuggets is to execute a definition pattern matching module. Previously, a list of over 200 positive and negative pre-computed patterns was loaded into memory. The target was inserted into these patterns and the resulting query submitted to an index including stopwords and punctuation. These are high-precision patterns that indicate information of a definitional nature.</p><p>To extend this method for TREC 2007, we developed a hierarchy of nugget patterns and automatically derived generic answer patterns. Questions are classified into this hierarchy and all answer patterns for the individual node and all parent nodes in the hierarchy of nuggets are selected and executed to construct a set of minimallyrequired information about the target depending on the class into which that target falls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Deriving Answer Patterns from a Question Class Hierarchy</head><p>The nugget hierarchy is developed based on question classes discovered from previous TREC question sets, resulting in 35 target classes (e.g. animal, actor, musician, literature). These nodes are then arranged using the WordNet hierarchy with pre-selected upper nodes such as person, organization, event and other entity. Each class has associated with it a set of minimal information that is customized to that class. For example, a person pattern set has information about full name, birth, death, place of birth, residence, occupation, etc. An event set has information about begin time, end time, duration, location, participants, etc. An organization holds information  Table 8: COGEX's proofs on number of members, for-profit or not, income, goals. Specializations such as musician contain patterns pertaining to instruments played, hit songs, and so on.</p><p>Answer nugget patterns are automatically generated using question-and-answer pairs from prior TREC question sets as seeds. Similar questions are grouped together and the textual patterns that form the answers are generalized into a number of patterns. Parts-of-speech can be generalized to form patterns with greater recall. Named entities are also specified in the patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Examples of Use</head><p>Examples of automatically-generated and generalized person pattern, demonstrating some of the permutations that were discovered in the corpus:</p><p>• var BE DT nationality JJ profession</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• nationality profession var</head><p>• nationality JJ profession var</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• var ( nationality profession</head><p>Where var is a generalization for any human target. Sentences containing these patterns are selected from the index and added to a collection of candidate answer nuggets. The nuggets are filtered for similarity of information within the set and filtered to remove information contained in previous FACTOID or LIST answers. The sentences are parsed and then trimmed by the system such that the lowest tree node that contains the pattern remains, plus or minus some padding to reach a minimum word ratio.</p><p>Generic patterns at the top of the hierarchy are also useful in retrieving interesting nuggets. Examples from the entity node are:</p><p>• var, which • var ( DT The higher the pattern is in the Question Class Hierarchy, the lower the weighting of the final score. This is done to limit spurious matches as the more specific patterns will have higher precision than these more generic ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Performance Metrics</head><p>The new technique implemented for this year's TREC yielded a 67.26% increase in pyramid score over the OTHER questions in TREC 2006 (0.281 versus 0.168). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion and Future Directions</head><p>TREC 2007 highlighted the requirement that any useful question answering system must be able to accurately extract answers from very large collections consisting of mixed formats and even noisy data. PowerAnswer 4 was able to meet this challenge with its robust document processing and indexing tools, as well as with new algorithms that focused on precision. Future directions for PowerAnswer will include continued refinement of the answer likelihood work as well as a continued emphasis on the knowledge drive approaches inherent in the application of COGEX and the consultation of external resources for answer verification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,228.36,202.72,154.99,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PowerAnswer 4 Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,72.00,352.72,226.89,8.97;7,72.00,364.72,226.93,8.97;7,72.00,376.72,226.90,8.97;7,72.00,388.60,226.76,8.97;7,72.00,400.60,133.65,8.97;7,81.96,424.60,216.60,8.97;7,72.00,436.60,226.97,8.97;7,72.00,448.60,136.45,8.97;7,212.16,447.04,3.49,6.28;7,219.48,448.60,32.90,8.97"><head>(</head><label></label><figDesc>P i ) Reese Witherspoon's gown was not vintage ... Golden Globe winner wearing the same glittery Chanel cocktail dress that Kirsten Dunst had worn to the awards in 2003.... seen asking Chanel President Maureen Chiquet at an after-party ... It contains three mentions of women, where only two of the assertions, Kirsten Dunst and Maureen Chiquet are returned as candidate answers. 1 In Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,210.84,164.88,5.84,8.07;8,80.40,177.96,6.47,8.07;8,86.88,176.27,3.65,5.47;8,86.88,178.44,356.97,8.48;8,99.60,188.40,374.37,7.05;8,99.60,198.36,430.41,7.05;8,99.60,208.32,408.81,7.05;8,99.60,218.28,78.70,7.05;8,80.40,228.84,6.47,8.07;8,86.88,227.15,3.65,5.47;8,86.88,229.32,367.77,8.60;8,99.60,239.28,198.10,7.05;8,313.68,239.28,160.30,7.05;8,99.60,249.24,430.41,7.05;8,99.60,259.20,408.81,7.05;8,99.60,269.16,78.70,7.05;8,80.40,281.04,227.98,8.07"><head>i</head><label></label><figDesc>x1) &amp; Dunst NN(x2) &amp; nn NNC(x3,x1,x2) &amp; human NE(x3) &amp; wear VB(e1,x3,x6) &amp; occurrence EV(e1) &amp; Chanel NN(x4) &amp; human NE(x4) &amp; clothing NN(x5) &amp; nn NNC(x6,x4,x5) &amp; to TO(e1,x9) &amp; award NN(x7) &amp; ceremony NN(x8) &amp; nn NNC(x9,x7,x8) &amp; occurrence EV(x9) &amp; DURING SR(e1,x9) &amp; THEME SR(x6,e1) &amp; AGENT SR(x3,e1) H 2 Maureen NN(x1) &amp; Chiquet NN(x2) &amp; nn NNC(x3,x1,x2) &amp; human NE(x3) &amp; wear VB(e1,x3,x6) &amp; occurrence EV(e1) Chanel NN(x4) &amp; human NE(x4) &amp; clothing NN(x5) &amp; nn NNC(x6,x4,x5) &amp; to TO(e1,x9) &amp; award NN(x7) &amp; ceremony NN(x8) &amp; nn NNC(x9,x7,x8) &amp; occurrence EV(x9) &amp; DURING SR(e1,x9) &amp; THEME SR(x6,e1) &amp; AGENT SR(x3,e1) A1 cocktail dress NN(x1) -&gt; clothing NN(x1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,72.00,321.64,226.99,171.23"><head>Table 2 :</head><label>2</label><figDesc>Examples of temporal expressions identified by Concept Tagger.</figDesc><table coords="3,72.00,367.60,226.99,125.27"><row><cell>All temporal expressions are normalized according to</cell></row><row><cell>the TimeML TIMEX3 (TimeML Working Group 2005)</cell></row><row><cell>standard making possible the extraction of accurate</cell></row><row><cell>answers from passages which do not have words in</cell></row><row><cell>common with the analyzed question. For example,</cell></row><row><cell>Q249.5's correct answer was extracted from a passage</cell></row><row><cell>which makes use of the frequency denoting concept</cell></row><row><cell>annually whose normalized value (EVERY P1Y) is</cell></row><row><cell>identical to each year, concept present in the question.</cell></row><row><cell>Q249.5:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,78.00,484.80,201.54,48.03"><head>How many grants does the Fulbright Program award each year? P: The program named after the former Senator J. William Fulbright awards approximately 4,500 new grants annually.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,313.20,199.32,226.77,178.23"><head></head><label></label><figDesc>;. . .(APW ENG 20060223.0001)    The correct answer for question Q253.2 In what year did Kurt Weill die? was extracted from the passage shown above.</figDesc><table coords="3,319.80,267.00,213.50,70.71"><row><cell>Relation</cell><cell>Question</cell></row><row><cell>Event-event</cell><cell></cell></row><row><cell>during(be,write)</cell><cell>Q241.4: How old was Jasper Fforde</cell></row><row><cell></cell><cell>when he wrote his first Thursday</cell></row><row><cell></cell><cell>Next novel?</cell></row><row><cell>Time-event</cell><cell></cell></row><row><cell>during( date,begin)</cell><cell>Q227.</cell></row></table><note coords="3,424.52,329.64,108.73,8.07;3,403.80,339.60,118.48,8.07;3,403.80,349.56,129.54,8.07;3,403.80,359.52,129.25,8.07;3,403.80,369.48,117.10,8.07"><p><p><p><p><p>2:</p>On what date did the court begin screening potential jurors? Ans: Jury selection began six days ago</p>[November 17, 2004]   </p>. . .</p>(APW ENG 20041123.0053)    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="3,352.44,395.08,148.30,8.97"><head>Table 3 :</head><label>3</label><figDesc>Examples of event relations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="4,72.00,223.24,226.90,192.81"><head>Table 4 :</head><label>4</label><figDesc>Examples of temporal expression resolutions in answer selection.system, with rules maximizing recall, (2) Pass the grammar annotated data through an ML system based on<ref type="bibr" coords="4,277.76,275.56,20.73,8.97;4,72.00,287.56,82.41,8.97" target="#b0">(Carreras, X. et al, 2003)</ref>, and (3) In the spirit of<ref type="bibr" coords="4,258.19,287.56,40.71,8.97;4,72.00,299.44,57.56,8.97" target="#b7">(Mikheev, A. et al, 1998)</ref> perform partial matching on the text.Rose starts by invoking a traditional pattern matching and lexicon based information extraction engine. Input rule files are compiled into a graph representation and a depth first search is performed to see if a certain token starts a pattern match. The output of this step is a BIO labeled document. The set of named entity types consists of an extended set from previous competitions. Among new to this competition are types that represent various quantities, and specific locations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="4,313.20,154.72,226.91,123.95"><head>Table 5 :</head><label>5</label><figDesc>Results from the named entity recognition system, Rose, on the MUC development and test sets (Restricted to ORG/LOC/PER types).</figDesc><table coords="4,319.20,210.00,220.37,68.67"><row><cell>Type</cell><cell cols="3">Times used Factoid Accuracy List F β=1</cell></row><row><cell>media</cell><cell>20</cell><cell>80%</cell><cell>65.5%</cell></row><row><cell>quantity duration</cell><cell>13</cell><cell>76.9%</cell><cell>N/A</cell></row><row><cell>facility building</cell><cell>7</cell><cell>100%</cell><cell>48.85%</cell></row><row><cell>quantity age</cell><cell>6</cell><cell>50%</cell><cell>N/A</cell></row><row><cell>quantity length</cell><cell>5</cell><cell>80%</cell><cell>N/A</cell></row><row><cell>All</cell><cell>66</cell><cell>77.5%</cell><cell>56.92%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="4,313.20,291.04,226.72,32.97"><head>Table 6 :</head><label>6</label><figDesc>Number of times each new type was used as an answer type. Types with less then 5 occurrences have been truncated.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="6,72.00,189.04,226.70,20.85"><head>Table 7 :</head><label>7</label><figDesc>Results for Using Answer Likelihood in Answer Ranking</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="8,313.20,578.32,226.69,130.29"><head>Table 9 :</head><label>9</label><figDesc>Table 9 illustrates the final results of Lymba Corporation's results in the TREC 2007 QA track. The second column summarizes the accuracy, f-score, and pyramid scores for factoid, list, and others respectively. Lymba's TREC 2007 Results</figDesc><table coords="8,377.52,635.76,95.76,51.51"><row><cell></cell><cell>PowerAnswer 4</cell></row><row><cell>Factoid</cell><cell>0.706</cell></row><row><cell>List</cell><cell>0.479</cell></row><row><cell>Other</cell><cell>0.281</cell></row><row><cell>Overall</cell><cell>0.484</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,88.08,684.00,210.60,8.07;7,72.00,693.96,226.77,8.07;7,72.00,703.92,226.68,8.07;7,72.00,713.88,109.56,8.07"><p>Because Reese Witherspoon/Witherspoon was already selected as an answer based on a different supporting passage, it is disregarded during the processing of this passage and its corresponding candidate answers.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="11">Acknowledgements</head><p>We would like to thank the researchers and engineers from <rs type="person">Lymba Corporation</rs> for their invaluable efforts towards this work. This work, and also our broader research in QA, is supported in part by the <rs type="funder">Disruptive Technology Office's (DTO) Advanced Question Answering for Intelligence (AQUAINT) Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,72.00,377.56,226.45,8.97;9,81.96,388.60,217.19,8.97;9,81.96,399.52,217.01,8.97;9,81.96,410.44,76.68,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,261.03,377.56,37.42,8.97;9,81.96,388.60,153.56,8.97">A simple named entity extractor using AdaBoost</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,249.48,388.60,49.68,8.97;9,81.96,399.52,217.01,8.97;9,81.96,410.44,76.68,8.97">Proceedings of the seventh conference on Natural language learning at HLT-NAACL</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,428.68,227.04,8.97;9,81.96,439.60,216.87,8.97;9,81.96,450.64,123.34,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,202.92,428.68,96.12,8.97;9,81.96,439.60,101.36,8.97">Bangor at TREC 2004: question answering track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Teahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,201.72,439.60,97.12,8.97;9,81.96,450.64,123.34,8.97">Proceedings of the thirteenth text retrieval conference</title>
		<meeting>the thirteenth text retrieval conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,468.76,227.01,8.97;9,81.96,479.80,217.08,8.97;9,81.96,490.72,216.95,8.97;9,81.96,501.64,42.23,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,235.19,479.80,63.85,8.97;9,81.96,490.72,216.95,8.97;9,81.96,501.64,42.23,8.97">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,519.88,226.83,8.97;9,81.96,530.79,217.01,8.97;9,81.96,541.84,216.93,8.97;9,81.96,552.76,154.66,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,195.83,530.79,103.14,8.97;9,81.96,541.84,114.53,8.97">Employing Two Question Answering Systems in TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.92,541.84,59.97,8.97;9,81.96,552.76,154.66,8.97">Proceedings of the fourteenth text retrieval conference</title>
		<meeting>the fourteenth text retrieval conference</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,570.99,226.80,8.97;9,81.96,581.92,216.50,8.97;9,81.96,592.84,135.84,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,206.51,570.99,92.30,8.97;9,81.96,581.92,203.74,8.97">A Probabilistic Framework for Answer Selection in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,81.96,592.84,113.24,8.97">Proceedings of NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,611.07,227.05,8.97;9,81.96,621.99,217.16,8.97;9,81.96,632.92,216.93,8.97;9,81.96,643.95,23.46,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,201.16,611.07,97.90,8.97;9,81.96,621.99,75.27,8.97">Fast methods for kernelbased text analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,172.92,621.99,126.21,8.97;9,81.96,632.92,213.07,8.97">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,662.07,226.81,8.97;9,81.96,673.11,179.74,8.97" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<title level="m" coord="9,128.94,662.07,169.88,8.97;9,81.96,673.11,135.82,8.97">Schutze, H 1999 Foundations of Statistical Natural Language Processing The</title>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,691.23,226.87,8.97;9,81.96,702.27,217.19,8.97;9,81.96,713.19,185.19,8.97;9,313.20,75.15,226.84,8.97;9,323.16,86.07,216.54,8.97;9,323.16,97.11,217.23,8.97;9,323.16,108.03,208.90,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,252.32,691.23,46.55,8.97;9,81.96,702.27,150.25,8.97;9,474.74,75.15,65.31,8.97;9,323.16,86.07,216.54,8.97;9,323.16,97.11,22.48,8.97">LCC-TE: A Hybrid Approach to Temporal Relation Identification in News</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mikheev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,249.48,702.27,49.68,8.97;9,81.96,713.19,185.19,8.97">Proceedings of the 7th Message Understanding Conference</title>
		<meeting>the 7th Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998. 2007</date>
		</imprint>
	</monogr>
	<note>Description of the LTG System Used for MUC-7. Text IN Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007</note>
</biblStruct>

<biblStruct coords="9,313.20,126.99,227.06,8.97;9,323.16,137.91,216.78,8.97;9,323.16,148.83,217.14,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,533.06,126.99,7.19,8.97;9,323.16,137.91,191.92,8.97">A Temporally-Enhanced PowerAnswer in TREC</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tatu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,333.72,148.83,206.58,8.97">Proceedings of the fifteenth text retrieval conference</title>
		<meeting>the fifteenth text retrieval conference</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,167.79,226.83,8.97;9,323.16,178.71,216.92,8.97;9,323.16,189.75,217.09,8.97;9,323.16,200.67,217.02,8.97;9,323.16,211.59,23.46,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,352.92,178.71,187.16,8.97;9,323.16,189.75,170.27,8.97">Cogex: A semantically and contextually enriched logic prover for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hodges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,509.28,189.75,30.97,8.97;9,323.16,200.67,65.26,8.97">Journal of Applied Logic</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="69" />
			<date type="published" when="2007-03">2007. March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,230.55,226.83,8.97;9,323.16,241.47,216.75,8.97;9,323.16,252.51,216.87,8.97;9,323.16,263.43,71.86,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,346.68,241.47,193.23,8.97;9,323.16,252.51,25.53,8.97">PowerAnswer 2: Experiments and Analysis over TREC</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,389.88,252.51,150.15,8.97;9,323.16,263.43,71.86,8.97">Proceedings of the thirteenth text retrieval conference</title>
		<meeting>the thirteenth text retrieval conference</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,282.39,227.29,8.97;9,323.16,293.31,216.93,8.97;9,323.16,304.35,148.86,8.97" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<title level="m" coord="9,393.72,282.39,146.77,8.97;9,323.16,293.31,216.93,8.97;9,323.16,304.35,70.58,8.97">SRILM -An Extensible Language Modeling Toolkit In Proc. Intl. Conf. on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,323.19,226.65,8.97;9,323.16,334.23,181.00,8.97" xml:id="b12">
	<monogr>
		<title level="m" coord="9,313.20,323.19,226.65,8.97;9,323.16,334.23,177.09,8.97">TimeML Working Group 2005 The TimeML Working Group. 2005. The TimeML 1.2 Specification</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
