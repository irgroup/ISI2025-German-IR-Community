<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,63.41,71.94,482.88,16.59">The University of Amsterdam at the TREC 2007 QA Track</title>
				<funder ref="#_2WxQ34R #_u8X6HjB #_zhH32yG #_sKzTMeT #_VANhZfj #_QVe35je #_uHTpC9h #_vwZDtSz #_gWMtEqk">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_yN2wTNB">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.57,116.83,79.07,11.06"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.60,116.83,144.44,11.06"><forename type="first">Valentin</forename><forename type="middle">Jijkoun</forename><surname>Mahboob</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.36,116.83,63.78,11.06"><forename type="first">Alam</forename><surname>Khalid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.21,128.76,96.20,11.06"><forename type="first">Joris</forename><surname>Van Rantwijk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.37,128.76,53.99,11.06"><forename type="first">Erik</forename><surname>Tjong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.68,128.76,51.82,11.06"><forename type="first">Kim</forename><surname>Sang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,63.41,71.94,482.88,16.59">The University of Amsterdam at the TREC 2007 QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EA6064CDD336084188C628CB75490690</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our participation in the TREC 2007 Question Answering (QA) track, we focused on three tasks. First, we processed the new blog corpus and converted it to formats which could be used by our QA system. Second, we rewrote the module interface code in Java in order to improve the maintainability of the system. And third, we added a new table stream which has learned associations between question properties and properties of candidate answers. In the three runs we submitted to the competition, we experimented with answer type checking and web re-ranking. In follow-up experiments we were able to further evaluate the contribution of these two factors, and to evaluate our new table lookup stream and combinations of streams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="627.915" lry="819.198"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="627.915" lry="819.198"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="627.915" lry="819.198"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="627.915" lry="819.198"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We participated in the TREC 2007 Question Answering (QA) track with the experiences of three earlier participations <ref type="bibr" coords="1,245.84,381.72,19.61,8.07">(2003)</ref><ref type="bibr" coords="1,265.45,381.72,3.92,8.07">(2004)</ref><ref type="bibr" coords="1,269.37,381.72,19.61,8.07">(2005)</ref>. Our prime goal this year was to evaluate a new QA stream, and to experiment with answer type checking, web re-ranking, and with combining QA streams.</p><p>From a system point of view, we continued with our efforts to approach QA as an XML retrieval task <ref type="bibr" coords="1,204.23,434.02,9.52,8.07" target="#b0">[1]</ref>. Since our last participation, we have standardized data access in the QA system by converting all of our data resources to fit in an XML database <ref type="bibr" coords="1,280.21,454.95,9.52,8.07" target="#b4">[5]</ref>. For the current evaluation, we have focused on three tasks:</p><p>1. Convert the new blog corpus to formats that can be used by our system. 2. Standardize the code that takes care of the communication between the different modules of the system by completely rewriting it in Java (previously it was written in Perl). 3. Include in the parallel architecture a new table stream which has learned associations between questions and candidate answers rather than rely on manually defined association rules.</p><p>This paper contains eight sections. After this introduction, we present a general overview of the system in section 2. Our work on processing the blog corpus is discussed in section 3 while section 4 describes our approach of QA as XML retrieval in more detail. Sections 5 and 6 present the two most recent modifications to the QA system: standardizing the code and adding a new table stream. Section 7 describes the submitted runs and discusses their performance. We conclude in section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM DESCRIPTION</head><p>The architecture of our Quartz QA system is an expanded version of a standard QA architecture consisting of parts dealing with question analysis, information retrieval, answer extraction, and answer postprocessing (clustering, ranking, and selection). The Quartz architecture consists of multiple answer extraction modules, or streams, which share common question and answer processing components. The answer extraction streams can be divided into three groups based on the corpus that they employ: the newspaper corpus, the new blog corpus, or the Web. Below, we describe these briefly.</p><p>The Quartz system (Figure <ref type="figure" coords="1,422.98,283.29,3.73,8.07" target="#fig_0">1</ref>) contains four streams that generate answers from the two data sources, the AQUAINT newspaper corpus and the blog corpus. The Table Lookup stream searches for answers in specialized knowledge bases which are extracted from the corpus offline (prior to question time) by predefined rules. These information extraction rules take advantage of the fact that certain answer types, such as birthdays, are typically expressed in one of a small set of easily identifiable patterns. The stream uses the analysis of a question and manually defined patterns to map questions to database queries. Our new stream, ML Table <ref type="table" coords="1,491.73,377.44,24.97,8.07">Lookup</ref>, performs the answer lookup task by using a mapping learned automatically from a set of training questions (see section 6 for a more elaborate description). The NGram stream extracts the word ngrams that most frequently occur with words from the question. First, passages are retrieved from the collection using a standard retrieval engine (Lucene) and a text query generated from the question. Second, the most frequent ngrams in these passages are generated and filtered and returned as answers.</p><p>The most advanced of the four streams is XQuesta. For a given question, it generates XPath queries for answer extraction, and executes them on an XML version of the corpus which contains both text and additional annotations. The annotations include information about part-of-speech, syntactic chunks, named entities, and temporal expressions. For each question, XQuesta only examines text passages relevant to the question (as identified by Lucene, see also section 4).</p><p>There is one stream which employs textual data outside the TREC document collection defined for the task: the NGram stream also retrieves answers by submitting automatically generated web queries to the Google web search engine and collecting most common ngrams from the returned snippets. The answer candidates found by this stream are not backed up by documents from the TREC collection as required by the task. For this reason such candidates are never returned as actual answers, but only used at the answer merging stage to adjust the ranking of answers that are found by other QA streams as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRE-PROCESSING THE BLOG CORPUS</head><p>In this year's TREC QA track the WEB_BLOG_06 collection (blog corpus) was introduced as an additional source of information. This  collection presented a number of challenges that are typical for web corpora, such as encoding issues, mix of languages, boilerplate text (e.g. advertisement), etc. In addition, the size of the blog corpus is much larger than previous corpora used for TREC QA.</p><p>To address these challenges we preprocess the blog corpus using a set of Perl scripts. First, we detect the character encoding of each blog post and convert all characters to UTF-8. Second, we detect the language of the blog post using TextCat <ref type="bibr" coords="2,209.65,365.55,10.46,8.07" target="#b2">[3]</ref> and remove all non-English blog posts. Third, we attempt to extract the content of the blog post using a set of templates 1 . If template matching fails, we fall back to extracting post titles and removing HTML tags only. Finally, the blog post content is split into sentences using a rulebased sentence splitter 2 .</p><p>Our content extraction method using templates was based on the observation that a large portion of the blog posts was published using major blog publishing platforms. For example, about 38% of the blog posts were published using the platform Blogger. These platforms usually use templates to generate the blog HTML pages. Consequently, it should be possible to extract the original content by reverse engineering the templates used to generate these pages. We manually created templates for the 7 most common publishing platforms, WordPress, Blogger, Typepad, Moveable Type, moveabletype.org, canalblog, and CommunityServer.</p><p>The blog corpus comprises more than 3 million blog posts split up over 3247 TREC format files. Due to the large size of the corpus, we had to run preprocessing on the grid, and some files could not be preprocessed in time. We were able to process 86.5% of the blog posts. 12.6% of all posts were classified as non-English posts and were not included in content extraction. Template-based content extraction was successful for only 19.4% of the blog posts. We assume that small changes in templates between different versions of publishing platforms are responsible for this relatively small number. Automatic learning of templates may be a way towards more robust content extraction. For the remaining 54.4% of the blog posts, only title extraction, HTML tag removal and sentence splitting was performed. 1 Using Template::Extract, available from http://cpan.org. 2 We use the Perl module Lingua::EN::Sentence (http://cpan.org).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QA AS XML RETRIEVAL</head><p>One of our QA streams, XQuesta implements the "Question Answering as Semistructured Retrieval" approach. We view various text analysis tools (sentence splitter, part-of-speech and named entity taggers, chunker, parsers) as black boxes producing stand-off XML annotations of the input text data, and using XML querying for data access. For incoming questions, the question analysis module generates one or more structural queries that are used to extract answers from relevant passages identified by a passage retrieval engine. In this year's version of the system, we processed the data collection offline, generating a large repository of XML annotations. Since annotation tools are independent, the results generally cannot be presented in a single XML tree: e.g., for each document, we have sentence-split, POS-tagged, chunked, NE-tagged, and syntactically parsed versions in separate XML DOM structures. To facilitate transparent simultaneous access to such multi-level structures we extended Nux, an open-source Java library for XQuery processing. When accessing an XML document with multiple annotations, we load separate XML files containing layers of stand-off annotation of the same data, and join them in a single new XML object (DOM tree). The new document can be accessed with the standard XQuery facilities. Additionally, we implement an XQuery function stand-off:wide that retrieves XML elements overlapping (character-wise) with the current element, irrespective of the annotation layer, based on offsets specified with start and end attributes. For example, the following XQuery extracts person names that occur as parts of syntactic objects of verbs: stand-off:wide(//phrase[@type="VP"]/phrase[@type="NP"])</p><p>/NE[@type="PERSON"]</p><p>This implementation was inspired by the select-wide XPath axis in the XQuery support in the MonetDB <ref type="bibr" coords="2,479.40,625.42,9.52,8.07" target="#b7">[8]</ref>. See <ref type="bibr" coords="2,511.42,625.42,10.45,8.07" target="#b1">[2]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">REWRITING INTERFACE CODE</head><p>The QA system we used in previous years consisted of many components but was mostly developed ad-hoc, i.e., without a consistent system architecture. As a result, the system was difficult to maintain and change. To address this problem we re-implemented large parts of the system following a modular system design. The goal is to develop a self-contained system that is consistent and can be maintained more easily. The main feature of the newly developed system architecture is that it consists of several modules which are cleanly separated by interfaces. This allows us to minimize dependencies between components. A detailed description of the new architecture can be found in <ref type="bibr" coords="3,156.13,120.40,10.46,8.07" target="#b5">[6]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">LEARNING TO FIND ANSWERS</head><p>As described in section 2, our offline information extraction module creates a database of simple relational facts to be used during question answering. The TableLookup QA stream uses a set of manually defined rules to map an analyzed incoming question into a database query. A new stream, MLTableLookup, uses supervised machine learning to train a classifier that performs this mapping. In this section we give an overview of our approach. We refer to <ref type="bibr" coords="3,282.45,227.50,10.45,8.07" target="#b6">[7]</ref> for further details.</p><p>The purpose of the table lookup stream is to map an incoming question Q to an SQL-like query "select AF from T where sim(QF, Q)", where AF and QF are fields of table T with QF being the field with information similar to that in the question while field AF contains a candidate answer. In this query formalism, the task of generating the query is a combination of two subtasks: (1) mapping an incoming question Q to a triple T, QF, AF (a tablelookup label) and ( <ref type="formula" coords="3,125.85,321.65,3.48,8.07">2</ref>) defining an appropriate similarity function sim(QF, Q), a task for which we use Lucene's vector space model.</p><p>The interesting and novel part of the stream is the query formulation part, i.e., training a classifier to predict table lookup labels. For this purpose, features were extracted from the questions from the TREC QA tasks of 2001-2003 with the Quartz question classification module. Pairs of features and answers were retrieved from table rows and these were used as training material for label prediction. A ten-fold cross validation test with this data set reached MRR scores of up to 0.287 which is good given that the tabular data only contains answers to a fraction of the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">SUBMITTED RUNS</head><p>We have submitted three different runs for the main task of the Question Answering track of TREC 2007. In all the runs, list questions are treated as factoid questions by supplying only one answer. For answering OTHER questions we employ our system developed for the WebCLEF 2007 task <ref type="bibr" coords="3,158.75,512.43,9.52,8.07" target="#b3">[4]</ref>. For a given topic, the system retrieves relevant passages, splits text in sentences, and estimates importance of sentences for the topic by computing a simple sentence centrality score based on lexical similarity with other sentences in the pool.</p><p>The first run (uams07main) was generated by the system described in section 2. In test runs of this system we had noticed two problems. First, although each stream performs internal answer type checks, the system frequently returned answers of the wrong semantic type, like 1792 in response to Who is Mozart?. The second problem was related to the final reranking module which was based on web frequency counts of the candidate answers. It displayed erratic behavior and sometimes returned orderings which seemed to be random.</p><p>In order to evaluate the effect of these two problems, we created two alternative runs. The first (uams07atch) was generated by applying an extra type check to the answers of the main run, filtering out answers that did not match the expected answer type according to a coarse-grained named entity scheme (classes: person, location, organization, miscellaneous, timex, quantity and other). This run selected the highest ranked answer that passed the filter, or NIL if no such answer was available.</p><p>For the second alternative run (uams07nwrr), we used the answers as ranked before running the web ranking module. We did not apply an extra final answer type check here since this would have made it more difficult to find out the exact effect of the reranking module. The additional type check would have had an influence on the result as well.</p><p>The results of the three runs can be found in Table <ref type="table" coords="3,511.17,141.32,3.36,8.07" target="#tab_1">1</ref>. Our main interest was the exact factoid accuracy. The main run performed worst (0.072). The run with extra answer type checks (uams07atch, 0.083) achieved approximately 20% relative improvement over the main run. The run without web reranking (uams07nwrr, 0.092) performed best, with a relative improvement of 46.2% over the main run.</p><p>The scores for list questions were comparable. Results for OTHER questions were consistently above the median score of all participants. Since we submitted the same answers to these questions in all three runs, we were surprised that there were differences between the scores. According to the evaluation organizers, these are caused by inconsistencies between assessors.</p><p>Since the submission deadline, we have performed additional experiments to evaluate different parts of our system. For the evaluations we have only looked at factoid questions of the main track of TREC QA 2007. The unofficial answer patterns for these questions <ref type="foot" coords="3,334.25,317.26,3.49,6.28" target="#foot_0">3</ref> were used to classify answers as correct and incorrect. We were interested in finding answers to the following questions:</p><p>1. The QA system performed better with either added answer type check or omitted web reranking. How well would it perform if both modifications were applied at the same time? 2. The system contains six independent QA streams. How well did each of these streams perform on the 2007 factoid questions? 3. The answers of the streams were generated by combining all streams. Can a combination of a subset of these streams achieve a better performance?</p><p>The results of the additional evaluation experiments can be found in Table <ref type="table" coords="3,348.71,472.33,3.36,8.07">2</ref>. We did not check justification, so the factoid accuracy scores should be interpreted as an approximation of the sum of the exact and unsupported scores of Table <ref type="table" coords="3,455.90,493.25,3.36,8.07" target="#tab_1">1</ref>.</p><p>In the first experiment, we performed an extra type check on the answers of the uams07nwrr no web reranking run. According to the factoid answer patterns, this approach achieved an accuracy of 0.168 which constitutes a 43.6% relative improvement over our best submitted run (Table <ref type="table" coords="3,393.94,545.56,3.24,8.07">2</ref>). Even with the two modifications, the performances of three of the individual streams (XQUESTA blog and the two table streams) are worse than our lowest-scoring submitted run. The two AQUAINT streams reach a similar performance as our best submitted run while the web ngrams stream performs a lot better. However, it should be mentioned that the last stream does not generate corpus justification for its answers, so using only this stream would lead to all answers being unsupported.</p><p>In the final experiment, we selected the top answers from the system that were supported by at least one of a combination of individual QA streams. We used the top three performing individual streams, web ngrams, XQUESTA AQUAINT and AQUAINT ngrams. For answers supported by web ngrams, we additionally required that they were supported by one of the other five QA streams in order to make sure that the answer could be found in the corpus.  We examined one combination of three streams and three combination of two streams. This approach led to the highest factoid performance score we have measured: 0.181 for the combination of web ngrams and XQUESTA AQUAINT (Table <ref type="table" coords="4,234.73,517.84,3.24,8.07">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>We described our participation in the main task of the TREC 2007 Question Answering track. This year, we have continued with our approach of QA-as-XML-retrieval setting, where incoming questions are converted to semistructured queries which are applied to a target collection which was automatically annotated with linguistic information at indexing time. Additionally, we added the new blog corpus to the target collection, built a new QA stream for tabular data accessed via learned associations and completely rewrote the architecture code of our QA system. Our prime goal was to experiment with answer type checking, web re-ranking, and combinations of streams, and to evaluate our new table lookup stream. Our experiments show that answer type checking can substantially improve performance, while web reranking can hurt the performance of our system. Substantial im-provements also resulted from post-deadline experiments combining different subsets of individual streams. These combinations also outperformed each individual stream. Performance of our new table lookup stream stayed far below the results obtained in cross validation on previous year's data and further analysis will be necessary to fully assess the factors influencing its performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,220.77,502.11,8.07;2,53.80,231.23,502.11,8.07;2,53.80,241.69,502.11,8.07;2,53.80,252.15,502.11,8.07;2,53.80,262.61,185.40,8.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Quartz-2007: the University of Amsterdam's Question Answering System. After question analysis, a question is forwarded to two table modules and two retrieval modules, all of which generate candidate answers. These four question processing streams use the two data sources for this task, the AQUAINT newspaper corpus and the blog corpus, as well as fact tables which were generated from these data sources, and the Web. Related candidate answers are combined and ranked by a postprocessing module, which produces the final list of answers to the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,53.80,382.71,239.10,8.07;4,53.80,393.17,239.10,8.07;4,53.80,403.63,239.10,8.07;4,53.80,414.09,239.11,8.07;4,53.80,424.55,239.10,8.07;4,53.80,435.01,239.10,8.07;4,53.80,445.47,239.10,8.07;4,53.80,455.93,218.51,8.07"><head>Table 2 :</head><label>2</label><figDesc>Approximate factoid accuracy scores (exact+unsupported) for post-deadline experiments. First, the three submitted runs followed by the best submitted run with additional answer type checks. Then the performances of the six individual streams as well as that of a baseline system which always answers NIL. Finally, an evaluation of four combinations of the best three individual QA streams. The final combination of XQUESTA AQUAINT and web ngrams performs best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,55.13,241.93,313.61"><head>Table 1 : Results for the main QA task: our three submitted runs as well as the median scores of all 51 submitted runs. Our three runs used the same answers for other questions (scoring differences are caused by assessor inconsistencies). Our main system (uams07main) generates more correct factoid answers with an additional answer type check (uams07atch). The best results were achieved when the final web reranking based step was omitted (uams07nwrr).</head><label>1</label><figDesc></figDesc><table coords="4,55.02,55.13,240.70,313.61"><row><cell></cell><cell cols="2">factoid accuracy</cell><cell></cell></row><row><cell>run</cell><cell cols="2">(exact,local,unsup.,inex.)</cell><cell cols="2">list F other F overall</cell></row><row><cell>uams07main</cell><cell cols="3">0.072 , 0.003 , 0.011 , 0.056 0.032</cell><cell>0.209</cell><cell>0.104</cell></row><row><cell>uams07atch</cell><cell cols="3">0.083 , 0.003 , 0.019 , 0.061 0.032</cell><cell>0.191</cell><cell>0.103</cell></row><row><cell>uams07nwrr</cell><cell cols="3">0.092 , 0.006 , 0.036 , 0.064 0.028</cell><cell>0.198</cell><cell>0.105</cell></row><row><cell cols="2">median scores 0.131</cell><cell></cell><cell>0.085</cell><cell>0.118</cell><cell>0.108</cell></row><row><cell cols="2">factoid accuracy</cell><cell></cell><cell></cell></row><row><cell cols="2">exact+unsup.</cell><cell>run description</cell><cell></cell></row><row><cell></cell><cell>0.080</cell><cell cols="2">uams07main: main run</cell></row><row><cell></cell><cell>0.096</cell><cell cols="3">uams07atch: with extra type checks</cell></row><row><cell></cell><cell>0.117</cell><cell cols="3">uams07nwrr: without web reranking</cell></row><row><cell></cell><cell>0.168</cell><cell cols="2">nwrr + extra type checks</cell></row><row><cell></cell><cell>0.028</cell><cell>learned tables (6)</cell><cell></cell></row><row><cell></cell><cell>0.045</cell><cell cols="2">baseline: always answer NIL</cell></row><row><cell></cell><cell>0.048</cell><cell>tables with rules (5)</cell><cell></cell></row><row><cell></cell><cell>0.056</cell><cell>XQUESTA blog (4)</cell><cell></cell></row><row><cell></cell><cell>0.111</cell><cell cols="2">AQUAINT ngrams (3)</cell></row><row><cell></cell><cell>0.123</cell><cell cols="2">XQUESTA AQUAINT (2)</cell></row><row><cell></cell><cell>0.163</cell><cell>web ngrams (1)</cell><cell></cell></row><row><cell></cell><cell>0.154</cell><cell cols="2">combination: (2) + (3)</cell></row><row><cell></cell><cell>0.163</cell><cell cols="2">combination: (1) + (3)</cell></row><row><cell></cell><cell>0.166</cell><cell cols="2">combination: (1) + (2) + (3)</cell></row><row><cell></cell><cell>0.181</cell><cell cols="2">combination: (1) + (2)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,321.30,711.19,207.47,8.07"><p>http://ilps.science.uva.nl/∼erikt/trec2007/patterns2007.txt</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006 640.001.501</rs>, <rs type="grantNumber">640.002.501</rs>, and <rs type="grantNumber">C.2324.-0114</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yN2wTNB">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_2WxQ34R">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_u8X6HjB">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_zhH32yG">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_sKzTMeT">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_VANhZfj">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_QVe35je">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_uHTpC9h">
					<idno type="grant-number">612.069.006 640.001.501</idno>
				</org>
				<org type="funding" xml:id="_vwZDtSz">
					<idno type="grant-number">640.002.501</idno>
				</org>
				<org type="funding" xml:id="_gWMtEqk">
					<idno type="grant-number">C.2324.-0114</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,334.76,242.44,221.16,8.07;4,321.30,252.91,234.62,8.07;4,321.30,263.37,234.62,8.07;4,321.30,273.83,174.69,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,396.55,252.91,159.37,8.07;4,321.30,263.37,86.52,8.07">Towards a multi-stream question answeringas-xml-retrieval strategy</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fissaha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Tjong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,427.76,263.37,128.16,8.07;4,321.30,273.83,124.67,8.07">Proceedings of the Fourteenth Text Retrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text Retrieval Conference (TREC 2005)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,336.73,291.46,219.19,8.07;4,321.30,301.92,234.62,8.07;4,321.30,312.38,234.62,8.07;4,321.30,322.84,234.62,8.07;4,321.30,333.30,171.88,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,388.60,301.92,167.31,8.07;4,321.30,312.38,111.02,8.07">Representing and querying multi-dimensional markup for question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Alink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,450.39,312.38,105.53,8.07;4,321.30,322.84,234.62,8.07;4,321.30,333.30,145.68,8.07">Proceedings of the 5th Workshop on NLP and XML (NLPXML-2006): Multi-Dimensional Markup in Natural Language Processing</title>
		<meeting>the 5th Workshop on NLP and XML (NLPXML-2006): Multi-Dimensional Markup in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.53,350.94,222.38,8.07;4,321.30,361.40,234.62,8.07;4,321.30,371.86,234.62,8.07;4,321.30,382.32,153.34,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,427.00,350.94,125.26,8.07">N-Gram-Based Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Trenkle</surname></persName>
		</author>
		<ptr target="http://www.let.rug.nl/∼vannoord/TextCat/" />
	</analytic>
	<monogr>
		<title level="m" coord="4,332.07,361.40,223.84,8.07;4,321.30,371.86,147.55,8.07">Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.05,399.95,220.87,8.07;4,321.30,410.41,234.62,8.07;4,321.30,420.88,234.62,8.07;4,321.30,431.34,234.62,8.07;4,321.30,442.92,102.84,6.35" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,445.27,399.95,110.65,8.07;4,321.30,410.41,216.30,8.07">The University of Amsterdam at WebCLEF 2007: Using centrality to rank web snippets</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/2007/working_notes" />
	</analytic>
	<monogr>
		<title level="m" coord="4,446.54,420.88,109.37,8.07;4,321.30,431.34,55.85,8.07">Working Notes for the CLEF 2007 Workshop</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,334.22,459.43,221.69,8.07;4,321.30,469.89,234.62,8.07;4,321.30,480.35,234.62,8.07;4,321.30,490.81,20.17,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,370.51,469.89,181.37,8.07">The University of Amsterdam at QA@CLEF 2006</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van Rantwijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Tjong</forename><surname>Kim Sang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,331.24,480.35,160.18,8.07">Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,336.34,508.45,219.58,8.07;4,321.30,518.91,234.62,8.07;4,321.30,529.37,234.62,8.07;4,321.30,539.83,153.45,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,503.58,518.91,52.33,8.07;4,321.30,529.37,110.62,8.07">The university of amsterdam at clef@qa 2007</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van Rantwijk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Tjong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,451.06,529.37,104.86,8.07;4,321.30,539.83,53.75,8.07">Working Notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.08,557.46,220.84,8.07;4,321.30,567.92,234.62,8.07;4,321.30,578.38,234.62,8.07;4,321.30,588.85,112.27,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,491.83,557.46,64.09,8.07;4,321.30,567.92,146.92,8.07">Machine learning for question answering from tabular data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,486.37,567.92,69.55,8.07;4,321.30,578.38,234.62,8.07;4,321.30,588.85,85.94,8.07">FlexDBIST-07 Second International Workshop on Flexible Database and Information Systems Technology</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,333.99,606.48,184.69,8.07" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Monetdb</surname></persName>
		</author>
		<ptr target="http://www.monetdb.nl" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
