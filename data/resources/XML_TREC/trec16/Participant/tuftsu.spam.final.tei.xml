<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,218.28,84.76,175.43,12.93;1,191.40,102.64,229.34,12.93">Relaxed Online SVMs in the TREC Spam Filtering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.64,137.80,56.05,9.83"><forename type="first">D</forename><surname>Sculley</surname></persName>
							<email>dsculley@cs.tufts.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tufts University Medford</orgName>
								<address>
									<postCode>02155</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.23,137.80,118.20,9.83"><forename type="first">Gabriel</forename><forename type="middle">M</forename><surname>Wachman</surname></persName>
							<email>gwachm01@cs.tufts.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tufts University Medford</orgName>
								<address>
									<postCode>02155</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,218.28,84.76,175.43,12.93;1,191.40,102.64,229.34,12.93">Relaxed Online SVMs in the TREC Spam Filtering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8773F101A0C15A9C6EDBE827888F5353</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relaxed Online Support Vector Machines (ROSVMs) have recently been proposed as an efficient methodology for attaining an approximate SVM solution for streaming data such as the online spam filtering task. Here, we apply ROSVMs in the TREC 2007 Spam filtering track and report results. In particular, we explore the effect of various slidingwindow sizes, trading off computation cost against classification performance with good results. We also test a variant of fixed-uncertainty sampling for Online Active Learning. The best results with this approach give classification performance near to that of the fully supervised approach while requiring only a small fraction of the examples to be labeled. 1. Space restrictions preclude a general discussion of SVMs in this paper. Those wishing a general review of SVM methods are referred to the excellent text by [19].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It has been nearly ten years since Support Vector Machines (SVMs) were shown to give state of the art performance on high-dimensional classification problems in general, and text classification in particular <ref type="bibr" coords="1,243.50,383.86,16.96,10.91" target="#b10">[11,</ref><ref type="bibr" coords="1,264.99,383.86,12.72,10.91" target="#b11">12]</ref>. 1 In that time, the machine learning community has repeatedly suggested that SVMs are a strong choice for content-based spam filtering <ref type="bibr" coords="1,90.00,410.98,11.56,10.91" target="#b6">[7,</ref><ref type="bibr" coords="1,105.12,410.98,13.95,10.91" target="#b12">13,</ref><ref type="bibr" coords="1,122.65,410.98,12.72,10.91" target="#b17">18]</ref>. However, to our knowledge SVM methodology has yet to be applied in a largescale spam filtering application, such as an industrial scale email system. Spam filtering practitioners have preferred to apply other machine learning techniques. These methods include several variants of the Naive Bayes classifier <ref type="bibr" coords="1,342.72,451.66,11.44,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,357.97,451.66,8.43,10.91" target="#b8">9,</ref><ref type="bibr" coords="1,370.20,451.66,13.95,10.91" target="#b9">10,</ref><ref type="bibr" coords="1,387.96,451.66,12.72,10.91" target="#b15">16]</ref>, linear classifiers such as Logistic Regression <ref type="bibr" coords="1,184.07,465.22,11.44,10.91" target="#b7">[8]</ref> and Perceptron Algorithm variants <ref type="bibr" coords="1,367.20,465.22,15.97,10.91" target="#b21">[22]</ref>, compression-based methods <ref type="bibr" coords="1,90.00,478.78,11.56,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,105.12,478.78,7.70,10.91" target="#b2">3]</ref>, and ensemble methods <ref type="bibr" coords="1,231.97,478.78,15.97,10.91" target="#b14">[15]</ref>.</p><p>Anti-spam practitioners have avoided SVMs for two reasons. First, until recently it was considered unclear whether SVMs actually gave state of the art performance on online spam filtering tasks <ref type="bibr" coords="1,157.81,520.18,11.44,10.91" target="#b3">[4,</ref><ref type="bibr" coords="1,172.69,520.18,7.62,10.91" target="#b4">5]</ref>. However, experiments on the benchmark spam filtering data sets from TREC 2005 and TREC 2006 have been reported that show SVMs indeed give state of the art classification performance on these tasks, exceeding that of all other single classifiers, and well within confidence bounds of the mark set by ensemble methods <ref type="bibr" coords="1,439.92,560.74,16.06,10.91" target="#b20">[21]</ref>.</p><p>Second, SVMs carry high training cost that make them impractical in large-scale settings. Relaxed Online SVMs (ROSVMs) have been proposed that compute an approximate SVM solution at greatly reduced expense <ref type="bibr" coords="1,293.30,602.26,15.97,10.91" target="#b20">[21]</ref>. ROSVMs use a sliding window to cap the size of the classical SVM optimization problem for streaming data, such as a (potentially limitless) series of messages to be classified. ROSVMs further reduce cost by updating their model less frequently than classical SVMs, and by allowing early termination by the iterative SVM solver. ROSVMs have given near state of the art performance on the TREC 2005 and 2006 public data sets; we now test them on TREC 2007 data and tasks.</p><p>In the remainder of this paper, we review the ROSVM algorithm and its implementation. We detail the Online Active Learning approach used by our ROSVM in the current tests.</p><p>We report results on the TREC 2007 Spam Filtering tasks on the trec07p data set, and compare these to baseline results given by one standard open-source spam filter. Our concluding discussion focuses on identifying open questions in spam filtering research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relaxed Online SVMs</head><p>The ROSVM methodology gives an approximate Online SVM solution at much reduced cost that does not grow with the amount of data that has been encountered <ref type="bibr" coords="2,463.09,222.46,16.06,10.91" target="#b20">[21]</ref>. This is accomplished with three base strategies, which we review briefly in this section.</p><p>Sliding Window. Recall that SVMs classify a new example x with the function:</p><formula xml:id="formula_0" coords="2,237.72,275.52,136.56,33.66">f (x) = m i α i y i &lt; x i , x &gt; +b</formula><p>Here, each of the n training examples x i has an associated label y i and weight α i . An SVM solver such as Platt's SMO finds optimal values for each of the n alpha values by solving a now classic quadratic optimization problem <ref type="bibr" coords="2,348.03,344.86,16.96,10.91" target="#b16">[17,</ref><ref type="bibr" coords="2,368.67,344.86,12.69,10.91" target="#b18">19]</ref>. As the number of examples in the training data grows, the number of weights to set also grows and the cost of training increases.</p><p>In the sliding window strategy, we restrict the size of the optimization problem by only optimizing the weights for the p most recent examples. That is, at step n (where n &gt; p) we treat the set of examples {x n-p+1 , .., x n } as our set of training data, and only optimize the values for {α n-p+1 , .., α n }. Each weight value α i ∈ {α 1 , .., α n-p } associated with an example outside the sliding window is fixed at the value it was most recently given when x i was still in the sliding window. This reduces the size of the optimization problem from one that increases with n to one that is bound by p. Experiments reported in <ref type="bibr" coords="2,467.30,466.78,16.96,10.91" target="#b20">[21]</ref> showed that near state of the art results can be produced with a relatively small lookback buffer of size 5000 or less, dramatically reducing computational cost.</p><p>Our submissions to TREC 2007 explore three sizes of lookback buffer, with p set to 500, 1000, and 5000, in order to explore the tradeoff between computational cost and classification performance.</p><p>Redefining Well Classified. For Online SVMs, it is only necessary to update the model when an example is encountered that is not well classified. The formal definition of well classified in this setting is defined by the Karush-Kuhn-Tucker (KKT) conditions, which will be satisfied by an Online SVM on all new examples x i except those for which y i f (x i ) ≤ 1. Intuitively, we must re-train an Online SVM whenever we encounter an example that lies within the margins or which is mistakenly classified. This will guarantee that our model is consistent with all the data seen so far.</p><p>The ROSVM approach relaxes this requirement by re-defining what it means to be well classified. We set a parameter m, where 0 ≤ m ≤ 1, and only re-train when we find an example for which y i f (x i ) ≤ m. That is, we allow a model to continue classifying so long as it has been shown to be only slightly inconsistent with the data seen so far. This approach reduces cost by reducing the number of times we update the model. Experiments reported in <ref type="bibr" coords="3,103.45,102.70,16.96,10.91" target="#b20">[21]</ref> show that, for spam data, a value of m = 0.8 gives results indistinguishable from state of the art results at half the cost, and values as low as m = 0 performs nearly as well with additional savings. In our submissions, we use m = 0.8</p><p>Reducing Iterations Platt's SMO is an iterative solver, making repeated passes over the data set to find optimal alpha values <ref type="bibr" coords="3,293.55,166.66,15.97,10.91" target="#b16">[17]</ref>. The third cost-reducing strategy is to enforce a maximum number of iterations in the outer loop of the SMO algorithm (complete SMO pseudo-code is given in <ref type="bibr" coords="3,236.18,193.66,16.11,10.91" target="#b16">[17]</ref>), limiting the total number of passes over the training data for each update. This is similar to setting a loose tolerance for convergence, but is perhaps a more Draconian approach. Our previous tests have shown that restricting the maximum number of iterations to even very low values has negligible effect on classification performance for online spam filtering tasks <ref type="bibr" coords="3,298.23,247.90,15.97,10.91" target="#b20">[21]</ref>. This is because there are many successive training updates over the course of online filtering. In our submissions, we set the maximum number of SMO outer-loop iterations to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation Issues</head><p>In this section, we explore some of the implementation issues involved in making the ROSVMs run fast enough to participate in the TREC spam filtering task.</p><p>Binary 4-mer Feature Space. Our tests have found that a binary 4-mer feature space gives consistently best results on spam data. This is a high dimensional feature space, containing a dimension for each possible substring of length exactly 4 drawn from the alphabet of all possible single-byte characters. This approach allows inexact string matching <ref type="bibr" coords="3,90.00,419.50,15.97,10.91" target="#b13">[14]</ref>, which provides a measure of robustness against spammer techniques such as word obfuscation <ref type="bibr" coords="3,148.33,433.06,15.97,10.91" target="#b22">[23]</ref>. This feature space gives significantly improved performance over a more traditional word-based feature space <ref type="bibr" coords="3,274.34,446.62,15.97,10.91" target="#b20">[21]</ref>. This choice of feature space also allows the filtering method to be language-independent, and to learn from non-textual data such as attachments. <ref type="foot" coords="3,150.96,472.66,4.23,6.13" target="#foot_0">2</ref> Our tests have agreed with results reported by <ref type="bibr" coords="3,400.58,473.74,11.44,10.91" target="#b6">[7]</ref> and <ref type="bibr" coords="3,439.36,473.74,15.97,10.91" target="#b15">[16]</ref>, finding that binary valued features out-perform count-based and TFIDF-based feature scoring methods on the spam filtering data sets that we have tested. We map binary 4-mer features for the first 3000 characters of a given message string. The feature vectors are normalized using the Euclidian norm during classification and training.</p><p>The drawback to this representation is the dimensionality is even higher than standard text-classification problems. In the worst case, there could be 2 32 unique features present in the data. In practice, we have found 3,700,809 unique 4-mers in the trec07p data set, 3,332,440 unique 4-mers in the trec05p-1 data set, and 1,424,606 unique 4-mers in the trec06p data set when considering the first 3,000 characters of each message. <ref type="foot" coords="3,464.88,595.30,4.23,6.13" target="#foot_1">3</ref> Thus, the feature space remains sparse -the vast majority of possible 4-mers features do not occur.</p><p>Sparse Binary Inner Products For a linear SVM, computing inner products for two vectors is the core task in an SVM solver such as Platt's SMO <ref type="bibr" coords="4,410.90,89.14,15.95,10.91" target="#b16">[17]</ref>. We optimize our ROSVM for the special case of computing inner products on sparse binary vectors, allowing several optimizations over the traditional linked-list implementation of sparse vectors. The inner products are fast enough that a cache of inner-product values <ref type="bibr" coords="4,426.00,129.82,17.07,10.91" target="#b18">[19]</ref> is not required, greatly reducing the memory footprint of the system.</p><p>In our implementation, each example is stored as an array of integers, each integer indexing a particular non-zero feature. The array of feature indices is stored in sorted order, from lowest to highest. This allows a binary inner product to be computed with no pointer-redirects (as occur with linked-list implementations), excellent memory locality, and using only comparison and increment operators.</p><p>Alpha Seeding. As reported by <ref type="bibr" coords="4,264.37,231.70,10.92,10.91" target="#b5">[6]</ref>, alpha seeding can speed up the training time of SVMs considerably. The main idea is that starting with a good initial guess α ′ i that is close to the optimal value α i for each example x i dramatically reduces the amount of work required for an iterative SVM solver, such as SMO, to converge to the optimal solution. Because we are working in an online setting where updates happen incrementally, we use the assumption that the set of alpha values found in the previous update is a good initial starting point for the next update. In practice, this means that we can simply keep the alpha values in memory after the SVM solver has finished updating its hypothesis.</p><p>Sliding Window. We use a ring buffer store the p most recent examples in the data stream. Each time we encounter a new example, it takes the place of the oldest example in the buffer. The alpha value of the new example is initially set to 0. All SVM updates are done using this buffer as the training data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear Hypothesis.</head><p>Because we deal only with the linear kernel, we follow the suggestion of <ref type="bibr" coords="4,429.48,434.98,16.96,10.91" target="#b16">[17]</ref> and rewrite the dual form SVM classification function to the primal form, using a single aggregate weight vector to store the hypothesis:</p><formula xml:id="formula_1" coords="4,90.00,475.54,262.56,32.85">f (x) =&lt; w, x &gt; +b Naturally, w = m i α i y i x i .</formula><p>Because the SMO algorithm uses the current hypothesis to determine convergence and to select candidates for optimization, it is necessary to maintain a current aggregate weight vector at all times. Thus, every time an alpha value α i for an example x i is changed during training to a new value α ′ i , the weight vector is updated with:</p><formula xml:id="formula_2" coords="4,252.24,559.31,107.05,13.28">w = w + (α ′ i -α i )y i x i</formula><p>When an example x j leaves the sliding window of examples, we do not alter the weight vector w, but we do remove the example from the ring buffer of examples and no longer update its alpha value. Thus, α j is fixed permanently at its most recent optimal value. similar to the setting explored in <ref type="bibr" coords="5,245.06,168.82,16.96,10.91" target="#b19">[20]</ref> -a filter was shown one message at a time and asked to make a classification prediction. After prediction, the filter was allowed to request a groundtruth label for that message, with the goal of achieving strong classification performance with as few label requests as possible. The TREC scenario added two additional factors: the maximum amount of label requests a filter could make was fixed in advance by a label quota, and the filter was told how many messages remained in the online filtering task at a given point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Online Active Learning</head><p>Our approach to the active task is based on the fixed margin sampling method proposed in <ref type="bibr" coords="5,103.93,277.42,15.97,10.91" target="#b19">[20]</ref>: a threshold t was fixed, and label requests were made whenever a message was classified with |f (x)| &lt; t until the label quota was exhausted. We used a slight variant of this idea: when there were at least 50 label requests available, we set t = 0.8, and when there were fewer than 50 remaining, we set t = 0.2. The goal of this heuristic was to reserve some label requests for particularly difficult messages appearing later in the data stream. However, our results show that this approach was not ideal, as in two cases not all the labels were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The experiments we report here are all performed on the trec07p data set of 50,199 spam emails and 25,220 ham emails, and the private mrx3 corpus of 8082 ham and 153893 spam, each set in a canonical ordering for online evaluation. We applied three filters, all of which were ROSVM-based filters with parameter settings as shown in Table <ref type="table" coords="5,429.72,451.78,4.22,10.91" target="#tab_0">1</ref>, which were fixed before any tests were run on this data set. For a rough estimate of speed, Table <ref type="table" coords="5,493.68,465.34,5.45,10.91" target="#tab_0">1</ref> also shows wall-clock times for a complete run on the trec07p full task, using a Sunfire x86 64 machine with 8G RAM with no other load. Note that these timing results were performed using a pre-tokenized form of this data set, in order to remove overhead common to all filtering systems. All other results in this paper were found using the interface from the TREC Spam toolkit, which was significantly slower.</p><p>There were four main tasks for each of our three filters on these data sets. The full task gave supervised feedback to the filter after every prediction. The delay task gave supervised feedback to the filter for only the first 10,000 messages in the corpus. The partial task provided feedback for 30,388 messages in the corpus that belong to a subset of users. Finally, the active task allowed the filter to request a label for any message immediately following classification during online filtering. A maximum of 1000 labels were provided; all other messages were unlabeled for this task. Further details of the experimental design are available in the TREC 2007 Spam Track overview.</p><p>Results on full task. The results for all three ROSVM filters were quite strong on the full feedback tasks, with (1-ROCA)% scores of about 0.01 on trec07p and as low as 0.0042 Table <ref type="table" coords="7,120.00,338.62,4.22,10.91">3</ref>: Summary of Results on (1-ROCA)% measure for experiments on trec07p data set with three partial evaluations. The first column gives results on partial feedback evaluating only those messages that belong to the partial users. The second column shows results on partial feedback for all messages other than those of the partial users. In the third column, we report results evaluating those messages that belong to the partial users when full feedback is given on all messages in the corpus.</p><p>on mrx3 (see Table <ref type="table" coords="7,184.19,468.46,4.21,10.91" target="#tab_1">2</ref>). We were surprised to see that on trec07p, using a lookback buffer of size 5000 did not give significantly better results than the buffer of size 500. This is in contrast to the results on mrx3 and previous results on the trec06p and trec05p-1 data sets, where the larger lookback buffer gave improved performance. For all filters, there was a performance hit (evident as an upward spike in the learning curves in Figure <ref type="figure" coords="7,170.65,538.90,4.82,10.91">1</ref>) just after 20,000 messages on trec07p. We found that this was caused by message 23337 in the data set, which appeared to have a noisy label. The message is a German-language stock-tip message in all capital letters -apparently spam. All of our filters labeled this message as spam with high confidence, but it was given a ground truth label of ham. The effect of this noisy label did not permanently degrade results, as all filters recovered to the 0.01 (1-ROCA)% level over time.</p><p>Results on delay task. As shown in Table <ref type="table" coords="7,305.88,635.98,4.22,10.91" target="#tab_1">2</ref>, our filters gave reduced (but still reasonably strong) performance on the delay task for both trec07p and mrx3. This shows that while a filter may be expected to generalize into the future, there is (at least in these data sets) some amount of measurable concept drift associated with spam data. Results on partial task Surprisingly to us, this setting proved the most difficult in the group and our filters had far lower (1-ROCA)% scores on this task than even on the delay task, despite getting feedback on more than three times as many messages. Examining the results further shows that this degradation in performance is not due to overall lack of labels, as the performance on the partial user's own messages was very strong. Rather, we find that the model trained specifically on the partial users' messages does not transfer well to messages sent to other people -as shown by the very poor (1-ROCA)% scores when considering only these other messages. (See Table <ref type="table" coords="8,331.55,330.22,4.22,10.91">3</ref>.)</p><p>Interestingly, we find that training on the other messages (in addition to the partial user's) does not significantly harm performance on the partial user's messages. This suggests that in a multi-user spam-filtering system, it may be equally good to have individual user filters or a single system-wide filter. However, an individually trained filter may not be expected to perform well for different users.</p><p>Results on active task Our results for the active task fall into two groups. The tftS1F filter had a (1-ROCA)% score of roughly 0.014, which approaches the result for full feedback while requesting labels for only one message in seventy-five. For tftS1F and tftS3F, results were significantly worse than that of full feedback -in the 0.04 range on the (1-ROCA)% score. The difference between these two groups was that the strong results were generated by a filter that used its entire label quota, while the weaker results were given by filters that did not exhaust their label quotas. This highlights one of the weaknesses of the fixed-margin sampling method -it is difficult to determine a priori exactly how many labels the filter will request for a given choice of t.</p><p>We considered the possibility that an optimal strategy would be to simply request many labels at the beginning of the task, so that any learned knowledge would be applied for as long as possible in the online setting. However, when we tested the effect of requesting labels for the first 1000 examples, we found that these results were significantly worse than using 1000 labels selected with active learning. This was even true for the first 10,000 labeled examples (as tested in the delay task) in comparison with the best of the three active results using 1000 labels. For completeness, we also with 1000 labels sampled uniformly at random, and found that these results were far worse than the active learners. These results are summarized in Table <ref type="table" coords="8,211.68,676.66,4.22,10.91" target="#tab_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion: Future Directions</head><p>The top-level performance at TREC has consistently increased over the three years of the spam filtering track, to the point where performance on the fully supervised online filtering task is approaching perfection, with ROC areas greater than 0.9999 and messages mis-classified at a rate of one in a thousand or better -even when starting from zero knowledge. It seems unlikely that inter-annotator agreement would be high enough from human evaluations to make further improvements measurable in this domain.</p><p>However, this scenario of full feedback on all messages is unrealistic in live settings. The delay, partial, and active tasks from this year's competition were all strong steps towards addressing differences between laboratory and real-world settings. We plan to continue work in this vein, especially with regard to noisy labels, malicious labels, and the use of unlabeled data.</p><p>Additionally, there are a wide variety of spam domains that are, as yet, unexplored on a TREC-level scale. These include detecting content-based spam in blogs, wikis, social networking sites, SMS text messages, image sharing sites, and collaborative tagging efforts -to name but a few. Continued work transferring and augmenting techniques from email spam filtering to these domains is needed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,663.10,431.99,24.47"><head>Table 1 :</head><label>1</label><figDesc>Parameter settings and wall-clock computation time on full task for tftS filters.</figDesc><table coords="5,96.96,78.93,418.34,46.20"><row><cell>filter</cell><cell cols="5">sliding window update margin max iters cost C wall clock (full)</cell></row><row><cell>tftS1F</cell><cell>500</cell><cell>0.8</cell><cell>1</cell><cell>100</cell><cell>224s</cell></row><row><cell>tftS2F</cell><cell>1000</cell><cell>0.5</cell><cell>1</cell><cell>100</cell><cell>214s</cell></row><row><cell>tftS3F</cell><cell>5000</cell><cell>0.8</cell><cell>1</cell><cell>100</cell><cell>3087s</cell></row></table><note coords="4,90.00,663.10,431.98,10.91;4,90.00,676.66,431.99,10.91"><p>This year's TREC Spam Filtering track included an Online Active Learning task, rather than the previous pool-based Active Learning task from 2006. The structure of this task was</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,82.00,432.07,596.21"><head>Table 2 :</head><label>2</label><figDesc>Summary of Results on (1-ROCA)% measure for experiments on trec07p data set with full feedback, partial feedback from one user only, delay feedback with labels provided only for the first 10,000 messages, and active learning selection of 1000 labels. Results for spamprobe are included for comparison.</figDesc><table coords="6,90.00,82.00,432.07,596.21"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS1F-active</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS1F-delay</cell></row><row><cell></cell><cell>50.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS1F-full tftS1F-partial</cell></row><row><cell>(1-ROCA)% (logit scale)</cell><cell>10.00 1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10000</cell><cell>20000</cell><cell>30000</cell><cell>40000</cell><cell>50000</cell><cell>60000</cell><cell>70000</cell><cell>80000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Messages</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS2F-active</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS2F-delay</cell></row><row><cell></cell><cell>50.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS2F-full tftS2F-partial</cell></row><row><cell>(1-ROCA)% (logit scale)</cell><cell>10.00 1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10000</cell><cell>20000</cell><cell>30000</cell><cell>40000</cell><cell>50000</cell><cell>60000</cell><cell>70000</cell><cell>80000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Messages</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS3F-active</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS3F-delay</cell></row><row><cell></cell><cell>50.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tftS3F-full tftS3F-partial</cell></row><row><cell>(1-ROCA)% (logit scale)</cell><cell>10.00 1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10000</cell><cell>20000</cell><cell>30000</cell><cell>40000</cell><cell>50000</cell><cell>60000</cell><cell>70000</cell><cell>80000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Messages</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 1: Learning curves for tftS1F, tftS2F, and tftS3F on full, delay, partial, and</cell></row><row><cell cols="6">active tasks using trec07p data set.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,78.93,431.92,100.08"><head>Table 4 :</head><label>4</label><figDesc>Active Learning Comparisons. Results for tests on trec07p with active learning with 1000 label requests, compared to scenarios where only the first 1000 messages were labeled, or 1000 were chosen uniformly at random for labeling.</figDesc><table coords="8,193.44,78.93,218.23,46.20"><row><cell>filter</cell><cell cols="3">active first 1000 uniform 1000</cell></row><row><cell cols="2">tftS1F 0.0413</cell><cell>0.1095</cell><cell>2.9636</cell></row><row><cell cols="2">tftS2F 0.0144</cell><cell>0.1105</cell><cell>2.9541</cell></row><row><cell cols="2">tftS3F 0.0476</cell><cell>0.1092</cell><cell>2.8670</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,105.00,634.32,416.93,8.97;3,105.00,645.24,417.14,8.97;3,105.00,656.28,57.28,8.97"><p>Interestingly, we have also found that using additional inexact-string matching features, such as gappy features and wildcard features<ref type="bibr" coords="3,226.91,645.24,14.31,8.97" target="#b21">[22]</ref> do not give added performance over the simpler 4-mers with ROSVMs on spam data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,105.00,667.20,416.88,8.97;3,105.00,678.12,225.82,8.97"><p>Gordon Cormack has suggested hashing these features to reduce the dimensionality of this problem, which is a suggestion we will explore in upcoming work.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.45,352.30,409.76,10.91;9,112.44,365.86,409.45,10.91;9,112.44,379.42,409.54,10.91;9,112.44,392.98,72.87,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,131.88,365.86,237.52,10.91">An evaluation of naive bayesian anti-spam filtering</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutsias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">V</forename><surname>Chandrinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,390.00,365.86,131.89,10.91;9,112.44,379.42,409.54,10.91;9,112.44,392.98,39.50,10.91">Proceedings of Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning</title>
		<meeting>Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.45,416.14,409.60,10.91;9,112.44,429.70,409.76,10.91;9,112.44,443.14,70.37,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,237.76,416.14,192.23,10.91">Spam filtering using compression models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Filipic</surname></persName>
		</author>
		<idno>IJS-DP-9227</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,468.62,429.70,48.92,10.91">L jubljana</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Jozef Stefan Institute</publisher>
			<pubPlace>Slovenia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Intelligent Systems</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,112.45,466.30,409.64,10.91;9,112.44,479.86,373.24,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,424.20,466.30,97.88,10.91;9,112.44,479.86,166.94,10.91">Spam filtering using statistical data compression models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Filipič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,289.44,479.86,98.95,10.91">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2673" to="2698" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.45,502.90,409.29,10.91;9,112.44,516.46,326.43,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,249.98,502.90,189.85,10.91">Batch and on-line spam filter evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,463.08,502.90,58.66,10.91;9,112.44,516.46,292.77,10.91">CEAS 2006: Proceedings of the Third Conference on Email and Anti-Spam</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.45,539.62,409.39,10.91;9,112.44,553.18,228.41,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,249.14,539.62,192.86,10.91">On-line supervised spam filter evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,451.20,539.62,70.64,10.91;9,112.44,553.18,139.29,10.91">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.45,576.22,409.63,10.91;9,112.44,589.78,409.41,10.91;9,112.44,603.34,231.51,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,263.29,576.22,206.43,10.91">Alpha seeding for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,497.16,576.22,24.92,10.91;9,112.44,589.78,409.41,10.91;9,112.44,603.34,123.42,10.91">KDD &apos;00: Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="345" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.45,626.50,409.12,10.91;9,112.44,640.06,304.60,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,286.02,626.50,230.92,10.91">Support vector machines for spam categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.44,640.06,187.46,10.91">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1048" to="1054" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.45,663.10,409.54,10.91;9,112.44,676.66,297.63,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,237.76,663.10,195.06,10.91">Online discriminative spam filter training</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,455.04,663.10,66.95,10.91;9,112.44,676.66,263.39,10.91">Proceedings of the Third Conference on Email and Anti-Spam (CEAS)</title>
		<meeting>the Third Conference on Email and Anti-Spam (CEAS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,75.70,170.11,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,172.34,75.70,75.21,10.91">A plan for spam</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,97.42,207.31,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Graham</surname></persName>
		</author>
		<title level="m" coord="10,172.34,97.42,113.88,10.91">Better bayesian filtering</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,119.14,409.57,10.91;10,112.44,132.70,409.45,10.91;10,112.44,146.26,191.79,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,180.84,119.14,341.18,10.91;10,112.44,132.70,78.71,10.91">Text categorization with suport vector machines: Learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.56,132.70,301.33,10.91;10,112.44,146.26,83.78,10.91">ECML &apos;98: Proceedings of the 10th European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,167.98,409.46,10.91;10,112.44,181.54,409.63,10.91;10,112.44,195.10,24.63,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,177.48,167.98,344.42,10.91;10,112.44,181.54,121.43,10.91">Advances in Kernel Methods -Support Vector Learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<editor>B. Scholkopf and C. Burges and A. Smola</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT-Press</publisher>
		</imprint>
	</monogr>
	<note>Making large-scale SVM learning practical</note>
</biblStruct>

<biblStruct coords="10,112.44,216.94,409.36,10.91;10,112.44,230.50,409.52,10.91;10,112.44,243.94,340.83,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,248.30,216.94,273.51,10.91;10,112.44,230.50,102.35,10.91">SVM-based filtering of e-mail spam with content-specific misclassification costs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alspector</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,239.52,230.50,282.44,10.91;10,112.44,243.94,307.36,10.91">Proceedings of the TextDM&apos;01 Workshop on Text Miningheld at the 2001 IEEE International Conference on Data Mining</title>
		<meeting>the TextDM&apos;01 Workshop on Text Miningheld at the 2001 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,265.78,409.54,10.91;10,112.44,279.34,409.42,10.91;10,112.44,292.90,111.53,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,300.64,265.78,221.34,10.91;10,112.44,279.34,97.01,10.91">The spectrum kernel: A string kernel for svm protein classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.92,279.34,264.97,10.91">Proceedings of the Pacific Symposium on Biocomputing</title>
		<meeting>the Pacific Symposium on Biocomputing</meeting>
		<imprint>
			<date type="published" when="2002-01">January 2002</date>
			<biblScope unit="page" from="564" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,314.62,409.43,10.91;10,112.44,328.18,409.49,10.91;10,112.44,341.74,280.59,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,322.12,314.62,121.77,10.91">On-line spam filter fusion</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,468.84,314.62,53.04,10.91;10,112.44,328.18,409.49,10.91;10,112.44,341.74,173.32,10.91">SIGIR &apos;06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,363.46,409.63,10.91;10,112.44,377.02,376.23,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Metsis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<title level="m" coord="10,358.86,363.46,163.21,10.91;10,112.44,377.02,342.00,10.91">Spam filtering with naive bayeswhich naive bayes? Third Conference on Email and Anti-Spam (CEAS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,398.74,409.32,10.91;10,112.44,412.30,409.59,10.91;10,112.44,425.86,255.63,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,155.06,398.74,366.70,10.91;10,112.44,412.30,41.20,10.91">Sequenital minimal optimization: A fast algorithm for training support vector machines</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.48,412.30,95.55,10.91;10,112.44,425.86,165.01,10.91">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.44,447.70,409.54,10.91;10,112.44,461.26,409.59,10.91;10,112.44,474.70,24.63,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,214.35,447.70,307.64,10.91;10,112.44,461.26,41.83,10.91">Exploring support vector machines and random forests for spam detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,179.88,461.26,336.24,10.91">Proceedings of the First Conference on Email and Anti-Spam (CEAS)</title>
		<meeting>the First Conference on Email and Anti-Spam (CEAS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.45,496.54,409.51,10.91;10,112.44,510.10,267.63,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="10,251.76,496.54,270.19,10.91;10,112.44,510.10,175.30,10.91">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.45,531.82,409.62,10.91;10,112.44,545.38,367.23,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,171.74,531.82,328.65,10.91">Online active learning methods for fast label-efficient spam filtering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.44,545.38,361.89,10.91">CEAS 2007: Proceedings of the Fourth Conference on Email and Anti-Spam</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.45,567.10,409.50,10.91;10,112.44,580.66,244.47,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,252.64,567.10,184.17,10.91">Relaxed online SVMs for spam filtering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,457.68,567.10,64.27,10.91;10,112.44,580.66,211.27,10.91">The Thirtieth Annual ACM SIGIR Conference Proceedings</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.45,602.50,409.51,10.91;10,112.44,615.94,409.48,10.91;10,112.44,629.50,212.07,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,312.29,602.50,209.67,10.91;10,112.44,615.94,249.42,10.91">Spam filtering using inexact string matching in explicit feature space with on-line linear classifiers</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wachman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,383.64,615.94,138.28,10.91;10,112.44,629.50,178.87,10.91">The Fifteenth Text REtrieval Conference (TREC 2006) Proceedings</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.45,651.34,409.52,10.91;10,112.44,664.90,174.99,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,245.09,651.34,169.30,10.91">On attacking statistical spam filters</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Wittel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,422.40,651.34,99.56,10.91;10,112.44,664.90,141.33,10.91">CEAS: First Conference on Email and Anti-Spam</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
