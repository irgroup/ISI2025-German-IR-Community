<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_6EVkZSD">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D253815B352258817664A1C99BD67DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Oregon Health &amp; Science University submission to the TREC 2007 Genomics Track approached the entity list question answering task using a modular object oriented system framework. A system object coordinates a collection of processing objects into a pipe that constructs a set of queries, retrieves passage, and then processes those passages into a final output answer set. Using the framework we applied multiple levels of synonym expansion and a ranked series of topic queries with a range of specificities in order to retrieve all of the likely relevant passages with the most likely ranked higher. We then applied sentence pruning to the head and tail of each passage using both NLP and term-based techniques.</p><p>Overall scores finished around the TREC Genomics mean for each of the four measures. Careful passage retrieval, including synonym expansion and multiple query construction, as well as sentence pruning was essential in achieving acceptable performance on this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The 2007 Text Retrieval Conference (TREC) Genomics Track consisted of a single question answering task, which was scored by four measures. The question answering task presented a biological question where the expected answer was to include one or more entities of a given type. There were 36 separate question topics covering 14 different entity types. These topics were collected by surveying working biologists who were asked to provide questions from their own research. The topics were rephrased into questions which asked for a specific entity type. The challenge was to extract text passages specifically answering each topic question from a large corpus of over 160,000 biomedical full text articles selected from journals known to publish papers on genomics research. The answers for each topic were pooled and then evaluated by domain experts to create a gold standard. Each submission was scored and averaged over all of the topics for document mean average precision (MAP), character-based mean passage precision, and aspect precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>With the ever-expanding knowledge base of science embedded in the biomedical literature, the importance of effective text processing tools to aid biomedical researchers continues to increase <ref type="bibr" coords="1,474.53,174.78,83.52,9.02;1,324.00,186.79,18.06,9.02" target="#b1">(Cohen and Hersh, 2005</ref><ref type="bibr" coords="1,342.05,186.79,106.07,9.02" target="#b2">, Hunter and Cohen, 2006</ref><ref type="bibr" coords="1,448.12,186.79,65.92,9.02" target="#b3">, Roberts, 2006)</ref>. One way that the biomedical literature is used by scientists is to determine what is currently known about a subject related to their field of research. For automated systems to aid scientists with this task, the task must be framed in a formal manner amenable to machine processing. One formal, yet flexible way to represent these kinds of information needs is as list entity type questions. In list entity questions, the answer to the information need includes one or more entities (or things) of a given type. The type is given as part of the question and can be used by the information system to enhance the ability of the system to address the question. Since the type of the concepts of interests that answer a given question are often obvious to the biologist from the question itself, this form of information need representation can be both useful to the biologist and convenient for the computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The OHSU Biomedical Question Answering System Framework</head><p>For example, suppose a biologist is interested in understanding what treatments have been tested for Alzheimer's disease in mouse models. Assuming that the treatments are pharmacological agents, the question can be represented as "What [DRUGS] have been tested in mouse models of Alzheimer's disease?". The list entity type is given in uppercase, surrounded by square brackets. This is both easy for the user to input and provides quite a bit of information and context for the question answering system to work with.</p><p>Useful answers to these types of questions will include not just a list of the relevant entities, but also the surrounding text passage. The user then reviews the passages, determine which provide substantiated entities and answers, and understand the meaning and relationships present within the answer to the question. A good set of results will contain both a high proportion of substantiated and correct answers, as well as covering a high proportion of the different entities that answer the question. For example, if the question is "What centrosomal <ref type="bibr" coords="1,376.03,630.81,38.95,9.02">[GENES]</ref> are implicated in diseases of brain development?" and there are ten genes involved, then ten correct passages all about the same gene are less desirable than ten passages each about one of the involved genes.</p><p>The TREC 2007 Genomics track task attempted to emulate this list entity question-based type of information need in a controlled, comparable form, using a specific full text literature corpus. The literature corpus was the same as used in the 2006 Genomics track and consisted of 162,259 full text HTML articles, published between the years 1995 and 2006, and downloaded with the permission of the publishers from the Highwire Press web site (http://highwire.stanford.edu/). This literature corpus includes 49 journals that were known to publish articles on genomics subjects. However, the articles in the corpus included everything available from the web site, not just genomics articles.</p><p>Using the given literature corpus, the task was set up to be a extraction-based question answering task with the additional requirement that each relevant passage must include at least one correct entity of the given type. Questions were given in the form as shown above, as sentences with the entity type given in square brackets. Answers were required to come from contiguous passages in the fixed literature corpus. Systems were to submit a ranked list of up to 1000 character offset passages for each of the 36 topic questions. The character based passage specified the PubMed ID (PMID), starting offset in characters, and length of passage corresponding to the passage within the HTML file being nominated as relevant to answering the question.</p><p>Submissions were free to use any size passage they desired, whether that be sentences, sentences fragments, paragraphs, etc. However, the maximum allowable passage was restricted by enforcing the rule that a passage could not include any HTML &lt;P&gt; or &lt;/P&gt; tags. This was the same maximum passage criteria used in the 2006 Genomics track. This effectively limited submitted passages to around one paragraph of text, although entire reference sections from the end of papers sometimes were included by this rule. A file of the maximum legal spans was provided by the track administrators.</p><p>The submission for a system consisted of a ranked list of up to 1000 passages for each topic. A character-based gold standard set of answers was created from the submissions by using pooling, combined with expert judging. Submitted answers were mapped to their containing maximum legal span, and then were pooled for judging by taking the top ranked spans from each entry until 1000 spans were collected for each topic.</p><p>Human judges with expertise in biology were then asked to rate each pooled span for relevance, and select the relevant answer text from a plain text version from the pooled HTML span. These plain text selections were then mapped back to the original HTML file using a string alignment algorithm to create the gold standard set of passages. Each gold standard passage also had assigned to it by the judges one or more entity terms. The set of entity terms were chosen by the judges when reviewing the passages for relevance, and then assigned to each relevant passage in a second review pass. This ensured consistent application of the entities with each topic question.</p><p>Each system was scored four ways, each measure being a variation of mean average precision (MAP). The first measure was DOCUMENT MAP. This took the highest ranked passage for a document as the document rank. The document MAP was computed in the standard way from this ranked list of PMIDs</p><p>The second and third measures used were characterbased passage MAP scores, which measured the cumulative overlap between characters in relevant and nominated passages at each point of correct passage recall. The PASSAGE measure was the same as used in the 2006 Genomics track, and measured the fraction of relevant characters averaged over each nominated passage. The PASSAGE2 measure was added this year. It measures the average fraction of relevant characters averaged over each nominated character. In essence, every character is a little document, and is relevant or not. This measure is somewhat more robust to passage length and was intended to replace PASSAGE as the primary character measure this year. These measures evaluate the proportion of retrieved text that is relevant to answering the user's question. For example, a passage score of 0.50 would imply that approximately half the characters in the retrieved passage was included with a correct answer to the topic questions.</p><p>The fourth measure was ASPECT MAP. This took the highest rank of a passage with a given assigned entity term as the recall rank for that entity term. This measures the completeness of the range of coverage of the system output. Systems that nominate passages that mention more distinct entities higher in the rankings score higher than those that nominate passages with little entity coverage. Note that since, for some questions, some entities may be mentioned very frequently and others hardly at all, the ASPECT MAP measures something distinct from DOCUMENT and PASSAGE MAPs, but is just as important to the user.</p><p>Having all of these separate measures allows a more detailed study of what algorithms and approaches are helpful in proving the different important qualities necessary for a good list entity answer extraction system. While good document-based MAP is essential, all by itself it does not aid the user much since full text papers can be long and time consuming to read. Character-based MAP allows users to focus their reading and time on only the sections most likely to be helpful. Finally, aspectbased MAP measures the ability of a system to provide broad coverage of a topic. Together these separate measures characterize the effectiveness of the various systems and techniques that can be applied to list entitybased for biomedical question answering.</p><p>The Oregon Health &amp; Science University submission to the TREC 2007 Genomics Track approached the question answer extraction task as an opportunity to create a modular, object oriented framework for building question answering systems. The system is build as an assemblage of modules, implemented as objects using the Python programming language, also using that language to interface to other systems or libraries as necessary.</p><p>The framework consists of a QASystem object, which coordinates the actions of a group of processing modules in a pipeline. The QASystem object creates and maintains a Blob object that collects and stores all the accumulated results of the processing objects as [name: value] pairs, where the name is a string and the value can be any object. A processing object gets its input data from the Blob object and also stores any output in the blob. The final results for each application of the information extraction system to a topic are also stored in the blob. The first object coordinated by the QASystem is the QueryGenerator, which takes as input the topic question as given by the TREC Genomics Track topics file, and produces a series of one or more queries. The results of this are then passed to a PassageRetrieval object, which applies the queries and returns a set of retrieved passages. The QASystem then applies any number of Filter objects which can filter out passages, re-rank the passages, trim or edit passages, or in fact, manipulate the output of the PassageRetrieval object in any way necessary. The final result of the pipeline processing is a nominated set of ranked passages.</p><p>The overall architecture of the pipeline framework is shown in Figure <ref type="figure" coords="3,395.19,132.78,3.77,9.02" target="#fig_1">1</ref>. For the initial implementation of the system, there is only one filter module and each of the modules does its job in a fairly straightforward manner that will be described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query Generation</head><p>The goal of the QueryGenerator module is to transform the topic question into a Boolean expression for processing by PassageRetrieval module later in the pipeline. The QueryGenerator used in this version of the system actually creates two queries, a more specific primary query, and a less specific secondary general query. Each query is build off of term groups, where each term group is a list of terms generated from a word or phrase found in the topic question itself. The primary query essentially requires the search to match, at least one term from each term group (the term groups are ANDed), and the secondary query does not (the term groups are ORed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QASystem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics Topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranked Passages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranked Passages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QueryGenerator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PassageRetrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter</head><p>Filter Filter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter Filter Filter</head><p>Blob Blob</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blob Blob</head><p>Term groups are generated in several ways. First, a greedy search is done from the topic question to an indexed version of the MeSH terminology. The MeSH Substances database was included as a source of synonyms for one version of our system. Matching terms are expanded with MeSH synonyms into term groups. Second, gene names are matched and expanded using a dictionary extracted from Entrez and expanded using an orthographic variant generator <ref type="bibr" coords="3,448.93,453.93,56.25,9.02" target="#b1">(Cohen, 2005)</ref>. Third, each entity type has a set of manually compiled associated keywords and these are used to generate a term group along with orthographic variations. Synonyms that included a parenthetical expression were separated as separate terms. Two part phrases separated by commas were split and put back together in inverted order as additional synonyms. Finally, stop words were removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Retrieval</head><p>The passage retrieval phase was built upon the Python implementation of the open-source text retrieval engine Lucene (http://lucene.apache.org). The text corpus was parsed into maximum legal spans, and then each span was separately indexed by Lucene. Once this initial indexing was finished, we used a PassageRetrieval module in our framework to query the index and retrieve the potentially relevant passages. This step used the primary and secondary queries that were built by the QueryGenerator module. If the primary query did not return the maximum allowed 1000 passages per question, the secondary query was used as a back-off strategy. Passages returned by the primary query were ranked in tf*idf order as computed and returned by Lucene. The tf*idf of passages returned by the secondary query were filter so that there were no duplication with the primary query and then scaled by the minimum tf*idf score from the primary query and then merged into a ranked list. This ensured that the passages returned by the primary query ranked higher than those of the secondary. Using this method we retrieved the maximum number of passages (1000) for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Passage Filtering</head><p>The final processing module trimmed the retrieved passages using MMTx Metamap <ref type="bibr" coords="4,203.20,213.90,70.05,9.02" target="#b0">(Aronson, 2001)</ref> as named entity recognition engine. Similar techniques were used by other participants in the 2006 TREC genomics track <ref type="bibr" coords="4,80.96,249.91,137.79,9.02" target="#b2">(Demner-Fushman et al., 2006)</ref>. Each passage retrieved by the previous stage was split into sentences, and each sentence was scored for matching entities. The UMLS semantic types of entities that were scored varied based on the entity type given in the initial topic question. This list was assembled by hand and represents just an initial implementation. Certainly once the TREC results are available as training data, each individual semantic type could be evaluated as to its usefulness for each entity question type.</p><p>MMTx natural language processing is complex and runs rather slowly. While the earlier stages of the system ran quickly and essentially in real-time, the MMTx based filtering slowed down the system dramatically, requiring overnight runs. Because of this, the system implementation made two approximations when performing sentence scoring. First, each passage was processed by MMTx and the found entity types and phrases returned. Then the passage was split into sentences and these sentences were matched against the extracted phrases to count how many times entity phrases occurred in each sentence. This count was used as the sentence score.</p><p>The second approximation was to only process the first 100 ranked passages. This is because the pooling used by TREC would not likely reach below the 30 th or 40 th passage, and therefore there was reduced incentive to spend a lot of time processing passages that were less likely to be relevant and unlikely to be scored.</p><p>After sentences were scored, the module trimmed the passage using the following method. The average count over all the sentences was computed and any first or last sentence in the current passage that had a count less than half the average was trimmed off. This process was repeated on the newly trimmed passage until no more sentences could be trimmed.</p><p>Trimmed passages were not re-ranked by this phase, that is, passages were ranked by their original tf*idf score as for the full untrimmed passage. The only exceptions to this are for passages that had a total count score of zero, these passages were demoted to the end of the ranking. The EXCLUSIONS version of our system removed passages that appeared to be keyword or abbreviations sections. All of our submitted runs were automatic, with no manual query generation, and no tuning or modifications to the system based on the results or retrieval of initial runs on the official topics. We performed some basic system debugging and sanitychecking using the training topics given with the track protocol description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>Performance was determined by the official scoring program for the track, trecgen2007_eval. The results of our three official runs and several subsequent evaluation runs are presented in Table <ref type="table" coords="4,440.45,304.09,3.76,9.02" target="#tab_0">1</ref>. For comparison, the high, low, and mean scores for all runs submitted to the track are shown for each of the three measures.</p><p>The runs labeled OHSUQA, OHSUQASUB, and OHSUQASUBEX are our officially submitted runs. These runs were output from the system as described above, with a few small variations. The OHSUQASUB and OHSUQASUBEX runs include the MeSH substances database in the synonym generation step, while the OHSUQA run does not. The OHSUQASUBEX run includes a few simple pattern matching rules to filter out passages labeled "keywords" and "abbreviations". This was found to improve performance a bit on some training data we created based on last years Genomics track task.  In the table, below the official submitted runs, we show results for several variations of our system that remove specific features and implementation decisions. The row labeled LuceneOnly implements a QueryGenerator that only produces a primary query, and uses the PassageRetrieval module, again with no passage filter modules. The row labeled Lucene+Backoff includes the QueryGenerator and PassageRetrieval modules including the primary and secondary queries, but does not include any passage filter modules. For comparison, the bottom four rows of the table show overall summary results for all runs officially submitted to the TREC Genomics track this year.</p><p>Figure <ref type="figure" coords="6,93.25,228.78,5.01,9.02" target="#fig_1">1</ref> shows the Average Precision (AP) for each topic comparing the results of the OHSUQA system to the TREC MEAN and MAX runs. The x-axis shows both the topic number and the required list entity type for that topic. In general the document AP performance is somewhat less than the best systems, but usually at least as good as or better than the median, but with many exceptions, such as topics 209, 216 and 220.</p><p>Figure <ref type="figure" coords="6,91.93,324.78,5.01,9.02">2</ref> shows the relationship between document and passage performance for our OHSUQA system. For the most part the per-topic PASSAGE2 average precision of our system is better than the average of submitted TREC runs. However, for about eight topics the document retrieval performance is very poor, and this is directly reflected in the passage retrieval score. This is especially true for topics 209 and 220, where the average submitted PASSAGE2 scores are 0.1430 and 0.1156 respectively, but our system failed to retrieve any documents.</p><p>Because of this strong correlation between PASSAGE2 and DOCUMENT performance, it seemed likely that the best way to improve overall performance would be to refine our initial passage retrieval strategy. While our originally submitted runs used a simple two-stage primary query and secondary backoff strategy, it was clear that for some topics the primary query was too specific, and the secondary too general. Furthermore, by inspecting the generated queries for topics such as 200, it was clear that sometimes our synonym expansion system did not find any synonyms for important topic concepts, especially for genes and proteins.</p><p>In our submitted system runs we assumed that we could increase precision by limiting the source of gene and protein synonyms to a manually selected set of species. This assumption turned out to be incorrect, and the system failed to generate enough gene and protein synonyms. We improved the synonym generation of our system by including an expanded synonym dictionary generated from all species included in the Entrez gene file.</p><p>We also extended the query generation and passage retrieval to use a three stage (instead of two stage) query system. The primary query was generated as in our submitted system, but a new secondary query was built by requiring at least one term from the term group with the most members. The tertiary query was identical to the secondary query in the previous system. Therefore we have three queries with a range of specificity going from very specific (requiring at least one synonym from each term group), to less specific (requiring at least one synonym from the largest term group), to very general (no terms required). All passages were ranked by tf*idf score as before.</p><p>The results of this extended system are shown in Table <ref type="table" coords="6,323.99,204.78,5.01,9.02" target="#tab_0">1</ref> as the run labeled ThreeStageBackoff. The DOCUMENT and ASPECT scores are improved over the baseline OHSUQA system. The PASSAGE score are worse, as would be expected since no passage trimming is used in the ThreeStageBackoff system.</p><p>Finally we wanted to determine whether the time consuming MMTx-based NLP was worth the cost. To the ThreeStageBackoff system we added a simple sentence trimmer that works much like the MMTx based trimmer above, but instead of counting indentified UMLS concepts this system simply counts the number of identified query terms in each sentence. Sentences with less than half the average number of query terms are pruned from the head and tail of the passage as with the MMTx-based approach. These results are shown in the table as the run labeled "3StageBackoff+TermTrim" This system produced PASSAGE scores equal to or better than the MMTx-based system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Several things are made clear from Table <ref type="table" coords="6,511.29,448.08,3.77,9.02" target="#tab_0">1</ref>. A single automatically generated specific query was not sufficient to retrieve all of the relevant passages. The two-stage strategy was better than a single query, and the threephase query was better than the two stage. However, these staged queries are dependent upon having good synonym dictionaries in order to recognize concepts of interest and also to require terms for those concepts in the query. Our synonym dictionaries were automatically generated from MeSH and Entrez with no manual editing except for a small stop list of excessively common words (e.g. "protein"). For some of the topics, the synonym dictionary "misses" resulting in very poor DOCUMENT scores, and therefore poor PASSAGE and ASPECT scores. Since our indexing using was composed of the maximum legal spans, poor DOCUMENT scores are approximately equivalent to poor paragraph retrieval in our system. Studying the effect of the indexing passage size on document performance is an area that needs further study. It would be useful to understand how performance is affected by single sentence indexing, as well as by including leading/trailing paragraphs or MeSH terms when indexing maximal length spans. Single sentence indexing would have the advantage of not requiring subsequent passage trimming.. The passage trimming does help somewhat, as evidenced by the PASSAGE and PASSAGE2 scores for the 3StageBackoff+TermTrim system. It is not clear that NLP-based passage trimming offers better potential than simple synonym term based trimming. However, the configuration and tuning of the NLP-based passage trimming is complex, and will require much further work to determine which UMLS semantic types are most informative about sentence relevance for each entity type. The semantic types used in the current system were determined entirely by inspection. Leave one-out comparisons for each query and each semantic type could be used to determine which semantic types are appropriate for each entity type.</p><p>Finally, Figure <ref type="figure" coords="7,133.01,276.78,5.01,9.02" target="#fig_1">1</ref> makes it clear that our passage retrieval performance is well below the top retrieval for all automatic systems. This is an area where there is a major opportunity to improve our system. Modifications to our approach that could be advantageous include the use of MeSH terms when indexing the document corpus and performing the queries, as well as a more sophisticated means of generating synonyms. The multistage query approach could also incorporate the use of hypernyms and hyponyms into the search strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>The TREC Genomics track list entity question answering extraction task was a new one for this year, modified from last year by providing the entity type. Like last year, this task pushed the envelope of genomics biomedical information retrieval, but this year models a real-world use case more closely, and covers a broader range of question and entity types than ever before. The current gold standard collection will now enable further research and tuning of genomics list entity question answering systems. This will likely result in further system performance improvements that will motivate both task oriented studies and motivate actual use of these systems by biomedical researchers as part of their regular workflow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,144.30,71.09,65.70,11.72;1,210.00,70.21,3.51,6.32;1,213.54,71.09,45.48,11.72;1,259.02,70.21,3.51,6.32;1,262.50,71.09,53.45,11.72;1,315.96,70.21,3.51,6.32;1,319.44,71.09,52.74,11.72;1,372.12,70.21,3.51,6.32;1,375.66,71.09,89.47,11.72;1,465.18,70.21,3.51,6.32;1,164.70,98.28,277.37,9.88;1,442.02,96.01,3.51,6.32;1,445.56,98.28,2.75,9.88;1,79.80,110.94,449.90,9.88;1,529.68,108.67,3.51,6.32;1,179.70,123.66,253.57,9.88"><head></head><label></label><figDesc>A. M. Cohen 1 , J. Yang 1 , S. Fisher 2 , B. Roark 2 , and W.R. Hersh 1 Department of Medical Informatics and Clinical Epidemiology 1 , Department of Computer Science and Electrical Engineering, OGI School of Science and Engineering 2 Oregon Health &amp; Science University, Portland, OR, USA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,54.00,600.33,197.33,8.10"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Question answering framework architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,56.28,374.31,498.28,8.10;5,54.00,385.29,329.97,8.10"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Figure1. DOCUMENT average precision on each topic for the OHSUQA system, compared to TREC MEDIAN and MAX for automatic runs. The x-axis shows both the topic number and the required list entity type for that topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,315.72,483.85,250.43,176.29"><head>Table 1 .</head><label>1</label><figDesc>System performance on all measures averaged across all topics.</figDesc><table coords="4,315.72,511.15,250.43,148.99"><row><cell>RUN</cell><cell>DOC MAP</cell><cell>PASSAGE MAP</cell><cell>PASSAGE2 MAP</cell><cell>ASPECT MAP</cell></row><row><cell>OHSUQA</cell><cell>0.1719</cell><cell>0.0403</cell><cell>0.0440</cell><cell>0.1075</cell></row><row><cell>OHSUQASUB</cell><cell>0.1684</cell><cell>0.0388</cell><cell>0.0434</cell><cell>0.1080</cell></row><row><cell>OHSUQASUBEX</cell><cell>0.1695</cell><cell>0.0392</cell><cell>0.0439</cell><cell>0.1104</cell></row><row><cell>LuceneOnly</cell><cell>0.1450</cell><cell>0.0263</cell><cell>0.0317</cell><cell>0.1003</cell></row><row><cell>Lucene+Backoff</cell><cell>0.1602</cell><cell>0.0294</cell><cell>0.0354</cell><cell>0.1088</cell></row><row><cell>3StageBackoff</cell><cell>0.1900</cell><cell>0.0364</cell><cell>0.0432</cell><cell>0.1291</cell></row><row><cell>3StageBackoff+TermTrim</cell><cell>0.1897</cell><cell>0.0415</cell><cell>0.0460</cell><cell>0.1286</cell></row><row><cell>TREC MIN</cell><cell>0.0329</cell><cell>0.0029</cell><cell>0.0008</cell><cell>0.0197</cell></row><row><cell>TREC MEDIAN</cell><cell>0.1897</cell><cell>0.0565</cell><cell>0.0377</cell><cell>0.1311</cell></row><row><cell>TREC MEAN</cell><cell>0.1862</cell><cell>0.0560</cell><cell>0.0398</cell><cell>0.1326</cell></row><row><cell>TREC MAX</cell><cell>0.3286</cell><cell>0.0976</cell><cell>0.1148</cell><cell>0.2631</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by Grant <rs type="grantNumber">ITR-0325160</rs> from the <rs type="funder">National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6EVkZSD">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,72.00,670.68,177.51,9.02;7,54.00,682.68,195.60,9.02;7,54.00,694.68,181.04,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,164.47,670.68,85.04,9.02;7,54.00,682.68,195.60,9.02;7,54.00,694.68,73.20,9.02">Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,134.22,694.68,67.69,9.02">Proc AMIA Symp</title>
		<imprint>
			<biblScope unit="page" from="17" to="21" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,342.00,72.78,193.78,9.02;7,324.00,84.78,201.11,9.02;7,324.00,96.78,229.23,9.02;7,324.00,108.79,184.15,9.02;7,324.00,120.79,179.15,9.02;7,342.00,132.78,193.91,9.02;7,324.00,144.78,208.87,9.02;7,324.00,156.78,100.33,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,428.36,72.78,107.42,9.02;7,324.00,84.78,201.11,9.02;7,324.00,96.78,45.45,9.02;7,488.65,132.78,47.26,9.02;7,324.00,144.78,155.05,9.02">Unsupervised gene/protein entity normalization using automatically extracted dictionaries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,386.52,96.78,166.72,9.02;7,324.00,108.79,184.15,9.02;7,324.00,120.79,174.36,9.02">Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics, Proceedings of the BioLINK2005 Workshop</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
	<note>A survey of current work in biomedical text mining</note>
</biblStruct>

<biblStruct coords="7,342.00,168.78,208.32,9.02;7,324.00,180.78,232.08,9.02;7,324.00,192.78,189.53,9.02;7,324.00,204.79,202.21,9.02;7,324.00,216.79,224.63,9.02;7,324.00,228.80,195.30,9.02;7,342.00,240.78,191.10,9.02;7,324.00,252.78,222.82,9.02;7,324.00,264.78,45.92,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,324.00,204.79,202.21,9.02;7,324.00,216.79,224.63,9.02;7,324.00,228.80,105.01,9.02;7,486.97,240.78,46.13,9.02;7,324.00,252.78,177.36,9.02">LHNCBC-2006-072 Finding Relevant Passages in Scientific Articles: Fusion of Automatic Approaches vs. an Interactive Team Effort</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">C</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Loane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">H</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,435.37,228.80,43.50,9.02;7,509.10,252.78,33.90,9.02">Proc TREC</title>
		<imprint>
			<biblScope unit="volume">569</biblScope>
			<biblScope unit="page" from="589" to="594" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>Mol Cell</note>
</biblStruct>

<biblStruct coords="7,342.00,276.78,205.97,9.02;7,324.01,288.78,176.98,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,431.68,276.78,116.29,9.02;7,324.01,288.78,28.98,9.02">Mining literature for systems biology</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,359.59,288.78,107.27,9.02">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">399</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
