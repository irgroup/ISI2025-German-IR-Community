<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.55,100.06,372.33,12.91;1,173.76,117.99,265.89,12.91">TREC2007 QUESTION ANSWERING EXPERIMENTS AT TOKYO INSTITUTE OF TECHNOLOGY</title>
				<funder>
					<orgName type="full">Japanese</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.86,148.52,113.69,10.76"><forename type="first">Edward</forename><forename type="middle">W D</forename><surname>Whittaker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1 Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo, Japan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.40,148.52,80.47,10.76"><forename type="first">Matthias</forename><forename type="middle">H</forename><surname>Heie</surname></persName>
							<email>heie@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1 Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo, Japan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.94,148.52,70.96,10.76"><forename type="first">Josef</forename><forename type="middle">R</forename><surname>Novak</surname></persName>
							<email>novakj@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1 Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo, Japan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,424.81,148.52,68.74,10.76"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
							<email>furui@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1 Ookayama, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Tokyo, Japan</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.55,100.06,372.33,12.91;1,173.76,117.99,265.89,12.91">TREC2007 QUESTION ANSWERING EXPERIMENTS AT TOKYO INSTITUTE OF TECHNOLOGY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">623951BDCA5D1380C4FB0DFA2C692A21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe Tokyo Institute of Technology's attempt at the TREC2007 question answering (QA) track. Keeping the same theoretical QA model as for the TREC2006 task, this year we once again focused on the factoid QA task, while investigating a new method for sentence retrieval. We deviated from our earlier approach of using web data, and instead relied solely on the supplied news wire and blog data. Our factoid and list score fell significantly from last year, while we achieved a higher other question score compared to TREC2006, using sentence retrieval rather than last year's summarization method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper we describe the application of our data-driven and non-linguistic framework for the QA task of TREC2007. Three runs were submitted for evaluation (asked07a,b,c).</p><p>As in TREC2005 <ref type="bibr" coords="1,143.14,474.08,11.62,8.97" target="#b1">[1]</ref> and TREC2006 <ref type="bibr" coords="1,224.11,474.08,10.58,8.97" target="#b2">[2]</ref>, our focus was on the factoid question task. We experimented with retrieving sentences for answer extraction using an approach that was successfully applied in Question Answering on speech transcripts (QAst) <ref type="bibr" coords="1,113.14,521.90,10.58,8.97" target="#b3">[3]</ref>, a pilot task at the Cross Language Evaluation Forum (CLEF) evaluations 2007. In this approach, a language model (LM) is generated for each sentence and these models are combined with document LMs to take advantage of contextual information. We ran experiments using both this sentence retrieval technique and document retrieval, and further combined sentence retrieval and document retrieval using a method of system combination that has been found to be robust and effective at boosting overall system performance.</p><p>For the list task, an extension to the factoid QA system was used, similar to what we did last year. For the other question task we filtered sentences retrieved by our sentence retrieval module, using two different methods.</p><p>As data sources we used the AQUAINT-2 and Blog06 collections. This contrasts with our approach in earlier years, where we relied solely on web data and then projected answers into the AQUAINT-1 collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SENTENCE RETRIEVAL</head><p>This section explains the LM-based sentence retrieval method presented in <ref type="bibr" coords="1,366.12,279.54,10.58,8.97" target="#b4">[4]</ref>.</p><p>Language modeling for IR has gained in popularity over the last decade since the approach was proposed <ref type="bibr" coords="1,514.99,303.77,10.58,8.97" target="#b5">[5]</ref>. Under this approach a LM is estimated for each document. The documents are then ranked according to the conditional probability P (Q|D), the probability of generating the query Q given the document D.</p><p>A language model based approach to sentence retrieval for QA is presented in <ref type="bibr" coords="1,410.94,375.81,10.58,8.97" target="#b6">[6]</ref>. Due to lack of data to train the sentence specific LM, it is assumed that all words are independent, hence unigrams are used:</p><formula xml:id="formula_0" coords="1,389.00,421.27,170.00,31.72">P (Q|S) = |Q| i=1 P (q i |S),<label>(1)</label></formula><p>where q i is the ith query term in the query Q = (q 1 ...q |Q| ) composed of |Q| query terms. Smoothing methods are normally employed with LMs to avoid the problem of zero probabilities when one of the query terms does not occur in the document. This is typically achieved by redistributing probability mass from the document model to a background collection model P (Q|B). We use absolute discounting, where the probability of a query term q given a sentence S is calculated as:</p><formula xml:id="formula_1" coords="1,360.77,579.28,198.23,50.61">P 1 (q|S) = max{tf (q, S) -δ, 0} l(S) + δ • h(S, δ) l(S) • P (q|B),<label>(2)</label></formula><p>where tf (q, S) is the term frequency of q in S, l(S) is the length (number of words) of S, δ is the discount parameter, h(S, δ) is the count of how many unique words in S have a term frequency higher than δ, and P (q|B) is the unigram probability of the query term q according to the background collection model. Note that if δ &lt; 1 then h(S, δ) is equal to the number of unique words in S.</p><p>A problem with the model presented in <ref type="bibr" coords="2,231.67,75.16,11.62,8.97" target="#b6">[6]</ref> is that words relevant to the sentence might not occur in the sentence itself, but in the surrounding text. For example, for the question Where was George Bush born?, the sentence He was born in Connecticut in an article about George Bush should ideally be assigned a high probability, despite the sentence missing important query terms. To account for this, we train document LMs, P 1 (q|D), in the same manner as for P 1 (q|S) in Eq. ( <ref type="formula" coords="2,74.68,170.80,3.53,8.97" target="#formula_1">2</ref>), and perform a linear interpolation between P 1 (q|S) and P 1 (q|D):</p><formula xml:id="formula_2" coords="2,85.91,202.15,212.29,10.71">P 2 (q|S) = (1 -α) • P 1 (q|S) + α • P 1 (q|D),<label>(3)</label></formula><p>where 0 ≤ α ≤ 1 is an interpolation parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ANSWER EXTRACTION</head><p>For answer extraction we used the same framework as in TREC2006 <ref type="bibr" coords="2,103.47,284.41,10.59,8.97" target="#b2">[2]</ref>, described in detail in <ref type="bibr" coords="2,212.00,284.41,10.58,8.97" target="#b7">[7]</ref>. The explanation below is largely re-produced from <ref type="bibr" coords="2,192.61,296.37,10.58,8.97" target="#b8">[8]</ref>.</p><p>We model the most straightforward and obvious dependence of the probability of an answer A depending on a question Q:</p><formula xml:id="formula_3" coords="2,121.75,355.47,176.45,9.96">P (A | Q) = P (A | W, X),<label>(4)</label></formula><p>where A and Q are considered to be strings of l A words A = a 1 , . . . , a lA and l Q words Q = q 1 , . . . , q lQ , respectively. Here W = w 1 , . . . , w lW represents a set of features describing the "question-type" part of Q such as when, why, how, etc. while X = x 1 , . . . , x lX represents a set of features that describe the "information-bearing" part of Q i.e. what the question is actually about and what it refers to. For example, in the questions, Where was Tom Cruise married? and When was Tom Cruise married?, the information-bearing component is identical in both cases whereas the question-type component is different.</p><p>Finding the best answer Â involves a search over all available A for the one which maximizes the probability of the above model i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Â = arg max</head><formula xml:id="formula_4" coords="2,162.37,539.67,135.83,15.32">A P (A | W, X).<label>(5)</label></formula><p>Given the correct probability distribution, this is guaranteed to give us the optimal answer in a maximum likelihood sense. We don't know this distribution and it is still difficult to model but, using Bayes' rule and making various simplifying, modeling and conditional independence assumptions (as described in detail in <ref type="bibr" coords="2,142.98,620.18,11.20,8.97" target="#b7">[7]</ref>) Equation ( <ref type="formula" coords="2,204.14,620.18,3.87,8.97" target="#formula_4">5</ref>) can be rearranged to give</p><formula xml:id="formula_5" coords="2,112.18,655.38,182.15,40.90">arg max A P (A | X) answer retrieval model • P (W | A) answer f ilter model . (<label>6</label></formula><formula xml:id="formula_6" coords="2,294.33,656.05,3.87,8.97">)</formula><p>The P (A | X) model is essentially a statistical language model that models the probability of an answer sequence A given a set of information-bearing features X. We call this model the answer retrieval model and do not examine it further in this paper (see <ref type="bibr" coords="2,403.47,99.07,11.62,8.97" target="#b7">[7]</ref> for more details).</p><p>The P (W | A) model matches a potential answer A with features in the question-type set W . For example, it relates place names with where-type questions. In general, there are many valid and equiprobable A for a given W so this component can only re-rank candidate answers obtained by the retrieval model. We call this component the answer filter model and it is structured as follows.</p><p>The question-type feature set W = w 1 , . . . , w lW is constructed by extracting n-tuples (n = 1, 2, . . .) such as Where, In what and When were from the input question Q. A set of |V W | = 2522 single-word features is extracted based on frequency of occurrence in our collection of example questions.</p><p>Modeling the complex relationship between W and A directly is non-trivial. We therefore introduce an intermediate variable representing classes of example questions-andanswers (q-and-a) c e for e = 1 . . . |C E | drawn from the set C E . In order to construct these classes, given a set E of example q-and-a, we then define a mapping function f : E → C E which maps each example q-and-a t j for j = 1 . . . |E| into a particular class f (t j ) = e. Thus each class c e may be defined as the union of all component q-and-a features from each t j satisfying f (t j ) = e. Finally, to facilitate modeling we say that W is conditionally independent of c e given A so that</p><formula xml:id="formula_7" coords="2,343.25,407.53,215.74,31.59">P (W | A) = |CE | e=1 P (W | c e W ) • P (c e A | A),<label>(7)</label></formula><p>where c e W and c e A refer respectively to the subsets of questiontype features and example answers for the class c e .</p><p>Assuming conditional independence of the answer words in class c e given A, and making the modeling assumption that the jth answer word a e j in the example class c e is dependent only on the jth answer word in A we obtain:</p><formula xml:id="formula_8" coords="2,344.74,543.37,214.25,31.72">P (W | A) = |CE | e=1 P (W | c e ) • l A e j=1 P (a e j | a j ).<label>(8)</label></formula><p>Since our set of example q-and-a cannot be expected to cover all the possible answers to questions that may be asked we perform a similar operation to that above to give us the following:  <ref type="table" coords="3,80.48,147.18,3.73,8.97">1</ref>. Descriptions of systems developed for TREC2007 including the 3 submitted runs asked07a,b,c and the intermediate run asked07i that was not submitted for evaluation.</p><formula xml:id="formula_9" coords="2,317.50,656.32,241.49,42.60">P (W | A) = |CE | e=1 P (W | c e ) l A e j=1 |CA| k=1 P (a e j | c k )P (c k | a j ),<label>(9)</label></formula><p>the probabilities of multi-word answers so we take the geometric mean of the length of the answer (not shown in Equation ( <ref type="formula" coords="3,76.15,223.04,3.73,8.97" target="#formula_9">9</ref>)) and normalize P (W | A) accordingly.</p><p>The model given by Equation ( <ref type="formula" coords="3,196.33,235.20,3.87,8.97" target="#formula_9">9</ref>) is referred to as model ONE, while the model given by Equation ( <ref type="formula" coords="3,229.76,247.16,3.87,8.97" target="#formula_7">7</ref>) are referred to as model TWO. Model TWO is described in detail in <ref type="bibr" coords="3,268.67,259.11,10.58,8.97">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SYSTEM COMBINATION</head><p>For the run asked07c the answers for the factoid and list tasks were generated through combination of multiple systems. Answer combination was performed by simply summing the inverse rank of an answer a from each component system s to generate a new score for the answer as follows:</p><formula xml:id="formula_10" coords="3,130.07,377.94,168.14,27.09">score(a) = s 1 r s (a) .<label>(10)</label></formula><p>For the other question task, no system combination was performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL WORK</head><p>Three different runs (asked07a,b,c) were submitted for evaluation with characteristics given in Table <ref type="table" coords="3,236.45,487.82,3.73,8.97">1</ref>.</p><p>Run asked07a used sentence retrieval and model ONE for answer extraction. Run asked07b used document retrieval and model ONE for answer extraction. We executed an intermediate run asked07i that used document retrieval and model TWO for answer extraction. Run asked07c combined asked07i with asked07a and asked07b using system combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data pre-processing, indexing and retrieval</head><p>This year we chose to use only the AQUAINT-2 and Blog06 collections as data sources, where all reference answers (for questions with an answer) are drawn from. This contrasts with our approach in TREC2005 and TREC2006, where all answers were extracted from web data.</p><p>Raw text was extracted from the XML format of the AQU-AINT-2 and Blog06 collections. This text was converted to upper-case and cleaned using a series of regular expressions. Moreover, the text was sentence segmented, using a rule-based algorithm, to facilitate efficient sentence retrieval. The preprocessed documents were then indexed using the Xapian search engine library <ref type="foot" coords="3,385.14,221.46,3.49,6.28" target="#foot_0">1</ref> . A set of 41 stopwords was used during indexing and retrieval. 1000 documents were retrieved for each question, and used directly by the answer extraction module in run asked07b and asked07i. In run asked07a the sentences in these documents were re-ranked using the sentence retrieval model described in Section 2.</p><p>Questions were cleaned in the same way as for documents. If the target for a question did not appear character-for-character in the question string, it was simply appended to the end of the question string. Common question-type words, such as when, why, how, etc. for factoid questions, and list, name, etc. for list questions, were removed. For other questions, the query terms used were the words describing the target in the question file. Each question was treated independently of all other questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Factoid question task</head><p>For system development this year we optimized performance on earlier TREC evaluation questions. As in TREC2006, the filter model was trained by using 288812 example q-and-a from the Knowledge Master (KM) data<ref type="foot" coords="3,477.64,486.23,3.49,6.28" target="#foot_1">2</ref> plus q-and-a from the TREC-8 to TREC-13 evaluations.</p><p>The most frequent 224000 words from the AQUAINT-1 corpus, augmented with a large set of numbers, in total 819316 tokens were used to obtain C A for |C A | = 504 clusters as described in <ref type="bibr" coords="3,393.79,549.53,10.58,8.97" target="#b7">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">List question task</head><p>This year once again no development was performed on list questions. Our factoid QA system always outputs a list of candidate answers ranked by their probabilities. The issue for the list task is therefore to determine how many of the top answers to output so as to maximize the F-score. We chose simply to output the top 10 answers of the answer extraction module.  <ref type="table" coords="4,212.01,146.78,3.73,8.97">2</ref>. Performance on the 3 tasks of the 3 submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factoid task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Other question task</head><p>This year we used our sentence retrieval module to answer other questions. This differs from last year's evaluation, where we treated the answering of other questions as a summarization task and employed a variation on a method used for speech summarization for this purpose <ref type="bibr" coords="4,181.16,253.89,15.27,8.97" target="#b10">[10]</ref>.</p><p>For this task we did two experiments. In the first experiment we retrieved sentences using our sentence retrieval model. The retrieved sentences, from which nuggets were to be extracted, were first cleaned to remove words that are unlikely to be required in a nugget but which occur frequently in the data. Duplicate sentences were also removed. After that we simply submitted the 10 highest ranking sentences.</p><p>Our second experiment was similar, but here we performed a simple comparison using the minimum Levenshtein difference between each new sentence and those that had already been selected. Sentences that were found to be too similar to already chosen sentences, were discarded.</p><p>We used the results of the first experiment for runs ask-ed07a and asked07c, and the results of the second experiment for run asked07b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS AND DISCUSSION</head><p>The results for all 3 submitted runs on all 3 tasks are shown in Table <ref type="table" coords="4,89.05,498.48,3.73,8.97">2</ref>.</p><p>This year, as in previous evaluations, our focus was on the factoid task. We were especially interested in comparing our new sentence retrieval approach to document retrieval. Our hope was that supplying the answer extraction module with fewer sentences, ranked high by our sentence retrieval module, would perform better than supplying it with a larger amount of whole documents, consisting of more noisy data, at the risk of lower recall and redundancy. Moreover, searching for answers in a few sentences significantly reduces the execution speed of the system.</p><p>The results show that retrieving 100 sentences for answer extraction (run asked07a), performs nearly the same as retrieving 1000 documents (run asked07b). The amount of questions with a globally right answer is 12.2% for both these runs. When the other metrics are also included (locally correct, unsupported and inexact), the performance is slightly worse for sentence retrieval than for document retrieval: 19.7% vs. 20.6%. Manual inspection shows that for approximately 87% of the questions, a correct answer is contained in the 1000 documents retrieved for each question. For the 100 sentences retrieved for each question, the corresponding number is 67%.</p><p>For the list task, the performance is approximately the same for both these runs.</p><p>Since our method of system combination had been found to be robust and effective at boosting overall system performance in previous evaluations, we hoped this would also be the case when combining sentence retrieval and document retrieval, as we did in run asked07c. Unfortunately the percentage of globally right answers goes down slightly to 11.9%. However, by including all the metrics we achieve a score of 21.1%, a higher score than in our other submitted runs this year. The score for the list task is also higher with this combination of models than in the two other runs.</p><p>For the other questions, it should be noted that the same answers were submitted for run asked07a and run asked-07c, thus the difference in score is due to human judgment. Since run asked07b achieves a score between asked07a and asked07c, we can conclude that using Levenshtein difference had no impact on performance.</p><p>For the factoid question task and the list question task, our results were considerably lower than last year. In TREC2006 we got 25.1% of the factoid questions right. For the list task, the best score was 0.074. We believe this decline in performance is due to our decision not to use web data this year. This meant there was less data redundancy and therefore fewer examples of correct answers in the retrieved data, which affected the performance of our statistical approach. For other questions, however, we achieve a significant improvement from last year, when our best run yielded a score of 0.064. Thus there is reason to believe that our sentence retrieval approach is more suitable to this task than the summarization technique previously employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper we have given an overview of our methods and results for the TREC2007 question answering evaluation. Our primary focus was on the factoid task. This year's scores were significantly lower than in last year's evaluation, where we relied on web data. The sentence retrieval method employed did not perform much different from document retrieval, but speeded up the QA process significantly. However, using our sentence retrieval approach on the other question task performed significantly better than the summarization technique employed last year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,315.22,701.71,243.77,22.34"><head>Table</head><label></label><figDesc>where c k is a concrete class in the set of |C A | answer classes C A . The independence assumption leads to underestimating</figDesc><table coords="3,70.33,73.96,470.77,60.37"><row><cell>System</cell><cell>Data source</cell><cell cols="4">Ret. model Filter model (factoid&amp;list) Lev. dist. (other) Submitted run</cell></row><row><cell cols="2">asked07a AQUAINT-2+Blog06</cell><cell>Sent.</cell><cell>ONE</cell><cell>no</cell><cell>yes</cell></row><row><cell cols="2">asked07b AQUAINT-2+Blog06</cell><cell>Doc.</cell><cell>ONE</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="3">asked07c AQUAINT-2+Blog06 Sent.+Doc.</cell><cell>ONE+TWO</cell><cell>no</cell><cell>yes</cell></row><row><cell cols="2">asked07i AQUAINT-2+Blog06</cell><cell>Doc.</cell><cell>TWO</cell><cell>-</cell><cell>no</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,329.56,706.39,105.14,6.26"><p>http://www.xapian.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,329.56,716.12,114.70,6.26"><p>http://www.greatauk.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">ACKNOWLEDGMENTS</head><p>This research was supported by the <rs type="funder">Japanese</rs> government 21st century COE programme on large-scale knowledge resources.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,139.83,188.59,76.68,8.97" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,210.85,227.18,8.97;5,71.02,222.80,227.16,8.97;5,71.02,234.76,191.16,8.97" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<title level="m" coord="5,76.55,222.80,221.63,8.97;5,71.02,234.76,161.24,8.97">TREC2005 Question Answering Experiments at Tokyo Institute of Technology&quot;, Proc. TREC-14</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,254.68,227.18,8.97;5,71.02,266.64,227.16,8.97;5,71.02,278.59,191.16,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,76.55,266.64,221.63,8.97;5,71.02,278.59,90.75,8.97">TREC2006 Question Answering Experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chatain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,173.26,278.59,59.00,8.97">Proc. TREC-15</title>
		<meeting>TREC-15</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,298.52,227.18,8.97;5,71.02,310.47,227.16,8.97;5,71.02,322.43,215.05,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,76.43,310.47,221.75,8.97;5,71.02,322.43,90.75,8.97">CLEF2007 Question Answering Experiments at Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="5,173.26,322.43,82.54,8.97">Working Notes CLEF</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,342.36,227.18,8.97;5,71.02,354.31,227.18,8.97;5,71.02,366.26,213.49,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,292.79,342.36,5.41,8.97;5,71.02,354.31,227.18,8.97;5,71.02,366.26,74.74,8.97">A Language Modeling Approach to Question Answering on Speech Transcripts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,156.16,366.26,45.05,8.97">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,386.19,227.16,8.97;5,71.02,398.15,227.17,8.97;5,71.02,410.10,17.42,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,169.55,386.19,128.64,8.97;5,71.02,398.15,97.99,8.97">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,180.91,398.15,47.99,8.97">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,430.03,227.17,8.97;5,71.02,441.98,227.18,8.97;5,71.02,453.94,127.65,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,190.93,430.03,107.27,8.97;5,71.02,441.98,227.18,8.97;5,71.02,453.94,29.90,8.97">Comparing Improved Language Models for Sentence Retrieval in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Merkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,111.97,453.94,57.07,8.97">Proc. CLIN-17</title>
		<meeting>CLIN-17</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,473.86,227.18,8.97;5,71.02,485.82,227.18,8.97;5,71.02,497.77,219.82,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,249.65,473.86,48.56,8.97;5,71.02,485.82,227.18,8.97;5,71.02,497.77,52.87,8.97">A Statistical Pattern Recognition Approach to Question Answering using Web Data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,134.96,497.77,73.35,8.97">Proc. Cyberworlds</title>
		<meeting>Cyberworlds</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="421" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,517.70,227.17,8.97;5,71.02,529.65,227.17,8.97;5,71.02,541.61,227.17,8.97;5,71.02,553.56,75.53,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,90.41,529.65,207.77,8.97;5,71.02,541.61,149.63,8.97">NTCIR-6 CLQA Question Answering Experiments at the Tokyo Institute of Technology</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,233.88,541.61,59.43,8.97">Proc. NTCIR-6</title>
		<meeting>NTCIR-6</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="198" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,71.02,573.49,227.18,8.97;5,71.02,585.44,227.18,8.97;5,71.02,597.40,227.19,8.97;5,71.02,609.35,227.18,8.97;5,71.02,621.31,35.67,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,75.72,585.44,222.48,8.97;5,71.02,597.40,227.19,8.97;5,71.02,609.35,79.75,8.97">Using Singular Value Decomposition to Compute Answer Similarity in a Language Independent Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,214.05,609.35,39.33,8.97">Proc. LKR</title>
		<imprint>
			<biblScope unit="page" from="267" to="279" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct coords="5,76.01,641.23,222.18,8.97;5,71.02,653.19,227.17,8.97;5,71.02,665.15,202.72,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,225.38,641.23,72.80,8.97;5,71.02,653.19,227.17,8.97;5,71.02,665.15,28.17,8.97">Automatic Speech Summarization Based on Sentence Extraction and Compaction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,109.76,665.15,53.54,8.97">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="236" to="239" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
