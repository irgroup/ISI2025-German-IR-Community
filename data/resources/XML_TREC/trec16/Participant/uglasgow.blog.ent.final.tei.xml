<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.20,71.96,313.29,16.59;1,71.64,91.88,466.45,16.59">University of Glasgow at TREC 2007: Experiments in Blog and Enterprise Tracks with Terrier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.04,137.15,66.61,10.76"><forename type="first">David</forename><surname>Hannah</surname></persName>
							<email>hannahd@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.29,137.15,80.76,10.76"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.63,137.15,37.73,10.76"><forename type="first">Jie</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.76,137.15,33.71,10.76"><forename type="first">Ben</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,405.14,137.15,52.94,10.76"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.20,71.96,313.29,16.59;1,71.64,91.88,466.45,16.59">University of Glasgow at TREC 2007: Experiments in Blog and Enterprise Tracks with Terrier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F72BD22AB91DF386866F925CD4EE0EAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2007, we participate in four tasks of the Blog and Enterprise tracks. We continue experiments using Terrier 1 [14], our modular and scalable Information Retrieval (IR) platform, and the Divergence From Randomness (DFR) framework. In particular, for the Blog track opinion finding task, we propose a statistical term weighting approach to identify opinionated documents. An alternative approach based on an opinion identification tool is also utilised. Overall, a 15% improvement over a non-opinionated baseline is observed in applying the statistical term weighting approach. In the Expert Search task of the Enterprise track, we investigate the use of proximity between query terms and candidate name occurrences in documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This year, in our participation in TREC 2007, we participate in the Enterprise and Blog tracks. For both tracks, we continue the research and development of the Terrier platform, and continue developing state-of-the-art weighting models using the Divergence from Randomness (DFR) paradigm.</p><p>In the expert search task of the Enterprise track, we continue our research on our voting techniques for expert search on the new CERC test collection. In particular, we investigate the usefulness of candidate and query term proximity and also how query expansion can be successfully applied to the expert search task. For the document search task, we investigate the combination of document priors, and techniques to take feedback documents into account.</p><p>In our first participation in the Blog track, we participate in all tasks, namely the opinion finding task (and polarity subtask), and the blog distillation (aka. feed search) task. In the opinion finding task, we deploy two opinion detection techniques. The first is based on a dictionary of weighted terms, which we use to identify opinions in blog documents. The second technique is based on the application of the OpinionFinder tool <ref type="bibr" coords="1,187.70,609.60,14.87,8.07" target="#b19">[19]</ref> to detect subjectivity and opinions in documents.</p><p>Lastly for the blog distillation task, we view this as a ranking of aggregates, which is similar to the expert search task. For this reason, our participation in the blog distillation task revolves around the adaption of our voting techniques for expert search.</p><p>Our paper is structured as follows: We describe the DFR weighting models that we apply in this work in Section 2; and the indexing procedure that we used in Section 3. Both document search tasks are then described, namely the opinion finding task of the Blog track in Section 4, and the document search task of the Enterprise track in Section 5. We describe the expert search task of the Enterprise track in Section 6, followed by the closely-related blog (feed) distillation task of the Blog track in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODELS</head><p>Following from previous years, our research in Terrier cent res in extending the Divergence From Randomness framework (DFR) <ref type="bibr" coords="1,543.12,352.68,9.69,8.07" target="#b1">[1]</ref>. The remainder of this section is organised as follows. Section 2.1 presents existing field-based DFR weighting models, while Section 2.2 presents our existing DFR model, which captures term dependence and proximity. Section 2.3 presents the Bo1 DFR term weighting model for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Field-based Divergence From Randomness (DFR) Weighting Models</head><p>Document structure (or fields), such as the title and the anchor text of incoming hyperlinks, have been shown to be effective in Web IR <ref type="bibr" coords="1,348.68,470.04,9.51,8.07" target="#b4">[4]</ref>. Robertson et al. <ref type="bibr" coords="1,429.14,470.04,14.87,8.07" target="#b18">[18]</ref> observed that the linear combination of scores, which has been the approach mostly used for the combination of fields, is difficult to interpret due to the nonlinear relation between the scores and the term frequencies in each of the fields. In addition, Hawking et al. <ref type="bibr" coords="1,462.57,511.80,10.43,8.07" target="#b6">[6]</ref> showed that the length normalisation that should be applied to each field depends on the nature of the field. Zaragoza et al. <ref type="bibr" coords="1,449.44,532.80,14.87,8.07" target="#b20">[20]</ref> introduced a field-based version of BM25, called BM25F, which applies length normalisation and weighting of the fields independently. Macdonald et al. <ref type="bibr" coords="1,545.52,553.68,10.43,8.07" target="#b8">[8]</ref> also introduced Normalisation 2F in the DFR framework for performing independent term frequency normalisation and weighting of fields.</p><p>In this work, we use a field-based model from the DFR framework, namely PL2F. Using the PL2F model, the relevance score of a document d for a query Q is given by:</p><formula xml:id="formula_0" coords="1,320.40,631.79,235.55,24.26">score(d, Q) = X t∈Q qtw • 1 tf n + 1 `tf n • log 2 tf n λ<label>(1)</label></formula><p>+(λ -tf n) • log 2 e + 0.5 • log 2 (2π • tf n) ẃhere λ is the mean and variance of a Poisson distribution, given by λ = F/N ; F is the frequency of the query term t in the whole collection, and N is the number of documents in the whole collection. The query term weight qtw is given by qtf /qtfmax; qtf is the query term frequency; qtfmax is the maximum query term frequency among the query terms.</p><p>In PL2F, tf n corresponds to the weighted sum of the normalised term frequencies tf f for each used field f , known as Normalisation 2F <ref type="bibr" coords="2,66.36,99.48,9.70,8.07" target="#b8">[8]</ref>:</p><formula xml:id="formula_1" coords="2,61.68,114.78,231.22,24.80">tf n = X f " w f • tf f • log 2 (1 + c f • avg l f l f ) « , (c f &gt; 0) (2)</formula><p>where tf f is the frequency of term t in field f of document d; l f is the length in tokens of field f in document d, and avg l f is the average length of the field across all documents; c f is a hyperparameter for each field, which controls the term frequency normalisation; the importance of the term occurring in field f is controlled by the weight w f . Note that the classical DFR weighting model PL2 can be generated by using Normalisation 2 instead of Normalisation 2F for tf n in Equation (1) above. Normalisation 2 is given by:</p><formula xml:id="formula_2" coords="2,102.00,247.56,187.43,20.85">tf n = tf • log 2 (1 + c • avg l l )(c &gt; 0) (<label>3</label></formula><formula xml:id="formula_3" coords="2,289.43,253.92,3.48,8.07">)</formula><p>where tf is the frequency of term t in the document d; l is the length of the document in tokens, and avg l is the average length of all documents; c is a hyper-parameter that controls the normalisation applied to the term frequency with respect to l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Term Dependence in the Divergence From Randomness (DFR) Framework</head><p>We believe that taking into account the dependence and proximity of query terms in documents can increase the retrieval effectiveness. To this end, we extend the DFR framework with models for capturing the dependence of query terms in documents. Following <ref type="bibr" coords="2,67.80,391.32,9.51,8.07" target="#b2">[2]</ref>, the models are based on the occurrences of pairs of query terms that appear within a given number of terms of each other in the document. The introduced weighting models assign scores to pairs of query terms, in addition to the single query terms.</p><p>The score of a document d for a query Q is given as follows:</p><formula xml:id="formula_4" coords="2,64.32,449.58,228.58,21.83">score(d, Q) = X t∈Q qtw • score(d, t) + X p∈Q 2 score(d, p)<label>(4)</label></formula><p>where score(d, t) is the score assigned to a query term t in the document d; p corresponds to a pair of query terms; Q2 is the set that contains all the possible combinations of two query terms. In Equation ( <ref type="formula" coords="2,92.33,510.96,3.17,8.07" target="#formula_4">4</ref>), the score P t∈Q qtw • score(d, t) can be estimated by any DFR weighting model, with or without fields. The weight score(d, p) of a pair of query terms in a document is computed as follows:</p><formula xml:id="formula_5" coords="2,101.52,558.11,191.38,10.50">score(d, p) = -log 2 (Pp1) • (1 -Pp2)<label>(5)</label></formula><p>where Pp1 corresponds to the probability that there is a document in which a pair of query terms p occurs a given number of times. Pp1 can be computed with any randomness model from the DFR framework, such as the Poisson approximation to the Binomial distribution. Pp2 corresponds to the probability of seeing the query term pair once more, after having seen it a given number of times. Pp2 can be computed using any of the after-effect models in the DFR framework. The difference between score(d, p) and score(d, t) is that the former depends on counts of occurrences of the pair of query terms p, while the latter depends on counts of occurrences of the query term t. This year, we apply the pBiL2 randomness model <ref type="bibr" coords="2,254.25,690.23,9.51,8.07" target="#b7">[7]</ref>, which does not consider the collection frequency of pairs of query terms. It is based on the binomial randomness model, and computes the score of a pair of query terms in a document as follows:</p><formula xml:id="formula_6" coords="2,316.80,72.47,240.27,48.18">score(d, p) = 1 pf n + 1 • " -log 2 (avg w -1)! + log 2 pf n! + log 2 (avg w -1 -pf n)! -pf n log 2 (pp)<label>(6)</label></formula><formula xml:id="formula_7" coords="2,426.24,125.52,124.31,11.70">-(avg w -1 -pf n) log 2 (p ′ p )</formula><p>"</p><p>where avg w = T -N(ws-1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>is the average number of windows of size ws tokens in each document in the collection, N is the number of documents in the collection, and T is the total number of tokens in the collection. pp = 1 avg w-1 , p ′ p = 1 -pp, and pf n is the normalised frequency of the tuple p, as obtained using Normalisation 2:</p><formula xml:id="formula_8" coords="2,327.12,199.85,134.48,12.45">pf n = pf • log 2 (1 + cp • avg w-1 l-ws ).</formula><p>When Normalisation 2 is applied to calculate pf n, pf is the number of windows of size ws in document d in which the tuple p occurs. l is the length of the document in tokens and cp &gt; 0 is a hyper-parameter that controls the normalisation applied to pf n frequency against the number of windows in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Bo1 Term Weighting Model for Query Expansion</head><p>Terrier implements a list of DFR-based term weighting models for query expansion. The basic idea of these term weighting models is to measure the divergence of a term's distribution in a pseudo-relevance set from its distribution in the whole collection. The higher this divergence is, the more likely the term is related to the query's topic. Among the term weighting models implemented in Terrier, Bo1 is one of the best-performing ones <ref type="bibr" coords="2,496.11,360.72,9.51,8.07" target="#b1">[1]</ref>.</p><p>The Bo1 term weighting model is based on the Bose-Einstein statistics. Using this model, the weight of a term t in the exp doc top-ranked documents is given by:</p><formula xml:id="formula_9" coords="2,357.84,407.15,198.10,20.85">w(t) = tfx • log 2 1 + Pn Pn + log 2 (1 + Pn)<label>(7)</label></formula><p>where exp doc usually ranges from 3 to 10 <ref type="bibr" coords="2,485.32,434.52,9.51,8.07" target="#b1">[1]</ref>. Then, the top exp term with the largest w(t) from the exp doc top-ranked documents are selected to be added to the query. exp term is usually larger than exp doc <ref type="bibr" coords="2,391.32,465.96,9.51,8.07" target="#b1">[1]</ref>. Pn is given by F N . F is the frequency of the term in the collection, and N is the number of documents in the collection. tfx is the frequency of the query term in the exp doc top-ranked documents.</p><p>Terrier employs a parameter-free function to determine qtw when query expansion has been applied (see Equation ( <ref type="formula" coords="2,500.43,518.28,3.14,8.07" target="#formula_0">1</ref>)). The query term weight of a query term is then given as follows:</p><formula xml:id="formula_10" coords="2,334.08,544.44,221.87,20.90">qtw = qtf qtfmax + w(t) lim F →tfx w(t)<label>(8)</label></formula><formula xml:id="formula_11" coords="2,357.60,568.68,181.07,20.85">= Fmax log 2 1 + Pn,max Pn,max + log 2 (1 + Pn,max)</formula><p>where lim F →tfx w(t) is the upper bound of w(t). Pn,max is given by Fmax/N . Fmax is the F (frequency in the collection) of the term with the maximum w(t) in the top-ranked documents. If a query term does not appear in the most informative terms from the top-ranked documents, its query term weight remains equal to the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INDEXING</head><p>etc.) and homepages of each blog were collected. In our participation in the Blog track, we index only the permalinks component of the collection. There are approximately 3.2 million documents in the permalinks component.</p><p>For the Enterprise track, a new collection has been deployed this year, namely the CSIRO Enterprise Research Collection (CERC), which is a crawl of the csiro.au domain (370k documents). CSIRO is a real Enterprise-sized organisation, and this collection is a more realistic setting for experimentation in Enterprise IR than the previous enterprise W3C collection.</p><p>For indexing purposes, we treat both collections in the same way, using the Terrier IR platform <ref type="bibr" coords="3,156.64,172.68,13.70,8.07" target="#b14">[14]</ref>. In particular, to support the fieldbased weighting models, we index separate fields of the documents, namely the content, the title, and the anchor text of the incoming hyperlinks. Each term is stemmed using Porter's English stemmer, and normal English stopwords are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BLOG TRACK: OPINION FINDING TASK</head><p>In our participation in the opinion finding task, we aim to test two novel approaches to opinion detection. The first one is a lightweight dictionary-based statistical approach, and the second one applies techniques in Natural Language Processing (NLP) for subjectivity analysis. We conduct experiments to see to which extent these two approaches improve the performance in opinion detection over the baseline. We introduce the two opinion detection approaches in Section 4.1 and discuss our experiments in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Opinion Detection Approaches</head><p>Firstly, inspired by participants in last year's opinion finding task <ref type="bibr" coords="3,72.12,389.75,13.70,8.07" target="#b15">[15]</ref>, we propose a dictionary-based statistical approach to opinion detection based on a list of approximately 12,000 English words derived from various linguistic sources. For a set of training queries, we assume that D(Rel) is the document set containing all relevant documents, and D(opRel) is the document set containing all opinionated relevant documents. D(opRel) is a subset of D(Rel). For each term t in the word list, we measure wopn(t), the divergence of the term's distribution in D(opRel) from that in D(Rel). This divergence value measures how a term stands out from the opinionated documents, compared with all relevant, yet not necessarily opinionated, documents. The higher the divergence is, the more opinionated the term is. In our experiments, the opinion weight wopn(t) is assigned using the Bo1 term weighting model in Equation <ref type="bibr" coords="3,97.08,525.71,9.51,8.07" target="#b7">(7)</ref>. We submit the 100 most weighted terms as a query Qopn to the system, and assign an opinion score Score(d, Qopn) to each document according to Qopn, using the PL2 document weighting model (see Equations ( <ref type="formula" coords="3,174.83,557.15,3.48,8.07" target="#formula_0">1</ref>) &amp; ( <ref type="formula" coords="3,196.67,557.15,3.36,8.07" target="#formula_2">3</ref>)) with the default parameter setting c = 1.</p><p>For each retrieved document for a given new query Q, we combine the relevance score Score(d, Q) produced by a document weighting model (e.g. PL2F in Equations ( <ref type="formula" coords="3,181.47,598.91,3.48,8.07" target="#formula_0">1</ref>) &amp; (2)) with the opinion score Score(d, Qopn). Our combination method is as follows:</p><formula xml:id="formula_12" coords="3,66.24,627.35,226.66,20.85">Scorecom(d, Q) = -k log2P (op|d, Qopn) + Score(d, Q) (9)</formula><p>where </p><p>where Ret(Qopn) is the set of documents containing at least one of the 100 most opinionated terms in the dictionary. The final document ranking for a given new query Q is based on the combined relevance score Scorecom(d, Q). We have experimented with different combination methods such as the linear combination and the rank-based combination on last year's opinion finding task topics. The above combination method seems to be the most effective.</p><p>Our second opinion detection approach uses OpinionFinder <ref type="bibr" coords="3,538.38,185.40,13.93,8.07" target="#b19">[19]</ref>, a freely available toolkit, which identifies subjective sentences in text. For a given document, we adapt OpinionFinder to produce an opinion score for each document, based on the identified opinionated sentences. We define the opinion score Score(d, OF ) of a document d produced by OpinionFinder as follows:</p><formula xml:id="formula_14" coords="3,367.80,253.08,188.02,20.84">Score(d, OF ) = sumdif f • #subj #sent<label>(11)</label></formula><p>where #subj and #sent are the number of subjective sentences and the number of sentences in the document, respectively. sumdif f is the sum of the dif f value of each subjective sentence in the document, showing the confidence level of subjectivity estimated by OpinionFinder.</p><p>For a given new query, such an opinion score is then combined with the relevance score Score(d, Q) to produce the final relevance score in the same way as described above for the dictionary-based approach. The only difference is to use Score(d, Qopn) instead of Score(d, OF ) in Equations ( <ref type="formula" coords="3,438.59,375.60,3.48,8.07">9</ref>) &amp; <ref type="bibr" coords="3,459.59,375.60,13.70,8.07" target="#b10">(10)</ref>. The parameter k in Equation ( <ref type="formula" coords="3,355.31,386.04,3.48,8.07">9</ref>) is set to 100 based on training on last year's opinion finding task topics.</p><p>Our dictionary-based approach is light-weight because the opinion scoring of the documents are performed offline (i.e. prior to retrieval), and such a scoring process is not computationally expensive. Compared with the dictionary-based approach, our second approach is based on the NLP subjectivity analysis techniques, which is more computationally expensive than the first one -for instance calculating opinion scores from the dictionary takes a few seconds, while running OpinionFinder on a subset of the collection can takes weeks of CPU hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>All our six submitted runs use the PL2F field-based weighting model in Equations ( <ref type="formula" coords="3,393.11,533.40,3.48,8.07" target="#formula_0">1</ref>) &amp; (2). Our opinion retrieval runs are summarised in Table <ref type="table" coords="3,379.94,543.84,3.34,8.07" target="#tab_1">1</ref>. Firstly, on top of the title-only baseline (uog-BOPF), run uogBOPFProx tests the use of the DFR-based pBiL2 term proximity model (Equation ( <ref type="formula" coords="3,438.64,564.72,3.36,8.07" target="#formula_6">6</ref>)) in enhancing retrieval performance. Run uogBOPFProxW differs from uogBOPFProx by the use of our first opinion detection approach. Secondly, compared to the title-description baseline (uogBOPFTD), run uogBOPFTDW uses the first opinion detection approach, while run uogBOPFTDOF applies our NLP-based opinion detection approach using Opinion-Finder. Because of the time constraint, we only finished parsing a small portion of the retrieved documents using OpinionFinder for our submitted run. In this paper, we also report the results obtained based on a complete parsing of the retrieved documents using Opin-ionFinder.</p><p>Finally, one polarity run was submitted, where the opinion categorisation is based on the dictionary-based opinion finding approach. For each type of opinion relevance degree (positive, negative or mixed), we measure the divergence of each term's distri-  bution in the documents with this type of opinion relevance degree from its distribution in all relevant documents. We submit the top 100 positive, negative or mixed terms as a query to the system to score the polarity orientation of the documents in the collection. Each document is then categorised into the type (i.e. positive, negative or mixed) with the highest score. Table <ref type="table" coords="4,85.32,439.20,4.48,8.07" target="#tab_2">2</ref> summarises the retrieval performance of our submitted runs in terms of topic relevance (rel) and opinion finding (op). In this table, a value in bold indicates a significant difference (p ≤ 0.05) from the baseline run according to the Wilcoxon matchedpairs signed-ranks test. From the three title-only runs, we find that runs uogBOPFProx and uogBOPFProxW provide a statistically significant improvement over the baseline run uogBOPF in both topic relevance and opinion finding. This shows that the use of term proximity and the weighted dictionary is helpful in finding opinionated documents. In particular, the dictionary-based approach markedly improves the baseline (15.8% between uogBOPF-Prox and uogBOPFProxW in MAP, see Table <ref type="table" coords="4,224.32,554.28,3.23,8.07" target="#tab_2">2</ref>). Moreover, it is interesting to see that the use of the dictionary for opinion finding improves the retrieval performance in both topic relevance and opinion finding. This is probably due to the fact that the blog articles are often opinionated. As a result, an approach improving the opinion finding performance is likely to improve the topic relevance. From the three title-description (TD) runs, we also observe an improvement in both topic relevance and opinion finding brought by the weighted dictionary. In addition, row uogBOPFTD-OFa gives the result obtained using OpinionFinder with a complete parsing of the retrieved documents. Compared with the baseline uogBOPFTD, OpinionFinder brings a statistically significant improvement in both topic relevance and opinion finding, if the parsing of the retrieved documents is complete. Finally, the last row shows that our only submitted polarity run gives a RAccuracy (a ranked classification accuracy measure <ref type="bibr" coords="4,194.94,711.23,14.29,8.07" target="#b13">[13]</ref>) that is higher than the median of all participants.</p><p>In our additional runs, we investigate if the use of document fields helps improve the retrieval performance. Table <ref type="table" coords="4,516.94,378.12,4.48,8.07" target="#tab_3">3</ref> provides the results obtained for our runs when the document fields feature is disabled. In this case, only the document content is used for retrieval. By comparing the results with (Table <ref type="table" coords="4,486.25,409.56,3.71,8.07" target="#tab_2">2</ref>) and without (Table <ref type="table" coords="4,329.76,420.00,3.71,8.07" target="#tab_3">3</ref>) the use of document fields, we find no statistically significant difference in the retrieval performance in these two tables. The results suggest that the document content index is adequate enough for this task. The use of additional document structure information does not seem to be beneficial.</p><p>Finally, we apply a language filter that removes non-English documents from the retrieved set. Table <ref type="table" coords="4,447.54,482.76,4.48,8.07" target="#tab_5">4</ref> contains the results obtained if the document fields feature is disabled and a language filter is applied. Compared with the results obtained without the use of the language filter (see Table <ref type="table" coords="4,409.10,514.20,3.23,8.07" target="#tab_3">3</ref>), we find that the retrieval performance is markedly improved when the opinion finding feature is disabled.</p><p>In the Blog track opinion finding task, we have mainly tested two novel approaches for detecting subjectivity in documents. The light-weight statistical dictionary-based approach provides statistically significant improvement in opinion retrieval over the baseline (15.8% between uogBOPFProx and uogBOPFProxW in MAP, see Table <ref type="table" coords="4,353.88,587.40,3.31,8.07" target="#tab_2">2</ref>); The NLP-based approach using OpinionFinder also achieves similar improvement when the parsing for the retrieval documents is complete. Moreover, from our additional runs, we find that the use of document fields does not seem to be helpful, and the use of the language filter is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ENTERPRISE TRACK: DOCUMENT SEARCH TASK</head><p>In our participation in the Document Search task, we aim to test a list of techniques for using the feedback documents for enhancing the retrieval performance, using different sources of evidence, including the click-distance, inlinks, and a combination of inlinks with URL-length. The feedback documents are given by the track organisers which are known to be relevant to the topics used in this task. Section 5.1 describes the different sources of evidence of relevance used in our experiments. Section 5.2 presents our experiments in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Different Sources of Evidence</head><p>We apply three different sources of evidence for utilising the feedback documents, namely click-distance, inlinks, and URL-length.</p><p>The underlying hypothesis of the click-distance evidence is that the documents, adjacent to a known relevant document in the linking structure, are likely to be relevant. We conduct a breath-first search for the shortest path in the hyperlink graph between each document in the ranking and the feedback documents. If a shortest path minDist is not found within a maxDist links of the feedback document, then the distance is assumed to be maxDist + 1. The click-distance evidence is then combined with the relevance score Score(d, Q) by the inverse form of the sigmoid function in <ref type="bibr" coords="5,63.00,246.72,9.70,8.07" target="#b5">[5]</ref>:</p><formula xml:id="formula_15" coords="5,57.84,260.69,231.23,31.29">Scorecom(d, Q) = w (minDist + 0.5) a + k a (minDist + 0.5) a + Score(d, Q)<label>(12</label></formula><p>) where w, a and k are parameters. We use the parameter values suggested in <ref type="bibr" coords="5,100.68,304.80,10.43,8.07" target="#b5">[5]</ref> which are w = 1.8, a = 0.6 and k = 1.</p><p>In addition to the click-distance evidence, we considered the following two sources of query-independent evidence, namely inlinks and URL-length:</p><p>• Inlinks: Documents in the Web are connected through hyperlinks. A hyper-link is a connection between a source and a target document. A high number of incoming links indicates that many documents consider the given document to be of a high quality.</p><p>• URL-length: Simply counts the number of symbols in the URL. For example trec.nist.gov has a character length 13.</p><p>When using a query-independent feature for retrieval, the relevance score of a retrieved document d for a query Q is altered in order to take the document prior probability into account as follows:</p><formula xml:id="formula_16" coords="5,96.36,482.40,196.42,8.97">score(d, Q) = score(d, Q) + log(P (E))<label>(13)</label></formula><p>where P(E) is the prior probability of the query independent feature E in document d.</p><p>From our previous study in <ref type="bibr" coords="5,164.34,521.16,13.70,8.07" target="#b16">[16]</ref>, we found that it is possible to make a further improvement on the retrieval performance if we integrate more than one source of query-independent evidence. We used the conditional combination method, which takes the possible dependence between query-independent evidence into account <ref type="bibr" coords="5,53.76,573.48,13.70,8.07" target="#b16">[16]</ref>. Two query-independent features as combined by:</p><formula xml:id="formula_17" coords="5,109.32,590.16,183.46,8.97">P (E1, E2) = P (E2|E1) • P (E1)<label>(14)</label></formula><p>where P(E1) is the prior probability of the query independent feature E1; P(E2|E1) is the conditional probability of the query independent feature E2, given E1; P(E1|E2) is the probability that both E1 and E2 occur <ref type="bibr" coords="5,132.62,639.36,13.70,8.07" target="#b16">[16]</ref>. Naturally, we can extend this technique to combine more than two sources of query-independent evidence. When using the combination of query-independent feature described in Equation ( <ref type="formula" coords="5,130.34,670.80,7.43,8.07" target="#formula_17">14</ref>) for retrieval, the score of a retrieved document d for a query Q is altered, in order to take the combined query-independent evidence into account as follows:  </p><formula xml:id="formula_18" coords="5,86.04,708.35,206.74,8.97">score(d, Q) = score(d, Q) + log(P (E1, E2))<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments</head><p>We submitted four runs, all of which apply the PL2F DFR fieldsbased weighting model in Equations ( <ref type="formula" coords="5,452.87,177.72,3.48,8.07" target="#formula_0">1</ref>) &amp; (2). Our submitted runs are summarised in Table <ref type="table" coords="5,409.36,188.16,3.34,8.07" target="#tab_7">5</ref>. Our baseline run is uogEDSF, which applies the PL2F field-based weighting model. The parameter values used in PL2F are shown in Table <ref type="table" coords="5,462.05,209.04,19.84,8.07" target="#tab_17">14On</ref> top of the baseline (uogEDSF), run uogEDSINLPRI tests the inlinks query-independent evidence, and run uogEDSComPri tests the combination of inlinks with URL-length. Finally, run uogEDSCLCDIS tests the use of click-distance. In our submitted runs, the training of the queryindependent evidence, namely inlinks and URL-length, was done using the given feedback documents. The target evaluation measure of the training process is the Mean Reciprocal Rank (MRR) since there are only very few feedback documents per query.</p><p>Table <ref type="table" coords="5,347.64,303.24,4.48,8.07" target="#tab_8">6</ref> summarises the results of our submitted runs on the final 50 judged queries. The table shows that our baseline (uogEDSF) provides a robust retrieval performance that is higher than the median MAP of all participating systems. Run uogEDSCLCDIS, which uses the click-distance evidence, performs slightly better than our baseline, but with no statistically significant difference according to the Wilcoxon matched-pairs signed-ranks test. In addition, we see that the use of the query-independent evidence does not improve MAP and precision at 10 though it helps in MRR (inlinks especially can make a statistically significant improvement over the baseline) since it is the target evaluation measure for our training. We suggest this is due to the fact that our training process overfitted the small number of the given feedback documents.</p><p>Therefore, we re-run the experiments where the training of the query independent evidence is conducted on the .GOV collection (TREC 2003 Web Track mixed task), by optimising MAP. In this case the feedback documents are not used. The results presented in Table <ref type="table" coords="5,339.12,481.08,4.48,8.07" target="#tab_8">6</ref> show that the use of query-independent evidence does not improve MAP and P@10 over the baseline (the only exception is the URL-length evidence on P@10). However, the MRR measure is improved for all three sources of query-independent evidence.</p><p>We also re-run the experiments where the training of the query independent evidence is conducted on the .GOV2 collection (TREC 2006 namedpage finding task), by optimising MRR. The results show that the use of query independent evidence leads to improvements over the baseline at P@10 and MRR measures (the only exception is inlinks on P@10). However, no improvement over the baseline is observed for MAP (see bottom part of Table <ref type="table" coords="5,517.47,585.72,3.23,8.07" target="#tab_8">6</ref>).</p><p>Overall, with various different training settings, it was not possible to improve the baseline MAP by using the query independent evidence, suggesting that the training issue needs to be further investigated (e.g. use of more training queries)</p><p>To conclude, in this task, we have tested the use of different sources of evidence for utilising the feedback documents. According to our experimental results, the use of click-distance works the best in our submitted runs with a slight positive difference from the baseline; The use of the query-independent feature can improve precision at 10 and MRR over the baseline if the training is appropriately conducted. More training data is possibly required for a better performance on MAP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ENTERPRISE TRACK: EXPERT SEARCH TASK</head><p>We participated in the expert search task of the TREC 2007 Enterprise track, with the aim of continuing to test and develop our Voting Model for expert search <ref type="bibr" coords="6,168.96,303.72,9.51,8.07">[9]</ref>. In the expert search task, systems are asked to rank candidate experts with respect to their predicted expertise about a query, using documentary evidence of expertise found in the collection.</p><p>Our participation to the expert search task of TREC 2007 strengthens the Voting Model for expert search by testing it on a new test collection. We also test two forms of proximity and two forms of query expansion. In particular, we investigate how the proximity of candidate name occurrences to query terms can be applied within an expert search system. Indeed, a document may contain occurrences of several candidates' names. The closer a candidate name occurs to the terms of the query, the more likely that the document is a higher quality indicator of expertise. In this technique, we strengthen votes from expertise evidence where the candidate's name occurs in close proximity to the terms of the query.</p><p>Moreover, we compare two techniques for query expansion (QE) when applied to the expert search task. In the first of these, documentcentric QE <ref type="bibr" coords="6,93.86,481.56,13.70,8.07" target="#b11">[11]</ref>, QE is performed on the underlying ranking of documents. In the second, known as candidate topic-centric QE <ref type="bibr" coords="6,275.70,492.00,13.75,8.07" target="#b12">[12]</ref>, where the pseudo-relevant set is taken as the top-ranked profile documents associated to the top-ranked candidates.</p><p>The remainder of this section is structured as follows: In Section 6.1, we give an overview of the Voting Model for expert search; Section 6.2 details how we take candidate and query term proximity into account in the Voting Model; and Section 6.3 provides details on both forms of query expansion; We detail the experimental setup in Section 6.4; and provide results and conclusions in Section 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Voting Model</head><p>In our voting model for expert search, instead of directly ranking candidates, we consider the ranking of documents, with respect to the query Q, which we denote R(Q). We propose that the ranking of candidates can be modelled as a voting process, from the retrieved documents in R(Q) to the profiles of candidates: every time a document is retrieved and is associated with a candidate, then this is a vote for that candidate to have relevant expertise to Q. The votes for each candidate are then appropriately aggregated to form a ranking of candidates, taking into account the number of voting documents for that candidate, and the relevance score of the voting documents. Our voting model is extensible and general, and is not collection or topics dependent.</p><p>In <ref type="bibr" coords="6,335.20,266.27,9.51,8.07">[9]</ref>, we defined twelve voting techniques for aggregating votes for candidates, adapted from existing data fusion techniques. In this work, we apply only the robust and effective expCombMNZ voting technique for ranking candidates. expCombMNZ ranks candidates by considering the sum of the exponential of the relevance scores of the documents associated with each candidate's profile. Moreover, it includes a component which takes into account the number of documents in R(Q) associated to each candidate, hence explicitly modelling the number of votes made by the documents for each candidate. Hence, in expCombMNZ, the score of a candidate C's expertise to a query Q is given by:</p><formula xml:id="formula_19" coords="6,318.00,388.19,234.10,34.58">score cand expCombM NZ (C, Q) = R(Q) ∩ prof ile(C) • X d ∈ R(Q)∩ prof ile(C) exp(score(d, Q)) (<label>16</label></formula><formula xml:id="formula_20" coords="6,552.11,404.52,3.72,8.07">)</formula><p>where R(Q) ∩ prof ile(C) is the number of documents from the profile of candidate C that are in the ranking R(Q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Candidate -Query Term Proximity</head><p>Some types of documents can have many topic areas and many occurrences of candidate names (for instance, the minutes of a meeting). In such documents, the closer a candidate's name occurrence is to the query terms, the more likely that the document is a high quality indicator of expertise for that candidate <ref type="bibr" coords="6,511.38,518.28,9.68,8.07" target="#b3">[3,</ref><ref type="bibr" coords="6,523.55,518.28,10.59,8.07" target="#b17">17]</ref>.</p><p>We define the proximity of candidate and query terms in terms of the DFR term proximity document weighting models defined in Section 2.2. The term proximity model is designed to measure the informativeness in a document of a pair of query terms occurring in close proximity. We adapt this to the expert search task and into the expCombMNZ voting technique (Equation ( <ref type="formula" coords="6,488.80,581.04,6.71,8.07" target="#formula_19">16</ref>)), by measuring the informativeness of a query term occurring in close proximity to a candidate's name, as follows:</p><formula xml:id="formula_21" coords="6,335.40,619.19,220.42,61.35">score cand expCombM NZP rox (C, Q) = (17) R(Q) ∩ prof ile(C) • X d ∈ R(Q)∩ prof ile(C) exp( score(d, Q) + X p=name(C)×t∈Q score(d, p))</formula><p>Here p is a tuple consisting of a term t from the query and the full name of candidate C. score(d, p) can be calculated using any DFR weighting model <ref type="bibr" coords="6,399.80,711.24,9.51,8.07" target="#b7">[7]</ref>, however, for efficiency reasons, we use the pBiL2 model (Equation ( <ref type="formula" coords="7,161.44,57.60,3.36,8.07" target="#formula_6">6</ref>)) because it does not consider the frequency of tuple p in the collection but only in the document. Hence, in this way, we are able to use the same weighting model to count and weight candidate occurrences in close proximity to query terms as we proposed in <ref type="bibr" coords="7,164.58,99.48,10.43,8.07" target="#b7">[7]</ref> to weight the informativeness of query terms occurring in close proximity. Note that the approach proposed here does not remove evidence of expertise for a candidate where the candidate's name does not occur near a query term, as this may result in a relevant candidate not being retrieved for a difficult query (i.e. the relevant candidate had only sparse evidence of expertise). Instead, candidate with names occurring in close proximity to query terms are given stronger votes in the Voting Model, and hence should be ranked higher in the final ranking of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Query Expansion in Expert Search</head><p>Query Expansion (QE) has previously been shown to be useful in adhoc document retrieval tasks. We have been investigating how QE can be applied in the expert search task. In particular, we have proposed two forms of QE. Firstly, using the underlying ranking of documents R(Q) applied in the voting model, it is clear that query expansion can be applied on R(Q), to improve the quality of the ranking of documents, and hence the accuracy of the ranking of candidates <ref type="bibr" coords="7,104.64,300.72,13.70,8.07" target="#b11">[11]</ref>. In this scenario, the pseudo-relevant set is the top-ranked documents in the document ranking R(Q).</p><p>However, it would be better to apply QE in expert search where the items of the pseudo-relevant set are in fact the top-ranked candidates. While we proposed candidate centric QE in <ref type="bibr" coords="7,232.12,342.60,13.70,8.07" target="#b11">[11]</ref>, this method did not perform well, due to the occurrence of topic drift within candidate profiles. Topic drift is when a candidate has many interests represented in their profile, and using all documents in the profile can cause the QE to fail, by selecting expansion terms unrelated to the original query <ref type="bibr" coords="7,157.02,394.91,13.70,8.07" target="#b12">[12]</ref>. We hence proposed a new form of QE for expert search, known as candidate topic centric QE, in which the pseudo-relevant set contains only the top-ranked documents for the top-ranked candidate profiles <ref type="bibr" coords="7,209.82,426.23,13.70,8.07" target="#b12">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Setup</head><p>In contrast to previous years of the expert search task at TRECs 2005 and 2006, for TREC 2007, there is no 'master list' of candidates deployed as a central part of the test collection. Indeed, participants are more realistically expected to identify and rank candidates themselves.</p><p>To identify candidates, we looked to identify email addresses in the CERC collection. As many email addresses are obfuscated to avoid detection by spam robots, we attempted to identify as many alternatives for the @ symbol as possible (for example AT , {at} etc.). Once the email addresses were extracted, we removed email addresses not matching the format firstname.lastname@cs iro.au. Lastly, because there are many example email addresses in the corpus, we removed the email addresses containing the sequence of characters name (e.g. the actual address firstname. lastname@csiro.au is removed).</p><p>The second stage is associating documents with each candidate. The full name of each candidate is derived from their email address. We look for documents that match the full name (and hence the email address) of each candidate C, and associate them with that candidate.</p><p>For all our submitted runs, we apply the same index used in the document search task. Standard stopwords are removed, and Porter's English stemmer is applied. Moreover, to generate the underlying ranking of documents, we apply the PL2F document weighting model (Equation ( <ref type="formula" coords="7,157.70,711.24,3.14,8.07" target="#formula_0">1</ref> title and anchor text. For query expansion, we apply the Bo1 term weighting model from the DFR framework (Equation ( <ref type="formula" coords="7,523.84,165.24,3.14,8.07" target="#formula_9">7</ref>)). For training, we used the same setting for the PL2F document weighting as that applied in the document search task of the Enterprise task -see Table <ref type="table" coords="7,376.08,196.56,7.41,8.07" target="#tab_17">14</ref>. For training expert search specific features of runs (e.g. query expansion, candidate-query term proximity), we trained on TREC 2005 and 2006 expert search tasks.</p><p>Unfortunately, two small bugs affected our identifying of candidates and documents. Firstly, we did not correctly describe the HTML entity for the @ symbol, by mistakenly writing &amp;64; instead of &amp;#64;. This had the effect of not identifying 302 candidates (of which 16 relevant candidates were omitted). Secondly, our custom written Perl scripts only identified at most one expert per line of the HTML documents in the CERC collection. This had the effect of omitting a total of 346 candidate-document associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Experiments and Results</head><p>We submitted four runs to the expert search task of the Enterprise track. Along with the unsubmitted baseline, these were:</p><p>• uogEXFeMNZ: is our baseline run (unsubmitted). It applies the PL2F DFR document weighting model (Equations ( <ref type="formula" coords="7,549.11,383.64,3.48,8.07" target="#formula_0">1</ref>) &amp; ( <ref type="formula" coords="7,351.24,394.08,3.36,8.07">2</ref>)) to generate the underlying ranking of documents, combined with the expCombMNZ voting technique to rank experts.</p><p>• uogEXFeMNZP: improves upon the baseline run by applying query term proximity, pBiL2, (Equation ( <ref type="formula" coords="7,506.62,445.20,3.36,8.07" target="#formula_6">6</ref>)) to the underlying ranking of documents.</p><p>• uogEXFeMNZcP: applies the candidate -query term proximity technique described in Section 6.2 above. Baseline is uogEXFeMNZ.</p><p>• uogEXFeMNZdQ: applies document-centric QE <ref type="bibr" coords="7,517.63,515.87,14.87,8.07" target="#b11">[11]</ref> to the baseline.</p><p>• uogEXFeMNZQE: applies candidate-topic-centric QE <ref type="bibr" coords="7,540.91,546.11,14.87,8.07" target="#b12">[12]</ref> to the baseline.</p><p>The salient features of the runs are described in Table <ref type="table" coords="7,522.68,575.15,3.34,8.07" target="#tab_10">7</ref>. Moreover, Table <ref type="table" coords="7,357.86,585.71,4.48,8.07" target="#tab_11">8</ref> details the retrieval performance of the submitted runs and our unsubmitted baseline run (uogEXFeMNZ). Retrieval performance is measured in terms of mean average precision (MAP) and mean reciprocal rank (MRR). Moreover, we also provide corrected retrieval performances using the improved candidate profile sets. From the results, it is firstly noticeable that using the improved candidate profile sets markedly improves the retrieval performance of all runs. The largest of these improvements in the run applying candidate -query term proximity (uogEXFeMNZcP), which jumps from 0.3138 to 0.4419 MAP (41% improvement). Over all the corrected runs, this run performs the best, followed by the normal query term proximity approach (uogEXFeMNZPP). Candidate topic centric QE shows a very small improvement over the corrected baseline, while document centric QE shows a small decrease. Compared to the median, the corrected runs are well above the median performance for MAP, and above median MRR also. Table <ref type="table" coords="8,85.92,414.24,4.48,8.07" target="#tab_12">9</ref> details the performance of a selection of voting techniques from the Voting Model for expert search <ref type="bibr" coords="8,221.78,424.68,9.51,8.07">[9]</ref>. All voting techniques use the PL2F document weighting model as well as the improved candidate profile sets and candidate query term proximity. From this we can see that the CombMAX voting technique is best for the MRR evaluation measure, and the MRR voting technique performs best for the MAP evaluation measure. The CombMAX, expCombSUM and expCombMNZ techniques markedly improve over the Votes, CombSUM, CombMNZ and MRR voting techniques, for both evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Expert Search Task Conclusions</head><p>Overall, we demonstrated that the voting techniques from the Voting Model can be successfully applied on the new and more realistic CERC expert search test collection. Our results show the candidate -query term proximity method we proposed can be effectively applied to the expert search task, and will result in a marked increase in both MAP and MRR retrieval performances. Finally, (for the second year running), we have been affected by a bug in the generation of candidate profiles, and that the accuracy of candidate expertise evidence is a highly important factor in the retrieval performance of the expert search system.</p><p>In terms of experimental conclusions, the proposed candidate query term proximity technique shows a marked improvement (17%) over the baseline, followed by the more traditional proximity applied on the document ranking. For the query expansion, the document centric QE did not work while the candidate topic centric QE showed little positive difference from the baseline, and not as much as the increases exhibited by the proximity runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">BLOG TRACK: FEED DISTILLATION TASK</head><p>In TREC 2007, we also participated in the blog distillation (feed distillation) task of the Blog track, where we aim to test the applicability of our novel voting model for Expert Search <ref type="bibr" coords="8,519.80,105.48,10.43,8.07">[9]</ref> to this task. Firstly, in the blog distillation task, the aim of each system is to identify the blogs (feeds<ref type="foot" coords="8,412.08,124.56,2.99,5.38" target="#foot_2">2</ref> ) that have a principle recurring interest in the query topic <ref type="bibr" coords="8,385.10,136.80,13.70,8.07" target="#b13">[13]</ref>. We believe that the blog distillation task can be seen as a voting process: A blogger with an interest in a topic will blog regularly about the topic, and these blog posts will be retrieved in response to a query topic. Each time a blog post is retrieved about a query topic, that can be seen as a vote for that blog to have an interest in the topic area. Indeed, this task is then very similar to the expert search task, in that both tasks aggregate the documents that are ranked in response to a query. Hence, our main investigation in our TREC 2007 participation is to determine if our Voting Model for expert search (which we also applied for the expert search task in Section 6) can be successfully applied to this task also.</p><p>In this task, we have three central research hypotheses: Firstly, is the voting paradigm depicted by the Voting Model for expert search an accurate depiction of the blog distillation task, and hence can the voting techniques be successfully applied in this task; Secondly, can we improve the effectiveness of the blog distillation system by giving less consideration to feeds that do not have a cohesive set of associated documents; and lastly, can the anchor text from homepages to homepages be of benefit to the retrieval performance of the search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Cohesiveness</head><p>In <ref type="bibr" coords="8,336.63,377.76,13.70,8.07" target="#b12">[12]</ref>, we defined three measures of cohesiveness for expert search, within the context of query expansion for expert search. A measure of cohesiveness examines all the documents associated with an aggregate, and measures on average, how different each document is from all the documents associated to the aggregate. In particular, we proposed three measures of cohesiveness, based on Cosine distance, Kullback-Leibler divergence, and profile size. In TREC 2007, we aim to test whether the cohesiveness measures can successfully identify feeds that blog about a very diverse set of topics, and hence are less likely to be principally devoted to the area of the query topic.</p><p>In TREC 2007, we only apply the cohesiveness measure based on the Cosine distance, as the profile size-based measure is not intuitive in the blogosphere context, while the multiple logarithm function calls in the Kullback-Leibler divergence based cohesiveness measure make it slower to apply at the scale of the TREC Blogs06 collection. The cohesiveness of a blog B can be measured using the Cosine measure from the vector-space framework as follows:</p><formula xml:id="formula_22" coords="8,338.40,569.99,217.43,52.95">CohesivenessCos(B) = 1 posts(B) • X d∈posts(B) P t∈posts(B) tf d • tfB q P t∈d (tf d ) 2 q P t∈posts(B) (tfB) 2<label>(18)</label></formula><p>where posts(B) denotes the set of blog posts (i.e. documents) associated with blog B. Moreover, tf d is the term frequency of term t in document d, and tfB is the total term frequency of term t in all documents associated with blog B (denoted t ∈ posts(B)). CohesivenessCos measures the mean divergence between every document in the blog and the blog itself. Note that CohesivenessCos is bounded between 0 and 1, where 1 means that the documents represent the entire blog completely.</p><p>We integrate the cohesiveness score with the score cand(B, Q) of a blog to a query as follows:</p><formula xml:id="formula_23" coords="9,66.24,104.15,226.54,22.41">score cand(B, Q) = score cand(B, Q) (19) + log(1 + CohesivenessCos(B))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Additional Homepage Anchor Text</head><p>Blogs often link to other blogs with similar interests, particularly in the 'blogroll' at the side of a blog's homepage. Because of this linkage, we choose to use some additional anchor text information from the blogrolls from blogs in the collection. In doing so, we hoped that this would bring additional textual evidence about the blogger's interests, and also would identify the more authoritative blogs (i.e. those most linked to by other blogs in the general topic area).</p><p>Recall from Section 3 that we have already indexed the permalinks (documents) part of the Blogs06 collection. In doing so, we extracted all anchor text coming from documents to documents. However, such anchor text does not include the blogroll normally found on the homepage of each blog.</p><p>Therefore in addition, we extracted anchor text linking from the homepage component of the collection to other homepages in the collection. This obtained, on average, an additional 49 tokens of anchor text per blog, in addition to the mean 21 tokens of anchor text already associated with each document in the collection.</p><p>We combined the additional anchor text information with the blog scores as follows:</p><formula xml:id="formula_24" coords="9,81.84,362.87,210.94,22.47">score cand(B, Q) = score cand(B, Q) (20) + score cand Anch (B, Q)</formula><p>where score cand Anch (B, Q) is the score of additional homepage anchor text calculated using the PL2 weighting model (Equations (1) &amp; ( <ref type="formula" coords="9,101.53,413.64,3.14,8.07" target="#formula_2">3</ref>)). This has the effect of boosting blogs that have anchor text linking to their homepages containing query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Experimental Setup</head><p>Similar to our participation in the opinion finding task of the Blog track, and as described in Section 3, we index the permalinks component of Blogs06 collection using the Terrier IR platform <ref type="bibr" coords="9,73.82,486.24,13.70,8.07" target="#b14">[14]</ref>, by removing standard stopwords and applying Porter's stemming for English. Note that we do not index the feeds component of the collection.</p><p>For the underlying ranking of blog posts, we apply the PL2F field-based document weighting model (Equation (1)), using the content, title and anchor text of incoming hyperlinks as the fields. The parameter values for PL2F were exactly the same as those applied in the opinion finding task -i.e. they were trained on the TREC 2006 opinion finding task (Actual values are reported in Table <ref type="table" coords="9,66.96,580.44,7.06,8.07" target="#tab_16">13</ref>).</p><p>For the associations between blog posts (documents) and blogs, we use the mappings provided by the collection, so that every document is associated to its corresponding blog. Hence, when a blog document is retrieved for a query topic, this can be seen as a vote for the corresponding blog to have an interest in the topic area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Experiments and Results</head><p>We submitted 4 runs to the Blog Distillation task of the TREC-2007 Blog Track, which test our hypotheses for this task. The first run is a baseline run.</p><p>• uogBDFeMNZ is our baseline run. score the predicted relevance of feeds to the query topic.</p><p>• uogBDFeMNZP improves on the baseline run, by boosting the rank of documents in the document ranking where the query terms occur in close proximity. We use the PBiL2 DFR term dependence model (Equation ( <ref type="formula" coords="9,467.78,323.16,3.36,8.07" target="#formula_6">6</ref>)) to model the proximity of query terms in the documents.</p><p>• uogBDFeMNZpC investigates our cohesiveness hypothesis in this task. Feeds with a cohesive set of blog posts that all discuss similar topic(s) will be ranked higher than feeds with a highly diverse set of associated blog posts.</p><p>• uogBDFeMNZhA investigates how the application of additional anchor text can be used to improve the performance of the blog retrieval system.</p><p>Table <ref type="table" coords="9,348.12,439.32,8.92,8.07" target="#tab_13">10</ref> summarises the salient features of our submitted runs. Moreover, Table <ref type="table" coords="9,378.10,449.88,8.92,8.07" target="#tab_14">11</ref> presents the results of the submitted runs. The evaluation measures in this task are Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at 10 (P@10). From the results, we observe that all submitted runs are perform markedly higher than the median of all participating system. Moreover, the run applying proximity (uogBDFeMNZP) improves slightly over the baseline (uogBDFeMNZ) for all three evaluation measures. Runs applying additional anchor text (uogBDFeMNZhA) and cohesiveness (uogBDFeMNZpC) do not improve over the baseline.</p><p>In Table <ref type="table" coords="9,359.68,543.96,7.41,8.07" target="#tab_15">12</ref>, we compare the retrieval effectiveness of various voting techniques we proposed in <ref type="bibr" coords="9,442.46,554.40,10.43,8.07">[9]</ref> -note that we do not apply proximity since it does not bring marked improvements over the voting techniques alone. From the results, it is noticeable that the expCombMNZ technique performs best for MAP and P@10 evaluation measures, while expCombSUM performs best for the MRR measure. The Votes, CombSUM, CombMNZ and MRR techniques all perform similarly on the MAP evaluation measure, while the MRR voting technique is better for the MRR evaluation measure, and not as good as others for P@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Blog Distillation Task Conclusions</head><p>Our participation to the blog distillation task at the TREC 2007 Blog track was successful as it demonstrated that the voting techniques that we proposed in <ref type="bibr" coords="9,417.74,690.24,10.43,8.07">[9]</ref>  larger setting of potential experts (i.e. 100,000 blogs), and given the promising retrieval performance demonstrated here by our runs, that it performs well in this larger setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>In TREC 2007, we participate in two tracks, namely the Blog track and the Enterprise track. For the Blog track, our main research conclusion in the opinion finding task is that a purely statistical and lightweight approach based on a weighted dictionary is very effective in detecting opinions, in particular, leading to 15.8% improvement over the baseline. In addition, the OpinionFinder tool can be as effective as the dictionary-based approach if applied on all of the retrieved documents. For the blog distillation task, we showed a connection to the expert search task, by successfully adapting voting techniques that we have previously developed for expert search.</p><p>In the Enterprise track, our main finding for the document search task is that the click distance is the most effective feature for identifying related documents based on the feedback evidence. The use of priors and their combination did not work as well as expected, mainly due to a lack of adequate training. Finally, in the expert search task, the use of candidate and query term proximity benefited retrieval performance markedly, and that for the query expansion techniques, the more realistic candidate-centric form helped most, improving slightly on the baseline. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.76,55.04,239.70,219.45"><head>Table 1 : Techniques applied in the submitted runs in the Blog track opinion finding task.</head><label>1</label><figDesc></figDesc><table coords="4,59.16,55.04,234.30,219.45"><row><cell>Run</cell><cell></cell><cell cols="2">Techniques</cell><cell></cell></row><row><cell cols="2">uogBOPF(Base)</cell><cell cols="3">T-only queries + PL2F</cell></row><row><cell>uogBOPFProx</cell><cell></cell><cell cols="3">uogBOPF + proximity</cell></row><row><cell cols="2">uogBOPFProxW</cell><cell cols="3">uogBOPFProx + dictionary</cell></row><row><cell cols="5">uogBOPFTD(Base) TD queries + PL2F</cell></row><row><cell cols="2">uogBOPFTDW</cell><cell cols="3">uogBOPFTD + dictionary</cell></row><row><cell cols="2">uogBOPFTDOF</cell><cell cols="3">uogBOPFTDW + OpinionFinder</cell></row><row><cell>Run</cell><cell cols="5">MAP(rel) P@10(rel) MAP(op) P@10(op)</cell></row><row><cell>median</cell><cell cols="2">0.3340</cell><cell>-</cell><cell>0.2416</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Title-only runs</cell><cell></cell></row><row><cell>uogBOPF(Base)</cell><cell cols="2">0.3532</cell><cell>0.6120</cell><cell>0.2596</cell><cell>0.4200</cell></row><row><cell>uogBOPFProx</cell><cell cols="2">0.3812</cell><cell>0.6740</cell><cell>0.2817</cell><cell>0.4540</cell></row><row><cell>uogBOPFProxW</cell><cell cols="2">0.4160</cell><cell>0.7200</cell><cell>0.3264</cell><cell>0.5520</cell></row><row><cell></cell><cell></cell><cell cols="2">Title-description runs</cell><cell></cell></row><row><cell>uogBOPFTD(Base)</cell><cell cols="2">0.3868</cell><cell>0.7420</cell><cell>0.2971</cell><cell>0.4880</cell></row><row><cell>uogBOPFTDW</cell><cell cols="2">0.4033</cell><cell>0.7600</cell><cell>0.3182</cell><cell>0.5580</cell></row><row><cell>uogBOPFTDOF</cell><cell cols="2">0.3872</cell><cell>0.7300</cell><cell>0.2995</cell><cell>0.4920</cell></row><row><cell>uogBOPFTDOFa</cell><cell cols="2">0.4064</cell><cell>0.7560</cell><cell>0.3251</cell><cell>0.5620</cell></row><row><cell>uogBOPFPol</cell><cell cols="3">RAccuracy: 0.1460</cell><cell cols="2">median: 0.1227</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,53.76,286.80,239.26,70.83"><head>Table 2 : Results of submitted runs in the opinion finding task. uogBOPFTDOFa is an additional run for which the parsing of the retrieved documents using OpinionFinder is completed. uogBOPFPol is our polarity run. All submitted runs are above the median of all participating systems</head><label>2</label><figDesc></figDesc><table /><note coords="4,203.14,328.68,89.88,8.07;4,53.76,338.52,239.22,8.97;4,53.76,349.56,227.24,8.07"><p>. A value in bold indicates a significant difference (p ≤ 0.05) from the baseline run according to the Wilcoxon matched-pairs signed-ranks test.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,316.80,56.00,239.70,128.41"><head>Table 3 : Results of submitted runs in the opinion finding task when the document fields feature is disabled. A value in bold in- dicates a significant difference</head><label>3</label><figDesc></figDesc><table coords="4,322.20,56.00,234.30,86.73"><row><cell>Run</cell><cell cols="4">MAP(rel) P@10(rel) MAP(op) P@10(op)</cell></row><row><cell></cell><cell cols="2">Title-only runs</cell><cell></cell><cell></cell></row><row><cell>uogBOPF(Base)</cell><cell>0.3464</cell><cell>0.5960</cell><cell>0.2583</cell><cell>0.4260</cell></row><row><cell>uogBOPFProx</cell><cell>0.3809</cell><cell>0.6580</cell><cell>0.2847</cell><cell>0.4720</cell></row><row><cell>uogBOPFProxW</cell><cell>0.4076</cell><cell>0.7100</cell><cell>0.3256</cell><cell>0.5540</cell></row><row><cell></cell><cell cols="2">Title-description runs</cell><cell></cell><cell></cell></row><row><cell>uogBOPFTD(Base)</cell><cell>0.3797</cell><cell>0.7300</cell><cell>0.2847</cell><cell>0.4820</cell></row><row><cell>uogBOPFTDW</cell><cell>0.3892</cell><cell>0.7300</cell><cell>0.3100</cell><cell>0.4840</cell></row><row><cell>uogBOPFTDOFa</cell><cell>0.3963</cell><cell>0.7480</cell><cell>0.3133</cell><cell>0.5440</cell></row></table><note coords="4,433.20,175.44,59.91,8.97"><p>(p ≤ 0.05) from</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,316.80,176.04,239.70,111.17"><head>the baseline run according to the Wilcoxon matched-pairs signed-ranks test.</head><label></label><figDesc></figDesc><table coords="4,322.20,202.40,234.30,84.81"><row><cell>Run</cell><cell cols="4">MAP(rel) P@10(rel) MAP(op) P@10(op)</cell></row><row><cell></cell><cell cols="2">Title-only runs</cell><cell></cell><cell></cell></row><row><cell>uogBOPF(Base)</cell><cell>0.3677</cell><cell>0.6180</cell><cell>0.2722</cell><cell>0.4380</cell></row><row><cell>uogBOPFProx</cell><cell>0.4041</cell><cell>0.6800</cell><cell>0.3007</cell><cell>0.4840</cell></row><row><cell>uogBOPFProxW</cell><cell>0.4114</cell><cell>0.7100</cell><cell>0.3279</cell><cell>0.5540</cell></row><row><cell></cell><cell cols="2">Title-description runs</cell><cell></cell><cell></cell></row><row><cell>uogBOPFTD(Base)</cell><cell>0.3967</cell><cell>0.7220</cell><cell>0.2968</cell><cell>0.4980</cell></row><row><cell>uogBOPFTDW</cell><cell>0.3897</cell><cell>0.7180</cell><cell>0.3060</cell><cell>0.5440</cell></row><row><cell>uogBOPFTDOFa</cell><cell>0.3950</cell><cell>0.7280</cell><cell>0.3082</cell><cell>0.5600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,316.80,299.64,239.19,49.95"><head>Table 4 : Results of submitted runs in the opinion finding task when the document fields feature is disabled and language filter is applied. A value in bold indicates a significant difference</head><label>4</label><figDesc></figDesc><table /><note coords="4,316.80,330.36,239.10,8.97;4,316.80,341.52,124.76,8.07"><p>(p ≤ 0.05) from the baseline run according to the Wilcoxon matched-pairs signed-ranks test.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,316.80,110.88,239.14,18.51"><head>Table 5 : Techniques applied in the submitted runs in the En- terprise track document search task.</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,53.76,55.04,502.21,176.89"><head>Table 6 : The results of our official and unofficial runs in the Enterprise track Document Search task. The second row contains the median MAP of all participating systems. Value in bold indicates a significant difference</head><label>6</label><figDesc></figDesc><table coords="6,91.68,55.04,426.26,145.65"><row><cell>Run</cell><cell></cell><cell>-</cell><cell></cell><cell>MAP</cell><cell>P@10</cell><cell>MRR</cell></row><row><cell>median</cell><cell></cell><cell>-</cell><cell></cell><cell>0.3072</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Feature</cell><cell>Training Data</cell><cell>Training Measure</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Official Runs</cell><cell></cell><cell></cell></row><row><cell>uogEDSF(Base)</cell><cell>-</cell><cell>Feedback documents</cell><cell>MRR</cell><cell cols="2">0.3393 0.4840 0.8092</cell></row><row><cell>uogEDSINLPRI</cell><cell>inlinks</cell><cell>Feedback documents</cell><cell>MRR</cell><cell cols="2">0.2694 0.4600 0.8680</cell></row><row><cell>uogEDSComPri</cell><cell>inlinks + URL length</cell><cell>Feedback documents</cell><cell>MRR</cell><cell cols="2">0.2190 0.4820 0.8505</cell></row><row><cell>uogEDSCLCDIS</cell><cell>click distance</cell><cell>Feedback documents</cell><cell>MRR</cell><cell cols="2">0.3442 0.4940 0.8236</cell></row><row><cell></cell><cell></cell><cell>Unofficial Runs</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell>URL length</cell><cell>Feedback documents</cell><cell>MRR</cell><cell cols="2">0.3002 0.4840 0.8381</cell></row><row><cell>-</cell><cell>inlinks</cell><cell>TREC 2003 Web Track mixed task</cell><cell>MAP</cell><cell cols="2">0.3162 0.4740 0.8531</cell></row><row><cell>-</cell><cell>inlinks + URL length</cell><cell>TREC 2003 Web Track mixed task</cell><cell>MAP</cell><cell cols="2">0.2322 0.4720 0.8511</cell></row><row><cell>-</cell><cell>URL length</cell><cell>TREC 2003 Web Track mixed task</cell><cell>MAP</cell><cell cols="2">0.3281 0.4880 0.8110</cell></row><row><cell>-</cell><cell>inlinks</cell><cell>TREC 2006 namedpage finding task</cell><cell>MRR</cell><cell cols="2">0.3000 0.4680 0.8548</cell></row><row><cell>-</cell><cell cols="2">inlinks + URL length TREC 2006 namedpage finding task</cell><cell>MRR</cell><cell cols="2">0.2382 0.4940 0.8566</cell></row><row><cell>-</cell><cell>URL length</cell><cell>TREC 2006 namedpage finding task</cell><cell>MRR</cell><cell cols="2">0.3249 0.5140 0.8183</cell></row></table><note coords="6,392.16,222.96,60.27,8.97"><p>(p ≤ 0.05) from</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="6,53.76,223.56,502.11,18.51"><head>the baseline run according to the Wilcoxon matched-pairs signed-ranks test.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="7,160.83,55.44,394.95,663.87"><head>Table 7 : Salient features of our Enterprise track expert search task submitted runs.</head><label>7</label><figDesc>)), with three fields, namely content,</figDesc><table coords="7,323.76,55.44,225.11,60.75"><row><cell>Run Name</cell><cell>Salient Features</cell></row><row><cell>uogEXFeMNZ</cell><cell>PL2F &amp; expCombMNZ voting technique</cell></row><row><cell>uogEXFeMNZP</cell><cell>+ query term proximity</cell></row><row><cell>uogEXFeMNZcP</cell><cell>+ candidate query term proximity</cell></row><row><cell>uogEXFeMNZdQ</cell><cell>+ document centric QE</cell></row><row><cell cols="2">uogEXFeMNZQE + candidate topic centric QE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="8,53.76,55.44,239.29,275.55"><head>Table 8 : The mean average precision (MAP) and mean recip- rocal rank (MRR) of our Enterprise track expert search task submitted runs, as well as the median performance achieved by all participating systems. Submitted is using the profile set we use for our submitted runs, while corrected depicts the re- trieval performance when the improved profile sets are applied. All runs use title only topics. uogEXFeMNZ is an additional, unsubmitted baseline run.</head><label>8</label><figDesc></figDesc><table coords="8,67.20,55.44,212.20,275.55"><row><cell></cell><cell cols="2">Submitted</cell><cell cols="2">Corrected</cell></row><row><cell>Run Name</cell><cell>MAP</cell><cell>MRR</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>Best</cell><cell cols="4">0.70010 0.9345 0.7010 0.9345</cell></row><row><cell>Median</cell><cell cols="4">0.2468 0.5011 0.2468 0.5011</cell></row><row><cell>uogEXFeMNZcP</cell><cell cols="4">0.3138 0.4475 0.4419 0.5802</cell></row><row><cell>uogEXFeMNZdQ</cell><cell cols="4">0.3122 0.4597 0.3748 0.5266</cell></row><row><cell>uogEXFeMNZP</cell><cell cols="4">0.3042 0.4239 0.3811 0.5024</cell></row><row><cell cols="5">uogEXFeMNZQE 0.2686 0.3670 0.3783 0.5149</cell></row><row><cell>uogEXFeMNZ</cell><cell>-</cell><cell>-</cell><cell cols="2">0.3782 0.5057</cell></row><row><cell>Method</cell><cell></cell><cell>MAP</cell><cell>MRR</cell><cell></cell></row><row><cell>VotesProx</cell><cell></cell><cell cols="2">0.2171 0.2754</cell><cell></cell></row><row><cell cols="2">CombMAXProx</cell><cell cols="2">0.4332 0.6034</cell><cell></cell></row><row><cell cols="2">CombSUMProx</cell><cell cols="2">0.3190 0.4139</cell><cell></cell></row><row><cell cols="2">CombMNZProx</cell><cell cols="2">0.2569 0.3223</cell><cell></cell></row><row><cell cols="4">expCombSUMProx 0.4370 0.5905</cell><cell></cell></row><row><cell cols="4">expCombMNZprox 0.4419 0.5802</cell><cell></cell></row><row><cell>MRRProx</cell><cell></cell><cell cols="2">0.3148 0.4081</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="8,53.76,343.56,239.04,18.51"><head>Table 9 : Comparison of the retrieval performance of various voting techniques in the Enterprise track expert search task.</head><label>9</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="9,76.20,55.44,479.82,663.87"><head>Table 10 : Salient features of our Blog track feed distillation task submitted runs.</head><label>10</label><figDesc></figDesc><table coords="9,76.20,700.68,216.71,18.63"><row><cell>It uses the PL2F weight-</cell></row><row><cell>ing model together with expCombMNZ voting technique to</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="9,316.80,218.88,239.35,49.83"><head>Table 11 : The mean average precision (MAP), Reciprocal Rank (MRR), and precision at 10 (P@10) of our submitted Blog track feed distillation task runs, as well as the median performance achieved by all participating systems. MRR and P@10 medians are not available.</head><label>11</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="9,316.80,690.24,239.16,29.07"><head>Table 12 : Comparison of the retrieval performance of various voting techniques in the Blog track feed distillation task.</head><label>12</label><figDesc>can be successfully applied to this task. While these techniques have been previously tested in smaller Enterprise settings with thousands of experts, this task has a much</figDesc><table coords="10,92.88,55.44,160.84,81.63"><row><cell>Method</cell><cell>MAP</cell><cell>MRR</cell><cell>P@10</cell></row><row><cell>Votes</cell><cell cols="3">0.2574 0.6108 0.4867</cell></row><row><cell>CombMAX</cell><cell cols="3">0.2074 0.7034 0.3756</cell></row><row><cell>CombSUM</cell><cell cols="3">0.2669 0.6399 0.4867</cell></row><row><cell>CombMNZ</cell><cell cols="3">0.2631 0.6249 0.4844</cell></row><row><cell cols="4">expCombSUM 0.2663 0.7726 0.5000</cell></row><row><cell cols="4">expCombMNZ 0.2909 0.7686 0.5222</cell></row><row><cell>MRR</cell><cell cols="3">0.2666 0.7711 0.4511</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="11,53.76,55.04,239.29,215.82"><head>Table 13 : The parameter values used in our TREC 2007 Blog track opinion finding task runs for title-only (T) and title- description (TD) queries. The title-only settings are also used in the blog distillation task.</head><label>13</label><figDesc></figDesc><table coords="11,80.88,55.04,184.94,215.82"><row><cell>Parameter</cell><cell cols="4">Equations(s) Value(T) Value(TD)</cell></row><row><cell>PL2F w body</cell><cell cols="2">(1) &amp; (2)</cell><cell>1.19</cell><cell>0.97</cell></row><row><cell>PL2F w title</cell><cell cols="2">(1) &amp; (2)</cell><cell>3.88</cell><cell>1.11</cell></row><row><cell cols="3">PL2F w anchor (1) &amp; (2)</cell><cell>0.11</cell><cell>0.77</cell></row><row><cell>PL2F c body</cell><cell cols="2">(1) &amp; (2)</cell><cell>9.34</cell><cell>2.73</cell></row><row><cell>PL2F c title</cell><cell cols="2">(1) &amp; (2)</cell><cell>22.28</cell><cell>98.70</cell></row><row><cell>PL2F c anchor</cell><cell cols="2">(1) &amp; (2)</cell><cell>8.16</cell><cell>0.84</cell></row><row><cell>pBiL2 cp</cell><cell cols="2">(6) &amp; (3)</cell><cell>40.00</cell><cell>-</cell></row><row><cell>pBiL2 dist</cell><cell>(6)</cell><cell></cell><cell>2</cell><cell>-</cell></row><row><cell>Parameter</cell><cell></cell><cell cols="2">Equation(s) Value</cell></row><row><cell cols="2">PL2F w body</cell><cell>(1) &amp; (2)</cell><cell>1.197</cell></row><row><cell cols="2">PL2F w title</cell><cell>(1) &amp; (2)</cell><cell>33.20</cell></row><row><cell cols="3">PL2F w anchor (1) &amp; (2)</cell><cell>43.23</cell></row><row><cell cols="2">PL2F c body</cell><cell>(1) &amp; (2)</cell><cell>12.25</cell></row><row><cell cols="2">PL2F c title</cell><cell>(1) &amp; (2)</cell><cell>66.83</cell></row><row><cell cols="2">PL2F c anchor</cell><cell>(1) &amp; (2)</cell><cell>45.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="11,53.76,283.08,239.20,18.51"><head>Table 14 : The parameter values used in our submitted runs to the TREC 2007 Enterprise Track.</head><label>14</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.32,702.24,139.89,8.07;1,53.76,711.24,101.82,8.07"><p>Information on Terrier can be found at: http://ir.dcs.gla.ac.uk/terrier/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="2,325.80,679.80,230.12,8.07;2,316.80,690.24,239.02,8.07;2,316.80,700.68,238.90,8.07;2,316.80,711.24,239.11,8.07"><p>This year we participate in both the Blog and Enterprise tracks. The test collection for the Blog track is the TREC Blogs06 test collection<ref type="bibr" coords="2,342.96,700.68,13.70,8.07" target="#b10">[10]</ref>, which is a crawl of 100k blogs over an 11-week period. During this time, the blog posts (permalinks), feeds (RSS XML</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="8,321.24,702.24,234.44,8.07;8,316.80,711.24,206.88,8.07"><p>In this task, we will use the term feed and blog interchangeably, as each blog in the collection has one corresponding feed.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Erik Graf</rs> for his assistance in running the OpinionFinder tool, and <rs type="person">Christina Lioma</rs> for identifying various linguistical sources of opinionated words. Moreover, we would like to thank the three friendly assessors who assisted us in our TREC assessment workload this year.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,321.29,55.55,96.83,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,77.04,205.67,8.07;10,331.80,87.47,186.99,8.07;10,331.80,98.04,104.86,8.07" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,369.60,77.04,167.87,8.07;10,331.80,87.47,139.29,8.07">Probabilistic Models for Information Retrieval based on Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,331.80,109.44,218.13,8.07;10,331.80,119.87,212.84,8.07;10,331.80,130.32,20.00,8.07" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,478.62,109.44,71.31,8.07;10,331.80,119.87,117.67,8.07">Italian Monolingual Information Retrieval with Prosit</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2002</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,141.84,207.66,8.07;10,331.80,152.27,222.61,8.07;10,331.80,162.72,93.12,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,437.26,141.84,102.20,8.07;10,331.80,152.27,108.39,8.07">Research on expert search at enterprise track of TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,456.48,152.27,97.93,8.07">Proceedings of TREC 2005</title>
		<meeting>TREC 2005<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,174.23,216.82,8.07;10,331.80,184.67,205.56,8.07;10,331.80,195.11,20.00,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,440.16,174.23,108.47,8.07;10,331.80,184.67,16.69,8.07">Overview of TREC 2004 Web track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,363.84,184.67,96.13,8.07">Proceedings of TREC 2004</title>
		<meeting>TREC 2004<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,206.63,204.68,8.07;10,331.80,217.07,202.12,8.07;10,331.80,227.51,183.89,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,331.80,217.07,188.42,8.07">Relevance weighting for query independent evidence</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.80,227.51,95.42,8.07">Proceesings of SIGIR 2005</title>
		<meeting>eesings of SIGIR 2005<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,238.91,199.44,8.07;10,331.80,249.47,194.12,8.07;10,331.80,259.91,110.08,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,478.43,238.91,52.81,8.07;10,331.80,249.47,77.41,8.07">Towards better Weighting of Anchors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Upstill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,425.40,249.47,96.50,8.07">Proceedings of SIGIR 2004</title>
		<meeting>SIGIR 2004<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="512" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,271.31,221.24,8.07;10,331.80,281.75,220.69,8.07;10,331.80,292.31,224.08,8.07;10,331.80,302.75,139.68,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,357.60,281.75,194.89,8.07;10,331.80,292.31,154.74,8.07">University of Glasgow at TREC 2006: Experiments in Terabyte and Enterprise Tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,502.32,292.31,53.56,8.07;10,331.80,302.75,40.12,8.07">Proceedings of TREC 2006</title>
		<meeting>TREC 2006<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,314.15,216.81,8.07;10,331.80,324.59,209.96,8.07;10,331.80,335.16,211.72,8.07;10,331.80,345.60,102.44,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,331.80,324.59,209.96,8.07;10,331.80,335.16,207.38,8.07">University of Glasgow at WebCLEF 2005: Experiments in Per-Field Normalisation and Language Specific Stemming</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,331.80,345.60,22.10,8.07">CLEF</title>
		<imprint>
			<biblScope unit="page" from="898" to="907" />
			<date type="published" when="2005">2005. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.80,357.00,218.68,8.07;10,331.80,367.44,193.38,8.07;10,331.80,377.88,186.82,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,433.41,357.00,117.07,8.07;10,331.80,367.44,179.75,8.07">Voting for Candidates: Adapting Data Fusion Techniques for an Expert Search Task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.80,377.88,98.52,8.07">Proceedings of CIKM 2006</title>
		<meeting>CIKM 2006<address><addrLine>Arlington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,389.40,217.35,8.07;10,331.80,399.84,223.41,8.07;10,331.80,410.28,205.82,8.07;10,331.80,420.72,104.86,8.07;10,331.80,431.51,177.94,7.29;10,331.80,442.20,194.38,7.05" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<ptr target="http://www.dcs.gla.ac.uk/∼craigm/publications/macdonald06creating.pdf" />
		<title level="m" coord="10,437.97,389.40,115.62,8.07;10,331.80,399.84,186.88,8.07">The TREC Blogs06 Collection : Creating and Analysing a Blog Test Collection DCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,336.24,453.12,208.13,8.07;10,331.80,463.56,209.20,8.07;10,331.80,474.12,20.00,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,437.97,453.12,106.41,8.07;10,331.80,463.56,48.66,8.07">Using Relevance Feedback in Expert Search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,396.48,463.56,94.01,8.07">Proceedings of ECIR 2007</title>
		<meeting>ECIR 2007<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,485.52,190.63,8.07;10,331.80,495.96,209.72,8.07;10,331.80,506.40,84.14,8.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,437.97,485.52,88.91,8.07;10,331.80,495.96,93.40,8.07">Expertise drift and query expansion in expert search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,441.00,495.96,96.56,8.07">Proceedings of CIKM 2007</title>
		<meeting>CIKM 2007<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,517.92,202.98,8.07;10,331.80,528.36,199.76,8.07;10,331.80,538.80,93.12,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,481.46,517.92,57.76,8.07;10,331.80,528.36,83.89,8.07">Overview of the TREC 2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,431.52,528.36,96.06,8.07">Proceedings of TREC 2007</title>
		<meeting>TREC 2007<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,550.32,216.16,8.07;10,331.80,560.75,189.94,8.07;10,331.80,571.20,220.57,8.07;10,331.80,581.63,109.12,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,370.08,560.75,151.66,8.07;10,331.80,571.20,109.45,8.07">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,457.08,571.20,95.29,8.07;10,331.80,581.63,33.35,8.07">Proceedings of OSIR 2006 Workshop</title>
		<meeting>OSIR 2006 Workshop<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,593.15,188.80,8.07;10,331.80,603.60,201.14,8.07;10,331.80,614.04,195.60,8.07" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,375.29,603.60,144.03,8.07">Overview of the TREC 2006 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.80,614.04,96.06,8.07">Proceedings of TREC 2006</title>
		<meeting>TREC 2006<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,625.56,205.66,8.07;10,331.80,636.00,208.16,8.07;10,331.80,646.44,66.28,8.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,412.99,625.56,128.92,8.07;10,331.80,636.00,94.54,8.07">Combination of Document Priors in Web Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,441.84,636.00,94.13,8.07">Proceedings of ECIR 2007</title>
		<meeting>ECIR 2007<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,657.84,207.34,8.07;10,331.80,668.40,218.52,8.07;10,331.80,678.84,144.83,8.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,423.85,657.84,119.73,8.07;10,331.80,668.40,124.21,8.07">Hierarchical language models for expert finding in enterprise corpora</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,472.32,668.40,78.00,8.07;10,331.80,678.84,16.00,8.07">Proceedings of ICTAI 2006</title>
		<meeting>ICTAI 2006<address><addrLine>Washington D.C., USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,690.24,202.68,8.07;10,331.80,700.67,219.81,8.07;10,331.80,711.24,178.19,8.07" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,488.26,690.24,50.66,8.07;10,331.80,700.67,137.56,8.07">Simple BM25 Extension to Multiple Weighted Fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,484.68,700.67,66.93,8.07;10,331.80,711.24,40.63,8.07">Proceedings of the CIKM 2004</title>
		<meeting>the CIKM 2004<address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,73.20,321.12,192.30,8.07;11,68.76,331.56,208.28,8.07;11,68.76,342.00,188.64,8.07;11,68.76,352.44,176.58,8.07;11,68.76,363.00,199.25,8.07" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,68.76,342.00,175.35,8.07">OpinionFinder: a system for subjectivity analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,68.76,352.44,176.58,8.07;11,68.76,363.00,55.29,8.07">Proceedings of HLT/EMNLP 2005 on Interactive Demonstrations</title>
		<meeting>HLT/EMNLP 2005 on Interactive Demonstrations<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,73.20,374.40,178.68,8.07;11,68.76,384.84,209.77,8.07;11,68.76,395.28,222.60,8.07;11,68.76,405.84,42.44,8.07" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,119.47,384.84,159.06,8.07;11,68.76,395.28,42.53,8.07">Microsoft Cambridge at TREC 13: Web and Hard Tracks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.20,395.28,109.33,8.07">Proceedings of the TREC 2004</title>
		<meeting>the TREC 2004<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
