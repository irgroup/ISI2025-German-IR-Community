<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,115.92,113.46,380.17,14.93;1,271.65,135.38,68.71,14.93">University of Lethbridge&apos;s Participation in TREC-2007 QA Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,231.94,167.69,57.46,10.37"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
							<email>chali@cs.uleth.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Lethbridge</orgName>
								<address>
									<addrLine>4401 University Drive Lethbridge</addrLine>
									<postCode>T1K 3M4</postCode>
									<settlement>Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.64,167.69,67.43,10.37"><forename type="first">Shafiq</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
							<email>jotys@cs.uleth.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Lethbridge</orgName>
								<address>
									<addrLine>4401 University Drive Lethbridge</addrLine>
									<postCode>T1K 3M4</postCode>
									<settlement>Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,115.92,113.46,380.17,14.93;1,271.65,135.38,68.71,14.93">University of Lethbridge&apos;s Participation in TREC-2007 QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">782CF69124B368D2F04852F9F136A188</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) is retrieving answers to natural language questions from a collection of documents rather than retrieving relevant documents containing the keywords of the query which is performed by search engines. What a user usually wants is often a precise answer to a question. For example, given the question "Who won the nobel prize in peace in 2006?" what a user really wants is the answer "Dr. Muhammad Yunus", in stead of reading through lots of documents that contain the words "win", "nobel","prize", "peace <ref type="bibr" coords="1,277.52,361.49,72.65,8.64">" and "2006" etc.</ref> This means that question answering systems will possibly be integral to the next generation of search engines.</p><p>The Text Retrieval Conference, TREC<ref type="foot" coords="1,272.13,383.47,3.69,6.39" target="#foot_0">1</ref> QA track is the major large-scale evaluation environment for open-domain question answering systems. The questions in the TREC-2007 QA track are clustered by target, which is the overall theme or topic of the questions. The track has three types of questions: 1. factoid that require only one correct response, 2. list that require a non redundant list of correct responses and 3. other questions that require a non redundant list of facts about the target that has not already been discovered by a previous answer.</p><p>We took the approach of designing a question answering system that is based on document tagging and question classification. Question classification extracts useful information (i.e. answer type) from the question about how to answer the question. Document tagging extracts useful information from the documents, which will be used in finding the answer to the question. We used different available tools to tag the documents. Our system classifies the questions using manually developed rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The global architecture of our system is shown in Figure ??. Each of the modules of the system is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Normalization</head><p>Before classifying the questions, our system changes some of the questions into a standard form that will be easier to classify.</p><p>Questions with 's after the question word means is. For example, the the question Where's Montenegro? is changed to "Where is Montenegro?" For any question starting with a preposition </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Pronoun Resolution</head><p>The questions addressing a target often refer to the target by pronouns (i.e. he, she, it, they, his, her, its, their). We replace the personal pronouns (i.e. he, she, it, they) by the target and the possessive pronouns (i.e. his, her, its, their) by target's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">"What" Factoid Normalization</head><p>There are different ways to say the same thing. For example, the questions: "Name a film in which Jude Law acted.", "Jude Law was in what movie?", "Jude Law acted in which film?", "What is a film starring Jude Law?", ask the same information. So, they should all be classified as What questions, and more specifically, as What-Film questions. Our system will handle questions that involve the word which as a what question. Questions, like "Jude Law acted in which film?" and "Jude Law was in what movie?", that do not start with a question word are changed to "What film was Jude Law acted in?" and "What movie was Jude Law in?". For this method, our system will first put a What at the beginning of the question, followed by the second half the question, forming What movie in these examples. Then, was is added followed by the first half of the question, resulting in What movie was Jude Law acted in?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">"What" Definition Normalization</head><p>There are questions that ask for why someone was famous, and questions requiring a definition of what something was. We normalize them to the form "What is X?" or "Who is X?", where X is the target. For example, the questions: "What is Colin Powell best known for?" and "Define thalassemia." are changed to "Who is Colin Powell?" and "What is thalessemia?" respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">"What" List Normalization</head><p>Some list questions include the number of entities to be included in the list of answers which is helpful in returning answers for these questions, so we extract the number, and reformat these questions to a general format. For each of these questions, the number is passed on to be used when the answer list is returned. Examples of normalization of these questions are: "What are titles of books written by Krugman?" is normalized to "What titles of books written by Krugman?" and "Name 10 auto immune diseases." is normalized to "What auto immune diseases?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Classification</head><p>Our system classifies questions into one of the categories: who, when, why, how, where and what. Any question that does not include one of the first five stems (Whom is considered as Who) is considered as what question. Once the questions are categorized into one of the categories, a finer classification is done to extract the answer type of the question. Classifying questions in this way aids our system in finding the answer type of the question, as well as helping to create the query to retrieve passages from our information retrieval system. Our system classifies questions using rules that we derived through observation. Supervised machine learning can also be used to classify questions, using methods similar to those outlined by ?) and ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">When Questions</head><p>When questions always ask for a DATE, but there are a few that are only asking for a day, and not a full year. The ones asking for a day will be asking for a special day that might appear on the same day each year, like a birthday, or Labour Day. Most where questions require a type of location. There are other where questions that ask for a certain college or university someone went to. They are outlined in Table ??. For questions asking for schools, the answer extractor will extracts all the schools. Other where questions ask for some kind of location more specifically the named entities (NE): CITY, PROVINCE, COUNTRY, GE-OGRAPHICAL LOCATION. For these questions, the answer extractor will extract these entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Type Answer Type Pattern Example</head><p>Where-School UNIVERSITY college or university or school or degree</p><p>Where did Bill Gates go to college? Where-Location LOCATION all the rest Where is Las Vegas?</p><p>Table <ref type="table" coords="4,257.79,188.32,3.88,8.64">3</ref>: Where Question Categories</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Why Questions</head><p>These questions require reasons, which are not entities that our system currently recognizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">How Questions</head><p>Most how questions are of the form, "How X", where adjective X gives clues about what kind of entity the question is asking for. The answer type for most of the how questions is straightforward. some sample categories are shown in Table ??. The exception is the how many questions (not shown in table) because they require a count of a certain entity. These questions start with the phrase How many and after it comes the entity (nouns) needed to be counted. Examples of How many questions are: How many rooms does the Las Vegas MGM Grand Hotel have?, How many layers of skin do we have? To extract the entity, first we extract How many from the start of each question, then tag the question with a shallow parse. The reason we take off the How many first is because most taggers and parsers are trained on documents without question words and because of that, they do not do a very good job of tagging questions (?). The last noun of the first noun phrase of each tagged question will be the entity that needs to be counted. In the above example the entities will be, respectively: rooms, and layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6">What Questions</head><p>Some What questions are classified by pattern matching and, for the rest of questions, the question focus is used to classify the questions. The what questions that are classified by patterns are in Table ??. What definition questions are handled similarly to the "who definition" questions, where the target is given to the answer extractor. What Acro questions are looking for either the acronym for a certain entity, or an acronym stands for. For the "What Verb" questions, the answer type is ambiguous. For example, there are many types of things that can be invented, discovered, and eaten. Our system does not answer these questions now. These questions can be answered using patterns in The what questions that are not classified by pattern matching can be classified by discovering the focus of the question. In the question, "What country is the leading exporter of goats?", the question focus is COUNTRY. Once the questions are all tagged with chunked parts of speech, patterns are used to extract the question focus. The focus extracting patterns are: 1.</p><formula xml:id="formula_0" coords="5,108.00,282.19,396.00,32.73">[NP What (type or kind or breed)] [PP of] [NP X], 2. [NP What X] [NP What] [NP X], 3. ('s or ') X], 4. [NP What] [VP (is or was)] [NP X], 5. [NP Name X], and 6. [NP Name] [NP X].</formula><p>These focuses are then matched to a named entity, and that will be considered the answer type of the question. For each of these questions, the answer type is passed to the answer extractor.</p><p>There are questions that have a focus that is unique, and is not a named entity that is currently tagged by OAK system (?). For these questions, the hypernym set of WordNet is used to try to get a list of entities that are of the type of the focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Creation and Passage Retrieval</head><p>We are using java based Lucene<ref type="foot" coords="5,239.48,411.05,3.69,6.39" target="#foot_1">2</ref> as our information retrieval system. ?) notes that because of the Lucene system's ability to have a greater understanding of natural language, the passages retrieved can be more relevant to the query. We index all paragraphs in the document collection in case folded and stemmed fashion. Therefore, "Bank" will be indexed as the same as the "banking". The query in Lucene can be thought as "Vector-space on top of Boolean", that means it will satisfy the boolean true as well as it will rank documents. We query the documents using three methods:</p><p>We take all the nouns, verbs and adjectives from the question that are not stop words, and find synonyms for each of them, using WordNet. We use these synonyms to form a query, where each word in the synonym set for a word is separated by OR, and each synonym set is separated by AND. Our system does not include the words contained in the question focus when creating the expanded query. This query ensures that all documents retrieved should be somehow related to the question. These queries are often too restrictive and retrieve no documents.</p><p>If the first method retrieves no documents, then our system forms a boolean query with the proper nouns and dates from the question. This will usually retrieve the document with the answer to the question, but will also retrieve many documents unrelated to the question.</p><p>If the previous two methods retrieve no documents, then a vector-space search is performed taking all the important words in the query along with their synonyms.</p><p>For definition questions we do a query just on the subject (focus) to be defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Document Tagging</head><p>The document tagging module of our system tags useful information from the passages retrieved by Lucene. We use the Lingpipe<ref type="foot" coords="5,241.94,673.18,3.69,6.39" target="#foot_2">3</ref> to resolve the coreference. We use OAK Tagger (?) to tag the passages with: 1. Part of Speech 2. Chunked Part of Speech and 3. Named Entity.</p><p>Each word in WordNet has multiple senses for the different ways the word can be used. Word sense disambiguation (WSD) is the process of determining in which sense a word is being used. Once the system knows the correct sense that the word is being used, WordNet can be used to determine synonyms. This is useful for seeing if a word from the question is associated with a word in the passage by being in the same synonym set. To tag the correct sense we developed a WSD system (?). It creates a list of important words in the passage. Then it measures the semantic relations (repition, synonym, hypernym, hyponym, gloss etc.) among the words using WordNet. We assign weights to the semantic relations. The sense that is mostly connected to the other words in the passage is said to be the sense it is being used in.</p><p>WordNet has sets for each word called hyponyms which are words that are examples of that word. This hyponym list can be loaded as a list of examples and a system can be made to tag these examples inside the document, similar to how OAK system tags NEs. To use this method, our system should first know what word's hyponym list is going to be used to tag the document. Knowing the NE our system is looking for will allow our system to tag only the entities relevant to the question. These entities have already been discovered in question classification. The hyponym list is then extracted and compared to the words of the document, and the entities that are in the list are tagged as possible answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Answer Extraction for List and Factoid Questions</head><p>Since the answer types and question types are known from the question classifier, and the documents are tagged with both shallow parse and named entities, extracting the information from the documents will use patterns to extract the named entities associated with the answer type. For some questions, the answer is not tagged and will be extracted using patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">"How Many" Questions</head><p>How many questions are answered using patterns and not named entities since these questions looks for a count of a certain entity. Answers will appear frequently in the tagged documents in the pattern "[NP NUMBER ENTITY]". For the question, "How many hexagons are on a soccer ball?", the entity to be counted is hexagons, so the pattern it is trying to match is [NP NUMBER hexagons]. WordNet is also used to get synonyms for the entity that is being counted. For example, given the question, "How many floors are in the Empire State Building?" the synset (floor, level, storey and story) of the word floor can be used to extract answers from the passage: "The Empire State Building climbed to an unthinkable height of 102 stories in that city four years later."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Names of People</head><p>The OAK system tags people in four ways: 1. PERSON, 2. LASTNAME, 3. FEMALE FIRSTNAME and 4. MALE FIRSTNAME. This presents a problem because the OAK tagger tags everything at once from a list of names. Some LASTNAMES are missing from the list of tags, and other names are also names of cities, such as the name Paris. The answer extracting module needs to extract the full name of the person when it is available. We used the patterns: "</p><formula xml:id="formula_1" coords="6,393.41,591.10,110.58,8.59">[A-Z ]NAME [A-Za-z]NNP</formula><p>[A-Za-z]/NNP" and " NAME TYPE NAME/NNP NAME TYPE NAME/NNP " to extract the names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4">Dates</head><p>Some dates that are tagged by OAK do not fit into either of these categories and are considered relative dates. These include today, this year, this month, next week and midnight. These dates are not helpful in answering questions, and are eliminated right away. Also, answers to questions that are looking for a particular date should have a four digit year in them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5">Quantities</head><p>OAK tags each quantity it sees as a quantity, but does not tag quantities together that are of the same measurement but in different units. For instance, it will tag the measurement 4 foot 2 inches as PHYSICAL EXTENT 4 foot PHYSICAL EXTENT 2 inches . We extracted the full quantity using patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.6">Other Types of Questions</head><p>For the rest of the questions, possible answers are extracted by extracting the named entities associated with the answer type of the question. This is done by pattern matching with " NE X " where X is a possible answer if NE is a named entity tag corresponding the the answer type of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.7">Definition Questions</head><p>In fact finding questions, there is always a topic that is being sought. If the question is "Who is X?" (X being a name of a person), there will be different methods for finding facts for it, compared to a non-person entity that will be phrased as "What is a Y?" In order to find patterns to extract the facts, I took all the definition questions and the sentences in the collection that contain the answers to these questions from the previous TREC years. We chunk the sentences using OAK chunker. I discovered the patterns for answers from these chunked sentences. Our current definition patterns include, with X representing facts and TARGET representing the subject of the fact:1. "[NP X TARGET]", 2. "[TARGET] , X (, or . or ;)", 3. "[TARGET] (is or are) X (. or ;)", 4. "X called [TARGET]", 5. "[BEGINNING OF SENTENCE] [TARGET], X, is X" and 6. "[TARGET] and other [NP X ]".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Answer Ranking</head><p>This module gives a score to each possible answer, and returns the one with the highest score, or the answers with the highest scores if a list of answers is required. Many systems ((?), (?) and (?)) use lexical patterns to extract answers from the documents, in contrast to our approach of extracting named entities. Our system used the extracting patterns instead to rank answers. The following patterns are used to rank the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Date of Birth and Date of Death</head><p>The date of birth and date of death of a person are sometimes put in brackets, after a person's name, e.g. "PERSON ( DATE -DATE )". For example, Elvis Presley , James Dean  are the new men. contains such patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">"What" Location Questions</head><p>When a location is the answer type of the question, and the question contains a preposition like in, on, from or near, those words will frequently appear before the location entity that is the answer. For questions that contain on and from, in can also appear before the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">"When" Questions Ending in a Verb</head><p>Some questions end with a verb that represents the particular action that is being asked about. Examples of these questions are: When was Microsoft established?, When was Hiroshima bombed? etc. For these questions, an answer is usually found in the following pattern, "VERB in DATE ", where VERB represents a verb with a stem that is a synonym of the last verb from the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.4">Who Action</head><p>Who questions often contain the action of being. For example: Who was Khmer Rouge's first leader? There are some questions that contain a physical action that are not being: Who wrote "Dubliners"? These actions will be represented in the passages that the answers are in, but might take different forms. For these types of questions, our system will only take the pattern of the whole action. For example, the question "Who wrote"Dubliners"?", has the action "wrote "Dubliners"", and will be the pattern our system uses to rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.5">Word Association</head><p>Lexical chains (?) are created when we associate words together that have a similar theme. ?) discussed using WordNet to help with the associations between words. Besides synonym, hypernym, hyponym and gloss each word in WordNet has a derived form as well. The derived form for a noun is a verb that is associated with the noun. With these tools there can be a path formed from the question words to the words of the passage of a possible answer.</p><p>In order to find this association, we extracted the important words(i.e. nouns, verbs, adjectives and adverbs) from the passage with candidate answer as well as from the question. Then we perform our WSD algorithm on these words(where the passage words preceeds the question words) and form lexical chains. Now, we check for each of the question words whether we have got any chain or not. A chain indicates there is a association between the question word and the passage words. In this way we count the number of association. For example, Question: For which newspaper does Krugman write?</p><p>Passage: Paul Krugman is also an author and a columnist for The New York Times. the passage words author and columnist are associated with the question word write by their semantic relation(gloss of author and columnist in this case). So, here the number of association is two one is for Krugman(repitition) and second one is for write(gloss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.6">WordNet Glossary</head><p>Our system extracts the WordNet glossary entry for the proper nouns in the question. For definition questions, the glossary entry for the target is used. For factoid questions, our system tags the glossary entry for named entities, and if it contains the answer type, that answer is given a higher rank. For definition questions, the WordNet glossary entry is passed to the redundancy checker to rank facts. For the question, "Where is Belize located?", Belize is a proper noun and its WordNet gloss is: "a country on the northeastern coast of Central America on the Caribbean; formerly under British control. The gloss tagged with POS and NE is: For where questions, GEOLOGICAL REGION is an accepted answer type, and both Central America and Caribbean are acceptable answers for this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.7">Target distance</head><p>One other method that is commonly used to rank answers is the distance between a possible answer and keywords from the question. Our system calculates distance as the count of words and punctuation between the important words from the question and the possible answer. If there are no such entities in a question, then this method is not used to rank answers for that question. For example, the question "What does Kurt Vonnegut do for a living?", has a target of Kurt Vonnegut.</p><p>The passage contains the answer to this question: " Trust a crowd, says author Kurt Vonnegut, to look at the wrong end of a miracle every time." The answer author is considered, by our system, to be one word away from the target of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.8">Redundancy Checking</head><p>Answers that appear frequently is given a higher rank if the retrieved documents are related to the question. If the question requires more than one answer, each answer returned by the system should be unique. When this module returns an answer, it performs a check to see if the answer has already been included in the final answer list. This check is performed by checking whether the answer contains nouns that have already appeared in a previous answer.</p><p>For definition questions, an answer can contain more than one fact. For example for the target Jane Goodall, the phrase, the British primatologist, gets extracted. This contains the fact that she is British, and the fact that she is a primatologist. If this is the case, then the phrase that contains two or more facts is added to the list of answers, and if either of the contained facts were found in the list, their score will be added to the score of the combination fact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.9">Answer Ranking Formula</head><p>Factoid and List Questions: The ranking formula is developed by reviewing which weights for the methods above yielded the highest accuracy on the corpus of questions and their answers. For Factoid and List Questions our answer ranking formula is: score of an occurance of a candidate answer = (6 × w 1 ) + (w 2 ) + (3 × w 3 ) + 1 w 4 where, w 1 denotes whether the answer is found in a pattern associated with the question type. The value will be 1 if it was found in such a pattern, and 0 if it was not.</p><p>w 2 denotes how many words from the question are represented in the passage with the answer, plus 3 more points for each word that is represented by a disambiguated word.</p><p>w 3 denotes if the answer appears in the WordNet glossary for important words from the question.</p><p>(value of 0 if it does not and 1 if it does)</p><p>w 4 denotes the distance between the important words from the question and the answer.</p><p>Each answer's final rank is the sum of the ranks of the occurrences of that answer.</p><p>Definition Questions: We rank answers to definition questions, giving them one point for repetition and four points if found in the WordNet gloss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Out of 360 FACTOID questions in TREC-2007, 16 had no known correct response in the document collection, hence "NIL" is the correct answer for these questions. For the 85 LIST questions in TREC 2007, our system achieved the "average F-measure" score of 0.132 with recall and precision weighted equally (α = 0.5). Our system achieved the "average pyramid F-score" (with β = 3) of 0.030 over 70 OTHER questions. In TREC 2007, there were questions about 70 topics (targets or series). Our system achieved average per-series score of 0.141. This year we have improved our rank in all three types of questions. This overall improvement was primarily because of our expanded classification and the addition of word dependencies to answer ranking. Our system extracted the answers from the top 100 (50 AQUAINT-2 + 50 BLOG06) relevant documents provided by TREC instead of using the whole collection. Our system did not find answers (i.e. NIL) for 153 FACTOID questions. According to the TREC-2007 evaluation there were 16 questions with "NIL" answers. Our system could find 8 correct "NIL" answers. This clearly indicates that if we would use the whole document collection, our system could have found more answers (i.e. less number of "NIL" answers) hence, we could have scored very good results in TREC 2007.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,263.22,350.04,85.56,8.64;2,108.00,72.86,411.00,261.75"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: QA System</figDesc><graphic coords="2,108.00,72.86,411.00,261.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,132.91,532.80,362.91,8.64;8,132.91,544.76,350.51,8.64;8,139.26,556.71,272.72,8.64"><head></head><label></label><figDesc>a/DT country/NN on/IN the/DT northeastern/JJ coast/NN of/IN GEOLOGICAL REGION Central/NNP America/NNP on/IN the/DT GEOLOGICAL REGION Caribbean/NNP ;/: formerly/RB under/IN NATIONALITY British/JJ control/NN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,118.47,582.10,375.06,128.01"><head>Table 5 :</head><label>5</label><figDesc>What Simple Question Categories the syntactic parse of the documents. Once we add a syntactic parse of the documents to our system, We will attempt to handle these questions.</figDesc><table coords="4,118.47,582.10,375.06,128.01"><row><cell cols="2">Question Type Answer Type</cell><cell>Pattern</cell><cell>Example</cell></row><row><cell>How-Large</cell><cell>AREA or VOL-</cell><cell>How [big or</cell><cell>How big is Mars?</cell></row><row><cell></cell><cell>UME</cell><cell>large]</cell><cell></cell></row><row><cell cols="2">How-Accurate PERCENTAGE</cell><cell>How [accurate]</cell><cell>How accurate is HIV tests?</cell></row><row><cell>How-Often</cell><cell>TIME PERIOD</cell><cell>How [often or fre-</cell><cell>How often is someone murdered</cell></row><row><cell></cell><cell></cell><cell>quent]</cell><cell>in USA?</cell></row><row><cell>How-Temp</cell><cell>TEMPERATURE</cell><cell>How [warm or</cell><cell>How hot is the sun?</cell></row><row><cell></cell><cell></cell><cell>cold or hot]</cell><cell></cell></row><row><cell>How-Old</cell><cell>AGE</cell><cell>How old</cell><cell>How old is the universe?</cell></row><row><cell></cell><cell cols="3">Table 4: How sample Question Categories</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,108.00,526.49,396.00,92.86"><head>Table ? ?</head><label>?</label><figDesc>shows our scores for FACTOID questions.</figDesc><table coords="9,113.98,562.70,385.09,56.64"><row><cell>Total Globally</cell><cell>Locally</cell><cell cols="5">Wrong Unsupported Inexact Accuracy "NIL"</cell><cell>"NIL"</cell></row><row><cell>Correct</cell><cell>Correct</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Preci-</cell><cell>Recall</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sion</cell><cell></cell></row><row><cell>360 93</cell><cell>5</cell><cell>231</cell><cell>12</cell><cell>19</cell><cell>0.258</cell><cell>8/153 =</cell><cell>8/16 =</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.052</cell><cell>.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,215.72,632.66,180.55,8.64"><head>Table 6 :</head><label>6</label><figDesc>UofL Score for FACTOID questions</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,122.35,687.21,60.55,6.91"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,122.35,705.86,99.02,6.91"><p>http://jkarta.apache.org/lucene/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,122.35,715.67,101.32,6.91"><p>http://www.alias-i.com/lingpipe</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,108.00,249.96,396.00,8.82;10,122.94,261.91,381.05,8.82;10,122.94,274.05,22.02,8.64;10,108.00,293.97,396.00,8.64;10,122.94,305.75,381.05,8.82;10,122.94,317.70,221.26,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,270.50,250.14,197.38,8.64;10,466.36,293.97,37.64,8.64;10,122.94,305.93,149.25,8.64">Word sense disambiguation using lexical cohesion</title>
		<author>
			<persName coords=""><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shafiq</forename><forename type="middle">R Acl</forename><surname>Joty ; Prague</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Melz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,486.19,249.96,17.81,8.59;10,122.94,261.91,278.84,8.59;10,291.01,305.75,212.99,8.59;10,122.94,317.70,51.61,8.59">Proceedings of the 4th International Conference on Semantic Evaluations</title>
		<meeting>the 4th International Conference on Semantic Evaluations<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2007. 2003</date>
			<biblScope unit="page" from="772" to="781" />
		</imprint>
	</monogr>
	<note>Proceedings of the Twelfth Text REtreival Conference (TREC 2003)</note>
</biblStruct>

<biblStruct coords="10,108.00,337.63,396.00,8.82;10,122.94,349.58,381.06,8.59;10,122.94,361.54,381.06,8.82;10,122.94,373.67,29.60,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,201.10,337.81,232.56,8.64">Parsing and question classification for question answering</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,454.65,337.63,49.35,8.59;10,122.94,349.58,381.06,8.59;10,122.94,361.54,281.27,8.59">Proceedings of the Association for Computational Linguistics 39th Annual Meeting and 10th Conference of the European Chapter Workshop on Open-Domain Question Answering</title>
		<meeting>the Association for Computational Linguistics 39th Annual Meeting and 10th Conference of the European Chapter Workshop on Open-Domain Question Answering<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,393.42,396.00,8.82;10,122.94,405.38,289.05,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,222.40,393.60,113.24,8.64">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,353.99,393.42,150.01,8.59;10,122.94,405.38,223.03,8.59">Proceedings of the 19th International Conference on Computational Linguistics (COLING-02)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING-02)<address><addrLine>Taipei,Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,425.48,396.00,8.64;10,122.94,437.26,381.05,8.82;10,122.94,449.21,244.44,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,489.62,425.48,14.39,8.64;10,122.94,437.43,245.54,8.64">Experiments and analysis of lcc&apos;s two qa systems over trec2004</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.06,437.26,114.94,8.59;10,122.94,449.21,139.82,8.59">Proceedings of the 13th Text REtrevial Conference (TREC 2004)</title>
		<meeting>the 13th Text REtrevial Conference (TREC 2004)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,469.14,396.00,8.82;10,122.94,481.09,190.07,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,272.93,469.32,150.98,8.64">Lexical chains for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,444.06,469.14,59.93,8.59;10,122.94,481.09,56.96,8.59">Proceedings of COLING 2002</title>
		<meeting>COLING 2002<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="674" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,501.20,396.00,8.64;10,122.94,512.97,231.75,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,236.17,501.20,267.83,8.64;10,122.94,513.15,61.97,8.64">Lexical cohesion cornputed by thesaural relations as an indicator of structure of text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,191.95,512.97,104.65,8.59">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,533.08,396.00,8.64;10,122.94,544.85,381.05,8.82;10,122.94,556.99,162.87,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,481.31,533.08,22.69,8.64;10,122.94,545.03,78.00,8.64">Ibm&apos;s piquant in trec2003</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wlty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mahindru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.07,544.85,275.64,8.59">Proceedings of the Twelfth Text REtreival Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtreival Conference (TREC 2003)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,576.91,396.00,8.64;10,122.94,588.69,381.06,8.82;10,122.94,600.64,153.75,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,267.69,576.91,236.31,8.64;10,122.94,588.87,13.08,8.64">Learning surface text patterns for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,154.34,588.69,349.67,8.59;10,122.94,600.64,22.58,8.59">Proceedings of the 40th Annual meeting of the association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual meeting of the association for Computational Linguistics (ACL)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.00,620.75,389.71,8.64" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<ptr target="http://nlp.nyu.edu/oak" />
		<title level="m" coord="10,184.35,620.75,216.56,8.64">Proteus project oak system (english sentence analyzer)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
