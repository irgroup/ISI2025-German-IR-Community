<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.76,71.96,346.24,12.91;1,257.64,89.96,100.13,12.91">FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.72,128.43,85.83,9.82"><forename type="first">Giambattista</forename><surname>Amati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Ugo Bordoni</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.72,128.43,77.43,9.82"><forename type="first">Edgardo</forename><surname>Ambrosi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IASI &quot;Antonio Ruberti&quot; -CNR</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.68,128.43,64.95,9.82"><forename type="first">Marco</forename><surname>Bianchi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IASI &quot;Antonio Ruberti&quot; -CNR</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.04,128.43,64.73,9.82"><forename type="first">Carlo</forename><surname>Gaibisso</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IASI &quot;Antonio Ruberti&quot; -CNR</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,276.60,141.99,75.63,9.82"><forename type="first">Giorgio</forename><surname>Gambosi</surname></persName>
							<email>gambosi@mat.uniroma2.it</email>
							<affiliation key="aff2">
								<orgName type="department">Matematics Department</orgName>
								<orgName type="institution">of University &quot;Tor Vergata&quot;</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.76,71.96,346.24,12.91;1,257.64,89.96,100.13,12.91">FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EAA3FC0A8D0501D8FC70324FCD0E35B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a fully automatic and weighted dictionary to be used in topical opinion retrieval. We also define a simple topical opinion retrieval function that is free from parameters, so that the retrieval does not need any learning or tuning phase.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A blog is basically a diary made up of items (posts) that are posted on a regular basis and, typically, displayed to visitors in reverse chronological order. Posts comprises text, hypertext, images, and links to other web pages, video, audio and other files. The TREC Blog track <ref type="bibr" coords="1,297.47,363.39,12.75,9.82" target="#b5">[6]</ref> was first introduced in TREC 2006 and ran again in 2007, with the claimed purpose of to explore information seeking behavior in the blogosphere.</p><p>A test collection, called Blog06, was created for the TREC Blog track <ref type="bibr" coords="1,464.99,404.07,11.59,9.82" target="#b2">[3]</ref>. The blog collection was crawled over a period of 11 weeks <ref type="bibr" coords="1,399.83,417.63,73.61,9.82">(December 2005</ref><ref type="bibr" coords="1,476.87,417.63,3.63,9.82;1,134.76,431.19,60.98,9.82">-February 2006</ref>). The total size of the collection amounts to 148 GB with three main different components: feeds (38.6 GB), permalinks (88.8GB) <ref type="foot" coords="1,424.56,442.80,3.99,7.17" target="#foot_0">1</ref> , and homepages (20.8 GB). The collection contains spam as well as possibly non-blogs and non-English pages. For our experimentation we considered only the permalink component, consisting of 3.2 million of Web pages, each one containing a post and its related comments.</p><p>Blog track 2006 only ran the opinion retrieval task. This task has been ran again in 2007, with a polarity subtask. Additionally, a new task has been added, the Blog Distillation (Feed Search) task. We have only taken part in the opinion retrieval task. This task focuses on a specific aspect of blogs: the opinionated nature of posts, that is it requires locating those blog posts that express an opinion about a given target. It can be summarized as "What do people think about &lt;target&gt; ?". The target can be a "traditional" named entity (a name of a person, location, or organization), but also a concept (such as a type of technology), a product name, or an event.</p><p>Blog track 2007 adopted the same assessment procedure defined in 2006. The retrieval unit is document from the permalink component of the Blog06 test collection. The content of a blog post is defined as the content of the post itself and the contents of all comments to the post: if the relevant content is in a comment, then the permalink is declared to be relevant. The objective is to run again the opinion retrieval task with 50 new topics, that NIST selected from query logs provided by commercial blog search engines.</p><p>The following scale has been used for the assessment:</p><p>• -1, as not judged. The content of the post was not examined due to offensive URL or header (spam). • 0, as not relevant. The post and its comments were examined, and does not contain any information about the target, or refers to it only in passing. • 1, as relevant. The post or its comments contain information about the target, but do not express an opinion towards it.</p><p>Furthermore, if the post or its comments are not only relevant, but also contain an explicit expression of opinion or sentiment about the target, showing some personal attitude of the writer(s), then the document is alternatively labeled as follows:</p><p>• 2, as relevant, and containing negative opinions. The post contains an explicit expression of opinion or sentiment about the target, showing some personal attitude of the writer(s), and the opinion expressed is explicitly negative about, or against, the target. • 3, as relevant, and containing mixed positive and negative opinions. Same as 2, but contains both positive and negative opinions. • 4, as relevant, and containing positive opinions. Same as 2, but the opinion expressed is explicitly positive about, or supporting, the target.</p><p>Evaluation metrics are precision/recall based, such as the Mean Average Precision (MAP), but we focused our attention on Precision at 10 documents (P@10), because it is often used for Web search evaluation.</p><p>For the Blog track of TREC 2007, we create a dictionary of opinionated words from the Blog 2006 relevance data. We have two information theoretic measures to accomplish the automatic construction of an opinionated vocabulary. The first measure is based on a DFR (Divergence From Randomness) model and defines the weight of opinionated terms within documents. The second measure is based on the maximization of the entropy in the set Opin = {d : d is relevant and opinionated} of all relevant and opinionated documents, and it is used to filter the words into a sequence of classes V k ⊃ V k+1 with 1 ≤ k ≤ |Opin|. Our aim is to define the optimal k such that V k is both as small as possible (for a real-time implementation purpose) and as effective as possible for opinion detection.</p><p>We take three assumptions for an automatic construction of an opinionated dictionary:</p><p>-Content-bearing words maximize the probability Prob(posterior|prior) of observing the posterior probability of occurrence in the subset Opin of opinionated and relevant documents given the prior probability of occurrence in all relevant documents. In other words, content-bearing words occur with similar relative frequencies in both opinionated relevant and strictly relevant sets of documents. -Opinion-bearing words instead minimize the probability Prob(posterior|prior) of observing the posterior probability of occurrence in the subset Opin of opinionated and relevant documents given the prior probability of occurrence in all relevant documents. The weight of an opinion-bearing word is provided by the -log of such a probability (DFR model), and thus opinionbearing words maximize the divergence -log 2 Prob(posterior|prior). -Best opinion-bearing words also maximize the entropy in the subset Opin of opinionated and relevant documents. In other words opinion-bearing words occur more randomly than content-bearing words in the subset Opin of opinionated and relevant documents. An approximating way to maximize entropy is to consider terms with highest divergence and that belong to a large number k of opinionated documents.</p><p>Our experimentation in the BLOG TREC 2007 consists of three phases:</p><p>1. data pre-processing and topic relevance retrieval; 2. semi-automatic construction of a sentimental dictionary with weighted terms; 3. topic opinionated relevance retrieval.</p><p>These phases are detailed described in Sections 2, 3 and 4, respectively. Finally, in Section 5 we report and discuss on the experimentation activity and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data pre-processing and topic relevance retrieval</head><p>The data pre-processing phase is aimed to remove not English documents from the collection. This goal is achieved by Lingpipe <ref type="bibr" coords="3,357.48,583.35,11.59,9.82" target="#b0">[1]</ref>, a suite of Java libraries for the linguistic analysis of human language. LingPipe's text classifiers learn by example. For each language being classified, a sample of text is used as training data. LingPipe learns the distribution of characters per language using character language models. Character language models provide state-of-the-art accuracy for text classification. Character-level models are particularly wellsuited to language ID because they do not require tokenized input; tokenizers are often language-specific. A text classifier has been trained using the Leipzig Corpora Collection <ref type="bibr" coords="4,222.60,142.11,12.63,9.82" target="#b1">[2]</ref> with the aim to recognize up to 15 other than English.</p><p>The Leipzig Corpora Collection is a freely available resource for corpora and corpus statistics covering more than 20 languages at the time being. The corpora are identical in format and similar in size and content. They contain randomly selected sentences in the language of the corpus and are available in sizes of 100,000 sentences, 300,000 sentences, 1 million sentences etc.. By means of Lingpipe classifier we removed 541.725 permalinks (16.86% of all documents) but only 74 were relevant (0.61% of all relevant documents). On the other hand, we have eliminated 8.09% of false positive documents (documents retrieved but labeled 0). This proves that language classification is very useful in both topical and opinion finding retrieval. For the topic relevance retrieval we adopted Terrier <ref type="bibr" coords="4,386.75,291.51,11.59,9.82" target="#b4">[5]</ref>. Due to the large dimension of BLOG track collection we have developed a distributed version of Terrier, and we run our experiments on a cluster of 13 machines (1 broker + 12 nodes). From a software architecture perspective, we adopted a document partitioning strategy, and we solved the "results merging" problem providing global statistics to each server query. Thanks to the distributed version of Terrier we can index the entire collection in less than 45 minutes: this is very useful to tune the indexing Terrier parameters and to quickly test the effectiveness of pre-processing activities. The index that we used for BLOG TREC 2007 was generated indexing all permalink files except for those marked as not English document by the LingPipe classifier and using the weak Porter stemmer. As retrieval models we use the parametric model PL2, with its parameter c set to 9, and the parameter-free model DPH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Automatic construction of a sentimental dictionary with weighted terms</head><p>The automatic construction/weighting of a sentimental dictionary is a particularly ambitious objective: to automatically identify sentiment-bearing terms, and to automatically assign them an opinion weight.</p><p>To achieve these goals we learned from the set of relevant documents (labeled 1, 2, 3 or 4 from TREC 2006 relevance data) and the set of the opinionated ones (labeled 2, 3 or 4). Our hypothesis is that opinion-bearing words distribute more randomly in the set of opinionated documents than semantic-bearing terms, but less randomly than the non-informative terms. Starting from this idea, we used a Divergence From Randomness (DFR) query expansion model to filter several levels of candidate terms, and we selected the level that maximized the number of "opinion-bearing" words.</p><p>More precisely, we first compute the asymmetric Kullback-Leibler divergence (KL) in the opinionated set with respect to the set of relevant documents obtaining a set of candidate opinion-bearing terms. Then, we compute different layers of opinionated terms by document frequency, as a way to maximize the opinion entropy for all such candidate terms. A generic k level contains all index terms occurring in at least k relevant and opinionated documents. Therefore the higher the number of documents containing a term, the higher is the probability that the term is opinionated. Furthermore, the larger k is chosen, the less the terms that are selected.</p><p>Our goal was to find the optimal level k that maximize the number of "opinion-bearing" words. To evaluate a generic k level, we executed the following steps:</p><p>1. we reduce the noise caused by the presence of many proper names, numerical terms, dates, and typos words by excluding all the index terms that contain numerical substrings or that are not included in the WordNet <ref type="bibr" coords="5,467.76,323.19,12.75,9.82" target="#b3">[4]</ref> dictionary. 2. we use a sentimental dictionary SentV semi-manually built by third parts <ref type="bibr" coords="5,471.35,349.71,12.55,9.82" target="#b6">[7,</ref><ref type="bibr" coords="5,483.90,349.71,8.36,9.82" target="#b7">8]</ref> to assess the automatic dictionary by precision and recall with respect to the sentimental dictionary SentV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">we study the effectiveness of combining both dictionaries.</head><p>All runs presented in Section 5 have been performed using all (not filtered) index terms occurring at the 100-th level. We refer to this set of terms as learned dictionary. Note that the learned dictionary also contains terms that does not occur in the sentimental dictionary.</p><p>Each term in the learned dictionary has a learned sentimental weight coinciding with the weight returned by the query expansion model.</p><p>Table <ref type="table" coords="5,177.72,490.11,5.45,9.82">1</ref> shows an excerpt from the used learned dictionary of all terms classified as strong subjective in the sentimental dictionary SentV, and reports some statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topic opinionated relevance retrieval</head><p>In this section we show how we compute the final topic-opinionated relevance documents list. Let us assume to have an opinionated vocabulary, OpinV such that each term t ∈ OpinV has a weight w t . Opinionated and relevant document ranking is obtained in four steps: Table <ref type="table" coords="6,158.52,70.96,3.34,8.07">1</ref>. The excerpt is from the set of index terms extracted at level k = 100 with their opinionated scores. The size of the learned dictionary amounts to 4222 index terms. We reduced the noise caused by the presence of proper names, numerical terms, dates, and meaningless words removing 3505 index terms (45.36% of the index terms occurring at level k = 100). The number of "opinion-bearing" terms occurring both in the learned and in the sentimental dictionary SentV is 1529 (36% of the learned dictionary terms), with 945 terms classified as strong subjective and 584 terms classified weak subjective by the sentimental dictionary (61% and 39% of the terms in the intersection of the learned and the sentimental dictionaries, respectively).</p><p>An excerpt from the 945 strong subjective terms of the learned dictionary abide 0,0023 inaccurate 0,0064 . . . . . . abject 0,0031 inane 0,0009 wish 0.0060 absolute 0,0029 inappropriate 0,0028 wonder 0.0068 absurd 0,0076 incapable 0,0072 wonderful 0.0025 abusive 0,0047 incessant 0,0052 woo 0,0024 abyss 0,0008 inclin 0,0043 worri 0,0020 acclaim 0,0008 incoherent 0,0010 worse 0,0044 accuse 0,0012 incompetent 0,0012 worst 0,0041 activist 0,0023 incomprehensible 0,0018 worth 0,0018 actual 0,0069 inconvenient 0,0026 worthless 0,0097 admir 0,0024 incredible 0,0048 worthwhile 0,0016 admirable 0,0030 indefensible 0,0011 wound 0,0046 admire 0,0011 indicative 0,0017 wrath 0,0021 admit 0,0063 indifferent 0,0029 yeah 0,0070 . . . . . . indispensable 0,0033 yearn 0,0049 1. A content-only scored document list is obtained from Section 2. It means that each document has already associated a content score: we use the DFR models PL2 and DPH as retrieval functions to provide the content score of the documents content score(d||q) = score DF R (d||q).</p><p>We obtain a content rank for all documents:</p><p>content rank(d||q).</p><p>2. We submit all terms of OpinV as query-terms with their weights w t and assign a query-independent score to the set of retrieved documents by means of a DFR model:</p><formula xml:id="formula_0" coords="6,195.72,609.74,240.75,11.49">opinion score(d||OpinV) = score DF R (d||OpinV).</formula><p>The opinionated score with respect to a topic q is defined as follows:</p><p>opinion score(d||q) = opinion score(d||OpinV) content rank(d||q) .</p><p>We thus obtain a opinion rank for all documents:</p><p>opinion rank(d||q).</p><p>3. We further boost document ranking with the dual function of opinion score(d||q):</p><p>content score + (d||q) = content score(d||q) opinion rank(d||q) .</p><p>4. Finally, we re-rank the documents by content score + (d||q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head><p>Results of submitted runs are shown in Table <ref type="table" coords="7,333.41,292.35,4.06,9.82" target="#tab_0">2</ref>.</p><p>The run labeled as FIUbPL2 represents the baseline. It is computed adopting the PL2 model (C=9) with all opinion-finding features turned off. • The second run (FIUdPL2) is a second baseline based on FIUbPL2. It rearranges the topic-based rank with respect to the sentimental query, by using the algorithm described in Section 4 with DPH as model for the first opinion score. In this run we do not use a learned dictionary V k , for some level k, but only the strong subjective terms of the sentimental dictionary SentV |Strong (the use of the whole sentimental dictionary performs less). Each term in OpinV = SentV |Strong has a weight w t equal to 1. This run is useful to compare with the runs obtained by using a learned dictionary V k . It also provides us useful indications of the effectiveness of the re-ranking score function content score + (d||q). • The run labeled FIUlPL2, performed best in terms of MAP. It has been designed as described in Section 4, where the initial ranking is given by the baseline FIUbPL2 and DPH as model for the opinion score. In this run we use both learned dictionary V k , for k = 100, and SentV restricted to only the strong subjective terms. Each term in OpinV = SentV |Strong has a weight equal to 1 + αw t , where α is set to the value ∼ 1000 that normalizes the greatest opinionated term weight w t in V 100 to 1, and w t is added when the term is also in V 100 . • The run labeled as FIUlDPH differs from FIUlPL2 because we use the parameter free model DPH instead of PL2 in the step 1 of the algorithm described in Section 4.</p><p>• The run labeled as FIUlP22 differs from FIUlPL2 because we use PL2 in the step 2 of the algorithm described in Section 4. • Finally, FIUDDPH differs from FIUlDPH mainly because we conside both the title and the description topics fields in the step 1 of the algorithm described in Section 4. Furthermore, the content-only score is obtained applying the query expansion technique (we used a parameter free model of query expansion with 3 top ranked documents and 20 expansion terms).</p><p>All our official runs were evaluated by trec eval as they were baselines, because we updated the final ranks but not the final topical-opinion scores. Using the Terrier evaluation tool, which instead evaluates runs by ranks and not by scores, our best title-only run is FIUIDPH for precision at 10 (topic 0.7160, opinion 0.5360 with +13.7% and +18% respectively over the baseline FIUbPL2), FIUIPL2 for MAP (topic 0.3910, opinion 0.3210 with +9.3% and +17.7%) and for R precision (topic 0.4264, opinion 0.3679 with +6.6% and +14.8%). Our actual results are reported in Table <ref type="table" coords="8,348.84,277.11,4.06,9.82" target="#tab_0">2</ref>. It is worth to note that, with respect to the results presented in the "Overview of the TREC2007 Blog Track" paper <ref type="bibr" coords="8,161.88,304.23,11.50,9.82" target="#b8">[9]</ref>, the FIUIPL2 is the best run in terms of improvements over the baseline, and is the fourth run among all the automatic title-only runs submitted to the opinion track. Considering the difference from median average precision per topic of the FIUlPL2 run (see Figure <ref type="figure" coords="8,245.13,541.47,3.91,9.82" target="#fig_0">1</ref>), we notice that only 10 topics are under the median and that we could improve the performance of most of them adopting some proximity strategy.</p><p>Moreover, Table <ref type="table" coords="8,225.42,583.35,5.45,9.82" target="#tab_1">3</ref> summarizes some new automatic runs performed as variations of the FIUlPL2 run. Comparing Tables <ref type="table" coords="8,332.60,596.91,5.45,9.82" target="#tab_0">2</ref> and<ref type="table" coords="8,358.40,596.91,5.45,9.82" target="#tab_1">3</ref> we can draw the following conclusions: weights of V100 title 0.3948 0.6920 0.3166 0.5320 1. Semi-manually built dictionaries can be successfully used in opinion finding retrieval. However, the fully automatic dictionary performs better than the semi-manual one (compare the submitted run FIUdPL2 to the new run of Table <ref type="table" coords="9,178.44,462.15,5.45,9.82" target="#tab_1">3</ref> labeled by V 100 ). 2. Different combinations of semi-manually built dictionaries with our fully automatic dictionary improves topical opinion retrieval. 3. Automatic weighting of opinionated terms by DFR improves topical opinion retrieval. 4. It is not the exhaustively of the semi-manual dictionary but its quality that improves performance. Using the intersection SentV |Strong ∩ V 100 of the semi-manual dictionary containing all terms labeled as strong subjective with the automatic one, and weighting the opinionated terms as in the run FIUlPL2, we obtain the best combination strategy. 5. The parameter free model DPH provides the best precision at 10.</p><p>We have shown a very simple opinion retrieval model free from parameters to rerank the set of retrieved documents. The opinion retrieval function requires the computation of an absolute or query-independent opinion rank of the retrieved documents. To obtain this sentimental score we have learned a dictionary of opinion-bearing words from blog track data of TREC 2006. The technique to extract and weight terms is that used in query expansion. We have then submitted such a sentimental dictionary as a query. However, dictionaries may contain many thousand of words, and we have thus further filtered the words obtaining smaller and smaller sentimental dictionaries. It is possible to achieve as good performance as we have achieved with the official runs with a very restricted number of opinion-bearing terms, but this issue is not fully presented here and will be studied in the next participation to the blog track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,134.76,240.64,345.66,8.07;9,134.76,251.56,115.80,8.07;9,136.68,258.84,342.00,131.32"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Difference from median average precision per topic of the FIUlPL2 run. The 10 topics under the median are also listed.</figDesc><graphic coords="9,136.68,258.84,342.00,131.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,158.04,366.52,299.20,119.67"><head>Table 2 .</head><label>2</label><figDesc>Summary of the FIU Blog track automatic runs.</figDesc><table coords="8,357.00,388.48,88.96,8.07"><row><cell>Relevance</cell><cell>Opinion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,143.04,70.84,329.20,128.12"><head>Table 3 .</head><label>3</label><figDesc>Summary of new automatic runs. Variations of the FIUIPL2 run</figDesc><table coords="9,372.00,92.80,88.96,8.07"><row><cell>Relevance</cell><cell>Opinion</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,145.68,600.88,334.59,8.07;1,145.68,611.80,98.26,8.07"><p>Permalinks are URL links of blogging entries that exist even after they pass from the front page into the blog archives.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.10,304.96,274.18,8.07" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,175.32,304.96,103.18,8.07">Lingpipe named entity tagger</title>
		<author>
			<persName coords=""><surname>Alias-I</surname></persName>
		</author>
		<ptr target="http://www.alias-i.com/lingpipe/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,316.00,342.16,8.07;10,147.00,326.92,333.70,8.07;10,147.00,337.84,20.00,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,339.72,316.00,140.54,8.07;10,147.00,326.92,110.31,8.07">The Leipzig corpora collection -monolingual corpora of standard size</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Quasthoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,272.64,326.92,119.49,8.07">Proceedings of Corpus Linguistic</title>
		<meeting>Corpus Linguistic<address><addrLine>Birmingham, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,348.88,342.42,8.07;10,147.00,359.80,333.48,8.07;10,147.00,370.72,55.52,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,257.85,348.88,222.67,8.07;10,147.00,359.80,49.11,8.07">The trec blogs06 collection : Creating and analyzing a blog test collection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,203.76,359.80,71.46,8.07">Dcs technical report</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science of the University of Glasgow</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,381.76,342.59,8.07;10,147.00,392.68,333.29,8.07;10,147.00,403.60,154.78,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,215.97,381.76,141.68,8.07">Wordnet: a lexical database for English</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,376.32,381.76,104.37,8.07;10,147.00,392.68,159.35,8.07">HLT &apos;91: Proceedings of the workshop on Speech and Natural Language</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="483" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,414.64,342.18,8.07;10,147.00,425.56,333.60,8.07;10,147.00,436.48,247.52,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,422.88,414.64,57.41,8.07;10,147.00,425.56,205.28,8.07">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,369.36,425.56,111.25,8.07;10,147.00,436.48,221.21,8.07">Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)</title>
		<meeting>ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,447.52,342.28,8.07;10,147.00,458.44,333.57,8.07;10,147.00,469.36,160.10,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,400.67,447.52,79.71,8.07;10,147.00,458.44,55.45,8.07">Overview of the trec-2006 blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,229.74,458.44,164.27,8.07">Proceedings of the Text REtrieval Conference</title>
		<meeting>the Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,480.28,342.28,8.07;10,147.00,491.32,182.84,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,288.33,480.28,192.05,8.07;10,147.00,491.32,47.75,8.07">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,201.48,491.32,101.36,8.07">Proceedings of HLT-EMNLP</title>
		<meeting>HLT-EMNLP</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,502.24,342.32,8.07;10,147.00,513.16,202.88,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,340.69,502.24,139.73,8.07;10,147.00,513.16,24.54,8.07">Recognizing strong and weak opinion clauses</title>
		<author>
			<persName coords=""><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,178.32,513.16,96.99,8.07">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.10,524.20,342.48,8.07;10,147.00,535.12,333.42,8.07;10,147.00,546.04,81.20,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,306.02,524.20,149.24,8.07">Overview of the TREC2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,147.00,535.12,165.71,8.07">Proceedings of the Text REtrieval Conference</title>
		<meeting>the Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
