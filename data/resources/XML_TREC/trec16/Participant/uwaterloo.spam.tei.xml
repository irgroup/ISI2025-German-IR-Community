<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,70.44,63.63,471.47,16.20;1,282.84,84.51,46.73,16.20">University of Waterloo Participation in the TREC 2007 Spam Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.04,120.27,151.56,16.20"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
						</author>
						<title level="a" type="main" coord="1,70.44,63.63,471.47,16.20;1,282.84,84.51,46.73,16.20">University of Waterloo Participation in the TREC 2007 Spam Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76ECD78DA70F7279E94C22615A598FAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the first year that we have submitted participant runs to TREC (Cormack is track coordinator). In parallel with running the track, we have investigated two new filtering methods, inspired by the excellent results that others have shown in the task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Markov Compression (DMC) modeling (wat2 prefix).</head><p>We invented the DMC model some 20 years ago for the purpose of data compression. Bratko showed at TREC 2005 that compression models could work well, and we found that DMC could work even better than the PPM and CWT models used by Bratko. On previous TREC tasks, DMC has been competitive with the best <ref type="bibr" coords="1,150.84,296.68,177.58,10.80">(Bratko et al, JMLR December 2006)</ref>, and one of our runs (wat2 prefix) implements this method. Logistic Regression on character 4-grams. (wat1 prefix). We and others <ref type="bibr" coords="1,448.92,325.48,77.16,10.80;1,86.76,339.64,49.90,10.80">(Cormack et al., SIGIR 07;</ref><ref type="bibr" coords="1,139.56,339.64,103.36,10.80">Sculley, SIGIR 2007)</ref> have observed that character 4-grams work much better than bags of words, escpecially if only the first 3k or so characters are considered.</p><p>We use simple on-line graduate ascent implementation of logistic regression (Goodman and Yih, CEAS 2006) based on 4-grams. While our tests show that this approach is perhaps not quite as good as Sculley's on-line SVM, it is much simpler and faster, being implemented in its entirety in about 100 lines of C code. It also offers the potential advantage that its output is a probability (actually log-odds) which makes it easier to interpret and calibrate when considered as a component of a system to filter spam. SVM output, in contrast has only a geometric interpretation which cannot directly be mapped to a probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On-line fusion of DMC and Logistic regression. (wat3 prefix)</head><p>We used on-line logistic regression to fuse the results of our wat1 and wat2 methods giving a very fast algorithm that appears to give better results than any previously used at TREC. It remains to be seen whether this method beats Sculley's ROLSVM (other than in speed, where it prevails hands down.) Threshold-based active learning (wat4 prefix). We adapted our logistic regression filter (wat1 prefix) to train only on messages for which the absolute value of the score was less than some threshold. The TREC 2006 has used to calibrate the threshold so as to approximate the best performance for various quota values.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
