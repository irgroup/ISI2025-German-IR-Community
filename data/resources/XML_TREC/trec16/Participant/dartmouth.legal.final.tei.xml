<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,192.96,74.68,225.84,10.80">Dartmouth College at TREC 2007 Legal Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.44,102.28,78.24,10.80"><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Dartmouth College</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.96,102.28,75.84,10.80"><forename type="first">Paul</forename><surname>Thompson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Dartmouth College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,192.96,74.68,225.84,10.80">Dartmouth College at TREC 2007 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F0522C6C8EAF53B5F0015673D1EF8B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes Dartmouth College's approach and results for the 2007 TREC Legal Track. Our original plan was to use the Combination of Expert Opinion (CEO) algorithm [1], to combine the search results from several search engines. However, we did not have enough time to build the index for more than one search engine by the time for submission for official runs. The official results described here are based only on the Lemur / Indri [2] search engine.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>This year, all Legal Track participants were required to submit at least one automatic run in which only the RequestText field was used with no human intervention. In other runs any fields could be used, e.g., processing the Boolean query sentences to find synonyms of keywords or parsing the complaints text to find more context of the request.</p><p>Our plan was to use the CEO algorithm <ref type="bibr" coords="1,273.84,364.06,12.80,9.94" target="#b0">[1]</ref> to combine the results returned from several search engines. This algorithm uses a probability model to compute a final score for a document and then sorts the retrieved documents according to this score. This algorithm can also receive relevance feedback to train the weights given to different search engines. In our experiments to date we have not used this feature due to lack of time. Our official results are only based on using the Indri <ref type="bibr" coords="1,130.32,427.42,12.80,9.94" target="#b1">[2]</ref> search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steps</head><p>After downloading the xml files from the TREC ftp site, we used a program written by the University of Massachusetts to convert the whole data set from xml to TREC format. Since the Lemur toolkit supports TREC documents, we ran the program provided along with the toolkit to build the index for the Indri <ref type="bibr" coords="1,214.56,517.18,12.80,9.94" target="#b1">[2]</ref> search engine.</p><p>Another search engine we planned to use was Lucene <ref type="bibr" coords="1,343.68,542.38,11.61,9.94" target="#b2">[3]</ref>, an open source project in Apache. Unfortunately, it does not support TREC format. Therefore a simple parser was written to extract documents which were then passed to Lucene <ref type="bibr" coords="1,295.44,567.58,11.61,9.94" target="#b2">[3]</ref>. In addition to the DOCNO and the content text of a document, other meaningful fields such as titles, authors, document type, and organization were extracted. Others fields that use numbers such as page count were not indexed.</p><p>The CEO algorithm expects scores to be combined to be probabilities. Because the scores returned by Indri <ref type="bibr" coords="1,170.88,630.94,12.80,9.94" target="#b1">[2]</ref> are the logarithms of probabilities of relevance, we normalize them to be within the range of zero and one. By passing document IDs and normalized scores as arguments, the function returns a list of documents sorted by the final score.</p><p>Our results are all automatic runs. The first run, which is also our only official run, is the result returned by the Indri <ref type="bibr" coords="2,188.64,113.50,12.80,9.94" target="#b1">[2]</ref> search engine. The second run is the unofficial result returned by the Lucene <ref type="bibr" coords="2,126.48,126.22,12.80,9.94" target="#b2">[3]</ref> search engine, while the third one combines the previous two runs using the CEO algorithm.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall-Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>As a default the CEO algorithm gives equal weight to retrieval engines being combined to produce its final probability of relevance for a document. Since no training was done, this default mode of combination was used. As shown in the tables recall at 5 and 10 documents was slightly better with the combined run than with either search engine alone. In further experiments we plan to use relevance judgments from the relevance feedback task to train the algorithm. We expect this training to lead to better performance of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>From just passing the whole request text as the query this year, we plan to further process the request in the future. Through syntactic analysis we hope to find keywords in the sentence automatically and then to use query expansion to produce our final query.</p><p>We also plan to combine the results of more search engines in the future. Recall and precision might be increased if more relevant documents are retrieved, given the use of more search engines. We also plan to make more use of the CEO algorithm's relevance feedback capability.</p><p>Through training the algorithm can assign more accurate weights to the engines being combined, which should lead to the final combined ranking being more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We wish to thank Afra Zomorodian for making a computer with a large memory available for this project. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,92.64,369.27,181.20,9.07;2,332.16,369.27,193.20,9.07"><head></head><label></label><figDesc>Figure 1.a: Recall-Precision Graph (Indri) Figure 1.b: Document Level Averages (Indri)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,130.32,313.83,180.24,9.07;3,332.88,313.83,192.48,9.07"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.a: Recall-Precision Graph (CEO) Figure 2.b: Document Level Averages (CEO)</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,106.80,100.78,415.44,9.94;4,90.00,113.50,378.24,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,190.57,100.78,331.67,9.94;4,90.00,113.50,40.37,9.94">A Combination of Expert Opinion Approach to Probabilistic Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,142.08,113.50,173.95,9.94">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="371" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.60,126.22,197.95,9.94" xml:id="b1">
	<monogr>
		<ptr target="http://www.lemurproject.org/" />
		<title level="m" coord="4,105.60,126.22,61.66,9.94">Lemur Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.60,138.70,216.67,9.94" xml:id="b2">
	<monogr>
		<ptr target="http://lucene.apache.org/" />
		<title level="m" coord="4,105.60,138.70,100.99,9.94">Apache Lucene Project</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
