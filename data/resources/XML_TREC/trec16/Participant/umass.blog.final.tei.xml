<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.84,154.65,309.88,15.55">UMass at TREC 2007 Blog Distillation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,219.24,186.94,66.23,10.99"><forename type="first">Jangwon</forename><surname>Seo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.14,186.94,79.79,10.99"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.84,154.65,309.88,15.55">UMass at TREC 2007 Blog Distillation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D9DEEC91CFF61C032214F0D144AB392</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The focus of the blog distillation task is finding blogs with a principle, recurring interest in a specific topic. For this task, we considered a blog as a collection of postings and used resource selection approaches. Further, we investigated techniques that penalized general blogs and combined resource selection techniques. This combination demonstrated significant improvements over baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We participated in the blog distillation task in the blog track in TREC 2007. This task was to find relevant blogs for any specific topic. A blog can be considered as a collection composed of its own postings. From this point of view, our intuition was that finding relevant blogs is similar to finding relevant collections in a distributed search environment. Therefore, we employed resource selection techniques normally used for distributed information retrieval. Further, in order to take advantage of the topical characteristics of blogs, we suggested a supplementary factor to penalize general blogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Processing</head><p>Although this task is often referred as the feed distillation task, directly using the feed collection might not be effective. RSS, the subscription method that is currently most prevalent, does not have any requirement that feeds have to have a summary of the corresponding posting content, contrary to ATOM.</p><p>In many cases, a RSS feed has no other meaningful information than the posted date and the title. Moreover, the title in the feed is sometimes not the title of the corresponding posting but the title of the blog. In this case, we cannot infer what the blog or postings are about from the feed. Therefore, we decided not to use the feed collection for now. Of course, exploiting feeds is still possible to augment retrieval performance later.</p><p>Consequently, our target collection was the permalink collection. This collection contains a considerable amount of splogs and non-english blogs which are intentionally added. We did not get rid of this "noise" from the permalink collection because we expected that our techniques should be robust enough to deal with it. Instead, we removed only HTML tags in each posting. We used only blogs which have valid blog IDs (BLOGHPNO). Further, we used the blog IDs instead of feed IDs (FEEDNO) as keys to identify what blog each posting belongs to because a blog might have more than one feed link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Resource Selection Techniques</head><p>Given that a blog is a collection of postings, finding relevant blogs is similar to resource selection for finding relevant collections. In that sense, using resource selection approaches for this task looks desirable. A variety of resource selection techniques have been explored in distributed information retrieval. Here, we introduce three techniques for blog distillation task. Note that all described techniques are based on language modeling techniques <ref type="bibr" coords="1,430.45,665.25,10.00,9.96" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline: Global Representation</head><p>We can handle a blog as a document. That is, all postings are concatenated into a virtual document.</p><p>After that, we can build a language model from each virtual document. This is one of the simplest and most widely used methods <ref type="bibr" coords="2,190.56,193.53,10.57,9.96" target="#b0">[1,</ref><ref type="bibr" coords="2,204.48,193.53,6.97,9.96" target="#b3">4]</ref>.</p><p>A ranking function for Global Representation is the same as query likelihood:</p><formula xml:id="formula_0" coords="2,135.72,235.89,101.20,10.33">φ GR (Q, c i ) = P (Q|D ci )</formula><p>where Q and D ci are a query and a virtual document with a blog id c i , respectively.</p><p>However, this technique has a critical weakness. If a posting is longer than the others, then this model can be biased by the long posting regardless of its relevance.</p><p>We refer to this method as "Global Representation" and use it as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo Cluster Selection</head><p>Clustering is an effective approach for distributed information retrieval <ref type="bibr" coords="2,159.62,395.73,9.91,9.96" target="#b4">[5]</ref>. That is why a topic-based document set from collections which are not generally topic-centric can be gathered by clustering. But, although there are quite a few exceptions, a blog typically addresses a small number of topics. By exploiting this property, we propose how to construct a pseudo clustering without actually clustering documents. We can get a ranked list by searching a posting collection index with a topic. We assume that the highly ranked documents from the same blog address similar topics. That is, we can consider a set of these documents as a pseudo-cluster. To retrieve such pseudo-clusters, we use a new cluster representation method introduced by Liu and Croft <ref type="bibr" coords="2,260.82,551.13,9.91,9.96" target="#b2">[3]</ref>. They showed that a cluster representation using a geometric mean of language models can be a good alternative to other representations. Our ranking function is formed as follows.</p><formula xml:id="formula_1" coords="2,118.56,614.90,133.47,30.45">φ P CS (Q, c i ) = ( K j=1 P (Q|d ij )) 1 K</formula><p>This method uses a fixed parameter K independent of clusters. However, it is possible that each blog does not have enough documents equal to or greater than K from the blog in the top N ranked list. To compensate for the original method, we estimate the upper bound of the geometric mean using the minimum query likelihood score in the list as follows.</p><formula xml:id="formula_2" coords="2,372.72,200.25,104.44,17.30">d min = arg min dij P (Q|d ij )</formula><p>With this value, we can compensate our ranking function as follows.</p><formula xml:id="formula_3" coords="2,322.80,259.18,202.23,30.48">φ P CS (Q, c i ) = (P (Q|d min ) K-ñi ñi j=1 P (Q|d ij )) 1 K</formula><p>We call this method "Pseudo Cluster Selection".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combination of Global Representation and Pseudo Cluster Selection</head><p>In blog search, we cannot say that blogs which have more relevant postings are necessarily more relevant than other blogs which do not. For example, blog A is daily updated by adding one or two new postings. About 60% of them are relevant. We may decide that the blog is relevant. On the other hand, blog B is hourly updated by adding hundreds of postings. About 5% of them are relevant. It is likely that blog B has more relevant documents than blog A. But, is blog B more relevant? To simplify this case, let's recall the goal of the blog (feed) distillation task. The task can be interpreted as finding blogs where we can get relevant information when we subscribe to feeds from the blog. In this sense, is blog B still relevant? If we subscribe blog B, then a great number of feeds will be delivered. We have to struggle to find a few relevant documents among them. After all, blog B seems irrelevant in this task. Therefore, we need to penalize blogs which address diverse topics. We use the Pseudo Cluster Selection score for the base score and the Global Representation score for the penalizing factor, and we multiply them as follows.</p><formula xml:id="formula_4" coords="2,350.16,662.61,150.88,12.97">φ(Q, c i ) = φ P CS (Q, c i ) • φ GR (Q, c i )</formula><p>If a blog is not topic-centric, then its virtual document in the global representation has word distributions which are not concentrated on any specific topic keywords but widely scattered. Further, the global representation is originally designed for estimating relevance. Therefore, the global representation can be an appropriate factor to reflect the topic-centric characteristic and relevance of the blog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We used the Indri 1 search engine as our foundation for experiments. We built two indexes. We concatenated documents with the same blog ID (BLOGHPNO) in the permalink collection into a virtual document and made a new collection with them. The first index was built on this collection for Global Representation. The second index is for Pseudo Cluster Selection and was built on the permalink collection. We used the Krovetz stemmer and the standard stopwords for all indexes. To implement the above mentioned techniques, we post-processed the initial search results from Indri and converted Blog IDs (BLOGHPNO) in the search result to Feed IDs (FEEDNO).</p><p>We performed four runs. For three of these runs, we used only titles of topics as queries. The three runs were for Global Representation, Pseudo Cluster Selection and the combination thereof, respectively. For the fourth run, we used both titles and descriptions as queries. We applied the combination of Global Representation and Pseudo Cluster Selection to this run.</p><p>Our systems have some parameters. Global Representation has a Dirichlet smoothing parameter for the language models, i.e. µ. Pseudo Cluster Selection has two parameters, i.e., K and mu. To learn the parameters, we used relevance judgments which were made by ourselves. The relevance judgment contains about 2500 judgments for 50 topics. The topics were chosen from topics of the TREC 2003 web distillation task and the TREC 2004 web distillation task. We performed 10-fold cross validation by randomly partitioning the training data. The evaluation measure for the training was the mean average precision (MAP). Summary of the submitted runs. UMaTiGR, UMaTiPCS and UMaTiPCSwGR are referred to the title-only runs using Global Representation, Pseudo Cluster Selection and the combination thereof, respectively. UMaTDPCSwGR is referred to the run by the combination of Pseudo Cluster Selection and Global Representation with the titles and the description. We compared the results by using mean average precision (MAP) and precision at 10 (P@10). We performed the paired t-tests with pvalue &lt; 0.05. The differences between all pairs are statistically significant except the difference between UMaTiGR and UMaTiPCS for P@10.</p><p>Finally, we used the mean of learned parameters for each partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The results from our runs are given in Table <ref type="table" coords="3,511.35,440.13,3.90,9.96" target="#tab_0">1</ref>. For title-only runs, the combination of Global Representation and Pseudo Cluster Selection (UMaTiPC-SwGR) outperformed others as we expected. It is somewhat surprising that the simple and naive Global Representation (UMaTiGR) shows the better performance than does Pseudo Cluster Representation (UMaTiPCS). The run using the titles and the descriptions (UMaTDPCSwGR) achieved the best performance of our runs. This shows that descriptions are helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Post-submission Experiment</head><p>We did another experiment using the collection preprocessed in a different way as a post-submission experiment. We used a feed ID (FEEDNO) instead of a blog ID (BLOGHPNO) as a key to identify which blog each posting belongs to because we found that some Run MAP P@10 UMaTiGR 0.3454 0.4889 UMaTiPCS 0.3155 0.4600 UMaTiPCSwGR 0.3725 0.5356 UMaTDPCSwGR 0.4051 0.5733 Table <ref type="table" coords="4,102.12,196.17,3.90,9.96">2</ref>: Summary of the post submission runs. UMaTiGR, UMaTiPCS and UMaTiPCSwGR are referred to the title-only runs using Global Representation, Pseudo Cluster Selection and the combination thereof, respectively. UMaTDPCSwGR is referred to the run by the combination of Pseudo Cluster Selection and Global Representation with the titles and the description. We compared the results by using mean average precision (MAP) and precision at 10 (P@10). We performed the paired t-tests with pvalue &lt; 0.05. The differences between all pairs are statistically significant except the difference between UMaTiGR and UMaTiPCS for P@10. relevant documents in the relevance judgment set for TREC 2007 Blog Distillation Task do not have valid blog IDs. Accordingly, the collection for Global Representation was newly created by concatenating postings with the same feed ID. Table <ref type="table" coords="4,220.44,417.69,4.98,9.96">2</ref> shows the result.</p><p>The post-submission runs show better performance than our submitted runs using the only documents with valid blog IDs. It shows that runs submitted by many other groups, which contributed to the relevance judgment pool, contain many documents with invalid Blog IDs. Nevertheless, the post-submission result shows the same aspect as the submitted result. Global Representation (UMaTiGR) is better than Pseudo Cluster Representation (UMaTiPCS) and the combination of Global Representation and Pseudo Cluster Selection (UMaTiPCSwGR / UMaT-DPCSwGR) is still the most effective method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We applied resource selection techniques to this task and showed that they work well. Further, we showed that the effectiveness can be increased by using an advanced technique, which combines the features in order to penalize diverse blogs. Therefore, we con-clude that resource selection techniques can be a good approach to this task; accordingly, we plan to explore more advanced resource selection techniques later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This work was supported in part by the Center for Intelligent Information Retrieval and in part by NHN Corp. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,83.04,126.09,422.59,548.62"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="3,83.04,665.98,138.46,8.73"><row><cell>1 http://www.lemurproject.org/indri/</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,326.16,320.49,213.29,9.96;4,326.16,332.49,212.97,9.96;4,326.16,344.37,213.10,9.96;4,326.16,356.37,162.64,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,392.53,320.49,143.06,9.96">Distributed information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,444.72,332.83,94.41,9.18;4,326.16,344.71,58.39,9.18">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="127" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.16,376.29,213.01,9.96;4,326.16,388.29,213.08,9.96;4,326.16,400.17,191.43,9.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,498.60,376.63,40.58,9.18;4,326.16,388.63,150.55,9.18">Language Modeling for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.16,420.09,213.24,9.96;4,326.16,432.09,213.29,9.96;4,326.16,444.09,213.23,9.96;4,326.16,455.97,76.20,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,492.40,420.09,47.00,9.96;4,326.16,432.09,213.29,9.96;4,326.16,444.09,55.20,9.96">Evaluating text representations for retrieving the best group of documents</title>
		<author>
			<persName coords=""><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">CIIR Technical Report</note>
</biblStruct>

<biblStruct coords="4,326.16,475.89,213.28,9.96;4,326.16,487.89,213.10,9.96;4,326.16,500.23,213.06,9.18;4,326.16,512.11,212.95,9.18;4,326.16,523.77,203.65,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,461.19,475.89,78.25,9.96;4,326.16,487.89,109.45,9.96">Effective retrieval of distributed collections</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,462.84,488.23,76.42,9.18;4,326.16,500.23,213.06,9.18;4,326.16,512.11,212.95,9.18;4,326.16,524.11,105.07,9.18">SIGIR &apos;98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.16,543.69,213.29,9.96;4,326.16,555.69,213.18,9.96;4,326.16,567.57,212.96,9.96;4,326.16,579.57,213.09,9.96;4,326.16,591.45,144.87,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,487.48,543.69,51.97,9.96;4,326.16,555.69,187.34,9.96">Topic-based language models for distributed retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,431.52,567.91,107.60,9.18;4,326.16,579.91,37.27,9.18">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="151" to="172" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
