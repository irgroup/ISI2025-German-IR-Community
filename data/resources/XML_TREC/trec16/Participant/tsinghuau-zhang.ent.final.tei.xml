<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,179.58,76.31,243.40,12.58;1,423.60,78.76,3.00,5.40">THUIR at TREC2007: Enterprise Track 1</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,157.80,99.90,42.88,9.02"><forename type="first">Yupeng</forename><surname>Fu</surname></persName>
							<email>yupeng.fu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.52,99.90,39.69,9.02"><forename type="first">Yufei</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.55,99.90,37.56,9.02"><forename type="first">Tong</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.25,99.90,39.81,9.02"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.70,99.90,42.54,9.02"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.32,99.90,53.62,9.02"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,179.58,76.31,243.40,12.58;1,423.60,78.76,3.00,5.40">THUIR at TREC2007: Enterprise Track 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">572461034D63C47F76AE6E30E2E977E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participate in document search and expert search of Enterprise Track in TREC2007. The motive behind the TREC Enterprise Track is to study the issues searching the documents and experts inside an enterprise environment, which has not been sufficiently addressed in research. In document search, we focus on the key overview page pre-selection methods and link analysis algorithms. In expert search, we develop methods to detect expert identifiers and experimented based on our previous PDD model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the third year that the IR groups of Tsinghua University participated in TREC Enterprise Track. Different from previous tracks, TREC introduced a new enterprise corpus and new tasks. The approaches we've studied this year include link analysis among documents, person entity identification, topic distillation with key resource pre-selection, results combination and some other technologies.</p><p>For document search task, we mainly investigate the effects of key source pre-selection and link analysis among the documents. We first observe the high quality resource distribution. Some features are studied to find overview pages. We also do some link analysis: both HITS and PageRank algorithms are employed to evaluate the page quality. Besides, we attempted a novel link analysis method which involved the document similarity.</p><p>For expert finding task, a lot of efforts have been made on name identification. We built personal description documents (PDD) for each candidate from various types of resources. We obtain retrieved results from each description document collection. And with the help of EM algorithm we combine the results from different corpus to generate a merged ranking list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Search</head><p>The task of document search is defined to retrieve those documents which help the science communicator create an overview page in the given topic area. These will tend to be authoritative pages such as project homepages and documents dedicated to the topic, rather than pages that make passing mention of the topic. There are two potential approaches to find those "overview page". The first one is to build a query independent classifier that selects those documents with specific features to be required key pages. The other one is to adapt link analysis to predict those authoritative pages. Both approaches were attempted in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Key pages pre-selection using query independent features</head><p>To retrieval key pages, we can first retrieval query relevant pages from the whole corpus then mine those overview pages. However, the volume of the whole corpus is large that may take much time to retrieve. Therefore we try to do some data cleansing work to pick those key pages out according to some query independent features. From the example pages provided by NIST and some other overview pages browsed we noticed that the overview pages are under similar templates: many out-links, many internal links and similar design of layout. However, we believe that recognizing overview exactly according to the template of layout is not robust. So we tried to build a classifier to select overview pages using features like amount of out-links and in-links.</p><p>Figure <ref type="figure" coords="2,112.69,91.95,5.25,9.45">1</ref> shows that the distribution of the amount of out-links in overview pages provided by TREC. From the distribution we notice that most overview pages contain a lot of out-links from 115 to 180, contrast to the fact that among the whole corpus more than ninety percent of pages are with out-links less than 100. We conduct an experiment that only retrieve from those documents which have number of out-links more than 100. We use the example overview pages as relevant pages. The results show that the performance is 10% higher than retrieving from the whole corpus while the size of the selected corpus is only 10% of the whole one.</p><p>Figure <ref type="figure" coords="2,180.05,341.55,3.95,9.45">1</ref>. The distribution of the amount of out-links in example pages Similarly, we make the hypothesis that the overview pages are those authoritative ones which may be linked by hub pages. Therefore the in-link anchor text may provide much useful navigational information. The experiments results validate our supposition that the anchor text based retrieval outperform the full text retrieval by more than 20% in MAP.</p><p>Some other features are also attempted to identify overview pages, such the length of page URLs, the amount of in-link. We also implement a decision tree to combine these features to better construct the overview page corpus. Finally, the performance of the results retrieved from the document set after distillation achieved 35% improvement than from the full text collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adapting link analysis for finding authoritative pages</head><p>While authority of pages on the Internet has been an active area of research, estimating authority of web pages inside an enterprise such as CSIRO is an open question. Since the difference of the amount of web pages in corpus and the link structure, whether the previous link analysis algorithms such as HITS and Page Rank is effective is need to explore.</p><p>We first used Page Rank <ref type="bibr" coords="2,192.92,571.95,12.23,9.45" target="#b0">[1]</ref> to estimate the quality of the web pages. However, the link structure of the web site CSIRO is quite different from the environment that the algorithm applies to. So after executing the Page Rank process, we get some pages with very large scores like the homepage and index pages. However, it is difficult to use these importance scores in conjunction with query-specific IR scores to rank the query results. Several known approaches are attempted but all failed to improve the performance.</p><p>Another famous link analysis algorithm we tried is HITS <ref type="bibr" coords="2,326.50,649.95,11.19,9.45" target="#b1">[2]</ref>. This idea has an intuitive parallel for finding overview pages. Although the pages with high authority scores are those overview pages, it's hard to combine these scores with content based scores. We tried to use EM algorithms <ref type="bibr" coords="2,375.11,681.15,12.27,9.45" target="#b2">[3]</ref> to aggregate them through linear combination but the improvement is trivial.</p><p>The overview pages we want to find more or less are both good hubs and authoritative pages, which means the required overview pages link to important pages and other good pages also links to them. Therefore for a given page, we add the similarities of those pages which the link to the page as the new score. This reflects the idea that considering the similarity of pages instead of merely analyzing the link relation. For example, if a hub points to an authoritative page and the hub page is a good one, then the authoritative may get a lot of reinforcement. However, if only a little part of the hub page is relevant to the page it links to, the authoritative page may get too much. Plus the strength of the link relation is more or less reflected by the anchor text. Therefore involving the similarity of anchor text to the algorithm may better quantitatively determine the strength of reinforcement. The experiments results show that the improvement achieved by the content-based link analysis is consistent and significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation results</head><p>In this year document search task, we used the examples pages as relevant results to train our systems. In total we submitted 4 runs which are listed in table <ref type="table" coords="3,252.02,213.15,3.95,9.45">1</ref> However, the performance results are much out of our anticipation. The reasons lies in that first, the documents retrieved from sub-collection are much less than those retrieved from whole collection. So in the pooling process, there may be some documents our runs do not cover; second, the training set we used consists of example pages from topics, which may cause over-fit due to its small scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Expert Search</head><p>There are some differences from previous Expert Search Task. One main difference is that there is no master list of candidates. So our system should automatically detect and find expert identifiers, such as email address, names. We make a lot of efforts on person name entity recognition this year. Another important difference is that the judgments are more accurate than judgments made in previous years. The set of relevant experts is small because users only want to see the main contacts for their query, not a list of 10+ knowledgeable people. This setting quite makes sense. It requires us to design strategies to better rank candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expert identifiers detection</head><p>Because there is no master list of candidates, we should automatically detect expert identifiers. According to the guideline that among CSIRO the pattern of the emails is firstname.lastname@csiro.au, we extract all the email addresses from the corpus, including some variation such as that the @ symbol may be HTML-encoded. In total we get 3170 addresses. After eliminating the typo in name and name variations in addresses, we got 3131 candidate names from the email addresses.</p><p>There are some variations for English names. Given a name "firstname.lastname", at least five variations are possible: Firstname Lastname, Firstname.Lastname, Firstname Middlename Lastname, F. Lastname, F. M. Lastname. For the first three pattern of variation, we use Aho-Corasick algorithm to label. It takes O(m+n) time, where m is the length of the name and n is the length of the document. Labeling F.Lastname is a problem that a bit hard to tackle. Because it is possible that more than one person shares one abbreviation. For example, T. Thomas may represent Tom Thomas or Tim Thomas. Among the 3131 candidates, there are 162 ambiguous abbreviations. We tried to eliminate the ambiguity according the co-occurrence of other labeled names. However, there are still some ambiguous abbreviations hard to eliminate. Some other technologies such as pronouns eliminating are also integrated in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing PDD and merging results</head><p>As we did in previous expert finding task, we build person description document (PDD) for each candidate <ref type="bibr" coords="4,518.89,97.95,11.17,9.45" target="#b3">[4]</ref>. We extract some candidate relevant information from the document as expertise, for example, the context around the expert identifier.</p><p>Besides, we also extract information from candidate's homepages. In total we find 477 homepages, which are about 15.2% of the amount of candidates. We name the PDD constructed from the homepages as detailed PDD (DPDD). There are two other collections of PDDs we built. One is from the anchor text while the other is from the key overview pages corpus as described in document search section.</p><p>To merge the ranking results retrieved from each PDD collection, we tried EM algorithm to assign the weight to each ranking similarity. When the ratio of the weights is parallel to the ratio of the MAP achieved by each ranking list, the merged list achieves the best performance <ref type="bibr" coords="4,330.43,238.35,11.18,9.45" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation results</head><p>In this year expert search task, we submitted 4 runs which are listed in table 2. The results show the effectiveness of our combination of PDDs.</p><p>Table2 In document search task, we attempted a promising content-based link analysis algorithm. We find the link structure of intranet and a website is totally different from the Internet. Therefore we will try to investigate the link analysis algorithms applying in intranet. A bigger ambition is that we would like to experiment our content-based link analysis algorithm on bigger data sets and to propose an algorithm integrating the link analysis and similarity ranking.</p><p>For expert finding task, in the past two years we proposed PDD ranking model (2005), and expertise propagation algorithm (2006) which focuses on the social network analysis. In this year, we build different PDD collection from different subsets of the corpus. So each PDD generated has different confidence and reliability.</p><p>How to estimate the confidence of the expertise and how to merge the ranking lists is what we focus on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,222.80,313.53,176.41,9.45;4,309.90,329.73,23.90,9.45;4,61.50,345.75,100.68,9.45;4,309.92,345.75,28.89,9.45;4,61.50,361.83,195.18,9.45;4,309.94,361.83,28.88,9.45;4,61.50,377.91,199.94,9.45;4,309.77,377.91,28.93,9.45;4,61.50,394.05,199.66,9.45;4,309.85,394.05,28.94,9.45;4,61.50,415.82,156.79,10.80"><head></head><label></label><figDesc>. The official results of expert search runs MAP</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,70.14,743.61,463.65,8.10;1,70.03,759.21,395.47,8.10"><p>Supported by the Chinese National Key Foundation Research &amp; Development Plan (2004CB318108)，Natural Science Foundation (60621062, 60503064，60736044) and National 863 High Technology Project (2006AA01Z141)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,65.27,608.16,468.62,9.02;4,71.52,623.16,209.11,9.02" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,296.75,608.16,232.34,9.02">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="4,65.27,638.16,468.59,9.02;4,71.52,653.16,199.75,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,175.80,638.16,211.92,9.02">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,396.18,638.16,137.68,9.02;4,71.52,653.16,195.38,9.02">Proceedings of the Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,65.27,668.16,468.53,9.02;4,71.52,683.16,145.78,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,256.09,668.16,263.71,9.02">Maximum-likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,527.37,668.16,6.42,9.02;4,71.52,683.16,101.91,9.02">J. Royal Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,65.27,698.16,468.61,9.02;4,71.52,713.16,244.73,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,128.69,698.16,169.19,9.02">THUIR at TREC 2005: Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,157.13,713.16,104.74,9.02">Proceedings of TREC2005</title>
		<meeting>TREC2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>CST Dept, Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>NIST</note>
</biblStruct>

<biblStruct coords="4,65.27,728.16,468.45,9.02;4,71.52,743.16,395.66,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,170.38,728.16,363.35,9.02;4,71.52,743.16,21.96,9.02">Probabilistic Model Supported Rank Aggregation for the Semantic Concept Detection in Video</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,99.81,743.16,301.01,9.02">ACM International Conference on Image and Video Retrieval (CIVR 2007)</title>
		<imprint>
			<date type="published" when="2007-11">July 9-11 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
