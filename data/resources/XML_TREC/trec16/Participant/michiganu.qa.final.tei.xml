<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,78.80,92.85,433.93,15.12">Michigan State University at the 2007 TREC ciQA Evaluation</title>
				<funder ref="#_9bPPVgM">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.11,125.33,62.43,10.48"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
							<email>zhangch6@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48824</postCode>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.69,125.33,85.27,10.48"><forename type="first">Matthew</forename><surname>Gerber</surname></persName>
							<email>gerberm2@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48824</postCode>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.11,125.33,73.32,10.48"><forename type="first">Tyler</forename><surname>Baldwin</surname></persName>
							<email>baldwi96@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48824</postCode>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,157.36,139.28,86.01,10.48"><forename type="first">Steve</forename><surname>Emelander</surname></persName>
							<email>emeland4@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48824</postCode>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.52,139.28,72.01,10.48"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
							<email>jchai@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48824</postCode>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.68,139.28,46.49,10.48"><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<email>rongjin@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University East Lansing</orgName>
								<address>
									<postCode>48824</postCode>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,78.80,92.85,433.93,15.12">Michigan State University at the 2007 TREC ciQA Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E16A21CCA1662931A3C29F1E18844648</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is our first participation in the ciQA task. Instead of exploring conversation strategies in question answering <ref type="bibr" coords="1,108.07,284.28,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,121.92,284.28,7.01,8.74" target="#b3">4]</ref>, we decided to focus on simple interaction strategies using relevance feedback. In our view, the ciQA task is not designed to evaluate user initiative interaction strategies. Since NIST assessors act as users, the motivation to take an initiative is lacking. It is not clear how to encourage the assessors to take the initiative (e.g., by asking additional questions) during the interaction process. We feel in such a setting, relevance feedback or any kind of system initiative interaction strategies seem more appropriate. Therefore, we have focused on variations of relevance feedback in this year's evaluation.</p><p>For the initial runs, our two submissions were based on two distinct approaches, a heuristic approach and a machine learning approach. Since only two interactive runs can be submitted for evaluation, we decided to focus on one aspect of variation. The only difference between the two interactive and final run systems is how the feedback is solicited and incorporated. Since there are many parameters inherent in the evaluation that affect the outcome of the final runs, only varying one parameter will hopefully allow us to make some preliminary observations about how feedback solicitation can affect final performance.</p><p>Although manual runs were allowed in this evaluation, all of our runs were created automatically. The following steps were taken during the evaluation. For each topic, the system first generated a query based on its question template and narrative and used this query to retrieve relevant documents. The retrieved documents were then segmented into sentences, which were further ranked and put together as the initial run results. The interactive web pages were generated based on the results from the initial runs. These pages were accessed by NIST assessors. Feedback from assessors was used to create the final run results.</p><p>In the following sections, we describe in detail the steps taken to create our initial runs and final runs. We also discuss what we have learned from this exercise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Initial Runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Formulation and Document Retrieval</head><p>The system applied a template dependent strategy to formulate queries for each topic. For most templates, the phrases in every slot of the templates and named entities in the narratives were used to form queries. The named entities were extracted using BBN's named entity identifier <ref type="bibr" coords="1,353.24,610.48,9.96,8.74" target="#b1">[2]</ref>. For the template What [relationship] exist between [entity] and [entity]?, the filler of the [relationship] slot (e.g., financial relationships, common interests) was not used in query formulation. For the financial relationship topics, finance-related terms such as money, trade, etc. from the narrative were used in the query. Figure <ref type="figure" coords="1,403.08,646.35,4.98,8.74" target="#fig_0">1</ref> shows an example of query formulation. Please note that our query has the form of phrase-and-phrase structure, which was utilized in the latter procedure of answer extraction and ranking. When these queries were used to retrieve documents, they were treated as bags-of-words.</p><p>Template: What evidence is there for transport of [military equipment and weaponry] from [South Africa] to [Pakistan]?</p><p>Narrative: The analyst is interested in South African arms support to Pakistan and the effect such support or sales has on relations of both countries with India. Additionally, the analyst would like to know what nuclear arms involvement, if any, exists between South Africa and Pakistan.</p><p>Query: military equip weaponry, South Africa, Pakistan, India The Lemur<ref type="foot" coords="2,124.22,187.02,3.97,6.12" target="#foot_0">1</ref> retrieval engine was used to retrieve the top 100 most relevant documents from the AQUAINT II corpus. The retrieved documents were further combined with 100 documents (per topic) provided by NIST, which resulted in an average of 155 documents per topic. During this combination, a simple algorithm was applied to ensure the retrieval score for each document was on the same scale despite the fact that documents were coming from two different sources. These documents were used for answer extraction and ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Extraction and Ranking</head><p>The documents produced in the previous step were split using the sentence detector provided by the OpenNLP toolkit<ref type="foot" coords="2,139.23,293.08,3.97,6.12" target="#foot_1">2</ref> . The initial run results were produced by ranking these sentences and selecting the top 7000 characters as answers. Two different ranking algorithms were used to produce two initial run submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Heuristic Approach</head><p>This approach was based on the following heurisitcs developed at the University of Waterloo <ref type="bibr" coords="2,481.90,362.85,14.61,8.74" target="#b9">[10]</ref>, which exhibited the best performance in the 2006 initial run evaluations <ref type="bibr" coords="2,351.71,374.80,9.96,8.74" target="#b4">[5]</ref>:</p><p>1. Rank sentences by the number of query phrases they contain. A sentence is judged to contain a phrase if it contains any term in the phrase.</p><p>2. Break ties by the number of query terms the sentences contain.</p><p>3. Break further ties by the average inverse document frequency (IDF) for all non-stop words in each sentence.</p><p>Since, for some templates (e.g.,What evidence is there for transport of [goods] from [entity] to [entity]? and What financial relationships exist between [entity] and [entity]?), special named entities signal whether a sentence contains a quality answer, we extended the original heuristic by incorporting a named entity check as follows:</p><p>1. Rank sentences by the number of query phrases they contain.</p><p>2. Break ties by the number of query terms they contain.</p><p>3. Break further ties by whether they contain special named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Break further ties by the average IDF.</head><p>Named entities were extracted using BBN's IdentiFinder. Different templates required checking for different types of named entities. For example, for the transportation template (What evidence is there for transport of [goods] from [entity] to [entity]?), the named entity types quantity, location, and cardinal were used. The extended heuristics have shown to improve the results on 2006 data compared to the original heuristics (see Section 2.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Machine Learning Approach</head><p>Based on the publicly available data from the 2006 ciQA evaluation <ref type="bibr" coords="3,367.18,74.35,9.96,8.74" target="#b4">[5]</ref>, we developed a machine learning approach to predict whether a sentence is considered a good answer to a complex question. We examined the candidate sentences for the 30 topics from 2006 and labeled those that were considered quality answers, i.e., those that cover the meaning of the answer nuggets.</p><p>We formulated the machine learning approach as a binary classification problem, and used a logistic regression model <ref type="bibr" coords="3,137.60,134.13,10.52,8.74" target="#b6">[7]</ref> to perform the classification. For each sentence, the classifier predicted the probability that it belonged to the positive class (true positive answers being quality answers). The probabilities were then used to rank the candidate answer sentences.</p><p>We identified four groups of features for the classification:</p><p>• Query-independent sentence-level features:</p><p>-Sentence length, in terms of non-whitespace characters.</p><p>-The average IDF of non-stop words in the sentence.</p><p>-The average log-IDF of non-stop words in the sentence.</p><p>• Query-dependent sentence-level features:</p><p>-The retrieval score of the document containing the sentence.</p><p>-The number of query phrases the sentence contains. A sentence is judged to contain a phrase if it contains any term in the phrase. -The number of query terms contained in the sentence.</p><p>• Document-dependent sentence-level features:</p><p>-The percentage of sentences in the document that share terms with the sentence under consideration, which is the same as lexical bonds in <ref type="bibr" coords="3,299.47,362.36,14.61,8.74" target="#b9">[10]</ref>. -The position of the sentence in the document.</p><p>-The position of the sentence in the paragraph containing it.</p><p>• Binary features indicating whether or not the sentence contains certain types of named entities:</p><p>-Countries, cities, states, provinces -Physical locations (bodies of water, mountains, continents, etc.)</p><formula xml:id="formula_0" coords="3,97.88,457.83,97.69,110.89">-Organization names -Person names -Substance names -Dates -Cardinal numbers -Money amounts -Percentages -Quantity numbers</formula><p>We trained a model for each of the template types seperately, except for the common interests and familial/organizational ties templates. For the latter two templates, a model trained on the entire dataset was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Empirical Comparison of the Two Answer Ranking Approaches</head><p>Before we applied these approaches on this year's data, we evaluated these two approaches using data from the 2006 evaluation. For the machine learning approach, we used leave-one-topic-out cross validation. The evaluation metric was the official pyramid F-score measurement <ref type="bibr" coords="3,343.35,670.09,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="3,357.19,670.09,7.01,8.74" target="#b7">8]</ref>. The results are shown in Table <ref type="table" coords="3,510.17,670.09,3.87,8.74" target="#tab_0">1</ref>.</p><p>The results demonstrate that both approaches outperformed last year's automatic run results. Additionally, the machine learning approach exhibits a small advantage over the simple heuristic approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interaction</head><p>The interaction component plays a vital role in the ciQA evaluations. One observation about the current evaluation is that results are judged after the interaction is completed rather than during the interaction. Intermediate results after each run of interaction are not considered. Thus, it does not matter if a system is fast or slow in delivering its final answer. Based on this observation, we designed two types of interactive systems. In one system, after each round of interaction, user feedback is immediately passed to the backend processing components (e.g., answer extraction and re-ranking) to generate a new set of candidate answers.</p><p>We call this sytem the interaction system. In the second system, during interaction, the system only collects feedback from the user. Once the interaction is completed, the feedback is sent to the backend to generate potential answers. We call this system the collection system. The reason we started with relevance feedback as the basic interaction strategy is two-part. First, relevance feedback has shown to be effective in information retrieval <ref type="bibr" coords="4,134.55,274.53,9.96,8.74" target="#b5">[6]</ref>. Second, relevance feedback provides a reasonable baseline for our future work on other interaction strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Interaction System</head><p>A traditional relevance feedback approach is used in the interaction system. For each interaction round, the top 20 sentences are displayed to the user for relevance feedback. Based on the feedback, the system re-ranks the remaining sentences in the list and shows the next top 20 on the re-ranked list. The re-ranking function is a variation of the Rocchio algorithm <ref type="bibr" coords="4,360.53,368.63,9.96,8.74" target="#b0">[1]</ref>. After each interaction, the system computes a weight for each term in the vocabulary based on all feedback given:</p><formula xml:id="formula_1" coords="4,147.82,399.96,377.70,26.88">w(t) = count(t, q) + 1 |pos| f ∈pos count(t, f ) - 1 |neg| f ∈neg count(t, f ) (<label>1</label></formula><formula xml:id="formula_2" coords="4,525.52,406.70,4.24,8.74">)</formula><p>where q is the query used for document retrieval and answer extraction, pos is the set of positive feedback answers, neg is the set of negative feedback answers, and count(t, f ) is the frequency of term t in feedback answer f .</p><p>Our modification to this function ignores terms with negative weights:</p><formula xml:id="formula_3" coords="4,236.18,492.11,293.59,20.69">w (t) = w(t) w(t) ≥ 0 0 w(t) &lt; 0 (2)</formula><p>This is based on the assumption that for this specific ciQA task, positive answers are much fewer than negative answers, so positive feedback should carry much more weight than a negative feedback.</p><p>Terms and their corresponding weights become a new, modified query. The system then re-ranks the sentences based on their similarities with the modified query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Collection System</head><p>The collection system is designed to allow users to browse through sentences to provide relevance feedback with the help of a list of automatically generated filters. Our assumption is that many users are knowledgable. They may want to control the types of sentences they see and provide feedback. The collection system does not undertake any backend processing, but rather collects as much relevant information as possible. Figure <ref type="figure" coords="4,524.79,639.55,4.98,8.74" target="#fig_1">2</ref> shows a snapshot of the collection system. The left panel contains the filters generated by the system. Each filter is either a query term or an informative non-query noun that appears in the question template or narrative and has hyponyms or meronyms in top 200 ranked sentences. The filter list also contains salient named entities from the answer passages. The user can select one or more filters that are related to his or  After collecting the feedback from the user, the final sentence list is produced using the same re-ranking method as the interaction system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Results</head><p>We submitted two initial runs and two final runs. We picked the initial run results generated by the heuristic approach to solicit user feedback and create our final run results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initial Run Results</head><p>The evaluation results of our two initial runs are shown in Table <ref type="table" coords="6,355.24,347.38,3.87,8.74" target="#tab_1">2</ref>. The heurisitic approach achieved an F-measure score of 35.9%, and the learning-based approach achieved an F-measure score of 38.7%. These results are ranked third and sixth among eleven automatic initial runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Final Run Results</head><p>The initial run results based on the heurisitic approach were used in the interaction systems for feedback. Unfortunately, a major network failure occured during the two days when the NIST assessors tried to interact with our pages. On the first day, the entire MSU network was unaccessible. Based on the limited feedback we did receive, two of our final runs (one from each interaction system) were produced. Table <ref type="table" coords="6,495.03,453.43,4.98,8.74" target="#tab_2">3</ref> shows the results of our final runs. Compared to the baseline system (e.g., MSUciQAiHeu), there was a small improvement for both final runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>Although the experiments with the interactive systems did not go smoothly due to network issues, we were still able to make some observations based on the limited feedback our systems received.</p><p>First, as shown in Table <ref type="table" coords="6,185.55,556.02,3.87,8.74" target="#tab_3">4</ref>, the amount of feedback received by the collection system is significantly less than the amount received by the interaction system. Note that, despite the network problems, since the two interaction systems for a particular topic were accessed at roughly the same time, it is possible to compare the two systems. Our original assumption was that because no backend processing was involved in the collection system, all the interaction time (i.e., 5 minutes) would be devoted to collecting feedback. Thus more feedback should be received compared to the interaction system where a portion of the interaction time would be used for the backend processing. However, as shown in Table <ref type="table" coords="6,397.18,627.75,3.87,8.74" target="#tab_3">4</ref>, the results disconfirmed our assumption. The collection interface requires a user to be more motivated to choose different combinations of filters to navigate through results. Possible lack of motivation and the more complex interface may contribute to the ineffectiveness of the collection system.</p><p>Table <ref type="table" coords="6,104.27,675.57,4.98,8.74" target="#tab_4">5</ref> shows the percentage of positive and negative feedback received for both interfaces. In general, the ratio of positive feedback is quite high, which means our initial sentence ranking algorithm provides a 62% 15% 23% *Statistics on the interaction interface only count the top 10 sentences for each topic reasonable baseline. This ratio is even higher for the collection interface. Since positive feedback is much more valuable than negative feedback, the idea of enabling the user to explore their desired information through filters (as in the collection system) appears to have some merit.</p><p>Furthermore, we are interested in examining the consistency of the feedback. There were 247 sentences (for all topics) for which both interfaces received user feedback. Out of these sentences, 25 of them were provided with conflicting feedback by the same assessor. Figure <ref type="figure" coords="7,346.86,272.57,4.98,8.74" target="#fig_2">3</ref> shows the distribution of these conflicts among the eight assessors. We can see that the feedback consistency is largely dependent on individual assessors (e.g., for one assessor, more than one third of the feedback was conflicting). This observation indicates that it is sometimes difficult for a human to judge the correctness of an answer, which motivates further development of evaluation methodologies for the ciQA task. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,198.57,156.74,194.40,8.77"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of query formulation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,196.50,570.64,198.53,8.74;5,61.77,174.22,467.99,374.39"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A snapshot of the collection system</figDesc><graphic coords="5,61.77,174.22,467.99,374.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,61.77,278.88,468.00,8.74;8,61.77,290.84,143.07,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Consistency of feedbacks given to both interfaces for eight assessors, ranking from the least consistent to the most consistent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,111.49,62.88,368.57,45.15"><head>Table 1 :</head><label>1</label><figDesc>Empirical results from two sentence ranking approaches on 2006 ciQA data</figDesc><table coords="4,204.17,74.58,183.19,33.44"><row><cell></cell><cell cols="3">Precision Recall F (β = 3)</cell></row><row><cell>Heuristic</cell><cell>0.080</cell><cell>0.441</cell><cell>0.282</cell></row><row><cell>Learning</cell><cell>0.085</cell><cell>0.450</cell><cell>0.293</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,157.88,62.88,275.77,43.21"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results of two initial runs on 2007 data</figDesc><table coords="6,157.88,72.64,275.77,33.45"><row><cell>Run tag</cell><cell cols="2">Sentence ranking approach Average F score</cell></row><row><cell>MSUciQAiHeu</cell><cell>Heuristic</cell><cell>0.359</cell></row><row><cell>MSUciQAiLrn</cell><cell>Learning</cell><cell>0.387</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,61.77,130.17,468.00,89.04"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results of two final runs on 2007 data When the filters are selected, the sentences in the right panel are updated to reflect the filter selection. Users can then give feedback on the displayed sentences.</figDesc><table coords="6,61.77,139.93,400.63,67.32"><row><cell>Run tag</cell><cell>Interface</cell><cell cols="2">Average F score Improvement over baseline</cell></row><row><cell cols="2">MSUciQAfInt Interaction</cell><cell>0.370</cell><cell>0.011</cell></row><row><cell cols="2">MSUciQAfCol Collection</cell><cell>0.361</cell><cell>0.002</cell></row><row><cell>her information needs.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,108.82,62.88,373.91,45.15"><head>Table 4 :</head><label>4</label><figDesc>Per-topic feedback statistics of user relevance feedback for our two interfaces</figDesc><table coords="7,217.58,74.58,156.37,33.44"><row><cell></cell><cell cols="3">Min Max Average</cell></row><row><cell>Interaction</cell><cell>7</cell><cell>105</cell><cell>30</cell></row><row><cell>Collection</cell><cell>2</cell><cell>80</cell><cell>14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,132.17,132.11,327.18,45.15"><head>Table 5 :</head><label>5</label><figDesc>Statistics of positive and negative feedbacks for our two interfaces</figDesc><table coords="7,206.25,143.81,179.04,33.44"><row><cell>+</cell><cell>-</cell><cell>No feedback</cell></row><row><cell cols="2">Interaction* 60% 31%</cell><cell>9%</cell></row><row><cell>Collection</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,77.01,685.19,108.46,6.99"><p>http://www.lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,77.01,694.70,90.81,6.99"><p>http://www.opennlp.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We would like to thank <rs type="institution">BBN</rs> for the IdentiFinder software. This work was partially supported by the <rs type="programName">Disruptive Technology Office (DTO) Phase III program</rs> for the <rs type="institution">Advanced Question and Answering for Intelligence (AQUAINT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9bPPVgM">
					<orgName type="program" subtype="full">Disruptive Technology Office (DTO) Phase III program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,82.25,453.83,400.91,8.74" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m" coord="7,247.81,453.83,129.54,8.74">Modern Information Retrieval</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,82.25,473.76,447.52,8.74;7,82.25,485.72,447.52,8.74;7,82.25,497.67,22.69,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,334.50,473.76,195.27,8.74;7,82.25,485.72,23.75,8.74">Nymble: a high-performance learning namefinder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,125.81,485.72,332.17,8.74">The Proceedings of Fifth Conference on Applied Natural Language Processing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,82.25,517.60,447.52,8.74;7,82.25,529.55,447.52,8.74;7,82.25,541.51,196.60,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,250.64,517.60,279.12,8.74;7,82.25,529.55,30.82,8.74">Automated performance assessment in interactive question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,138.49,529.55,391.27,8.74;7,82.25,541.51,166.44,8.74">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development on Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,82.25,561.43,447.52,8.74;7,82.25,573.39,447.52,8.74;7,82.25,585.34,264.71,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,252.77,561.43,277.00,8.74;7,82.25,573.39,147.98,8.74">Towards conversational qa: Automated identification of problematic situations and user intent</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,254.04,573.39,275.73,8.74;7,82.25,585.34,48.78,8.74">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,82.25,605.27,447.52,8.74;7,82.25,617.22,261.32,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,230.25,605.27,220.52,8.74">Overview of the trec 2006 question answering track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,471.07,605.27,58.70,8.74;7,82.25,617.22,230.94,8.74">The Fifteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>TREC 2006) Proceedings</note>
</biblStruct>

<biblStruct coords="7,82.25,637.15,447.52,8.74;7,82.25,649.10,447.52,8.74;7,82.25,661.06,304.37,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,234.37,637.15,295.40,8.74;7,82.25,649.10,113.31,8.74">A case for interaction: a study of interactive information retrieval behavior and effectiveness</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koenemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,219.36,649.10,310.41,8.74;7,82.25,661.06,79.25,8.74">CHI &apos;96: Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,82.25,680.98,447.52,8.74;7,82.25,692.94,89.11,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,270.49,680.98,168.10,8.74">Ridge estimators in logistic regression</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Le Cessie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Houwelingen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,450.56,680.98,75.37,8.74">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="201" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,82.25,320.73,447.52,8.74;8,82.25,332.68,447.52,8.74;8,82.25,344.64,447.52,8.74;8,82.25,356.59,377.58,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,114.29,320.73,415.48,8.74;8,82.25,332.68,99.30,8.74">Is question answering better than information retrieval? towards a task-based evaluation framework for question series</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,200.07,332.68,329.69,8.74;8,82.25,344.64,34.50,8.74;8,341.99,344.64,155.45,8.74">Human Language Technologies 2007: The Conference of the North American Chapter</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-04">April 2007</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
	<note>Proceedings of the Main Conference</note>
</biblStruct>

<biblStruct coords="8,82.25,376.52,447.52,8.74;8,82.25,388.47,447.52,8.74;8,82.25,400.43,447.52,8.74;8,82.25,412.38,22.69,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,233.80,376.52,194.22,8.74">Will pyramids built of nuggets topple over?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,450.47,376.52,79.29,8.74;8,82.25,388.47,447.52,8.74;8,82.25,400.43,279.04,8.74">Proceedings of the 2006 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2006)</title>
		<meeting>the 2006 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2006)<address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,82.25,432.31,447.52,8.74;8,82.25,444.26,447.52,8.74;8,82.25,456.22,63.40,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,261.90,432.31,267.87,8.74;8,82.25,444.26,153.91,8.74">Identifying relationships between entities in text for complex interactive question answering task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karamuftuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,260.27,444.26,269.50,8.74;8,82.25,456.22,33.24,8.74">The Fifteenth Text REtrieval Conference (TREC 2006) Proceedings</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
