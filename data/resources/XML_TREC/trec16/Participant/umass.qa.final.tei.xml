<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.20,100.98,429.77,16.41;1,154.20,122.82,303.60,16.41">UMass Complex Interactive Question Answering (ciQA) 2007: Human Performance as Question Answerers</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_kJRTe8s">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_qX9sHmZ">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.60,156.37,87.40,10.72"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.04,156.37,61.00,10.72"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.90,156.37,90.44,10.72"><forename type="first">Blagovest</forename><surname>Dachev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.20,100.98,429.77,16.41;1,154.20,122.82,303.60,16.41">UMass Complex Interactive Question Answering (ciQA) 2007: Human Performance as Question Answerers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9418A0A516D06F7DCD508167CD8E223</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Every day, people widely use information retrieval (IR) systems to answer their questions. We utilized the TREC 2007 complex, interactive question answering (ciQA) track to measure the performance of humans using an interactive IR system to answer questions. Using our IR system, assessors searched for relevant documents and recorded answers to their questions. We submitted the assessors' answers without modification as one of our runs. For our other submission, one of the authors used our IR system for an unlimited time and recorded answers to the questions. We found that human performance using an interactive IR system for question answering is variable but that interactive IR systems offer the potential for superior question answering performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The complex, interactive question answering (ciQA) track looks at complex information needs and aims to investigate the performance gains attainable when a QA system has the chance to interact with users. This year, for each question the track allowed participants to provide a web address (URL) at which the participants could provide any sort of web page to interact with the assessor for 5 minutes.</p><p>Today when users have questions, one of their likely tactics for finding answers is to use an information retrieval (IR) system. The ciQA track provided us with the unique opportunity to measure the assessors' abilities to answer their own questions using an interactive IR system. Rather than attempt to use interaction to boost the performance of a QA system, we wanted to see how good users are at answering their questions using information retrieval.</p><p>A good information retrieval system should inherently be a good complex, interactive question answering system. Or as we told the assessors, "Our belief is that human searchers, such as yourself, can find answers faster and more accurately than computers." Rather than build question answering systems, our intent is to build interactive IR systems that enable people to answer questions.</p><p>To measure the assessors' performance at answering their questions using an IR system, we created an interactive, web-based IR system. Assessors were free to issue queries, view documents, and save answers. We did not place any restrictions on the type of answer the assessors could enter and save. Assessors could copy text from a displayed document, a result snippet, or type in their own answer.</p><p>While confident that the assessors would find answers, we hedged our bet by utilizing our two runs to give the assessors 10 minutes on each question. For each run, we provided the same IR system and when the assessor returned to a question, any previously saved answers were still displayed.</p><p>One of our submitted runs was the exact set of answers saved by the assessors after all 10 minutes of interaction. As such, the assessors judged their own saved answers. This allows us to not only compare an interactive IR system's performance to the other submitted interactive QA systems, but to also measure to what extent assessors agree with themselves.</p><p>A concern with interactive systems is that users have difficulty in judging recall and often quit searching too soon. For our other run, we had one of the authors use the system for an unlimited time to find as many answers for each question as possible. This run allows us to get a sense of the maximum performance of our interactive system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We built a fully interactive IR system with facilities for recording answers to questions. We stress that the IR system was fully interactive because in past ciQA and HARD track evaluations, participants As Figure <ref type="figure" coords="2,131.09,411.48,4.98,8.78" target="#fig_0">1</ref> shows, the user interface to our IR system consisted of three vertically oriented panes. The left pane showed search results with query-biased snippets. Clicking on a result showed the respective document in the middle pane and also changed the color of the link allowing users to keep track of already examined documents. Both the snippets and displayed document had query terms highlighted. The right hand pane provided a text box allowing the user to enter and save an answer to the question. A list of the user's saved answers appeared below the answer entry box. The assessors could delete saved answers.</p><p>Each assessor had the chance to read a tutorial on how to use our search engine and about our research goals. At the end of the tutorial, they could experiment with the live system and the ciQA designated throw away question.</p><p>Our IR front end client was a modern AJAX-like interface written in XHTML, CSS, and JavaScript. Submitting queries, clicking on results to view documents, and saving answers all occurred within the same web page and did not require an entire page refresh for each event. We built the back-end server using a combination of the Apache web server, PHP, mySQL, C++, and the Indri [6, 5] retrieval system. We supported a simple query language. Users could specify phrases by enclosing a phrase with double quotes. Users could also force all results to contain a query term by preceding the term with a plus sign. The Indri query language provides support for both of these features.</p><p>When the assessor first accesses the system for a given question, the system shows the results for a default query created automatically from the templated question as shown in Figure <ref type="figure" coords="2,478.98,507.84,3.90,8.78" target="#fig_0">1</ref>. To create the query, we extracted the terms within the slots of the template and then removed stop words. The remaining terms formed a bag of words query that we implemented using Indri's #combine operator. For example, the question "What is the position of [Hank Aaron] with respect to [Barry Bond's use of steroids]?" results in the Indri query #combine(Hank Aaron Barry Bonds steroids). Our tutorial encouraged assessors to change the query to meet their needs.</p><p>We used the same system for each run and also used the same tutorial for each run. We provided a link to explain to the assessor why they were seeing the same tutorial a second time. For each question, the interface showed previously saved answers and also kept track of viewed documents to allow the links to the documents to be properly highlighted. The system did not save any query state and thus the assessor saw for a second time the default query and results when returning to the interface.</p><p>We annotated the sentences in the AQUAINT2 collection using a locally modified version of a sentence splitter <ref type="bibr" coords="3,107.34,135.84,9.98,8.78" target="#b0">[1]</ref>. To construct the query-biased snippets for each document, we converted the user's query to a bag-of-words query and then retrieved the top two scoring sentences from the document.</p><p>We stemmed all words with the Porter stemmer built into Indri and used an in-house list of 418 stop words. For retrieval, we used Indri's default parameters, which includes setting the Dirichlet prior smoothing parameter to a value of 2500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions</head><p>We constructed our baseline submission, UMass-BaseAut, to be similar to the displayed query-biased snippets for each question's default query. As described above, the default query consisted of the words from the slots of the templated question. Using the default query, we retrieved the top 10 documents, which are the same 10 documents shown initially to the assessor. For each of these documents, we returned at most the top 2 sentences as answers for a maximum of 20 answers per topic. Some documents contained a single sentence and as a result we returned less than 20 answers in some cases. Unlike our displayed snippets, we did not truncate the sentences returned as answers. Our baseline represents the state from which the assessors started their usage of the system.</p><p>Our second submission, UMassIntA, consists of the answers provided by the assessors during their interactions with the system. For both of our allowed runs, we had the assessors interact with the same live IR system to obtain 10 minutes of interaction on each question. As described above, the system retained its state between "runs." Assessors directly entered answers to the questions. We submitted the answers saved by the assessors with no modification. Some assessors did delete some of their submitted answers, and we did not submit deleted answers. As we used the interaction automatically to produce the submission, we marked this as an automatic run. During one day of interaction, the assessors experienced network slowdowns, which likely hurt their ability to find and record answers.</p><p>We constructed our third submission, UMass-IntM, by having one of the authors use our interactive system without a time limit to find answers. For questions where we found over 7000 non-whitespace characters of answers, we edited the answers to fit within the limit. Most answers are in the form of snippets of text extracted directly from the source documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Table <ref type="table" coords="3,338.77,171.48,4.98,8.78" target="#tab_0">1</ref> summarizes the results of our three submissions. Measures are computed in the same manner as the 2006 ciQA track <ref type="bibr" coords="3,404.97,195.36,10.45,8.78" target="#b1">[2]</ref> with pyramid nugget based scoring <ref type="bibr" coords="3,344.89,207.24,9.99,8.78" target="#b3">[4]</ref>. The official results report the F measure with a β = 3, which places 3 times more importance on recall than precision. We also show the F measure with β = 1, which places equal weight on recall and precision. Unless we specify otherwise, all F score mentions will be with respect to the official F score with a β = 3.</p><p>The median F score of all ciQA final runs (both automatic and manual) was reported by NIST to be 0.361. The best and worst scores were reported to be 0.503 and 0.156, respectively.</p><p>We were somewhat surprised by our baseline's performance. The baseline, UMassBaseAut, had a F score of 0.318, which was only 12% below the median performance of all final submitted runs (0.361). Our baseline appears to be representative of many of the baselines in its strong performance relative to the submitted runs. The median performance of the submitted baselines was 0.359 as reported by NIST.</p><p>Our automatic submission, UMassIntA, performed 9% better than our baseline with an F score of 0.347 and performed near the median performance of all final runs. The improvement over the baseline came from an improved precision that rose from 0.154 for the baseline to 0.427 for the assessor's answers. If we place equal importance on recall and precision (β = 1), UMassIntA achieves a 59% improvement over the baseline with an F score of 0.333 compared to the baseline's 0.210.</p><p>Our manual submission, UMassIntM, matched the best reported performance with an average F (β = 3) score of 0.503. Even with issues of inter-annotator agreement, we see this as informative of the performance attainable by a human using an IR system to answer questions. With an equal weight given to precision and recall (β = 1), UMassIntM performs 12% worse than UMassIntA (F score of 0.293 compared to 0.333).</p><p>While we have just begun to analyze our results, we have observed that significant variation exists among the assessor's performance at answering their questions using our system. For the 4 topics assigned to assessor 5, the assessor only entered one answer into The average recall, precision and F scores for our three submissions and the number of questions for which F with β = 3 was the best, worst, and greater than the question median for final runs. The number of questions for which the submission scored a zero is reported in the column labeled #Zeros. Of note, only one question in common between UMassBaseAut and UMassIntA received a zero.</p><p>our system. The assessor later judged this single answer as not containing a single nugget. Thus, assessor 5 produced 4 of the 5 zero scoring questions for our UMassIntA submission. In contrast, assessor 8's answers to three of the assessor's four questions achieved the highest F scores of all submitted runs. It appears that some of assessor 5's interactions with our system occurred during a network slowdown, which could partly explain the lack of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Lin <ref type="bibr" coords="4,91.23,376.80,10.55,8.78" target="#b2">[3]</ref> proposed an evaluation framework of recall curves and using this framework compared the performance of an IR system to the submitted runs for the TREC 2004 and 2005 question answering tracks. Lin found that while the QA systems were superior for factoid questions, for other questions the performance of the IR and QA systems were similar. Lin's experiment is similar to our baseline submission where the results being measured are the static ranked lists generated from a query. In contrast, our work looks at the performance of an IR system being used as an interactive tool for question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our results suggest that interactive IR systems are inherently powerful for question answering. We believe that bridging the gap in performance between our automatic and manual submissions will be possible with the creation of innovative interaction mechanisms that allow users to better find, understand, and organize both documents and answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,116.88,356.16,373.84,8.78;2,72.00,73.16,470.10,267.90"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A screenshot of the web-based interface for our fully interactive, IR system.</figDesc><graphic coords="2,72.00,73.16,470.10,267.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,73.68,461.42,78.38"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,78.72,73.68,454.71,57.02"><row><cell></cell><cell></cell><cell cols="2">Average over 30 Questions</cell><cell></cell><cell></cell><cell cols="2">F (β = 3) Summary</cell><cell></cell></row><row><cell>Submission</cell><cell cols="8">Recall Precision F (β = 3) F (β = 1) #Best #Worst #Zeros #&gt;Median</cell></row><row><cell cols="2">UMassBaseAut 0.374</cell><cell>0.154</cell><cell>0.318</cell><cell>0.210</cell><cell>NA</cell><cell>NA</cell><cell>4</cell><cell>12</cell></row><row><cell>UMassIntA</cell><cell>0.362</cell><cell>0.427</cell><cell>0.347</cell><cell>0.333</cell><cell>5</cell><cell>6</cell><cell>5</cell><cell>1 4</cell></row><row><cell>UMassIntM</cell><cell>0.658</cell><cell>0.210</cell><cell>0.503</cell><cell>0.293</cell><cell>8</cell><cell>0</cell><cell>0</cell><cell>2 3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> under contract number <rs type="grantNumber">HR0011-06-C-0023</rs>, and in part by <rs type="funder">NSF</rs> Nano # <rs type="grantNumber">DMI-0531171</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qX9sHmZ">
					<idno type="grant-number">HR0011-06-C-0023</idno>
				</org>
				<org type="funding" xml:id="_kJRTe8s">
					<idno type="grant-number">DMI-0531171</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,326.52,335.52,213.51,8.78;4,326.52,347.52,213.56,8.78;4,326.52,359.40,214.15,9.01;4,326.52,373.98,139.45,8.35" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,465.83,335.52,74.21,8.78;4,326.52,347.52,213.56,8.78;4,326.52,359.40,47.69,8.78">University of Illinois at Urbana-Champaign. Sentence segmentation tool</title>
		<ptr target="http://l2r.cs.uiuc.edu/~cogcomp/atool.php?tkey=SS" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Cognitive Computation Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.52,391.32,213.72,8.78;4,326.52,403.32,206.66,9.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,417.07,391.32,123.18,8.78;4,326.52,403.32,42.42,8.78">Overview of the TREC 2006 ciQA task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,377.40,403.32,58.04,9.23">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.53,423.24,213.49,8.78;4,326.52,435.12,213.74,8.78;4,326.52,447.12,213.58,8.78;4,326.52,459.12,155.66,9.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,373.47,423.24,166.55,8.78;4,326.52,435.12,213.74,8.78;4,326.52,447.12,190.62,8.78">Is Question Answering Better than Information Retrieval? Towards a Task-Based Evaluation Framework for Question Series</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,326.52,459.12,82.30,9.23">HLT/NAACL 2007</title>
		<imprint>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.52,479.04,213.60,8.78;4,326.52,490.92,213.48,9.23;4,326.52,502.92,22.93,9.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,477.46,479.04,62.67,8.78;4,326.52,490.92,129.06,8.78">Will pyramids built of nuggets topple over?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,479.52,490.92,60.48,9.23">HLT/NAACL</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.50,522.84,213.66,8.78;4,326.52,534.84,213.44,8.78;4,326.52,546.72,157.82,9.23" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,455.46,522.84,84.69,8.78;4,326.52,534.84,213.44,8.78;4,326.52,546.72,34.78,8.78">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,369.36,546.72,16.66,9.23">IPM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,326.52,566.64,213.49,8.78;4,326.52,578.64,213.49,8.78;4,326.52,590.52,213.52,8.78;4,326.52,602.52,213.58,8.78;4,326.52,614.52,66.38,8.78" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,357.23,578.64,182.77,8.78;4,326.52,590.52,182.92,8.78">Indri: A language-model based search engine for complex queries (extended version)</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno>IR-407</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>CIIR, CS Dept., U. of Mass. Amherst</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
