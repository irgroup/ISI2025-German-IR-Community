<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.62,97.96,280.04,14.42">CSIR at TREC 2007 Expert Search Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,253.50,139.17,38.94,8.10"><forename type="first">Jiepu</forename><surname>Jiang</surname></persName>
							<email>jiangjiepu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Studies of Information Resources</orgName>
								<orgName type="department" key="dep2">School of Information Management</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.28,139.17,25.44,8.10"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Studies of Information Resources</orgName>
								<orgName type="department" key="dep2">School of Information Management</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.08,139.17,29.71,8.10"><forename type="first">Dan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Studies of Information Resources</orgName>
								<orgName type="department" key="dep2">School of Information Management</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.62,97.96,280.04,14.42">CSIR at TREC 2007 Expert Search Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE21FA802CC6C06514605F4881FB5DEF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the second year for the participation of Center for Studies of Information Resources (CSIR) in the TREC Expert Search Task. Rather than using the candidate profile based approach, a simplified two stage approach is used in our experiment, that is, documents are ranked based on topics, and each expert is scored intuitively by the weights of documents the expert appeared. Instead of the modeling of expert search, we have mainly focused on the effect of document filtering in the expert search. In our experiment, only the top n ranked topic-relevant documents where the expert also appeared are calculated into the expert score. The tuned value of n under W3C corpus for a best performance is 10, which is proved to be stable under CERC corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Expert Search Task of TREC Enterprise Track, quite different from the traditional ones, aims at creating a ranked list of experts in given topics rather than relevant documents. In TREC 2005 and TREC 2006, a canonical list of people, namely a list of names and email addressed of the candidate experts, was extracted officially and provided to all participants along with the corpus data. A ready-made candidate list lessened the procedure of expert recognition and helped the participants to concentrate on the IR models.</p><p>TREC 2007 has chosen CSIRO as the enterprise of interest and proposed some new challenges for participants. Most obviously, the candidate list is not available anymore and participants need to extract experts themselves. Though a real situation it is when searching for experts from enterprise website, the absence of candidate list has brought the participants a series of problems. Firstly, the recognition of person names and email addresses is required; secondly, it is necessary to filter out persons appeared in the corpus because they may be ones outside the CSIRO or stuffs rather than experts even inside the CSIRO; furthermore, the occurrences of an expert in the corpus are diverse in format and a mergence is needed accordingly.</p><p>The previous works of TREC Expert Search Task, reflected by the notebooks of the participants, have the following two characteristics. On the one hand, the expert profile creating approach was widely used and implemented diversely. Among them, the window-based technique, which is included in systems of 6 participants <ref type="bibr" coords="1,202.43,728.28,10.78,9.02" target="#b0">[1,</ref><ref type="bibr" coords="1,215.81,728.28,7.48,9.02" target="#b1">2,</ref><ref type="bibr" coords="1,225.89,728.28,7.48,9.02" target="#b2">3,</ref><ref type="bibr" coords="1,235.91,728.28,7.54,9.02" target="#b3">4,</ref><ref type="bibr" coords="1,245.99,728.28,7.54,9.02" target="#b4">5,</ref><ref type="bibr" coords="1,256.07,728.28,8.37,9.02" target="#b5">6]</ref> out of 9 who used the expert profile creating approach <ref type="bibr" coords="1,487.19,728.28,10.84,9.02" target="#b0">[1,</ref><ref type="bibr" coords="1,500.58,728.28,7.54,9.02" target="#b1">2,</ref><ref type="bibr" coords="1,90.00,743.88,7.53,9.02" target="#b2">3,</ref><ref type="bibr" coords="1,100.61,743.88,7.53,9.02" target="#b3">4,</ref><ref type="bibr" coords="1,111.21,743.88,7.53,9.02" target="#b4">5,</ref><ref type="bibr" coords="1,121.82,743.88,7.53,9.02" target="#b5">6,</ref><ref type="bibr" coords="1,132.43,743.88,7.53,9.02" target="#b6">7,</ref><ref type="bibr" coords="1,143.04,743.88,7.53,9.02" target="#b7">8,</ref><ref type="bibr" coords="1,153.64,743.88,7.22,9.02" target="#b8">9]</ref>, is the most frequently adopted method. Besides, Jennifer et al <ref type="bibr" coords="1,422.65,743.88,11.72,9.02" target="#b0">[1]</ref> proposed the top sentence approach, the whole document approach and the summarization approach to create profiles of experts. In their work, different methods of profile creating are performed separately and merged into the final profile of experts. On the other hand, the two stage approach of expert search is also frequently used and receives good performance. Besides, external information was used by some participants and proved effective to improve the final result. For example, Zhu et al <ref type="bibr" coords="2,432.21,123.30,11.69,9.02" target="#b1">[2]</ref> considered the page rank of each document as the way to measure the importance of each document. Jennifer et al <ref type="bibr" coords="2,493.70,138.90,11.68,9.02" target="#b0">[1]</ref> utilized the Google Scholar to return publications of each candidate and treated them as supplementary information to judge the expertise of candidate.</p><p>Last year, we used a window-based approach to create expert profile and then retrieved the profile information to generate a ranking list of experts by traditional IR method. This year, we reverted to the two stage approach which is simplified and focus mainly on the effect of document filtering in expert search. In our experiments, documents are ranked based on topic and only the top ranked topic-relevant documents where the expert appeared are summed into the expert score. It is assumed in our experiment that the expert who occurs in the top ranked documents to a given topic should be more relevant to the topic, which is similar to Thijs Westerveld <ref type="bibr" coords="2,329.34,289.32,10.64,9.02" target="#b6">[7]</ref>. By using this method, a better result is achieved this year. Due to the time limitation, external information and the query expansion approach are not used in our experiment, although they are generally accepted as an effective way of enhancing the final results.</p><p>In section 2, an emphasis on the approach and model adopted for this year is given. In section 3, we give a brief introduction to the whole procedure of our experiment. The analysis and evaluation of the results are discussed in section 4. At last, a conclusion is made and the future work is pointed out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Approach and the Model</head><p>There are usually two approaches of expert search: the expert profile creating approach and the two stage approach. The former, as mentioned in the introduction, collects evidence around the expert occurrence as the profile for experts and used the profile to match the query. However, the combination of document fragments into the profile of an expert may change the original semantic meaning of the documents. For example, given a topic "semantic web" and three documents, the first document contains one phrase "semantic web"; the second document contains one "semantic" in the phrase "semantic translation" and the third document contains two "web" in the phrase "web mining".</p><p>Obviously, the first document is relevant while the other two not. But if the second and the third document are combined together, the traditional IR model will treat the new document as one more relevant to topic "semantic web" than the first one, although it is not in fact. To avoid this problem, we have adopted the latter approach. This approach assumes that the expert appears in the relevant documents of a given topic possibly possesses expertise of this area. As a result, traditional IR method is used firstly to generate ranked lists of the relevant documents to each given topic. Persons occur in the relevant documents will be considered later as the possible experts of the topic.</p><p>Given a query Q, a result ranking of documents is retrieved by traditional IR method and W i is the weight for the ith document retrieved which is computed based on BM25. Then W e , the weight of the expert e for query Q, equals to the linear combination of the weight of documents which are retrieved for Q and contain occurrences of the expert e's evidence. The simple formula is as follows:</p><formula xml:id="formula_0" coords="3,272.82,83.03,232.50,28.30">e i i D w w ∈ = ∑ (1)</formula><p>Where D is the set of relevant documents for query Q containing the occurrences of the expert e.</p><p>In the document ranking list, the higher the document ranks, the more relevant it is to a given topic. On the contrary, it is quite likely that the documents with lower ranks are not relevant to the topic but will affect the effectiveness of retrieval. Thus, we assume that a cutoff of the document list should be useful for filtering the irrelevant documents. The cutoff value is tuned based on data of TREC 2006 and the best tuned value is used for our runs this year. Details of our experiment will be discussed in section 3.</p><p>Besides using the ranking list according to topic relevance, we also use a document ranking list by expert evidence (expert name and expert email). That is, we firstly get a document ranking list by expert evidence, and then use the topics for filtering. The underlying idea is that the document which contains more evidence of an expert should be more useful in representing the expert's expertise. As the above one, a cutoff is also set for this method in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment</head><p>Our experiment is based on OKAPI 2.51 in a Linux environment. The whole procedure can be divided into 3 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Cleaning</head><p>The CERC collection contains various resource crawled from CSIRO website. Most of the documents in the collection are html pages, while other resource such as formatted files, plain texts, octet-stream files, script, logs etc. are also included. A glance at the ingredient of CERC collection is in Html tags, semantic or non-semantic, impede text retrieval to a certain extent. The non-semantic html tags, mostly used only for controlling the display of web pages, provide little information for retrieval and need to be removed. Content within semantic html tags is usually more important and should be assigned to a higher weight at retrieval. We use html-parser [10] to traverse the pages and output cleaned corpus data. During the process of traverse, content within &lt;title&gt; and &lt;meta&gt; is extracted into separate field, while other tags are removed except for a part of &lt;a&gt; tags which are linked to email addresses because they are useful in expert recognition.</p><p>Formatted files, when returned after an http request, are described in self-defined XML tags. As a result, the same method which is used on html pages can be applied to clean data and extract important information. Furthermore, script and multimedia resource is dismissed since it contains little information of expert and contributes no more than 0.5% of the corpus data. Other resource is retained for the difficulty of cleaning due to diversified formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expert Recognition</head><p>The absence of candidate list this year requires a task of expert recognition. Though person names and email addresses can be extracted from documents, filtering is needed to distinguish experts from staffs, experts inside the enterprise from those outside. Further, a combination of results is required to merge the email address and different name formats of the same expert.</p><p>The domain of email address is an indication direct and effective for distinguishing experts inside from outside. On the assumption that each expert owns an email at the domain of the enterprise and the email appears at least once in the documents, the procedure of expert recognition in the system includes the following three steps.</p><p>Firstly, email address is extracted from documents and divided into two groups by judging whether its domain belongs to the enterprise or not. The group of emails at the enterprise domain will be used in the next step to generate characteristics of each expert. In this group, the email with a username containing words seldom used for person name is filtered for its high possibility to be and email for public use. Another group, supposed to be mostly owned by outsiders, will provide a determinant to avoid a person name from being recognized as a valid expert if hardly any positive clew exists. The documents where email occurs are recorded and merged into the occurrences of experts in the next step.</p><p>In the second step, an automatic procedure is performed to generate characteristics of experts and record the documents where they appeared. The characteristic of an expert is defined as a triad, &lt;email, name, occurrence&gt;, and can be renewed by any representation which is compatible and owns information more specific. Two sets of characteristics can be created in the system to represent groups of expert inside and outside the enterprise, namely E i and E o . The whole procedure can be described as follows: for each priority, E i and E o are scanned orderly with rules of this priority to renew every element within them, that is, if a name found in the document accords with current rule and does not exist in the other group, it can be absorbed into the current element with the occurrence recorded.</p><p>In the last step, each expert is identified in the designated format by the most specific person name found in the documents.</p><p>Though no official list of expert recognition is available in this task, it is revealed in the official list of relevant experts that 29 experts out of 152 who appeared in the relevant list were not found in our experiment. Besides the flaw of techniques in expert recognition process, the reason may reside in the incomplete truth of the undertaken assumption. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expert Retrieval</head><p>When experts are recognized and their occurrences are found in documents, traditional IR methods are used to retrieve relevant documents of a given topic. In our experiment, we use OKAPI and model BM25 to compute the relevance score of documents and generate a ranking list of relevant documents to each topic.</p><p>As mentioned in Section 2, the weight of an expert to a topic equals to the linear combination of the weight of documents which are retrieved for the topic and have the occurrences of the expert evidence. But the documents with a lower rank are highly possibly not relevant to the given topic, which means if an expert occurs in a large amount of the lower ranked documents, the linear combination of the weight of documents will largely increase the expert weight, although they cannot truly support the expert to be relevant with the given topic indeed.</p><p>To solve this problem, a cutoff n is set to filter out the lower ranked documents, that is, only the top n ranked topic-relevant documents where the expert also occurs are located to compute the relevance score for the expert. As illustrated in Table <ref type="table" coords="5,304.96,501.72,3.75,9.02" target="#tab_2">2</ref>, this filtering is effective for the final result. Figure <ref type="figure" coords="6,139.34,76.50,5.01,9.02" target="#fig_0">1</ref> shows the MAP results based on the experiments conducted under different cutoff n. A baseline result of the experiment which involves no filter of ranked documents is given to make a contrast. It is revealed evidently that a great improvement has been made when filtering is considered in the experiment. MAP keeps increasing along with the cutoff n until it reaches the peak when cutoff n is set to 10. Then MAP begins to decrease gradually.</p><p>The already trained cutoff value 10 is used directly in our run WHU10 this year and receives the best MAP results of our 4 runs as we expected before. WHUC5 and WHU15, where the cutoff value is assigned to 5 and 15, are also submitted to be the control group. Another run, WHUE10, is conducted under a different method, that is, we firstly get a document ranking list by expert evidence (person name and email), and then use the topics for filtering. In this run, cutoff is also set to 10. The results of each run will be emphasized in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>In section 2 and section 3, the basic model and method adopted and the whole procedure of our experiment are introduced. This year, we submitted 4 runs to TREC for evaluation. WHU10 uses the best value of cutoff trained for data of TREC 2006 and receives the best MAP result in our 4 runs. WHUC5 and WHU15 use the 5 and 15 as the cutoff for filtering and receive relatively worse performance than WHU10. The baseline run is conducted without considering cutoff for filtering and receives worse result than WHU10 and WHU15. WHUE10 is conducted under a different method as we mentioned in section 3 and receives results a little lower than WHU10 in MAP, RECIP-RANK and P@10 but better in R-PREC and BPREF.</p><p>Results of WHU10 and WHU15 and the contrast between them and the baseline accord with the outcome of experiment under the data of TREC 2006. The method used on WHUE10 is also proved to be an effective way for expert search. Though no experiment is achieved to find the best cutoff value of WHUE10, it receives results better than WHUC5 and WHU10 and a little worse than WHU10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In the TREC Expert Search Task this year, a better result, respectively, is achieved by our system than the work of last year. The future work will mainly concern with refinements of our model.</p><p>Firstly, the frequency of an expert appeared in one document is not considered in the method used for WHU10, WHUC5 and WHU15, which could also be taken into account in the computation of document weight. Further, a thorough experiment is needed to find out the best cutoff value in the method used for WHUE10 and to make a judgment between the different two methods of the document ranking based approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,172.86,755.97,267.75,8.10;5,120.36,525.24,354.48,213.48"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. MAP results at different cutoff values, using data of TREC 2006</figDesc><graphic coords="5,120.36,525.24,354.48,213.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,503.10,391.75,203.26"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table coords="3,113.22,533.55,368.53,172.80"><row><cell>Data Type</cell><cell>Details</cell><cell>Document Count</cell><cell>Percentage</cell></row><row><cell>HTML Page</cell><cell>text/html</cell><cell>328546</cell><cell>88.62%</cell></row><row><cell>Formatted File</cell><cell>pdf, word, rtf, ppt, excel</cell><cell>13682</cell><cell>3.69%</cell></row><row><cell>Octet-stream</cell><cell>application/octet-stream</cell><cell>10080</cell><cell>2.27%</cell></row><row><cell>Unknown data</cell><cell>unknown</cell><cell>10000</cell><cell>2.70%</cell></row><row><cell>Plain Text</cell><cell>mostly log file</cell><cell>6271</cell><cell>1.69%</cell></row><row><cell>Script</cell><cell>CSS, JavaScript</cell><cell>957</cell><cell>0.26%</cell></row><row><cell>Multimedia</cell><cell>asf, real, wmv, bmp</cell><cell>472</cell><cell>0.13%</cell></row><row><cell>XML</cell><cell>text/xml</cell><cell>179</cell><cell>0.05%</cell></row><row><cell>Other</cell><cell></cell><cell>528</cell><cell>0.14%</cell></row><row><cell>Total</cell><cell></cell><cell>370715</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,226.56,722.19,160.23,8.10"><head>Table 1 :</head><label>1</label><figDesc>The ingredient of CERC collection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,113.70,153.75,330.32,100.14"><head>Table 2 :</head><label>2</label><figDesc>Priority and the according rules in expert recognition.</figDesc><table coords="5,113.70,153.75,330.32,76.20"><row><cell>Priority</cell><cell>Rule</cell></row><row><cell>1</cell><cell>Person name with hyper-link to an email address</cell></row><row><cell>2</cell><cell>Full name absorbable appeared in the same page</cell></row><row><cell>3</cell><cell>First name is initial and absorbable, appeared in the same page</cell></row><row><cell>4</cell><cell>Person name appeared in the same page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,322.68,415.37,162.28"><head>Table 3 .</head><label>3</label><figDesc>Table3gives a general look at the evaluation of these 4 runs and a baseline run. Results of our 4 run this year and the baseline results.</figDesc><table coords="6,107.46,368.73,377.70,92.34"><row><cell>Run</cell><cell>cutoff</cell><cell>MAP</cell><cell>R-PREC</cell><cell>BPREF</cell><cell>RECIP-RANK</cell><cell>P@10</cell></row><row><cell>baseline</cell><cell>--</cell><cell>0.2582</cell><cell>0.2030</cell><cell>0.5934</cell><cell>0.3866</cell><cell>0.0980</cell></row><row><cell>WHU10</cell><cell>10</cell><cell>0.3399</cell><cell>0.2862</cell><cell>0.6392</cell><cell>0.4738</cell><cell>0.1220</cell></row><row><cell>WHUC5</cell><cell>5</cell><cell>0.2193</cell><cell>0.1483</cell><cell>0.5579</cell><cell>0.3054</cell><cell>0.0620</cell></row><row><cell>WHU15</cell><cell>15</cell><cell>0.3060</cell><cell>0.2744</cell><cell>0.6392</cell><cell>0.4404</cell><cell>0.1120</cell></row><row><cell>WHUE10</cell><cell>10</cell><cell>0.3280</cell><cell>0.3105</cell><cell>0.6399</cell><cell>0.4674</cell><cell>0.1020</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.89,203.52,399.49,9.02;7,90.00,219.12,415.33,9.02;7,90.00,234.72,35.01,9.02;7,125.04,232.63,5.04,5.83;7,132.60,234.72,132.40,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,273.76,219.12,147.11,9.02">IBM in TREC 2006 Enterprise Tack</title>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Averboch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,439.95,219.12,65.38,9.02;7,90.00,234.72,35.01,9.02;7,125.04,232.63,5.04,5.83;7,132.60,234.72,103.03,9.02">the Proceedings of the 15 th Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.16,260.28,400.22,9.02;7,90.00,275.88,346.42,9.02;7,436.44,273.79,5.04,5.83;7,446.46,275.88,58.96,9.02;7,90.00,291.48,73.64,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,408.30,260.28,97.08,9.02;7,90.00,275.88,211.45,9.02">The Open University at TREC 2006 Enterprise Track Expert Search Task</title>
		<author>
			<persName coords=""><forename type="first">Jianhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Rüger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Eisenstadt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrico</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,324.25,275.88,112.17,9.02;7,436.44,273.79,5.04,5.83;7,446.46,275.88,58.96,9.02;7,90.00,291.48,44.22,9.02">the Proceedings of the 15 th Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.09,317.10,400.27,9.02;7,90.00,332.70,24.75,9.02;7,114.78,330.61,5.04,5.83;7,122.34,332.70,132.49,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,275.64,317.10,160.80,9.02">BUPT at TREC 2006: Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">Qian</forename><surname>Zhao Ru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,444.12,317.10,61.24,9.02;7,90.00,332.70,24.75,9.02;7,114.78,330.61,5.04,5.83;7,122.34,332.70,103.03,9.02">Proceedings of the 15 th Text Retrieval Conference</title>
		<meeting>the 15 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,104.38,358.32,401.12,9.02;7,90.00,373.92,119.74,9.02;7,209.76,371.83,4.98,5.83;7,217.26,373.92,132.56,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,371.57,358.32,133.93,9.02;7,90.00,373.92,25.51,9.02">Window-based Enterprise Expert Search</title>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haozhen</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,122.28,373.92,87.46,9.02;7,209.76,371.83,4.98,5.83;7,217.26,373.92,103.09,9.02">Proceedings of the 15 th Text Retrieval Conference</title>
		<meeting>the 15 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,104.79,399.48,400.58,9.02;7,90.00,415.08,87.53,9.02;7,177.54,412.99,5.04,5.83;7,185.04,415.08,132.55,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,303.42,399.48,197.65,9.02">Ricoh Research at TREC 2006: Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">Ganmei</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yueyan</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,90.00,415.08,87.53,9.02;7,177.54,412.99,5.04,5.83;7,185.04,415.08,103.08,9.02">Proceedings of the 15 th Text Retrieval Conference</title>
		<meeting>the 15 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,104.65,440.70,400.63,9.02;7,90.00,456.30,260.26,9.02;7,350.28,454.21,5.04,5.83;7,357.78,456.30,132.55,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,426.26,440.70,79.02,9.02;7,90.00,456.30,165.71,9.02">Research on Expert Search at Enterprise Track of TREC 2006</title>
		<author>
			<persName coords=""><forename type="first">Shenghua</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huizhong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,262.74,456.30,87.53,9.02;7,350.28,454.21,5.04,5.83;7,357.78,456.30,103.09,9.02">Proceedings of the 15 th Text Retrieval Conference</title>
		<meeting>the 15 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,104.98,481.92,400.40,9.02;7,90.00,497.52,35.02,9.02;7,125.04,495.43,5.04,5.83;7,132.60,497.52,132.40,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,180.29,481.92,267.63,9.02">Correlating Topic Rankings and Person Rankings to Find Experts</title>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,455.40,481.92,49.97,9.02;7,90.00,497.52,35.02,9.02;7,125.04,495.43,5.04,5.83;7,132.60,497.52,103.03,9.02">Proceedings of the 15 th Text Retrieval Conference</title>
		<meeting>the 15 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.59,523.08,399.76,9.02;7,90.00,538.68,257.01,9.02;7,347.04,536.59,5.04,5.83;7,354.54,538.68,132.55,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,308.46,523.08,196.89,9.02;7,90.00,538.68,162.58,9.02">Language Models for Enterprise Search: Query Expansion and Combination of Evidence</title>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,259.50,538.68,87.51,9.02;7,347.04,536.59,5.04,5.83;7,354.54,538.68,103.09,9.02">Proceedings of the 15 th Text Retrieval Conference</title>
		<meeting>the 15 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.21,564.30,400.13,9.02;7,90.00,579.90,10.05,9.02;7,100.08,577.81,5.04,5.83;7,107.58,579.90,132.44,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,256.02,564.30,164.37,9.02">UMass at TREC 2006: Enterprise Track</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Desislava Petkova</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,428.22,564.30,77.12,9.02;7,90.00,579.90,10.05,9.02;7,100.08,577.81,5.04,5.83;7,107.58,579.90,102.97,9.02">Proceedings of the 15 th Text Retrieval Conference</title>
		<meeting>the 15 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
