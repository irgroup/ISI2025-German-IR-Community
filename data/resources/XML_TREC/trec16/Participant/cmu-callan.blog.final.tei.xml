<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.08,125.13,357.85,15.12">Retrieval and Feedback Models for Blog Distillation</title>
				<funder ref="#_CBg8dt3">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_GBJpBA5">
					<orgName type="full">DARPA PRIME</orgName>
				</funder>
				<funder ref="#_8e5T5jM">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.30,157.61,76.16,10.48"><forename type="first">Jonathan</forename><surname>Elsas</surname></persName>
							<email>jelsas@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.30,157.61,75.30,10.48"><forename type="first">Jaime</forename><surname>Arguello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.67,157.61,65.19,10.48"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.97,157.61,83.73,10.48"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.08,125.13,357.85,15.12">Retrieval and Feedback Models for Blog Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6E803082E769331F10DCA41B3EB890D9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our system and results for the Feed Distillation task in the Blog track at TREC 2007. Our experiments focus on two dimensions of the task: (1) a large-document model (feed retrieval) vs. a small-document model (entry or post retrieval) and (2) a novel query expansion method using the link structure and link text found within Wikipedia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Blog distillation (or "feed search") is the task of finding blog feeds with a principle, recurring interest in X, where X is some information need expressed as a query. Thus, the input to the system is a query and the output is ranked list of blog feeds. Tailoring a system for feed search requires making several design decisions. In this work, we explored two different decisions:</p><p>1. Is it most effective to treat this task as feed retrieval, viewing each feed as a single document; or entry retrieval, where ranked entries are aggregated into an overall feed ranking?</p><p>2. How can query expansion be appropriately performed for this task? Two different approaches are compared. The first one is based on pseudo-relevance feedback using the target collection. The second is a simple novel technique that expands the query with ngrams obtained from Wikipedia hyperlinks. Exploiting corpora other than the target corpus for query expansion has proven a valuable technique, especially for expanding difficult queries <ref type="bibr" coords="1,448.88,287.69,9.96,8.74" target="#b6">[7]</ref>.</p><p>The four runs submitted to the Blog Distillation task correspond to varying both of these dimensions. Throughout our experiments, all retrieval was done with the Indri<ref type="foot" coords="1,445.72,342.33,3.97,6.12" target="#foot_0">1</ref> retrieval engine using only terms from the topic title.</p><p>The remaining of this paper is organized as follows. Section 1 describes the pre-processing steps. Section 2 describes the target-corpus-and Wikipedia-based query expansion techniques. Section 3 describes the two retrieval models used, as well as our methods of parameter selection for the different features used in those models. Experimental results and analysis are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Corpus Pre-processing</head><p>For all of the runs submitted, we only used the information contained within the feed documents. The BLOG06 collection contains approximately 100k feed documents, which are a mix of ATOM and RSS XML. These two formats contain different XML elements which were mapped to a unified representation in order to make use of the structural elements within the feeds. We used the Universal Feed Parser 2 pack-age for Python 3 to abstract the different data elements across all feed types to a single universal representation. For details on the mapping between ATOM and RSS elements refer to the Universal Feed Parser documentation. Documents were stemmed using the Krovetz stemmer and common stop words were removed as well as manually identified web-and feed-specific stop words such as "www", "html" and "wordpress". We filtered documents that were selfidentified as non-English (in their feed.lang or channel.language elements) and feeds with fewer than 4 posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two Query Expansions Models</head><p>Query expansion is a well-studied technique used in ad hoc retrieval to improve retrieval performance, particularly for queries with insufficient content. On the TREC 2007 Blog Distillation task, the average number of words per topic title 4 was 1.91. Expanding such terse queries with as many relevant terms has a strong potential for improving precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indri's relevance model</head><p>Our first query expansion feature used Indri's built-in facilities for pseudo-relevance feedback <ref type="bibr" coords="2,91.80,468.16,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,105.08,468.16,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,115.60,468.16,7.01,8.74" target="#b3">4]</ref>. To generate our query expansion terms, we constructed a Full Dependence Model query <ref type="bibr" coords="2,91.80,492.07,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,105.14,492.07,7.75,8.74" target="#b3">4]</ref> with the terms in the topic title. 5 For all of our submissions, this query was run against the entire indexed feeds and did not take advantage of any indexed document structure. In preliminary experimentation this yielded the best re-3 http://www.python.org/ 4 Only text from the topic title was used to query the system on all 4 runs submitted to the track, as the topic title more closely resembles the types of queries a real user might submit to a search engine compared to compared to the topic description and narrative 5 Throughout these experiments, parameters for the full dependence model queries were set identically to <ref type="bibr" coords="2,289.73,625.07,8.47,6.99" target="#b4">[5]</ref>: 0.8 for the unigram feature weights and 0.1 for the window and proximity feature weights.</p><p>sults. Using this query, N = 10 documents were retrieved and a relevance model was built with those returned results. The top k = 50 most likely terms were extracted from that relevance model, and these terms constituted our relevance model query Q RM . This query was then used as a feature for our unified feed and entry queries. N and k were set to values that had previously been shown to be effective for pseudo-relevance feedback in other tasks <ref type="bibr" coords="2,414.62,205.80,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wikipedia for query expansion</head><p>Some prior work has explored using using Wikipedia for query expansion. In <ref type="bibr" coords="2,469.44,266.88,9.96,8.74" target="#b0">[1]</ref>, Collins-Thompson and Callan combine term association evidence from WordNet<ref type="foot" coords="2,417.54,289.21,3.97,6.12" target="#foot_2">6</ref> and Wikipedia<ref type="foot" coords="2,492.53,289.21,3.97,6.12" target="#foot_3">7</ref> in a Markov chain framework for query expansion. In <ref type="bibr" coords="2,323.81,314.70,9.96,8.74" target="#b7">[8]</ref>, Li et al. use Wikipedia for query expansion more directly. In their algorithm, as in our approach, each test query was run on both the target corpus and Wikipedia. Wikipedia articles were ranked differently, however, utilizing article metadata unique to Wikipedia. Each Wikipedia article belongs to one or more categories. A weight W c was assigned to each category c based on the number of articles belonging to c ranking among the top 100. Then, each Wikipedia article d was ranked by a linear combination of its original document score and W d = cat(d)∈c W c , the sum of the weights W c for each category c to which d belongs. Twenty expansion terms were selected ad hoc from the top 40 Wikipedia articles.</p><p>Wikipedia articles are available for download in their original markup language, called Wikitext, which encodes useful metadata such as the article's title and its hyperlinks. Each hyperlink contains both the title of the target page and optional anchor text. In cases where no anchor text is specified, it resolves to the title of the target page. During preprocessing, a sample of about 650, 000 articles from the English portion of the Wikipedia were indexed using Indri. Our simple algorithm was motivated by the observation that valuable expansion ngrams are contained in hyperlinks pointing to Wikipedia articles that are relevant to the base query.</p><p>First, the seed query was run against the Wikipedia corpus. The top N ranked articles were added to set D N . Then, all anchor phrases used in hyperlinks in D N were added to set A (D N ) . Note that an anchor phrase a i in A (D N ) may occur several times in D N and different occurrences of a i need not be associated with hyperlinks to the same article. For example, suppose that the seed query is space exploration. Within the top N articles, the phrase NASA may occur several times as the anchor text of a hyperlink. Some of these hyperlinks may link to the Wikipedia article on the National Aeronautics and Space Administration, while others may link to the Wikipedia article on the NASA Ames Research Center, and so forth. A single occurrence of anchor phrase a i is denoted as a ij and the target article of the hyperlink anchored by a ij is denoted as target(a ij ). Each anchor phrase a i was scored according to score(a i ) =</p><formula xml:id="formula_0" coords="3,145.81,396.03,152.93,41.56">ai j ∈A (D N ) I(rank(target(a ij )) ≤ T ) × (T -rank(target(a ij ))) .</formula><p>The identity function I(•) equals 1 if rank(target(a ij )) ≤ T and 0 otherwise. Intuitively, the score of anchor ngram a i is highest when the hyperlinks anchored by a i link to many articles that are ranked highly against the seed query. In our runs, N = 1000 and T = 500, and were selected ad hoc. Anchor ngrams occurring less than 3 times were ignored and the 20 top scoring anchor ngrams were selected. Their scores were normalized to sum to 1 and each ngram's normalized score was used to weight it with respect to the other expansion ngrams.</p><p>N and T may seem large compared to parameters typical of PRF. Intuitively, N and T play different roles. N controls the size of the search space. T controls the range of topical aspect of the ngrams considered. Thus, a large N and a small T increases the chance of finding synonyms or paraphrases of the same concept by focusing on many anchor ngrams that link to the same highly-ranked article. With larger values of T , it is expected that ngrams will relate to a wider range of topics. Larger values of T also increase the risk of extracting irrelevant ngrams. By setting N and T large in our runs, we aim for high synonym variability and broad topical aspect coverage. One natural question is how sensitive this method is to parameters N and T . Ultimately, anchor ngrams are scored proportional to the rank of their hyperlink's target page. Initial experiments varying N and T with T sufficiently large (≥ 100) showed stability in the top ranked expansion phrases.</p><p>Finally, the resulting query, Q W is given by: #weight( score(a 1 ) DM a1 score(a 2 ) DM a2 . . . score(a 20 ) DM a20 )</p><p>where DM ai is the Full-dependence model query formed with the anchor text ngram, a i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Two Retrieval Models</head><p>As described above, we investigated two models of feed retrieval: (1) the large document approach, where each feed was treated as a single document and then ranked in the typical fashion, and (2) the small document approach, where posts were the unit of retrieval and feeds were ranked based on the quantity and quality of their retrieved posts. The following sections describe these two approaches in detail and our method for selecting parameters used in these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Large Document Model</head><p>The large document approach does not distinguish between post content and number of posts within a feed. We did retain the structural elements present in feeds such as feed.title and feed.entry, but treated these as features of the monolithic feed document. We performed retrieval in a standard fashion on these feed doc-uments, utilizing the feed and entry structural elements.</p><p>The query features used in the large document model are given below:</p><p>• Full Dependence Model on the feed.title field (DM T ),</p><p>• Full Dependence Model on the feed.entry field(s) (DM E ),</p><p>• Indri's Relevance Modeling PRF (Q RM ), and</p><formula xml:id="formula_1" coords="4,96.78,218.60,159.30,9.65">• Wikipedia-based expansion (Q W ).</formula><p>and the final Indri query is as follows:</p><formula xml:id="formula_2" coords="4,120.06,256.73,151.82,24.60">#weight( λ T DM T λ E DM E λ RM Q RM λ W Q W )</formula><p>where λ i &gt; 0, λ i = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Small Document Model</head><p>The small document model views the feeds as different document collections and the entries as documents within those collections. Under this framework, the feed retrieval task can be seen as analogous to that of resource selection or ranking in federated search -given a query, find the document collections most likely to contain relevant documents.</p><p>Our approach to resource ranking was similar to the Relevant Document Distribution Estimation (ReDDE) <ref type="bibr" coords="4,177.97,463.74,9.96,8.74" target="#b5">[6]</ref>. In that approach, given known (true or estimated) collection sizes and a database of sampled documents from all collections, collections are ranked by retrieving from the sampled database and summing the document scores from that sampled retrieval. The basic ReDDE resource scoring formula for collection C j is:</p><formula xml:id="formula_3" coords="4,119.27,569.92,158.23,22.77">Rel q (j) = di∈Cj P (rel|d i )P (d i |C j )N Cj</formula><p>where P (rel|d i ) is the probability of document relevance for the query, P (d i |C j ) is the probability of selecting the document from collection C j (or, as is typically the case, from our sampled version of the true collection), and N Cj is the size of the collection.</p><p>To support a simplified federated search model of feed retrieval, we chose to create a new collection by sampling the posts from each feed. The BLOG06 corpus contains feeds ranking in size from just 1 or 2 posts to feeds with several hundred. Figure <ref type="figure" coords="4,368.29,182.08,4.98,8.74" target="#fig_0">1</ref> illustrates the distribution of feed sizes in the corpus. When creating the corpus for our federated search model, we sampled 100 posts per feed (with replacement), letting us assume a uniform N Cj = 100 ∀j. Assuming all posts are equally likely to be retrieved for each feed, i.e. P (d i |C j ) = 1/100, the above resource scoring formula simplifies to:</p><formula xml:id="formula_4" coords="4,366.47,558.97,103.47,22.77">Rel q (j) = di∈Cj P (rel|d i )</formula><p>There is one difference between our approach and the ReDDE approach. In the ReDDE approach, N Cj is the size of the original collection (possibly estimated), and not the size of the sampled collection N Cj . Here, we set N Cj = N Cj = 100 ∀j.</p><p>By doing so, a feed's original, pre-sampled size does not directly factor into the scoring function. This choice was made because the goal of the task is to find feeds with a central interest in some topic X, irrespective of feed size.</p><p>The scoring function, Rel q (j), can be easily expressed in the Indri query language:</p><formula xml:id="formula_5" coords="5,116.55,192.22,159.73,9.65">#wsum( 1.0 #combine[entry]( Q E ))</formula><p>where Q E is our entry query, the inner #combine[entry] produces scores over entries within a single feed, and the outer #wsum adds these scores to generate a feed-level score. Note that there is no #sum operator in the Indri query language, necessitating the constant 1.0 in the query, which doesn't have any effect on the final ranking.</p><p>The entry query Q E used the same pseudorelevance feedback features described above:</p><formula xml:id="formula_6" coords="5,106.65,344.28,179.53,9.65">#weight(λ E DM E λ RM Q RM λ W Q W )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Selection</head><p>The above queries have a number of free parameters that must be chosen appropriately for effective retrieval. To do this, we selected a small subset of the queries (956, 964, 966, 986, 989, 991 and two others not included in the evaluation), performed initial retrieval experiments using simple bag-of-words queries, and judged the top 50 documents retrieved as relevant/nonrelevant. We used this small training set to tune our parameters via a simple grid-search. Table <ref type="table" coords="5,91.80,511.00,4.98,8.74" target="#tab_0">1</ref> gives the parameter settings that maximized mean average precision for all runs using both retrieval models and different pseudo relevance feedback features.</p><p>Although we used a small subset of the evaluation queries to train our system, we do not believe our results are strongly biased towards these queries. Our best run's performance (CMUfeedW) on these training queries was highly variable, achieving the best performance on only one of the training queries (989) and close to our worst performance on several  Retrieval performance was superior with Wikipedia-based query expansion than without. Adding Wikipedia-based expansion improved performance in 30/45 queries under the small document model and 34/45 queries under the large document model. The largest improvement under both document models, based on average precision, was for query 967, home baking. An improvement of 6, 780% was achieved under the small document model and 683% under the large document model. Table <ref type="table" coords="5,447.19,618.72,3.87,8.74" target="#tab_4">3</ref>, shows the expansion terms obtained in descending order of confidence for both retrieval models.   One limitation of our Wikipedia-based approach is that its parameters (e.g., the number of expansion terms) remain constant irrespective of the seed query. This is troublesome in cases where the topic drifts rapidly down the ranked list of Wikipedia articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results &amp; Discussion</head><p>In conclusion, our experimental results showed that the large document approach outperformed the small document approach for this task. Additionally, the simple method of finding query expansion terms and phrases from Wikipedia proved to be effective across runs. The two retrieval models and the Wikipedia feedback model present interesting research questions. Alternate sampling and rank aggregation methods may improve the performance of the small document model. The use of anchor text for query expansion could be explored further, beyond Wikipedia and feed distillation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,347.29,431.84,136.60,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Blog size distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,91.80,238.45,209.22,8.74;6,91.80,250.40,206.53,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Best &amp; Median AP per query compared to CMUfeedW (ordered by CMUfeedW).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,310.98,96.99,209.22,138.24"><head>Table 1 :</head><label>1</label><figDesc>Query weight settings. RM=Relevance Model PRF, W=Wikipedia PRF others. Figure2shows the performance of this run with the training queries clearly indicated.</figDesc><table coords="5,322.54,96.99,185.13,52.49"><row><cell>Model</cell><cell>PRF</cell><cell>λT</cell><cell cols="3">λE λRM λW</cell></row><row><cell>large-doc</cell><cell cols="3">RM RM+W 0.2 0.3 0.2 0.6</cell><cell>0.2 0.1</cell><cell>-0.4</cell></row><row><cell>small-doc</cell><cell>RM RM+W</cell><cell>--</cell><cell>0.3 0.3</cell><cell>0.7 0.5</cell><cell>-0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,310.98,272.40,209.22,182.67"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="5,310.98,272.40,209.22,182.67"><row><cell cols="4">shows the performance of our four</cell></row><row><cell cols="4">runs: large document (CMUfeed) vs. small</cell></row><row><cell cols="4">document (CMUentry) retrieval models and the</cell></row><row><cell cols="4">Wikipedia (*W) expansion model. The large</cell></row><row><cell cols="4">document model clearly outperformed the small</cell></row><row><cell cols="4">document model, and Wikipedia-based expan-</cell></row><row><cell cols="4">sion improved average performance of all runs.</cell></row><row><cell cols="4">Figure 2 shows our best run (CMUfeedW) com-</cell></row><row><cell cols="4">pared to the per-query best and median average</cell></row><row><cell>precision values.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>MAP</cell><cell>R-prec</cell><cell>P10</cell></row><row><cell>CMUfeed</cell><cell>0.3385</cell><cell>0.4087</cell><cell>0.4733</cell></row><row><cell>CMUfeedW</cell><cell cols="3">0.3695 0.4245 0.5356</cell></row><row><cell>CMUentry</cell><cell>0.2453</cell><cell>0.3277</cell><cell>0.4089</cell></row><row><cell>CMUentryW</cell><cell>0.2552</cell><cell>0.3384</cell><cell>0.4267</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,338.78,468.09,153.62,8.74"><head>Table 2 :</head><label>2</label><figDesc>Performance of our 4 runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,91.80,419.73,209.21,32.65"><head>Table 3 :</head><label>3</label><figDesc>Top 10 expansion terms/phrases for topic 967, home baking, for both of our expansion models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,326.22,634.49,108.46,6.99"><p>http://www.lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,326.22,644.00,82.40,6.99"><p>http://feedparser.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,326.22,634.49,113.40,6.99"><p>http://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="2,326.22,644.00,100.92,6.99"><p>http://www.wikipedia.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the eRulemaking project and <rs type="funder">NSF</rs> grant <rs type="grantNumber">IIS-0240334</rs>, and <rs type="funder">DARPA</rs> contract <rs type="grantNumber">IBM W0550432</rs> (<rs type="funder">DARPA PRIME</rs> Agreement # <rs type="grantNumber">HR0011-06-2-0001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CBg8dt3">
					<idno type="grant-number">IIS-0240334</idno>
				</org>
				<org type="funding" xml:id="_8e5T5jM">
					<idno type="grant-number">IBM W0550432</idno>
				</org>
				<org type="funding" xml:id="_GBJpBA5">
					<idno type="grant-number">HR0011-06-2-0001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,326.48,269.41,193.72,8.74;6,326.48,281.37,193.72,8.74;6,326.48,293.32,120.49,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,326.48,281.37,189.01,8.74">Query expansion using random walk models</title>
		<author>
			<persName coords=""><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,338.93,293.32,78.02,8.74">Proc of CIKM &apos;05</title>
		<meeting>of CIKM &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,326.48,311.43,193.72,8.74;6,326.48,323.38,193.72,8.74;6,326.48,335.34,193.72,8.74;6,326.48,347.30,105.16,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,502.35,311.43,17.85,8.74;6,326.48,323.38,133.70,8.74">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,487.54,323.38,32.67,8.74;6,326.48,335.34,41.90,8.74">Proc of SIGIR 01</title>
		<meeting>of SIGIR 01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,326.48,365.40,193.72,8.74;6,326.48,377.36,193.72,8.74;6,326.48,389.31,118.15,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,371.72,377.36,144.32,8.74">Indri at trec 2004: Terabyte track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,338.93,389.31,75.37,8.74">Proc of TREC 04</title>
		<meeting>of TREC 04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,326.48,407.42,193.72,8.74;6,326.48,419.38,193.72,8.74;6,326.48,431.33,118.15,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,371.72,419.38,144.32,8.74">Indri at trec 2005: Terabyte track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,338.93,431.33,75.37,8.74">Proc of TREC 05</title>
		<meeting>of TREC 05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,326.48,449.44,193.72,8.74;6,326.48,461.40,193.72,8.74;6,326.48,473.35,193.72,8.74;6,326.48,485.31,175.48,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,512.73,449.44,7.47,8.74;6,326.48,461.40,193.72,8.74;6,326.48,473.35,29.59,8.74">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,375.42,473.35,73.52,8.74">Proc of SIGIR 05</title>
		<meeting>of SIGIR 05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,326.48,503.42,193.72,8.74;6,326.48,515.37,193.72,8.74;6,326.48,527.33,193.72,8.74;6,326.48,539.28,193.72,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,437.30,503.42,82.90,8.74;6,326.48,515.37,193.72,8.74;6,326.48,527.33,35.91,8.74">Relevant document distribution estimation method for resource selection</title>
		<author>
			<persName coords=""><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,386.29,527.33,77.45,8.74">Proc of SIGIR 03</title>
		<meeting>of SIGIR 03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,326.48,557.39,193.72,8.74;6,326.48,569.35,193.72,8.74;6,326.48,581.30,57.30,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,420.77,557.39,99.42,8.74;6,326.48,569.35,88.72,8.74">Trec report: The trec robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,433.32,569.35,86.87,8.74;6,326.48,581.30,26.08,8.74">Proc of ACM SIGIR Forum</title>
		<meeting>of ACM SIGIR Forum</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,326.48,599.41,193.72,8.74;6,326.48,611.37,193.72,8.74;6,326.48,623.32,193.72,8.74;6,326.48,635.28,73.84,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,364.93,611.37,155.26,8.74;6,326.48,623.32,139.83,8.74">Improving weak ad-hoc queries using wikipedia as external corpus</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W P</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">L</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,488.64,623.32,31.56,8.74;6,326.48,635.28,43.83,8.74">Proc of SIGIR &apos;07</title>
		<meeting>of SIGIR &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
