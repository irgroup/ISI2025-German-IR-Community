<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.97,83.88,301.76,15.49;1,233.36,105.80,142.99,15.49;1,178.51,129.46,252.71,12.90">Bootstrapping Language Associated with Biomedical Entities The AID Group at TREC Genomics 2007</title>
				<funder ref="#_gEPnjfc">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_uDQM2bP">
					<orgName type="full">Dutch Ministry of Education, Culture and Science (OC&amp;W)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.86,160.20,58.77,10.76"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
							<email>emeij@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.88,160.20,86.96,10.76"><forename type="first">Sophia</forename><surname>Katrenko</surname></persName>
							<email>katrenko@science.uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.97,83.88,301.76,15.49;1,233.36,105.80,142.99,15.49;1,178.51,129.46,252.71,12.90">Bootstrapping Language Associated with Biomedical Entities The AID Group at TREC Genomics 2007</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7F061755D06A3696D75CB8BF54C5162</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Genomics 2007 task included recognizing topic-specific entities in the returned passages. To address this task, we have designed and implemented a novel data-driven approach by combining information extraction with language modeling techniques. Instead of using an exhaustive list of all possible instances for an entity type, we look at the language usage around each entity type and use that as a classifier to determine whether or not a piece of text discusses such an entity type. We do so by comparing it with language models of the passages. E.g., given the entity type "genes", our approach can measure the gene-iness of a piece of text.</p><p>Our algorithm works as follows. Given an entity type, it first uses Hearst patterns to extract instances of the type. To extract more instances, we look for new contextual patterns around the instances and use them as input for a bootstrapping method, in which new instances and patterns are discovered iteratively. Afterwards, all discovered instances and patterns are used to find the sentences in the collection which are most on par with the requested entity type. A language model is then generated from these sentences and, at retrieval time, we use this model to rerank retrieved passages.</p><p>As to the results of our submitted runs, we find that our baseline run performs well above the median of all participant's scores. Additionally, we find that applying our proposed method helps those entity types most for which there are unambiguous patterns and numerous instances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our aim for this year's TREC Genomics track was to experiment with a statistical language modeling approach to entity recognition. This year's topics each contain an explicit entity type, of which instances need to be retrieved within the returned passages. To this end, we take the results of a baseline retrieval run and rerank the passages according to the divergence of their language models with the language model of the requested entity type, which we acquire through a bootstrapping approach. Additionally, we report on a run which selects the most relevant sentences from the 10,000 highest ranking paragraphs. The remainder of this paper is organized as follows. Section 2 describes the retrieval model we employ and the various preprocessing steps we have applied. Section 3 describes our entity recognition algorithm in more detail. In Section 4 we present the experimental setup and we elaborate on the details of the runs we have submitted. In Section 5 we report on their performance and we end with a concluding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Model</head><p>In all our experiments we adopt a standard query-likelihood approach <ref type="bibr" coords="1,355.69,504.18,10.79,8.97" target="#b3">[4,</ref><ref type="bibr" coords="1,368.87,504.18,7.47,8.97" target="#b5">6,</ref><ref type="bibr" coords="1,378.74,504.18,7.19,8.97" target="#b8">9]</ref>, which means we rank documents according to their likelihood of generating the query. Assuming query terms to be independent:</p><formula xml:id="formula_0" coords="1,379.98,544.23,175.94,24.15">P(Q|D) ∝ ∏ q∈Q P(q|θ D ),<label>(1)</label></formula><p>where θ D is a language model of document D, and q the individual query terms in query Q. P(•|θ D ) can be estimated using maximum-likelihood estimates, which means using the frequency of a query term in a document: P(q|θ D ) = c(q, D)/|D|. Here, c(q, D) indicates the count of term q in document D and |D| the length of the particular document. However, to avoid zero probabilities, we instead smooth this esti mate using a Dirichlet prior <ref type="bibr" coords="1,445.09,664.74,10.79,8.97" target="#b0">[1,</ref><ref type="bibr" coords="1,458.28,664.74,11.83,8.97" target="#b10">11]</ref>, which is formulated as:</p><formula xml:id="formula_1" coords="1,359.14,695.97,196.78,25.38">P(Q|D) = ∏ q∈Q c(q, D) + µP(q|θ C ) |D| + µ ,<label>(2)</label></formula><p>where θ C is the language model of a large reference corpus C (such as the collection) and µ a constant by which to tune the influence of the reference model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entity Recognition</head><p>Our working hypothesis is that we can use the language model associated with an entity type as a classifier to determine whether some piece of text discusses that entity. Instead of looking for explicit instances of a particular type, we are observing the language use around it. In other words, what is the language that people use, when they are talking about a particular entity? In a way, this approach can be construed as a different approach to biasing relevance models. Recent work showed that biasing the generation of a query model towards queryspecific MeSH terms has a positive effect on retrieval performance <ref type="bibr" coords="2,93.98,276.73,10.79,8.97" target="#b6">[7,</ref><ref type="bibr" coords="2,107.35,276.73,7.19,8.97" target="#b7">8]</ref>. However, instead of generating a relevance model for an entire query, we are now using a reranking approach geared specifically towards the requested entity type for a query.</p><p>The main problem is how to determine the parameters of the language model for an entity type. We approach this problem by starting out with a bootstrapping approach-which has been used succesfully for named entity recognition tasks in the past <ref type="bibr" coords="2,97.90,375.36,10.79,8.97" target="#b1">[2,</ref><ref type="bibr" coords="2,111.46,375.36,11.83,8.97" target="#b9">10]</ref>. In this approach, one begins with an initial pool of instances of named entities and an empty pool of contextual patterns. In each iteration, the patterns with the highest score are identified and added to the pattern pool. Further, the patterns from this pool are used to extract new entities of the same type. In our setting, we define a contextual pattern based on the immediate context of a given entity (two tokens to the right and left of it) in the documents in the collection. We adopt the scoring scheme proposed by Thelen and Riloff <ref type="bibr" coords="2,81.43,497.90,16.60,8.97" target="#b9">[10]</ref> to rank patterns and entity candidates. Given that a pattern p i extracts W i words, E i of which are known entities, its score is calculated as:</p><formula xml:id="formula_2" coords="2,109.38,538.86,183.52,23.48">score P (p i ) = E i W i • log 2 (E i ).<label>(3)</label></formula><p>Thelen and Riloff <ref type="bibr" coords="2,129.81,569.99,16.60,8.97" target="#b9">[10]</ref> suggest adding N patterns with the highest score P to the pattern pool. In our experiments it turned out to be sufficient to add all patterns which have a non-zero score. In addition, we also discard all patterns which consist of stop words only, since they do not provide enough evidence to be used for accurate entity recognition.</p><p>Once the patterns are added to the pattern pool, they can subsequently be used to extract new entities. An entity candidate w i is considered to be good if it is covered by many patterns for an entity type, consequently: where E j is the number of distinct entities extracted by pattern p j and M is the number of all patterns that extract w i . Then, the top 5 candidates are added to the entity pool. The procedure of pattern/entity selection is repeated until it reaches a certain threshold.</p><formula xml:id="formula_3" coords="2,96.41,698.09,196.50,26.36">score W (w i ) = ∑ M j=1 log 2 (E j + 1) M ,<label>(4)</label></formula><p>As there is no information as to which entities are frequent enough to start the bootstrapping process, we use Hearst patterns <ref type="bibr" coords="2,338.77,359.86,11.61,8.97" target="#b2">[3]</ref> to extract the initial list of entities. The Hearst patterns we employ are the following:</p><formula xml:id="formula_4" coords="2,316.81,371.81,239.10,20.92">such [ENTITY TYPE]s as *, [ENTITY TYPE]s such as *, * is a [ENTITY TYPE], *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and other [ENTITY TYPE]s, [ENTITY TYPE]s including *, [EN-</head><p>TITY TYPE]s, especially *. In the abovementioned patterns, the wildcard stands for instances of the entity. We do not use any form of parsing and, thus, multi-term entities are not considered.</p><p>Some examples of the final patterns per entity type are given in Table <ref type="table" coords="2,351.45,470.73,3.74,8.97" target="#tab_0">1</ref>. We observe that some patterns are quite specific, whereas other refer to the entities of more than one topic. For instance, that the * is caused can be used in context of a disease name as well as in the context of the symptoms. Such ambiguous patterns might cause problems while creating a language model of a given topic. In Section 5 we provide some per-topic details as to the results of this approach. Now that we have a set of patterns and entities per entity type E, we retrieve the S most relevant sentences from the collection and create a language model by sampling i.i.d. from them:</p><formula xml:id="formula_5" coords="2,366.36,610.48,185.68,24.25">P(t|θ E ) = ∑ s∈S P(s|θ E ) • P(t|s), (<label>5</label></formula><formula xml:id="formula_6" coords="2,552.04,616.03,3.87,8.97">)</formula><p>where t denotes a vocabulary term. Then, at retrieval time, we use this model as a classifier by reranking an initial set of passages d according to the KL-divergence with this model:</p><formula xml:id="formula_7" coords="2,346.40,693.03,209.52,24.80">D kl (θ E ||θ D ) = ∑ t P(t|θ E ) • log P(t|θ E ) P(t|θ D ) .<label>(6)</label></formula><p>In this section we detail the specifics of our experiments as well as our submitted runs. All topics are morphologically normalized as described by Huang et al. <ref type="bibr" coords="3,117.16,374.46,11.61,8.97" target="#b4">[5]</ref> and stemmed using a Porter stemmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Passage Identification</head><p>The main task for the 2007 TREC Genomics track is passage retrieval, for which we use the paragraphs in the documents. Additionally, we experiment with a more focused approach. First, 10,000 paragraps are obtained using the querylikelihood approach with Dirichlet smoothing as described in Eq. 2. Then, we look at the individual sentences within those paragraphs and determine their relevance-again using Eq. 2-and the most relevant ones are returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runs</head><p>The three runs we have submitted have the following characteristics:</p><p>AIDrun1 baseline run, using paragraphs only, ranked according to Eq. 2. The smoothing parameter µ in Eq. 2 is set to 100 and P(d) is assumed to be uniform.</p><p>AIDrun2 same as AIDrun1, but in this run we return the most relevant sentences from the top 10,000 paragraphs, as detailed in subsection 4.2.</p><p>AIDrun3 same as AIDrun1, with the top 1,000 results reranked using the algorithm described in Section 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><formula xml:id="formula_8" coords="4,141.28,68.11,309.40,164.38">+ - + + PROTEINS ± ± ± ± GENES ± ± ± ± DRUGS - - - - CELL OR TISSUE TYPES - - - - SIGNS OR SYMPTOMS - - ± ± TOXICITIES ± ± 0 0 BIOLOGICAL SUBSTANCES 0 0 0 0 ANTIBODIES - + 0 0 DISEASES 0 0 0 0 PATHWAYS 0 0 0 0 MOLECULAR FUNCTIONS - - ± ± STRAINS - - 0 0 TUMOR TYPES + - 0 0</formula><p>Table <ref type="table" coords="4,216.40,246.27,3.88,8.97">3</ref>: Impact of bootstrapping: AIDrun3 vs. AIDrun1</p><p>Figure <ref type="figure" coords="4,82.75,276.32,4.98,8.97">2</ref> (next page) displays the difference of our baseline run, AIDrun1, as compared to the median scores of all participants. Looking at the overall picture, our run seems to improve over the median on almost all topics, except for topics 220 and 221.</p><p>Unfortunately, the retrieved instances for the entity types from the topics were not directly evaluated and, thus, we can only report on the end-to-end retrieval performance on the various measures. The results of the run employing our proposed approach to entity recognition (AIDrun3) as compared to the baseline (AIDrun1) can be found in Figure <ref type="figure" coords="4,287.92,400.13,4.98,8.97" target="#fig_0">3</ref> (next page). Our hypothesis is that the language models for the entity types PROTEINS and GENES are the most accurate. This hypothesis is based on the results of the bootstrapping process. Protein and gene names are often mentioned in text and this results in a high number of contextual patterns.</p><p>In contrast, instances of PATHWAYS or STRAINS are more difficult to detect. To verify this hypothesis, we perform a more elaborate comparison of AIDrun3 against our baseline run, AIDrun1. In Table <ref type="table" coords="4,146.78,507.73,3.74,8.97">3</ref>, + stands for the positive impact on all topics corresponding to a particular entity type, ± means a partially positive impact (on some topics but not all of them), -presents a decrease on a topic, and 0 stands for no change compared against AIDrun1. As expected, PRO-TEINS, GENES and MUTATIONS are the topics which gain from our proposed method most. Note, however, that the distribution of queries is not uniform, i.e. some entity types are represented by one query only (ANTIBODIES, DISEASES, STRAINS and TUMOR TYPES), while some other are more frequent (e.g., PROTEINS, GENES).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>For our participation in this year's TREC Genomics track, we experimented with a language modeling approach to rec-ognizing entity types. Instead of using a more or less extensive list of possible instances, we look at the language usage associated with an entity type to detect whether or not a piece of text discusses such an entity. To this end, we have developed a model which uses a bootstrapping approach to iteratively look for new contextual patterns and instances of a particular entity type. Then, we retrieve sentences from the test collection using the found patterns and instances and construct a language model by sampling from those sentences. At retrieval time, we rerank found passages by the divergence of their respective language models with the language model of the requested entity type. We hypothesized that our approach works best for entity types which have many unambiguous instances and contextual patterns. To test this hypothesis, we take a baseline run-which performs well above the median of all participant's scores on itself-and apply our proposed method to it. The results of this run indicate that our approach does indeed help those entity types for which there are unambiguous patterns and numerous instances most.</p><p>-  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,59.42,704.96,99.95,8.97;5,197.54,704.96,352.76,8.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The difference AIDrun3 and AIDrun1, sorted decreasingly. The labels indicate the associated topic id's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,322.79,56.97,217.74,194.28"><head>Table 1 :</head><label>1</label><figDesc>Examples of patterns</figDesc><table coords="2,322.79,56.97,217.74,171.35"><row><cell cols="2">ENTITY TYPE PATTERNS</cell></row><row><cell>GENES</cell><cell>expression of * in the, of the * gene and,</cell></row><row><cell></cell><cell>clusters of * can be</cell></row><row><cell>PROTEINS</cell><cell>effect on * binding to,</cell></row><row><cell></cell><cell>cleavage of * was observed,</cell></row><row><cell></cell><cell>associated with * and the,</cell></row><row><cell>DISEASES</cell><cell>episodes of * in patients,</cell></row><row><cell></cell><cell>patients with * compared with,</cell></row><row><cell></cell><cell>treatment of * in children</cell></row><row><cell>DRUGS</cell><cell>doses of * in human, effect of * therapy on,</cell></row><row><cell cols="2">MUTATIONS if the * mutation is, that the * mutation was</cell></row><row><cell>CELL OR</cell><cell>cells and * in vivo, in the * cell layer,</cell></row><row><cell cols="2">TISSUE TYPE studies of * maturation have</cell></row><row><cell>STRAINS</cell><cell>in the * strain is, bred to * mice to</cell></row><row><cell>SIGNS OR</cell><cell>recovery from * can take, that the * is caused</cell></row><row><cell>SYMPTOMS</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,316.81,57.12,239.10,78.56"><head>Table 2 :</head><label>2</label><figDesc>The results of our submitted runs (best scores in boldface).</figDesc><table coords="3,325.98,57.12,220.82,43.87"><row><cell></cell><cell cols="4">DOCUMENT ASPECT PASSAGE PASSAGE2</cell></row><row><cell>AIDrun1</cell><cell>0.241</cell><cell>0.156</cell><cell>0.064</cell><cell>0.069</cell></row><row><cell>AIDrun2</cell><cell>0.195</cell><cell>0.088</cell><cell>0.071</cell><cell>0.025</cell></row><row><cell>AIDrun3</cell><cell>0.154</cell><cell>0.085</cell><cell>0.039</cell><cell>0.040</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,316.81,192.63,239.11,524.68"><head>Table 2</head><label>2</label><figDesc>lists the results of our submitted runs. As is clear from this table, the baseline run performs best on all accounts, except for the PASSAGE evaluation measure. The effect on this particular measure is a clear artefact of its nature, which favours shorter passages. Figure1gives a visual representation of the per-topic differences for AIDrun2 versus AIDrun1 in terms of PASSAGE and PASSAGE2 MAP respectively. From these graphs it's clear what the difference is between these measures on returning sentences instead of full paragraphs.</figDesc><table coords="3,324.82,332.66,228.39,333.85"><row><cell></cell><cell></cell><cell>Passage</cell></row><row><cell></cell><cell>0.50</cell><cell></cell></row><row><cell></cell><cell>0.40</cell><cell></cell></row><row><cell></cell><cell>0.30 0.20</cell><cell>222 209 213</cell></row><row><cell>∆ MAP</cell><cell>-0.20 -0.10 0.10 0.00</cell><cell>226 219 208 218 206 215 227 200 205 216 232 223 234 212 207 225 220 202 211 231 201 233 228 217 221 229 210 235 224 203 204 230 214</cell></row><row><cell></cell><cell>-0.30</cell><cell></cell></row><row><cell></cell><cell>-0.40</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Passage2</cell></row><row><cell></cell><cell>0.50</cell><cell></cell></row><row><cell></cell><cell>0.40</cell><cell></cell></row><row><cell></cell><cell>0.30</cell><cell></cell></row><row><cell></cell><cell>0.20</cell><cell></cell></row><row><cell>∆ MAP</cell><cell>-0.30 -0.20 -0.10 0.10 0.00</cell><cell>209 205 223 208 225 220 202 207 231 211 232 216 219 233 206 227 217 228 201 221 226 218 210 213 224 234 222 215 212 200 229 230 203 235 214 204</cell></row><row><cell></cell><cell>-0.40</cell><cell></cell></row></table><note coords="3,316.81,684.44,57.42,8.97;3,419.39,684.44,136.53,8.97;3,316.81,696.39,239.10,8.97;3,316.81,708.35,200.56,8.97"><p>Figure 1: The between AIDrun2 and AIDrun1 in terms of both the passage evaluation metrics, sorted decreasingly. The labels indicate the associated topic id's.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,53.80,63.00,502.12,624.40"><head></head><label></label><figDesc>The difference between AIDrun1 and the median of the scores of all participants, sorted decreasingly. The labels indicate the associated topic id's.</figDesc><table coords="5,53.80,63.00,448.77,624.40"><row><cell></cell><cell></cell><cell>Document</cell><cell></cell><cell></cell><cell>Aspect</cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell>0.50</cell><cell></cell></row><row><cell>∆ MAP</cell><cell>-0.20 -0.10 0.00 0.10 0.20 0.30 0.40</cell><cell>229 214 235 213 232 224 215 204 200 205 210 209 219 226 218 233 228 206 230 234 208 203 222 217 223 207 216 202 225 231 212 227 201 211</cell><cell>∆ MAP</cell><cell>-0.20 -0.10 0.00 0.10 0.20 0.30 0.40</cell><cell>214 229 224 209 235 204 232 228 206 219 234 200 215 222 205 203 213 216 217 210 207 233 223 202 208 201 225 231 211 218 227 226 230 212</cell></row><row><cell></cell><cell>-0.40 -0.30</cell><cell>220 221</cell><cell></cell><cell>-0.40 -0.30</cell><cell>221 220</cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell>-0.50</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Passage</cell><cell></cell><cell></cell><cell>Passage2</cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell>0.50</cell><cell></cell></row><row><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell>0.40</cell><cell></cell></row><row><cell>∆ MAP</cell><cell>-0.10 0.00 0.10 0.20 0.30</cell><cell>235 214 229 200 204 203 213 224 222 215 219 234 210 232 228 230 217 206 216 233 218 223 226 207 212 205 202 225 208 231 211 201 209 227 220</cell><cell>∆ MAP</cell><cell>-0.10 0.00 0.10 0.20 0.30</cell><cell>204 235 203 214 229 200 222 209 230 215 213 212 234 224 219 210 226 218 206 232 228 217 208 233 216 223 207 205 231 225 202 201 211 227 220 221</cell></row><row><cell></cell><cell>-0.20</cell><cell>221</cell><cell></cell><cell>-0.20</cell><cell></cell></row><row><cell></cell><cell>-0.30</cell><cell></cell><cell></cell><cell>-0.30</cell><cell></cell></row><row><cell></cell><cell>-0.40</cell><cell></cell><cell></cell><cell>-0.40</cell><cell></cell></row><row><cell></cell><cell>-0.50</cell><cell></cell><cell></cell><cell>-0.50</cell><cell></cell></row><row><cell cols="2">0.40 0.50 Figure 2: -0.70 -0.60 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0.10 0.20 0.30 ∆ MAP</cell><cell>Document 221 222 234 202 217 220 225 231 218 213 227 207 216 226 212 201 228 210 233 235 211 205 219 229 232 223 200 203 215 224 230 206 208 209 214 204</cell><cell>∆ MAP</cell><cell>-0.70 0.40 0.50 -0.30 -0.20 -0.10 0.10 0.20 0.30 0.00 -0.40 -0.60 -0.50</cell><cell>Aspect 217 231 210 202 220 225 211 207 233 227 215 216 218 226 213 212 208 221 234 201 205 235 206 223 222 219 200 203 230 228 232 224 229 209 204 214</cell></row><row><cell></cell><cell></cell><cell>Passage</cell><cell></cell><cell></cell><cell>Passage2</cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell>0.50</cell><cell></cell></row><row><cell></cell><cell>0.40</cell><cell></cell><cell></cell><cell>0.40</cell><cell></cell></row><row><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell>0.30</cell><cell></cell></row><row><cell>∆ MAP</cell><cell>-0.30 -0.20 -0.10 0.00 0.10 0.20</cell><cell>212 201 213 210 227 226 218 216 205 231 233 207 211 225 220 202 217 221 223 234 206 232 228 208 215 219 224 235 200 222 203 229 230 204 214 209</cell><cell>∆ MAP</cell><cell>-0.30 -0.20 -0.10 0.10 0.20 0.00</cell><cell>212 213 210 227 201 218 226 216 205 231 211 207 225 220 202 233 217 223 234 221 206 232 228 208 215 219 224 200 222 235 203 229 230 209 214 204</cell></row><row><cell></cell><cell>-0.40</cell><cell></cell><cell></cell><cell>-0.40</cell><cell></cell></row><row><cell></cell><cell>-0.50</cell><cell></cell><cell></cell><cell>-0.50</cell><cell></cell></row><row><cell></cell><cell>-0.60</cell><cell></cell><cell></cell><cell>-0.60</cell><cell></cell></row><row><cell></cell><cell>-0.70</cell><cell></cell><cell></cell><cell>-0.70</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>We would like to thank <rs type="person">Maarten de Rijke</rs>, <rs type="person">Willem van Hage</rs>, <rs type="person">Scott Marshall</rs>, and <rs type="person">Marco Roos</rs> for their contributions and many insightful discussions. This work was carried out in the context of the <rs type="programName">Virtual Laboratory for e-Science project</rs> (http://www.vl-e.nl). This project is supported by a <rs type="grantName">BSIK grant</rs> from the <rs type="funder">Dutch Ministry of Education, Culture and Science (OC&amp;W)</rs> and is part of the <rs type="programName">ICT innovation program</rs> of the <rs type="institution">Ministry of Economic Affairs (EZ)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uDQM2bP">
					<orgName type="grant-name">BSIK grant</orgName>
					<orgName type="program" subtype="full">Virtual Laboratory for e-Science project</orgName>
				</org>
				<org type="funding" xml:id="_gEPnjfc">
					<orgName type="program" subtype="full">ICT innovation program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,70.40,79.22,222.50,8.97;6,65.42,91.18,227.49,8.97;6,65.42,103.13,62.54,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,217.09,79.22,75.81,8.97;6,65.42,91.18,188.38,8.97">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,272.44,91.18,15.35,8.97">ACL</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,123.06,222.50,8.97;6,65.42,135.01,181.51,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,207.13,123.06,85.77,8.97;6,65.42,135.01,112.22,8.97">Unsupervised models for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,196.00,135.01,46.98,8.97">EMNLP &apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,154.94,222.50,8.97;6,65.42,166.89,163.46,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,149.25,154.94,143.65,8.97;6,65.42,166.89,90.18,8.97">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,174.08,166.89,50.85,8.97">COLING &apos;92</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,186.82,222.51,8.97;6,65.42,198.77,207.01,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,156.10,186.82,136.80,8.97;6,65.42,198.77,65.14,8.97">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="6,70.40,218.70,222.50,8.97;6,65.42,230.66,227.48,8.97;6,65.42,242.61,125.65,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,229.51,218.70,63.39,8.97;6,65.42,230.66,126.91,8.97">York University at TREC 2005 Genomics track</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,215.99,230.66,76.90,8.97;6,65.42,242.61,121.28,8.97">Proceedings of the 14th Text Retrieval Conference</title>
		<meeting>the 14th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,262.54,222.50,8.97;6,65.42,274.49,226.93,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,144.47,262.54,148.43,8.97;6,65.42,274.49,85.07,8.97">Variations on Language Modeling for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="6,70.40,294.42,222.50,8.97;6,65.42,306.37,227.48,8.97;6,65.42,318.33,227.48,8.97;6,65.42,330.28,171.42,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,214.95,294.42,77.95,8.97;6,65.42,306.37,227.48,8.97;6,65.42,318.33,58.14,8.97">Integrating conceptual knowledge into relevance models: A model and estimation method</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,143.50,318.33,149.40,8.97;6,65.42,330.28,112.47,8.97">International Conference on the Theory of Information Retrieval</title>
		<imprint>
			<date type="published" when="2007">2007a. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,350.21,222.50,8.97;6,65.42,362.16,227.49,8.97;6,65.42,374.12,227.48,8.97;6,65.42,386.07,139.61,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,225.96,350.21,66.94,8.97;6,65.42,362.16,227.49,8.97;6,65.42,374.12,21.91,8.97">Thesaurus-based feedback to support mixed search and browsing environments</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,107.44,374.12,185.46,8.97;6,65.42,386.07,80.12,8.97">Proceedings of the 11th European Conference on Digital Libraries</title>
		<meeting>the 11th European Conference on Digital Libraries</meeting>
		<imprint>
			<publisher>ECDL</publisher>
			<date type="published" when="2007">2007b. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,406.00,222.50,8.97;6,65.42,417.95,213.92,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,223.37,406.00,69.53,8.97;6,65.42,417.95,152.86,8.97">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,236.17,417.95,39.22,8.97">SIGIR &apos;98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,437.88,217.52,8.97;6,65.42,449.83,227.48,8.97;6,65.42,461.79,131.00,8.97" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,229.12,437.88,63.78,8.97;6,65.42,449.83,227.48,8.97;6,65.42,461.79,61.76,8.97">A boostrapping method for learning semantic lexicons using extraction pattern contexts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<idno>EMNLP &apos;02</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,481.71,217.52,8.97;6,65.42,493.67,227.48,8.97;6,65.42,505.62,110.71,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="6,206.04,481.71,86.86,8.97;6,65.42,493.67,227.48,8.97;6,65.42,505.62,49.66,8.97">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<idno>SIGIR &apos;01</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
