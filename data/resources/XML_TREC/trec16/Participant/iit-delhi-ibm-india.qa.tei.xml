<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,86.76,105.47,420.91,15.68;1,81.48,127.43,431.51,15.68">IITD-IBMIRL System for Question Answering using Pattern Matching, Semantic Type and Semantic Category Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-10-17">October 17, 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,78.00,159.43,158.91,11.42"><roleName>Ganesh</roleName><forename type="first">Ashish</forename><forename type="middle">Kumar</forename><surname>Saxena</surname></persName>
							<email>ashish.ksaxena@yahoo.co.in</email>
						</author>
						<author>
							<persName coords="1,240.85,159.43,95.73,11.42"><forename type="first">Viswanath</forename><surname>Sambhu</surname></persName>
						</author>
						<author>
							<persName coords="1,346.94,159.43,127.27,11.42"><forename type="first">L</forename><forename type="middle">Venkata</forename><surname>Subramaniam</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM India Research Lab</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,490.46,159.43,72.79,11.42"><forename type="first">Saroj</forename><surname>Kaushik</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<addrLine>New Delhi</addrLine>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,86.76,105.47,420.91,15.68;1,81.48,127.43,431.51,15.68">IITD-IBMIRL System for Question Answering using Pattern Matching, Semantic Type and Semantic Category Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-10-17">October 17, 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">D7EB4A8AC9DA94420647FFDA099ABC71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Question Answering (QA) system aims to return exact answers to natural language questions. While today information retrieval techniques are quite successful at locating within large collections of documents those that are relevant to a user's query, QA techniques that extract the exact answer from these retrieved documents still do not obtain very good accuracies. We approached the TREC 2007 Question Answering task as a semantics based question to answer matching problem. Given a question we aimed to extract the relevant semantic entities in it so that we can pin-point the answer. In this paper we show that our technique obtains reasonable accuracy compared to other systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the TREC QA task is to foster research on QA systems to improve the state of the art. A QA system works by returning exact answers to natural language questions. A question such as "In 2003 who was the Secretary General of the United Nations," has only one exact answer. Given such a question a QA system should return the exact answer to it.</p><p>Our team took part in the TREC QA main task of 2007. This was the first time our team participated in the QA task. The main task was the same as in TREC 2006, in that the test set consisted of question series where each series asked for information regarding a particular target. These targets included people, organizations, events and other entities. The questions on each target comprised of three types, factoid questions, list questions and one "Other" question. Factoid questions have exactly one correct answer. A list question has a list as its answer and the answer to the "Other" question is to be interesting information about the target that is not covered by the preceding factoid and list questions in the series.</p><p>The major difference between the 2007 main task and the 2006 main task was that questions were asked over both blog documents and newswire articles, rather than just newswire. A blog document is defined to be a blog post and its follow-up comments (a permalink). The blog collection contains well-formed English as well as badly-formed English and spam, and mining blogs for answers introduced significant new challenges in at least two aspects that are very important for functional QA systems: 1) being able to handle language that is not well-formed, and 2) dealing with discourse structures that are more informal and less reliable than newswire.</p><p>In this paper we describe our approach to the TREC 2007 QA Task. We approached the TREC 2007 Question Answering task as a semantics based question to answer matching problem. Given a question we aimed to extract the relevant semantic entities in it so that we can pin-point the answer. Overall our scores were well above the median score of the TREC 2007 runs from all teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Our Question answering system consists of several phases that work in sequential manner. Each phase reduces the amount of data, the system has to handle from then on. The advantage of this approach is that progressive phases can perform more expensive operations on the data. The system is broadly divided into two main modules,the Question Processing Module, and the Answer Retrieval Module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Processing Module</head><p>This is the module which takes the input set of questions and converts into a form that can be processed by the answer retrieval module. This module consists of question pre-processing, keyword generation, significant keyword selection and question classification.</p><p>Question pre-processing contains stopword removal and stemming. In question classification the questions are classified to yield their expected answer types and in Query Generation a parser is used in order to identify certain significant words that are given more weightage than the normal keywords in the construction of the query for both document retrieval and answer retrieval. Figure <ref type="figure" coords="2,240.74,414.37,4.98,9.96" target="#fig_0">1</ref> illustrates this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Retrieval Module</head><p>This module takes as input the keywords along with keyword significance scores and expected answer type all produced by the Question Processing Module. Using the keywords the answer retrieval module first finds the documents relevant to the question. Only the top N documents are used for the next step. This greatly reduces the amount of text that need to be handled in subsequent steps. Next from these documents we select the relevant sentences. In this sentence selection phase, all sentences are scored against the question and only the most relevant sentences are picked. In the final phase, we pin-point the answer within a sentence. As per the TREC requirement, this answer should be exact and correct. Figure <ref type="figure" coords="2,197.52,652.09,4.98,9.96" target="#fig_1">2</ref> illustrates this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Processing</head><p>In this section we explain in more detail our question processing module introduced in the previous section. We will explain the various sub parts of this module in detail here. Our goal is to not only extract the keywords but also as much semantic information as possible from the question that helps in getting the exact answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Pre-processing</head><p>This stage implements stop word elimination and stemming. A list if frequently occurring words is considered for stop word removal and Porter's Stemming algorithm <ref type="bibr" coords="2,393.73,256.33,15.49,9.96" target="#b19">[20]</ref> was used in order to stem the words. Our stop word list comprised of 50 common English words such as it, the, at, etc. The output of this step comprises the set of all keywords from the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Significant Keywords Selection</head><p>The quality of the answer-retrieval engine depends on the richness of the query that is given to it. In order to obtain the most relevant answers for the questions, the query tries to highlight certain words present in the question as significant words. The following words within the keywords are considered significant:</p><p>1. Words referring full or part of the target (As the target is already provided identifying this is very easy)</p><p>2. Words that refer to the object of the question (Question Object)</p><p>3. Noun phrases in the absence of question object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Question Object</head><p>We have observed that the object of the question is often one the significant words and hence that has to be identified. This can be detected with the help of a Parser (We have used Stanford Parser <ref type="bibr" coords="2,491.55,628.21,15.49,9.96" target="#b16">[17]</ref> for all parser implementations). We have exploited the feature of this parser that recognizes the dependencies and detects the object. For example in the question, What company produces his records? , the parser detects that produces and records are connected and that records is the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Significant Noun Phrases</head><p>Even though most in most of the questions we are able to identify the object, there are exceptions. In such a case, we try to use all the Noun Phrases in the question minus the stop words as significant words. These noun phrases are obtained with the help of the parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question Classification</head><p>Question classification is the process of categorizing the question into one of the predetermined classes. This stage is needed because we need to know the expected answer type before returning an answer. We have used a rule based classifier that classifies a question into fine grained categories and their corresponding coarse categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Categories</head><p>There are 8 coarse grained categories and 59 finegrained categories. The coarse-grained categories we selected were PERSON, LOCATION, OR-GANIZATION, NUMBER, TITLE, JOBTITLE, DATE and MONEY. We deemed these as the important categories based on past TRECs. The Named Entity Recognizer in the retrieval part has the same names as these 8 types and therefore, this is helpful in directly matching the indexed documents. The fine-grained types provide additional information for the answer retrieval module. The fine grained classes are almost similar to those present in the UIUC Dataset <ref type="bibr" coords="3,431.16,370.69,15.49,9.96" target="#b18">[19]</ref> but with few additions and deletions. The categories used in our system are listed in Table <ref type="table" coords="3,416.87,394.57,3.90,9.96" target="#tab_0">1</ref>. To classify a question into the coarse and fine grained classes we defined a set of rules. Our rule set comprised of 300 ordered rules. The ordering implied that certain rules had precedence over others. We give some examples here,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">PERSON</head><p>All Who questions are classified as requiring answer type PERSON. Questions containing What, Which and Name with words like architect, engineer, artist etc. were for example categorized as requiring answer type PERSON -INDIVIDUAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">LOCATION</head><p>All Where questions are classified as LOCATION. Further questions containing What, which and Name with country, state, city, town, ocean or river refer to more specific locations and can be put in the respective fine grained categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">NUMBER</head><p>How questions mostly refer to numbers. They are detected by checking for other words like many, far, long, deep, fast, hot, etc. For example, How many always refers to a count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.6">ORGANIZATION</head><p>what, which and Name with other words like team, league, organization, institution, school, college, etc., are assigned to this class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.7">ENTITY</head><p>Entity types are detected mostly with the specific entities like colour, creature, product etc. that occur along with what, which and Name questions.</p><p>We built these rules using past years questions. We also used wordnet and wikipedia to associate entities with their categories. For example, town is assigned to the class LOCATION -CITY using wordnet hypernym.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Question Focus</head><p>In the absence of an expected answer type we need some focus to be determined for retrieving the correct answer. For us the question focus is the word that is associated with the question phrase like what, when, etc. Generally it is the Noun Phrase For example, for the question What part of the Soldiers' anatomy reminded the Indians of the buffalo?, the Focus is part. There was no rule to classify this question, but using question focus, we could classify this as ENTY:part. We found this module to succeed in a few cases where the question classification failed. This module only fired if the classification didn't return any results.</p><p>The question processing step gives as its output the list of all keywords, the list of significant keywords and the expected answer type. The significant keywords and the expected answer type are the extra semantic information that have been extracted to aid in the answer retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Answer Retrieval Module</head><p>In this module given a question and a set of documents, we retrieve the exact answer to the question from the given documents. It is possible that there is no answer in the given document collection for a given question. In this section we enumerate the steps to getting the answer to a question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document Retrieval Module</head><p>The first step is the information retrieval task. From the set of all given documents we identify the top relevant documents for a given question. The document retrieval module enables this in a fast and efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Indexing</head><p>The goal of storing an index is to optimize the speed and performance of finding relevant documents for a search query. Without an index, the search would scan every document in the corpus, which would take a considerable amount of time and computing power. For example, an index of 1000 documents can be queried within milliseconds, where a raw scan of 1000 documents could take hours. The trade off for the time saved during retrieval is that additional storage is required to store the index and that it takes a considerable amount of time to update. Lucene is a free and open source information retrieval library, originally implemented in Java. It is suitable for any application which requires full text indexing and searching capability. We indexed the complete TREC 2007 QA data collection using Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Full-text searching</head><p>The Document Retrieval Module identifies the documents or paragraphs in the document set that are likely to contain the answer. Using the keywords found by the question processing module we retrieve the relevant documents. We give more weightage to significant keywords determined by the question processing module in ranking the retrieved documents. We keep only the top N ranked documents for a given query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exact Answer Selection</head><p>In this module given the relevant documents we now select the exact answer to a given question. We implemented four separate ways of doing this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Semantic Type Recognition</head><p>The semantic type recognizer extracts the answer based on the expected answer type.</p><p>Example The semantic type matching is based on a named entity recognizer which extracts all the named entities from the answer text.</p><p>We have used statistical model based Named Entity Recognizer (NER) which is trained using newswire training set. NER seeks to locate and classify elements in text into predefined categories PERSON, LOCATION, ORGANIZATION, NUM-BER, TITLE, JOBTITLE, DATE, MONEY, etc.</p><p>The first step in answer extraction is to get the most relevant sentence that is likely to contain the answer. Two factors are considered for ranking the sentences: number of keywords occurring in the sentence, and whether the sentence contains the same answer type as the question. The Sentences are scored using tf/idf <ref type="bibr" coords="6,177.73,448.33,10.00,9.96" target="#b0">[1]</ref>. The tf/idf score was scaled by the count of the query terms that appear within the Sentence. Each Sentence (Sentence of the candidate articles) is scored. Once the most significant sentence has been found, the named entity with the correct answer type is selected as the answer to the given question. The tf/idf weight (term frequency inverse document frequency) is a weight used in information retrieval and text mining. A high weight in tf/idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents, the weight hence tends to filter out common terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Text Pattern Matching</head><p>Some questions are difficult to answer using semantic type based strategy. We developed simple pat-terns for answering such questions. Specifically, we developed patterns for acronym expansion questions, date of birth questions and location questions. These patterns are derived from the answers to the questions of previous years TREC data. We extracted about hundred patterns for these three class of questions. This approach extracts answers from the surface structure of the retrieved documents by relying on an extensive list of patterns <ref type="bibr" coords="6,302.16,185.89,14.60,9.96" target="#b10">[11]</ref>. Although building extensive lists of such patterns is time consuming, this approach has high precision. The approach is based on the assumption that answers can be identified by their correspondence to patterns describing the structure of strings carrying certain semantics. These patterns, are like regular expressions.</p><p>Example: What does the abbreviation CSPI stand for? (Ques. 248.1)</p><p>The text contains "Former North Carolina basketball coach Dean Smith, former Nebraska football coach Tom Osborne, 246 university presidents, the American Medical Association and the Center for Science in the Public Interest (CSPI) have not...." the pattern matcher for this is based on matching the first letters of adjacent words Example: Which year was Mozart born? The string "Mozart (1756-1791)" contains answers to the questions about Mozart's birth and death years, allowing construction of the pattern: "capitalized word; parenthesis; four digits; dash; four digits; parenthesis ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Semantic Class Recognition</head><p>For many of the questions the question processing module is not able to return the expected answer type. For such questions we exploited knowledge about hypernym relationship in WordNet. Word-Net is the well-known English ontology freely available on the Web and covers the vast majority of nouns, verbs, adjectives, and adverbs from the English language. For questions asking about certain categories such as animal, disease, plant, color, etc., we evaluated each noun or noun phrase in the sentence using knowledge about hypernym relationship in WordNet. For example, In what US state was Barack Obama born? (Ques. 272.1) the answer type is state. For each candidate answer, we used WordNet to check whether state is one of its hypernyms. Hawaii has state as it's hypernym, and hence it is chosen as final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">External Resource</head><p>We also implemented a module for retrieving answers from the WikiPedia <ref type="bibr" coords="7,190.45,159.85,15.49,9.96" target="#b15">[16]</ref> InfoBox. We compared keywords from the question with infobox entries to retrieve the exact answer. We use the "target" as the basic element to retrieve the wikipedia page, and then use its infobox to get specific answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup and Results</head><p>We used the data available for participants of the QA track of the 2007 TREC conference. This data includes AQUAINT news-wire data and BLOG data. The questions in TREC 2007 are grouped by topic. The competition consisted of 70 topics, with a total of 515 questions. These questions are divided into three different types: factoid, list and other. Factoid questions require a single fact as answer. Lists asks for a list of answers and other is answered by giving any additional information about the topic. There are 360 factoid questions in the question set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Question Processing</head><p>As detailed in Section 3 we obtained the keywords, and their significance. We also obtained the expected answer type for each question. To obtain the expected answer type we made rules based on the previous years' TREC questions. 92% of the TREC2006 questions were classified out which 95% were correct. This year 84% (374) of all the 445 Factoid and List Questions could be classified and the classifier accuracy stood at 95% (358 correct classifications).</p><p>Our rule-based classifier produces better results compared with Machine-Learning based classifiers. For example, the Machine-Learning classifier by Li and Roth <ref type="bibr" coords="7,114.73,652.09,15.49,9.96" target="#b17">[18]</ref> gives an accuracy of only about 30-40 percent with fine-grained classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Answering FACTOID questions</head><p>Factoid questions have only one correct answer. Using the keywords extracted we select the top 50 documents for a question. Next we select the relevant sentences based on keyword matching. These sentences are ranked and the best phrase (with the highest score and matching the expected answer type) is returned as answer to a FACTOID question. We combined the answers returned by the exact answer selection modules in a weighted manner.</p><p>We got an accuracy value of 0.183 in factoid questions which is higher than the median score which is 0.131. We got 4.3% correct answers from BLOG data and rest 14% correct answers from AQUAINT data. It was expected to get more correct answers from newswire data as we trained and tested our system on newswire data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Answering LIST questions</head><p>List questions are similar to factoid question, except there is more than one correct and distinct answer to the question. By analyzing candidate answers produced by an existing Question Answering system, we can have multiple answers for a question. Instead of giving single answer, the system will produce a list of top K candidate answers. The Average F score over 85 list questions of 0.125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Answering OTHER questions</head><p>We observe that Target followed by VB (Verb-Phrase) is a good candidate for giving interesting information about the target. Sentences those are having NUMBERS/DATE, can be considered as priority candidates for answering OTHER question. The average Pyramid F score over 70 'other' questions is 0.208 for our system which is higher than median score of 0.118.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We believe there is a lot of promise in the semantics based approach that we followed this year. Our system performed reasonably well in the TREC 2007 QA task. Our scores were well above the median. We observed that we got more answers from the newswire data than the blog data. This could be because we didn't have any modules in place to take care of the informal nature of blog text.</p><p>Our QA system is still in the development stage. Some of the subsystems had not been fully tested before the TREC experiments due to time constraints. We are continuing our effort to develop and improve the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,193.56,267.13,207.55,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Block Diagram of Question Processor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,185.52,430.45,223.21,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of Answer Retrieval Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,85.21,220.11,349.32"><head>Table 1 :</head><label>1</label><figDesc>Coarse and fine-grained classes</figDesc><table coords="5,72.00,96.85,220.11,337.68"><row><cell>Coarse</cell><cell></cell><cell>Fine</cell><cell></cell></row><row><cell>ABBR</cell><cell cols="2">abbreviation expansion</cell><cell></cell></row><row><cell>DATE</cell><cell>date week</cell><cell>month year</cell><cell>other</cell></row><row><cell></cell><cell>animal</cell><cell>body</cell><cell>book</cell></row><row><cell></cell><cell>color</cell><cell>currency</cell><cell>disease</cell></row><row><cell></cell><cell>event</cell><cell>food</cell><cell>instr</cell></row><row><cell></cell><cell>language</cell><cell>letter</cell><cell>medicine</cell></row><row><cell>ENTY</cell><cell>movie plant</cell><cell>music position</cell><cell>physt prize</cell></row><row><cell></cell><cell>product</cell><cell>religion</cell><cell>song</cell></row><row><cell></cell><cell>sport</cell><cell cols="2">substance symbol</cell></row><row><cell></cell><cell>tvshow</cell><cell>url</cell><cell>vehicle</cell></row><row><cell></cell><cell>word</cell><cell></cell><cell></cell></row><row><cell>JOB</cell><cell>jobtitle</cell><cell></cell><cell></cell></row><row><cell>LOC</cell><cell>city other</cell><cell>country state</cell><cell>geo</cell></row><row><cell></cell><cell>age</cell><cell>count</cell><cell>distance</cell></row><row><cell></cell><cell>duration</cell><cell>money</cell><cell>percent</cell></row><row><cell>NUM</cell><cell>phone</cell><cell>size</cell><cell>speed</cell></row><row><cell></cell><cell>temp</cell><cell>time</cell><cell>volume</cell></row><row><cell></cell><cell>weight</cell><cell>zip</cell><cell></cell></row><row><cell>ORG</cell><cell>organization</cell><cell></cell><cell></cell></row><row><cell>PER</cell><cell>individual</cell><cell>name</cell><cell></cell></row><row><cell cols="2">TITLE title</cell><cell></cell><cell></cell></row><row><cell cols="4">attached to the question phrase (what, when etc.,).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.00,85.21,220.14,192.60"><head>Table 2 :</head><label>2</label><figDesc>Results of our system compared to other systems</figDesc><table coords="8,78.00,108.85,212.10,168.96"><row><cell></cell><cell cols="3">Run1 Run2 Run3</cell><cell>med</cell></row><row><cell>FACTOID</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell cols="4">0.172 0.181 0.183 0.131</cell></row><row><cell>incorrect</cell><cell>269</cell><cell>265</cell><cell>262</cell></row><row><cell>unsupported</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>inexact</cell><cell>20</cell><cell>21</cell><cell>22</cell></row><row><cell>locally correct</cell><cell>6</cell><cell>6</cell><cell>7</cell></row><row><cell cols="2">globally correct 62</cell><cell>65</cell><cell>66</cell></row><row><cell>LIST</cell><cell></cell><cell></cell><cell></cell></row><row><cell>avg F score</cell><cell cols="4">0.120 0.123 0.125 0.085</cell></row><row><cell>OTHER</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pyr F score</cell><cell cols="4">0.152 0.209 0.208 0.118</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell></cell></row><row><cell>per-ser score</cell><cell cols="4">0.150 0.173 0.174 0.108</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,87.48,461.53,204.79,9.96;8,87.48,473.41,204.94,9.96;8,87.48,485.41,204.71,9.96;8,87.48,497.29,204.79,9.96;8,87.48,509.29,204.52,9.96;8,87.48,521.29,204.90,9.96;8,87.48,533.17,22.93,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,158.13,473.41,134.29,9.96;8,87.48,485.41,204.71,9.96;8,87.48,497.29,68.55,9.96">Performance Issues and Error Analysis in an Open-Domain Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,166.20,497.29,126.07,9.96;8,87.48,509.29,204.52,9.96;8,87.48,521.29,45.55,9.96">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.48,556.81,204.78,9.96;8,87.48,568.81,204.68,9.96;8,87.48,580.69,204.74,9.96;8,87.48,592.69,195.77,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,92.36,568.81,164.35,9.96">Question Answering in Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Junk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,273.12,568.81,19.05,9.96;8,87.48,580.69,157.76,9.96">Proceedings of the TREC-9 Conference</title>
		<meeting>the TREC-9 Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.48,616.33,204.83,9.96;8,87.48,628.21,204.86,9.96;8,87.48,640.21,204.81,9.96;8,87.48,652.09,204.81,9.96;8,87.48,664.09,194.17,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,219.96,616.33,72.35,9.96;8,87.48,628.21,131.62,9.96">Building a question answering test collection</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dawnz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,236.52,628.21,55.83,9.96;8,87.48,640.21,204.81,9.96;8,87.48,652.09,204.81,9.96;8,87.48,664.09,26.26,9.96">23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Athens</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,317.64,78.25,204.72,9.96;8,317.64,90.25,204.56,9.96;8,317.64,102.25,204.76,9.96;8,317.64,114.13,22.93,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,504.25,78.25,18.11,9.96;8,317.64,90.25,171.27,9.96">Disambiguation of Proper Names in Text</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wacholder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ravin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,503.16,90.25,19.05,9.96;8,317.64,102.25,94.36,9.96">Proceedings of ANLP &apos;97</title>
		<meeting>ANLP &apos;97<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04">April 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,317.64,135.37,204.83,9.96;8,317.64,147.37,204.67,9.96;8,317.64,159.37,204.69,9.96;8,317.64,171.25,204.82,9.96;8,317.64,183.25,122.61,9.96;8,480.75,183.25,41.73,9.96;8,317.64,195.13,113.19,9.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,510.54,135.37,11.93,9.96;8,317.64,147.37,204.67,9.96;8,317.64,159.37,204.69,9.96;8,317.64,171.25,92.47,9.96">Issues, Tasks and Program Structures to Roadmap Research in Question &amp; Answering (Q &amp; A)</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhri</surname></persName>
		</author>
		<ptr target="http://vww-nlpir.nist.gov/projects/duc/roadmap-ping.html" />
		<imprint>
			<date type="published" when="2000-10">October 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,317.64,216.49,205.02,9.96;8,317.64,228.37,204.68,9.96;8,317.64,240.37,204.57,9.96;8,317.64,252.25,179.05,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,323.65,228.37,198.68,9.96;8,317.64,240.37,156.92,9.96">An efficient easily adaptable system for interpreting natural language queries</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,487.92,240.37,34.29,9.96;8,317.64,252.25,82.15,9.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="110" to="122" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,317.64,273.61,204.88,9.96;8,317.64,285.49,204.72,9.96;8,317.64,297.49,204.46,9.96;8,317.64,309.37,66.86,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,386.35,273.61,136.17,9.96;8,317.64,285.49,204.72,9.96;8,317.64,297.49,112.12,9.96">Procedures as a representation for data in a computer program for understanding natural language</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,441.84,297.49,80.26,9.96;8,317.64,309.37,12.52,9.96">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,317.64,330.73,204.71,9.96;8,317.64,342.61,204.78,9.96;8,317.64,354.61,204.67,9.96;8,317.64,366.49,204.80,9.96;8,317.64,378.49,204.80,9.96;8,317.64,390.49,77.17,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,450.72,330.73,71.63,9.96;8,317.64,342.61,204.78,9.96;8,317.64,354.61,41.74,9.96">Web based pattern mining and matching approach to question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,374.16,354.61,148.15,9.96;8,317.64,366.49,154.87,9.96">Proceedings of the Eleventh Text Retrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text Retrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,317.64,411.73,204.91,9.96;8,317.64,423.61,204.66,9.96;8,317.64,435.61,204.57,9.96;8,317.64,447.61,204.72,9.96;8,317.64,459.49,56.66,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,422.60,411.73,99.95,9.96;8,317.64,423.61,129.60,9.96">Information extraction supported question answering</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,460.32,423.61,61.98,9.96;8,317.64,435.61,204.57,9.96;8,317.64,447.61,7.93,9.96">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,322.56,480.73,39.36,9.96;8,379.34,480.73,30.73,9.96;8,427.45,480.73,27.59,9.96;8,472.35,480.73,50.16,9.96;8,317.64,492.73,180.75,9.96" xml:id="b9">
	<monogr>
		<ptr target="http://www.alias-i.com/lingpipe/)" />
		<title level="m" coord="8,322.56,480.73,39.36,9.96;8,379.34,480.73,30.73,9.96;8,427.45,480.73,27.59,9.96;8,472.35,480.73,45.60,9.96">LingPipe Named Entity Recognizer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,322.56,513.97,199.88,9.96;8,317.64,525.97,204.86,9.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,443.81,513.97,78.63,9.96;8,317.64,525.97,169.90,9.96">The University of Sheffield&apos;s TREC 2004 QA Experiments</title>
		<author>
			<persName coords=""><forename type="first">Saggion</forename><surname>Greenwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,322.56,547.21,199.85,9.96;8,317.64,559.21,204.80,9.96;8,317.64,571.09,204.74,9.96;8,317.64,583.09,62.18,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,323.19,559.21,199.25,9.96;8,317.64,571.09,25.84,9.96">The role of named entities in text classification</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Armour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,358.32,571.09,110.14,9.96">Proceedings CLiNE 2005</title>
		<meeting>CLiNE 2005<address><addrLine>Gatineau, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,322.56,604.33,199.86,9.96;8,317.64,616.33,204.75,9.96;8,317.64,628.21,204.62,9.96;8,317.64,640.21,204.66,9.96;8,317.64,652.09,204.81,9.96;8,317.64,664.09,143.68,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,506.78,604.33,15.64,9.96;8,317.64,616.33,204.75,9.96;8,317.64,628.21,106.28,9.96">Use of Patterns for Detection of Answer Strings: A Systematic Approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,440.04,628.21,82.22,9.96;8,317.64,640.21,204.66,9.96;8,317.64,652.09,22.51,9.96">Proceedings of the Eleventh Text Retrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text Retrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.52,78.25,199.96,9.96;9,87.48,90.25,204.82,9.96;9,87.48,102.25,204.74,9.96;9,87.48,114.13,204.69,9.96;9,87.48,126.13,186.64,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,273.41,78.25,19.07,9.96;9,87.48,90.25,204.82,9.96;9,87.48,102.25,115.62,9.96">Patterns and Potential Answer Expressions as Clues to the Right Answers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,215.52,102.25,76.70,9.96;9,87.48,114.13,141.68,9.96">Proceedings of the Tenth Text Retrieval Conference</title>
		<meeting>the Tenth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.52,146.05,199.92,9.96;9,87.48,158.05,204.72,9.96;9,87.48,169.93,204.79,9.96;9,87.48,181.93,204.92,9.96;9,87.48,193.81,120.86,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,119.55,158.05,172.65,9.96;9,87.48,169.93,78.12,9.96">A Question/Answer Typology with Surface Text Patterns</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,177.24,169.93,115.03,9.96;9,87.48,181.93,204.92,9.96;9,87.48,193.81,13.42,9.96">Proceedings of the DARPA Human Language Technology Conference, 247-250</title>
		<meeting>the DARPA Human Language Technology Conference, 247-250<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.52,213.73,199.59,9.96;9,87.48,225.73,109.00,9.96" xml:id="b15">
	<monogr>
		<ptr target="http://en.wikipedia.org" />
		<title level="m" coord="9,97.28,213.73,151.23,9.96">Wikipedia, the free encyclopedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.52,245.65,199.80,9.96;9,87.48,257.65,204.67,9.96;9,87.48,269.53,204.67,9.96;9,87.48,281.53,204.67,9.96;9,87.48,293.53,204.98,9.96;9,87.48,305.41,191.52,9.96;9,87.48,317.41,139.94,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,251.11,245.65,41.21,9.96;9,87.48,257.65,204.67,9.96;9,87.48,269.53,126.47,9.96">Fast Exact Inference with a Factored Model for Natural Language Parsing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/software/lex-parser.shtml" />
	</analytic>
	<monogr>
		<title level="m" coord="9,234.00,269.53,58.16,9.96;9,87.48,281.53,204.67,9.96;9,87.48,293.53,58.03,9.96">Advances in Neural Information Processing Systems 15 (NIPS 2002)</title>
		<meeting><address><addrLine>MA; Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.52,337.33,199.79,9.96;9,87.48,349.33,204.80,9.96;9,87.48,361.21,204.61,9.96;9,87.48,373.21,93.97,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,184.08,337.33,108.23,9.96;9,87.48,349.33,25.20,9.96">Learning Question Classifiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,130.56,349.33,161.72,9.96;9,87.48,361.21,204.61,9.96;9,87.48,373.21,63.59,9.96">Proceedings of the 19th International Conference on Computational Linguistics (COLING &apos;02)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING &apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.52,393.13,199.84,9.96;9,87.48,405.01,204.81,9.96;9,87.48,417.01,24.36,9.96;9,130.09,417.01,57.63,9.96;9,205.94,417.01,30.62,9.96;9,258.52,417.01,33.68,9.96;9,87.48,429.01,204.83,9.96;9,87.48,440.89,204.64,9.96;9,87.48,452.89,182.97,9.96;9,87.48,464.89,112.70,9.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,233.53,393.13,58.84,9.96;9,87.48,405.01,163.18,9.96">Experimental Data for Question Classification</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="http://l2r.cs.uiuc.edu/%7Ecogcomp/Data/QA/QC/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,271.70,405.01,20.60,9.96;9,87.48,417.01,24.36,9.96;9,130.09,417.01,57.63,9.96;9,205.94,417.01,25.52,9.96">Cognitive Computation Group</title>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.52,484.81,199.86,9.96;9,87.48,496.69,183.51,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,134.78,484.81,149.11,9.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,87.48,496.69,34.34,9.96">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
