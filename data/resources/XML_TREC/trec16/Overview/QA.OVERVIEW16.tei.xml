<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.81,115.49,388.38,14.93">Overview of the TREC 2007 Question Answering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.86,149.22,79.92,10.37"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
							<email>hoa.dang@nist.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.49,149.22,57.80,10.37"><forename type="first">Diane</forename><surname>Kelly</surname></persName>
							<email>dianek@email.unc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Carolina Chapel Hill</addrLine>
									<postCode>27599</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.26,149.22,52.15,10.37"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.81,115.49,388.38,14.93">Overview of the TREC 2007 Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92A1A8B31DB030885DBDFADE6628B29E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC 2007 question answering (QA) track contained two tasks: the main task consisting of series of factoid, list, and "Other" questions organized around a set of targets, and the complex, interactive question answering (ciQA) task. The main task differed from previous years in that the document collection comprised blogs in addition to newswire documents, requiring systems to process diverse genres of unstructured text. The evaluation of factoid and list responses distinguished between answers that were globally correct (with respect to the document collection), and those that were only locally correct (with respect to the supporting document but not to the overall document collection). The ciQA task provided a framework for participants to investigate interaction in the context of complex information needs. Standing in for surrogate users, assessors interacted with QA systems live over the Web; this setup allowed participants to experiment with more complex interfaces but also revealed limitations in the ciQA design for evaluation of interactive systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the TREC question answering (QA) track is to foster research on systems that directly return answers, rather than documents containing answers, in response to a natural language question. Since its inception in <ref type="bibr" coords="1,127.85,578.79,67.61,9.46">TREC-8 (1999)</ref>, the track has steadily expanded both the type and difficulty of the questions asked. The first several editions of the track focused on factoid questions. A factoid question is a fact-based, short answer question such as How many calories are there in a Big Mac? The task in the TREC 2003 QA track contained list and definition questions in addition to factoid questions <ref type="bibr" coords="1,396.91,619.44,72.56,9.46" target="#b8">(Voorhees, 2004)</ref>. A list question asks for different answer instances that satisfy the information need, such as List the names of chewing gums. Answering such questions requires a system to assemble a response from information located in multiple documents. A definition question asks for interesting information about a particular person or thing such as Who is Vlad the Impaler? or What is a golden parachute? Definition questions also require systems to locate information in multiple documents, but in this case the information of interest is much less crisply delineated.</p><p>Since TREC 2004 <ref type="bibr" coords="2,171.11,102.96,77.82,9.46">(Voorhees, 2005a)</ref>, factoid and list questions have been grouped into different series, where each series is associated with a target and the questions in the series ask for some information about the target. In addition, the final question in each series is an explicit "Other" question, which is to be interpreted as "Tell me other interesting things about this target I don't know enough to ask directly". This last question is roughly equivalent to the definition questions in the TREC 2003 task. The series format supports the evaluation of different types of questions (factoid, list and Other) while providing an abstraction of a real user session with a QA system.</p><p>In TREC 2004, the target for a series could be a person, organization, or thing. Events were added as possible targets in TREC 2005, requiring that answers must be temporally correct with respect to the timeframe defined by the series. In TREC 2006, that requirement for sensitivity to temporal dependencies was made explicit in the distinction between locally and globally correct answers, so that answers for questions phrased in the present tense must not only be supported by the supporting document (locally correct), but must also be the most up-to-date answer in the document collection (globally correct).</p><p>The main task in the TREC 2007 QA track repeated the question series format, but with a significant change in the genre of the document collection. Instead of just newswire, the document collection contained both newswire and blogs. Mining blogs for answers introduced significant new challenges in at least two aspects that are very important for real-world QA systems: 1) being able to handle language that is not wellformed, and 2) dealing with discourse structures that are more informal and less reliable than newswire. Based on its successful application in TREC 2006 <ref type="bibr" coords="2,300.67,346.84,95.59,9.46" target="#b1">(Dang and Lin, 2007)</ref>, the nugget pyramid evaluation method became the official evaluation method for the Other questions in TREC 2007.</p><p>In addition to the main task, the TREC 2007 QA track repeated the complex, interactive QA (ciQA) task of TREC 2006. At the TREC 2006 workshop, participants indicated that they wanted to have longer, more complex interactions in the ciQA task rather than short interactions via cached interaction forms. Participants proposed trying "live interactions" for 2007. Under this setup, the interactive QA system was located at a URL (Uniform Resource Locator) on the participant's machine, and NIST assessors simply navigated to the URL. The advantage was that participants were able to explore more complex interactions and interfaces. However, this setup placed the burden on participants to have their systems accessible during the entire interaction period and to record all desired data during the interaction.</p><p>The remainder of this paper describes each of the two tasks in the TREC 2007 QA track in more detail. Section 2 describes the questions, evaluation methods, and results for the main task, while Section 3 discusses the ciQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Main Task</head><p>The scenario for the main task in the TREC 2007 QA track was that an adult, native speaker of English is looking for information about a target of interest. The target could be a person, organization, thing, or event. The user was assumed to be an "average" reader of U.S. newspapers. Serving as surrogate users, NIST assessors developed the questions and judged the system responses.</p><p>The main task required systems to provide answers to a series of related questions. A question series, which focused on a target, consisted of several factoid questions, one or two list questions, and exactly one Other question. The order of questions in the series and the type of each question (factoid, list, or Other) were all explicitly encoded in the test set. Example series are shown in Figure <ref type="figure" coords="2,412.17,659.85,4.09,9.46" target="#fig_0">1</ref>. The final test set contained 70 series; the targets of these series are given in  ORGANIZATIONs, 15 were EVENTs, and 19 were THINGs. The series contained a total of 360 factoid questions, 85 list questions, and 70 Other questions. Each series contained 6-10 questions (counting the Other question), with most series containing 7 questions.</p><p>Answers were to be drawn from a document collection comprising the Blog06 corpus <ref type="bibr" coords="3,468.56,524.01,71.44,9.46;3,72.00,537.56,58.49,9.46" target="#b6">(Macdonald and Ounis, 2006)</ref> and the AQUAINT-2 Corpus of English News Text. The AQUAINT-2 collection contains approximately 2.5 GB of text (about 907K documents) spanning the time period of October 2004 -March 2006; articles are in English and come from a variety of sources including Agence France Presse, Central News Agency (Taiwan), Xinhua News Agency, Los Angeles Times-Washington Post News Service, New York Times, and The Associated Press. Blog06 documents were collected by polling 100,649 RSS and Atom feeds over an 11 week period <ref type="bibr" coords="3,203.07,605.31,84.12,9.46">(December 6, 2005</ref><ref type="bibr" coords="3,290.01,605.31,88.61,9.46">-February 21, 2006)</ref>. A blog document is defined to be a blog post plus its follow-up comments (a permalink). As a convenience for track participants, NIST made available document rankings of the top 1000 documents per target for each of two corpora, as produced using the PRISE document retrieval system, with the target as the query.</p><p>Participants were allowed two weeks to download the test data and submit their results. All processing of the questions was required to be strictly automatic. Systems were required to process series independently from one another, and to process an individual series in question order. That is, systems were allowed to use questions and answers from earlier questions in a series to answer later questions in the same series, but could not "look ahead" and use later questions to help answer earlier questions. Thus, question series can be viewed as an abstraction of an information-seeking dialogue between the user and the system; cf. <ref type="bibr" coords="5,515.16,116.50,24.84,9.46;5,72.00,130.05,50.14,9.46" target="#b4">(Kato et al., 2004)</ref>. In total, 51 runs from 21 participants were submitted to the main task.</p><p>The evaluation of a single run can be decomposed into component evaluations for each of the question types and a final per-series score. Each of the three question types has its own response format and evaluation method. The individual component evaluations in 2007 were identical to those used in the TREC 2006 QA track, except that the official scores for Other questions were computed using multiple assessors' judgments of the importance of information nuggets, and assessors were not restricted in the criteria they could use in distinguishing between locally correct and globally correct answers for factoid and list questions. An aggregate score was computed for each series in a run using a simple average of the component scores of questions in that series, and the final score for the run was computed as the average of its per-series scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Factoid questions</head><p>The system response to a factoid question was either exactly one [doc-id, answer-string] pair or the literal string 'NIL'. Since there was no guarantee that a factoid question had an answer in the document collection, NIL was returned by the system when it believed there was no answer. Otherwise, answer-string was a string containing precisely an answer to the question, and doc-id was the id of a document in the collection that supported answer-string as an answer.</p><p>Each response was independently judged by two human assessors. When the two assessors disagreed in their judgments, a third adjudicator made the final determination. Each response was assigned exactly one of the following five judgments:</p><p>incorrect: the answer string does not contain a correct answer or the answer is not responsive; not supported: the answer string contains a correct answer but the document returned does not support that answer;</p><p>not exact: the answer string contains a correct answer and the document supports that answer, but the string contains more than just the answer or is missing bits of the answer;</p><p>locally correct: the answer string consists of exactly a correct answer that is supported by the document returned, but the document collection contains a contradictory answer that the assessor believes is better;</p><p>globally correct: the answer string consists of exactly the correct answer, that answer is supported by the document returned, and the document collection does not contain a contradictory answer that the assessor believes is better.</p><p>To be responsive, an answer string was required to contain appropriate units and to refer to the correct "famous" entity (e.g., the Taj Mahal casino is not responsive if the question asks about "the Taj Mahal"). Questions also had to be interpreted in the time frame implied by the question series. For example, if the target was the event "France wins World Cup in soccer" and the question was Who was the coach of the French team? then the correct answer must be "Aime Jacquet", the name of the coach of the French team in 1998 when France won the World Cup, and not just the name of any past or current coach of the team. NIL responses were correct only if there was no known answer to the question in the collection. NIL was correct for 16 of the 360 factoid questions in the test set. For 26 questions, no system returned the correct answer, although those questions did have a correct answer found by the assessors.</p><p>It may be the case (especially with the inclusion of blogs) that different documents support contradictory answers as being correct. An exact answer-string that is supported in its associated document is assumed to be globally correct unless there is a better, contradictory answer supported elsewhere in the document collection. The assessor was allowed to use any number of criteria in determining that one answer was better than another, including recency of the supporting document, the amount of support provided by each supporting document, the number of distinct sources that support the answer as being correct, and the credibility or authoritativeness of the source. The assessor marked as globally correct one or more of the most credible of the known locally correct answers. "Global" correctness was defined with respect to the document collection, and not necessarily with respect to the real world.</p><p>The main evaluation metric for the factoid component was accuracy, the fraction of questions judged to be globally correct. Table <ref type="table" coords="6,189.44,469.42,5.45,9.46" target="#tab_2">2</ref> shows the most accurate run for the factoid component for each of the top 10 groups. Also reported are the recall and precision of recognizing when no answer exists in the document collection. NIL precision is the ratio of the number of times NIL was returned and correct to the number of times it was returned; NIL recall is the ratio of the number of times NIL was returned and correct to the number of times it was correct in the entire test set (16). If NIL was never returned, NIL precision is undefined and NIL recall is zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">List questions</head><p>A list question asks for different instances of a particular type. The correct answer for a list question is the set of all such distinct instances in the document collection. A system's response to a list question consists of an unordered set of [doc-id, answer-string] pairs such that each answer-string represents a correct answer instance.</p><p>Each instance was evaluated in the same manner as the factoid questions, i.e., assigned one of the following judgments: incorrect, not supported, not exact, locally correct, and globally correct. Instances that were judged to be globally correct were then manually grouped into equivalence classes, where each equivalence class was considered a distinct answer. Thus, systems were not rewarded (and were in fact penalized) for returning equivalent answers multiple times.</p><p>The final set of known globally correct answers for a list question was compiled from the union of distinct globally correct answers across all runs plus additional distinct answers the assessor found during question development. For the 85 list questions in the test set, the median number of known distinct globally correct answers per question was 7, with a minimum of 2 and a maximum of 64. A system's response to a list question was scored using instance precision (IP) and instance recall (IR) based on the complete list of known distinct globally correct answers. Let S be the number of such answers, D be the number of distinct globally correct answers returned by the system, and N be the total number of instances returned by the system. Then IP = D/N and IR = D/S. Precision and recall were then combined to produce an F-score with equal weight given to recall and precision:</p><formula xml:id="formula_0" coords="7,263.11,449.20,84.49,24.43">F = 2 × IP × IR IP + IR</formula><p>The score for the list component of a run was the average F-score over the 85 questions. Table <ref type="table" coords="7,492.49,484.71,5.45,9.46" target="#tab_3">3</ref> gives the average F-score of the run with the best list component score for each of the top 10 groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other questions</head><p>The Other questions were evaluated using the methodology originally developed for the TREC 2003 definition questions. A system's response for an Other question consisted of an unordered set of [doc-id, answer-string] pairs. The answer strings were presumed to contain interesting "nuggets" about the series target that had not yet been covered by earlier questions in the series. The requirement to not repeat information already covered by earlier questions in the series made answering Other questions more difficult than answering TREC 2003 definition questions.</p><p>Judging the quality of the systems' responses was performed in two steps. In the first step, all of the answer strings from all of the systems were presented to an assessor in a single list. Using all the answer strings and searches performed during question development, the assessor created a list of information nuggets about the target. An information nugget in the context of an Other question is defined as an atomic piece of information about the target that is interesting (in the assessor's opinion) and is not part of an earlier question in the series or an answer to an earlier question in the series. An information nugget is considered atomic if the assessor could make a binary decision as to whether the nugget appears in a response. Once the nugget list was created for a target, the assessor decided which were vital, meaning that the information must be returned for a response to be good. Non-vital ("okay") nuggets acted as "don't care" conditions in that the assessor believed the information in the nugget to be interesting enough that returning the information was acceptable in, but not necessary for, a good response.</p><p>In the second step of the evaluation process, the assessor went through each system's output in turn and marked which nuggets appeared in the response. An answer string contained a nugget if there was a conceptual match between the answer string and the nugget; that is, the match was independent of the particular wording used in either the nugget or the system output. A nugget match was marked at most once per response-if the system output contained more than one match for a nugget, an arbitrary match was marked and the remainder were left unmarked. A single [doc-id, answer-string] pair in a system response could match 0, 1, or multiple nuggets.</p><p>To address some of the weaknesses of using vital/okay judgments from a single assessor <ref type="bibr" coords="8,484.26,482.11,55.75,9.46;8,72.00,495.66,51.08,9.46" target="#b3">(Hildebrandt et al., 2004)</ref>, <ref type="bibr" coords="8,131.03,495.66,147.97,9.46">Lin and Demner-Fushman (2006)</ref> proposed an extension called "nugget pyramids", in which multiple assessors provide judgments of whether a nugget was vital or simply okay. <ref type="bibr" coords="8,446.86,509.21,93.14,9.46" target="#b1">Dang and Lin (2007)</ref> subsequently verified the efficacy of this method, and thus NIST adopted the pyramid extension for computing F-scores for Other responses. Nine different sets of vital/okay judgments were solicited from eight unique assessors (the primary assessor who originally created the nuggets later assigned vital/okay labels again). Each assessor was given all the questions for the series, as well as the nuggets created by the primary assessor. Using the pyramid procedure, a weight was assigned to each nugget based on the number of assessors who marked it as vital.</p><p>Given the nugget list and the set of nuggets matched in a system's response, nugget recall was computed as the ratio of the sum of weights of matched nuggets to the sum of weights of all nuggets in the list. Nugget precision was much more difficult to compute since there was no effective way of enumerating all the concepts contained in a particular answer string. Instead, a measure based on length (in non-whitespace characters) was used as an approximation to nugget precision. The length-based measure granted an allowance of 100 characters for each nugget matched. If the total system output was less than this number of characters, the value of nugget precision was 1.0. Otherwise, the measure's value decreased as the length increased according to the following formula:</p><formula xml:id="formula_1" coords="9,247.66,110.74,116.67,24.43">1 - length -allowance length .</formula><p>The final score for an Other question was an F-score, with nugget recall weighted more heavily than nugget precision:</p><formula xml:id="formula_2" coords="9,219.32,181.70,173.37,26.62">F (β) = (β 2 + 1) × precision × recall β 2 × precision + recall .</formula><p>The score for the Other questions component was the average F-score (β=3) over the 70 Other questions. Table <ref type="table" coords="9,98.69,234.09,5.45,9.46" target="#tab_4">4</ref> gives the F-score for the best scoring Other question component for each of the top 10 groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Per-series Combined Scores</head><p>The three component scores measure a system's ability to process each type of question, but may not reflect the system's overall usefulness to a user. Since each individual series is an abstraction of a single user's interaction with the system, taking the individual series as the basic unit of evaluation should provide a more accurate representation of the effectiveness of the system from an individual user's perspective. Since each series is a mixture of different question types, we can compute a weighted average of the scores of the three question types on a per-series basis, and take the average of the per-series weighted scores as the final score for the run <ref type="bibr" coords="9,120.77,370.93,78.31,9.46">(Voorhees, 2005b)</ref>. In 2007, the weighted score for an individual series was computed as:</p><formula xml:id="formula_3" coords="9,181.73,391.71,248.54,24.43">WeightedScore = 1 3 × Factoid + 1 3 × List + 1 3 × Other.</formula><p>To compute the weighted score for an individual series, only the scores for questions belonging to that series were included in the computation. Since each of the component scores ranges between 0 and 1, the weighted score is also in that range. The final per-series score of each run is simply the average of individual per-series weighted scores.</p><p>We fit a two-way analysis of variance (ANOVA) model with the target type and the best run from each group as factors, and the per-series score as the dependent variable; we found significant differences between runs (p essentially equal to 0). To determine which runs were significantly different from each other, we performed a multiple comparison using Tukey's honestly significant difference criterion and controlling for the experiment-wise Type I error so that the probability of declaring a difference between two runs to be significant, when it is actually not, is at most 5%. Table <ref type="table" coords="10,330.01,102.96,5.45,9.46" target="#tab_5">5</ref> shows the results of the multiple comparison for the 10 groups with the highest final per-series score; runs sharing a common letter are not significantly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discussion</head><p>Despite the inclusion of the blog corpus, which was expected to make the QA task more difficult, the best component scores in the main task were higher in 2007, after having generally declined each year since TREC 2004.</p><p>For each series, an attempt had been made during question development to include at least one question whose answer was found in the Blog06 corpus but not in the AQUAINT-2 corpus. This could be the answer to a factoid question, one of the items answering a list question, or (in rare cases) a nugget for the Other question. NIST assessors varied in their ability to locate blog-specific information that was suitable for the series. In some cases, the assessor could not find an answer in the AQUAINT-2 corpus during topic development, but the answer was later found in AQUAINT-2 during the assessment of system responses. In the end, only 15.0% (54/360) of the factoid questions had an answer that could be found only in the Blog06 corpus; 24.8% (235/946) of the distinct items answering a list question could be found only in the Blog06 corpus; and at most 6.1% (45/735) of the distinct nuggets answering an Other question could be found only in the Blog06 corpus.</p><p>The positive contribution of answers from blog documents to the various component scores was likely depressed due to the nature of the questions asked. Because factoid and list questions generally requested factual information, it is not surprising that most of their answers would come from newswire rather than blogs. In addition, assessors tend to place more credibility on newswire documents than blog posts, so if a blog answer contradicted a newswire answer, the newswire answer would be judged as the globally correct one, and the blog answer would at best be judged as locally correct; the effect would be more pronounced for factoid questions (which generally have only one globally correct answer) than for list questions (which allow multiple answers). Finally, assessors were most interested in factual information about their targets, and consequently found very little interesting Other information nuggets in the blog documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Complex Interactive QA (ciQA) Task</head><p>In TREC 2007, the goals of the complex, interactive question answering (ciQA) task remained unchanged from the previous year-to push the frontiers of question answering in two directions:</p><p>• A move away from "factoid" questions towards more complex information needs that exist within richer user contexts.</p><p>• A move away from the one-shot interaction model implicit in previous evaluations towards a model based on interactions with users.</p><p>The ciQA task in TREC 2007 represented the second iteration of the evaluation, which started in 2006. The TREC 2006 ciQA task <ref type="bibr" coords="10,191.71,653.45,79.13,9.46" target="#b1">(Dang et al., 2007)</ref>, in turn, descended from the TREC 2005 HARD track, which focused on single-iteration clarification dialogues <ref type="bibr" coords="10,288.70,667.00,56.53,9.46" target="#b0">(Allan, 2006)</ref>. However, there were substantial changes in the evaluation methodology: in TREC 2006, participants "encapsulated" their interactions in HTML forms that were sent to NIST. This year, the task moved to completely "live" systems where assessors accessed individual QA systems, running at the participants' sites, over the Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Corpus</head><p>The ciQA task used the newswire portion of the corpus used by the main QA task (excluding the blog data). Participants were provided with the top 100 documents as retrieved by the PRISE system, using the question template verbatim as the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complex "Relationship" Questions</head><p>The complex information needs explored by ciQA remained unchanged from last year; they represent extensions and refinements of so-called "relationship" questions piloted in TREC 2005 <ref type="bibr" coords="11,447.70,261.23,92.30,9.46;11,72.00,274.78,23.48,9.46" target="#b7">(Voorhees and Dang, 2006)</ref>.</p><p>The concept of a "relationship" is defined as the ability of one entity to influence another, including both the means to influence and the motivation for doing so. Eight "spheres of influence" were noted in a previous pilot study funded by the AQUAINT research program: financial, movement of goods, family ties, communication pathways, organizational ties, co-location, common interests, and temporal. Evidence for both the existence or absence of ties is relevant. The particular relationships of interest naturally depend on the context.</p><p>A relationship question in the ciQA task, referred to as a topic, is composed of two parts. Consider an example:</p><p>Template: What evidence is there for transport of [drugs] from [Mexico] to [the U.S.]? Narrative: The analyst would like to know of efforts to curtail the transport of drugs from Mexico to the U.S. Specifically, the analyst would like to know of the success of the efforts by local or international authorities.</p><p>The question template is a stylized information need that has a fixed structure and free slots whose instantiation varies across different topics. The narrative is free-form natural language text that elaborates on the information need, providing, for example, user context, a more articulated statement of interest, focus on particular topical aspects, etc.</p><p>The ciQA task employed the following templates, which were the same as those used in TREC 2006:</p><p>• What evidence is there for transport of [goods] from [entity] to [entity]?</p><p>• <ref type="bibr" coords="11,99.27,567.83,86.64,9.81">What [relationship]</ref> exist between [entity] and [entity]? (where [relationship] is an element of {"financial relationships", "organizational ties", "familial ties", "common interests"})</p><p>• What influence/effect do(es) [entity] have on/in [entity]?</p><p>• What is the position of [entity] with respect to [issue]?</p><p>• Is there evidence to support the involvement of [entity] in [event/entity]?</p><p>Thirty topics were developed for this year's task, but they were not distributed evenly across the five templates. In addition, one "throw-away" topic was included for training purposes. Table <ref type="table" coords="12,140.81,220.78,4.24,9.46">6</ref>: Mapping between each NIST assessor and the topics they were responsible for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Interaction Design</head><p>The purpose of the interactive aspect of ciQA was to provide a framework for participants to investigate interaction in the QA context. Unlike in TREC 2006, participants were able to deploy full-fledged Webbased QA systems with which the assessors engaged for five minutes per topic. There were no restrictions on the nature of the interaction or the system, except that it had to be accessible from a Web browser.</p><p>Anything ranging from mixed-initiative dialogues to graphical interfaces was allowed.</p><p>To initiate interactions, assessors were directed to URLs provided by the participants. Assessors interacted with each system for five minutes per topic. The interaction length included time spent loading/rendering the page, as well as any delay caused by network traffic. It was the participant's responsibility to ensure that the QA system was Web-accessible during the period of time the assessors were scheduled to interact with submitted systems (a three-day period). If assessors were unable to access the participant's QA system, they skipped that interaction and did not return to it later.</p><p>The "throw-away" topic described earlier was used to familiarize assessors with systems before they completed actual test topics. Like other topics, the training period lasted five minutes, and could consist of anything that the participants wanted (e.g., a structured tutorial to introduce system features).</p><p>The interactions were completed at NIST using a Redhat Enterprise Linux 4 workstation with a 20-inch LCD monitor with 1600×1200 resolution and true color display (millions of colors), and a Firefox Web browser, v2.0.0.6. In addition, Flash, Acroread, and RealPlayer were enabled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Experimental Protocol</head><p>The basic setup for the task was as follows: Participants first submitted initial runs and URL files to NIST. The URL files provided pointers to the participants' Web-based QA system (one for each topic). Included in the URL files were also pointers to screenshots of the interface, supplied by the participants for archival purposes. NIST assessors interacted with the Web-based QA systems during a three-day period. Results of those interactions were available immediately to participants, since they hosted their own systems. It was each participant's own responsibility to instrument their system to collect whatever data was necessary; NIST did not keep track of the interactions. Eight assessors participated in the task. Most assessors completed four topics; the mapping between assessors and topics is shown in Table <ref type="table" coords="12,437.78,636.99,4.09,9.46">6</ref>.</p><p>Approximately two weeks following the interaction period, participants submitted final runs based on the results of the interactions to NIST. Assessors evaluated both initial and final runs.</p><p>Each participant was allowed to submit a maximum of 2 initial runs, 2 URL files, and 2 final runs. Manual runs were accepted, but had to be marked as such in the run submission interface. The interactive part of ciQA was optional; groups that did not wish to participate in the interactive aspect were asked to simply not submit URL files (however, every team engaged in the interactions). For each final run, participants were asked to supply the run tag of its corresponding initial run-this provided pairs of corresponding initial-final runs that isolated the effects of the interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Evaluation Methodology</head><p>System responses were evaluated using the "nugget pyramid" extension of the nugget-based methodology used in previous TREC QA tasks <ref type="bibr" coords="13,229.33,206.73,152.75,9.46">(Lin and Demner-Fushman, 2006)</ref>. Nine different sets of vital/okay judgments were solicited from eight unique assessors (the assessor who originally created the nuggets later assigned vital/okay labels again). Additional analyses included recall by length plots, as described in <ref type="bibr" coords="13,518.49,233.83,21.51,9.46;13,72.00,247.38,23.48,9.46" target="#b5">(Lin, 2007)</ref>. A recall plot quantifies pyramid recall as a function of response length, which provides a rough model of how quickly a user can learn about the topic by reading system responses in sequential order. For more information on how this is computed, please refer to <ref type="bibr" coords="13,330.48,274.47,80.13,9.46" target="#b1">(Dang et al., 2007)</ref>.</p><p>In addition to runs submitted by participants, we separately prepared a sentence retrieval baseline, similar to the one prepared last year. This provided a task-wide baseline to serve as a point of comparison. For each topic, the verbatim question template was used as a query to Lucene, which returned the top 20 documents. These documents were then tokenized into individual sentences. Sentences that contained at least one nonstopword from the question were retained and returned as the baseline run (up to a quota of 5,000 characters). Sentence order within each document and across the ranked list was preserved. The interaction associated with this run asked the assessor for relevance judgments on each of the sentences. Three options were given: "relevant", "not relevant", and "no opinion". The final run was prepared by simply removing those sentences judged not relevant-this had the effect of pulling more sentences from documents lower in the ranked list.</p><p>After assessors finished their interactions, they completed an online exit questionnaire which asked them to evaluate the various interactions. Assessors evaluated interactions according to several dimensions related to ease of use, usefulness, and effectiveness using 5-point scales. Assessors were also able to provide qualitative feedback about each interaction. Small screenshots of each system were displayed to remind assessors of each interaction. The order of these screenshots (and the order in which assessors evaluated each interaction) was random. A portion of the exit questionnaire, displaying the ciQA baseline interaction (described above), can be seen in Figure <ref type="figure" coords="13,254.27,491.26,4.09,9.46">2</ref>. At the end of the exit questionnaire, assessors were presented with four open-ended questions that asked them about their overall experiences. These questions were:</p><p>1. Of all interactions, which was your favorite and why? 2. What annoyed you about the interactions and why? 3. How different did you find the various interactions from one another and why? 4. Anything else?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The ciQA task drew participation from seven groups. NIST received twelve initial runs and twelve final runs. A total of fourteen URL files were submitted. For the purposes of evaluation, the sentence retrieval baseline was treated like any other submission. In total, there were twelve initial-final pairs (and the sentence retrieval baseline).</p><p>Figure <ref type="figure" coords="14,102.99,252.73,4.24,9.46">2</ref>: Portion of exit questionnaire for the baseline interaction. On the left the assessor sees a screenshot of the system (not meant to be readable, but simply as a reminder); questions are shown on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">System Effectiveness</head><p>The pyramid F-scores of the initial-final run pairs are shown in Table <ref type="table" coords="14,388.93,320.44,4.09,9.46" target="#tab_7">7</ref>. By comparing the score of the corresponding runs, we can quantify the effect of the interaction on system performance. The scatter plot in Figure <ref type="figure" coords="14,103.15,347.54,5.45,9.46" target="#fig_1">3</ref> presents a different view of the results-the initial score is plotted on the x axis, and the final score is plotted on the y axis. Points above the reference line y = x represent cases where interaction improved performance.</p><p>We note two striking observations: First, unlike last year <ref type="bibr" coords="14,334.18,388.19,78.96,9.46" target="#b1">(Dang et al., 2007)</ref>, most systems outperformed the baseline. 1 This is encouraging for the development of the field as a whole. Second, many interactions were detrimental, i.e., the pyramid F-score of the final run was higher than that of the initial run. Once again, this was different from last year, where interactions generally yielded small gains. We believe this effect to be caused by a combination of factors: problems with the task setup (more below); technical issues in deploying live Web-based QA systems; and the broadening of the design space that truly allows for effective and non-effective interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Assessors Feedback about Interactions</head><p>The majority of interactions submitted by participants involved eliciting some type of relevance feedback from assessors. Items presented to assessors for feedback varied and included terms, sentences, articles from Wikipedia, and entire answer sets. A couple of systems asked assessors to interactively construct answers to their questions using sentences and documents. One interaction technique asked assessors to respond to open-ended questions modeled after a reference exchange, while another technique asked assessors to indicate their preferences for answer types. While most of the interactions went smoothly, at least two sites had network difficulties which impacted the interactions assessors had with their systems.</p><p>Figure <ref type="figure" coords="14,120.15,614.74,5.45,9.46">4</ref> presents the mean quantitative ratings provided by subjects for three questions:</p><p>1. How easy was it to understand how to interact with this system?</p><p>1 There were indexing issues with UNC's initial submission, which readily explains one of the two below-baseline performers. The other run, from the University of Maryland, experimented with abstractive techniques for question answering-i.e., the runs contained responses that were not found in any source document.  2. How coherent was this interaction?</p><p>3. Overall, how would you rate the quality of the interaction?</p><p>In all cases, higher scores are more positive. It is important to note that this is a small, unrepresentative, and unusual sample, so these results should be viewed cautiously. These results are by no means definitive and/or generalizable beyond this evaluation context.</p><p>Interactions that were rated the lowest with respect to understanding (interactions 8, 13 and 14) and coherence (interactions 8 and 14) had information-dense interfaces and often required multiple steps (one of these interactions was answer construction). Interactions rated most positively for these two attributes were traditional relevance feedback interfaces. Interestingly, understanding and coherence were positively correlated with one another (r = 0.949, p &lt; 0.01), but were negatively correlated with assessors' overall quality ratings (r = -0.533, p &lt; 0.05 and r = -0.674, p &lt; 0.01, respectively). The interaction that received the lowest quality assessment scored fairly high on understanding and coherence. This interaction was the ciQA baseline interaction, which elicited sentence-level relevance feedback. The interaction that received some of the lowest assessments for understanding and coherence received one of the highest overall quality scores (interaction 14). This interaction consisted of building answers and may have received a higher quality score because of its novelty. It also engaged assessors in the most interaction which may be why scores on these three measures differ.</p><p>The qualitative feedback from the final set of questions asking assessors about their entire experiences showed that assessors preferred the traditional relevance feedback interactions, felt considerable time pressure, and did not like the complicated interactions. One assessor indicated a preference for one of the answer construction interactions, while another did not like this interaction. At least two assessors were puzzled about the use of Wikipedia and were displeased with this interaction.</p><p>Data from the exit questionnaire should viewed cautiously for several reasons. Some interactions were less than perfect because of network problems, so assessors' evaluations, in part, reflect this. Assessors' comments indicated that they felt huge time pressures, which may by why such an overwhelming preference was indicated for simple, easily understood and executed interactions such as those that employed relevance feedback.</p><p>One of the most interesting results of this evaluation was that it revealed several limitations of this style of evaluation in the context of TREC. Many of the limitations stem from the fact that assessors already know a great deal about their topics before they engage in interactions. The approach in TREC has traditionally been to have the same person develop the topic and assess its answers, since the assessor is supposed to act as a surrogate user with his/her own particular information needs. However, in developing the topic for ciQA, this "user" researches the topic (to make sure that it is a suitable topic for the particular document collection) and consequently knows more about the topic than a naive user issuing the query.<ref type="foot" coords="17,411.11,209.30,3.99,6.91" target="#foot_0">2</ref> NIST assessors are unusual "users" and it is unrealistic to expect them to assume dual roles as assessors (during topic development and answer evaluation) and naive users (during the interactions).</p><p>Helping users learn more about their topics and helping systems learn more about users are central goals of interactive systems. The exit questionnaire reveals that interactive techniques for addressing these goals cannot be evaluated using the ciQA experimental framework. Additionally, not all ciQA participants understood that assessors already knew the answers to the questions they were asking so there may also have been a mismatch between participants' and assessors' expectations of the interactions.</p><p>4 Future of the QA Track TREC 2007 revealed limitations in the ciQA design for evaluating interactive systems. These limitations could not be reconciled within the NIST evaluation framework, and hence it was decided not to attempt another interactive QA task in 2008.</p><p>The primary goal of the TREC 2007 main task (and what distinguished it from previous TREC QA tasks) was the introduction of blog text to encourage research in NLP techniques that would handle illformed language and discourse structures that are more informal and less reliable than newswire. Questions were asked over a combined newswire (AQUAINT-2) and blog (Blog06) corpus, rather than only a blog corpus, in order to ease participants' transition from newswire. However, because most of the TREC 2007 questions requested factual information, they did not specifically test systems' ability to process blog text, as answers still came predominantly from the AQUAINT-2 corpus.</p><p>This mismatch between the corpus and the information need expressed in the questions naturally suggests that in order to move away from traditional newswire towards blogs, the QA task should be changed so that the questions are more targeted towards characteristics that are particular to blogs. Because blogs naturally contain a large amount of opinions, it was decided that the QA task for 2008 should focus on questions that ask about people's opinions. Questions would still be grouped into series focused by a particular target (person, organization, etc.), but there would be no factoid questions. 3 Rather, each series would comprise rigid list questions (e.g., "What people have good opinions of Sean Hannity?") which would be evaluated in the same manner as TREC 2007 list questions, and squishy list questions (e.g., "What reasons do people give for liking Sean Hannity?") which would be evaluated with the nugget pyramid method used for TREC 2007 Other questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,436.57,468.00,9.46;3,72.00,450.12,344.67,9.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample question series from the test set. Series 219 has a PERSON as the target, series 254 has an ORGANIZATION as the target, and series 269 has an EVENT as the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,72.00,631.63,468.00,9.46;15,72.00,644.83,468.00,9.81;15,72.00,658.73,253.95,9.46"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatter plot showing initial and final pyramid F-scores for each run pair submitted to the TREC 2007 ciQA task. Points above the line y = x represent interactions that increased answer quality. Note that most systems outperformed the sentence retrieval baseline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="14,90.00,72.00,432.01,164.32"><head></head><label></label><figDesc></figDesc><graphic coords="14,90.00,72.00,432.01,164.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="16,126.00,72.00,359.99,229.30"><head></head><label></label><figDesc></figDesc><graphic coords="16,126.00,72.00,359.99,229.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,290.14,673.40,249.86,9.46"><head>Table 1 .</head><label>1</label><figDesc>Of the 70 targets, 19 were PERSONs, 17 were</figDesc><table coords="3,78.38,75.78,481.70,335.43"><row><cell cols="2">219 Target: Iraqi defector Curveball</cell></row><row><cell cols="2">219.1 FACTOID What year did Curveball defect?</cell></row><row><cell cols="2">219.2 FACTOID What was Curveball's profession?</cell></row><row><cell cols="2">219.3 FACTOID What is Curveball's real name?</cell></row><row><cell cols="2">219.4 FACTOID Which intelligence service employed Curveball?</cell></row><row><cell>219.5 LIST</cell><cell>Which US government officials accepted his claims regarding Iraqi weapons labs?</cell></row><row><cell cols="2">219.6 FACTOID Where does Curveball now live?</cell></row><row><cell>219.7 OTHER</cell><cell></cell></row><row><cell cols="2">254 Target: House of Chanel</cell></row><row><cell cols="2">254.1 FACTOID Who founded the House of Chanel?</cell></row><row><cell cols="2">254.2 FACTOID In what year was the company founded?</cell></row><row><cell cols="2">254.3 FACTOID Who is the president of the House of Chanel?</cell></row><row><cell cols="2">254.4 FACTOID Who took over the House of Chanel in 1983?</cell></row><row><cell>254.5 LIST</cell><cell>What women have worn Chanel clothing to award ceremonies?</cell></row><row><cell>254.6 LIST</cell><cell>What museums have displayed Chanel clothing?</cell></row><row><cell cols="2">254.7 FACTOID What Chanel creation is the top-selling fragrance in the world?</cell></row><row><cell>254.8 OTHER</cell><cell></cell></row><row><cell cols="2">269 Target: Pakistan earthquakes of October 2005</cell></row><row><cell cols="2">269.1 FACTOID On what date did this earthquake strike?</cell></row><row><cell>269.2 LIST</cell><cell>What countries were affected by this earthquake?</cell></row><row><cell cols="2">269.3 FACTOID What was the final death toll from this earthquake?</cell></row><row><cell cols="2">269.4 FACTOID What was the strength of this earthquake?</cell></row><row><cell cols="2">269.5 FACTOID Where was the epicenter (latitude and longitude)?</cell></row><row><cell>269.6 LIST</cell><cell>What countries supplied aid?</cell></row><row><cell>269.7 OTHER</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,75.67,157.86,467.88,449.66"><head>Table 1 :</head><label>1</label><figDesc>Targets of the 70 question series.</figDesc><table coords="4,75.67,157.86,467.88,415.11"><row><cell>216 Paul Krugman</cell><cell>251 Lyme disease</cell></row><row><cell>217 Jay-Z</cell><cell>252 American Girl dolls</cell></row><row><cell>218 impressionist Darrell Hammond</cell><cell>253 Kurt Weill</cell></row><row><cell>219 Iraqi defector Curveball</cell><cell>254 House of Chanel</cell></row><row><cell>220 International Management Group (IMG)</cell><cell>255 British American Tobacco (BAT)</cell></row><row><cell>221 U.S. Mint</cell><cell>256 Buffalo Soldiers</cell></row><row><cell>222 3M</cell><cell>257 2005 DARPA Grand Challenge</cell></row><row><cell>223 Merrill Lynch &amp; Co.</cell><cell>258 2005 presidential election in Egypt</cell></row><row><cell>224 WWE</cell><cell>259 2005 World Snooker Championships</cell></row><row><cell>225 Sago Mine disaster</cell><cell>260 Teenage Mutant Ninja Turtles (TMNT)</cell></row><row><cell cols="2">226 Harriet Miers withdraws nomination to Supreme Court 261 marsupials</cell></row><row><cell>227 Robert Blake criminal trial</cell><cell>262 kumquat</cell></row><row><cell>228 March Madness 2006</cell><cell>263 Ayn Rand</cell></row><row><cell>229 first partial face transplant</cell><cell>264 Alan Greenspan</cell></row><row><cell>230 AMT</cell><cell>265 Mahmud (or Mahmood, Mahmoud) Ahmadinejad</cell></row><row><cell>231 USS Abraham Lincoln</cell><cell>266 Rafik Hariri, former Lebanese Prime Minister</cell></row><row><cell>232 Dulles Airport</cell><cell>267 FISA Court</cell></row><row><cell>233 comic strip Blondie</cell><cell>268 Israel evacuation of the Gaza Strip</cell></row><row><cell>234 Irving Berlin</cell><cell>269 Pakistan earthquakes of October 2005</cell></row><row><cell>235 Susan Butcher</cell><cell>270 The Mars rovers, Spirit and Opportunity</cell></row><row><cell>236 Boston Pops</cell><cell>271 Jon Bon Jovi</cell></row><row><cell>237 Cunard Cruise Lines</cell><cell>272 Barack Obama</cell></row><row><cell>238 2004 Baseball World Series</cell><cell>273 Rush Limbaugh</cell></row><row><cell>239 game show Jeopardy</cell><cell>274 Exxon Mobile Corp</cell></row><row><cell>240 Harry Potter and the Goblet of Fire</cell><cell>275 Dixie Chicks</cell></row><row><cell>241 Jasper Fforde</cell><cell>276 B-17 bomber</cell></row><row><cell>242 Guinness Brewery</cell><cell>277 Boeing 777 aircraft</cell></row><row><cell>243 2005 London terror bombing attacks</cell><cell>278 St. Peter's Basilica</cell></row><row><cell>244 Rubik's Cube Competitions</cell><cell>279 Australian wine</cell></row><row><cell>245 hybrid cars</cell><cell>280 Angkor Wat temples</cell></row><row><cell>246 Michael Brown</cell><cell>281 Joseph Steffen</cell></row><row><cell>247 Ella Fitzgerald</cell><cell>282 Orhan Pamuk</cell></row><row><cell>248 CSPI</cell><cell>283 Habitat for Humanity</cell></row><row><cell>249 Fulbright Program</cell><cell>284 CAFTA approval by U.S. Congress</cell></row><row><cell>250 publication of Danish cartoons of Mohammed</cell><cell>285 Yeti</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,509.71,678.98,30.29,9.46"><head>Table 2 :</head><label>2</label><figDesc>Evaluation scores for the factoid component. Scores are shown for the best run from the top 10 groups.</figDesc><table coords="5,509.71,678.98,30.29,9.46"><row><cell>French</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.00,74.78,468.00,193.71"><head>Table 3 :</head><label>3</label><figDesc>Average F-scores for the list question component. Scores are shown for the best run from the top 10 groups.</figDesc><table coords="7,125.10,74.78,361.80,145.35"><row><cell>Run Tag</cell><cell>Submitter</cell><cell>F</cell></row><row><cell>LymbaPA07</cell><cell>Lymba Corporation</cell><cell>0.479</cell></row><row><cell>LCCFerret</cell><cell>Language Computer Corporation</cell><cell>0.324</cell></row><row><cell>ILQUA1</cell><cell>State University of New York (SUNY) at Albany</cell><cell>0.147</cell></row><row><cell>QASCU3</cell><cell>Concordia University</cell><cell>0.145</cell></row><row><cell>Ephyra3</cell><cell cols="2">Carnegie Mellon University and Universitaet Karlsruhe 0.144</cell></row><row><cell>UofL</cell><cell>University of Lethbridge</cell><cell>0.132</cell></row><row><cell>FDUQAT16B</cell><cell>Fudan University</cell><cell>0.131</cell></row><row><cell cols="2">IITDIBM2007T Indian Institute Of Technology, Delhi</cell><cell>0.125</cell></row><row><cell>FDUQAT16A</cell><cell>Fudan University</cell><cell>0.107</cell></row><row><cell>pronto07run3</cell><cell>Universita di Roma "La Sapienza"</cell><cell>0.103</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,72.00,74.43,468.00,194.20"><head>Table 4 :</head><label>4</label><figDesc>Average F-scores (β = 3) for the Other questions. Scores are shown for the best run from the top 10 groups.</figDesc><table coords="8,131.49,74.43,349.03,145.70"><row><cell>Run Tag</cell><cell>Submitter</cell><cell>F(β = 3)</cell></row><row><cell>FDUQAT16B</cell><cell>Fudan University</cell><cell>0.329</cell></row><row><cell>lsv2007c</cell><cell>Saarland University</cell><cell>0.299</cell></row><row><cell>QASCU2</cell><cell>Concordia University</cell><cell>0.281</cell></row><row><cell>LymbaPA07</cell><cell>Lymba Corporation</cell><cell>0.281</cell></row><row><cell>LCCFerret</cell><cell>Language Computer Corporation</cell><cell>0.261</cell></row><row><cell>ILQUA1</cell><cell>State University of New York (SUNY) at Albany</cell><cell>0.242</cell></row><row><cell>csail3</cell><cell>Massachusetts Institute of Technology (MIT)</cell><cell>0.235</cell></row><row><cell>uams07main</cell><cell>University of Amsterdam</cell><cell>0.209</cell></row><row><cell cols="2">IITDIBM2007S Indian Institute Of Technology, Delhi</cell><cell>0.209</cell></row><row><cell>QUANTA</cell><cell>Tsinghua University (State Key Lab)</cell><cell>0.194</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,72.42,538.93,467.16,151.18"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table coords="9,81.52,538.93,448.96,128.59"><row><cell>RunID</cell><cell>Submitter</cell><cell>Score</cell><cell></cell></row><row><cell>LymbaPA07</cell><cell>Lymba Corporation</cell><cell>0.4839 A</cell><cell></cell></row><row><cell>LCCFerret</cell><cell>Language Computer Corporation</cell><cell>0.3575</cell><cell>B</cell></row><row><cell>FDUQAT16B</cell><cell>Fudan University</cell><cell>0.2310</cell><cell>C</cell></row><row><cell>lsv2007c</cell><cell>Saarland University</cell><cell>0.2296</cell><cell>C</cell></row><row><cell>QASCU1</cell><cell>Concordia University</cell><cell>0.2216</cell><cell>C D</cell></row><row><cell>ILQUA1</cell><cell>State University of New York (SUNY) at Albany</cell><cell>0.2023</cell><cell>C D E</cell></row><row><cell>Ephyra1</cell><cell cols="2">Carnegie Mellon University and Universitaet Karlsruhe 0.1804</cell><cell>C D E F</cell></row><row><cell cols="2">IITDIBM2007T Indian Institute Of Technology, Delhi</cell><cell>0.1735</cell><cell>D E F</cell></row><row><cell>QUANTA</cell><cell>Tsinghua University (State Key Lab)</cell><cell>0.1592</cell><cell>E F</cell></row><row><cell>csail3</cell><cell>Massachusetts Institute of Technology (MIT)</cell><cell>0.1415</cell><cell>F</cell></row></table><note coords="9,110.99,680.65,428.60,9.46"><p>Multiple comparison of the best run from the top 10 groups, based on ANOVA of per-series score.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="15,72.00,96.28,468.00,518.24"><head>Table 7 :</head><label>7</label><figDesc>Performance of the twelve initial-final run pairs submitted to the TREC 2007 ciQA task. The sentence retrieval baseline is provided as a reference.</figDesc><table coords="15,313.85,96.28,194.23,9.81"><row><cell>Run tags</cell><cell>Pyramid F-Score</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="17,88.14,640.22,73.09,7.77"><p>Results of questions</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="17,163.91,640.22,376.10,7.77;17,72.00,651.18,468.00,7.77;17,72.00,662.14,243.46,7.77;17,84.65,671.45,2.99,5.18;17,88.14,673.32,451.86,7.77;17,72.00,684.28,340.61,7.77"><p>3, 4, and 5 from the exit questionnaire, which asked assessors to indicate how much they learned about their topics through the interaction (see Figure2for specific questions) are not presented because some assessors indicated that these values were low because they already knew about their topics.3 It was pointed out that asking factoid type questions about opinions seemed inappropriate, and after nine years of factoid questions (starting in TREC 1999), it was time to retire factoids from the QA track in any case.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="18,72.00,100.70,468.00,9.46;18,88.94,114.06,312.60,9.39" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,166.57,100.70,352.91,9.46">HARD track overview in TREC 2005: High accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,88.94,114.06,248.66,9.39">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2006. TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,136.42,468.00,9.46;18,88.94,149.78,451.07,9.64;18,88.94,163.33,423.91,9.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,224.97,136.42,315.03,9.46;18,88.94,149.96,227.01,9.46">Different structures for evaluating answers to complex questions: Pyramids won&apos;t topple, and neither will human assessors</title>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,336.19,149.78,203.81,9.39;18,88.94,163.33,237.43,9.39">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007)</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="768" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,185.68,468.00,9.46;18,88.94,199.05,451.07,9.64;18,88.94,212.78,21.51,9.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,317.88,185.68,222.12,9.46;18,88.94,199.23,23.47,9.46">Overview of the TREC 2006 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Trang</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diane</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,135.99,199.05,305.46,9.39">Proceedings of the Fifteenth Text REtrieval Conference (TREC 2006)</title>
		<meeting>the Fifteenth Text REtrieval Conference (TREC 2006)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,234.95,468.00,9.46;18,88.94,248.31,451.06,9.64;18,88.94,261.86,451.07,9.39;18,88.94,275.41,88.47,9.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,336.53,234.95,203.47,9.46;18,88.94,248.50,80.87,9.46">Answering definition questions with multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,189.58,248.31,350.42,9.39;18,88.94,261.86,451.07,9.39;18,88.94,275.41,23.48,9.39">Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004)</title>
		<meeting>the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,297.77,468.00,9.46;18,88.94,311.31,451.07,9.46;18,88.94,324.68,451.06,9.64;18,88.94,338.41,22.01,9.46;18,72.00,360.40,468.00,9.64;18,88.94,373.95,451.07,9.39;18,88.94,387.49,451.06,9.64;18,88.94,401.23,47.00,9.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,427.20,297.77,112.80,9.46;18,88.94,311.31,430.66,9.46;18,279.03,360.58,190.16,9.46">Handling information access dialogue through QA technologies-A novel challenge for open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Tsuneaki</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun'ichi</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fumito</forename><surname>Masui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,88.94,324.68,384.28,9.39;18,485.96,360.40,54.04,9.39;18,88.94,373.95,451.07,9.39;18,88.94,387.49,322.58,9.39">Proceedings of the 2006 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2006)</title>
		<editor>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</editor>
		<meeting>the 2006 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2006)<address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05">2004. May. 2006</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
	<note>Proceedings of the HLT-NAACL 2004 Workshop on Pragmatics of Question Answering</note>
</biblStruct>

<biblStruct coords="18,72.00,423.40,468.00,9.46;18,88.94,436.76,451.06,9.64;18,88.94,450.31,451.07,9.39;18,88.94,463.86,260.26,9.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,169.11,423.40,370.89,9.46;18,88.94,436.95,131.21,9.46">Is question answering better than information retrieval? A task-based evaluation framework for question series</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,242.96,436.76,297.03,9.39;18,88.94,450.31,451.07,9.39;18,88.94,463.86,85.99,9.39">Proceedings of the 2007 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2007)</title>
		<meeting>the 2007 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2007)<address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,486.21,468.00,9.46;18,88.94,499.76,451.07,9.46;18,88.94,513.31,100.69,9.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="18,255.87,486.21,284.13,9.46;18,88.94,499.76,41.59,9.46">The TREC blog06 collection: Creating and analysing a blog test collection</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report DCS Technical Report</note>
</biblStruct>

<biblStruct coords="18,72.00,535.48,468.00,9.46;18,88.94,548.85,312.60,9.39" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,272.23,535.48,248.33,9.46">Overview of the TREC 2005 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,88.94,548.85,248.66,9.39">Proceedings of the Fourteenth Text REtrieval Conference</title>
		<meeting>the Fourteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2006. TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,571.01,468.00,9.64;18,88.94,584.56,273.44,9.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,190.49,571.20,246.24,9.46">Overview of the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,458.19,571.01,81.81,9.39;18,88.94,584.56,208.45,9.39">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="54" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,606.73,468.00,9.64;18,88.94,620.28,287.74,9.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,194.87,606.92,242.82,9.46">Overview of the TREC 2004 Question Answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,458.31,606.73,81.70,9.39;18,88.94,620.28,222.74,9.39">Proceedings of the Thirteenth Text REtreival Conference (TREC 2004)</title>
		<meeting>the Thirteenth Text REtreival Conference (TREC 2004)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,72.00,642.64,468.00,9.46;18,88.94,656.00,451.06,9.64;18,88.94,669.55,349.21,9.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,204.46,642.64,331.33,9.46">Using question series to evaluate question answering system effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,102.06,656.00,437.94,9.39;18,88.94,669.55,273.31,9.39">Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005)</title>
		<meeting>the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
