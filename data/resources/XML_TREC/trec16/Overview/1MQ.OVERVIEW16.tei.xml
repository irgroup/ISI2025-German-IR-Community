<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.48,112.05,247.04,15.12">Million Query Track 2007 Overview</title>
				<funder ref="#_zk4dS58 #_Yzf4T3a">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_XX8Tp4k">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Live Labs</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.45,144.53,61.58,10.48"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Intelligent Information Retrieval</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.44,144.53,75.38,10.48"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Intelligent Information Retrieval</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.01,144.53,87.16,10.48"><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam+</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,148.21,158.48,68.37,10.48"><forename type="first">Virgil</forename><surname>Pavlu+</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.40,158.48,87.90,10.48"><forename type="first">Blagovest</forename><surname>Dachev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Intelligent Information Retrieval</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.62,158.48,111.17,10.48"><forename type="first">Evangelos</forename><surname>Kanoulas+</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.48,112.05,247.04,15.12">Million Query Track 2007 Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">15475A7C3949CD88B81AB48CD4F7A7C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Million Query (1MQ) track ran for the first time in TREC 2007. It was designed to serve two purposes. First, it was an exploration of ad-hoc retrieval on a large collection of documents. Second, it investigated questions of system evaluation, particularly whether it is better to evaluate using many shallow judgments or fewer thorough judgments.</p><p>Participants in this track were assigned two tasks: (1) run 10,000 queries against a 426Gb collection of documents at least once and (2) judge documents for relevance with respect to some number of queries.</p><p>Section 1 describes how the corpus and queries were selected, details the submission formats, and provides a brief description of all submitted runs. Section 2 provides an overview of the judging process, including a sketch of how it alternated between two methods for selecting the small set of documents to be judged. Sections 3 and 4 provide details of those two selection methods, developed at UMass and NEU, respectively. The sections also provide some analysis of the results.</p><p>In Section 6 we present some statistics about the judging process, such as the total number of queries judged, how many by each approach, and so on. We present some additional results and analysis of the overall track in Sections 7 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Phase I: Running Queries</head><p>The first phase of the track required that participating sites submit their retrieval runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Corpus</head><p>The 1MQ track used the so-called "terabyte" or "GOV2" collection of documents. This corpus is a collection of Web data crawled from Web sites in the .gov domain in early 2004. The collection is believed to include a large proportion of the .gov pages that were crawlable at that time, including HTML and text, plus the extracted text of PDF, Word, and PostScript files. Any document longer than 256Kb was truncated to that size at the time the collection was built. Binary files are not included as part of the collection, though were captured separately for use in judging.</p><p>The GOV2 collection includes 25 million documents in 426 gigabytes. The collection was made available by the University of Glasgow, distributed on a hard disk that was shipped to participants for an amount intended to cover the cost of preparing and shipping the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Queries</head><p>Topics for this task were drawn from a large collection of queries that were collected by a large Internet search engine. Each of the chosen queries is likely to have at least one relevant document in the GOV2 collection because logs showed a clickthrough on one page captured by GOV2. Obviously there is no guarantee that the clicked page is relevant, but it increases the chance of the query being appropriate for the collection.</p><p>These topics are short, title-length (in TREC parlance) queries. In the judging phase, they were developed into full-blown TREC topics.</p><p>Ten thousand (10,000) queries were selected for the official run. The 10,000 queries included 150 queries that were judged in the context of the 2005 Terabyte Track <ref type="bibr" coords="2,350.69,134.93,15.50,8.74" target="#b11">[12]</ref> (though one of these had no relevant documents and was therefore excluded).</p><p>No quality control was imposed on the 10,000 selected queries. The hope was that most of them would be good quality queries, but it was recognized that some were likely to be partially or entirely non-English, to contain spelling errors, or even to be incomprehensible to anyone other than the person who originally created them.</p><p>The queries were distributed in a text file where each line has the format "N:query word or words". Here, N is the query number, is followed by a colon, and immediately followed by the query itself. For example, the line (from a training query) "32:barack obama internships" means that query number 32 is the 3-word query "barack obama internships". All queries were provided in lowercase and with no punctuation (it is not clear whether that formatting is a result of processing or because people use lowercase and do not use punctuation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Submissions</head><p>Sites were permitted to provide up to five runs. Every submitted run was included in the judging pool and all were treated equally.</p><p>A run consisted of up to the top 1,000 documents for each of the 10,000 queries. The submission format was a standard TREC format of exactly six columns per line with at least one space between the columns. For example: 100 Q0 ZF08-175-870 1 9876 mysys1 100 Q0 ZF08-306-044 2 9875 mysys2 where:</p><p>1. The first column is the topic number.</p><p>2. The second column is unused but must always be the string "Q0" (letter Q, number zero).</p><p>3. The third column is the official document number of the retrieved document, found in the &lt;DOCNO&gt; field of the document.</p><p>4. The fourth column is the rank of that document for that query.</p><p>5. The fifth column is the score this system generated to rank this document.</p><p>6. The six column was a "run tag," a unique identifier for each group and run.</p><p>If a site would normally have returned no documents for a query, it instead returned the single document "GX000-00-0000000" at rank one. Doing so maintained consistent evaluation results (averages over the same number of queries) and did not break any evaluation tools being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Submitted runs</head><p>The following is a brief summary of some of the submitted runs. The summaries were provided by the sites themselves and are listed in alphabetical order. (When no full summary is available, the brief summary information from the submissions has been used.)</p><p>ARSC/University of Alaska Fairbanks The ARSC multisearch system is a heterogeneous distributed information retrieval simulation and demonstration implementation. The purpose of the simulation is to illustrate performance issues in Grid Information Retrieval applications by partitioning the GOV2 collection into a large number of hosts and searching each host independently of the others. Previous TREC Terabyte Track experiments using the ARSC multisearch system have focused on the IR performance of multisearch result-set merging and the efficiency gains from truncating result-sets from a large collection of hosts before merging.</p><p>The primary task of the ARSC multisearch system in the 2007 TREC Million Query experiment is to estimate the number of hosts or subcollections of GOV2 that can be used to process 10,000 queries within the TREC Million Query Track time constraints. The secondary and ongoing task is to construct an effective strategy for picking a subsets of the GOV2 collections to search at query-time. The hostselection strategy used for this experiment was to restrict searches to hosts that returned the most relevant documents in previous TREC Terabyte Tracks.</p><p>Exegy Exegy's submission for the TREC 2007 million query track consisted of results obtained by running the queries against the raw data, i.e., the data was not indexed. The hardware-accelerated streaming engine used to perform the search is the Exegy Text Miner (XTM), developed at Exegy, inc. The search engine's architecture is novel: XTM is a hybrid system (heterogeneous compute platform) employing general purpose processors (GPPs) and field programmable gate arrays (FPGAs) in a hardware-software co-design architecture to perform the search. The GPPs are responsible for inputting the data to the FPGAs and reading and post-processing the search results that the FPGAs output. The FPGAs perform the actual search and due to the high degree of parallelism available (including pipelining) are able to do so much more efficiently than the GPP.</p><p>For the million query track the results for a particular query were obtained by searching for the exact query string within the corpus. This brute force approach, although naïve, returned relevant results for most of the queries. The mean-average precision for the results was 0.3106 and 0.0529 using the UMass and the NEU approaches, respectively. More importantly, XTM completed the search for the entire set of the 10,000 queries on the unindexed data in less than two and a half hours.</p><p>Heilongjiang Institute of Technology, China Used Lemur.</p><p>IBM Haifa This year, the experiments of IBM Haifa were focused on the scoring function of Lucene, an Apache open-source search engine. The main goal was to bring Lucene's ranking function to the same level as the state-of-the-art ranking formulas like those traditionally used by TREC participants. Lucene's scoring function was modified to include better document length normalization, and a better term-weight setting following to the SMART model.</p><p>Lucene then compared to Juru, the home-brewed search engine used by the group in previous TREC conferences. In order to examine the ranking function alone, both Lucene and Juru used the same HTML parser, the same anchor text, and the same query parsing process including stop-word removal, synonym expansion, and phrase expansion. Based on the 149 topics of the Terabyte tracks, the results of modified Lucene significantly outperform the original Lucene and are comparable to Juru's results.</p><p>In addition, a shallow query log analysis was conducted over the 10K query log. Based on the query log, a specific stop-list and a synonym-table were constructed to be used by both search engines.</p><p>Northeastern University We used several standard Lemur built in systems (tfidf bm25, tfidf log, kl abs,kl dir,inquery,cos, okapi) and combined their output (metasearch) using the hedge algorithm.</p><p>RMIT Zettair Dirichlet smoothed language model run.</p><p>SabIR Standard smart ltu.Lnu run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>University of Amsterdam</head><p>The University of Amsterdam, in collaboration with the University of Twente, participated with the main aim to compare results of the earlier Terabyte tracks to the Million Query track. Specifically, what is the impact of shallow pooling methods on the (apparent) effectiveness of retrieval techniques? And what is the impact of substantially larger numbers of topics? We submitted a number of runs using different document representations (such as full-text, title-fields, or incoming anchor-texts) to increase pool diversity. The initial results show broad agreement in system rankings over various measures on topic sets judged at both Terabyte and Million Query tracks, with runs using the full-text index giving superior results on all measures. There are some noteworthy upsets: measures using the Million Query judged topics show stronger correlation with precision at early ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>University of Massachusetts Amherst</head><p>The base UMass Amherst submissions were a simple query likelihood model and the dependence model approach fielded during the terabyte track last year. We also tried some simple automatic spelling correction on top of each baseline to deal with errors of that kind. All runs were done using the Indri retrieval system.</p><p>University of Melbourne Four types of runs were submitted:</p><p>1. A topic-only run using a similarity metric based on a language model with Dirichlet smoothing as describe by Zhai and Lafferty (2004).</p><p>2. Submit query to public web search engine, retrieve snippet information for top 5 documents, add unique terms from snippets to query, run expanded query using same similarity metric just described.</p><p>3. A standard impact-based ranking.</p><p>4. A merging of the language modeling and the impact runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Phase I: Relevance judgments and judging</head><p>After all runs were submitted, a subset of the topics were judged. The goal was to provide a small number of judgments for a large number of topics. For TREC 2007, over 1700 queries were judged, a large increase over the more typical 50 queries judged by other tracks in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Judging overview</head><p>Judging was done by assessors at NIST and by participants in the track. Non-participants were welcome (encouraged!) to provide judgments, too, though very few such judgments occurred. Some of the judgments came from an Information Retrieval class project, and some were provided by hired assessors at UMass. The bulk of judgments, however, came from the NIST assessors. The process looked roughly like this from the perspective of someone judging:</p><p>1. The assessment system presented 10 queries randomly selected from the evaluation set of 10,000 queries.</p><p>2. The assessor selected one of those ten queries to judge. The others were returned to the pool.</p><p>3. The assessor provided the description and narrative parts of the query, creating a full TREC topic. This information was used by the assessor to keep focus on what is relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>The system presented a GOV2 document (Web page) and asked whether it was relevant to the query. Judgments were on a three-way scale to mimic the Terabyte Track from years past: highly relevant, relevant, or not relevant. Consistent with past practice, the distinction between the first two was up to the assessor.</p><p>5. The assessor was required to continue judging until 40 documents has been judged. An assessor could optionally continue beyond the 40, but few did.</p><p>The system for carrying out those judgments was built at UMass on top of the Drupal content management platform<ref type="foot" coords="4,109.39,597.04,3.97,6.12" target="#foot_0">1</ref> . The same system was used as the starting point for relevance judgments in the Enterprise track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Selection of documents for judging</head><p>Two approaches to selecting documents were used:</p><p>Minimal Test Collection (MTC) method. In this method, documents are selected by how much they inform us about the difference in mean average precision given all the judgments that were made up to that point <ref type="bibr" coords="4,159.01,688.73,14.61,8.74" target="#b9">[10]</ref>. Because average precision is quadratic in relevance judgments, the amount each relevant document contributes is a function of the total number of judgments made and the ranks they appear at. Nonrelevant documents also contribute to our knowledge: if a document is nonrelevant, it tells us that certain terms cannot contribute anything to average precision. We quantify how much a document will contribute if it turns out to be relevant or nonrelevant, then select the one that we expect to contribute the most. This method is further described below in Section 3.</p><p>Statistical evaluation (statMAP) method. This method draws and judges a specific random sample of documents from the given ranked lists and produces unbiased, low-variance estimates of average precision, R-precision, and precision at standard cutoffs from these judged documents <ref type="bibr" coords="5,475.68,166.81,9.96,8.74" target="#b0">[1]</ref>. Additional (non-random) judged documents may also be included in the estimation process, further improving the quality of the estimates. This method is further described below in Section 4.</p><p>For each query, one of the following happened:</p><p>1. The pages to be judged for the query were selected by the "expected AP method." A minimum of 40 documents were judged, though the assessor was allowed to continue beyond 40 if so motivated.</p><p>2. The pages to be judged for the query were selected by the "statistical evaluation method." A minimum of 40 documents were judged, though the assessor was allowed continue beyond 40 if so motivated.</p><p>3. The pages to be judged were selected by alternating between the two methods until each has selected 20 pages. If a page was selected by more than one method, it was presented for judgment only once. The process continues until at least 40 pages have been judged (typically 20 per method), though the assessor was allowed continue beyond 40 if so motivated. (See Section 5.)</p><p>The assignments were made such that option (3) was selected half the time and the other two options each occurred 1/4 of the time. When completed, roughly half of the queries therefore had parallel judgments of 20 or more pages by each method, and the other half had 40 or more judgments by a single method.</p><p>In addition, a small pool of 50 queries were randomly selected for multiple judging. With a small random chance, the assessor's ten queries were drawn from that pool rather than the full pool. Whereas in the full pool no query was considered by more than one person, in the multiple judging pool, a query could be considered by any or even all assessors-though no assessor was shown the same query more than once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UMass Method</head><p>The UMass algorithm is a greedy anytime algorithm. It iteratively orders documents according to how much information they provide about a difference in average precision, presents the top document to be judged, and, based on the judgment, re-weights and re-orders the documents. Algorithm 3.1 shows the high-level pseudo-code for the algorithm, which we call MTC for minimal test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3.1 MTC(S, Q)</head><p>Require: a set of ranked lists S, a set of qrels Q (possibly empty)</p><p>1: q = get-qrels(Q) 2: w = init-weights(S, q) 3: loop</p><formula xml:id="formula_0" coords="5,77.37,607.25,90.14,21.88">4: i * = arg max i w 5:</formula><p>request judgment for document i *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>receive judgment j i * for document i *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>w = update-weights(i * , S)</p><p>8:</p><formula xml:id="formula_1" coords="5,98.90,656.65,35.80,9.65">q i * = j i *</formula><p>Here q is a vector of relevance judgments read in from a qrels file if one exists (for example if an assessor is resuming judging a topic that he had previously stopped). get-qrels simply translates (document, judgment) pairs into vector indexes such that q i = 1 if the document has been judged relevant and 0 otherwise; if an assessor is just starting a topic, q i will be 0 for all i. w is a vector of document weights (see below). We assume that there's a global ordering of documents, so that the relevance of document i can be found at index i in q, and its weight at the same index in w.</p><p>The init-weights, set-weights, and update-weights functions are where the real work happens. The pseudo-code below is rather complicated, so first some notational conventions: We shall use i, j = 1 • • • n to enumerate n documents and s = 1 • • • m to enumerate m systems. Capital bold letters are matrices. Column and row vectors for a matrix M are denoted M .i (for the ith column vector) or M i. (for the ith row vector). Matrix cells are referred to with nonbold subscripted letters, e.g. M ij . Lowercase bold letters are vectors, and lowercase nonbold letters are scalars. Superscripts are never exponents, always some type of index. Algorithm 3.2 init-weights(S, q) Require: ranked lists S, a vector of judgments q</p><formula xml:id="formula_2" coords="6,77.37,227.55,98.61,21.88">1: V R = V N = [0] n×m 2:</formula><p>for all s ∈ S do</p><formula xml:id="formula_3" coords="6,77.37,253.01,69.68,20.34">3: C = [0] n×n 4:</formula><p>for all pairs of documents (i, j) do 5:</p><formula xml:id="formula_4" coords="6,72.00,276.95,224.93,121.24">C ij = 1/ max{r s (i), r s (j)} 6: V R .s = Cq + diag(C) 7: V N .s = C(1 -q) 8: return set-weights() Algorithm 3.3 set-weights() Require: access to global weight matrices V R , V N 1: w = [0] n 2:</formula><p>for all unjudged documents i do 3:</p><formula xml:id="formula_5" coords="6,98.90,400.22,108.04,12.32">w R i = max V R i. -min V R i.</formula><p>4:</p><formula xml:id="formula_6" coords="6,98.90,412.18,110.39,12.32">w N i = max V N i. -min V N i.</formula><p>5:</p><formula xml:id="formula_7" coords="6,77.37,424.13,107.09,22.27">w i = max{w R i , w N i } 6: return w</formula><p>Algorithm 3.2 initializes the weight vector. At line 1 we create two "global" weight matrices in which each element V is is the effect a judgment will have on the average precision of system s (see below for more detail). We iterate over systems (line 2), for each run creating a coefficient matrix C (lines 3-5). Each pair of documents has an associated coefficient 1/ max{r s (i), r s (j)}, where r s (i) is the rank of document i in system s (infinity if document i is unranked). In lines 6 and 7, we multiply the coefficient matrix by the qrels vector and assign the resulting vector to the corresponding system column of the weight matrix. At the end of this loop, the matrices V R , V N contain the individual system weights for every document. Each column s contains the weights for system s and each row i the weights for document i.</p><p>The global weight of a document is the maximum difference between pairs of system weights. Global weights are set with the set-weights function, shown in Algorithm 3.3. For each row in the weight matrices, it finds the maximum and minimum weights in any system. The difference between these is the maximum pairwise difference. Then the maximum of w R i and w N i is the final weight of the document. After each judgment, update-weights (Algorithm 3.4) is called to update the global weight matrices and recomputes the document weights. C is constructed by pulling the i * th column from each of the m coefficient matrices C defined in set-weights. We construct it from scratch rather than keep all m C matrices in memory. Global weight matrices are updated simply by adding or subtracting C depending on the judgment to i * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Running Time</head><p>MTC loops until the assessor quits or all documents have been judged. Within the loop, finding the maximum-weight document (line 4) is in O(n). update-weights loops over systems and documents for a </p><formula xml:id="formula_8" coords="7,72.00,135.50,245.52,107.41">for all documents i, C is = 1/ max{r s (i * ), r s (i)} 4: if i * is relevant then 5: V R = V R + C 6: else 7: V N = V N -C 8: return set-weights() runtime in O(m • n). set-weights is also in O(n • m):</formula><p>each max or min is over m elements, and four of them happen n times. Therefore the total runtime for each iteration is in</p><formula xml:id="formula_9" coords="7,395.65,245.56,40.65,8.74">O(m • n).</formula><p>init-weights is in O(m • n 2 ): we loop over m systems, each time performing O(n 2 ) operations to construct C and perform matrix-vector multiplication. Since MTC can iterate up to n times, the total runtime is in O(m • n 2 ).</p><p>In practice, the algorithm was fast enough that assessors experienced no noticeable delay between submitting a judgment and receiving the next document, even though an entire O(m • n) iteration takes place in between. init-weights was slow enough to be noticed, but it ran only once, in the background while assessors defined a topic description and narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explanation</head><p>The pseudo-code is rather opaque, and it may not be immediately clear how it implements the algorithm described in our previous work. Here is the explanation.</p><p>In previous work we showed AP s ∝ i j A ij X i X j , where X i is a binary indicator of the relevance of document i and A ij = 1/ max{r s (i), r s (j)}. See Section 3.3.1 for more details.</p><p>Define a lower bound for AP s in which every unjudged document is assumed to be nonrelevant. An upper bound is similarly defined by assuming every unjudged document relevant. Denote the bounds AP s and AP s respectively.</p><p>Consider document i, ranked at r s (i) by system s. If we judge it relevant, AP s will increase by j|xj =1 a ij x j . If we judge it nonrelevant, AP s will decrease by j|xj =0 a ij x j . These are matrix elements V R is and V N is respectively, computed at steps 4-7 in init-weights and steps 2-7 in update-weights. Now suppose we have two systems s 1 and s 2 . We want to judge the document that's going to have the greatest effect on ∆AP = AP s1 -AP s2 . We can bound ∆AP as we did AP above, but the bounds are much hard to compute exactly. It turns out that that does not matter: it can be proven that the judgment that reduces the upper bound of ∆AP the most is a nonrelevant judgment to the document that maximizes</p><formula xml:id="formula_10" coords="7,72.00,543.70,41.74,12.32">V N is1 -V N is2</formula><p>, and the judgment that increases the lower bound the most is a relevant judgment to the document that maximizes V R is1 -V R is2 . Since we of course do not know the judgment in advance, the final weight of document i is the maximum of these two quantities.</p><p>When we have more than two systems, we simply calculate the weight for each pair and take the maximum over all pairs as the document weight. Since the maximum over all pairs is simply the maximum weight for any system minus the minimum weight for any system, this can be calculated in linear time, as steps 3-5 of set-weights show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">UMass Evaluation</head><p>The evaluation tool mtc-eval takes as input one or more retrieval systems. It calculates EM AP (Eq. 1 below) for each system; these are used to rank the systems. Additionally, it computes E[∆AP ], V ar[∆AP ], and P (∆AP &lt; 0) (Eqs. 2, 3, 4 respectively) for each topic and each pair of systems, and E∆M AP , V∆M AP , and P (∆M AP &lt; 0) (Eqs. 5, 6, 7 respectively) for each pair of systems. More details are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Expected Mean Average Precision</head><p>As we showed in Carterette et al., average precision can be written as a quadratic equation over Bernoulli trials X i for the relevance of document i:</p><formula xml:id="formula_11" coords="8,246.38,122.38,118.35,30.49">AP s = 1 |R| n i=1 j≥i A ij X i X j</formula><p>where A ij = 1/ max{r s (i), r s (j)}.</p><p>Let p i = p(X i = 1). The expectation of AP s is:</p><formula xml:id="formula_12" coords="8,213.45,194.40,185.11,33.62">E[AP s ] ≈ 1 p i n i   A ii p i + j&gt;i A ij p i p j  </formula><p>We can likewise define the expected value of MAP, EM AP , by summing over many topics:</p><formula xml:id="formula_13" coords="8,256.35,261.56,283.65,20.06">EM AP s = t∈T E[AP st ]<label>(1)</label></formula><p>Systems submitted to the track were ranked by EM AP . Probabilities p i can be estimated in several different ways; Section 3 describes the method we used in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">∆M AP and Confidence</head><p>In our previous work we have been more interested in the difference in MAP between two systems rather than the MAPs themselves. In this section we describe ∆M AP and the idea of confidence that an observed difference between systems is "real".</p><p>As in Section 3.2, suppose we have two retrieval systems s 1 and s 2 . Define ∆AP = AP s1 -AP s2 . We can write ∆AP in closed form as:</p><formula xml:id="formula_14" coords="8,252.45,410.43,106.20,30.49">∆AP = n i=1 j≥i C ij X i X j</formula><p>where C ij = 1/ max{r s1 (i), r s1 (j)} -1/ max{r s2 (i), r s2 (j)}.</p><p>∆AP has a distribution over all possible assignments of relevance to the unjudged documents. Some assignments will result in ∆AP &lt; 0, some in ∆AP &gt; 0; if we believe that ∆AP &lt; 0 but there are many possible sets of judgments that could result in ∆AP &gt; 0, then we should say that we have low confidence in our belief.</p><p>As it turns out, ∆AP converges to a normal distribution. This makes it very easy to determine confidence: we simply calculate the expectation and variance of ∆AP and plug them into the normal cumulative density function provided by any statistics software package.</p><p>The expectation and variance of ∆AP are:</p><formula xml:id="formula_15" coords="8,241.62,565.57,298.38,33.62">E[∆AP ] = 1 p i i   C ii p i + j&gt;i C ij p i p j  <label>(2)</label></formula><p>V ar[∆AP ] = 1</p><formula xml:id="formula_16" coords="8,236.09,609.80,303.91,51.76">( p i ) 2 i C 2 ii p i q i + j&gt;i C 2 ij p i p j (1 -p i p j ) + i =j 2C ii C ij p i p j q i + k&gt;j =i 2C ij C ik p i p j p k q i<label>(3)</label></formula><p>Confidence in a difference in average precision is then defined as</p><formula xml:id="formula_17" coords="8,199.56,694.00,336.20,23.70">confidence = P (∆AP &lt; 0) = Φ -E[∆AP ] V ar[∆AP ] (<label>4</label></formula><formula xml:id="formula_18" coords="8,535.76,700.74,4.24,8.74">)</formula><p>where Φ is the normal cumulative density function. This can be very easily extended to determining our confidence in a difference in M AP . The expectation and variance of ∆M AP are:</p><formula xml:id="formula_19" coords="9,250.28,116.45,126.82,26.80">E∆M AP = 1 |T | t∈T E[∆AP t ]</formula><p>(5)</p><formula xml:id="formula_20" coords="9,234.90,147.02,305.10,26.80">V∆M AP = 1 |T | 2 t∈T V ar[∆AP t ]<label>(6)</label></formula><p>and</p><formula xml:id="formula_21" coords="9,198.53,203.73,341.47,23.61">confidence = P (∆M AP &lt; 0) = Φ -E∆M AP √ V∆M AP<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Estimating Relevance</head><p>The formulas above require probabilities of relevance for unjudged documents. We used the "expert aggregation" model described in <ref type="bibr" coords="9,194.70,272.14,9.96,8.74" target="#b8">[9]</ref>. We will not present details here, but the goal is to estimate the relevance of unjudged documents based on the performance of systems over the judged documents. The model takes into account:</p><p>1. the relative frequency of relevant and nonrelevant documents for a topic;</p><p>2. the ability of a system to retrieve relevant documents;</p><p>3. the ability of a system to rank relevant documents highly;</p><p>4. the ability of a system to not retrieve nonrelevant documents;</p><p>5. variance over different systems using similar algorithms to rank.</p><p>Fitting the model is a three-step process: first, ranks are mapped to decreasing probabilities based on the number of judged relevant and judged nonrelevant documents identified for each topic. Second, these probabilities are calibrated to each system's ability to retrieve relevant documents at each rank. Finally, the systems' calibrated probabilities and the available judgments are used to train a logistic regression classifier for relevance. The model predicts probabilities of relevance for all unjudged documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEU Evaluation Method</head><p>In this section, we describe the statistical sampling evaluation methodology, statAP, developed at Northeastern University and employed in the Million Query track. We begin with a simple example in order to provide intuition for the sampling strategy ultimately employed, and we then proceed to describe the specific application of this intuition to the general problem of retrieval evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sampling Theory and Intuition</head><p>As a simple example, suppose that we are given a ranked list of documents (d 1 , d 2 , . . .), and we are interested in determining the precision-at-cutoff 1000, i.e., the fraction of the top 1000 documents that are relevant. Let PC (1000) denote this value. One obvious solution is to examine each of the top 1000 documents and return the number of relevant documents seen divided by 1000. Such a solution requires 1000 relevance judgments and returns the exact value of PC (1000) with perfect certainty. This is analogous to forecasting an election by polling each and every registered voter and asking how they intend to vote: In principle, one would determine, with certainty, the exact fraction of voters who would vote for a given candidate on that day. In practice, the cost associated with such "complete surveys" is prohibitively expensive. In election forecasting, market analysis, quality control, and a host of other problem domains, random sampling techniques are used instead <ref type="bibr" coords="9,106.37,707.93,14.61,8.74" target="#b14">[15]</ref>.</p><p>In random sampling, one trades-off exactitude and certainty for efficiency. Returning to our PC (1000) example, we could instead estimate PC (1000) with some confidence by sampling in the obvious manner: Draw m documents uniformly at random from among the top 1000, judge those documents, and return the number of relevant documents seen divided by m -this is analogous to a random poll of registered voters in election forecasting. In statistical parlance, we have a sample space of documents indexed by k ∈ {1, . . . , 1000}, we have a sampling distribution over those documents p k = 1/1000 for all 1 ≤ k ≤ 1000, and we have a random variable X corresponding to the relevance of documents,</p><formula xml:id="formula_22" coords="10,214.95,167.25,175.93,21.61">x k = rel (k) = 0 if d k is non-relevant 1 if d k is relevant.</formula><p>One can easily verify that the expected value of a single random draw is PC (1000)</p><formula xml:id="formula_23" coords="10,196.00,220.41,220.00,30.55">E[X] = 1000 k=1 p k • x k = 1 1000 1000 k=1 rel (k) = PC (1000),</formula><p>and the Law of Large Numbers and the Central Limit Theorem dictate that the average of a set S of m such random draws</p><formula xml:id="formula_24" coords="10,222.58,280.15,166.84,26.88">PC (1000) = 1 m k∈S X k = 1 m k∈S rel (k)</formula><p>will converge to its expectation, PC (1000), quickly <ref type="bibr" coords="10,297.47,314.30,15.50,8.74" target="#b12">[13]</ref> -this is the essence of random sampling. Random sampling gives rise to a number of natural questions: (1) How should the random sample be drawn? In sampling with replacement, each item is drawn independently and at random according to the distribution given (uniform in our example), and repetitions may occur; in sampling without replacement, a random subset of the items is drawn, and repetitions will not occur. While the former is much easier to analyze mathematically, the latter is often used in practice since one would not call the same registered voter twice (or ask an assessor to judge the same document twice) in a given survey. (2) How should the sampling distribution be formed? While PC (1000) seems to dictate a uniform sampling distribution, we shall see that non-uniform sampling gives rise to much more efficient and accurate estimates. (3) How can one quantify the accuracy and confidence in a statistical estimate? As more samples are drawn, one expects the accuracy of the estimate to increase, but by how much and with what confidence? In the paragraphs that follow, we address each of these questions, in reverse order.</p><p>While statistical estimates are generally designed to be correct in expectation, they may be high or low in practice (especially for small sample sizes) due to the nature of random sampling. The variability of an estimate is measured by its variance, and by the Central Limit Theorem, one can ascribe 95% confidence intervals to a sampling estimate given its variance. Returning to our PC (1000) example, suppose that (unknown to us) the actual PC (1000) was 0.25; then one can show that the variance in our random variable X is 0.1875 and that the variance in our sampling estimate is 0.1875/m, where m is the sample size. Note that the variance decreases as the sample size increases, as expected. Given this variance, one can derive 95% confidence intervals <ref type="bibr" coords="10,182.10,541.45,14.61,8.74" target="#b12">[13]</ref>, i.e., an error range within which we are 95% confident that our estimate will lie. <ref type="foot" coords="10,84.73,551.83,3.97,6.12" target="#foot_1">2</ref> For example, given a sample of size 50, our 95% confidence interval is +/ -0.12, while for a sample of size 500, our 95% confidence interval is +/ -0.038. This latter result states that with a sample of size 500, our estimate is likely to lie in the range [0.212, 0.288]. In order to increase the accuracy of our estimates, we must decrease the size of the confidence interval. In order to decrease the size of the confidence interval, we must decrease the variance in our estimate, 0.1875/m. This can be accomplished by either (1) decreasing the variance of the underlying random variable X (the 0.1875 factor) or (2) increasing the sample size m. Since increasing m increases our judgment effort, we shall focus on decreasing the variance of our random variable instead.</p><p>While our PC (1000) example seems to inherently dictate a uniform sampling distribution, one can reduce the variance of the underlying random variable X, and hence the sampling estimate, by employing nonuniform sampling. A maxim of sampling theory is that accurate estimates are obtained when one samples with probability proportional to size (PPS) <ref type="bibr" coords="10,265.27,684.91,14.61,8.74" target="#b14">[15]</ref>. Consider our election forecasting analogy: Suppose that our hypothetical candidate is known to have strong support in rural areas, weaker support in the suburbs, and almost no support in major cities. Then to obtain an accurate estimate of the vote total (or fraction of total votes) this candidate is likely to obtain, it makes sense to spend your (sampling) effort "where the votes are." In other words, one should spend the greatest effort in rural areas to get very accurate counts there, somewhat less effort in the suburbs, and little effort in major cites where very few people are likely to vote for the candidate in question. However, one must now compensate for the fact that the sampling distribution is non-uniform -if one were to simply return the fraction of polled voters who intend to vote for our hypothetical candidate when the sample is highly skewed toward the candidate's areas of strength, then one would erroneously conclude that the candidate would win in a landslide. To compensate for non-uniform sampling, one must under-count where one over-samples and over-count where one under-samples.</p><p>Returning to our PC (1000) example, employing a PPS strategy would dictate sampling "where the relevant documents are." Analogous to the election forecasting problem, we do have a prior belief about where the relevant documents are likely to reside -in the context of ranked retrieval, relevant documents are generally more likely to appear toward the top of the list. We can make use of this fact to reduce our sampling estimate's variance, so long as our assumption holds. Consider the non-uniform sampling distribution shown in Figure <ref type="figure" coords="11,199.60,254.49,4.98,8.74" target="#fig_1">1</ref> where</p><formula xml:id="formula_25" coords="11,226.44,275.75,152.95,20.69">p k = 1.5/1000 1 ≤ k ≤ 500 0.5/1000 501 ≤ k ≤ 1000.</formula><p>Here we have increased our probability of 0 100 200 300 400 500 600 700 800 900 1000 0 0.5 sampling the top half (where more relevant documents are likely to reside) and decreased our probability of sampling the bottom half (where fewer relevant documents are likely to reside).</p><p>In order to obtain the correct estimate, we must now "under-count" where we "oversample" and "over-count" where we "undersample." This is accomplished by modifying our random variable X as follows:</p><formula xml:id="formula_26" coords="11,223.51,448.05,158.81,20.69">x k = rel (k)/1.5 1 ≤ k ≤ 500 rel (k)/0.5 501 ≤ k ≤ 1000.</formula><p>Note that we over/under-count by precisely the factor that we under/over-sample; this ensures that the expectation is correct:</p><formula xml:id="formula_27" coords="11,172.41,514.40,265.99,65.46">E[X] = 1000 k=1 p k • x k = 500 k=1 1.5 1000 • rel (k) 1.5 + 500 k=1 0.5 1000 • rel (k) 0.5 = 1 1000 1000 k=1 rel (k) = PC (1000).</formula><p>For a given sample S of size m, our estimator is then a weighted average</p><formula xml:id="formula_28" coords="11,176.83,609.90,258.34,64.80">PC (1000) = 1 m k∈S X k = 1 m   k∈S : k≤500 rel (k) 1.5 + k∈S : k&gt;500 rel (k) 0.5  </formula><p>where we over/under-count appropriately. Note that our expectation and estimator are correct, independent of whether our assumption about the location of the relevant documents actually holds! However, if our assumption holds, then the variance of our random variable (and sampling estimate) will be reduced (and vice versa). Suppose that all of the relevant documents were located where we over-sample. Our expectation would be correct, and one can show that the variance of our random variable is reduced from 0.1875 to 0.1042 -we have sampled where the relevant documents are and obtained a more accurate count as a result. This reduction in variance yields a reduction in the 95% confidence interval for a sample of size 500 from +/ -0.038 to +/ -0.028, a 26% improvement. Conversely, if the relevant documents were located in the bottom half, the confidence interval would increase.</p><p>One could extend this idea to three (or more) strata, as in Figure <ref type="figure" coords="12,193.82,158.84,3.87,8.74" target="#fig_2">2</ref>. For each document k, let α k be the factor by which it is over/under-sampled with respect to the uniform distribution; for example, in Figure <ref type="figure" coords="12,262.90,194.71,3.87,8.74" target="#fig_1">1</ref>, α k is 1.5 or 0.5 for the appropriate ranges of k, while in Figure <ref type="figure" coords="12,156.99,218.62,3.87,8.74" target="#fig_2">2</ref>, α k is 1.5, 1, or 0.5 for appropriate ranges of k. For a sample S of size m drawn according to the distribution in question, the sampling estimator would be</p><formula xml:id="formula_29" coords="12,320.78,228.81,3.88,6.68">0</formula><formula xml:id="formula_30" coords="12,113.30,273.90,116.05,26.88">PC (1000) = 1 m k∈S rel (k) α k .</formula><p>In summary, one can sample with respect to any distribution, and so long as one over/under-counts appropriately, the estimator will be correct. Furthermore, if the sampling distribution places higher weight on the items of interest (e.g., relevant documents), then the variance of the estimator will be reduced, yielding higher accuracy. Finally, we note that sampling is often performed without replacement <ref type="bibr" coords="12,466.50,347.49,14.61,8.74" target="#b14">[15]</ref>. In this setting, the estimator changes somewhat, though the principles remain the same: sample where you think the relevant documents are in order to reduce variance and increase accuracy. The α k factors are replaced by inclusion probabilities π k , and the estimator must be normalized by the size of the sample space:</p><formula xml:id="formula_31" coords="12,242.38,402.77,127.23,26.88">PC (1000) = 1 1000 k∈S rel (k) π k .</formula><p>Modularity. The evaluation and sampling modules are completely independent: the sampling module produces the sample in a specific format but does not impose or assume a particular evaluation being used; conversely, the evaluation module uses the given sample, with no knowledge of or assumptions about the sampling strategy strategy empolyed (a strong improvement over method presented in <ref type="bibr" coords="12,464.03,475.80,10.30,8.74" target="#b4">[5]</ref>). In fact, the sampling technique used is known to work with many other estimators, while the estimator used is known to work with other sampling strategies <ref type="bibr" coords="12,229.94,499.71,9.96,8.74" target="#b7">[8]</ref>. This flexibility is particularly important if one has reason to believe that a different sampling strategy might work better for a given evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The sample</head><p>The sample is the set of documents selected for judging together with all information required for evaluation: in our case, that means (1) the documents ids, (2) the relevance assessments, and (3) the inclusion probability for each document.</p><p>The inclusion probability π k is simply the probability that the document k would be included in any sample of size m. In without-replacement sampling, π k = p k when m = 1 and π k approaches 1 as the sample size grows. For most common without-replacement sampling approaches, these inclusion probabilities are notoriously difficult to compute, especially for large sample sizes <ref type="bibr" coords="12,356.30,629.51,9.96,8.74" target="#b7">[8]</ref>.</p><p>Additional judged documents, obtained deterministically, can be added to the existing sample with associated inclusion probability of 1. This is a useful feature as often in practice separate judgments are available; it matches perfectly the design of the Million Query Track pooling strategy, where for more than 800 topics a mixed pool of documents was created (half randomly sampled, half deterministically chosen).</p><p>Additionally, deterministic judgments may arise in at least two other natural ways: First when large-scale judging is done by assessors, it might be desirable to deterministically judge a given depth-pool (say the top 50 documents of every list to be evaluated) and then invoke the sampling strategy to judge additional documents. (This strategy was employed in Terabyte 06 Track). Second, if it is determined that additional samples are required (say for a new run with many unjudged documents), one can judge either hand-picked documents and/or sampled documents and combine them with the original sample. Any collisions (where a document is sampled and separately deterministically judged) are handled by setting inclusion probability to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>Given a sample S of judged documents along with inclusion probabilities, we discuss here how to estimate quantities of interest (AP , R-precision, precision-at-cutoff).</p><p>For AP estimates, which we view as mean of a population of precision values, we adapt the generalized ratio estimator for unequal probability designs (very popular on polls, election strategies, market research etc.), as described in <ref type="bibr" coords="13,165.40,217.08,14.61,8.74" target="#b14">[15]</ref>:</p><formula xml:id="formula_32" coords="13,267.14,228.04,75.86,25.38">X = k∈S v k /π k k∈S 1/π k</formula><p>where v k is the value associated with item k (e.g., the relevance of a document, a vote for a candidate, the size of a potential donation, etc.). For us, the "values" we wish to average are the precisions at relevant documents, and the ratio estimator for AP is thus</p><formula xml:id="formula_33" coords="13,237.38,307.03,302.62,40.38">AP = rel(k)=1 P C(rank(k))/π k rel(k)=1 1/π k<label>(8)</label></formula><p>where</p><formula xml:id="formula_34" coords="13,100.07,360.22,111.51,20.33">P C(r) = 1 r rank<label>(k)≤r rel(k)</label></formula><p>π k estimates precision at rank r and k ∈ S iterates through sampled documents only). Note that AP mimics the well known formula AP = sum of precisions at rel docs number of rel docs because the numerator is an unbiased estimator for the sum of precision values at relevant ranks, while the denominator is an unbiased estimator of the number of relevant documents in the collection: R = rel(k)=1 1 π k . Combining the estimates for R and for precision at rank, P C(r), we obtain also an estimate for R-precision:</p><formula xml:id="formula_35" coords="13,217.14,455.11,322.86,28.84">RP = P C( R) = 1 R rank(k)≤ R rel(k) π k .<label>(9)</label></formula><p>Finally, we note that the variance in our estimates can be estimated as well, and from this, one can determine confidence intervals in all estimates produced. Details may be found our companion paper <ref type="bibr" coords="13,516.12,508.36,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sampling strategy</head><p>There are many ways one can Figure <ref type="figure" coords="13,308.57,670.45,3.87,8.74" target="#fig_0">3</ref>: statAP: Sampling and evaluation design. imagine sampling from a given distribution <ref type="bibr" coords="13,114.33,578.55,9.96,8.74" target="#b7">[8]</ref>. Essentially, sampling consists of a sampling distribution over documents (that should be dictated by the ranks of documents in the ranked lists and therefore naturally biased towards relevant documents) and a sampling strategy (sometimes called "selection") that produces inclusion probabilities roughly proportional to the sampling distribution. Following are our proposed choices for both the distribution and the selection algorithms; many others could work just as well. In the Million Query Track, due to unexpected server behavior, both the sampling distribution and the selection strategy were altered, yielding suboptimal chosen samples; nevertheless, we were able to compute the inclusion probability for each selected document and run the evaluation, though at a reduced accuracy and efficiency.</p><p>Sampling distribution (prior). It has been shown that average precision induces a good relevance prior over the ranked documents of a list. The AP -prior has been used with sampling techniques <ref type="bibr" coords="14,510.00,122.98,12.58,8.74" target="#b4">[5]</ref>; in metasearch (data fusion) <ref type="bibr" coords="14,185.24,134.93,9.96,8.74" target="#b3">[4]</ref>; in automatic assessment of query difficulty <ref type="bibr" coords="14,396.02,134.93,9.96,8.74" target="#b1">[2]</ref>; and in on-line application to pooling <ref type="bibr" coords="14,100.88,146.89,12.38,8.74" target="#b2">[3]</ref>. It has also been shown that this prior can be averaged over multiple lists to obtain a global prior over documents <ref type="bibr" coords="14,134.76,158.84,13.81,8.74" target="#b4">[5]</ref>. An accurate description together with motivation and intuition can be found in <ref type="bibr" coords="14,508.36,158.84,9.96,8.74" target="#b4">[5]</ref>.</p><p>For a given ranked list of documents, let Z be the size of the list. Then the prior distribution weight associated with any rank r, 1 ≤ r ≤ Z, is given by</p><formula xml:id="formula_36" coords="14,190.13,199.72,349.87,22.31">W (r) = 1 2Z 1 + 1 r + 1 r + 1 + • • • + 1 Z ≈ 1 2Z log Z r .<label>(10)</label></formula><p>We used for experimentation the above described prior, averaged per document over all run lists; Note that the our sampling strategy works with any prior over documents.</p><p>Stratified sampling strategy. The most important considerations are: handle non-uniform sampling distribution; without replacement so we can easily add other judged documents; probabilities proportional with size (pps) minimizes variance by obtaining inclusion probabilities π k roughly proportional with precision values P C rank(d) ; and computability of inclusion probabilities for documents (π k ) and for pairs of documents (π kl ). We adopt a method developed by Stevens <ref type="bibr" coords="14,281.71,301.90,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="14,294.96,301.90,11.62,8.74" target="#b13">14]</ref>, sometimes referred to as stratified sampling, that has all of the features enumerated above and it is very straight forward for our application. The details of our proposed sampling strategy can be found in <ref type="bibr" coords="14,268.88,325.81,9.96,8.74" target="#b0">[1]</ref>. Figure <ref type="figure" coords="14,319.40,325.81,4.98,8.74" target="#fig_0">3</ref> provides an overall view of the statAP sampling and evaluation methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Alternation of methods</head><p>Half of the queries were served by alternating between the UMass method MTC and the NEU method statMAP. The alternation was kept on separate "tracks", so that a judgment on a document served by statMAP would not affect the document weights for MTC. If, say, statMAP selected a document that MTC had already served (and therefore that had already been judged), the judgment was recorded for statMAP without showing the document to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Statistics</head><p>The following statistics reflect the status of judgments as of <ref type="bibr" coords="14,346.22,493.86,79.57,8.74">October 16, 2007.</ref> Those are not the same judgments that were used by the track participants for their notebook papers, though the differences are small.</p><p>1,755 queries were judged A total of 22 of those queries were judged by more than one person. 10 were judged by two people 5 were judged by 3 people 4 were judged by 4 people 3 were judged by 5 or more people</p><p>The actual assignment of topics to judging method was done in advance based on topic number. The following was the distribution of topics to methods: 443 of those used the MTC (UMass-only) method 471 used the statMAP (NEU-only) method 432 alternated, starting with MTC 409 alternated, starting with statMAP Since assessors were shown a set of queries and could choose from them, we wondered whether there was an order effect. That is, did people tend to select the first query or not. Here is the number of times someone selected a query for judging based on where in the list of 10 it was. The remaining judgments came from different sites, some (though not all) of which were participants. The number of judged queries ranked from 1 to 37 per site (other than those listed above).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>The 24 runs were evaluated over the TB set using trec eval and over the 1MQ set using EM AP and statMAP. If TB is representative, we should see that EM AP and statMAP agree with each other as well as TB about the relative ordering of systems. Our expectation is that statMAP will present better estimates of MAP while EM AP is more likely to present a correct ranking of systems.</p><p>The left side of Table <ref type="table" coords="15,184.77,677.33,4.98,8.74" target="#tab_0">1</ref> shows the MAP for our 24 systems over the 149 Terabyte queries, ranked from lowest to highest. The average number of unjudged documents in the top 100 retrieved is also shown. Since some of these systems did not contribute to the Terabyte judgments, they ranked quite a few unjudged documents.  The right side shows EM AP and statMAP over the queries judged for our experiment, in order of increasing MAP over Terabyte queries. It also shows the number of unjudged documents in the top 100. EM AP and statMAP are evaluated over somewhat different sets of queries; statMAP excludes queries judged by MTC and queries for which no relevant documents were found, while EM AP includes all queries, with those that have no relevant documents having some probability that a relevant document may yet be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UMass Evaluation</head><p>Overall, the rankings by EM AP and statMAP are fairly similar, and both are similar to the "gold standard". Figure <ref type="figure" coords="16,159.41,563.36,4.98,8.74" target="#fig_3">4</ref> shows a graphical representation of the two rankings compared to the ranking by Terabyte systems. Figure <ref type="figure" coords="16,188.89,575.31,4.98,8.74" target="#fig_4">5</ref> shows how statMAP, EM AP , and MAP over TB queries correlate. All three methods have identified the same three clusters of systems, separated in Table <ref type="table" coords="16,417.73,587.27,4.98,8.74" target="#tab_0">1</ref> by horizontal lines; within those clusters there is some variation in the rankings between methods. For statMAP estimates (Figure <ref type="figure" coords="16,532.25,599.22,3.87,8.74" target="#fig_4">5</ref>, left plot), besides the ranking correlation, we note the accuracy in terms of absolute difference with the TB MAP values by the line corresponding to the main diagonal. Some of the bigger differences between the methods are noted in Table <ref type="table" coords="16,400.75,635.09,4.98,8.74" target="#tab_0">1</ref> by a * indicating that the run moved four or more ranks from its position in the TB ranking, or a † indicating a difference of four or more ranks between EM AP and statMAP. Both methods presented about the same number of such disagreements, though not on the same systems. The biggest disagreements between EM AP and statMAP were on umelbexp and umelbsim, both of which EM AP ranked five places higher than statMAP. Each method settled on a different "winner": indriDM for the TB queries, JuruSynE for EM AP , and LucSpel0 and LucSyn0 tied by statMAP. However, these systems are all quite close in performance by all three methods.</p><p>An obvious concern about the gold standard is the correlation between the number of unjudged documents and MAP: the tau correlation is -.517, or -.608 when exegyexact (which often retrieved only one document) is excluded. This correlation persists for the number unjudged in the top 10. To ensure that we were not inadvertently ranking systems by the number of judged documents, we selected some of the top-retrieved documents in sparsely-judged systems for additional judgments. A total of 533 additional judgments only discovered 7 new relevant documents for the UAms systems, 4 new relevant documents for the ffind systems, but 58 for umelbexp. The new relevant judgments caused umelbexp to move up one rank. This suggests that while the ranking is fair for most systems, it is likely underestimating umelbexp's performance.</p><p>It is interesting that the three evaluations disagree as much as they do in light of work such as Zobel's <ref type="bibr" coords="17,521.73,170.80,14.61,8.74" target="#b15">[16]</ref>. There are at least three possible reasons for the disagreement: (1) the gold standard queries represent a different sample space than the rest; (2) the gold standard queries are incompletely judged; and (3) the assessors did not pick queries truly randomly. The fact that EM AP and statMAP agree with each other more than either agrees with the gold standard suggests to us that the gold standard is most useful as a loose guide to the relative differences between systems, but does not meaningfully reflect "truth" over the larger query sample. But the possibility of biased sampling affects the validity of the other two sets as well: as described above, assessors were allowed to choose from 10 different queries, and it is possible they chose queries that they could decide on clear intents for rather than queries that were unclear. It is difficult to determine how random query selection was. We might hypothesize that, due to order effects, if selection was entirely random we would expect to see the top most query selected most, followed by the second-ranked query, followed by the third, and so on, roughly conforming to a log-normal distribution. This in fact is not what happened; as the click rates in Section 6 show, assessors chose the top-ranked query slightly more often than the others (13.9% of all clicks), but the rest were roughly equal (around 10%). But this would only disprove random selection if we could guarantee that presentation bias holds in this situation. Nevertheless, it does lend weight to the idea that query selection was not random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Additional Analysis</head><p>In this section we present some additional statistics and analysis over the collected data. For more detailed analysis, in particular on the stability of rankings, tradeoffs between the numbers of queries and judgments, and reusability, we refer the reader to our companion work <ref type="bibr" coords="17,332.10,428.80,14.61,8.74" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Assessments</head><p>Assessors made a total of 33,077 judgments for the 801 alternating queries. Of these, 15,028 (45%) were chosen by both methods. 12,489 (38%) were chosen only by MTC, and 5,560 (17%) were chosen only by statMAP.</p><p>Forty-two of the 149 Terabyte topics ended up being selected by 1MQ assessors to be rejudged. For these 42 queries, there were 2,011 documents judged for both the 2007 1MQ track and the 2005 Terabyte track. Agreement on the relevance of these documents was 75%.</p><p>Looking at the difference in system rankings produced by the NIST assessors only versus those produced by the non-NIST assessors may provide a sort of "upper bound" Kendall's τ correlation, the best that can be expected given disagreement between assessors. Though τ = 0.9 is the usual standard, our observed correlation is 0.802. Nearly all of this is due to swaps in the top-ranked systems, which are very similar in quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Agreement on Statistical Significance</head><p>We evaluated statistical significance over the TB queries by a one-sided paired t-test at α = 0.05. A run denoted by a ‡ in Table <ref type="table" coords="17,179.43,652.87,4.98,8.74" target="#tab_0">1</ref> has a MAP significantly less than the next run in the ranking. (Considering the number of unjudged documents, some of these results should be taken with a grain of salt.) Significance is not transitive, so a significant difference between two adjacent runs does not always imply a significant difference between other runs. Both EM AP and statMAP swapped some significant pairs, though they agreed with each other for nearly all such swaps. Overall, MTC agreed with 92.4% of the significant differences between systems as measured over the 149 Terabyte topics. NEU agreed with 93.7% of the significant This difference is not significant by a one-sample proportion test (p = 0.54).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Confidence</head><p>As we described in Section 4, MTC is more interested in differences between pairs than in the value of EM AP . For nearly all the pairs the confidence was 1, meaning that we predict that additional judgments will not change the relative ordering of pairs. Table <ref type="table" coords="18,303.18,401.47,4.98,8.74" target="#tab_1">2</ref> shows the confidence in the difference in EM AP for selected pairs that had less than 100% confidence. Note that many of the high-ranked systems (the indri set and the Luc set) were difficult to differentiate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">ANOVA and Generalizability Theory</head><p>As extensively discussed in previous sections, 24 different runs were submitted to the Million Query track, where each run output a ranked list of documents for each one of 10, 000 queries. The ranked lists produced by all systems for a subset of 1, 755 queries were judged and their quality was assessed employing two different methodologies, MTC and NEU. Each of the two methodologies evaluated the quality of the ranked lists for the 1, 755 queries by the means of some estimate of average precision (AP) and the overall quality of each system by some estimate of mean average precision (MAP), resulting into two test collections.</p><p>There are two questions that naturally arise: (1) How reliable are the given performance comparisons, and (2) how good are the test collections? We answer these two questions by employing Generalizability Theory (GT) <ref type="bibr" coords="18,132.71,567.30,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="18,146.54,567.30,7.01,8.74" target="#b6">7]</ref>.</p><p>In particular, GT provides the appropriate tools to answers the question: To what extent does the variance of the observed average precision (AP) values reflect real performance difference between the systems as opposed to other sources of variance? During the first step of GT (the G-study), the AP value for a ranked list of documents produced by a single system ran over a single query can be decomposed into a number of uncorrelated effects (sources of variance), AP aq = µ + v a + v q + v aq,err where µ is the grand mean over all AP values, v a is the system effect, v q is the query effect and v aq,err is the system-query interaction effect along with any other effect not being considered. Apart from the grand mean that is a constant, each of the other effects is modeled as a random variable and therefore it has mean and variance. In the same manner as the AP value decomposition, the variance of the observed AP value is</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,72.00,75.43,158.82,9.32;7,230.82,73.88,4.08,6.12;7,235.40,75.46,15.08,8.74;7,72.00,89.22,221.84,8.77;7,293.84,87.68,4.08,6.12;7,298.42,89.25,100.60,8.74;7,72.00,101.17,198.67,8.77;7,270.83,99.63,6.00,6.12;7,277.37,101.17,13.09,8.77;7,290.62,99.63,6.31,6.12;7,77.37,113.13,46.44,8.77;7,123.81,118.19,18.22,6.12;7,77.37,125.09,67.88,8.77;7,77.37,138.43,6.59,6.99"><head>Algorithm 3 .</head><label>3</label><figDesc>4 update-weights(i * , S) Require: the index of the most recent judgment i * , a set of ranked lists S Require: access to global weight matrices V R , V N 1: C = [0] n×m 2: for s ∈ S do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,312.20,411.07,198.20,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Non-uniform sampling distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,304.15,257.37,214.31,8.74"><head>)Figure 2 :</head><label>2</label><figDesc>Figure 2: Non-uniform distrib. with three strata.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,98.93,280.61,414.15,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MTC and statMAP evaluation results sorted by evaluation over 149 Terabyte topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="16,72.00,447.84,468.00,8.74;16,72.00,459.80,291.70,8.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: From left, evaluation over Terabyte queries versus statMAP, evaluation over Terabyte queries versus EM AP , and statMAP evaluation versus EM AP evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="15,72.00,73.96,468.00,479.63"><head>Table 1 :</head><label>1</label><figDesc>Performance on 149 Terabyte topics, 1692 partially-judged topics per EM AP , and 1084 partiallyjudged queries per statMAP, along with the number of unjudged documents in the top 100 for both sets.</figDesc><table coords="15,72.00,73.96,468.00,479.63"><row><cell></cell><cell></cell><cell></cell><cell cols="2">149 Terabyte</cell><cell></cell><cell></cell><cell>1MQ</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">run name</cell><cell cols="2">unjudg MAP</cell><cell cols="3">unjudg EM AP</cell><cell cols="2">statMAP</cell><cell></cell></row><row><cell></cell><cell cols="2">UAms.AnLM</cell><cell cols="2">64.72 0.0278  ‡</cell><cell cols="3">90.75 0.0281</cell><cell>0.0650</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">UAms.TiLM</cell><cell cols="2">61.43 0.0392  ‡</cell><cell cols="3">89.40 0.0205</cell><cell>0.0938</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">exegyexact</cell><cell cols="2">8.81 0.0752  ‡</cell><cell cols="3">13.67 0.0184</cell><cell>0.0517</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">umelbexp</cell><cell cols="2">61.17 0.1251</cell><cell cols="4">91.85 0.0567  *  † 0.1436  †</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ffind07c</cell><cell></cell><cell cols="2">22.91 0.1272  ‡</cell><cell cols="3">77.94 0.0440</cell><cell>0.1531</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ffind07d</cell><cell></cell><cell cols="2">24.07 0.1360</cell><cell cols="3">82.11 0.0458</cell><cell>0.1612</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">sabmq07a1</cell><cell cols="2">21.69 0.1376</cell><cell cols="3">86.51 0.0494</cell><cell>0.1519</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">UAms.Sum6</cell><cell cols="2">32.74 0.1398  ‡</cell><cell cols="3">81.37 0.0555</cell><cell>0.1816</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">UAms.Sum8</cell><cell cols="2">24.40 0.1621</cell><cell cols="3">79.92 0.0580</cell><cell>0.1995</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">UAms.TeVS</cell><cell cols="2">21.11 0.1654</cell><cell cols="3">81.35 0.0503</cell><cell>0.1805</cell><cell></cell><cell></cell></row><row><cell></cell><cell>hedge0</cell><cell></cell><cell cols="2">16.90 0.1708  ‡</cell><cell cols="3">80.44 0.0647</cell><cell>0.2175</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">umelbimp</cell><cell cols="2">15.40 0.2499</cell><cell cols="3">80.83 0.0870</cell><cell>0.2568</cell><cell></cell><cell></cell></row><row><cell></cell><cell>umelbstd</cell><cell></cell><cell cols="2">11.48 0.2532  ‡</cell><cell cols="3">82.17 0.0877</cell><cell>0.2583</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">umelbsim</cell><cell cols="2">10.38 0.2641  ‡</cell><cell cols="4">80.17 0.1008  *  † 0.2891  †</cell><cell></cell><cell></cell></row><row><cell></cell><cell>hitir</cell><cell></cell><cell cols="2">9.06 0.2873</cell><cell cols="3">80.25 0.0888</cell><cell>0.2768</cell><cell></cell><cell></cell></row><row><cell></cell><cell>rmitbase</cell><cell></cell><cell cols="2">8.32 0.2936</cell><cell cols="3">79.28 0.0945</cell><cell>0.2950</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">indriQLSC</cell><cell cols="2">7.34 0.2939</cell><cell cols="3">79.18 0.0969</cell><cell>0.3040</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">LucSynEx</cell><cell cols="2">13.02 0.2939</cell><cell cols="3">78.23 0.1032  *</cell><cell>0.3184  *</cell><cell></cell><cell></cell></row><row><cell></cell><cell>LucSpel0</cell><cell></cell><cell cols="2">13.08 0.2940</cell><cell cols="3">78.27 0.1031</cell><cell>0.3194  *</cell><cell></cell><cell></cell></row><row><cell></cell><cell>LucSyn0</cell><cell></cell><cell cols="2">13.08 0.2940</cell><cell cols="3">78.27 0.1031</cell><cell>0.3194  *</cell><cell></cell><cell></cell></row><row><cell></cell><cell>indriQL</cell><cell></cell><cell cols="2">7.12 0.2960  ‡</cell><cell cols="3">78.80 0.0979  *</cell><cell>0.3086</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">JuruSynE</cell><cell cols="2">8.86 0.3135</cell><cell cols="3">78.36 0.1080</cell><cell>0.3117</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">indriDMCSC</cell><cell cols="2">9.79 0.3197</cell><cell cols="3">80.36 0.0962  *</cell><cell>0.2981  *</cell><cell></cell><cell></cell></row><row><cell></cell><cell>indriDM</cell><cell></cell><cell cols="2">8.67 0.3238</cell><cell cols="3">79.51 0.0981  *</cell><cell>0.3060  *</cell><cell></cell><cell></cell></row><row><cell>Rank</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>Count</cell><cell>213</cell><cell>157</cell><cell>144</cell><cell>148</cell><cell>169</cell><cell>141</cell><cell>118</cell><cell>145</cell><cell>156</cell><cell>139</cell></row><row><cell cols="11">Percent 13.9% 10.3% 9.4% 9.7% 11.0% 9.2% 7.7% 9.5% 10.2% 9.1%</cell></row><row><cell cols="11">(The numbers add up to 1530 rather than 1755 because this logging was included partway through the</cell></row><row><cell>judging process.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Judgments came from the following sources:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">1,478 NIST assessors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">97 CIIR hired annotators</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">47 IR class project</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="18,105.07,73.96,401.85,210.33"><head>Table 2 :</head><label>2</label><figDesc>Probability that a difference in MAP is less than zero for selected pairs of systems.</figDesc><table coords="18,205.97,73.96,200.06,188.46"><row><cell>pair</cell><cell>confidence</cell></row><row><cell>exegyexact &amp; UAmsT07MAnLM</cell><cell>0.9577</cell></row><row><cell>sabmq07a1 &amp; UAmsT07MTeVS</cell><cell>0.7113</cell></row><row><cell>UAmsT07MSum6 &amp; umelbexp</cell><cell>0.6639</cell></row><row><cell>umelbexp &amp; UAmsT07MSum8</cell><cell>0.6920</cell></row><row><cell>umelbimp &amp; umelbstd</cell><cell>0.6095</cell></row><row><cell>umelbimp &amp; hitir2007mq</cell><cell>0.7810</cell></row><row><cell>umelbstd &amp; hitir2007mq</cell><cell>0.6909</cell></row><row><cell>rmitbase &amp; indriDMCSC</cell><cell>0.8412</cell></row><row><cell>indriDMCSC &amp; indriQLSC</cell><cell>0.6552</cell></row><row><cell>indriDMCSC &amp; indriQL</cell><cell>0.8480</cell></row><row><cell>indriQLSC &amp; indriDM</cell><cell>0.7748</cell></row><row><cell>indriQL &amp; indriDM</cell><cell>0.5551</cell></row><row><cell>LucSyn0 &amp; LucSpel0</cell><cell>0.5842</cell></row><row><cell>LucSyn0 &amp; LucSynEx</cell><cell>0.6951</cell></row><row><cell>LucSpel0 &amp; LucSynEx</cell><cell>0.6809</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,87.24,708.22,64.70,6.99"><p>http://drupal.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="10,87.24,704.03,452.76,6.99;10,72.00,714.56,359.11,6.99"><p>For estimates obtained by averaging a random sample, the 95% confidence interval is roughly +/-1.965 standard deviations, where the standard deviation is the square root of the variance, i.e., 0.1875/m in our example.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> under contract number <rs type="grantNumber">HR0011-06-C-0023</rs>, in part by <rs type="funder">NSF</rs> grants <rs type="grantNumber">IIS-0534482</rs> and <rs type="grantNumber">IIS-0533625</rs>, and in part by <rs type="funder">Microsoft Live Labs</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XX8Tp4k">
					<idno type="grant-number">HR0011-06-C-0023</idno>
				</org>
				<org type="funding" xml:id="_zk4dS58">
					<idno type="grant-number">IIS-0534482</idno>
				</org>
				<org type="funding" xml:id="_Yzf4T3a">
					<idno type="grant-number">IIS-0533625</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>decomposed into the corresponding variance components,</p><p>Table <ref type="table" coords="19,115.81,281.20,4.98,8.74">3</ref> and Table <ref type="table" coords="19,175.26,281.20,4.98,8.74">4</ref> provide estimates of those variance components when the MTC and the NEU methodology is employed, respectively. The figures in Table <ref type="table" coords="19,340.94,293.15,4.98,8.74">3</ref> are based on 429 queries selected by MTC, while the figures in Table <ref type="table" coords="19,184.64,305.11,4.98,8.74">4</ref> are based on 459 selected by NEU. Note that each variance component reported in the two tables along with the corresponding percentage is calculated on a per query basis. Therefore, 65.42% (or 78.64%) would be the percentage of the total variance that corresponds to the system-query interaction if a system runs on a single query when the MTC (or NEU) methodology is employed.</p><p>While the G-study copes with the decomposition of variance of a single AP value into variance components due to a single system and a single query, the next step of GT (the D-study) considers the decomposition of the variance of the average of the AP values over all queries (MAP) into variance components due to a single system and a set of N q queries. The variance components in the D-study can be easily computed by using the variance components computed during G-study as follows, σ 2 (Q) = σ 2 (q)/N q , σ 2 (aQ) = σ 2 (aq)/N q while the variance due to the system effect remains the same.</p><p>Table <ref type="table" coords="19,115.98,449.46,4.98,8.74">5</ref> and Table <ref type="table" coords="19,175.93,449.46,4.98,8.74">6</ref> provide the percentage of the variance of the MAP values that is due to the system effect, i.e. σ 2 (a)/(σ 2 (a) + σ 2 (q)/N q + σ 2 (aq)/N q ) for different number of queries (N q ) for the two methodologies. As can be observed, for both MTC and NEU methodologies, the variance in the MAP scores, in a test design of 450 queries (i.e. approximately the design used in Million Query track) reflect the real performance difference between the systems, since the percentage of the total MAP variance that is due to the system effect is 98% for both methodologies. Therefore, any disagreement in the overall ranking of the systems by the two methodologies are due to the different estimators used by the two methodologies for computing AP and MAP values.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="19,92.48,689.29,447.52,8.74;19,92.48,701.24,447.52,8.74;19,92.48,713.20,45.11,8.74;20,172.29,73.96,106.05,9.65;20,330.78,73.96,9.96,8.74;20,361.01,73.96,14.94,8.74;20,391.22,73.96,14.94,8.74;20,421.44,73.96,14.94,8.74;20,172.29,86.32,267.42,8.74;20,136.57,108.73,338.87,8.74;20,172.29,131.39,106.05,9.65;20,330.78,131.39,9.96,8.74;20,361.01,131.39,14.94,8.74;20,391.22,131.39,14.94,8.74;20,421.44,131.39,14.94,8.74;20,172.29,143.75,267.42,8.74;20,137.47,166.16,337.07,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="19,219.55,689.29,272.42,8.74;20,172.29,73.96,106.05,9.65;20,330.78,73.96,9.96,8.74;20,361.01,73.96,14.94,8.74;20,391.22,73.96,14.94,8.74;20,421.44,73.96,14.94,8.74;20,172.29,86.32,267.42,8.74;20,136.57,108.73,334.05,8.74">Number of Queries (N q ) 50 100 200 450 % of total variance due to system 85% 92% 95% 98% Table 5: % of total variance due to system employing the MTC methodology</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<ptr target="http://www.ccs.neu.edu/home/jaa/papers/drafts/statAP.html" />
	</analytic>
	<monogr>
		<title level="m" coord="19,503.17,689.29,36.83,8.74;19,92.48,701.24,161.21,8.74">Working draft available at the following URL</title>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
	<note>A practical sampling strategy for efficient retrieval evaluation. Number of Queries (N q ) 50 100 200 450 % of total variance due to system 86% 93% 96% 98% Table 6: % of total variance due to system employing the NEU methodology</note>
</biblStruct>

<biblStruct coords="20,92.48,197.99,447.52,8.74;20,92.48,209.94,279.95,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="20,208.62,197.99,331.38,8.74;20,92.48,209.94,72.00,8.74">Query hardness estimation using Jensen-Shannon divergence among multiple scoring functions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,185.59,209.94,87.14,8.74">Proceedings of ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,229.87,447.52,8.74;20,92.48,241.82,201.86,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="20,259.75,229.87,275.95,8.74">A unified model for metasearch, pooling, and system evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Savell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,104.93,241.82,89.20,8.74">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,261.75,447.52,8.74;20,92.48,273.70,63.65,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,274.50,261.75,114.87,8.74">Measure-based metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,414.39,261.75,92.39,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="571" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,293.63,447.52,8.74;20,92.48,305.58,254.63,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="20,273.04,293.63,266.96,8.74;20,92.48,305.58,42.89,8.74">A statistical method for system evaluation using incomplete judgments</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,157.00,305.58,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,325.51,447.52,8.74;20,92.48,337.46,22.69,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="20,182.61,325.51,176.04,8.74">Test theory for assessing ir test collection</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bodoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,378.04,325.51,89.77,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,357.39,324.16,8.74" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Brennan</surname></persName>
		</author>
		<title level="m" coord="20,162.37,357.39,99.42,8.74">Generalizability Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,377.31,424.39,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="20,235.60,377.31,158.22,8.74">Sampling With Unequal Probabilities</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R W</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hanif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,397.24,447.53,8.74;20,92.48,409.19,22.69,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="20,159.47,397.24,201.73,8.74">Robust test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,384.15,397.24,91.61,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,429.12,447.52,8.74;20,92.48,441.07,190.10,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="20,308.30,429.12,210.38,8.74">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,92.48,441.07,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,461.00,447.52,8.74;20,92.48,472.96,134.42,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,377.71,461.00,158.17,8.74">Evaluation over thousands of queries</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,104.93,472.96,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,492.88,447.52,8.74;20,92.48,504.84,258.01,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,297.47,492.88,138.88,8.74">The TREC 2005 terabyte track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,460.60,492.88,79.40,8.74;20,92.48,504.84,227.47,8.74">Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text REtrieval Conference (TREC 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,524.76,405.20,8.74" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="20,143.39,524.76,183.69,8.74">Mathematical Statistics and Data Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Rice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Duxbury Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct coords="20,92.48,544.69,447.52,8.74;20,92.48,556.64,386.66,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="20,165.54,544.69,300.63,8.74">Sampling without replacement with probability proportional to size</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">L</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,476.96,544.69,63.04,8.74;20,92.48,556.64,221.37,8.74">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="397" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,92.48,576.57,302.24,8.74" xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sampling</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wiley-Interscience</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct coords="20,92.48,596.49,447.52,8.74;20,92.48,608.45,90.83,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="20,137.42,596.49,283.78,8.74">How reliable are the results of large-scale retrieval experiments?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,442.90,596.49,91.93,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
