<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.47,137.95,317.07,15.12">Overview of the TREC 2007 Enterprise Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.17,170.43,63.08,10.48"><forename type="first">Peter</forename><surname>Bailey</surname></persName>
							<email>pbailey@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,359.33,218.13,62.75,10.48"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Microsoft</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Nick Craswell MSR</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Arjen P. de Vries CWI</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">NIST</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.47,137.95,317.07,15.12">Overview of the TREC 2007 Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD56E94351C2B7A3FC0795DE45571B76</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the enterprise track is to conduct experiments with enterprise data that reflect the experiences of users in real organizations. This year, the track has introduced a new corpus with the goal to be more representative of real-world enterprise search, by involving actual members of the organization in the topic development process, performing their real work tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection</head><p>The CERC corpus (CSIRO Enterprise Research Collection, (http://es.csiro.au/cerc/)) represents the public-facing web of the Australian Commonwealth Scientific and Industrial Research Oganization (CSIRO). Here, we summarize the main characteristics of this corpus; a complete description of the collection is given in <ref type="bibr" coords="1,279.44,435.83,82.14,8.74" target="#b0">Bailey et al. (2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The collection consists of all the *.csiro.au (public) websites as they appeared in March 2007. The resulting data set consists of 370 715 documents, with total size 4.2 gigabytes. The web crawler visited the outward-facing pages of CSIRO in a fashion similar to the crawl used in CSIRO's own search engine. In fact, the same crawler technology that CSIRO uses was used to gather the CSIRO documents (http://www.funnelback.com/). The corpus contains approximately 7.9 million hyperlinks, and 95% of pages have one or more outgoing links containing anchor text. One participant extracted email addresses of 3678 individuals, with 38% of documents containing at least one mailto field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Users</head><p>A science communicator's role in CSIRO is to enhance CSIRO's public image and promote the capabilities of CSIRO by managing information and interacting with industry groups, government agencies, professional groups, media and the general public. Science Communicators read and create the outward-facing web pages of CSIRO (as opposed to internal documents). Therefore they were a natural choice when thinking of which users are a good match for our outward-facing crawl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tasks and Topics</head><p>The 2007 enterprise track defined two tasks: document search and expert search. Both search tasks are grounded in a 'missing overview page' scenario, where the science communicator has to construct a new overview page on the topic of interest, that enumerates the 'key pages' and a few 'key people' of interest. Given this scenario, the document search task models the problem of finding the set S of 'key pages', and the expert search task the problem of locating the 'key contacts' among CSIRO staff.</p><p>The primary method for involving Science Communicators was asking them to do topic development. A general email was sent to all science communicators, calling for them to create topics in their area. Examples of general queries from CSIRO's real public search site were given for inspiration. This yielded 25 usable topics from 9 science communicators from multiple CSIRO divisions. Being short of the standard 50 topics, we then approached one of these communicators who produced another 25 topics to complete the set.</p><p>Each topic description has a query and narrative, some examples of key reference URLs (on average 4 per topic) and a short list of key contacts (on average 3 per topic, varying from 1 to 11). The key reference URLs serve as a (admittedly somewhat poor) surrogate for click-log data. Note that both tasks have used the same set of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Assessments</head><p>For document search we used community judging. NIST formed pools and sent them to CSIRO, where the assessment system was hosted. Track participants then judged the pools through the CSIRO system (adapted from the assessment system used in the Million Query track).</p><p>The guidelines instructed the assessors to read the query and narrative, and optionally carry out a Web search to learn more about the subject. The guidelines also emphasized that science communicators are web-savvy users -so judgments should take into account that navigational answers and relevant homepages are important results in exploratory search behaviour. Relevance judgments were made on a three-point scale: 2: Highly likely to be a 'key page'. 1: Possible as a candidate for a page in S, or otherwise informative to help build an overview page, but not highly likely. 0: Not a 'key page' as unlikely to be included in S, because, e.g., not relevant, off-topic, not an important page on the topic, on-topic but out-of-date, not the right kind of navigation point, or too informal or too narrow an audience.</p><p>After the workshop, we investigated to what extent the people making relevance judgements for the document search task have been exchangeable, comparing assessments made by participants ('bronze' judges) to sampled re-assessments for 33 topics by the topic authors ('gold' judges) and/or other science communicators familiar with the task ('silver' judges). The main finding from the study is that the bronze judges may not be able to substitute for topic and task experts, due to changes in the relative performance of assessed systems, and gold judges are preferred. The full details of this post-TREC study can be found in <ref type="bibr" coords="2,423.81,599.49,82.14,8.74" target="#b1">Bailey et al. (2008)</ref>.</p><p>For expert search, we did no further judging, using the experts listed in the topic as our ground truth. 3 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document search</head><p>Systems return docids for document search. Participants submitted 43 automatic, 15 feedback and 5 manual runs. The pools for document search included the top 75 documents from two runs per participant.</p><p>Runs were evaluated on their capability to retrieve the key pages, using traditional retrieval measures including MAP and precision at fixed ranks; NDCG is reported to take into account the graded assessments.</p><p>Automatic runs may use the query and narrative fields of the topic, but each participating group had to submit at least one run using the query field only. Table <ref type="table" coords="3,406.32,479.96,4.98,8.74" target="#tab_0">1</ref> shows the best automatic run from each participating group based on mean average precision. Ordering on descending NDCG instead of MAP gives slightly different results; e.g., University of Waterloo's uwKLD run (using query expansion from pseudo-relevant documents) would come second and beat their best MAP-based uwtbase run, and the Open University's ouNarrAuto run (using the narrative for automatic query expansion) would give better results than the ouTopicOnly baseline. These observed differences seem to suggest that query expansion from documents or the topic narrative is more useful when trying to find the highly relevant documents than when just finding any type of relevant document.</p><p>Feedback runs can be thought of as simulating one type of click-based system. Using click logs, it is often possible to identify that we have seen this query before, and that one or two URLs were often clicked. In that case, it would be interesting to take those URLs as relevant and perform relevance feedback. Unfortunately, we do not have CSIRO click logs, but we can use the pages field of the topic, to simulate what would happen in such a case. Feedback runs should use the query and pages fields only (not the narrative field and no manual intervention).</p><p>There are at least two methods for evaluating relevance feedback in a way that allows a comparison between feedback and non-feedback runs. The predominant method in IR is to evaluate on the residual collection, that is, feedback documents are removed from all runs and the relevance judgments. In the web search engine community, another method known as promotion is used -the feedback documents are moved to the top of all rankings, or placed there if they have not been retrieved. Table <ref type="table" coords="4,151.29,389.93,4.98,8.74" target="#tab_1">2</ref> summarizes the results using residual-collection evaluation. For these scores, the key pages from the topics have been removed from both the qrels and the run. This allows feedback and non-feedback runs to be compared directly, but the residual-collection scores in Table <ref type="table" coords="4,135.09,425.80,4.98,8.74" target="#tab_1">2</ref> are not comparable to the scores in Table <ref type="table" coords="4,325.36,425.80,3.88,8.74" target="#tab_0">1</ref>. The overall best run is a feedback run, but the difference from the best automatic run is marginal (less than 1% in MAP). Not all groups submitted feedback runs, and for some groups that did, their feedback runs were worse than their non-feedback runs.</p><p>Table <ref type="table" coords="4,149.41,473.62,4.98,8.74" target="#tab_2">3</ref> reports again results for feedback runs, however this time using promotion evaluation. Here, the key pages are moved to or placed at the top of the ranking. This evaluation is another way to compare feedback and non-feedback runs to each other; by comparing the scores of baseline and feedback runs both with and without promotion, you can see if the feedback is generalizing beyond the feedback documents. The table lists only results for submitted feedback runs (so automatic runs are not included in this ranking). Only for Waterloo, UvA and Glasgow, using feedback information lead to their best results; the other teams submitted non-feedback runs that performed better than their feedback runs.</p><p>Manual runs involve humans in the loop at any stage, for example composing queries from the topics, manual term expansion, relevance feedback, or manual combination of results. Although DUT submitted a highly performing manual run (run DUTDST1, with MAP 0.402 and NDCG 0.725), it did not outperform the two best automatic runs (by CAS and York University), nor did it outperform the best feedback run (by University of Waterloo).</p><p>The remainder of this section reviews some highlights from the participant papers on their document search activities. Several teams experimented with web retrieval methods based on anchor text or determining a static ranking (e.g., by pagerank or URL length), but the results seem to indicate that the CSIRO data behaves differently from Web data and that these methods are less effective than expected. RMIT mentions the fact that most links originate from the noncontent part of the CSIRO pages, i.e., layout structure such as menu bars; SJTU and Tsinghua made independently the same observation and used the percentage of links to seperate layout from content and weight the latter stronger. Tsinghua reports an improvement using Pagerank and HITS, but the improved results are lower than the Lemur language modelling baseline without static weighting reported by RMIT. The participants who used the narrative, e.g. for query expansion, report improved effectiveness over their baseline systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expert search</head><p>Expert finding systems participating in the 2007 enterprise track had to return email addresses to identify candidate experts. Since no canonical list of candidate experts could be made available, the track required participants to extract the email addresses of the 'key people' from the data. Participants submitted 45 automatic, 4 feedback and 6 manual runs.</p><p>The evaluation results, summarized in Table <ref type="table" coords="5,326.84,657.33,3.88,8.74" target="#tab_3">4</ref>, measure the quality of the ranked list of people using traditional retrieval measures including MAP and precision at fixed ranks.</p><p>Tables <ref type="table" coords="5,154.55,681.24,4.98,8.74">6</ref> and<ref type="table" coords="5,182.76,681.24,4.98,8.74" target="#tab_4">5</ref> summarize the results of the feedback and manual runs. For expert search, the best runs are manual runs, but notice how many automatic runs have outperformed the We again highlight some findings from studying the participant papers. Most participants use some form of two-stage model. Several teams (e.g., SJTU, UvA) retrieved homepages of the identified candidate names to aid in the expertise assessment. Proximity between candidate mentions and query terms seems an important factor in SJTU, Glasgow and OU results. Both CAS and Twente experimented with query-specific graphs of expert-document pairs, but results</p><p>are not yet conclusive. What we can however conclude from this year's experiments is that the lack of candidate list has complicated the task significantly when compared to previous years. Almost all participants have used template matching to identify candidates from email occurrences in the corpus, sometimes including sophisticated heuristics to circumvent anti-spam measures and to exclude general group email addresses from consideration. Several participants report however that they had missed about half of the candidates that were found relevant in the assessments (with correspondingly lower effectiveness).</p><p>To validate the outcome of the experiments, we asked one science communicator to look into the highly-ranked non-relevant responses, and classify those as follows: E: Expert, but not key contact K: Knowledgable, but not expert N: Not knowledgable or expert S: Science Communicator U: Unknown status None of these responses has been reconsidered as a 'key contact' missing from the topic definition. For three topics authored by this science communicator, we found that the systems identified five different science communicators (S) as the experts. Two of the ranked experts were deemed knowledgeable staff members but not experts (K), and four clearly not knowledgeable (N). The remaining twenty-eight highly-ranked non-relevant responses had unknown expertise (U).</p><p>We conclude from this minor investigation that the generic methods of expert identification are not taking into account the context of the situated task -science communicators created the topic set, and would not have nominated themselves as the key contact.</p><p>The third year of the enterprise track has introduced the CERC corpus (CSIRO Enterprise Research Collection). The data consists of a crawl of the public-facing web of the Australian Commonwealth Scientific and Industrial Research Oganization (CSIRO). The track involved CSIRO's science communicators in the topic development process, with the goal to model accurately the search activities of real members of the enterprise.</p><p>The newly introduced document search task is motivated by a 'missing overview page' scenario, where a search is conducted to find a set of 'key pages' related to the topic in question; for example, to assist the science communicator to create the missing overview page. The topics provided a small number of example 'key pages' to facilitate experiments with relevance feedback strategies.</p><p>The expert search task follows naturally from the missing page scenario, where the 'key contacts' among CSIRO staff should be identified. As opposed to previous years, the 2007 expert search task did not provide a pre-defined list of candidates, and fewer experts were expected per topic. The expertise judgments originate from the topic authors themselves, and encode inside knowledge. For example, highly-ranked non-relevant candidate experts for some topics turned out to be science communicators and other knowledgeable people that are not seen as experts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,108.00,107.98,414.01,218.84"><head>Table 1 :</head><label>1</label><figDesc>Document search results for the automatic run with the highest MAP from each group.</figDesc><table coords="3,183.00,121.79,264.00,205.02"><row><cell>Group</cell><cell>Run</cell><cell cols="3">MAP NDCG P@20</cell></row><row><cell>CAS</cell><cell>DocRun02</cell><cell>0.422</cell><cell>0.743</cell><cell>0.527</cell></row><row><cell>York</cell><cell>york07ed4</cell><cell>0.416</cell><cell>0.730</cell><cell>0.513</cell></row><row><cell>Waterloo</cell><cell>uwtbase</cell><cell>0.388</cell><cell>0.707</cell><cell>0.508</cell></row><row><cell>RMIT</cell><cell>RmitQ</cell><cell>0.388</cell><cell>0.698</cell><cell>0.471</cell></row><row><cell>SJTU</cell><cell>SJTUEntDS02</cell><cell>0.374</cell><cell>0.692</cell><cell>0.475</cell></row><row><cell>UvA</cell><cell>uams07bfb</cell><cell>0.369</cell><cell>0.675</cell><cell>0.445</cell></row><row><cell>Tsinghua</cell><cell cols="2">THUDSFULLSR 0.366</cell><cell>0.701</cell><cell>0.461</cell></row><row><cell>UALR</cell><cell>UALR07Ent1</cell><cell>0.357</cell><cell>0.662</cell><cell>0.428</cell></row><row><cell>Fudan</cell><cell>FDUBase</cell><cell>0.350</cell><cell>0.664</cell><cell>0.426</cell></row><row><cell>OU</cell><cell>ouTopicOnly</cell><cell>0.345</cell><cell>0.646</cell><cell>0.464</cell></row><row><cell>Glasgow</cell><cell>uogEDSF</cell><cell>0.337</cell><cell>0.675</cell><cell>0.413</cell></row><row><cell>DUT</cell><cell>DUTDST4</cell><cell>0.336</cell><cell>0.644</cell><cell>0.441</cell></row><row><cell>Iowa</cell><cell>uiowa07entD2</cell><cell>0.310</cell><cell>0.597</cell><cell>0.413</cell></row><row><cell>Hyberdad</cell><cell cols="2">QRYBASICRUN 0.246</cell><cell>0.487</cell><cell>0.408</cell></row><row><cell>CSIRO</cell><cell>CSIROdsQonly</cell><cell>0.194</cell><cell>0.352</cell><cell>0.378</cell></row><row><cell cols="2">St. Petersburg insu2</cell><cell>0.028</cell><cell>0.185</cell><cell>0.041</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,108.00,107.98,414.00,230.79"><head>Table 2 :</head><label>2</label><figDesc>Document search results for the automatic or feedback run with the highest MAP from each group, using residual ranking. Feedback runs are labeled with a '*'.</figDesc><table coords="4,181.41,133.75,267.18,205.02"><row><cell>Group</cell><cell>Run</cell><cell cols="3">MAP NDCG P@20</cell></row><row><cell>Waterloo</cell><cell>uwRF*</cell><cell>0.395</cell><cell>0.691</cell><cell>0.479</cell></row><row><cell>York</cell><cell>york07ed4</cell><cell>0.386</cell><cell>0.677</cell><cell>0.472</cell></row><row><cell>UvA</cell><cell>uams07bfbex*</cell><cell>0.359</cell><cell>0.640</cell><cell>0.461</cell></row><row><cell>RMIT</cell><cell>RmitQ</cell><cell>0.357</cell><cell>0.633</cell><cell>0.423</cell></row><row><cell>CAS</cell><cell>DocRun02</cell><cell>0.353</cell><cell>0.666</cell><cell>0.457</cell></row><row><cell>UALR</cell><cell>UALR07Ent2*</cell><cell>0.344</cell><cell>0.623</cell><cell>0.423</cell></row><row><cell>SJTU</cell><cell>SJTUEntDS02</cell><cell>0.337</cell><cell>0.629</cell><cell>0.417</cell></row><row><cell>Fudan</cell><cell>FDUBase</cell><cell>0.320</cell><cell>0.591</cell><cell>0.382</cell></row><row><cell>Tsinghua</cell><cell>THUDSFULLSR</cell><cell>0.310</cell><cell>0.602</cell><cell>0.390</cell></row><row><cell>DUT</cell><cell>DUTDST2</cell><cell>0.298</cell><cell>0.577</cell><cell>0.386</cell></row><row><cell>OU</cell><cell>ouTopicOnly</cell><cell>0.296</cell><cell>0.582</cell><cell>0.401</cell></row><row><cell>Glasgow</cell><cell cols="2">uogEDSCLCDIS* 0.290</cell><cell>0.582</cell><cell>0.368</cell></row><row><cell>Iowa</cell><cell>uiowa07entD2</cell><cell>0.276</cell><cell>0.555</cell><cell>0.354</cell></row><row><cell>Hyberdad</cell><cell>QRYBASICRUN</cell><cell>0.202</cell><cell>0.413</cell><cell>0.353</cell></row><row><cell>CSIRO</cell><cell>CSIROdsQonly</cell><cell>0.127</cell><cell>0.282</cell><cell>0.305</cell></row><row><cell cols="2">St. Petersburg insu2</cell><cell>0.024</cell><cell>0.146</cell><cell>0.033</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,108.00,107.98,414.01,147.11"><head>Table 3 :</head><label>3</label><figDesc>Document search results for the feedback run with the highest MAP from each group, after promotion of the feedback documents.</figDesc><table coords="5,195.91,133.75,238.18,121.34"><row><cell>Group</cell><cell>Run</cell><cell cols="3">MAP NDCG P@20</cell></row><row><cell cols="2">Waterloo uwRF</cell><cell>0.500</cell><cell>0.787</cell><cell>0.585</cell></row><row><cell>UvA</cell><cell>uams07bfbex</cell><cell>0.470</cell><cell>0.750</cell><cell>0.555</cell></row><row><cell>UALR</cell><cell>UALR07Ent3</cell><cell>0.449</cell><cell>0.720</cell><cell>0.526</cell></row><row><cell>DUT</cell><cell>DUTDST3</cell><cell>0.424</cell><cell>0.696</cell><cell>0.523</cell></row><row><cell>Glasgow</cell><cell cols="2">uogEDSCLCDIS 0.411</cell><cell>0.714</cell><cell>0.482</cell></row><row><cell>Fudan</cell><cell>FDUFeedT</cell><cell>0.399</cell><cell>0.693</cell><cell>0.498</cell></row><row><cell>SJTU</cell><cell>SJTUEntDS04</cell><cell>0.387</cell><cell>0.706</cell><cell>0.501</cell></row><row><cell>Iowa</cell><cell>uiowa07entD4</cell><cell>0.370</cell><cell>0.672</cell><cell>0.474</cell></row><row><cell>CSIRO</cell><cell>CSIROdsQfb</cell><cell>0.256</cell><cell>0.435</cell><cell>0.436</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,121.06,281.28,387.88,206.88"><head>Table 4 :</head><label>4</label><figDesc>Expert ranking scores. The best run in each group according to MAP is shown.</figDesc><table coords="5,191.24,295.10,247.52,193.07"><row><cell>Group</cell><cell>Run</cell><cell>MAP</cell><cell>P@5</cell><cell>P@20</cell></row><row><cell cols="2">Tsinghua THUIRMPDD4</cell><cell cols="3">0.4632 0.2280 0.0910</cell></row><row><cell>SJTU</cell><cell>SJTUEntES03</cell><cell cols="3">0.4427 0.2360 0.0910</cell></row><row><cell>OU</cell><cell>ouExTitle</cell><cell cols="3">0.4337 0.2520 0.0950</cell></row><row><cell>CAS</cell><cell>ExpertRun02</cell><cell cols="3">0.3689 0.2040 0.0790</cell></row><row><cell>CSIRO</cell><cell>CSIROesQnarr</cell><cell cols="3">0.3655 0.2240 0.0770</cell></row><row><cell>Wuhan</cell><cell>WHU10</cell><cell cols="3">0.3399 0.1960 0.0710</cell></row><row><cell>Glasgow</cell><cell cols="4">uogEXFeMNZcP 0.3138 0.2200 0.0800</cell></row><row><cell>UvA</cell><cell>uams07exbl</cell><cell cols="3">0.3090 0.2080 0.0790</cell></row><row><cell>DUT</cell><cell>DUTEXP1</cell><cell cols="3">0.2630 0.1400 0.0580</cell></row><row><cell>Fudan</cell><cell>FDUn7e3</cell><cell cols="3">0.1788 0.1440 0.0610</cell></row><row><cell>Beijing</cell><cell>PRISRR</cell><cell cols="3">0.1571 0.0920 0.0440</cell></row><row><cell>Twente</cell><cell>qorwnewlinks</cell><cell cols="3">0.1481 0.1080 0.0540</cell></row><row><cell>Peking</cell><cell>zslrun</cell><cell cols="3">0.0944 0.0600 0.0220</cell></row><row><cell cols="2">Hyberbad AUTORUN</cell><cell cols="3">0.0939 0.0560 0.0330</cell></row><row><cell>UALR</cell><cell>UALR07Exp1</cell><cell cols="3">0.0200 0.0160 0.0130</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,108.00,107.98,322.05,221.04"><head>Table 5 :</head><label>5</label><figDesc>Expert ranking scores of feedback runs.</figDesc><table coords="6,108.00,121.79,322.05,207.22"><row><cell>Group</cell><cell>Run</cell><cell>MAP</cell><cell>P@5</cell><cell>P@20</cell></row><row><cell cols="5">CSIRO CSIROesQpage 0.3660 0.2040 0.0670</cell></row><row><cell>Iowa</cell><cell>uiowa07entE1</cell><cell cols="3">0.2828 0.1640 0.0710</cell></row><row><cell cols="2">Twente feedbackrun</cell><cell cols="3">0.2371 0.1480 0.0650</cell></row><row><cell cols="5">Table 6: Expert ranking scores of manual runs.</cell></row><row><cell cols="2">Group Run</cell><cell>MAP</cell><cell>P@5</cell><cell>P@20</cell></row><row><cell>OU</cell><cell>ouExNarrRF</cell><cell cols="3">0.4787 0.2720 0.0990</cell></row><row><cell>OU</cell><cell>ouExNarr</cell><cell cols="3">0.4675 0.2680 0.0980</cell></row><row><cell>DUT</cell><cell>DUTEXP3</cell><cell cols="3">0.3404 0.1840 0.0680</cell></row><row><cell>DUT</cell><cell>DUTEXP2</cell><cell cols="3">0.3324 0.1920 0.0640</cell></row><row><cell>DUT</cell><cell>DUTEXP4</cell><cell cols="3">0.1876 0.1000 0.0440</cell></row><row><cell cols="5">UALR UALR07Exp3 0.1840 0.1320 0.0360</cell></row><row><cell cols="2">other manual and the feedback runs.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,108.00,368.93,414.00,8.74;7,117.96,380.89,169.93,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,344.55,368.93,177.44,8.74;7,117.96,380.89,15.94,8.74">The CSIRO enterprise search test collection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,142.30,380.89,57.90,8.74">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="42" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.00,400.81,414.00,8.74;7,117.96,412.77,404.05,8.74;7,117.96,424.72,346.60,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,462.54,400.81,59.46,8.74;7,117.96,412.77,244.87,8.74">Relevance assessment: are judges exchangeable and does it matter?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,385.29,412.77,136.72,8.74;7,117.96,424.72,165.24,8.74">Proceedings of the 31st Annual International ACM SIGIR Conference</title>
		<meeting>the 31st Annual International ACM SIGIR Conference<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">July 20-24 2008</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
