<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.40,71.96,328.81,16.59">Overview of the TREC-2007 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.48,116.82,87.17,11.06"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.44,116.82,57.70,11.06"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.32,116.82,64.88,11.06"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
							<affiliation key="aff1">
								<orgName type="institution">NIST Gaithersburg</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.40,71.96,328.81,16.59">Overview of the TREC-2007 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AF8578EB99707F08203F1C2DEE04541</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The goal of the Blog track is to explore the information seeking behaviour in the blogosphere. It aims to create the required infrastructure to facilitate research into the blogosphere and to study retrieval from blogs and other related applied tasks. The track was introduced in 2006 with a main opinion finding task and an open task, which allowed participants the opportunity to influence the determination of a suitable second task for 2007 on other aspects of blogs besides their opinionated nature. As a result, we have created the first blog test collection, namely the TREC Blog06 collection, for adhoc retrieval and opinion finding. Further background information on the Blog track can be found in the 2006 track overview <ref type="bibr" coords="1,108.88,338.04,9.51,8.07" target="#b2">[2]</ref>.</p><p>TREC 2007 has continued using the Blog06 collection, and saw the addition of a new main task and a new subtask, namely a blog distillation (feed search) task and an opinion polarity subtask respectively, along with a second year of the opinion finding task. NIST developed the topics and relevance judgments for the opinion finding task, and its polarity subtask. For the blog distillation task, the participating groups created the topics and the associated relevance judgments. This second year of the track has seen an increased participation compared to 2006, with 20 groups submitting runs to the opinion finding task, 11 groups submitting runs to the polarity subtask, and 9 groups submitting runs to the blog distillation task. This paper provides an overview of each task, summarises the obtained results and draws conclusions for the future.</p><p>The remainder of this paper is structured as follows. Section 2 provides a short description of the used Blog06 collection. Section 3 describes the opinion finding task and its polarity subtask, providing an overview of the submitted runs, as well as a summary of the main used techniques by the participants. Section 4 describes the newly created blog distillation (feed search) task, and summarises the results of the runs and the main approaches deployed by the participating groups. We provide concluding remarks in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE BLOG06 TEST COLLECTION</head><p>All tasks in the TREC 2007 Blog track use the Blog06 collection, representing a large sample crawled from the blogosphere over an eleven week period from December 6, 2005 until February 21, 2006. The collection is 148GB in size, with three main components consisting of 38.6GB of XML feeds (i.e. the blog), 88.8GB of permalink documents (i.e. a single blog post and all its associated comments) and 28.8GB of HTML homepages (i.e. the main entry to the blog). In order to ensure that the Blog track experiments are conducted in a realistic and representative setting, the collection also includes spam, non-English documents, and some non-blogs documents such as RSS feeds.</p><p>The number of permalink documents in the collection is over 3.2 million, while the number of feeds is over 100,000 blogs. The permalink documents are used as a retrieval unit for the opinion finding task and its associated polarity subtask. For the blog distillation task, the feed documents are used as the retrieval unit. The collection has been distributed by the University of Glasgow since March 2006. Further information on the collection and how it was created can be found in <ref type="bibr" coords="1,402.62,283.31,9.51,8.07" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OPINION FINDING TASK</head><p>Many blogs are created by their authors as a mechanism for selfexpression. Extremely-accessible blog software has facilitated the act of blogging to a wide-ranging audience, their blogs reflecting their opinions, philosophies and emotions. The opinion finding task is an articulation of a user search task, where the information need seems to be of an opinion, or perspective-finding nature, rather than fact-finding. While no explicit scenario was associated with the opinion retrieval task, it aims to uncover the public sentiment towards a given entity (the "target"), and hence it can naturally be associated with settings such as tracking consumer-generated content, brand monitoring, and, more generally, media analysis. This is the second running of the opinion finding task in the Blog track. This year, it was the most popular task of the track, with 20 participating groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topics and Relevance Judgments</head><p>Similar to TREC 2006, the opinion retrieval task involved locating blog posts that express an opinion about a given target <ref type="bibr" coords="1,526.36,500.99,9.66,8.07" target="#b2">[2]</ref>. The target can be a "traditional" named entity, e.g. a name of a person, location, or organisation, but also a concept (such as a type of technology), a product name, or an event. The task can be summarised as What do people think about X, X being a target. The topic of the post is not required to be the same as the target, but an opinion about the target had to be present in the post or one of the comments to the post, as identified by the permalink.</p><p>Topics used in the opinion finding task follow the familiar title, description, and narrative structure, as used in topics in other TREC test collections. 50 topics were again selected by NIST from a larger query log obtained from a commercial blog search engine. The topics were created by NIST using the same methodology as last year, namely selecting queries from the query log, and building topics around those queries <ref type="bibr" coords="1,416.20,647.39,9.51,8.07" target="#b2">[2]</ref>. An example of a TREC 2007 topic is included in Figure <ref type="figure" coords="1,392.90,657.83,3.34,8.07" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pooling and Assessment Procedure</head><p>Participants could create queries manually or automatically from the 50 provided topics. They were allowed to submit up to six runs, including a compulsory automatic run using the title field of Pictures on an Ikea-related site that are not related to the store or its products are not relevant. &lt;/narr&gt; &lt;/top&gt; the topics, and another compulsory automatic run, using the title field of the topics, but with all opinion-finding features turned off. The latter was required to draw further conclusions on the extent to which a strong topic relevance baseline is required for an effective opinion retrieval system. It also helps to draw conclusions on the real effectiveness of the specifically used opinion finding approaches.</p><p>As mentioned in Section 2, for the purposes of the opinion finding task, the document retrieval unit in the collection is a single blog post plus all of its associated comments as identified by a permalink. However, participants were free to use any of the other Blog06 collection components for retrieval such as the XML feeds and/or the HTML homepages.</p><p>Overall, 20 groups participated in the opinion finding task, submitting 104 runs, including 98 automatic runs and 6 manual runs. The participants were asked to prioritise runs, in order to define which of their runs would be pooled. Like in TREC 2006, the guidelines of the Blog track encouraged participants to submit manual runs to improve the quality of the test collection. Each submitted run consisted of the top 1,000 opinionated documents (permalinks) for each topic. NIST formed the pools from the submitted runs using the three highest-priority runs per group, pooled to depth 80. In case of ties, the manual runs were preferred over the automatic runs, and among the automatic title-only tied runs, the compulsory ones were preferred.</p><p>NIST organised the relevance assessments for the opinion finding task, using the same assessment procedure defined in 2006 <ref type="bibr" coords="2,280.20,544.08,9.51,8.07" target="#b2">[2]</ref>, with some further tightening up of the guidelines given to the assessors. In particular, the assessment procedure had two levels. The first level assesses whether a given blog post, i.e. a permalink, contains information about the target and is therefore relevant. The second level assesses the opinionated nature of the blog post, if it was deemed relevant in the first assessment level. Given a topic and a blog post, assessors were asked to judge the content of the blog posts. The following scale was used for the assessment: 0 Not relevant. The post and its comments were examined, and do not contain any information about the target, or refers to it only in passing.</p><p>1 Relevant. The post or its comments contain information about the target, but do not express an opinion towards it. To be assessed as "Relevant", the information given about the tar- get should be substantial enough to be included in a report compiled about this entity.</p><p>If the post or its comments are not only on target, but also contain an explicit expression of opinion or sentiment towards the target, showing some personal attitude of the writer(s), then the document had to be judged using the three labels below: 2 Negatively opinionated. Contains an explicit expression of opinion or sentiment about the target, showing some personal attitude of the writer(s), and the opinion expressed is explicitly negative about, or against, the target.</p><p>3 Mixed. Same as ( <ref type="formula" coords="2,387.17,295.32,3.17,8.07">2</ref>), but contains both positive and negative opinions.</p><p>4 Positively opinionated. Same as ( <ref type="formula" coords="2,445.01,324.36,3.17,8.07">2</ref>), but the opinion expressed is explicitly positive about, or supporting, the target.</p><p>Posts that are opinionated, but for which the opinion expressed is ambiguous, mixed, or unclear, were judged simply as "mixed" (3 in the scale).</p><p>Table <ref type="table" coords="2,348.48,386.40,4.48,8.07" target="#tab_0">1</ref> shows a breakdown of the relevance assessment of the pooled documents, using the assessment procedure described above. About 78% of the pooled documents were judged as irrelevant. Moreover, there were roughly an equal percentage of negative and mixed opinionated documents, but slightly more positive opinionated documents, suggesting that overall, the bloggers had more positive opinions about the topics tackled by the TREC 2007 opinion finding topics set. Figure <ref type="figure" coords="2,411.02,459.72,4.48,8.07" target="#fig_2">2</ref> shows the number of relevant positive and negative opinionated documents for each topic. Topic "northernvoice" (914) or topic "mashup camp" (925) have only relevant positive opinionated documents in the pool, whereas topic "censure" (943) or topic "challenger" (923) have more negative than positive opinionated documents in the pool, perhaps illustrating the nature of these tackled topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overview of Results</head><p>Since the opinion finding task is an adhoc-like retrieval task, the primary measure for evaluating the retrieval performance of the participating groups is the mean average precision (MAP). Other metrics used for the opinion finding task are R-Precision (R-Prec), binary Preference (bPref), and Precision at 10 documents (P@10).</p><p>Table <ref type="table" coords="2,347.88,606.60,4.48,8.07" target="#tab_1">2</ref> provides the average best, median and worst MAP measures for each topic, across all submitted 104 runs. While these are not "real" runs, they provide a summary of how well the spread of participating systems is performing. In particular, it is of interest to note that the retrieval performances of the participating groups in TREC 2007 are markedly higher than those reported in TREC 2006 on the same task. For example, the median MAP measure of the submitted runs for the opinion finding task has increased from 0.1059 in TREC 2006 <ref type="bibr" coords="2,398.80,690.24,10.43,8.07" target="#b2">[2]</ref> to 0.2416 in TREC 2007. Further investigation is required in order to conclude whether this is due to the TREC 2007 topics being easier than those used in TREC 2006, or if  the increase is due to the use of more effective retrieval approaches by the participants. Table <ref type="table" coords="3,85.56,397.32,4.48,8.07" target="#tab_3">3</ref> shows the best-scoring opinion-finding title-only automatic run for each group in terms of MAP, and sorted in decreasing order. R-Prec, bPref and P@10 measures are also reported. Table <ref type="table" coords="3,288.36,418.32,4.48,8.07" target="#tab_5">4</ref> shows the best opinion-finding run from each group, in terms of MAP, regardless of the topic length used.</p><p>Each participating group was required to submit a compulsory automatic run, using only the title field of the topics, with all opinion finding features of the retrieval system turned off (i.e. a topicrelevance baseline run). The idea is to have a better understanding of the actual effectiveness of the opinion detection approaches deployed by the participating groups, allowing to draw conclusions as to whether the used opinion finding techniques actually help retrieving opinionated documents. Table <ref type="table" coords="3,198.98,522.84,4.48,8.07" target="#tab_7">5</ref> shows the best baseline run from each group, in terms of opinion-finding MAP. Comparing Tables <ref type="table" coords="3,93.36,543.84,4.48,8.07" target="#tab_3">3</ref> and<ref type="table" coords="3,115.68,543.84,3.34,8.07" target="#tab_7">5</ref>, it is interesting to note that only one of the top five performing opinion finding runs was actually a topic-relevance baseline run. In particular, out of the 5 best opinion-finding performing runs in Table <ref type="table" coords="3,133.36,575.16,3.34,8.07" target="#tab_3">3</ref>, only run uams07topic from the University of Amsterdam was a topic-relevance run.</p><p>In order to assess which opinion finding features and approaches deployed by the participating groups have actually worked, we compare the performance of the best performing opinion finding run of each group to its best submitted topic-relevance baseline. A relative increase in performance indicates that the used opinion finding features were useful. A relative decrease in performance indicates that the deployed opinion finding features did not help in retrieval. Table <ref type="table" coords="3,75.72,669.36,4.48,8.07" target="#tab_9">6</ref> shows the improvements of the best submitted compulsory automatic title-only runs over the baselines. Note that the best performing group on the opinion finding task, namely the UIC group, did not officially submit a baseline run, making it difficult to conclude on the success of their deployed opinion finding features. It is interesting to note that the best opinion finding run by the University of Amsterdam has decreased the performance of the their strongly performing uams07topic topic-relevance baseline by over 57%. On the other hand, the opinion finding features used by the University of Glasgow, Indiana University, and the University of Arkansas at Little Rock seem to be helpful, improving their performance on the task by 15.8%, 14% and 13.9%, respectively, depsite their good performing baselines.</p><p>Given the two levels assessment procedure, it is possible to evaluate the submitted runs in a classical adhoc fashion, i.e. based on the relevance of their returned documents (judged 1 or above, as described in Section 3.2 above). Table <ref type="table" coords="3,444.98,303.24,4.48,8.07" target="#tab_10">7</ref> reports the best run from each group in terms of topic-relevance, regardless of the topic length.</p><p>Moreover, Table <ref type="table" coords="3,386.37,324.12,4.48,8.07" target="#tab_2">8</ref> reports the Spearman's ρ and Kendall's τ correlation coefficients between opinion finding and topic relevance measures. The overall rankings of systems on both opinion-finding and topic relevance measures are very similar, as stressed by the obtained high correlations. A similar finding was observed in TREC 2006 <ref type="bibr" coords="3,337.32,376.44,9.51,8.07" target="#b2">[2]</ref>, suggesting again that good performances on the opinion finding task are strongly dominated by good performances on the underlying topic-relevance task. Figure <ref type="figure" coords="3,465.18,397.32,3.60,8.07" target="#fig_4">3</ref>(a) shows a scatter plot of opinion-finding MAP against topic-relevance MAP, which confirms that the correlation is very high.</p><p>Finally, we report on the extent to which the 17,958 presumed splog feeds and their associated 509,137 spam posts, which were injected into the Blog06 collection during its creation have infiltrated the pool. Table <ref type="table" coords="3,394.34,460.08,4.48,8.07" target="#tab_4">9</ref> provides details on the number of presumed splog posts which infiltrated each element of the relevance scale. In total, 7,086 assumed splog documents were pooled, less than 1.5% of the splog posts in the collection. Moreover, there was a roughly equal number of relevant only and opinionated splog posts, though those that were opinionated were mostly positive. Figure <ref type="figure" coords="3,526.38,512.40,4.48,8.07">4</ref> shows the average number of spam documents retrieved by all 104 submitted runs for each topic, in decreasing order.</p><p>Noticeably, unlike in last year's TREC 2006 topics set where the most spammed topics where about health, we note that topic 915 (namely "allianz") had by far the largest number of splog posts retrieved in the submitted runs (average 703 documents per run). Topic "grammys" (936) and topic "teri hatcher" also had a substantial number of splog posts retrieved (average 466 and 309 documents per run, respectively). These are widely popular topics, which might be prone to being spammed. Similar to TREC 2006 though, topics which retrieved far fewer spam documents, were concerning people not featuring in the tabloid news as often, such as topics 924 and 904: "mark driscoll" (23 documents) and "alterman" (9 documents), respectively.</p><p>Next, we examined how the participating systems had been affected by spam documents. Table <ref type="table" coords="3,444.48,679.80,8.92,8.07" target="#tab_11">10</ref> shows the mean number of splog documents in the top 10 ranked documents (denoted Spam@10), and for all the retrieved documents (Spam@all). The table also reports BadMAP, which is the Mean Average Precision when the pre-    sumed spam documents are treated as the relevant set. BadMAP shows when spam documents are retrieved at early ranks (a low BadMAP value is good, while a high BadMAP is bad as more spam documents are being retrieved at early ranks). From this table, we can see that some runs were less susceptible to spam documents than others. In particular, the run from UIC exhibited a perfect 0 BadMAP and the lowest Spam@10 and Spam@all measures, suggesting that this group has very successfully applied splog detection techniques (Indeed, UIC has experimented with a spam detection module in TREC 2007). In contrast, the run NTUAutoOp from NTU was affected much more by splog documents.</p><p>To see if runs that retrieved less spam documents were more likely to be high performing systems or low performing systems, we correlated the ranking of submitted runs by BadMAP, correlating this with opinion finding MAP. However, the correlation was low (ρ = 0.01, τ = 0.03), showing that for this task, systems which did remove spam documents were not any more likely to have a higher opinion retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Polarity Subtask</head><p>The polarity subtask was introduced in TREC 2007 as a natural extension of the opinion task, and was intended to represent a text classification-related task, requiring participants to determine the polarity (or orientation) of the opinions in the retrieved documents, namely whether the opinions in a given document are positive, negative or mixed. Participants were encouraged to use last years 50 opinion task queries, with their associated relevance judgments for training. Indeed, during the assessment procedure in the TREC 2006 blog track, for each document in the pool, the NIST assessors have specified the polarity of the relevant documents as described in Section 3.2 above: relevant negative opinion (judged as 2 in qrels); relevant mixed positive and negative (judged as 3 in qrels); relevant positive opinion (judged as 4 in qrels).</p><p>Groups participating in the opinion task and wishing to submit runs to the polarity subtask were asked to provide a corresponding and separate file for a submitted run to the opinion task, which details the predicted polarity for each retrieved document for each query. Submitted runs included the same documents in the same order as for the opinion finding runs, but with an additional polarity predictive label. Overall, 11 groups submitted 38 runs to the polarity subtask, including 32 automatic runs and 6 manual runs.</p><p>The initial intention was to evaluate the submitted runs using a classification accuracy measure (i.e. set precision). However, a measure like classification accuracy is comparable between runs only when every run classifies every document in the test set. In the polarity subtask, each run only provides a classification for the documents in its associated ranked opinion finding run. This presents three problems: not every run classifies the same documents, the treatment of unclassified documents is undefined, and no standard cutoff in the ranking is apparent.</p><p>To provide scores that are suitably comparable between runs, we report a measure called "R-accuracy" (R-Acc). This is the fraction of retrieved documents above rank R that are classified correctly, where R is the number of opinion-containing documents for that topic. The proposed measure is analogous to R-precision where only the correctly-classified opinion documents are counted as relevant. We also report accuracy at fixed rank cutoffs (A@10 and A@1000) as a secondary metric. For all measures, unjudged retrieved documents have no correct classification. The assumption is that if a submitted run had known that the document was not opinionated then the run should not have retrieved it, i.e. by retrieving it the run assumes that the document was opinionated, and hence must have wrongly classified it. Table <ref type="table" coords="6,84.84,306.12,8.92,8.07" target="#tab_12">12</ref> shows the best-scoring title-only polarity detection run for each group in terms of R-accuracy, and sorted in decreasing order of R-accuracy, while Table <ref type="table" coords="6,165.45,327.00,8.92,8.07" target="#tab_13">13</ref> shows the same information, but regardless of the topic length. Noticeable from these tables is that the runs appear to be clustered into two groups, those above 11% polarity detection R-accuracy, and those below.</p><p>It is interesting to note that the Spearman's ρ and Kendall's τ correlation coefficients between the polarity detection R-accuracy results and their corresponding opinion-finding MAP results over the 38 submitted polarity runs are very high (ρ = 0.9345 and τ =0.8065). This can be explained by the fact that the systems which are more successful at retrieving opinionated documents ahead of relevant ones, will then have more documents for which they can make a correct classification. Systems which perform poorly at retrieving opinionated documents are by definition not going to have the chance to classify as many documents correctly, hence the strong correlation is expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Participant Approaches</head><p>There were a wide range of deployed techniques by the participating groups. In this section, we focus on those groups whose use of opinion finding features have markedly improved their topicrelevance baseline as shown in Table <ref type="table" coords="6,195.50,536.64,3.34,8.07" target="#tab_9">6</ref>. Looking into the main features of the best submitted runs, we note the following: Indexing All the participating groups only indexed the Permalink component of the Blog06 collection, but the group from the University of Waterloo, which used all three components of the collection namely, Permalinks, Feeds and Homepages.</p><p>Retrieval Similar to TREC 2006, most of the participating groups used a two-stage approach for document retrieval <ref type="bibr" coords="6,255.80,627.48,9.51,8.07" target="#b2">[2]</ref>. In the first stage, documents are ranked using a variety of document weighting models ranging from BM25 (e.g. University of Indiana and University of Waterloo) to Divergence From Randomness models (e.g. University of Glasgow and FIU (Netlab team)), through language modelling (e.g. University of Amsterdam). Many participants used off-the-shelf systems such as Indri or Terrier. In the second stage of the retrieval process, the retrieved documents are re-ranked taking into account opinion finding features, often through a combination of scores mechanism.</p><p>Opinion Finding Features From looking at the results, we observe that there were two main effective approaches for detecting opinionated documents, which both led to improvements over a topic-relevance baseline. The first approach, used for example by the University of Glasgow and FIU, consists in automatically building a weighted dictionary from the relevance assessments of the TREC 2006's opinion finding task. The weight of each term in the dictionary estimates its opinionated discriminability. The weighted dictionary is then submitted as a query to generate an opinionated score for each document of the collection. The second approach, tested for example by the University of Arkansas at Little Rock and the University of Waterloo, uses a pre-compiled list of subjective terms and indicators and re-ranks the documents based on the proximity of the query terms to the aforementioned pre-compiled list of terms.</p><p>In the following, we provide more details on methods used by the 5 best performing groups, whose approaches for detecting opinionated documents have worked well, compared to a topic-relevance baseline as shown in Table <ref type="table" coords="6,414.72,543.83,3.47,8.07" target="#tab_9">6</ref>:</p><p>The University of Glasgow (UoG) experimented with two approaches for detecting opinionated documents, integrated into their Terrier search engine. The first purely statistical approach uses a compiled English word list collected from various available linguistic resources. UoG measured the opinionated discriminability of each term in the word list using an information theoretic divergence measure based on the relevance assessments of the TREC 2006's opinion finding task. They have then estimated the opinionated nature of each document in the collection with the PL2 Divergence from Randomness (DFR) weighting model, and using the weighted opinionated word list as a query. The same approach was used to detect polarity. Their second opinion detection approach uses OpinionFinder, a freely available toolkit, which identifies subjective sentences in text. For a given document, they adapted Opin-ionFinder to produce an opinion score for each document, based on the identified opinionated sentences.  detection approaches, UoG used the opinionated scores of the documents as prior evidence, and integrated them with the relevance scores produced by the document weighting model used. All their six submitted runs used the PL2F field-based weighting model. One of their topic-relevance baselines included a DFR-based proximity model. They found that the use of the word list-based statistical opinion detection approach markedly improved their topicrelevance only baseline, leading to a substantial and marked improvement of 15.8% compared to the topic-relevance baseline (run uogBOPFProxW vs run uogBOPFProx). Interestingly, they also found that the opinion finding technique based on the Opinion-Finder tool was as effective as the statistical word list-based approach, although it was less efficient. They also reported that the use of proximity search is helpful.</p><p>The University of Indiana (IndianaU) focused on combining multiple sources of evidence to detect opinionated blog postings. Their approach to opinion blog retrieval consisted of first applying traditional retrieval methods to retrieve on-topic blogs and then boosting the ranks of opinionated blogs based on combined opinion scores generated by multiple assessment methods. Indiana's opinion assessment/detection method is comprised of High Frequency Module, which identifies opinion blogs based on the frequently used opinion terms, low frequency module, which leverages uncommon/rare term patterns (e.g., 'sooo good') for expressing opinions, IU Module, which makes use of 'I' and 'You' collocations (e.g. 'I believe') that qualify opinion sentences, Wilson's lexicon module, which makes use of Wilson's subjective lexicons, and opinion acronym module, which utilises the small set of opinion acronyms (e.g., 'imho') that are likely to be missed by preceding modules. Indiana's training data consisted of TREC 2006's opinion finding relevance data supplemented by the external IMDB movie review data, both of which were used to tune their opinion scoring and fusion module in an interactive system optimisation mechanism called the Dynamic Tuning Interface. All of the lexicon terms were scored with positive and negative values, which facilitated their participation in the polarity subtask. They found that their opinion finding approach improves upon the topic-relevance only baseline.</p><p>The University of Arkansas at Little Rock (UArkansas) used various opinion finding heuristics on top of a topic-relevance baseline. Their best performing opinion finding run re-ranked the documents returned by the baseline, by taking into account the proximity of words such as "I", "you", "me", "us", we" and opinion indicator words such as "like", "feel","think","hate" to the actual query words. They found that such a simple proximity-based approach could markedly improve the opinion finding retrieval effectiveness of their topic relevance baseline (about 14% improvement). UArkansas also experimented with a machine learningbased approach, which re-ranks the baseline results by associating a category to the queries. This approach while slightly improving upon the performance of the topic-relevance baseline, was comparatively less successful than the proximity-based approach.</p><p>The Dalian University of Technology (DUT) filtered out all non-English blog posts during indexing. They used an external resource, namely the Wikipedia, and a manually built sentiment lexicon resource to find opinions. In the polarity subtask, DUT used a method based on SVM, to assess the polarity of the retrieved blog posts. Judging by the results, DUT found that their used sentiment resources had improved their initial topic-relevance baseline MAP with about 11%.</p><p>The University of Waterloo (UoW) used a manually constructed list of 1336 subjective adjectives in document ranking. The top 1000 documents retrieved using BM25 were re-ranked based on the proximity of each query term instance to the subjective adjectives. Experiments were also conducted with different types of queries constructed from the topic titles: single terms and userdefined phrases, i.e. phrases enclosed in quotation marks by the user. Some improvements over the topic-relevance baseline were achieved (about 5.8% improvement) when the initial document set was retrieved using phrases, while the subjective adjective-based re-ranking was done using single terms. UoW concluded that subjective adjectives located close to any word from the query are useful indicators of the presence of opinions expressed about the query topic.</p><p>It is of interest to make some comments about the submitted official runs by some participating groups. The University of Illinois at Chicago (UIC) achieved the top scoring opinion finding run. How-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Fields MAP R-prec b-Bref P@10 UIC <ref type="bibr" coords="8,143.68,66.24,4.11,8.07">(</ref> ever, they did not submit the compulsory topic-relevance baseline. Therefore, it is difficult to assess the usefulness of their opinion finding features. Nevertheless, UIC's retrieval system contained two sub-systems. The opinion retrieval system (ORS), which was modified from the TREC 2006 version, and was used for the main task and a polarity classification system (PCS), which was newly designed for the polarity subtask. UIC experimented with a single query-independent SVM classifier and tested a spam detection module.</p><p>The runs submitted by the University of Amsterdam (UvA) raise a few interesting issues. While they had a strongly performing topic-relevance baseline run (see run uams07topic in Table <ref type="table" coords="8,264.72,442.08,3.23,8.07" target="#tab_3">3</ref>), their used opinion finding features do not appear to be useful. UvA used the opinion finding task to compare the performance of an Indri implementation to their own mixture model. The mixture model combines different components of blog posts (e.g., headings, title, body) and assigns weights to these components based on tests on the TREC 2006 topics. Of both the baselines, the Indri system performed markedly better. To achieve better topical results, external (query) expansion on the AQUAINT-2 news corpus was performed. This expansion improves the performance of the Indri implementation, but hurts the mixture model. For opinion finding, UvA experimented with document priors in the mixture model based on either opinionated lexicons or the number of comments. The latter opinion finding features have not improved their opinion finding performance, markedly hurting their strongly performing uams07topic topic-relevance baseline run. In particular, run uams07topic is the 2nd top scoring title-only opinion finding run of the track, despite not using any opinion detection approach, suggesting that a strong retrieval baseline can do very well on the opinion finding task.</p><p>Interestingly, the Netlab team (FIU) used an approach that is very similar to the word list-based detection approach deployed by UoG, although developed separately. FIU used the DFR models, i.e. PL2 and the parameter free DPH, to assign both topic and opinion scores. A fully automatic and weighted dictionary was generated from TREC 2006's opinion finding relevance data. This dictionary was filtered and then submitted as a query to the Terrier search engine to get an initial query-independent opinion score of all re-trieved documents. Ranking is done in two passages: a first topicalopinion ranking is obtained from the query-independent opinion score divided by the content rank, then the final topical-opinion ranking is established from the content score divided by the previous topical-opinion rank. Since FIU updated the final ranks but not the final topical-opinion scores in the re-ranking, trec eval reported the same performance for all their official submitted runs. However, using the Terrier evaluation tool, which instead evaluates runs by ranks and not by scores, they show that FIU's opinion finding approach is actually effective. Indeed, their opinion finding run FIUIPL2 has about 17% improvement over their topic relevance baseline, an improvement in the same line as observed with UoG's wordlist-based approach, and expected given the similarities of the two groups's approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Summary of Opinion Finding Task</head><p>The additional requirement that each participating group submits a compulsory topic-relevance baseline run allowed us to draw more conclusions on those opinion detection approaches that have worked and those that have not, providing additional insights for future work.</p><p>The overall opinion finding performance of the participating groups this year was markedly higher than the one observed for the TREC 2006 topics set. However, it is difficult to assess whether this increase in performance is due to the better deployed opinion finding systems and techniques or whether it is due to the difficulty level of the topics set. Answering this question requires running this year's systems on last year's topics.</p><p>Finally, similar to last year's conclusion, there appears to be no strong evidence that spam was a major hindrance to the retrieval performance of the participating groups. focuses on an interesting feature of the blogs, namely the fact that feeds are aggregates of blog posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BLOG DISTILLATION (FEED SEARCH) TASK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivations</head><p>Blog search users often wish to identify blogs (i.e. feeds) about a given topic, which they can subscribe to and read on a regular basis. This user task is most often manifested in two scenarios:</p><p>• Filtering: The user subscribes to a repeating search in their RSS reader.</p><p>• Distillation: The user searches for blogs with a recurring central interest, and then adds these to their RSS reader.</p><p>For TREC 2007, the latter scenario was investigated i.e. blog distillation, which is a feed search task. The blog distillation task can be summarised as Find me a blog with a principle, recurring interest in X. For a given target X, systems should suggest feeds that are principally devoted to X over the timespan of the feed, and would be recommended to subscribe to as an interesting feed about X (i.e. a user may be interested in adding it to their RSS reader). This task is particularly interesting for the following reasons:</p><p>• A similar (yet-different) task has been investigated in the Enterprise track (Expert Search) in a smaller setting (around 1000 candidate experts on the W3C collection). For blog distillation, the Blog06 corpus contains around 100k blogs, and is a Web-like setting (with anchor text, linkage, spam, etc).</p><p>• A Topic distillation task was run in the Web track. In Topic distillation, site relevance was defined as (i) it is principally devoted to the topic, (ii) it provides credible information on the topic, and (iii) it is not part of a larger site also principally devoted to the topic.</p><p>While the definition of blog distillation as explained above is different, the idea is to provide the users with the key blogs about</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Fields R-Acc A@10 A@1000 UIC <ref type="bibr" coords="10,169.72,66.24,4.11,8.07">(</ref>  a given target. Note that point (iii) from the definition of the Web track Topic distillation task is not applicable in a blog setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topics and Relevance Judgments</head><p>For the purposes of the blog distillation task, the retrieval document units are documents from the feeds component of the Blog06 collection. However, similar to the opinion finding task, the participating groups were free to use any other component of the Blog06 test collection in their submitted runs.</p><p>The topics for the blog distillation were created and judged by the participating groups. Each participating group has been asked to provide 6 or 7 topics along with some relevant feeds. A standard search system for documents on the Blog06 collection using the Terrier search engine <ref type="bibr" coords="10,132.80,617.04,10.43,8.07" target="#b3">[3]</ref> was provided by the University of Glasgow to help the participating groups in creating their blog distillation topics. The system displays the corresponding feed for each returned document (i.e. blog post), as well as all the documents for a given feed. Eight groups contributed each 5 to 7 topics. 45 topics were finally chosen by NIST from the proposed set of topics. A sample blog distillation topic is shown in Figure <ref type="figure" coords="10,228.86,679.80,3.34,8.07" target="#fig_5">5</ref>.</p><p>Overall, 9 groups submitted runs and agreed to help in their relevance judgments. Once runs were submitted, NIST formed pool and sent them to the University of Glasgow, where the community assessment system was hosted. The community judgments system interface was ported directly from the TREC Enterprise judgment system for expert search task developed by Soboroff et al. <ref type="bibr" coords="10,526.16,253.80,9.81,8.07" target="#b5">[4]</ref>.</p><p>Participants were allowed to submit up to 4 runs, including a compulsory title-only run. Similar to the opinion finding task, the participants were asked to prioritise runs, in order to define which of their runs would be pooled. Each run has feeds ranked by their likelihood of having a principle (recurring) interest in the topic. Given the number of feeds in the collection (just over 100k feeds), each submitted run consisted of up to 100 feeds for each topic. A pool has then been formed by NIST from the 32 submitted runs, using the two highest-priority runs per group, pooled to depth 50.</p><p>For the assessment of the relevance of a feed, the assessors were asked to browse some of the documents of the feed, and then make a judgment on whether the feed has a recurring principle interest in the topic area. These guidelines are intentionally vague. A question that may arise is the number of documents (i.e. posts) that have to be read by the assessor for a given feed. Since there is no straightforward answer to this question, we decided to suggest that the assessors read enough documents of the feed such that they are certain that the feed has a more than passing interest in the topic area, and that they would be interested in subscribing to the feed in their RSS reader if they were interested in the topic area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overview of Results</head><p>The blog distillation task is another articulation of real user tasks in adhoc search behaviour on the blogosphere. Therefore, we use mean average precision (MAP) as the main metric for the evaluation of the retrieval performance of the submitted runs. In addition, we also report R-Precision (R-Prec), binary Preference (bPref), and Precision at 10 documents (P@10).</p><p>All submitted runs were automatic. Table <ref type="table" coords="10,479.32,564.71,8.92,8.07" target="#tab_14">14</ref> provides the average best, median and worst MAP measures for each topic, across all submitted 32 runs. Figure <ref type="figure" coords="10,414.64,585.71,4.48,8.07">6</ref> shows the distribution of the number of relevant feeds per topic in the pooled feeds, sorted in decreasing order. In particular, there appears to be a wide variance in the number of relevant feeds across the used 45 topics, with topics having as many as 153 relevant feeds (e.g. "christmas" (968) or "music" (978)), while other having as few as 5 relevant feeds (e.g. "Violence in Sudan" (964) or "machine learning" (982)).</p><p>Table <ref type="table" coords="10,348.84,658.91,8.92,8.07" target="#tab_17">15</ref> shows the best-scoring automatic title-only run from each participating group in terms of MAP, and sorted in decreasing order. Table <ref type="table" coords="10,363.28,679.79,8.92,8.07" target="#tab_18">16</ref> shows the best run from each group, regardless of the topic length used. Note that most of the 32 submitted runs were title-only runs. Indeed, there were 25 submitted runs using the title field only, 3 submitted runs used the title, description and narrative fields, 2 submitted runs used the title and description fields, and 2 submitted runs used the description field only. All the 10 best submitted blog distillation runs but one are title-only runs. Given the rather small number of submitted runs using long queries, it is difficult to draw conclusions as to whether the description and narrative fields of the topics might be helpful in the blog distillation task. We examined whether the participating systems in the blog distillation task had been affected by spam, i.e. how many splog feeds have infiltrated the pool. Table <ref type="table" coords="11,164.90,501.72,8.92,8.07" target="#tab_16">17</ref> shows the breakdown of the feed distillation pool in terms of splog feeds. Moreover, Table <ref type="table" coords="11,259.31,512.28,9.17,8.07" target="#tab_19">18</ref> shows the extent to which the 17,958 presumed splogs have infiltrated the submitted runs. We use the mean number of splog documents in the top 10 ranked documents (denoted Spam@10), in the retrieved documents (Spam@all), and finally BadMAP, which is the Mean Average Precision when the splog feeds are treated as the relevant set. Run UMaTiPCSwGR from UMass appears to be overall the least susceptible to splog feeds. On the contrary, run TDWHU200 was one of the most affected runs by splog feeds.</p><p>Similar to the analysis performed in Section 3.3, to see if runs that retrieved less splog feeds were more likely to be high performing systems or low performing systems, we correlated the ranking of submitted runs by BadMAP, correlating this with blog distillation MAP. For this task, a weak correlation was exhibited (ρ = -0.193, τ = -0.157), showing some evidence that systems which did remove splogs were likely to have a higher retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Participant Approaches</head><p>There were a wide range of deployed indexing and retrieval approaches for the blog distillation task. The exploratory nature of most of the used techniques characterises the novelty of the task and its interesting underlying features. The main features of the submitted runs are summarised below: Indexing Two types of indexes have been used. Three groups created an index using the Feeds component of the Blog06 collection, namely Carnegie Mellon University (CMU), the University of Texas, and the University of Wuhan. The rest of the groups only indexed the Permalinks component of the collection. Interestingly, CMU, the top performing group, experimented with both types of index, and concluded that an index based on the Feeds component of the Blog06 collection leads to a better retrieval performance on this task.</p><p>Retrieval Many groups approached the blog distillation task by connecting the task to other existing search tasks. For example, the University of Glasgow (UoG) explored the connection of blog distillation to the expert finding task of the Enterprise track, adapting their Voting Model paradigm to feed search. The University of Massachusetts looked at the blog distillation task as a resource selection problem in distributed search. Most of the groups that used an index based on Permalinks, have proposed various techniques to aggregate the scores of blog posts into a score for their composing feed. For the purposes of document retrieval, a range of document weighting models such as Language Modelling approaches and Divergence From Randomness models were used. Some groups have also experimented with classical information retrieval techniques, namely query expansion (e.g. CMU) and proximity search (e.g. UoG).</p><p>In the following, we provide a detailed description of the methods used by the top 3 performing groups in the blog distillation task:</p><p>Carnegie Mellon University (CMU) explored two indexing strategies, namely a large-document model (feed retrieval) and a smalldocument model (entry or blog post retrieval). Under the largedocument model, feeds were treated as the unit of retrieval. Under the small-document model, the blog posts were treated as the unit of retrieval and aggregated to produce a final ranking of feeds. They found that the large-document approach outperformed the smalldocument approach on average. CMU also experimented with a query expansion method using the link structure and link text found within an external resource, namely the Wikipedia. CMU found that the used Wikipedia-based query expansion approach improves results under both the large-and small-document models.</p><p>The University of Glasgow (UoG) only indexed the Permalink component of the Blog06 collection. They investigated the connections between the blog distillation task and the expert search task. UoG adapted their Voting Model paradigm for Expert Search, by ranking feeds according to the number of on-topic posts each feed has (number of votes), and the extent to which the posts are about the topic area (strength of votes) -these two sources of evidence about the interests of each blogger were combined using the exp-CombMNZ voting technique. Posts are ranked using the PL2F Divergence From Randomness (DFR) field-based weighting model. They found that the additional use of a DFR-based term proximity model improves the topicality of the underlying ranking of blog posts, leading to a more accurate aggregated ranking of blog posts and a better feed search performance.</p><p>The University of Massachusetts (UMass) used language modelling approaches. UMass used the Permalink component of the Blog06 test collection for indexing. UMass looked at this task as a resource selection problem in distributed information retrieval, since each feed can be considered as a collection composed of blog posts. The most critical issue of resource selection is how a collection is represented. UMass applied two approaches for representation in this task. Further, since blogs which address many general and shallow topics are unlikely to be relevant in this task, UMass introduced an approach to penalise such blogs, and found that this improves the retrieval effectiveness.</p><note type="other">Group Run MAP</note><p>Other approaches used by the participating groups included the investigation of blog specific approaches such as time-based priors and splog detection and filtering, or retrieval models variants to search from a feeds-based index. The University of Amsterdam (UvA) experimented with time-based priors. Their suggested idea is that more recent posts reflect better the current interest of a blogger. Results show that time-based priors, which order the feeds based on the score of the most relevant post from a feed, improve slightly over the baseline run. UvA also experimented with a relevant posts count, where for every feed the ratio of relevant posts to all posts in a feed is calculated and this score is combined with the feed relevance score from the baseline run. Results show that this has markedly decreased performance, suggesting that the combination parameters were not appropriate.</p><p>Kobe University (Seki et al.) experimented with splog detection, and filtering of non-English documents. Interestingly, their baseline is built by computing the similarity scores between a query and the posts included in the feed. They plotted a line for each blog site with the x-axis being the (normalised) post date and the y-axis being the computed similarity. The feeds are then ranked according to the descending order of the surface area under the plotted line. The intuition behind the proposed algorithm is that a relevant feed would frequently mention a given topic, and will constantly have a high similarity with the topic (query), resulting in a large surface area under the line of similarity scores. They found that filtering splogs and non-English documents improves their baseline.</p><p>Finally, the University of Texas' School of Information (UT) used a retrieval strategy based on a variant of the Kullback-Leibler (KL) divergence model. Given a query q the UT system derives a score for each feed f in the corpus by the negative KL-divergence between the query language model and the language model for f . The effectiveness of the proposed approach cannot be assessed without an experimental baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Summary of Blog Distillation Task</head><p>The blog distillation task was a new task in TREC 2007. Overall, some of the deployed retrieval approaches achieved reasonable retrieval performances. One of the issues that might need to be further investigated in this task is whether it is beneficial to use the Feeds component of the Blog06 collection, instead of or in addition to the Permalinks component.</p><p>There was a wide variance in the distribution of relevant feeds in the used 45 topics, suggesting that the guidelines for the topic creation and assessments still require tightening for future iterations of this task. However, the task, as exemplified by the exploratory nature of the participants runs, promises much research in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>The TREC 2007 Blog track included two main tasks, namely the opinion finding and Blog distillation (aka feed search) tasks, which we believe are good articulations of real user tasks in adhoc search behaviour on the blogosphere. The used tasks address two interesting components of blogs: the feed itself and its constituent blog posts and their corresponding comments. As a consequence, a new topics set has been created for the opinion finding task, and a new test collection has been created for the Blog distillation task, therefore contributing to the creation of reusable resources for supporting research into blog search.</p><p>Much remains to be learned about opinion finding, even though the runs submitted this year show that some participants have been successful in proposing new opinion detection techniques, which show some marked improvements on the respective topic-relevance baseline. Indeed, this year's findings also consolidate the findings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Spam@10 Spam@all BadMAP *10  <ref type="table" coords="13,237.23,172.08,7.41,8.07" target="#tab_17">15</ref>, in the order given. Spam@10 is the mean number of splog feeds in the top 10 ranked documents for each topic, Spam@all is the mean number of splog feeds retrieved for each topic. BadMAP is the Mean Average Precision when the splog feeds are treated as the relevant set. This shows when spam feeds are retrieved at high ranks. For all measures, lower means the system was better at not retrieving splogs.</p><p>of the previous Blog track 2006. In particular, a good performance in opinion finding is strongly dominated by its underlying topicrelevance baseline (i.e. opinion-finding MAP and topic-relevance MAP are very highly correlated). Indeed, a strongly performing topic-relevance baseline can still perform extremely well in opinion finding, as exemplified by the University of Amsterdam's submitted topic-relevance baseline. One possible methodology to have a better understanding of the deployed opinion detection techniques is to use a common and strong topic-relevance baseline for all participating groups.</p><p>For the polarity subtask, the overall performances of the participating groups are rather average, suggesting that the task of detecting the polarity of an opinion is still an open problem, which requires further research. We believe that polarity detection should be a more integral part of the opinion finding task, and not evaluated as in classification task-like manner. For future iterations of the opinion finding task, we believe that a better integration of the polarity component would involve creating a balanced number of topics, which explicitly specify whether they require positive or negative opinions to be retrieved. Evaluation can then be carried out in a more straightforward adhoc manner.</p><p>The Blog distillation task seems to have generated some very promising and interesting retrieval techniques. We plan to run the task again for 2008, in a similar fashion, but with clearer guidelines for the creation of the topics. This will provide further insights on the most effective techniques for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.76,53.28,26.98,7.05;2,64.56,63.72,129.22,7.05;2,64.56,84.60,118.54,7.05;2,64.56,105.60,102.46,7.05;2,59.16,116.04,204.58,7.05;2,53.76,126.48,37.78,7.05;2,64.56,136.92,91.66,7.05;2,64.56,147.36,188.38,7.05;2,53.76,157.80,199.06,7.05;2,53.76,168.36,193.90,7.05"><head></head><label></label><figDesc>Find opinions on Ikea or its products. &lt;/desc&gt; &lt;narr&gt; Narrative: Recommendations to shop at Ikea are relevant opinions. Recommendations of Ikea products are relevant opinions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,60.48,241.20,225.56,8.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Blog track 2007, opinion retrieval task, topic 930.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,53.76,246.12,239.14,8.07;3,53.76,256.56,104.12,8.07;3,71.01,54.10,205.17,179.72"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of positive and negative opinionated documents per topic in the pool.</figDesc><graphic coords="3,71.01,54.10,205.17,179.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,106.80,453.17,2.34,3.89;4,100.97,438.31,8.17,3.88;4,103.30,423.50,5.84,3.88;4,100.97,408.64,8.17,3.88;4,103.30,393.79,5.84,3.88;4,100.97,378.98,8.17,3.88;4,103.30,364.13,5.84,3.88;4,100.97,349.27,8.17,3.88;4,103.30,334.45,5.84,3.88;4,100.97,319.60,8.17,3.88;4,111.07,457.36,189.01,3.88;4,90.10,395.67,4.15,21.42;4,90.10,371.19,4.15,21.42;4,90.10,358.95,4.15,9.18;4,175.05,463.48,58.14,4.15;4,87.72,468.60,215.97,8.07;4,87.72,477.48,56.83,8.07;4,325.08,457.37,2.34,3.89;4,319.24,443.60,8.17,3.88;4,321.58,429.83,5.84,3.88;4,319.24,416.03,8.17,3.88;4,321.58,402.26,5.84,3.88;4,319.24,388.49,8.17,3.88;4,321.58,374.72,5.84,3.88;4,319.24,360.95,8.17,3.88;4,321.58,347.15,5.84,3.88;4,319.24,333.38,8.17,3.88;4,321.58,319.61,5.84,3.88;4,308.38,385.53,4.15,9.18;4,417.81,463.48,9.18,4.15;4,465.92,323.60,29.41,3.88;4,464.05,327.80,31.28,3.88;4,306.00,468.60,215.90,8.07;4,306.00,477.48,78.77,8.07"><head></head><label></label><figDesc>Scatter plot of opinion-finding MAP against topicrelevance MAP. Opinion finding MAP vs topic-relevance MAP, sorted by opinion-finding MAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,169.68,493.08,270.45,8.07"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Figures examining opinion-finding and topic-relevance MAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,62.52,447.84,221.48,8.07"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Blog track 2007, blog distillation task, topic 994.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,327.36,55.20,218.09,85.71"><head>Table 1 : Relevance assessments of documents in the pool.</head><label>1</label><figDesc></figDesc><table coords="2,334.92,55.20,202.89,65.10"><row><cell>Relevance Scale</cell><cell cols="2">Label Nbr. of Documents</cell><cell>%</cell></row><row><cell>Not Relevant</cell><cell>0</cell><cell cols="2">42434 77.7%</cell></row><row><cell>Adhoc-Relevant</cell><cell>1</cell><cell>5187</cell><cell>9.5%</cell></row><row><cell>Negative Opinionated</cell><cell>2</cell><cell>1844</cell><cell>3.4%</cell></row><row><cell>Mixed Opinionated</cell><cell>3</cell><cell>2196</cell><cell>4.0%</cell></row><row><cell>Positive Opinionated</cell><cell>4</cell><cell>2960</cell><cell>5.4%</cell></row><row><cell>(Total)</cell><cell>-</cell><cell cols="2">54621 100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.76,279.84,239.09,72.15"><head>Table 2 : Best, median and worst MAP measures for the 104 submitted runs to the opinion finding task.</head><label>2</label><figDesc></figDesc><table coords="3,69.84,279.84,207.02,39.87"><row><cell></cell><cell cols="2">Opinion-finding MAP Topic-relevance MAP</cell></row><row><cell>Best</cell><cell>0.5182</cell><cell>0.6382</cell></row><row><cell>Median</cell><cell>0.2416</cell><cell>0.3340</cell></row><row><cell>Worst</cell><cell>4.2e-05</cell><cell>0.0001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,316.80,55.24,239.26,103.67"><head>Table 8 : Correlation of system rankings between opinion- finding performance measures and topic-relevance perfor- mance measures. Both Spearman's Correlation Coefficient (ρ) and Kendall's Tau (τ ) are reported.</head><label>8</label><figDesc></figDesc><table coords="3,365.04,55.24,142.48,50.51"><row><cell>Evaluation Measure</cell><cell>ρ</cell><cell>τ</cell></row><row><cell>MAP</cell><cell cols="2">0.9778 0.8813</cell></row><row><cell>R-Prec</cell><cell cols="2">0.9677 0.8518</cell></row><row><cell>bPref</cell><cell cols="2">0.8118 0.9448</cell></row><row><cell>P@10</cell><cell cols="2">0.8032 0.9366</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,53.76,55.44,502.13,249.87"><head>Table 3 : Opinion finding results: the automatic title-only run from each of 20 groups with the best MAP, sorted by MAP. The best in each column is highlighted.</head><label>3</label><figDesc></figDesc><table coords="4,141.72,55.44,326.36,217.59"><row><cell>Group</cell><cell>Run</cell><cell>MAP</cell><cell>R-prec b-Bref P@10</cell></row><row><cell>UIC (Zhang)</cell><cell>uic1c</cell><cell cols="2">0.4341 0.4529 0.4724 0.690</cell></row><row><cell>UAmsterdam (deRijke)</cell><cell>uams07topic</cell><cell cols="2">0.3453 0.3872 0.3953 0.562</cell></row><row><cell>UGlasgow (Ounis)</cell><cell>uogBOPFProxW</cell><cell cols="2">0.3264 0.3657 0.3497 0.552</cell></row><row><cell>DalianU (Yang)</cell><cell>DUTRun2</cell><cell cols="2">0.3190 0.3671 0.3686 0.600</cell></row><row><cell>FudanU (Wu)</cell><cell cols="3">FDUTOSVMSem 0.3143 0.3465 0.3499 0.460</cell></row><row><cell>CAS (Liu)</cell><cell>Relevant</cell><cell cols="2">0.3041 0.3600 0.3779 0.446</cell></row><row><cell cols="2">UArkansas Littlerock (Bayrak) UALR07BlogIU</cell><cell cols="2">0.2911 0.3263 0.3134 0.580</cell></row><row><cell>IndianaU (Yang)</cell><cell>oqsnr2opt</cell><cell cols="2">0.2894 0.3572 0.3419 0.532</cell></row><row><cell>UNeuchatel (Savoy)</cell><cell>UniNEblog1</cell><cell cols="2">0.2770 0.3353 0.3074 0.492</cell></row><row><cell>FIU (Netlab team)</cell><cell>FIUbPL2</cell><cell cols="2">0.2728 0.3204 0.2925 0.454</cell></row><row><cell>UWaterloo (Olga)</cell><cell>UWopinion3</cell><cell cols="2">0.2631 0.3344 0.2980 0.496</cell></row><row><cell>Zhejiangu (Qiu)</cell><cell>EAGLE1</cell><cell cols="2">0.2561 0.3159 0.2867 0.428</cell></row><row><cell>CAS (NLPR-IACAS)</cell><cell>NLPRPST</cell><cell cols="2">0.2542 0.3168 0.2945 0.462</cell></row><row><cell>BUPT (Weiran)</cell><cell>prisOpnBasic</cell><cell cols="2">0.2466 0.3018 0.2835 0.456</cell></row><row><cell>KobeU (Eguchi)</cell><cell>KobePrMIR01</cell><cell cols="2">0.246 0.3011 0.2744 0.440</cell></row><row><cell>NTU (Chen)</cell><cell>NTUAutoOp</cell><cell cols="2">0.2282 0.2614 0.2577 0.464</cell></row><row><cell>KobeU (Seki)</cell><cell>Ku</cell><cell cols="2">0.1689 0.2417 0.2190 0.254</cell></row><row><cell>RGU (Mukras)</cell><cell>rgu0</cell><cell cols="2">0.1686 0.2266 0.2163 0.288</cell></row><row><cell>UBuffalo (Ruiz)</cell><cell>UB2</cell><cell cols="2">0.1013 0.1297 0.1238 0.144</cell></row><row><cell>Wuhan (Lu)</cell><cell>NOOPWHU1</cell><cell cols="2">0.0011 0.0071 0.0072 0.008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,53.76,531.20,502.02,181.74"><head>Table 9 : Occurrences of presumed splog documents in the opin- ion finding task pool.</head><label>9</label><figDesc></figDesc><table coords="4,82.68,531.20,268.14,141.11"><row><cell></cell><cell></cell><cell></cell><cell>800</cell></row><row><cell></cell><cell></cell><cell></cell><cell>700</cell></row><row><cell>Relevance Scale Not Relevant Adhoc-Relevant Negative Opinionated Mixed Opinionated Positive Opinionated (Total)</cell><cell>Nbr. of Splog Documents 6357 361 78 98 192 7086</cell><cell>Number of Spam Documents</cell><cell>200 300 400 500 600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell></row></table><note coords="4,438.86,675.38,13.01,4.92;4,316.80,694.44,238.98,8.07;4,316.80,704.88,36.44,8.07"><p><p>Topic</p>Figure 4: Distribution of number of spam documents retrieved per topic.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,53.76,286.80,502.19,18.51"><head>Table 4 : Opinion finding results: best run from each of the 20 groups, regardless of the used topic length. The best in each column is highlighted.</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,316.80,324.72,239.27,72.15"><head>Table 11 : Best, median and worst R-accuracy measures for the 38 submitted runs to the polarity subtask.</head><label>11</label><figDesc></figDesc><table coords="5,405.00,324.72,62.68,39.87"><row><cell></cell><cell>R-Acc</cell></row><row><cell>Best</cell><cell>0.2959</cell></row><row><cell cols="2">Median 0.1227</cell></row><row><cell>Worst</cell><cell>0.0004</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,316.80,690.24,239.04,29.07"><head>Table 5 : Opinion finding results: automatic title-only baseline runs from each of the group with the best MAP, sorted by MAP. In these runs, all opinion finding features are switched off. The best in each column is highlighted. Note that some groups did not submit the compulsory automatic title-only baseline run.</head><label>5</label><figDesc>Table 11 provides the average best, median and worst R-Acc measures for each topic, across all submitted 38 runs.</figDesc><table coords="6,141.12,55.44,327.56,186.27"><row><cell>Group</cell><cell>Run</cell><cell cols="2">MAP R-prec b-Bref P@10</cell></row><row><cell>UAmsterdam (deRijke)</cell><cell>uams07topic</cell><cell cols="2">0.3453 0.3872 0.3953 0.562</cell></row><row><cell>FudanU (Wu)</cell><cell cols="3">FDUNOpRSVMT 0.3178 0.3447 0.3498 0.452</cell></row><row><cell>CAS (Liu)</cell><cell>Relevant</cell><cell cols="2">0.3041 0.3600 0.3779 0.446</cell></row><row><cell>DalianU (Yang)</cell><cell>DUTRun1</cell><cell cols="2">0.2890 0.3368 0.3249 0.502</cell></row><row><cell>UGlasgow (Ounis)</cell><cell>uogBOPFProx</cell><cell cols="2">0.2817 0.3366 0.3098 0.454</cell></row><row><cell>UNeuchatel (Savoy)</cell><cell>UniNEblog1</cell><cell cols="2">0.277 0.3353 0.3074 0.492</cell></row><row><cell>FIU (Netlab team)</cell><cell>FIUbPL2</cell><cell cols="2">0.2728 0.3204 0.2925 0.454</cell></row><row><cell>Zhejiangu (Qiu)</cell><cell>EAGLE1</cell><cell cols="2">0.2561 0.3159 0.2867 0.428</cell></row><row><cell cols="2">UArkansas Littlerock (Bayrak) UALR07Base</cell><cell cols="2">0.2554 0.3145 0.2867 0.440</cell></row><row><cell>IndianaU (Yang)</cell><cell>oqsnr1Base</cell><cell cols="2">0.2537 0.323 0.3091 0.446</cell></row><row><cell>CAS (NLPR-IACAS)</cell><cell>NLPRPTONLY</cell><cell cols="2">0.2506 0.3166 0.2917 0.452</cell></row><row><cell>UWaterloo (Olga)</cell><cell>UWbasePhrase</cell><cell cols="2">0.2486 0.3087 0.2861 0.432</cell></row><row><cell>BUPT (Weiran)</cell><cell>prisOpnBasic</cell><cell cols="2">0.2466 0.3018 0.2835 0.456</cell></row><row><cell>NTU (Chen)</cell><cell>NTUAuto</cell><cell cols="2">0.2254 0.2795 0.2588 0.412</cell></row><row><cell>KobeU (Seki)</cell><cell>Ku</cell><cell>0.1689 0.2417 0.219</cell><cell>0.254</cell></row><row><cell>RGU (Mukras)</cell><cell>rgu0</cell><cell cols="2">0.1686 0.2266 0.2163 0.288</cell></row><row><cell>Wuhan (Lu)</cell><cell>NOOPWHU1</cell><cell cols="2">0.0011 0.0071 0.0072 0.008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,454.20,711.23,101.56,8.07"><head></head><label></label><figDesc>Using either of two opinion</figDesc><table coords="7,61.92,55.43,488.16,186.95"><row><cell>Group</cell><cell>Best Baseline</cell><cell cols="4">Baseline MAP Best Non-baseline Non Baseline MAP % Increase</cell></row><row><cell>UGlasgow (Ounis)</cell><cell>uogBOPFProx</cell><cell>0.2817</cell><cell>uogBOPFProxW</cell><cell>0.3264</cell><cell>15.87%</cell></row><row><cell>IndianaU (Yang)</cell><cell>oqsnr1Base</cell><cell>0.2537</cell><cell>oqsnr2opt</cell><cell>0.2894</cell><cell>14.07%</cell></row><row><cell cols="2">UArkansas Littlerock (Bayrak) UALR07Base</cell><cell>0.2554</cell><cell>UALR07BlogIU</cell><cell>0.2911</cell><cell>13.98%</cell></row><row><cell>DalianU (Yang)</cell><cell>DUTRun1</cell><cell>0.289</cell><cell>DUTRun2</cell><cell>0.319</cell><cell>10.38%</cell></row><row><cell>UWaterloo (Olga)</cell><cell>UWbasePhrase</cell><cell>0.2486</cell><cell>UWopinion3</cell><cell>0.2631</cell><cell>5.83%</cell></row><row><cell>CAS (NLPR-IACAS)</cell><cell>NLPRPTONLY</cell><cell>0.2506</cell><cell>NLPRPST</cell><cell>0.2542</cell><cell>1.44%</cell></row><row><cell>NTU (Chen)</cell><cell>NTUAuto</cell><cell>0.2254</cell><cell>NTUAutoOp</cell><cell>0.2282</cell><cell>1.24%</cell></row><row><cell>FudanU (Wu)</cell><cell>FDUNOpRSVMT</cell><cell>0.3178</cell><cell>FDUTisdOpSVM</cell><cell>0.3179</cell><cell>0.03%</cell></row><row><cell>FIU (Netlab team)</cell><cell>FIUbPL2</cell><cell>0.2728</cell><cell>FIUdPL2</cell><cell>0.2728</cell><cell>0.00%</cell></row><row><cell>Wuhan (Lu)</cell><cell>NOOPWHU1</cell><cell>0.0011</cell><cell>OTWHU101</cell><cell>0.0011</cell><cell>0.00%</cell></row><row><cell>KobeU (Seki)</cell><cell>Ku</cell><cell>0.1689</cell><cell>KuKnn</cell><cell>0.1657</cell><cell>-1.89%</cell></row><row><cell>Zhejiangu (Qiu)</cell><cell>EAGLE1</cell><cell>0.2561</cell><cell>EAGLE2</cell><cell>0.2493</cell><cell>-2.66%</cell></row><row><cell>CAS (Liu)</cell><cell>Relevant</cell><cell>0.3041</cell><cell>DrapOpi</cell><cell>0.1659</cell><cell>-45.45%</cell></row><row><cell>RGU (Mukras)</cell><cell>rgu0</cell><cell>0.1686</cell><cell>rgu2</cell><cell>0.0892</cell><cell>-47.09%</cell></row><row><cell>UAmsterdam (deRijke)</cell><cell>uams07topic</cell><cell>0.3453</cell><cell>uams07mmqop</cell><cell>0.1459</cell><cell>-57.75%</cell></row><row><cell>BUPT (Weiran)</cell><cell>prisOpnBasic</cell><cell>0.2466</cell><cell>prisOpnC2</cell><cell>0.0821</cell><cell>-66.71%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,53.76,256.20,502.39,28.95"><head>Table 6 : What worked. Improvements over the baselines, for automatic title-only runs. The best in each column is highlighted. Some groups did not submit title-only baseline runs (e.g. UIC group), and some did not submit any run with specific opinion finding features</head><label>6</label><figDesc></figDesc><table /><note coords="7,86.76,277.08,68.72,8.07"><p>(e.g. UNeuchatel).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="8,53.76,66.24,502.01,239.07"><head>Table 7 : Topic-relevance results: run from each of the 20 groups with the best topic-relevance MAP, sorted by MAP. The best in each column is highlighted.</head><label>7</label><figDesc></figDesc><table coords="8,126.00,66.24,356.68,206.79"><row><cell>Zhang)</cell><cell>uic1c</cell><cell>T</cell><cell cols="2">0.4819 0.5181 0.5484 0.868</cell></row><row><cell>UAmsterdam (deRijke)</cell><cell>uams07topic</cell><cell>T</cell><cell cols="2">0.4741 0.523 0.5702 0.762</cell></row><row><cell>FudanU (Wu)</cell><cell>FDUTisdOpSVM</cell><cell>T</cell><cell cols="2">0.4714 0.4889 0.5432 0.654</cell></row><row><cell>IndianaU (Yang)</cell><cell>oqlr2fopt</cell><cell cols="3">TDN 0.4347 0.4653 0.5022 0.822</cell></row><row><cell>CAS (Liu)</cell><cell>Relevant</cell><cell>T</cell><cell cols="2">0.4302 0.4949 0.5658 0.662</cell></row><row><cell>DalianU (Yang)</cell><cell>DUTRun2</cell><cell>T</cell><cell cols="2">0.4247 0.4750 0.5164 0.784</cell></row><row><cell>UGlasgow (Ounis)</cell><cell>uogBOPFProxW</cell><cell>T</cell><cell cols="2">0.4160 0.4436 0.4618 0.720</cell></row><row><cell>UNeuchatel (Savoy)</cell><cell>UniNEblog3</cell><cell>TD</cell><cell cols="2">0.4034 0.4296 0.4553 0.730</cell></row><row><cell>FIU (Netlab team)</cell><cell>FIUDDPH</cell><cell>TD</cell><cell cols="2">0.3907 0.4230 0.4692 0.714</cell></row><row><cell cols="2">UArkansas Littlerock (Bayrak) UALR07BlogIU</cell><cell>T</cell><cell cols="2">0.3612 0.3975 0.4122 0.734</cell></row><row><cell>UWaterloo (Olga)</cell><cell>UWopinion3</cell><cell>T</cell><cell>0.3490 0.4040 0.4020</cell><cell>0.68</cell></row><row><cell>Zhejiangu (Qiu)</cell><cell>EAGLE2</cell><cell>T</cell><cell cols="2">0.3409 0.3809 0.3992 0.644</cell></row><row><cell>CAS (NLPR-IACAS)</cell><cell>NLPRTD</cell><cell>TD</cell><cell cols="2">0.3373 0.3804 0.3894 0.586</cell></row><row><cell>KobeU (Eguchi)</cell><cell>KobePrMIR01</cell><cell>T</cell><cell cols="2">0.3292 0.3655 0.3852 0.606</cell></row><row><cell>BUPT (Weiran)</cell><cell>prisOpnBasic</cell><cell>T</cell><cell cols="2">0.3267 0.3633 0.3735 0.684</cell></row><row><cell>NTU (Chen)</cell><cell>NTUManual</cell><cell>T</cell><cell cols="2">0.3051 0.3309 0.3631 0.582</cell></row><row><cell>RGU (Mukras)</cell><cell>rgu0</cell><cell>T</cell><cell cols="2">0.2798 0.3533 0.3651 0.560</cell></row><row><cell>KobeU (Seki)</cell><cell>Ku</cell><cell>T</cell><cell cols="2">0.2590 0.3357 0.3503 0.476</cell></row><row><cell>UBuffalo (Ruiz)</cell><cell>UB1</cell><cell cols="3">TDN 0.2421 0.2818 0.2956 0.484</cell></row><row><cell>Wuhan (Lu)</cell><cell>NOOPWHU1</cell><cell>T</cell><cell>0.0016 0.0111 0.0100</cell><cell>0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="8,316.80,690.24,239.30,29.07"><head>Table 10 : Spam measures for runs from Table 3, in the order given. Spam@10 is the mean number of spam posts in the top 10 ranked documents for each topic, Spam@all is the mean number of spam posts retrieved for each topic. BadMAP is the Mean Average Precision when the spam documents are treated as the relevant set. This shows when spam documents are retrieved at high ranks. For all measures, lower means the system was better at not retrieving spam documents. The best in each column is highlighted.</head><label>10</label><figDesc>The blog distillation (feed search) task is a new task in the TREC 2007 Blog track, which was the result of the discussion that followed the introduction of the open task in TREC 2006. The task</figDesc><table coords="9,124.56,55.63,360.53,406.88"><row><cell>Group</cell><cell>Run</cell><cell cols="3">Spam@10 Spam@all BadMAP *10 -5</cell></row><row><cell>UIC (Zhang)</cell><cell>uic1c</cell><cell>0.56</cell><cell>33.86</cell><cell>0.0</cell></row><row><cell>UAmsterdam (deRijke)</cell><cell>uams07topic</cell><cell>0.92</cell><cell>104.14</cell><cell>2.8</cell></row><row><cell>UGlasgow (Ounis)</cell><cell>uogBOPFProxW</cell><cell>1.24</cell><cell>126.32</cell><cell>10.8</cell></row><row><cell>DalianU (Yang)</cell><cell>DUTRun2</cell><cell>0.74</cell><cell>55.66</cell><cell>3.0</cell></row><row><cell>FudanU (Wu)</cell><cell>FDUTOSVMSem</cell><cell>0.98</cell><cell>59.50</cell><cell>2.2</cell></row><row><cell>CAS (Liu)</cell><cell>Relevant</cell><cell>1.34</cell><cell>75.66</cell><cell>2.2</cell></row><row><cell cols="2">UArkansas Little Rock (Bayrak) UALR07BlogIU</cell><cell>0.88</cell><cell>121.74</cell><cell>10.2</cell></row><row><cell>IndianaU (Yang)</cell><cell>oqsnr2opt</cell><cell>0.98</cell><cell>181.20</cell><cell>13.0</cell></row><row><cell>UNeuchatel (Savoy)</cell><cell>UniNEblog1</cell><cell>1.18</cell><cell>139.18</cell><cell>12.2</cell></row><row><cell>FIU (Netlab team)</cell><cell>FIUbPL2</cell><cell>1.42</cell><cell>131.98</cell><cell>11.8</cell></row><row><cell>UWaterloo (Olga)</cell><cell>UWopinion3</cell><cell>1.16</cell><cell>75.88</cell><cell>7.2</cell></row><row><cell>Zhejiangu (Qiu)</cell><cell>EAGLE1</cell><cell>1.24</cell><cell>121.74</cell><cell>9.6</cell></row><row><cell>CAS (NLPR-IACAS)</cell><cell>NLPRPST</cell><cell>1.22</cell><cell>124.68</cell><cell>8.4</cell></row><row><cell>BUPT (Weiran)</cell><cell>prisOpnBasic</cell><cell>1.32</cell><cell>80.22</cell><cell>7.2</cell></row><row><cell>KobeU (Eguchi)</cell><cell>KobePrMIR01</cell><cell>1.54</cell><cell>157.82</cell><cell>13.4</cell></row><row><cell>NTU (Chen)</cell><cell>NTUAutoOp</cell><cell>0.94</cell><cell>161.70</cell><cell>15.0</cell></row><row><cell>KobeU (Seki)</cell><cell>Ku</cell><cell>2.12</cell><cell>153.42</cell><cell>10.6</cell></row><row><cell>RGU (Mukras)</cell><cell>rgu0</cell><cell>1.30</cell><cell>86.30</cell><cell>5.6</cell></row><row><cell>UBuffalo (Ruiz)</cell><cell>UB2</cell><cell>4.92</cell><cell>86.44</cell><cell>4.2</cell></row><row><cell>Wuhan (Lu)</cell><cell>NOOPWHU1</cell><cell>1.56</cell><cell>101.96</cell><cell>4.8</cell></row><row><cell>Group</cell><cell>Run</cell><cell cols="3">R-Acc A@10 A@1000</cell></row><row><cell>UIC (Zhang)</cell><cell>uic75cpnm</cell><cell cols="2">0.2295 0.3700</cell><cell>0.0493</cell></row><row><cell cols="2">UAmsterdam (de Rijke) uams07ipolt</cell><cell cols="2">0.1827 0.2640</cell><cell>0.0418</cell></row><row><cell>IndianaU (Yang)</cell><cell>oqsnr2optP</cell><cell cols="2">0.1799 0.2800</cell><cell>0.0401</cell></row><row><cell>DalianU (Yang)</cell><cell>DUTRun2P</cell><cell cols="2">0.1721 0.3080</cell><cell>0.0406</cell></row><row><cell>Zhejiangu (Qiu)</cell><cell>EAGLE2P</cell><cell cols="2">0.1510 0.2380</cell><cell>0.0427</cell></row><row><cell>UGlasgow (Ounis)</cell><cell>uogBOPFPol</cell><cell cols="2">0.1460 0.2020</cell><cell>0.0397</cell></row><row><cell>NTU (Chen)</cell><cell>NTUAutoOpP</cell><cell cols="2">0.0967 0.1860</cell><cell>0.0296</cell></row><row><cell>CAS (Liu)</cell><cell>DrapStmSub</cell><cell cols="2">0.0818 0.1060</cell><cell>0.0243</cell></row><row><cell>BUPT (Weiran)</cell><cell>pUB21</cell><cell cols="2">0.0418 0.0340</cell><cell>0.0148</cell></row><row><cell>Wuhan (Lu)</cell><cell cols="3">OTPSWHU102 0.0032 0.0040</cell><cell>0.0010</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="9,53.76,476.28,502.25,29.07"><head>Table 12 : Best polarity run for each group, in terms of R-accuracy. Each polarity runs corresponds to an automatic title-only opinion finding run. The best in each column is highlighted. Not all groups submitted polarity runs corresponding to automatic title-only opinion finding runs.</head><label>12</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="10,53.76,66.24,502.20,368.01"><head>Table 13 : Best polarity run for each group, in terms of R-accuracy, regardless of the topic length. The best in each column is highlighted. Not all groups submitted polarity runs.</head><label>13</label><figDesc></figDesc><table coords="10,53.76,66.24,399.76,368.01"><row><cell>Zhang)</cell><cell>uic75cpnm</cell><cell>T</cell><cell>0.2295 0.3700</cell><cell>0.0493</cell></row><row><cell>IndianaU (Yang)</cell><cell>oqlr2f2optP</cell><cell cols="2">TDN 0.1941 0.3080</cell><cell>0.0438</cell></row><row><cell cols="2">UAmsterdam (de Rijke) uams07ipolt</cell><cell>T</cell><cell>0.1827 0.2640</cell><cell>0.0418</cell></row><row><cell>DalianU (Yang)</cell><cell>DUTRun2P</cell><cell>T</cell><cell>0.1721 0.3080</cell><cell>0.0406</cell></row><row><cell>Zhejiangu (Qiu)</cell><cell>EAGLE2P</cell><cell>T</cell><cell>0.1510 0.2380</cell><cell>0.0427</cell></row><row><cell>UGlasgow (Ounis)</cell><cell>uogBOPFPol</cell><cell>T</cell><cell>0.1460 0.2020</cell><cell>0.0397</cell></row><row><cell>NTU (Chen)</cell><cell>NTUManualOpP</cell><cell>T</cell><cell>0.1161 0.2300</cell><cell>0.0348</cell></row><row><cell>CAS (Liu)</cell><cell>DrapStmSub</cell><cell>T</cell><cell>0.0818 0.1060</cell><cell>0.0243</cell></row><row><cell>BUPT (Weiran)</cell><cell>prisPolC2</cell><cell>T</cell><cell>0.0726 0.2020</cell><cell>0.0124</cell></row><row><cell>UBuffalo (Ruiz)</cell><cell>pUB11</cell><cell cols="2">TDN 0.0671 0.1000</cell><cell>0.0195</cell></row><row><cell>Wuhan (Lu)</cell><cell>OTPSWHU102</cell><cell>T</cell><cell>0.0032 0.0040</cell><cell>0.0010</cell></row><row><cell>&lt;top&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;num&gt; Number: 994 &lt;/num&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;title&gt; formula f1 &lt;/title&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;desc&gt; Description:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Blogs with interest in the formula</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>one (f1) motor racing, perhaps with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>driver news, team news, or event</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>news.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;/desc&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;narr&gt; Narrative:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relevant blogs will contain news</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and analysis from the Formula f1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>motor racing circuit. Blogs with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>documents not in English are not</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>relevant.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;/narr&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;/top&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="11,53.76,55.44,239.09,255.63"><head>Table 14 : Best, median and worst MAP measures for the 32 submitted runs to the blog distillation task.</head><label>14</label><figDesc></figDesc><table coords="11,141.96,55.44,62.68,39.87"><row><cell></cell><cell>MAP</cell></row><row><cell>Best</cell><cell>0.4671</cell></row><row><cell cols="2">Median 0.2035</cell></row><row><cell>Worst</cell><cell>0.0006</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="11,97.20,303.00,190.08,60.99"><head>Distribution of number of relevant feeds per topic</head><label></label><figDesc></figDesc><table coords="11,112.44,323.76,121.72,40.23"><row><cell cols="2">Relevance Scale Nbr. of Splogs</cell></row><row><cell>Not Relevant</cell><cell>2935</cell></row><row><cell>Relevant</cell><cell>255</cell></row><row><cell>(Total)</cell><cell>3190</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="11,53.76,377.76,239.26,18.51"><head>Table 17 : Occurrences of presumed splogs in the blog distilla- tion task pool.</head><label>17</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="12,53.76,55.44,502.17,239.43"><head>Table 15 : Blog distillation results: the automatic title-only run from each of 8 groups with the best MAP, sorted by MAP. Note that 1 group (UBerlin) did not submit a title-only run. The best in each column is highlighted.</head><label>15</label><figDesc></figDesc><table coords="12,340.44,55.44,128.48,8.07"><row><cell>R-prec b-Bref P@10</cell><cell>MRR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="12,53.76,308.64,502.19,18.51"><head>Table 16 : Blog distillation results: one run from each of 9 groups with the best MAP, sorted by MAP. The best in each column is highlighted.</head><label>16</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="13,53.76,55.63,413.09,124.52"><head>Table 18 : Spam measures for runs from Table</head><label>18</label><figDesc></figDesc><table coords="13,455.88,55.63,10.97,6.37"><row><cell>-5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The description of systems runs are based on paragraphs contributed by the participating groups. Thanks are also due to participants in the blog distillation task for creating and assessing topics. In particular all groups who submitted runs to the blog distillation task have participated in the relevance judging. Finally, we are grateful to <rs type="person">Arjen de Vries</rs> for providing his assessment system, and to <rs type="person">David Hannah</rs> for adapting it to the blog distillation task.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="13,58.25,614.87,96.95,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,68.76,626.52,213.87,8.07;13,68.76,636.96,223.41,8.07;13,68.76,647.40,205.82,8.07;13,68.76,657.84,104.86,8.07;13,68.76,668.88,178.06,7.05;13,68.76,679.32,194.38,7.05" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<ptr target="http://www.dcs.gla.ac.uk/∼craigm/publications/macdonald06creating.pdf" />
		<title level="m" coord="13,170.49,626.52,112.14,8.07;13,68.76,636.96,186.88,8.07">The TREC Blog06 Collection : Creating and Analysing a Blog Test Collection DCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="13,68.76,690.24,219.20,8.07;13,68.76,700.68,198.52,8.07;13,68.76,711.24,140.52,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,68.76,700.68,129.35,8.07">Overview of TREC-2006 Blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,213.60,700.68,53.68,8.07;13,68.76,711.24,40.59,8.07">Proceedings of TREC-2006</title>
		<meeting>TREC-2006<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,233.16,218.32,8.07;13,331.80,243.60,189.94,8.07;13,331.80,254.04,221.41,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,370.08,243.60,151.67,8.07;13,331.80,254.04,109.45,8.07">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,457.08,254.04,73.83,8.07">Proceedings of OSIR</title>
		<meeting>OSIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,264.60,109.12,8.07" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Workshop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Seattle, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,276.00,195.94,8.07;13,331.80,286.44,218.33,8.07;13,331.80,296.88,93.12,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,483.22,276.00,44.53,8.07;13,331.80,286.44,101.61,8.07">Overview of TREC-2006 Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,449.04,286.44,96.58,8.07">Proceedings of TREC-2006</title>
		<meeting>TREC-2006<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
