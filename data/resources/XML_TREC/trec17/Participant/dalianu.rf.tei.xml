<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.66,116.71,197.94,12.24">DUTIR at TREC 2008 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,133.38,140.27,53.60,9.19"><forename type="first">Jianmei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.29,140.27,33.79,9.19"><forename type="first">Wei</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.70,140.27,57.37,9.19"><forename type="first">Fengming</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.11,140.27,57.40,9.19"><forename type="first">Fuyang</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.80,140.58,34.38,8.77"><forename type="first">Rui</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.17,140.58,47.71,8.77"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.66,116.71,197.94,12.24">DUTIR at TREC 2008 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76F479A59CAA822FC02B202628EC578C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For opinion finding task our method of the combination of 5 Windows method and Pseudo Relevance Feedback behaves well, achieving an improvement of over 20% on the baseline adhoc results. For the polarity task we develop two different methods. One is a classification method, and the other uses queries to retrieve positive and negative documents respectively. In Blog Distillation task, Pseudo Relevance Feedback method helps improve the result a little, however, since its dependence on the top 10 retrieval result, the method still need to be improved in order to get better result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>IR Lab (Information Retrieval Laboratory) of DUT (Dalian University of Technology) participated in all of the tasks of the TREC 2008 Blog Track, including Baseline adhoc(blog post)retrieval task, Opinion Retrieval Task, Polarity Task and Feed Distillation Task. The paper gives a detailed description of the participation. Modules designed for different tasks are described separately below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASELINE ADHOC TASK</head><p>The baseline adhoc retrieval task requires locating blog posts that contain relevant information about a given topic target, however all opinion-finding retrieval techniques should be turned off. Since the corpus was cleared last year, it was indexed by indri directly. Then we constructed queries in two ways. The first way is just using the title field in the topics. The second way is using some query expansion when the result number was not attained to the requirement. We expanded queries by Wiki, indri query syntaxes. In a word, the methods we used in this part were very simple and we haven't used any opinion-finding retrieval techniques. The results are as follow and can be used as baselines for the next parts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OPINION RETRIEVAL TASK</head><p>In Opinion Retrieval Task, the main retrieval unit is permalink. Our method is composed of three steps: Data Preprocessing, Indexing, and Query Construction. The method in <ref type="bibr" coords="2,434.24,155.31,12.45,9.61" target="#b0">[1]</ref> was used for data preprocessing. In this chapter, two retrieval steps: topic retrieval and opinionative document ranking are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TOPIC RETRIEVAL</head><p>Our topic retrieval method mainly includes three phases: concept identification, query expansion, and pseudo-relevance feedback based retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Concept Identification</head><p>In topic retrieval the concepts were identified first. The concept in a query is defined as the single words or phrases that denote an entity. A query may contain multiple concepts. For example, a query "UN Commission on Human Rights" contains three concepts "UN Commission", "Human Rights" and the query "UN Commission on Human Rights" itself. The synonyms of the concepts were also identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Query Expansion</head><p>Query expansion aims to add a certain number of query-relevant terms to the original query in order to improve retrieval effectiveness. Be different from the general query expansion, here the recapitulative concepts were more focused on. For example, the concept "Frank Gehry architecture" will be expanded as "Walt Disney Concert Hall", "Weisman Art Museum" and other architectures which are designed by Frank Gehry. The online dictionary Wikipedia <ref type="bibr" coords="2,355.06,458.87,11.93,9.19" target="#b1">[2]</ref> was utilized to accomplish the expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Pseudo-relevance Feedback Based Retrieval</head><p>Lemur <ref type="bibr" coords="2,117.80,519.59,11.92,9.19" target="#b2">[3]</ref> was used to carry the pseudo-relevance feedback based retrieval. There are seven retrieval models in Lemur and Indri model was finally selected to retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OPINIONATIVE DOCUMENT RANKING</head><p>The topic relevant documents have been identified. But actually related to the queries and contain opinions are still need to be found. This is done by using the text window method. First the subjective sentences were found. If a sentence contains at least one sentimental word, it is identified as a subjective sentence. When there is a subjective sentence, two sentences were got to prior to it and two sentences following it to form a 5-sentence window. And then the search was done with the original query terms and the expanded query terms within this window. If certain restrictions are met, this subjective sentence is labeled as a relevant opinionative sentence (ROS). Otherwise it is discarded. A document having at least one ROS is considered to be a relevant opinionative document (ROD) of the query topic. Then the query-document opinion similarity, tagged as Sim op (d, Q), can be calculated by, <ref type="bibr" coords="3,246.96,115.38,29.03,10.67">( , )</ref> </p><formula xml:id="formula_0" coords="3,220.26,111.56,287.65,16.47">log(| ( , ) | 1) op Sim d Q ROS d Q = + (1)</formula><p>where Q denotes a query; d is a document in the ROD set of Q; |ROS(d, Q)| denote the size of ROS set in d for the query Q. The final ranking score (Sim) can be calculated by the following formula:</p><formula xml:id="formula_1" coords="3,231.48,187.10,276.43,17.00">* 1 * ir op Sim = k Sim k Sim + - ( ) (2)</formula><p>In the formula above, Sim ir is the query-document opinion similarity obtained by topic retrieval, Sim op is the query-document opinion similarity defined by formula 1.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">POLARITY TASK</head><p>Polarity classification task last year is developed in this year's Blog Track. The polarity task asks a system to retrieve both the subjective and the negative documents about every topic, and rank the results like in a user, with the mixed opinions not being included. For this task we applied two different approaches. One is a classification approach using SVM, the other is a retrieval method. The queries used in the retrieval method are composed for positive and negative opinions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">POLARITY IDENTIFICATION AND RETRIEVAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Classification Method</head><p>The classification method we used in the polarity task is based on the approach we used in the polarity subtask of Blog Track 2007 <ref type="bibr" coords="4,249.92,399.51,13.72,9.61" target="#b0">[1]</ref>. We trained two SVM classifiers using the provided results for the 50 topics in Blog Track 2006 as training data. And then the classifiers were used to classify the processed documents. First, polarity task deals with the relevant documents judged by the opinion retrieval task; these documents were extracted and processed. Then we suppose that not all sentences in the documents are opinioned and relevant. So only the sentences with the extended topic words, which were obtained in the opinion retrieval task, were extracted. We used a two-phase classifier strategy. The first classifier classifies the documents into two categories -relevant-not-opinioned (tagged as <ref type="bibr" coords="4,139.68,520.88,13.06,9.61" target="#b0">[1]</ref>) and relevant-opinioned (tagged as <ref type="bibr" coords="4,322.44,520.88,11.97,9.61" target="#b3">[5]</ref>). The second classifier classifies the documents in category <ref type="bibr" coords="4,189.54,536.06,13.45,9.61" target="#b3">[5]</ref> into three categories, which are relevant-positive-opinions (tagged as [4]), relevant-mixed-opinions (tagged as <ref type="bibr" coords="4,272.70,551.24,12.01,9.61" target="#b2">[3]</ref>), and relevant-negative-opinions (tagged as <ref type="bibr" coords="4,487.80,551.24,12.04,9.61" target="#b1">[2]</ref>). Documents in categories [4] and <ref type="bibr" coords="4,230.34,566.42,13.51,9.61" target="#b1">[2]</ref> are then the required documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Retrieval Method</head><p>The retrieval method aims to compose two queries to retrieve all the positive documents and all the negative documents respectively. And then the two retrieved document sets will be compared. Documents in both sets will be judged by a judging processor, which decides weather the document should be tagged as positive ([4]), negative ( <ref type="bibr" coords="4,341.25,657.45,11.77,9.61" target="#b1">[2]</ref>), or mixed ( <ref type="bibr" coords="4,414.64,657.45,11.77,9.61" target="#b2">[3]</ref>). Figure <ref type="figure" coords="4,471.75,657.45,5.34,9.61">1</ref> below shows the structure of the retrieval method system Figure <ref type="figure" coords="5,212.99,140.13,5.34,9.61">1</ref> The structure of the retrieval method system.</p><p>The queries were composed with sentimental lexicons and their weights. The sentimental lexicons used are the same lexicon we used in Blog Track 2007.Their weights are computed with reference from SentiWordNet <ref type="bibr" coords="5,180.48,413.25,11.39,9.61">[4]</ref>. The positive documents and negative documents are retrieved with different queries composed respectively by positive sentimental words and negative sentimental words. Indices of the query were built only with documents retrieved in opinion retrieval task. The judging processor is based on the rules below:</p><p>, ( </p><formula xml:id="formula_3" coords="5,266.94,474.90,240.97,50.56">- &gt; ⎧ ⎪ = ⎨ ⎪ - &gt; ⎩ (3)</formula><p>In which P d is rank of document d in the positive results, N d is the rank of document d in the negative results, and T n is total number of results retrieved. DUTIR08Run2P is the result of the classification method; DUTIR08Run5P and DUTIR08Run6P are the results of the retrieval method. The results suggest that our retrieval method is not as effective as the classification method, and with the same method results based on DUTIR08Run4P is relatively superior to results based on DUTIR08Run2P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">BLOG DISTILLATION TASK</head><p>In this task, feed files are preferred as retrieval units due to the need to submit feed no. Like the opinion retrieval task, our method is divided into three steps: Data Preprocessing, Indexing, and Query Construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DATA PREPROCESSING</head><p>Some manual work such as analyzing the possible formats of the feed files and recording the possible tags that contain desired information according different formats are necessary. Finally three types of feed formats were found according their different displaying styles: RSS, RDF, and ATOM. Moreover, some types of feed could be divided into smaller units such as Item or Entry. Whether for the whole feed or the smaller units in it, desired contents in them are often in the fixed tags such as &lt;description&gt;, &lt;content&gt;, &lt;summary&gt; and so on. So feed files with non English ones removed were later parsed according these tags with htmlparser <ref type="bibr" coords="6,444.67,476.36,12.45,9.61" target="#b4">[6]</ref> iteratively. When using htmlparser, these tags need to be defined and registered in order to identify these tags and extract contents among them. The size of original feed is 38.8GB and the size of feed after non English ones and undesired contents are removed becomes 17.5GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">INDEXING</head><p>Preprocessed data are then indexed with Indri Search Engine <ref type="bibr" coords="6,375.36,567.38,11.38,9.61" target="#b2">[3]</ref>. Feeds are different from permalinks, there are often redundant feeds among different files and the contents of them are varying or not. So when running query on the index, it needs to remove the redundant feeds and make a little adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EXPANDING THE QUERIES</head><p>In this step, we use description and narrative field to expand query. As run1 just uses the field of title, there are irrelevant answers in the results returned by the query, so we employ the other field to get rid of irrelevant answers. In run2, we use the title field and employ the pseudo feed back method <ref type="bibr" coords="6,122.31,703.94,12.45,9.61" target="#b3">[5]</ref> to get the answer. In run3, we build another query by using title, description, narrative field and so on. In run4, we use the query of run3 and pseudo feed back to get answers. This is an automatic run with all fields and is got by merging the pseudo feedback results from Permalinks and Feeds respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunID</head><p>Judging order Description MAP R-Precision b-Bref P@10 DUTIR08D Run1 DUTDRun4 is relatively superior to others. From Table <ref type="table" coords="7,339.09,370.65,4.03,9.61">7</ref>, we can see that pseudo feedback is useful to get precise answers. The MAP, R-Precision, b-Bref and P@10 are the highest of the four runs. From the runs3, we can see that the P@10 is higher than that of Runs1 and Runs2, and the pseudo feedback from runs3 is the highest. So we can say that p@10 is important for pseudo feedback. If we can use precise pages for pseudo feedback, we can get much higher results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,87.42,299.43,403.78,136.09"><head>Table 2</head><label>2</label><figDesc>is the runs and results submitted in Opinion Retrieval Task.</figDesc><table coords="3,104.16,334.64,387.04,100.88"><row><cell>RunID</cell><cell cols="2">Judging order Description</cell><cell>MAP</cell><cell>R-Precision P@10</cell></row><row><cell cols="2">DUTIR08Run1 3</cell><cell>topicWords + opinionWords</cell><cell cols="2">0.2866 0.3470</cell><cell>0.5193</cell></row><row><cell cols="2">DUTIR08Run2 4</cell><cell>topicWords + 5Windows</cell><cell cols="2">0.3291 0.3768</cell><cell>0.6007</cell></row><row><cell cols="2">DUTIR08Run3 2</cell><cell>Pseudo Relevance Feedback</cell><cell cols="2">0.2964 0.3493</cell><cell>0.5207</cell></row><row><cell cols="2">DUTIR08Run4 1</cell><cell cols="3">Pseudo Relevance Feedback+5Windows 0.3394 0.3818</cell><cell>0.6173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,104.16,446.49,387.04,136.03"><head>Table 2</head><label>2</label><figDesc>The results of Opinion Retrieval Runs Ranked by Opinion</figDesc><table coords="3,104.16,481.70,387.04,100.82"><row><cell>RunID</cell><cell cols="2">Judging order Description</cell><cell>MAP</cell><cell>R-Precision P@10</cell></row><row><cell cols="2">DUTIR08Run1 3</cell><cell>topicWords + opinionWords</cell><cell cols="2">0.3736 0.4369</cell><cell>0.6980</cell></row><row><cell cols="2">DUTIR08Run2 4</cell><cell>topicWords + 5Windows</cell><cell cols="2">0.4161 0.4604</cell><cell>0.7707</cell></row><row><cell cols="2">DUTIR08Run3 2</cell><cell>Pseudo Relevance Feedback</cell><cell cols="2">0.3925 0.4421</cell><cell>0.7033</cell></row><row><cell cols="2">DUTIR08Run4 1</cell><cell cols="3">Pseudo Relevance Feedback+5Windows 0.4239 0.4674</cell><cell>0.7773</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,87.42,593.55,420.60,100.63"><head>Table 3</head><label>3</label><figDesc>The results of Opinion Retrieval Runs Ranked by Topicrel According to Table2 and 3, 5Windows method works better than the method with opinion words method in both the opinion and topic relevance result. On the basis of 5 Windows method, the Pseudo Relevance Feedback contributes to a little improvement on both the opinion and topic relevance result. Moreover, the consistence between the result of Table2and Table3verifies the necessity of topic relevance retrieval before opinion retrieval.</figDesc><table coords="4,143.34,114.68,308.68,31.10"><row><cell>Best Baseline</cell><cell cols="4">Baseline MAP Best Non-Baseline Non Baseline MAP Increase</cell></row><row><cell cols="2">DUT08BRun1 0.2689</cell><cell>DUTIR08Run4</cell><cell>0.3394</cell><cell>26.22%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,87.42,156.81,420.43,55.09"><head>Table 4</head><label>4</label><figDesc>Improvement over the baselinesAccording to Table4, the combination of 5 Windows method and Pseudo Relevance Feedback behaves well in opinion finding task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,87.42,171.64,385.12,526.74"><head>Table 5 and</head><label>5</label><figDesc><ref type="bibr" coords="5,139.83,610.47,5.34,9.61" target="#b4">6</ref> show the runs and results submitted in polarity task.</figDesc><table coords="5,109.26,626.07,363.28,72.31"><row><cell>RunID</cell><cell>Based on</cell><cell>MAP</cell><cell>R-PREC</cell><cell>P@10</cell></row><row><cell>DUTIR08Run2P</cell><cell>DUTIR08Run2</cell><cell>0.0679</cell><cell>0.1185</cell><cell>0.2101</cell></row><row><cell>DUTIR08Run5P</cell><cell>DUTIR08Run4</cell><cell>0.0085</cell><cell>0.0282</cell><cell>0.0611</cell></row><row><cell>DUTIR08Run6P</cell><cell>DUTIR08Run2</cell><cell>0.0057</cell><cell>0.0261</cell><cell>0.0523</cell></row><row><cell></cell><cell cols="2">Table 5 Polarity Runs Positive</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,102.48,492.03,403.45,9.61" xml:id="b0">
	<monogr>
		<ptr target="http://trec.nist.gov/pubs/trec16/papers/dalianu.blog.final.pdf" />
		<title level="m" coord="7,102.48,492.35,138.95,9.19">DUTIR at TREC 2007 Blog Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,101.08,507.15,214.66,9.61" xml:id="b1">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Wikipedia" />
		<title level="m" coord="7,101.08,507.15,40.98,9.61">Wikipedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,101.96,522.33,206.14,9.61" xml:id="b2">
	<monogr>
		<ptr target="http://www.lemurproject.org/" />
		<title level="m" coord="7,101.96,522.33,74.13,9.61">The lemur Toolkit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,102.48,552.69,376.71,9.61;7,87.42,567.87,343.61,9.61" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/~jaime/TREC07ElsasJ.pdf" />
		<title level="m" coord="7,378.10,552.69,101.09,9.61;7,87.42,567.87,120.31,9.61">Retrieval and Feedback Models for Blog Distillation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,102.48,583.05,366.23,9.61" xml:id="b4">
	<monogr>
		<ptr target="http://sourceforge.net/project/showfiles.php?group_id=24399" />
		<title level="m" coord="7,102.48,583.05,96.40,9.61">HTML Parser Tool Kit</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
