<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,64.40,83.76,480.91,15.48">The Impact of Positive, Negative and Topical Relevance Feedback</title>
				<funder ref="#_7fVBaQn">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.09,116.28,80.71,10.75"><forename type="first">Rianne</forename><surname>Kaptein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.27,116.28,63.92,10.75"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.24,116.28,86.65,10.75"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Database Group</orgName>
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,64.40,83.76,480.91,15.48">The Impact of Positive, Negative and Topical Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E99C65EC461A8C25CA58424AF61549E7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This document contains a description of experiments for the 2008 Relevance Feedback track. We experiment with different amounts of feedback, including negative relevance feedback. Feedback is implemented using massive weighted query expansion. Parsimonious query expansion using only relevant documents and Jelinek-Mercer smoothing performs best on this relevance feedback track dataset. Additional blind feedback gives better results, except when the blind feedback set is of the same size as the explicit feedback set. On a small number of topics topical feedback is applied, which turns out to be mainly beneficial for early precision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this first year of the Relevance Feedback track we experiment with several relevance feedback approaches. Evaluation of feedback approaches is complicated because interaction with the system is dynamic, and performance depends on the feedback of users. Standard TREC evaluation measures are static and do not have a natural way to incorporate feedback <ref type="bibr" coords="1,111.47,446.48,10.58,8.64" target="#b5">[6]</ref>. The Relevance Feedback track is a first attempt to set up a framework in which relevance feedback approaches can be studied, evaluated and compared. To cope with the dynamic nature of the task, all feedback documents are removed from the result ranking before evaluation, creating a so-called residual ranking, on which the standard evaluation measures can be applied. Another option would be to freeze the feedback documents on their position in the initial ranking <ref type="bibr" coords="1,86.72,542.12,10.58,8.64" target="#b0">[1]</ref>.</p><p>This track allows us to explore the effects of using different amounts of relevance feedback, positive as well as negative feedback. We want to answer the following research questions:</p><p>1. Is it useful to combine pseudo-relevance feedback and explicit relevance feedback?</p><p>2. Can we exploit non-relevant documents for feedback?</p><p>3. How many feedback documents are needed ?</p><p>In addition, we experiment with another form of feedback, namely topical feedback. Instead of using relevant documents, topical feedback uses topic categories considered relevant to the query. Our last research question is:</p><p>4. Can we use topical feedback to improve retrieval results?</p><p>The rest of this paper is organized as follows. In Section 2, we discuss the details of the models we use for relevance and topical feedback. In Section 3, we first describe the experimental set-up, and then our experiments on the training and test data. Finally, we draw our conclusions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>We use different models in order to incorporate feedback from positive and negative relevance feedback and topical feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevance Feedback</head><p>Relevance feedback is applied using an adaptation of Lavrenko and Croft's Relevance Model <ref type="bibr" coords="1,476.64,390.19,10.58,8.64" target="#b3">[4]</ref>. Their relevance model provides a formal method to determine the probability P (w|R) of observing a word w in the documents relevant to a particular query. The method is a massive query expansion technique where the original query is completely replaced with a distribution over the entire vocabulary of the relevant feedback documents. Instead of completely replacing the original query, we include the original query with a weight W orig in the expanded query.</p><p>For all our experiments we use the Indri search engine <ref type="bibr" coords="1,541.81,497.78,10.58,8.64" target="#b6">[7]</ref>. Our baseline model is a standard language model. In the original baseline query Q orig each query term gets an equal weight of 1  |Q| . Our first relevance feedback approach only uses positive relevance feedback. The approach is similar to the implementation of pseudo-relevance feedback in Indri, and takes the following steps:</p><p>1. P (t|R) is estimated using the given relevant documents either using maximum likelihood estimation, or using a parsimonious model <ref type="bibr" coords="1,420.31,626.20,10.58,8.64" target="#b1">[2]</ref>.</p><p>The parsimonious model is estimated using Expectation-Maximization: 4. The fully expanded Indri query is now constructed as:</p><formula xml:id="formula_0" coords="1,348.09,673.59,210.32,22.31">E-step: e t = tf (t, R) • (1 -λ)P (t|R) (1 -λ)P (t|R) + λP (t|C)</formula><formula xml:id="formula_1" coords="2,90.67,235.04,165.35,9.65">#weight(W orig Q orig (1 -W orig ) Q R )</formula><p>5. Documents are retrieved based on the expanded query</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Negative Feedback</head><p>Until now, we only used the relevant feedback documents. Most of the feedback document sets also contain nonrelevant documents. We experiment with two approaches to also take into account the non-relevant feedback documents. For both approaches we first estimate a parsimonious model for the relevant documents P (t|R) and a parsimonious model for the negative documents P (t|N ). Typically some words, including the query terms, will occur in both the negative and the positive documents. The first approach (Comb QE) divides all terms in the positive model by their value in the negative model, or by a factor α if the term does not occur in the negative model. The probabilities are afterwards normalized to add up to 1. For α we use the value 0.001, which is equal to the threshold used in the parsimonious model estimation. This approach boosts probabilities of terms occurring in the positive but not in the negative model, assuming these terms will make a better distinction between relevant and non relevant documents.</p><p>The second approach (Neg QE) takes the positive model and adds all terms from the negative model that do not occur in the positive model with a negative weight. This approach is based on the assumption that if a term occurs in both the positive and the negative model, it is still a good term to use for feedback.</p><p>Both models are extensions to the original query, where the original query has a total weight of 1. In case that there are no non-relevant feedback documents, feedback set B only, the results will be the same as when using only the positive relevance feedback documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Topical Feedback</head><p>Besides the given relevance feedback sets, there are also some manual topics for which participants in the track can define their own relevance feedback. In our case we use topical categories as topical feedback. A topical category from the DMOZ directory is assigned to each query. We assume that all web sites in the chosen DMOZ category, and all of its direct subcategories are relevant to the query. The topical feedback model is build from the text on these web pages. Topical feedback is applied in the same way as explicit relevance feedback where instead of the relevant document(s) P (t|R), we now have the topical model P (t|T M ).</p><p>We implemented a second variant of the topical model, where the weights of the original query are adjusted according to the fraction of query words in the topical category title. If the query terms are equal to the category title, this topical model is a good match for the query, so the weight of the topical model terms can be high. On the other hand, if none of the query terms occur in the category title, it is unlikely that the topical feedback will contribute to retrieval performance, so the weight of the topical feedback is lowered. The original weights of the query words are 1  |Q| , the adjusted weights of the querywords are 1/(|Q| * fraction of query terms in category title). A fraction of 1/5 is used when none of the query terms occur in the category title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Set-up</head><p>The Relevance Feedback track test topics consist of 50 (even-numbered) topics from the Terabyte tracks and 214 (even-numbered) topics from the 2007 MQ track. We train on the odd-numbered Terabyte topics, since for these topics extensive relevance judgments are available.</p><p>For efficiency reasons we do not build an index of the complete .GOV2 collection. Instead we build an index using only the top 2,500 results of runs that we made in previous Terabyte and Million Query tracks. These previous runs are created by using a standard language model, with Jelinek-Mercer smoothing (λ = 0.1). We build one index which contains both the training and the test data. This index contains 742,664 documents, 9,228,163 unique terms and a total of 4,860,799,852 terms. Since this background corpus is much smaller, contains longer documents, and is biased towards the queries, the estimations of background probabilities may not reflect the whole corpus well.</p><p>For the training data no relevance feedback document sets are given, so we create these by taking the highest ranked documents of our Terabyte track run. The feedback sets contain the following documents:  </p><formula xml:id="formula_2" coords="2,326.78,617.19,117.94,35.36">• Set B: 1 relevant document • Set C: 3 relevant,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline</head><p>We use the language model of Indri for our experiments. To incorporate the explicit relevance feedback, we use weighted query expansion.</p><p>Besides the explicit relevance feedback we also do blind relevance feedback, based on Lavrenko and Croft's relevance model. Indri's blind relevance feedback is applied using parameters from <ref type="bibr" coords="3,135.79,265.20,10.58,8.64" target="#b4">[5]</ref>, i.e., number of feedback documents = 10, terms for query expansion = 50, weight original query = 0.5, µ= 1500. In addition we also use our own scripts to apply blind relevance feedback using query expansion in the same way as our explicit feedback. Again we use the top 10 retrieved documents.</p><p>We have made a number of baseline runs, that do not use explicit relevance feedback. The results on the training data, i.e. 75 odd-numbered Terabyte Track queries, are given in Table <ref type="table" coords="3,78.18,372.79,3.74,8.64" target="#tab_1">1</ref>. The following parameters can be adjusted:</p><p>• Two smoothing techniques are used: JM stands for Jelinek-Mercer smoothing with λ = 0.1, Dir. stands for Dirichlet smoothing with µ = 1500.</p><p>• A document prior based on document length (length prior).</p><p>• Blind relevance feedback, either using indri with the parameters given above (Indri), or by using query expansion (QE).</p><p>On our baseline runs Dirichlet smoothing achieves significantly better results than Jelinek-Mercer smoothing. Indri's blind feedback performs better, except on Bpref, than doing query expansion with our own scripts, probably due to a better optimization of parameters. From now on, when we apply blind feedback, we use Indri's blind feedback. Applying the length prior leads to a decrease in MAP and Bpref, but to an increase in P10. We will not apply a length prior in any of the other runs. a λ of 0.01, and a threshold of 0.001. The original query terms are included in the query with a total weight of 1, the weight of the added query terms together is also 1, which is the same as using W orig = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relevance Feedback</head><p>Our purpose here is to find the optimal parameters for this feedback set. Therefore, in this section before evaluation we only remove the given relevant document or documents from the ranking. Although it becomes more difficult to compare across different feedback sets, results within one feedback set are more accurate <ref type="foot" coords="3,401.18,319.44,3.49,6.05" target="#foot_0">1</ref> .</p><p>Comparing parsimonious and MLE query expansion, parsimonious query expansion consistently gives slightly better results, but the improvements are very small and not in all cases significant. For the other feedback sets we will always use parsimonious query expansion. The differences between Dirichlet and Jelinek-Mercer smoothing are much smaller here, only P10 seems to be better when Dirichlet smoothing is used. These results adhere to the results of the comparison of smoothing techniques in <ref type="bibr" coords="3,427.42,429.70,10.58,8.64" target="#b7">[8]</ref>. They find Dirichlet smoothing performs best on short queries, i.e. no query expansion. For long queries, i.e. when query expansion is used, Jelinek-Mercer is on average better, but average precision is almost identical to Dirichlet smoothing. For feedback set B, applying blind feedback on top of the explicit relevance feedback leads to considerable improvements.</p><p>Tables <ref type="table" coords="3,355.44,514.38,25.51,8.64" target="#tab_6">3 to 5</ref> give the results using feedback sets C, D and E. For these sets also non-relevant documents are provided. We use this negative feedback in two ways. The first method (Comb QE) divides all terms in the positive feedback model by their value in the negative model. The second method (Neg QE) takes the positive model and adds all terms from the negative model that do not occur in the positive model with a negative weight. For the feedback sets C, D and E we also still do query expansion using only the positive feedback documents. Results of the different query expansion methods depend also on the smoothing technique that is used.</p><p>Using feedback set C results of the three query expansion methods lie very close together, and there is not one method For feedback set D, parsimonious query expansion is best on all three evaluation measures. On the training data, using the negative relevance feedback information does not lead to better results than only using positive relevance feedback. Comparing the two methods (Comb QE and Neg QE), differences are small, combined query expansion in combination with Jelinek-Mercer smoothing seems to be the most promising approach.</p><p>Looking at all results, in general Dirichlet smoothing is to be preferred. Differences in MAP and Bpref are small, and sometimes Jelinek-Mercer smoothing also gives better results. Dirichlet smoothing however does give consistently better P10 values.</p><p>We can answer positively to our first research question whether blind feedback can be used in combination with explicit relevance feedback. For feedback sets B and C applying additional blind feedback leads to an increase in improvement, but for feedback sets D and E the improvements are declining. The explicit feedback sets D and E are equal or larger than the set of documents used for blind relevance feedback. Since the feedback sets B to E are selected using Figure <ref type="figure" coords="4,381.98,224.59,3.88,8.64">1</ref>: MAP improvement correlations an initial run very similar to the our new run, there will be a large overlap in the explicit feedback and the blind relevance feedback documents for the top 10 ranked documents. Feedback set D consist of the top 10 documents and is therefore the most similar to the blind feedback set of the top 10 ranked documents. For feedback set D, we see that applying additional blind relevance feedback leads to a decrease in MAP and P10, but an increase in Bpref. For feedback set E applying blind feedback leads to a small increase in performance on all three measures. Feedback set E contains of the first 100 documents, of which in this case only the relevant documents are used. Using this large amount of documents possibly leads to less focused query expansion terms, which can be corrected partly by including blind feedback using only the top 10 ranked documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Topical Feedback</head><p>We apply topical feedback on the manual topics of the RF track. For Terabyte topics 800-850 we use topical categories assigned by test users in a user study <ref type="bibr" coords="4,462.46,482.31,10.58,8.64" target="#b2">[3]</ref>. For the other topics topical categories are assigned by ourselves. We use oddnumbered topics 800-850 from the Terabyte track for training. Besides the standard topical query expansion (Topic QE), we also give results of the weighted topical query expansion (W. Topic QE). To create the topical model we use a λ of 0.01, and a threshold of 0.001. In each run we use Dirichlet smoothing. The parameters are whether blind feedback is applied, and whether a document length prior is used.</p><p>The weighted topical query expansion works because there is a weak (non-significant) correlation between improvement in MAP when topical query expansion is used, and the fraction of query terms in either the category title, or the top ranked terms of the topical language model, as can be seen in Figure <ref type="figure" coords="4,387.65,650.37,3.74,8.64">1</ref>.</p><p>Results of the manual topic runs can be found in Table <ref type="table" coords="4,548.44,663.00,3.74,8.64" target="#tab_7">6</ref>. Although on average the topical model feedback only leads to a small improvement of MAP over the baseline, for 8 out of 25 topics, the topical model feedback has best MAP of all models. In the run Weighted Topic QE, we reweigh the original query terms according to the inverse fraction of query terms that occur in the category title, i.e. if half of the query terms occur in the category title, we double the original query weights. These runs lead to better results and to improvements over blind relevance feedback, but they are not significant on our small training set of 25 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Test Results</head><p>On the test data we experiment with smoothing and query expansion methods. We make four runs using either Dirichlet or Jelinek-Mercer smoothing, and either parsimonious or combined query expansion. Our submitted official runs are the run using Dirichlet smoothing with parsimonious query expansion, and the run using Jelinek-Mercer smoothing and combined query expansion. All runs apply additional blind relevance feedback. The test data consist of 31 Terabyte track topics that are evaluated approximately according to the standard TREC evaluation strategy. All documents from feedback set E are removed before evaluation takes place. Additional assessments in Million Query track style are available for more topics. These results are similar to our results on the 31 fully judged topics, and will therefore not be reported here.</p><p>The results are given in Table <ref type="table" coords="5,186.54,710.82,3.74,8.64" target="#tab_8">7</ref>. Considering smoothing  techniques, the results are similar to the training results, i.e. there is little difference between results, but in most cases Jelinek-Mercer smoothing leads to better results. We can now answer our second research question: Can we exploit non-relevant documents for feedback? Comparing parsimonious query expansion using only relevant documents with combined query expansion using relevant and non-relevant documents, using the non-relevant documents does not lead to much improvement. The only improvement is achieved with feedback set E, looking at early precision. We can conclude that using query expansion techniques it is not useful to include non-relevant feedback documents.</p><p>Our third research question was: How many feedback documents are needed? When we look at the different feedback sets, we notice that more relevance information does not always lead to better results. The biggest improvements by far are achieved when going from no relevance feedback to using one relevant document. Part of this improvement might be attributed to the smoothing parameter settings, which are optimized for long queries. This applies especially to Jelinek-Mercer smoothing.</p><p>Since our feedback method uses parsimonious query expansion the feedback documents are summarized into a limited number of feedback terms that depend on the threshold parameter of the parsimonious model. The threshold param- eter is fixed on 0.001 for all feedback sets, which leads to around 100 to 300 terms to be included for query expansion. When the feedback set gets larger, relatively less of the feedback information is included in the feedback model, and improvements using larger feedback sets indeed declines as can be seen in Figure <ref type="figure" coords="6,143.46,459.25,3.74,8.64" target="#fig_2">2</ref>. Adjusting the threshold parameter of the parsimonious model to the size of the feedback model allowing more terms to be added when the feedback set is larger, might lead to better results.</p><p>In the official evaluation all set E documents are removed, so that runs using different feedback sets can be compared. We have made some extra evaluations in which only the feedback documents are removed from the runs as well as from the qrels to be able to accurately compare runs within one feedback set. Results of this evaluation are given in Table 8. Parsimonious query expansion in combination with Jelinek-Mercer smoothing leads to the best results for almost all feedback sets looking at MAP and P10. There is only one exception, for feedback set E using combined query expansion in combination with Dirichlet smoothing leads to the highest P10. The relations between the different (smoothing) methods are similar to the official results. When we compare the performance of the runs looking at the different feedback sets we see that results improve until set C, and then start to decline at set D. Only for set E the feedback results are lower than the baseline, but there is still a considerable number of relevant documents found among the documents that were not included in the original pool. Finally, our last research question is related to topical feedback. Table <ref type="table" coords="6,383.29,192.97,4.98,8.64" target="#tab_10">9</ref> shows the results for topical feedback. In contrast to the training results, using topical feedback does not lead to significant improvements over the baseline on the 13 test topics, for MAP no improvement at all is achieved. We do achieve more than 8% improvement in P10. Since the effect of using topical feedback varies a lot over different queries, the test set of 13 topics is a bit small to draw conclusions. Besides trying to improve retrieval results, topical context can also be used to aggregate or cluster search results into topic categories. So, even if we cannot draw any conclusions here about improving retrieval results using topical feedback it is still an interesting feature that can be used to improve users' search experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>From our experiments with different relevance feedback approaches we can conclude that our query expansion approach is effective, already with small amounts of relevance information. There are no significant differences between the different smoothing and query expansion approaches. For most feedback sets and evaluation measures parsimonious query expansion using only relevant documents in combination with Jelinek-Mercer smoothing works best. Adding information from non-relevant feedback documents does not lead to improvements. Additional blind feedback on top of the explicit relevance feedback does lead to better results, except when the blind feedback set size is equal to the relevance feedback set size.</p><p>Topical feedback can be used as an alternative to relevance feedback. Improvements over blind relevance feedback are achieved, especially for early precision. We would like to explore in more detail the topical feedback approach, and how topical feedback relates to relevance feedback. We found some indicators to predict the performance of topical feedback on individual queries, and it would be interesting to continue investigating performance indicators.</p><p>In our experiments we have used an index that does not include the complete .GOV2 collection, but a subset of documents based on previous runs. Since the feedback approaches introduce new query terms in the expanded queries, we might retrieve new relevant documents, that are currently not in the index, when we index the whole collection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,407.91,643.91,119.86,8.64;2,326.78,669.99,185.20,8.96;2,326.78,696.39,229.14,8.96;2,336.74,708.67,84.12,8.64"><head></head><label></label><figDesc>and 3 non-relevant documents • Set D: 10 documents, set C always included • Set E: All previously judged documents ( for training only 100 documents)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,356.19,390.72,160.35,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Official results: MAP and P10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,346.70,698.92,211.59,24.72"><head></head><label></label><figDesc>In the M-step terms that receive a probability below a threshold of 0.001 are removed from the model. In the next iteration the probabilities of the remaining terms are again normalized. λ determines the weight of the background model P (t|C).</figDesc><table coords="2,61.27,124.38,231.63,73.36"><row><cell>2. Terms P (t|R) are sorted, in case of MLE only the 50</cell></row><row><cell>top ranked terms are kept.</cell></row><row><cell>3. The relevance feedback part, Q R , of the expanded</cell></row><row><cell>query is constructed as:</cell></row><row><cell>#weight(P (t</cell></row></table><note coords="1,346.70,705.66,81.01,8.96;1,439.51,698.92,7.65,9.65;1,442.19,717.53,3.01,6.12;1,447.35,712.50,7.65,9.65;1,456.70,705.66,101.59,8.96;2,156.82,189.00,90.40,9.65"><p>M-step: P (t|R) = e t t e t , i.e. normalize the model i |R) t i ... P (t n |R) t n )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,60.26,64.07,226.17,90.42"><head>Table 1 :</head><label>1</label><figDesc>Baseline results</figDesc><table coords="3,60.26,80.56,226.17,73.93"><row><cell cols="3">Smoothing Blind FB Prior</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>JM</cell><cell>No</cell><cell>No</cell><cell cols="2">0.2135 0.2930 0.3595</cell></row><row><cell>JM</cell><cell>Indri</cell><cell>No</cell><cell cols="2">0.2645 0.3343 0.4500</cell></row><row><cell>Dir.</cell><cell>No</cell><cell>No</cell><cell cols="2">0.2837 0.3341 0.5446</cell></row><row><cell>Dir.</cell><cell>No</cell><cell>Yes</cell><cell cols="2">0.2774 0.3323 0.5500</cell></row><row><cell>Dir.</cell><cell>Indri</cell><cell>No</cell><cell cols="2">0.3155 0.3618 0.5797</cell></row><row><cell>Dir.</cell><cell>QE</cell><cell>No</cell><cell cols="2">0.3021 0.3727 0.5500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.80,639.09,239.10,80.37"><head>Table 2</head><label>2</label><figDesc>gives the results of applying relevance feedback using one relevant document as feedback (set B). Relevance feedback documents are used for query expansion, either using Maximum Likelihood Estimation (MLE QE) or a parsimonious model (Pars QE). In case of Maximum Likelihood Estimation the top 50 terms are used, and their probabilities are normalized to add up to 1. The parsimonious model uses</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,323.19,64.07,227.66,123.30"><head>Table 2 :</head><label>2</label><figDesc>Results feedback set B</figDesc><table coords="3,323.19,80.56,227.66,106.80"><row><cell>QE</cell><cell cols="2">Smoothing Blind FB</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>None</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.3044 0.3531 0.5500</cell></row><row><cell>Pars</cell><cell>JM</cell><cell>No</cell><cell cols="3">0.3205 0.3873 0.5662</cell></row><row><cell>MLE</cell><cell>JM</cell><cell>No</cell><cell cols="3">0.3055 0.3774 0.5608</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3198 0.3737 0.6216</cell></row><row><cell>MLE</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3152 0.3728 0.6189</cell></row><row><cell>Pars</cell><cell>JM</cell><cell>Yes</cell><cell cols="3">0.3239 0.4066 0.5892</cell></row><row><cell>MLE</cell><cell>JM</cell><cell>Yes</cell><cell cols="3">0.3199 0.4007 0.5865</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.3300 0.3919 0.6405</cell></row><row><cell>MLE</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.3266 0.3920 0.6338</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,60.17,64.07,230.17,123.32"><head>Table 3 :</head><label>3</label><figDesc>Results feedback set C</figDesc><table coords="4,60.17,80.59,230.17,106.80"><row><cell>QE</cell><cell cols="2">Smoothing Blind FB</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>None</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.2965 0.3468 0.5622</cell></row><row><cell>Pars</cell><cell>JM</cell><cell>No</cell><cell cols="3">0.3261 0.3869 0.5946</cell></row><row><cell>Pars</cell><cell>JM</cell><cell>Yes</cell><cell cols="3">0.3353 0.4095 0.6230</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3291 0.3794 0.6473</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.3341 0.3945 0.6405</cell></row><row><cell>Comb</cell><cell>JM</cell><cell>No</cell><cell cols="3">0.3298 0.3934 0.6257</cell></row><row><cell>Comb</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3247 0.3772 0.6446</cell></row><row><cell>Neg</cell><cell>JM</cell><cell>No</cell><cell cols="3">0.2967 0.3691 0.5554</cell></row><row><cell>Neg</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3243 0.3823 0.6311</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,60.17,212.25,230.17,90.42"><head>Table 4 :</head><label>4</label><figDesc>Results feedback set D</figDesc><table coords="4,60.17,228.74,230.17,73.93"><row><cell>QE</cell><cell cols="2">Smoothing Blind FB</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>None</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.2741 0.3299 0.5405</cell></row><row><cell>Pars</cell><cell>JM</cell><cell>No</cell><cell cols="3">0.3082 0.3761 0.5770</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3123 0.3701 0.6365</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.3110 0.3810 0.6216</cell></row><row><cell>Comb</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3081 0.3678 0.6243</cell></row><row><cell>Neg</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.3083 0.3767 0.6297</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,53.80,327.52,239.10,138.07"><head>Table 5 :</head><label>5</label><figDesc>Results feedback set E</figDesc><table coords="4,53.80,344.04,239.10,121.55"><row><cell>QE</cell><cell cols="2">Smoothing Blind FB</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>None</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.1079 0.2088 0.3176</cell></row><row><cell>Pars</cell><cell>JM</cell><cell>No</cell><cell cols="3">0.1341 0.2517 0.3946</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>No</cell><cell cols="3">0.1343 0.2431 0.4108</cell></row><row><cell>Pars</cell><cell>Dir.</cell><cell>Yes</cell><cell cols="3">0.1394 0.2504 0.4365</cell></row><row><cell cols="6">that is best for all evaluation measures. The combination</cell></row><row><cell cols="6">of Jelinek-Mercer smoothing and combined query expansion</cell></row><row><cell cols="6">gives the best MAP and Bpref. Best P10 is achieved using</cell></row><row><cell cols="6">parsimonious query expansion and Dirichlet smoothing.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,63.78,64.07,219.14,103.42"><head>Table 6 :</head><label>6</label><figDesc>Results manual topics</figDesc><table coords="5,63.78,82.61,219.14,84.88"><row><cell>QE</cell><cell cols="2">Blind FB Prior</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>None</cell><cell>No</cell><cell>No</cell><cell cols="3">0.2902 0.3415 0.5680</cell></row><row><cell>None</cell><cell>Yes</cell><cell>No</cell><cell cols="3">0.3267 0.3736 0.6120</cell></row><row><cell>Topic</cell><cell>No</cell><cell>No</cell><cell cols="3">0.2694 0.3392 0.5560</cell></row><row><cell>Topic</cell><cell>No</cell><cell>Yes</cell><cell cols="3">0.2789 0.3541 0.5160</cell></row><row><cell>Topic</cell><cell>Yes</cell><cell>No</cell><cell cols="3">0.3069 0.3710 0.5760</cell></row><row><cell>W. Topic</cell><cell>No</cell><cell>Yes</cell><cell cols="3">0.3023 0.3616 0.5560</cell></row><row><cell>W. Topic</cell><cell>Yes</cell><cell>Yes</cell><cell cols="3">0.3339 0.3847 0.6360</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="5,67.99,190.94,208.48,223.55"><head>Table 7 :</head><label>7</label><figDesc>Official results</figDesc><table coords="5,67.99,207.46,208.48,207.03"><row><cell>Set</cell><cell>QE</cell><cell>Smoothing</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>A</cell><cell>None</cell><cell>Dir.</cell><cell cols="3">0.1574 0.2296 0.2871</cell></row><row><cell>A</cell><cell>None</cell><cell>JM</cell><cell cols="3">0.1222 0.2205 0.2258</cell></row><row><cell>B</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.1930 0.2642 0.3516</cell></row><row><cell>B</cell><cell>Pars</cell><cell>JM</cell><cell cols="3">0.2017 0.2792 0.3903</cell></row><row><cell>B</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.1930 0.2642 0.3516</cell></row><row><cell>B</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.2017 0.2792 0.3903</cell></row><row><cell>C</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.1989 0.2713 0.3774</cell></row><row><cell>C</cell><cell>Pars</cell><cell>JM</cell><cell cols="3">0.2116 0.2869 0.3968</cell></row><row><cell>C</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.1898 0.2665 0.3871</cell></row><row><cell>C</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.1895 0.2663 0.3903</cell></row><row><cell>D</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.2059 0.2867 0.3484</cell></row><row><cell>D</cell><cell>Pars</cell><cell>JM</cell><cell cols="3">0.2120 0.2927 0.3806</cell></row><row><cell>D</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.2000 0.2846 0.3742</cell></row><row><cell>D</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.1898 0.2781 0.3774</cell></row><row><cell>E</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.2058 0.2909 0.3839</cell></row><row><cell>E</cell><cell>Pars</cell><cell>JM</cell><cell cols="3">0.2139 0.2985 0.3806</cell></row><row><cell>E</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.2132 0.2940 0.4226</cell></row><row><cell>E</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.2131 0.3037 0.4161</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="6,67.99,64.07,208.48,311.22"><head>Table 8 :</head><label>8</label><figDesc>Results test runs</figDesc><table coords="6,67.99,80.59,208.48,294.70"><row><cell>Set</cell><cell>QE</cell><cell>Smoothing</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>A</cell><cell>None</cell><cell>Dir.</cell><cell cols="3">0.3287 0.3663 0.6032</cell></row><row><cell>A</cell><cell>None</cell><cell>JM</cell><cell cols="3">0.2856 0.3328 0.4871</cell></row><row><cell>B</cell><cell>None</cell><cell>Dir.</cell><cell cols="3">0.3234 0.3618 0.5903</cell></row><row><cell>B</cell><cell>None</cell><cell>JM</cell><cell cols="3">0.2814 0.3284 0.4742</cell></row><row><cell>B</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.3570 0.3985 0.7000</cell></row><row><cell>B</cell><cell>Pars</cell><cell>JM.</cell><cell cols="3">0.3690 0.4097 0.6677</cell></row><row><cell>B</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.3570 0.3985 0.7000</cell></row><row><cell>B</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.3690 0.4097 0.6677</cell></row><row><cell>C</cell><cell>None</cell><cell>Dir.</cell><cell cols="3">0.3246 0.3598 0.6032</cell></row><row><cell>C</cell><cell>None</cell><cell>JM</cell><cell cols="3">0.2793 0.3259 0.4484</cell></row><row><cell>C</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.3674 0.4066 0.7452</cell></row><row><cell>C</cell><cell>Pars</cell><cell>JM.</cell><cell cols="3">0.3870 0.4262 0.7458</cell></row><row><cell>C</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.3602 0.4076 0.7258</cell></row><row><cell>C</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.3694 0.4154 0.7419</cell></row><row><cell>D</cell><cell>None</cell><cell>Dir.</cell><cell cols="3">0.3110 0.3486 0.5516</cell></row><row><cell>D</cell><cell>None</cell><cell>JM</cell><cell cols="3">0.2675 0.3150 0.4194</cell></row><row><cell>D</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.3552 0.3954 0.6613</cell></row><row><cell>D</cell><cell>Pars</cell><cell>JM.</cell><cell cols="3">0.3731 0.4133 0.7000</cell></row><row><cell>D</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.3483 0.3951 0.6903</cell></row><row><cell>D</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.3506 0.4012 0.6774</cell></row><row><cell>E</cell><cell>None</cell><cell>Dir.</cell><cell cols="3">0.1574 0.2296 0.2871</cell></row><row><cell>E</cell><cell>None</cell><cell>JM</cell><cell cols="3">0.1222 0.2205 0.2258</cell></row><row><cell>E</cell><cell>Pars</cell><cell>Dir.</cell><cell cols="3">0.2058 0.2909 0.3839</cell></row><row><cell>E</cell><cell>Pars</cell><cell>JM.</cell><cell cols="3">0.2139 0.2985 0.3806</cell></row><row><cell>E</cell><cell>Comb</cell><cell>Dir.</cell><cell cols="3">0.2132 0.2940 0.4226</cell></row><row><cell>E</cell><cell>Comb</cell><cell>JM</cell><cell cols="3">0.2131 0.3037 0.4161</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="6,349.54,64.07,173.64,81.50"><head>Table 9 :</head><label>9</label><figDesc>Results manual topics test runs</figDesc><table coords="6,349.54,82.61,173.64,62.97"><row><cell>QE</cell><cell>Prior</cell><cell>MAP</cell><cell>Bpref</cell><cell>P10</cell></row><row><cell>None</cell><cell>No</cell><cell cols="3">0.3873 0.4416 0.6385</cell></row><row><cell>Topic</cell><cell>No</cell><cell cols="3">0.3412 0.4139 0.6615</cell></row><row><cell>Topic</cell><cell>Yes</cell><cell cols="3">0.3332 0.4212 0.6923</cell></row><row><cell>W. Topic</cell><cell>No</cell><cell cols="3">0.3811 0.4417 0.6615</cell></row><row><cell>W. Topic</cell><cell>Yes</cell><cell cols="3">0.3674 0.4443 0.6692</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,331.16,693.19,224.76,6.91;3,316.81,702.66,239.10,6.91;3,316.81,712.12,157.28,6.91"><p>By accident the feedback documents were only removed from the runs and not from the qrels for all training results. Therefore the reported values of MAP and Bpref are lower than they should be.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research is funded by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs>, grant # <rs type="grantNumber">612.066.513</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7fVBaQn">
					<idno type="grant-number">612.066.513</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,69.24,73.17,223.67,7.77;7,69.23,84.13,223.67,7.77;7,69.23,94.93,223.67,7.94;7,69.23,105.89,223.67,7.93;7,69.23,117.01,56.04,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,209.63,73.17,83.28,7.77;7,69.23,84.13,223.67,7.77;7,69.23,95.09,65.95,7.77;7,216.39,94.93,76.52,7.73;7,69.23,105.89,197.41,7.73">Evaluation of feedback retrieval using modified freezing, residual collection and test and control groups</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cirillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Razon</surname></persName>
		</author>
		<editor>G. Salton</editor>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
	<note>The SMART retrieval system -experiments in automatic document processing</note>
</biblStruct>

<biblStruct coords="7,69.24,129.56,223.67,7.77;7,69.23,140.36,223.67,7.93;7,69.23,151.32,218.91,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,244.22,129.56,48.69,7.77;7,69.23,140.52,149.37,7.77">Parsimonious language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,235.58,140.36,57.32,7.73;7,69.23,151.32,33.32,7.73">Proceedings SI-GIR 2004</title>
		<meeting>SI-GIR 2004<address><addrLine>New York NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,69.24,164.03,223.67,7.77;7,69.23,174.83,223.67,7.93;7,69.23,185.79,127.87,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,167.66,164.03,121.68,7.77">Web directories as topical context</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,79.76,174.83,213.14,7.73;7,69.23,185.79,59.89,7.73">Proceedings of the 9th Dutch-Belgian Workshop on Information Retrieval</title>
		<meeting>the 9th Dutch-Belgian Workshop on Information Retrieval<address><addrLine>DIR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,69.24,198.50,223.67,7.77;7,69.23,209.30,223.67,7.93;7,69.23,220.26,142.31,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,176.98,198.50,115.93,7.77;7,69.23,209.46,9.15,7.77">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,94.59,209.30,198.31,7.73;7,69.23,220.26,116.07,7.73">ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,69.24,232.97,223.67,7.77;7,69.23,243.77,223.67,7.93;7,69.23,254.73,102.48,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,251.89,232.97,41.01,7.77;7,69.23,243.93,73.30,7.77">Indri at trec 2005: Terabyte track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,158.43,243.77,134.48,7.73;7,69.23,254.73,76.56,7.73">TREC: Experiment and Evaluation in Information Retrieval</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,69.24,267.44,223.67,7.77;7,69.23,278.24,223.67,7.93;7,69.23,289.20,114.13,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,233.14,267.44,59.77,7.77;7,69.23,278.40,118.24,7.77">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,197.84,278.24,95.06,7.73;7,69.23,289.20,44.43,7.73">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,69.24,301.91,223.67,7.77;7,69.23,312.87,223.67,7.77;7,69.23,323.67,223.67,7.93;7,69.23,334.63,55.04,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,272.98,301.91,19.92,7.77;7,69.23,312.87,220.15,7.77">Indri: a language-model based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,79.53,323.67,213.37,7.73;7,69.23,334.63,29.00,7.73">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,69.24,347.34,223.67,7.77;7,69.23,358.14,223.67,7.93;7,69.23,369.10,223.67,7.73;7,69.23,380.06,199.48,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,154.49,347.34,138.41,7.77;7,69.23,358.30,188.80,7.77">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,274.24,358.14,18.66,7.73;7,69.23,369.10,223.67,7.73;7,69.23,380.06,77.96,7.73">ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
