<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,80.00,76.28,452.00,14.18;1,194.00,103.28,225.00,14.18">FUB at TREC 2008 Relevance Feedback Track: Extending Rocchio with Distributional Term Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,215.00,144.96,86.81,10.63"><forename type="first">Andrea</forename><surname>Bernardini</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Fondazione Ugo Bordoni</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,309.58,144.96,87.44,10.63"><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
							<email>carpinet@fub.it</email>
							<affiliation key="aff0">
								<address>
									<settlement>Fondazione Ugo Bordoni</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,80.00,76.28,452.00,14.18;1,194.00,103.28,225.00,14.18">FUB at TREC 2008 Relevance Feedback Track: Extending Rocchio with Distributional Term Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76282CE4F034C2B0A8C9C61E4962B7BC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The main goals of our participation in the Relevance feedback track at TREC 2008 were the following.</p><p>-Test the effectiveness of using a combination of Rocchio and distributional term analysis on a relevance feedback task; so far, this approach has usually been used (with good results) in a pseudo-relevance setting.</p><p>-Test whether and when negative relevance feedback is useful; e.g., is negative relevance feedback most effective when the distribution of terms in the negative documents is different than the distribution in the positive documents?</p><p>-Study how the performance of relevance feedback varies as the size of the set of feedback documents grows.</p><p>-Check if /how the performance of relevance feedback is influenced by the size of the expanded query.</p><p>-Compare relevance feedback to pseudorelevance feedback; e.g, is relevance feedback not only more effective but also more robust than pseudo-relevance feedback?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our approach: combining Rocchio with term-ranking scores</head><p>The starting point is the improved version <ref type="bibr" coords="1,318.00,383.96,143.06,10.63" target="#b5">[Salton and Buckley 1990]</ref> of the original Rocchio's formula <ref type="bibr" coords="1,408.99,398.96,66.74,10.63" target="#b4">[Rocchio 1971</ref>]:</p><formula xml:id="formula_0" coords="1,318.94,448.21,223.06,26.94">new Q = α ⋅ orig Q + β R r r∈R ∑ -γ R' r' r' ∈R ' ∑<label>(1)</label></formula><p>Q n e w is a weighted term vector for the expanded query, Q orig is a weighted term vector for the original unexpanded query, R and R' are respectively the sets of relevant and nonrelevant documents, r and r' are term weighting vectors extracted from R and R', respectively. The weights in each vector are usually computed by a weighting scheme applied to the whole collection.</p><p>This approach is simple and computationally efficient, but it has the disadvantage that each term weight may reflect more the usefulness of that term with respect to the entire collection rather than its importance with respect to the user query. This issue can be addressed by studying the difference of term distribution between the subsets of relevant (or non-relevant) documents and the whole collection. It is expected that terms with little informative content will have the same distribution in any subset of the collection, whereas the terms that are most closely related to the query will have a comparatively higher probability of occurrence in the relevant (or non-relevant) documents.</p><p>The term-ranking scores can then be used not only to select the best expansion terms but also to reweight the expanded query using equation ( <ref type="formula" coords="2,130.74,345.96,4.63,10.63" target="#formula_0">1</ref>). This approach was early suggested in <ref type="bibr" coords="2,134.35,360.96,109.65,10.63" target="#b0">(Carpineto et al. 2001)</ref> and it has been adopted in several subsequent studies; e.g., <ref type="bibr" coords="2,99.49,390.96,196.51,10.63">Wong et al. 2008, Perez-Aguera and</ref><ref type="bibr" coords="2,72.00,405.96,57.62,10.63" target="#b3">Araujo 2008</ref>.</p><p>In this paper we use the B o 1 model in the Bose-Einstein statistics t o assign a score to each candidate expansion term. Bo1 evaluate the importance of a term by calculating the divergence of its distribution in a pseudo-relevance document set from a random distribution <ref type="bibr" coords="2,220.99,525.96,62.83,10.63" target="#b1">(Amati 2003)</ref>.</p><p>Bo1 estimates the score of term t as follows:</p><formula xml:id="formula_1" coords="2,72.00,570.96,221.00,11.77">score(t) = f R log 2 [(1+nf C ) / nf C )] + log 2 (1+nf C )</formula><p>where f R is the frequency of the term in the relevant documents, and nf C is given by the frequency of the term in the collection divided by the number of documents in the collection.</p><p>As the document-based weights used for the unexpanded query and the Bo1 scores used for the expansion terms had different scales, they were normalized by the maximum c o r r e s p o n d i n g w e i g h t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dealing with negative relevance feedback</head><p>A straightforward utilization of negative relevance feedback in equation ( <ref type="formula" coords="2,475.51,249.96,4.67,10.63" target="#formula_0">1</ref>) is possible but it is probably not the most effective choice. If we want to take full advantage of the information about nonrelevant documents, more selective policies for choosing the negative expansion terms are necessary. In particular, we do not want to downweight good positive terms which also happen to occur in negative documents. Our approach was to choose those terms in the negative documents that most contributed to the Bose Einstein divergence of the negative documents from the positive ones. For computational reasons, however, we approximated this criterion by using the divergence of the two set of feedback documents from the collection, because these measures can be computed more easily.</p><p>We defined two distinct methods, which will be referred to as method 1 and method 2 (in both methods the number of positive terms was set to 100). In method 1, we chose the the first 30 terms in the negative documents that most contributed to divergence from the collection, provided that they did not appear in the positive terms. In method 2, we chose the 100 terms in the negative documents with the greatest difference between the divergence of the negative documents from the collection and the divergence of the positive documents from the collection, regardless of whether or not such terms appeared also in the positive terms.</p><p>Interestingly, we found that method 1 yielded nearly the same results as those that would be produced by method 2 with only 30 negative terms. The main difference between method 1 and method 2 was thus the size of the set of negative terms used for expanding the query, i.e., 30 and 100, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To verify our hypotheses, we extended the relevance feedback module of the software Terrier <ref type="bibr" coords="3,108.23,346.96,82.48,10.63" target="#b2">[Ounis et al. 2006</ref>], used as underlying information retrieval system. The basic configuration of Terrier was the following: Pl2 DFR model (parameter c=1.0) for document ranking, and Bo1 for Terrier's default query expansion.</p><p>We did not use all the input relevance data because we noticed that the information contained in "relevant" documents (score 1) was not always very accurate. We concentrated only on "not relevant" documents (score 0) and "highly relevant" documents (score 2); "relevant documents" (score 1) were thus discarded (except for those topics with no "highly relevant" documents).</p><p>For the parameters in the Rocchio formula (equation 1), we chose a uniform set of values: α=β=γ=1. This choice may have adversely affected the results about the utility of negative relevance feedback because negative terms are usually weighted with lower values than positive terms.</p><p>In Table <ref type="table" coords="3,361.33,135.96,6.00,10.63" target="#tab_0">1</ref> we show the performance, averaged over the set of 31 evaluation topics, of the nine runs submitted by FUB (the first half for method 1, the second for method 2). As a general remark, we would like to note that these values were quite good on an absolute scale, as our best run was ranked in the first positions of the official results of the track.</p><p>The most important findings shown in Table <ref type="table" coords="3,318.00,270.96,6.00,10.63" target="#tab_0">1</ref> are that E1 consistently achieved the best results across all evaluation measures, and that the retrieval effectiveness of E1 was roughly twice as high as the baseline (i.e., A1), with a record improvement for P10.</p><p>A topic by topic analysis reveals that an increase in retrieval effectiveness was obtained by each run for nearly all topics. In Figure <ref type="figure" coords="3,536.00,390.96,6.00,10.63">1</ref> we show the performance of the two runs with most feedback relevance documents (i. e., E1 and E2) on individual topics. For instance, considering E1, its application was detrimental to the retrieval effectiveness for only three topics. Recalling that E1 and E2 make use of 30 and 100 negative terms, respectively, and that E1 produced the same terms as those which would be produced by E2 if it were restricted to 30 terms, Figure <ref type="figure" coords="4,191.00,548.96,6.00,10.63">1</ref> also shows that increasing the number of negative terms did not result in a performance improvement. In fact, for the overwhelming majority of the topics (28 out of 31), the method with fewer negative terms was better.</p><p>In Figure <ref type="figure" coords="4,376.94,488.96,6.00,10.63">2</ref> we show how the retrieval effectiveness of method 1 varied as the size of the feedback documents grew. While E1 was, on average, the best method (see Table <ref type="table" coords="4,529.00,533.96,4.33,10.63" target="#tab_0">1</ref>), Figure <ref type="figure" coords="4,353.36,548.96,6.00,10.63">2</ref> shows that the relative performance on individual topics differed considerably. For instance, on topic 768 ("women in state legislatures"), E1 performed worse than the baseline while the other runs did better than the baseline, sometimes by a large amount. A related (somewhat unexpected) observation is that the methods with smaller amounts of feedback seem more robust than the method with the highest amount of feedback, in the sense that the application of the former methods almost never resulted in a decrease of retrieval performance over the baseline.</p><p>To gain more insights into the relationship between retrieval performance and amount of feedback information, we ran a more controlled experiment in which we considered how the retrieval performance varies as the number of (highly) p o s i t i v e feedback documents increases.</p><p>We set the maximum number of feedback documents to 50, because several topics did not have a large number of highly relevant feedback documents. We also let the number of positive expansion terms vary in the range between 0 and 100 (while the number of negative expansion terms was kept fixed at 30). The results are shown in Table <ref type="table" coords="5,245.00,420.96,4.50,10.63" target="#tab_2">3</ref>.</p><p>In general, these results show that an increase in the number of documents and/or in the number of terms positively affected the retrieval effectiveness. In particular, the best results were obtained for the largest values of both parameters.</p><p>To get a more informative view of this behavior, in Figure <ref type="figure" coords="5,164.46,570.96,6.00,10.63" target="#fig_0">3</ref> we plot the MAP values taken from Table <ref type="table" coords="5,416.00,75.96,6.00,10.63">2</ref> as a function of the number of documents, keeping the number of terms constant. Dually, in Figure <ref type="figure" coords="5,405.78,120.96,6.00,10.63" target="#fig_1">4</ref> we plot the MAP values as a function of the number of terms, keeping the number of documents constant.</p><p>Table <ref type="table" coords="5,343.85,239.30,3.77,8.86">2</ref>. Retrieval performance of relevance feedback for various combinations of the highly relevant documents and the number of expansion terms. documents terms 1 3 10 25 50 0 0,1091 0,1091 0,1091 0,1091 0,1091 1 0,1084 0,1088 0,1185 0,1145 0,1132 5 0,1293 0,1396 0,1535 0,1507 0,1531 10 0,1330 0,1539 0,1610 0,1697 0,1711 15 0,1401 0,1567 0,1676 0,1745 0,1811 20 0,1452 0,1621 0,1722 0,1853 0,1894 25 0,1513 0,1688 0,1782 0,1924 0,1973 30 0,1522 0,1684 0,1780 0,1940 0,1996 35 0,1532 0,1711 0,1801 0,1962 0,2047 40 0,1570 0,1732 0,1838 0,1991 0,2093 45 0,1568 0,1765 0,1841 0,2001 0,2101 50 0,1571 0,1762 0,1849 0,2024 0,2124 55 0,1577 0,1779 0,1862 0,2025 0,2133 60 0,1580 0,1800 0,1866 0,2038 0,2125 65 0,1583 0,1795 0,1868 0,2062 0,2135 70 0,1579 0,1798 0,1865 0,2060 0,2137 75 0,1568 0,1788 0,1845 0,2045 0,2121 80 0,1564 0,1787 0,1853 0,2028 0,2129 85 0,1562 0,1791 0,1845 0,2026 0,2132 90 0,1561 0,1791 0,1843 0,2022 0,2136 95 0,1558 0,1791 0,1842 0,2023 0,2138 100 0,1553 0,1788 0,1856 0,2025 0,2138  Figure <ref type="figure" coords="7,105.85,90.96,6.00,10.63" target="#fig_0">3</ref> and Figure <ref type="figure" coords="7,168.61,90.96,6.00,10.63" target="#fig_1">4</ref> confirm that the retrieval performance grew almost monotonically with respect to each parameter. These results differ from those reported in earlier studies about the low effect of the main relevance feedback parameters on retrieval performance (e.g., <ref type="bibr" coords="7,72.00,180.96,129.35,10.63" target="#b5">Salton and Buckley 1990)</ref>, while they seem more consistent with some recent findings <ref type="bibr" coords="7,72.00,210.96,86.92,10.63" target="#b6">(Wong et al. 2008)</ref>.</p><p>We then evaluated how relevance feedback compares to pseudo-relevance feedback. In Table <ref type="table" coords="7,102.74,270.96,6.00,10.63" target="#tab_2">3</ref> we report the MAP values of the two methods, averaged over the 31 topics, for several values of the number of relevant or pseudo-relevant documents (we set the number of expansion terms to 100). Clearly, the benefits of using truly relevant documents (instead of top retrieved documents) are more tangible as their number becomes large, although a marked difference is observable even when we consider only one or three documents.  <ref type="figure" coords="7,526.02,255.96,4.50,10.63" target="#fig_2">5</ref>.</p><p>Apparently, the performance of relevance feedback was not always better than pseudorelevance feedback. Indeed, there were a few topics where the latter method seemed more effective. Note however that due to the skewed distribution of highly relevant documents in the feedback data (from 0 to 181 documents for a topic), we had to turn to moderately relevant documents when there were not enough highly relevant documents.</p><p>We checked that the topics with the worst performance of the relevance feedback method were exactly those for which there was a paucity of highly relevant documents. Furthermore, these results were obtained for three documents and ten terms, which was one of the least effective parameter choices for the relevance feedback method (see Table <ref type="table" coords="7,519.21,540.96,4.28,10.63">2</ref>).</p><p>Finally, we have made some preliminary experiments to evaluate the utility of negative information. We observed a very limited improvement over using just positive expansion terms, but this may be due to our specific experimental conditions (e.g., ineffective values of β and γ in equation 1, small proportion of negative terms in comparison to the positive ones, etc.). This issue needs more work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The main conclusions that can be drawn from our experiments are the following.</p><p>1) The use of distribution-based scores within the Rocchio's formula was an effective relevance feedback method.</p><p>2) The performance of relevance feedback in general increased as the number of feedback documents and the number of expansion terms grew, even when the two parameters were taken in combination.</p><p>3) Other conditions being equal, the use of truly relevant documents resulted in a clear performance improvement over using pseudorelevance feedback, both in terms of mean retrieval effectiveness and robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,72.00,295.30,469.00,8.86;6,72.00,306.30,171.02,8.86;6,72.00,72.00,482.00,210.00"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Retrieval performance of relevance feedback as a function of the number of feedback documents, using the number of expansion terms as a parameter.</figDesc><graphic coords="6,72.00,72.00,482.00,210.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,72.00,641.30,469.00,8.86;6,72.00,652.30,226.03,8.86;6,72.00,367.00,408.00,261.00"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Retrieval performance of relevance feedback as a function of the number of (positive) expansion terms, using the number of feedback documents as a parameter.</figDesc><graphic coords="6,72.00,367.00,408.00,261.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,72.00,332.30,469.00,8.86;8,72.00,343.30,181.02,8.86;8,72.00,177.00,468.00,138.00"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Improvement of relevance feedback (method 1) 1 over pseudo relevance feedback on individual topics (3 relevant documents and 10 expansion terms).</figDesc><graphic coords="8,72.00,177.00,468.00,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,318.00,509.30,208.00,159.86"><head>Table 1 .</head><label>1</label><figDesc>Mean performance of submitted runs.</figDesc><table coords="3,318.00,525.30,208.00,143.86"><row><cell>Run</cell><cell>Map</cell><cell>R-prec</cell><cell>P10</cell></row><row><cell>FubRF08.A1</cell><cell>0,1091</cell><cell>0,1483</cell><cell>0,19</cell></row><row><cell>FubRF08.B1</cell><cell>0,1803</cell><cell>0,2016</cell><cell>0,32</cell></row><row><cell>FubRF08.C1</cell><cell>0,1873</cell><cell>0,2169</cell><cell>0,32</cell></row><row><cell>FubRF08.D1</cell><cell>0,2017</cell><cell>0,2298</cell><cell>0,35</cell></row><row><cell>FubRF08.E1</cell><cell>0,214</cell><cell>0,2463</cell><cell>0,44</cell></row><row><cell>FubRF08.A2</cell><cell>0,1091</cell><cell>0,1483</cell><cell>0,19</cell></row><row><cell>FubRF08.C2</cell><cell>0,1752</cell><cell>0,2008</cell><cell>0,31</cell></row><row><cell>FubRF08.D2</cell><cell>0,1857</cell><cell>0,2174</cell><cell>0,34</cell></row><row><cell>FubRF08.E2</cell><cell>0,173</cell><cell>0,2105</cell><cell>0,35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,89.79,19.06,438.99,386.40"><head>814 820 804 742 726 796 792 722 782 850 746 840 772 766 706 764 800 708 846 704 776 760 828 714 788 790 768 836 738 806</head><label></label><figDesc></figDesc><table coords="4,89.79,19.06,437.71,386.40"><row><cell></cell><cell cols="2">0,9000</cell></row><row><cell></cell><cell cols="2">0,8000</cell><cell>Baseline</cell></row><row><cell></cell><cell cols="2">0,7000</cell><cell>E1</cell></row><row><cell>Map</cell><cell cols="2">0,4000 0,5000 0,6000</cell><cell>E2</cell></row><row><cell></cell><cell cols="2">0,3000</cell></row><row><cell></cell><cell cols="2">0,2000</cell></row><row><cell></cell><cell cols="2">0,1000</cell></row><row><cell></cell><cell cols="2">0,0000</cell></row><row><cell></cell><cell></cell><cell cols="2">814 820 804 742 726 796 792 722 782 850 746 840 772 766 706 764 800 708 846 704 776 760 828 714 788 790 768 836 738 806 808</cell></row><row><cell></cell><cell></cell><cell>Topic</cell></row><row><cell></cell><cell></cell><cell>0,8000 0,9000</cell><cell>Baseline</cell></row><row><cell></cell><cell></cell><cell>0,7000</cell><cell>B1</cell></row><row><cell></cell><cell></cell><cell>0,6000</cell><cell>C1</cell></row><row><cell cols="2">Map</cell><cell>0,4000 0,5000</cell><cell>D1 E1</cell></row><row><cell></cell><cell></cell><cell>0,3000</cell></row><row><cell></cell><cell></cell><cell>0,2000</cell></row><row><cell></cell><cell></cell><cell>0,1000</cell></row><row><cell></cell><cell></cell><cell>0,0000</cell></row><row><cell></cell><cell></cell><cell>Topic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,90.96,470.33,529.20"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="7,72.00,482.30,224.01,137.86"><row><cell cols="3">. Retrieval performance of relevance feedback</cell></row><row><cell cols="3">versus pseudo-relevance feedback, averaged over the set</cell></row><row><cell>of topics)</cell><cell></cell><cell></cell></row><row><cell cols="2">Documents Pseudo-Rel</cell><cell>Relevance</cell></row><row><cell></cell><cell>Feedback</cell><cell>Feedback</cell></row><row><cell>1</cell><cell>0.1358</cell><cell>0.1553</cell></row><row><cell>3</cell><cell>0.1387</cell><cell>0.1788</cell></row><row><cell>10</cell><cell>0.1680</cell><cell>0.1856</cell></row><row><cell>25</cell><cell>0.1672</cell><cell>0.2025</cell></row><row><cell>50</cell><cell>0,1718</cell><cell>0.2138</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments.</head><p>We would like to thank <rs type="person">Gianni Amati</rs> for helping with Terrier.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,72.00,106.96,224.00,10.63;9,72.00,121.96,224.00,10.63;9,72.00,136.96,224.01,10.63;9,72.00,151.96,224.00,10.63;9,72.00,166.96,159.04,10.63" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,281.02,121.96,14.98,10.63;9,72.00,136.96,224.01,10.63;9,72.00,151.96,86.72,10.63">An information theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>De Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,172.18,151.96,123.82,10.63;9,72.00,166.96,96.01,10.63">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,192.96,223.99,10.63;9,72.00,207.96,224.00,10.63;9,72.00,222.96,183.01,10.63;9,72.00,237.96,46.00,10.63" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,171.56,192.96,124.44,10.63;9,72.00,207.96,224.00,10.63;9,72.00,222.96,54.59,10.63">Probabilistic models for information retrieval based on divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="9,72.00,267.96,223.99,10.63;9,72.00,282.96,223.99,10.63;9,72.00,297.96,224.00,10.63;9,72.00,312.96,224.01,10.63;9,72.00,327.96,224.00,10.63;9,72.00,342.96,224.00,10.63;9,72.00,357.96,224.01,10.63;9,72.00,372.96,90.00,10.63" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,200.94,297.96,95.06,10.63;9,72.00,312.96,224.01,10.63;9,72.00,327.96,90.92,10.63">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,168.11,327.96,127.89,10.63;9,72.00,342.96,224.00,10.63;9,72.00,357.96,109.69,10.63">_In Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,402.96,224.00,10.63;9,72.00,417.96,224.00,10.63;9,72.00,432.96,224.00,10.63;9,318.00,75.96,43.62,10.63;9,377.17,75.96,55.32,10.63;9,448.00,75.96,94.00,10.63;9,318.00,90.96,224.01,10.63;9,318.00,105.96,224.00,10.63;9,318.00,120.96,224.00,10.63;9,318.00,135.96,224.00,10.63;9,318.00,150.96,102.00,10.63" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,111.46,417.96,184.54,10.63;9,72.00,432.96,224.00,10.63;9,318.00,75.96,43.62,10.63;9,377.17,75.96,55.32,10.63;9,448.00,75.96,94.00,10.63;9,318.00,90.96,58.85,10.63">Comparing and Combining Methods for Automatic Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Perez-Aguera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,318.00,120.96,224.00,10.63;9,318.00,135.96,86.80,10.63">Advances in Natural Language Processing and Applications</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
	<note>Research in Computing Science</note>
</biblStruct>

<biblStruct coords="9,318.00,180.96,224.00,10.63;9,318.00,195.96,224.01,10.63;9,318.00,210.96,224.00,10.63;9,318.00,225.96,224.00,10.63;9,318.00,240.96,86.01,10.63" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,433.00,180.96,109.00,10.63;9,318.00,195.96,99.29,10.63">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,438.34,195.96,103.67,10.63;9,318.00,210.96,224.00,10.63;9,318.00,225.96,55.29,10.63">The SMART retrieval system -experiments in automatic document processing</title>
		<meeting><address><addrLine>Salton, G., Ed; Englewood Cliffs</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,318.00,270.96,224.00,10.63;9,318.00,285.96,224.00,10.63;9,318.00,300.96,224.00,10.63;9,318.00,315.96,220.00,10.63" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,318.00,285.96,224.00,10.63;9,318.00,300.96,96.69,10.63">Improving Retrieval Performance by Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,423.21,300.96,118.80,10.63;9,318.00,315.96,152.13,10.63">Journal of the American Society for Information Sciences</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,318.00,345.96,224.00,10.63;9,318.00,360.96,224.01,10.63;9,318.00,375.96,224.33,10.63;9,318.00,390.96,224.00,10.63;9,318.00,405.96,224.00,10.63;9,318.00,420.96,27.00,10.63" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,478.00,360.96,64.01,10.63;9,318.00,375.96,224.33,10.63;9,318.00,390.96,156.40,10.63">Re-examining the effects of adding relevance information in a relevance feedback environment</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">V</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,483.42,390.96,58.58,10.63;9,318.00,405.96,147.64,10.63">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1086" to="1116" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
