<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.08,80.32,417.79,16.65;1,229.92,108.76,152.19,16.65">UniNE at TREC 2008: Fact and Opinion Retrieval in the Blogsphere</title>
				<funder ref="#_MeezmH3">
					<orgName type="full">Swiss NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.20,141.38,75.08,11.10"><forename type="first">Claire</forename><surname>Fautsch</surname></persName>
							<email>claire.fautsch@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel Rue Emile-Argand</orgName>
								<address>
									<addrLine>11</addrLine>
									<postCode>CH-2009</postCode>
									<settlement>Neuchatel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.44,141.38,81.47,11.10"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel Rue Emile-Argand</orgName>
								<address>
									<addrLine>11</addrLine>
									<postCode>CH-2009</postCode>
									<settlement>Neuchatel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.08,80.32,417.79,16.65;1,229.92,108.76,152.19,16.65">UniNE at TREC 2008: Fact and Opinion Retrieval in the Blogsphere</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B7E4188D8D24F02D096F006E3A23095</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the Blog track at the TREC 2008 evaluation campaign. The Blog track goes beyond simple document retrieval, its main goal is to identify opinionated blog posts and assign a polarity measure (positive, negative or mixed) to these information items. Available topics cover various target entities, such as people, location or product for example. This year's Blog task may be subdivided into three parts: First, retrieve relevant information (facts &amp; opinionated documents), second extract only opinionated documents (either positive, negative or mixed) and third classify opinionated documents as having a positive or negative polarity.</p><p>For the first part of our participation we evaluate different indexing strategies as well as various retrieval models such as Okapi (BM25) and two models derived from the Divergence from Randomness (DFR) paradigm. For the opinion and polarity detection part, we use two different approaches, an additive and a logistic-based model using characteristic terms to discriminate between various opinion classes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the Blog track <ref type="bibr" coords="1,141.48,537.76,11.71,8.96" target="#b1">[1]</ref> the retrieval unit consists of permalink documents, which are URLs pointing to a specific blog entry. In contrast to a corpus extracted from scientific papers or a news collection, blogposts are more subjective in nature and contain several points of view on various domains. Written by different kinds of users, a post retrieved following the request "TomTom" for might contain factual information about the navigation system, such as software specifications for example, but it might also contain more subjective information about the product such as ease of use. The ultimate goal of the Blog track is to find opinionated documents rather than present a ranked list of relevant documents containing either objective (facts) or subjective (opinions) content. Thus, in a first step the system would retrieve a set of relevant documents but then a second step this set would be separated into two subsets, one containing the documents without any opinions (facts) and the second containing documents expressing positive, negative or mixed opinions on the target entity. Finally the mixed-opinion documents would be eliminated and the positive and negative opinionated documents separated. Later in this paper, the documents retrieved during the first step will be referenced as a baseline or factual retrieval.</p><p>The rest of this paper is organized as follows. Section 2 describes the main features of the test-collection used. Section 3 explains the indexing approaches used and Section 4 introduces the models used for factual retrieval. In Section 5 we explain our opinion and polarity detection algorithms. Section 6 evaluates the different approaches as well as our official runs. The principal findings of our experiments are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BLOG TEST-COLLECTION</head><p>The Blog test collection contains approximately 148 GB of uncompressed data, consisting of 4,293,732 documents extracted from three sources: 753,681 feeds (or 17.6%), 3,215,171 permalinks (74.9%) and 324,880 homepages (7.6%). Their sizes are as follows: 38.6 GB for feeds (or 26.1%), 88.8 GB for permalinks (60%) and 20.8 GB for the homepages (14.1%). Only the permalink part is used in this evaluation campaign. This corpus was crawled between Dec. 2005 and Feb. 2006 (for more information see: http://ir.dcs.gla.ac.uk/test_collections/). Figures <ref type="figure" coords="1,356.28,586.12,4.97,8.96">1</ref> and<ref type="figure" coords="1,388.68,586.12,4.97,8.96">2</ref> show two blog document examples, including the date, URL source and permalink structures at the beginning of each document. Some information extracted during the crawl is placed after the &lt;DOCHDR&gt; tag. Additional pertinent information is placed after the &lt;DATA&gt; tag, along with ad links, name sequences (e.g., authors, countries, cities) plus various menu or site map items. Finally some factual information is included, such as some locations where various opinions can be found. The data of interest to us follows the &lt;DATA&gt; tag. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INDEXING APPROACHES</head><p>We used two different indexing approaches to index documents and queries. As a first and natural approach we chose words as indexing units and their generation was done in three steps. First, the text is tokenized (using spaces or punctuation marks), hyphenated terms are broken up into their components and acronyms are normalized (e.g., U.S. is converted into US). Second, uppercase letters are transformed into their lowercase forms and third, stop words are filtered out using the SMART list (571 entries). Based on the result of our previous experiments within the Blog track <ref type="bibr" coords="3,259.20,392.68,11.71,8.96" target="#b2">[2]</ref> or Genomics search <ref type="bibr" coords="3,126.12,404.44,10.68,8.96" target="#b3">[3]</ref>, we decided not to use a stemming technique.</p><p>In its indexing units our second indexing strategy uses single words and also compound constructions, with the latter being those composed of two consecutive words.</p><p>For example in the Query #1037 "New York Philharmonic Orchestra" we generated the following indexing units after stopword elimination: "york," "philharmonic," "orchestra," "york philharmonic," "philharmonic orchestra" ("new" is included in the stoplist). We decided to use this given the large number of queries containing proper names or company names such as "David Irving" (#1042), "George Clooney" (#1050) or "Christianity Today" (#921) for example should be considered as one single entity for both indexing and retrieval. Once again we did not apply any stemming procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FACTUAL RETRIEVAL</head><p>The first step in the Blog task was factual retrieval. To create our baseline runs (factual retrieval) we used different single IR models as described in Section 4.1. To produce more effective ranked results lists we applied different blind query expansion approaches as discussed in Section 4.2. Finally, we merged different isolated runs using a data fusion approach as presented in Section 4.3. This final ranked list of retrieved items was used as our baseline (classical ad hoc search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single IR Models</head><p>We considered three probabilistic retrieval models for our evaluation. As a first approach we used the Okapi (BM25) model <ref type="bibr" coords="3,388.20,145.72,10.68,8.96" target="#b4">[4]</ref>, evaluating the document D i score for the current query Q by applying the following formula:</p><formula xml:id="formula_0" coords="3,342.24,177.49,186.94,45.48">[ ]<label>1 1 ( 1) ( , ) log , where (1 )</label></formula><p>( / )</p><formula xml:id="formula_1" coords="3,328.20,174.22,224.95,47.83">j j ij i j t q j ij i n df k tf Score D Q qtf df K tf K k b b l avdl ∈   - + ⋅ = ⋅ ⋅     +   = ⋅ -+ ⋅ ∑ (1)</formula><p>in which the constant avdl was fixed at 837 for the wordbased indexing and at 1622 for our compound-based indexing. For both indexes the constant b was set to 0.4 and k 1 to 1.4.</p><p>As a second approach, we implemented two models derived from the Divergence from Randomness (DFR) paradigm <ref type="bibr" coords="3,367.44,310.72,10.68,8.96" target="#b5">[5]</ref>. In this case, the document score was evaluated as:</p><formula xml:id="formula_2" coords="3,374.64,337.13,176.71,23.41">( , ) j i j ij t q Score D Q qtf w ∈ = ⋅ ∑<label>(2)</label></formula><p>where qtf j denotes the frequency of term t j in query Q, and the weight w ij of term t j in document D i was based on a combination of two information measures as follows:</p><formula xml:id="formula_3" coords="3,329.64,407.29,210.19,11.95">w ij = Inf 1 ij • Inf 2 ij = -log 2 [Prob 1 ij (tf)] • (1 -Prob 2 ij (tf))</formula><p>As a first model, we implemented the PB2 scheme, defined by the following equations:</p><formula xml:id="formula_4" coords="3,329.64,452.06,229.63,44.59">Inf 1 ij = -log 2 [(e -λ j • λ j tf ij )/tf ij !] with λ j = tc j / n (3) Prob 2 ij = 1 -[(tc j +1) / (df j • (tfn ij + 1))] with tfn ij = tf ij • log 2 [1 + ((c•mean dl) / l i )]</formula><p>(4) where tc j indicates the number of occurrences of term t j in the collection, l i the length (number of indexing terms) of document D i , mean dl the average document length (fixed at 837 for word-based respectively at 1622 for compoundbased indexing approach), n the number of documents in the corpus, and c a constant (fixed at 5).</p><p>For the second model PL2, the implementation of Prob 1 ij is given by Equation <ref type="formula" coords="3,417.84,593.92,3.76,8.96">3</ref>, and Prob 2 ij by Equation <ref type="formula" coords="3,536.64,593.92,3.76,8.96" target="#formula_5">5</ref>, as shown below:</p><formula xml:id="formula_5" coords="3,329.64,618.25,229.63,11.95">Prob 2 ij = tfn ij / (tfn ij + 1)<label>(5)</label></formula><p>where λ j and tfn ij were defined previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Expansion Approaches</head><p>In an effort to improve retrieval effectiveness, various query expansion techniques were suggested <ref type="bibr" coords="3,507.24,683.44,10.68,8.96" target="#b6">[6]</ref>, <ref type="bibr" coords="3,525.48,683.44,10.68,8.96" target="#b3">[3]</ref>, and in our case we chose two of them. The first uses a blind query expansion based on Rocchio's method <ref type="bibr" coords="3,508.56,706.96,10.68,8.96" target="#b7">[7]</ref>, wherein the system would add the top m most important terms extracted from the top k documents retrieved in the original query. As a second query expansion approach we used Wikipedia 1 to enrich those queries based on terms extracted from a source different from the blogs. The title of the original topic description was sent to Wikipedia and the ten most frequent words from the first retrieved article were added to the original query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combining Different IR Models</head><p>It was assumed that combining different search models would improve retrieval effectiveness, due to the fact that each document representation might retrieve pertinent items not retrieved by others. On the other hand, we might assume that an item retrieved by many different indexing and/or search strategies would have a greater chance of being relevant for the query submitted <ref type="bibr" coords="4,249.48,263.32,10.68,8.96">[8]</ref>, <ref type="bibr" coords="4,266.28,263.32,10.68,8.96" target="#b9">[9]</ref>.</p><p>To combine two or more single runs, we applied the Z-Score operator <ref type="bibr" coords="4,115.20,290.80,16.75,8.96" target="#b10">[10]</ref> defined as:</p><formula xml:id="formula_6" coords="4,69.60,314.61,211.63,29.94">j j k k j j j RSV Mean Z Score RSV Stdev δ   - - = +       ∑<label>(6)</label></formula><p>with δ i = ((Mean j -Min j ) / Stdev j )</p><p>In this formula, the final document score (or its retrieval status value RSV k ) for a given document D k is the sum of the standardized document score computed for all isolated retrieval systems. This later value was defined as the document score for the corresponding document D k achieved by the jth run (RSV k j ) minus the corresponding mean (denoted Mean j ) and divided by the standard deviation (denoted Stdev j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OPINION AND POLARITY DETECTION</head><p>Following the baseline retrieval, the goal was to separate the retrieved documents into two classes, namely opinionated and non-opinionated documents, and then in a subsequent step assign a polarity to the opinionated documents.</p><p>In our view, opinion and polarity detection are closely related. Thus, after performing the baseline retrieval, our system would automatically judge the first 1,000 documents retrieved. For each retrieved document the system may classify it as positive, negative, mixed or neutral (the underlying document contains only factual information). To achieve this we calculated a score for each possible outcome class (positive, negative, mixed, and neutral), and then the highest of these four scores determined the choice of a final classification. 1 http://www.wikipedia.org/ Then for each document in the baseline we looked up the document in the judged set to obtain its classification. If the document was not there it was classified as unjudged.</p><p>Documents classified as positive, mixed or negative were considered to be opinionated, while neutral and unjudged documents were considered as non-opinionated. This classification also gave the document's polarity (positive or negative).</p><p>To calculate the classification scores, we used two different approaches, both being based on Muller's method for identifying a text's characteristic vocabulary <ref type="bibr" coords="4,324.00,211.84,15.43,8.96" target="#b11">[11]</ref>, as described in Section 5.1. We then presented our two suggested approaches, the additive model in Section 5.2 and the logistic approach in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Characteristic Vocabulary</head><p>In Muller's approach the basic idea is to use Z-score (or standard score) to determine which terms can properly characterize a document, when compared to other documents. To do so we needed a general corpus denoted C, containing a documents subset S for which we wanted to identify the characteristic vocabulary. For each term t in the subset S we calculated a Z-Score by applying Equation <ref type="bibr" coords="4,362.52,353.56,10.68,8.96" target="#b7">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'</head><p>' Pr ob( ) ( ) ' Pr ob( ) (1 Pr ob( ))</p><formula xml:id="formula_7" coords="4,328.68,366.15,226.15,26.24">f n t Z Score t n t t - - = ⋅ ⋅ -<label>(7)</label></formula><p>where f' was the observed number of occurrences of the term t in the document set S, and n' the size of S. Prob(t) is the probability of the occurrence of the term t in the entire collection C. This probability can be estimated according to the Maximum Likelihood Estimation (MLE) principle as Prob(t) = f/n, with f being the number of occurrences of t in C and n the size of C. Thus in Equation <ref type="formula" coords="4,362.52,478.12,3.76,8.96" target="#formula_7">7</ref>, we compared the expected number of occurrences of term t according to a binomial process (mean = n' . Prob(t)) with the observed number of occurrences in the subset S (denoted f'). In this binomial process the variance is defined as n' . Prob(t) . (1-Prob(t)) and the corresponding standard deviation becomes the denominator of Equation <ref type="formula" coords="4,426.12,544.36,3.76,8.96" target="#formula_7">7</ref>.</p><p>Terms having a Z-score between -ε and +ε would be considered as general terms occurring with the same frequencies in both the entire corpus C and the subset S.</p><p>The constant ε represents a threshold limit that was fixed at 3 in our experiments. On the other hand, terms having an absolute value for the Z-score higher than ε are considered overused (positive Z-score) or underused (negative Z-score) compared to the entire corpus C. Such terms therefore may be used to characterize the subset S.</p><p>In our case, we created the whole corpus C using all 150 queries available. For each query the 1,000 first retrieved documents would be included in C. Using the relevance assessments available for these queries (queries #850 to #950), we created four subsets, based on positive, negative, mixed or neutral documents, and thus identified the characteristic vocabulary for each of these polarities. For each possible classification, we now had a set of characteristic terms with their Z-score.</p><p>Defining the vocabulary characterizing the four different classes in one step, and in the second step it is to compute an overall score, as presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Additive Model</head><p>In our first approach we used characteristic term statistics to calculate the corresponding polarity score for each document. The scores were calculated by applying following formulae: </p><formula xml:id="formula_8" coords="5,75.72,253.38,96.53,90.71"># _ # # # _ # # # _ # # # _ # #</formula><formula xml:id="formula_9" coords="5,102.24,256.00,186.91,88.74">= + = + = + = +<label>(8)</label></formula><p>in which #PosOver indicated the number of terms in the evaluated document that tended to be overused in positive documents (i.e. Z-score &gt; ε) while #PosUnder indicated the number of terms that tended to be underused in the class of positive documents (i.e. Z-score &lt; -ε). Similarly, we defined the variables #NegOver, #NegUnder, #MixOver, #MixUnder, #NeuOver, #NeuUnder, but for their respective categories, namely negative, mixed and neutral.</p><p>The idea behind this first model is simply assigning the category to each document for which the underlying document has relatively the largest sum of overused terms.</p><p>Usually, the presence of many overused terms belonging to a particular class is sufficient to assign this class to the corresponding document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Logistic Regression</head><p>As a second classification approach we used logistic regression <ref type="bibr" coords="5,98.16,570.52,16.75,8.96" target="#b12">[12]</ref> to combine different sources of evidence. For each possible classification, we built a logistic regression model based on twelve covariates and fitted them using training queries #850 to #950 (for which the relevant assessments were available). Four of the twelve covariates are SumPos, SumNeg, SumMix, SumNeu (the sum of the Z-scores for all overused and underused terms for each respective category). As additional explanatory variables, we also use the 8 variables defined in Section 5. </p><formula xml:id="formula_10" coords="5,330.60,100.28,228.67,39.80">∑ + ∑ = = = + + 12 1 0 12 1 0 1 ) ( i i i i i i x x e e x β β β β π (9)</formula><p>where β i are the coefficients obtained from the fitting and x i the variables. These coefficients reflect the relative importance of each explanatory variable in the final score.</p><p>For each document, we compute the π(x) corresponding to the four possible categories and for the final decision we need simply to classify the post according to the maximum π(x) value. This approach accounts for the fact that some explanatory variables may have more importance than others in assigning the correct category. We must recognize however that the length of the underlying document (or post) is not directly taken into account in our model.</p><p>Our underling assumption is that all documents have a similar number of indexing tokens. As a final step we could simplify our logistic model by ignoring explanatory variables having a coefficient estimate (β i ) close to zero and for which a statistical test cannot reject the hypotheses that the real coefficient β i = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>To evaluate our various IR schemes, we adopted mean average precision (MAP) computed by trec_eval software to measure the retrieval performance (based on a maximum of 1,000 retrieved records). As the Blog task is composed of three distinct subtasks, namely the ad hoc retrieval task, the opinion retrieval task and the polarity task, we will present these subtasks in the three following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline Ad hoc Retrieval Task</head><p>A first step in the Blog track was the ad hoc retrieval task, where participants were asked to retrieve relevant information about a specified target. These runs also served as baselines for opinion and polarity detection. In addition the organizers provided 5 more baseline runs to facilitate comparisons between the various participants' opinion and polarity detection strategies. We based our official runs on two different indexes (single words under the label "W" and compound construction under the label "comp." see Section 3) and on two different probabilistic models (see Section 4). We evaluated these different approaches under three query formulations, T (title only), TD (title and description) and TD + . In the latter case, the system received the same TD topic formulation as previously but during the query representation process the system built a phrase query from the topic description's title section. Table <ref type="table" coords="5,408.24,695.56,4.97,8.96" target="#tab_4">1</ref> shows the results and As shown in Table <ref type="table" coords="6,135.72,197.44,4.97,8.96" target="#tab_4">1</ref> the performance for the Okapi and the DFR schemes is almost the same, with the Okapi perhaps having a slight advantage. This table also shows that using compound indexing approach (word pairs) or phrase (from the title section of the query) increases the performance. This can be explained by the fact that in the underling test collection numerous queries contain statements that should appear together or close together in the retrieved documents, such as names (e.g. #892 "Jim Moran", #902 "Steve Jobs" or #931 "fort mcmurray") or concepts (e.g. #1041 "federal shield law"). Finally it can also be observed that adding the descriptive part (D) in the query formulation might improve the MAP.  <ref type="table" coords="6,122.76,527.32,3.76,8.96">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Okapi model with various blind query expansions</head><p>Table <ref type="table" coords="6,83.04,558.40,4.97,8.96">2</ref> shows that Rocchio's blind query expansion might slightly improve the results, but only if a small number of terms is considered. When adding a higher number of terms to the original query, the system tends to include more frequent terms such as navigational terms (e.g. "home", "back", "next") that are not related to the original topic formulation. The resulting MAP tends therefore to decrease. Using Wikipedia as an external source of potentially useful search terms only slightly improves the results (an average improvement of +2.75% on MAP). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Opinion retrieval</head><p>In this subtask participants were asked to retrieve blog posts expressing an opinion about a given entity and then to discard factual posts. The evaluation measure adopted for the MAP meant the system was to produce a ranked list of retrieved items. The opinion expressed could either be positive, negative or mixed. Our opinion retrieval runs were based on our two baselines described in Section 6.1 as well as on the five baselines provided by the organizers. To detect opinion we used two approaches: Z-Score (denoted Z in the following tables) and logistic regression (denoted LR). This resulted in a total of 14 official runs. Table <ref type="table" coords="6,405.96,557.80,4.97,8.96" target="#tab_9">5</ref> lists the top three results for each of our opinion detection approaches.</p><p>Compared to the baseline results shown in Table <ref type="table" coords="6,523.92,585.28,4.97,8.96" target="#tab_8">4</ref> (under the column "Opinion MAP"), adding our opinion detection approaches after the factual retrieval process tended to hurt the MAP performance. For example, the run UniNEBlog1 achieved a MAP of 0.320 without any opinion detection and only 0.309 when using our simple additive model (-3.4%) or 0.224 with our logistic approach (-30%).</p><p>This was probably due to the fact that during the opinion detection phase we removed all the documents judged by our system to be non-opinionated. Ignoring such documents thus produced a list clearly comprising less than 1,000 documents. Finally, Table <ref type="table" coords="7,208.32,86.32,4.97,8.96" target="#tab_9">5</ref> shows that having a better baseline also provides a better opinion run and that for opinion detection our simple additive model performed slightly better than the logistic regression approach (+36.47% on opinion MAP). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunName</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Polarity Task</head><p>In this third part of the Blog task, the system retrieved opinionated posts separated into a ranked list of positive and negative opinionated documents. Documents containing mixed opinions were not to be considered. The evaluation was done based on the MAP value, and separately for documents classified as positive and negative. As for the opinion retrieval task, we applied our two approaches in order to detect polarity in the baseline runs. Those documents that our system judged as belonging to either of the mixed or neutral categories were eliminated.</p><p>Table <ref type="table" coords="7,79.32,454.72,4.97,8.96" target="#tab_10">6</ref> lists the three best results (over 12 official runs) for each classification task. It is worth mentioning that for the positive classification task, we had 149 queries and for the negative opinionated detection only 142 queries provided at least one good response. The resulting MAP values were relatively low compared to the previous opinionated blog detection run (see Table <ref type="table" coords="7,222.72,525.28,3.63,8.96" target="#tab_9">5</ref>).</p><p>For our official runs using logistic regression, we did not classify the documents into four categories (positive, negative, mixed and neutral) but instead into only three (positive, negative, mixed). This meant that instead of calculating four polarity scores, we calculated only three and assigned polarity to the highest one. Table <ref type="table" coords="7,254.52,599.80,4.97,8.96" target="#tab_11">7</ref> shows the results for the logistic regression approach, with three (without neutral) and four (with neutral) classifications. Using only three classification categories instead of four had a positive impact on performance, as can be seen from an examination of Table <ref type="table" coords="7,432.96,339.04,4.97,8.96" target="#tab_11">7</ref> (logistic regression method only). Most documents classified as "neutral" in the fourclassification approach were then eliminated. When we considered only three categories, these documents were mainly classified as positive. This phenomenon also explains the differences in positive and negative MAP in our official runs when logistic regression was used (see Table <ref type="table" coords="7,349.44,421.36,3.63,8.96" target="#tab_10">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>During this TREC 2008 Blog evaluation campaign we evaluated various indexing and search strategies, as well as two different opinion and polarity detection approaches.</p><p>For the factual or ad hoc baseline retrieval we examined the underlying characteristics of this corpus with the compound indexing scheme that would hopefully improve precision measures. Compared to the standard approach in which isolated words were used as indexing units, in the MAP we obtained there was a +11.1% average increase for title only queries, as well as a +7.7% increase for title and description topic formulations. These results strengthen the assumption that for Blog queries such a precision-oriented feature could be useful. In further research, we might consider using longer tokens sequences as indexing unit, rather than just word pairs. Longer queries such as #1037 "New York Philharmonic Orchestra" or #1008 "UN Commission on Human Rights" might for example obtain better precision.</p><p>For the opinion and polarity tasks, we applied our two approaches to the given baselines as well as to two of our own baselines. We noticed that applying no opinion detection provides better results than applying any one of our detection approaches. This was partially due to the fact that during opinion detection we eliminated some documents, either because they were judged "neutral" or because they were not contained in the judged pool of documents ("unjudged").</p><p>In a further step we will try to rerank the baselines instead of simply removing documents judged as nonopinionated. A second improvement to our approach could be judging each document at the retrieval phase instead of first creating a pool of judged documents. In this case we would no longer have any documents classified as "unjudged" although more hardware resources would be required. Polarity detection basically suffers from the same problem as opinion detection. Finally, we can conclude that having a good factual baseline is the most important part of opinion and polarity detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.88,74.56,504.41,636.32"><head>Example of LexisNexis blog page</head><label></label><figDesc>They were created from this corpus and express user information needs extracted from the log of a commercial search engine blog. Some examples are shown in Figure3.</figDesc><table coords="2,53.88,83.14,327.44,570.80"><row><cell>&lt;DOC&gt;</cell></row><row><cell>&lt;DOCNO&gt; BLOG06-20051212-051-0007599288</cell></row><row><cell>&lt;DATE_XML&gt; 2005-10-06T14:33:40+0000</cell></row><row><cell>&lt;FEEDNO&gt; BLOG06-feed-063542</cell></row><row><cell>&lt;FEEDURL&gt; http://</cell></row><row><cell>contentcentricblog.typepad.com/ecourts/index.rdf</cell></row><row><cell>&lt;PERMALINK&gt;</cell></row><row><cell>http://contentcentricblog.typepad.com/ecourts/20</cell></row><row><cell>05/10/efiling_launche.html#</cell></row><row><cell>&lt;DOCHDR&gt; …</cell></row><row><cell>Date: Fri, 30 Dec 2005 06:23:55 GMT</cell></row><row><cell>Accept-Ranges: bytes</cell></row><row><cell>Server: Apache</cell></row><row><cell>Vary: Accept-Encoding,User-Agent</cell></row><row><cell>Content-Type: text/html; charset=utf-8</cell></row><row><cell>…</cell></row><row><cell>&lt;DATA&gt;</cell></row><row><cell>electronic Filing &amp;amp; Service for Courts</cell></row><row><cell>…</cell></row><row><cell>October 06, 2005</cell></row><row><cell>eFiling Launches in Canada</cell></row><row><cell>Toronto, Ontario, Oct.03 /CCNMatthews/ -</cell></row><row><cell>LexisNexis Canada Inc., a leading provider of</cell></row><row><cell>comprehensive and authoritative legal, news, and</cell></row><row><cell>business information and tailored applications</cell></row><row><cell>to legal and corporate researchers, today</cell></row><row><cell>announced the launch of an electronic filing</cell></row><row><cell>pilot project with the Courts</cell></row><row><cell>…</cell></row><row><cell>Figure 1. &lt;DOC&gt;</cell></row><row><cell>&lt;DOCNO&gt; BLOG06-20060212-023-0012022784</cell></row><row><cell>&lt;DATE_XML&gt; 2006-02-10T19:08:00+0000</cell></row><row><cell>&lt;FEEDNO&gt; BLOG06-feed-055676</cell></row><row><cell>&lt;FEEDURL&gt; http://</cell></row><row><cell>lawprofessors.typepad.com/law_librarian_blog/ind</cell></row><row><cell>ex.rdf#</cell></row><row><cell>&lt;PERMALINK&gt;</cell></row><row><cell>http://lawprofessors.typepad.com/law_librarian_b</cell></row><row><cell>log/2006/02/free_district_c.html#</cell></row><row><cell>&lt;DOCHDR&gt; …</cell></row><row><cell>Connection: close</cell></row><row><cell>Date: Wed, 08 Mar 2006 14:33:59 GMT … &lt;DATA&gt; Law Librarian Blog Blog Editor Joe Hodnicki Associate Director for Library Operations Univ. of Cincinnati Law Library … News from PACER : In the spirit of the E-Government Act of 2002, modifications have been made to the District Court CM/ECF system to provide PACER customers with access to written opinions free of charge The modifications also allow PACER customers to search for written opinions using a new report that is free of charge. Written opinions have been defined by the Judicial Conference as any document issued by a judge or judges of the court sitting in that capacity, that sets forth a reasoned explanation for a court's decision. Figure 3.</cell></row></table><note coords="2,279.48,647.62,4.81,6.32;2,92.40,658.96,156.91,8.96;2,53.88,678.40,234.15,8.96;2,53.88,690.16,234.29,8.96;2,53.88,701.92,234.05,8.96;2,324.00,74.56,107.73,8.96"><p>… Figure 2. Example of blog document During this evaluation campaign a set of 50 new topics (Topics #1001 to #1050) as well as 100 old topics from 2006 and 2007 (respectively Topics #851 to #900 and #901 to #950) were used.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,324.00,133.18,234.19,585.74"><head>Three examples of Blog track topics</head><label></label><figDesc></figDesc><table coords="2,324.00,133.18,234.19,585.74"><row><cell>value = 2), mixed (relevance value = 3) or positive</cell><cell></cell></row><row><cell>(relevance value = 4). From an analysis of negative</cell><cell></cell></row><row><cell>opinions only (relevance value = 2), we found 8,340</cell><cell></cell></row><row><cell>correct answers (mean: 54.08; median: 33; min: 0; max:</cell><cell></cell></row><row><cell>533; standard deviation: 80.20). For positive opinions</cell><cell></cell></row><row><cell>only (relevance value = 4), we found 10,457 correct</cell><cell>&lt;num&gt; Number: 851</cell></row><row><cell>answers (mean: 66.42, median: 46; min: 0; max: 392; standard deviation: 68.99). Finally for mixed opinions only (relevance value = 3), we found 8,530 correct</cell><cell>&lt;title&gt; "March of the Penguins" &lt;desc&gt; Description: Provide opinion of the film documentary "March of the Penguins".</cell></row><row><cell>answers (mean: 55.48; median: 23; min: 0; max: 455;</cell><cell>&lt;narr&gt; Narrative:</cell></row><row><cell>standard deviation: 82.33). Thus it seems that the test collection tends to contain, in mean, more positive opinions (mean 66.42) than it does either mixed (mean:</cell><cell>Relevant documents should include opinions concerning the film documentary "March of the Penguins". Articles or comments about penguins outside the context of this film</cell></row><row><cell>55.48) or negative opinions (mean: 54.08) related to the</cell><cell>documentary are not relevant.</cell></row><row><cell>target entity.</cell><cell>&lt;num&gt; Number: 941</cell></row><row><cell></cell><cell>&lt;title&gt; "teri hatcher"</cell></row><row><cell></cell><cell>&lt;desc&gt; Description:</cell></row><row><cell></cell><cell>Find opinions about the actress Teri</cell></row><row><cell></cell><cell>Hatcher.</cell></row><row><cell></cell><cell>&lt;narr&gt; Narrative:</cell></row><row><cell></cell><cell>All statements of opinion regarding the</cell></row><row><cell></cell><cell>persona or work of film and television</cell></row><row><cell></cell><cell>actress Teri Hatcher are relevant.</cell></row><row><cell></cell><cell>&lt;num&gt; Number: 1040</cell></row><row><cell></cell><cell>&lt;title&gt; TomTom</cell></row><row><cell></cell><cell>&lt;desc&gt; Description:</cell></row><row><cell></cell><cell>What do people think about the TomTom GPS</cell></row><row><cell></cell><cell>navigation system?</cell></row><row><cell></cell><cell>&lt;narr&gt; Narrative:</cell></row><row><cell></cell><cell>How well does the TomTom GPS navigation</cell></row><row><cell></cell><cell>system meets the needs of its users?</cell></row><row><cell></cell><cell>Discussion of innovative features of the</cell></row><row><cell></cell><cell>system, whether designed by the</cell></row><row><cell></cell><cell>manufacturer or adapted by the users, are</cell></row><row><cell></cell><cell>relevant.</cell></row><row><cell></cell><cell>Based on relevance assessments (relevant facts &amp;</cell></row><row><cell></cell><cell>opinions, or relevance value ≥ 1) made on this test</cell></row><row><cell></cell><cell>collection, we listed 43,813 correct answers. The mean</cell></row><row><cell></cell><cell>number of relevant web pages per topic is 285.11</cell></row><row><cell></cell><cell>(median: 240.5; standard deviation: 222.08). Topic</cell></row><row><cell></cell><cell>#1013 ("Iceland European Union") returned the minimal</cell></row><row><cell></cell><cell>number of pertinent passages (12) while Topic #872</cell></row><row><cell></cell><cell>("brokeback mountain") produced the greatest number of</cell></row><row><cell></cell><cell>relevant passages (950).</cell></row><row><cell></cell><cell>Based on opinion-based relevance assessments (2 ≤</cell></row><row><cell></cell><cell>relevance value ≤ 4), we found 27,327 correct opinionated</cell></row><row><cell></cell><cell>posts. The mean number of relevant web pages per topic</cell></row><row><cell></cell><cell>is 175.99 (median: 138; standard deviation: 169.66).</cell></row><row><cell></cell><cell>Topic #877 ("sonic food industry"), Topic #910 ("Aperto</cell></row><row><cell></cell><cell>Networks") and Topic #950 ("Hitachi Data Systems")</cell></row><row><cell></cell><cell>returned a minimal number of pertinent passages (4) while</cell></row><row><cell></cell><cell>Topic #869 ("Muhammad cartoon") produced the greatest</cell></row><row><cell></cell><cell>number of relevant posts (826).</cell></row><row><cell></cell><cell>The opinion referring to the target entity and contained in</cell></row><row><cell></cell><cell>a retrieved blogpost may be negative (relevance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,324.00,695.56,234.09,20.72"><head>Table 1 . MAP of different IR models (ad hoc search) (Blog, T &amp; TD query formulations)</head><label>1</label><figDesc>Table 2 the results of our two different query expansion techniques.</figDesc><table coords="6,45.12,78.85,248.93,79.31"><row><cell>Model</cell><cell>T</cell><cell></cell><cell></cell><cell>TD</cell><cell>TD +</cell></row><row><cell></cell><cell>comp.</cell><cell>W</cell><cell>comp.</cell><cell>W</cell><cell>comp.</cell><cell>W</cell></row><row><cell cols="2">Okapi 0.374</cell><cell>0.337</cell><cell>0.403</cell><cell>0.372</cell><cell cols="2">0.400 0.390</cell></row><row><cell>PL2</cell><cell>0.368</cell><cell>0.336</cell><cell>0.398</cell><cell>0.378</cell><cell cols="2">0.396 0.392</cell></row><row><cell>PB2</cell><cell>0.362</cell><cell>0.321</cell><cell>0.394</cell><cell>0.358</cell><cell cols="2">0.374 0.380</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,53.88,74.56,504.27,637.88"><head>Table 3</head><label>3</label><figDesc>lists our two official baseline runs for the Blog track and Table4the MAP for both the topic (or ad hoc) search and opinion search for our two official baseline runs, as well as for the additional five baseline runs provided by the organizers.</figDesc><table coords="6,326.88,116.32,225.17,94.16"><row><cell>Run Name</cell><cell cols="3">Query Index Model</cell><cell>Expansion</cell></row><row><cell></cell><cell>T</cell><cell cols="2">comp. Okapi</cell><cell>Rocc. 5/20</cell></row><row><cell>UniNEBlog1</cell><cell>TD</cell><cell>comp.</cell><cell>PL2</cell><cell>none</cell></row><row><cell></cell><cell>TD +</cell><cell>W</cell><cell>PB2</cell><cell>none</cell></row><row><cell></cell><cell>T</cell><cell cols="2">comp. Okapi</cell><cell>Wikipedia</cell></row><row><cell>UniNEBlog2</cell><cell>T</cell><cell cols="2">comp. Okapi</cell><cell>Rocc. 5/10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,327.96,218.68,226.23,155.72"><head>Table 3 . Description of our two official baseline runs for ad hoc search</head><label>3</label><figDesc></figDesc><table coords="6,347.16,256.24,187.81,118.16"><row><cell>Run Name</cell><cell>Topic MAP</cell><cell>Opinion MAP</cell></row><row><cell>UniNEBlog1</cell><cell>0.424</cell><cell>0.320</cell></row><row><cell>UniNEBlog2</cell><cell>0.402</cell><cell>0.306</cell></row><row><cell>Baseline 1</cell><cell>0.370</cell><cell>0.263</cell></row><row><cell>Baseline 2</cell><cell>0.338</cell><cell>0.265</cell></row><row><cell>Baseline 3</cell><cell>0.424</cell><cell>0.320</cell></row><row><cell>Baseline 4</cell><cell>0.477</cell><cell>0.354</cell></row><row><cell>Baseline 5</cell><cell>0.442</cell><cell>0.314</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,331.08,377.20,219.99,20.60"><head>Table 4 . Ad hoc topic and opinion relevancy results for baseline runs</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="7,55.20,155.56,229.13,135.20"><head>Table 5 . MAP of both ad hoc search and opinion detection</head><label>5</label><figDesc></figDesc><table coords="7,55.20,155.56,229.13,110.36"><row><cell></cell><cell>Baseline</cell><cell>Topic</cell><cell>Opinion</cell></row><row><cell cols="2">UniNEopLR1 UniNEBlog1</cell><cell>0.230</cell><cell>0.224</cell></row><row><cell>UniNEopLRb4</cell><cell>baseline 4</cell><cell>0.228</cell><cell>0.228</cell></row><row><cell cols="2">UniNEopLR2 UniNEBlog2</cell><cell>0.220</cell><cell>0.212</cell></row><row><cell>UniNEopZ1</cell><cell>UniNEBlog1</cell><cell>0.393</cell><cell>0.309</cell></row><row><cell>UniNEopZb4</cell><cell>baseline 4</cell><cell>0.419</cell><cell>0.327</cell></row><row><cell>UniNEopZ2</cell><cell>UniNEBlog2</cell><cell>0.373</cell><cell>0.296</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="7,324.12,79.00,233.61,202.64"><head>Table 6 . MAP evaluation for polarity detection</head><label>6</label><figDesc></figDesc><table coords="7,324.12,79.00,233.61,202.64"><row><cell>RunName</cell><cell cols="2">Baseline</cell><cell>Positive</cell><cell>Negative</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell>MAP</cell></row><row><cell>UniNEpolLRb4</cell><cell cols="2">baseline 4</cell><cell>0.102</cell><cell>0.055</cell></row><row><cell cols="3">UniNEpolLR1 UniNEBlog1</cell><cell>0.103</cell><cell>0.057</cell></row><row><cell>UniNEpolLRb5</cell><cell cols="2">baseline 5</cell><cell>0.102</cell><cell>0.055</cell></row><row><cell>UniNEpolZb5</cell><cell cols="2">baseline 4</cell><cell>0.070</cell><cell>0.061</cell></row><row><cell>UniNEpolZ5</cell><cell cols="2">baseline 5</cell><cell>0.067</cell><cell>0.058</cell></row><row><cell>UniNEpolZ3</cell><cell cols="2">baseline 3</cell><cell>0.067</cell><cell>0.063</cell></row><row><cell>Baseline</cell><cell cols="2">With neutral</cell><cell cols="2">Without neutral</cell></row><row><cell></cell><cell cols="4">Positive Negative Positive negative</cell></row><row><cell>UniNEBlog1</cell><cell>0.065</cell><cell>0.046</cell><cell>0.103</cell><cell>0.057</cell></row><row><cell>UniNEBlog2</cell><cell>0.064</cell><cell>0.042</cell><cell>0.102</cell><cell>0.051</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="7,330.60,284.44,220.89,20.60"><head>Table 7 . Logistic regression approach with three or four classifications</head><label>7</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors would also like to thank the <rs type="institution">TREC Blog task organizers</rs> for their efforts in developing this specific testcollection. This research was supported in part by the <rs type="funder">Swiss NSF</rs> under Grant #<rs type="grantNumber">200021-113273</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MeezmH3">
					<idno type="grant-number">200021-113273</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,58.38,379.52,92.28,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.88,396.88,215.84,8.96;8,68.04,408.64,212.69,8.96;8,68.04,420.40,206.09,8.96;8,68.04,432.16,166.76,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,144.48,408.64,136.25,8.96;8,68.04,420.40,18.67,8.96">Overview of the TREC-2006 blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gilad Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<idno>NIST Publication #500-272</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,106.32,420.40,108.21,8.96">Proceedings of TREC-2006</title>
		<meeting>TREC-2006<address><addrLine>Gaithersburg (MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,71.88,447.88,215.55,8.96;8,68.04,459.64,168.89,8.96;8,68.04,471.40,218.72,8.96;8,68.04,483.16,104.96,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,165.84,447.88,121.59,8.96;8,68.04,459.64,151.10,8.96">IR-Specific Searches at TREC 2007: Genomics &amp; Blog Experiments</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fautsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,68.04,471.40,214.17,8.96">Proceedings TREC-2007, NIST publication #500-274</title>
		<meeting>TREC-2007, NIST publication #500-274<address><addrLine>Gaithersburg (MD)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,74.56,180.12,8.96;8,338.16,86.32,204.89,8.96;8,338.16,98.08,212.61,8.96;8,338.16,109.84,88.05,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,433.68,74.56,88.44,8.96;8,338.16,86.32,204.89,8.96;8,338.16,98.08,41.42,8.96">Searching in Medline: Stemming, Query Expansion, and Manual Indexing Evaluation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,388.80,98.08,156.98,8.96">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="781" to="789" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,125.56,177.33,8.96;8,338.16,137.32,199.17,8.96;8,338.16,149.08,214.05,8.96;8,338.16,160.84,30.93,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,338.16,137.32,193.56,8.96">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,338.16,149.08,156.98,8.96">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,176.56,190.89,8.96;8,338.16,188.32,206.09,8.96;8,338.16,200.08,214.11,8.96;8,338.16,211.84,187.65,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,482.28,176.56,50.61,8.96;8,338.16,188.32,206.09,8.96;8,338.16,200.08,127.91,8.96">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,475.68,200.08,76.59,8.96;8,338.16,211.84,92.79,8.96">ACM-Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,227.41,207.83,8.53;8,338.16,238.57,211.35,8.53;8,338.16,249.73,14.33,8.53" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,413.40,227.41,62.70,8.53">Query expansion</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,482.64,227.41,67.19,8.53;8,338.16,238.57,138.86,8.53">Annual Review of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="121" to="187" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,264.85,196.25,8.53;8,338.16,276.01,198.11,8.53;8,338.16,287.17,219.09,8.53;8,338.16,298.33,34.29,8.53" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,404.88,264.85,133.37,8.53;8,338.16,276.01,33.34,8.53">Relevance feedback in information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.88,276.01,83.39,8.53;8,338.16,287.17,24.66,8.53">The SMART Retrieval System</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs (NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,313.60,191.95,8.96;8,338.16,325.36,209.23,8.96;8,338.16,337.12,17.61,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,460.20,313.60,73.75,8.96;8,338.16,325.36,86.52,8.96">Fusion via a linear combination of scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,433.68,325.36,41.47,8.96">IR Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,352.84,197.61,8.96;8,338.16,364.60,194.45,8.96;8,338.16,376.36,205.29,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,441.48,352.84,98.13,8.96;8,338.16,364.60,32.18,8.96">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,390.24,364.60,82.77,8.96;8,365.88,376.36,109.29,8.96">NIST Publication #500-215</title>
		<meeting><address><addrLine>Gaithersburg (MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
	<note>Proceedings TREC-2</note>
</biblStruct>

<biblStruct coords="8,342.00,392.08,178.51,8.96;8,338.16,403.84,215.35,8.96;8,338.16,415.60,74.61,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,383.28,392.08,137.23,8.96;8,338.16,403.84,141.39,8.96">Combining Multiple Strategies for Effective Cross-Language Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,488.52,403.84,41.47,8.96">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="121" to="148" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,431.32,186.33,8.96;8,338.16,443.08,165.21,8.96" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,388.32,431.32,140.01,8.96;8,338.16,443.08,29.84,8.96">Principe et methodes de statistique lexicale</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Muller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Honoré Champion</publisher>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,342.00,458.80,170.73,8.96;8,338.16,470.56,198.69,8.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,446.88,458.80,65.85,8.96;8,338.16,470.56,42.62,8.96">Applied Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hosmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Leneshow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley Interscience</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
