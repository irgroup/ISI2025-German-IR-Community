<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.02,83.76,311.68,15.48;1,177.89,105.68,253.93,15.48">Incorporating Non-Relevance Information in the Estimation of Query Models</title>
				<funder ref="#_WRHTmXj">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_qxWwEEp #_5MxVfCu #_kaw4J4x #_vJTHM3a #_TQcQSvx">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_SExnwCf">
					<orgName type="full">Dutch Ministry of Education, Culture and Science</orgName>
				</funder>
				<funder ref="#_HhakhD9">
					<orgName type="full">Virtual Laboratory for e-Science project</orgName>
				</funder>
				<funder ref="#_XMj975Q">
					<orgName type="full">Dutch and Flemish Governments</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.22,138.20,58.77,10.75"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.95,138.20,97.60,10.75"><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.50,138.20,42.85,10.75"><forename type="first">Jiyin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.30,138.20,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.02,83.76,311.68,15.48;1,177.89,105.68,253.93,15.48">Incorporating Non-Relevance Information in the Estimation of Query Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB3A7F1FF22DF0F35A246BE596997FE1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of the University of Amsterdam's ILPS group in the relevance feedback track at TREC 2008. We introduce a new model which incorporates information from relevant and non-relevant documents to improve the estimation of query models. Our main findings are twofold: (i) in terms of statMAP, a larger number of judged non-relevant documents improves retrieval effectiveness and (ii) on the TREC Terabyte topics, we can effectively replace the estimates on the judged non-relevant documents with estimations on the document collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our participation in the relevance feedback track this year, our goal was to explicitly incorporate non-relevance information in the estimation of query models. Working with the language modeling approach to information retrieval, we base our model of non-relevant information on the Normalized Log Likelihood Ratio.</p><p>We discuss related work in Section 2, describe our retrieval approach in Section 3, and detail our model for capturing non-relevance in Section 4. In Section 5 we describe our runs; we then present our results in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our chief aim for participating in this year's TREC Relevance Feedback track is to extend previous approaches, such as the one proposed by <ref type="bibr" coords="1,151.48,603.23,108.00,8.64" target="#b6">Lavrenko and Croft (2001)</ref>, by explicitly incorporating non-relevance information. Such negative evidence is usually assumed to be implicit, i.e. in the case of estimating a model from some (pseudo-)relevant data, the absence of terms indicates their non-relevance status. This means, in a language modeling setting and for the sets of relevant documents R and non-relevant documents R, P(t|θ R) = 1 -P(t|θ R ). The TREC Relevance Feedback track gives us the opportunity to develop and evaluate models which explicitly capture non-relevance information and we participated to answer the following research questions. Can non-relevance information be effectively modeled to improve the estimation of a query model? Given our model, what is the effect of the relative size of the set of nonrelevant documents with respect to the relevant documents on retrieval effectiveness? And, finally, we ask the question whether and when explicit non-relevance information helps. In other words, what are the effects when we substitute the estimates on the non-relevant documents with more general estimates, such as from the collection. Some previous work has already experimented with using negative weights for non-relevance information, either in an ad-hoc or more principled fashion, with mixed results <ref type="bibr" coords="1,452.79,356.00,60.66,8.64" target="#b1">(Dunlop, 1997;</ref><ref type="bibr" coords="1,515.73,356.00,40.18,8.64" target="#b2">Ide, 1971;</ref><ref type="bibr" coords="1,316.81,367.96,72.53,8.64" target="#b8">Wang et al., 2008;</ref><ref type="bibr" coords="1,391.83,367.96,71.83,8.64" target="#b9">Wong et al., 2008)</ref>.</p><p>The model we propose leverages the distance between each relevant document and the set of non-relevant documents, by penalizing terms that occur frequently in the latter, similar to the intuitions described by <ref type="bibr" coords="1,479.74,415.78,72.02,8.64" target="#b8">Wang et al. (2008)</ref>. Instead of subtracting probabilities, however, we take a more principled approach based on the Normalized Log Likelihood Ratio (NLLR). Moreover, similar to other pseudorelevance feedback approaches, such as the one proposed by <ref type="bibr" coords="1,329.46,475.55,106.26,8.64" target="#b6">Lavrenko and Croft (2001)</ref>, we reward terms that appear frequently in the individual relevant documents. Although the NLLR is not a true distance between distributions (since it does not satisfy the triangle equality), we consider it to be a useful candidate for measuring the (dis)similarity between two probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieval Framework</head><p>We employ a language modeling approach to IR and rank documents by their log-likelihood of being relevant given a query. Without presenting details here we only provide our final formula for ranking documents, and refer the reader to <ref type="bibr" coords="1,316.81,641.36,77.20,8.64" target="#b0">(Balog et al., 2008)</ref> for the steps of deriving this equation:</p><formula xml:id="formula_0" coords="1,327.76,663.82,228.16,10.37">log P(D|Q) ∝ log P(D) + ∑ t∈Q P(t|θ Q ) • log P(t|θ D ). (1)</formula><p>Here, both documents and queries are represented as multinomial distributions over terms in the vocabulary, and are referred to as document model (θ D ) and query model (θ Q ), respectively. The third component of our ranking model is the document prior (P(D)), which is assumed to be uniform. Note that by using uniform priors, Eq. 1 gives the same ranking as scoring documents by measuring the KLdivergence between the query model θ Q and each document model θ D , in which the divergence is negated for ranking purposes <ref type="bibr" coords="2,91.71,129.01,99.76,8.64" target="#b5">(Lafferty and Zhai, 2001)</ref>.</p><p>Unless indicated otherwise, we estimate each document model by:</p><formula xml:id="formula_1" coords="2,84.86,174.66,208.05,9.72">P(t|θ D ) = (1 -λ D ) • P(t|D) + λ D • P(t),<label>(2)</label></formula><p>where λ D is a parameter by that we use to tune the amount of smoothing. P(t|D) indicates the maximum likelihood estimate (MLE) of term t on a document, i.e., P(t|D) = n(t, D)/ ∑ t n(t , D), and P(t) the MLE on the collection C:</p><formula xml:id="formula_2" coords="2,106.56,254.78,186.34,22.18">P(t) = P(t|C) = ∑ D n(t, D) |C| .<label>(3)</label></formula><p>As to the query model θ Q , we adopt the common approach to linearly interpolate the initial query with an expanded part <ref type="bibr" coords="2,71.44,313.90,75.62,8.64" target="#b0">(Balog et al., 2008;</ref><ref type="bibr" coords="2,149.21,313.90,81.00,8.64" target="#b4">Kurland et al., 2005;</ref><ref type="bibr" coords="2,232.36,313.90,60.54,8.64" target="#b7">Rocchio, 1971;</ref><ref type="bibr" coords="2,53.80,325.85,96.67,8.64" target="#b10">Zhai and Lafferty, 2001)</ref>:</p><formula xml:id="formula_3" coords="2,77.97,345.54,214.93,11.78">P(t|θ Q ) = λ Q • P(t| θQ ) + (1 -λ Q ) • P(t|Q),<label>(4)</label></formula><p>where P(t|Q) indicates the MLE on the initial query and the parameter λ Q controls the amount of interpolation. The main goal of our participation is to find ways of improving the query model θQ using (non-)relevance information.</p><p>4 Modeling Non-Relevance <ref type="bibr" coords="2,53.80,462.11,53.53,8.64" target="#b3">Kraaij (2004)</ref> defines the NLLR measure as being equivalent to determining the negative KL-divergence for document retrieval. It is formulated as:</p><formula xml:id="formula_4" coords="2,86.23,507.76,206.67,9.72">NLLR(Q|D) = H(θ Q , θ C ) -H(θ Q , θ D ),<label>(5)</label></formula><p>where H(θ, θ ) is the cross-entropy between two multinomial language models:</p><formula xml:id="formula_5" coords="2,87.81,563.84,169.88,87.10">H(θ, θ ) = H(θ) + KL(θ||θ ) = -∑ t P(t|θ) log P(t|θ) + ∑ t P(t|θ) log P(t|θ) P(t|θ ) = -∑ t P(t|θ) log P(t|θ ).</formula><p>Eq. 5 can be interpreted as the relationship between two language models θ Q and θ D , normalized by a third language model θ C (these three models are estimated using Eq. 4, Eq. 2, and Eq. 3 respectively). The NLLR is a measure of average surprise; the better a document model 'fits' a query distribution, the higher the score will be; H(θ Q , θ D ) will be smaller than H(θ Q , θ C ) for relevant documents. In other words, the smaller the cross entropy between the query and document model (i.e., when the document language model better fits the observations from the query language model), the higher it will be ranked.</p><p>Based on the NLLR measure, we have developed the following model by which we estimate P(t| θQ ) in Eq. 4. The intuition is to determine for each term, the probability that it was sampled from each relevant document as well as the probability that it was sampled from the set of non-relevant documents:</p><formula xml:id="formula_6" coords="2,363.87,207.53,145.00,20.40">P(t| θQ ) ∝ ∑ D∈R P(t|θ D )P(θ D |θ R ),</formula><p>We weigh each term by the distance between R and R and its importance in the current document by setting:</p><formula xml:id="formula_7" coords="2,364.61,273.24,191.31,23.63">P(θ D |θ R ) = NLLR(D|R) ∑ D NLLR(D |R) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8" coords="2,319.12,328.08,236.79,69.49">NLLR(D|R) = H(θ D , θ R) -H(θ R , θ D ) (7) = ∑ t P(t|θ D ) log P(t|θ R ) P(t|θ R) = ∑ t P(t|θ D ) log (1 -δ 1 )P(t|R) + δ 1 P(t) (1 -δ 2 )P(t| R) + δ 2 P(t) .</formula><p>The δ parameters provide us with the means to control the individual influence of each set of relevant and non-relevant documents versus a background model. P(t|R) and P(t| R) are estimated by considering the MLE on the documents in the respective set, i.e., for the set of relevant documents R:</p><formula xml:id="formula_9" coords="2,379.60,477.75,93.61,22.18">P(t|R) = ∑ D∈R P(t|D) |R| .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runs</head><p>We submitted 2 runs, each consisting of 5 separate runs (one for each set of provided relevance judgements). The capital letters in each run indicate the relevance judgements per topic used for that run: (A) no relevance judgements, (B) 3 relevant documents, (C) 3 relevant and 3 non-relevant documents, (D) 10 judged documents (division unknown), (E) large set of judgements (division and number unknown).</p><p>We have followed the following intuition for our submissions: given that we have knowledge on which documents are relevant and not relevant to the query, can we use this information to obtain a better estimate of our query model? We hypothesize that our model gains the most when the set of non-relevant documents is large enough to give a proper estimate on non-relevance. We expect the background collection to be a better estimate of non-relevance when the set of  <ref type="table" coords="3,77.63,205.65,3.88,8.64">1</ref>: Evaluation on the 31 TREC Terabyte topics (top10): significance tested against the baseline (set A).</p><p>judged non-relevant documents is small, but expect to obtain an increasingly good estimate using the non-relevant documents as the size of this set increases. Thus, we compare our model using explicit non-relevance information to the same model using the collection as a non-relevance model, by submitting two distinct runs: met6, using the set of non-relevant documents, and met9, using only the collection (δ 2 = 1, viz. Eq. 7).</p><p>Preprocessing and Parameter settings We did not perform any preprocessing of the data besides standard stopword removal and stemming using a Porter stemmer. For our models we need to estimate four parameters: δ 1 , δ 2 , λ D , and λ Q . We have used the odd numbered topics from the TREC Terabyte track (topics 701-850) and from the TREC Million Query track (topics 1-10000) as training data. We have performed sweeps (with steps of 0.1) over possible values for these parameters and select the parameter settings with the highest resulting MAP scores. The resulting set of parameters that we have used for met6 is given by: λ D = 0.2, λ Q = 0.4, δ 1 = 0.2, and δ 2 = 0.6. The settings for met9 are: λ D = 0.2, λ Q = 0.4, and δ 1 = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>The results of our 10 individual runs are listed in Table <ref type="table" coords="3,271.37,567.36,4.98,8.64">1</ref> and Table <ref type="table" coords="3,78.33,579.32,3.74,8.64" target="#tab_1">2</ref>. Figures <ref type="figure" coords="3,121.90,579.32,4.98,8.64">1</ref> and<ref type="figure" coords="3,146.56,579.32,4.98,8.64" target="#fig_0">2</ref> further illustrate the differences on the 31 TREC Terabyte topics. We use the Wilcoxon signedrank test to test for significant differences between runs and report on significant increases (or drops) for p &lt; .01 using (and ) and for p &lt; .05 using (and ). Note that the baseline runs (set A) are the same for both methods, since neither uses any kind of relevance information. The same holds for set B: in this set only relevant information is available and the two methods should therefore result in the same scores. Due to a small bug in the implementation, however, parameter δ 2 was not properly normalized, causing a slight difference in the retrieval results for met6 on set B. As stated earlier, we submitted our runs to explore three main research questions:</p><p>• Can non-relevance information be effectively modeled to improve the estimation of a query model?</p><p>• What is the effect of the relative size of the set of nonrelevant documents with respect to the relevant documents on retrieval effectiveness?</p><p>• What are the effects when we substitute the estimates on the non-relevant documents with more general estimates, such as from the collection.</p><p>The results reported in Table <ref type="table" coords="3,444.87,304.35,4.98,8.64">1</ref> and Figure <ref type="figure" coords="3,499.10,304.35,4.98,8.64" target="#fig_0">2</ref> with respect to met6 give an answer to the first question. In all conditions, i.e., in all three measures as well as for different relevance feedback sets, the retrieval performance improves over the baseline, which confirms that our model can effectively incorporate non-relevance information for query modeling. Given a limited amount of non-relevant documents (sets C and D), our model especially improves early precision, although not significantly. A larger amount of nonrelevant documents (set E) decreases overall retrieval effectiveness. From Figure <ref type="figure" coords="3,407.33,423.90,9.40,8.64" target="#fig_0">2a</ref> we observe that set E only outperforms the other sets at the very ends of the graph. Figure <ref type="figure" coords="3,550.93,435.86,4.98,8.64">1</ref> shows a per-topic breakdown of the difference in MAP between the two submitted runs. We observe that most topics are helped more using the collection-based estimates. We have to conclude that, for the TREC Terabyte topics, the estimation on the collection yields the highest retrieval performance and is thus a better estimate of non-relevance than the judged non-relevant documents. When we zoom out and look at the full range of available topics (Table <ref type="table" coords="3,391.02,543.45,3.60,8.64" target="#tab_1">2</ref>), we observe that both models improve statMAP over the baseline (set A) for the full set of topics. When the feedback set is small, met9 improves statMAP more effectively than met6, i.e., the background model is performing better than the non-relevant documents. On the largest set of feedback documents (set E) met6 obtains the highest statMAP score (although the difference with met9 is not significantly different for this set, tested using a Wilcoxon sign rank test). The difference does seem to suggest that the amount of non-relevance information needs to reach a certain size to outperform the estimation on the collection. Since we select the terms that are most likely to be sampled from the distribution of the relevant documents rather than non-relevant documents, it is crucial that the underlying relevant and non-relevant distributions can be accu-  rately estimated. While the relevant documents are topically concentrated, i.e., they are all related to a given query, the non-relevant documents can be topically diverse and therefore more difficult to be estimated when the number of examples is limited. The background information is generally a good approximation of the distribution of non-relevant documents, given that most of the documents in the collections are not relevant. On the other hand, as the size of the set of non-relevant examples increases, especially the queryspecific top-ranked non-relevant documents, we can more accurately estimate the true distribution of the non-relevant information, which enables our model to have more discriminative power. Where this cut-off point lies remains a topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The results presented here provide us with mixed evidence regarding the hypothesis we stipulated in Section 5. Some of the presented results (statMAP and Figure <ref type="figure" coords="4,508.25,476.53,8.48,8.64" target="#fig_0">2a</ref>) confirm the premise that, using met6, a larger number of judged nonrelevant documents improve retrieval effectiveness most. On the other hand, the overall results obtained on the 31 TREC Terabyte topics suggest that the collection is a viable and sufficient alternative. We would like to further explore the problem in two directions. First, we intend to investigate the impact of the available judged (non-)relevant documents and their properties with respect to the estimates on the collection. Second, given the relevance assessments, we will try to find better ways of estimating the true distribution of the (non-)relevant information within our framework. We believe that, instead of using maximum likelihood estimates, more sophisticated estimation methods may be explored and applied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,53.80,446.09,502.12,9.03"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision-recall plots of met6 (a) and met9 (b) on the various feedback sets and the 31 TREC Terabyte topics (top10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,316.81,58.19,239.10,74.35"><head>Table 2 :</head><label>2</label><figDesc>Evaluation with statMAP: significance tested against baseline (set A).</figDesc><table coords="3,328.71,58.19,204.44,37.55"><row><cell>setA</cell><cell>setB</cell><cell>setC</cell><cell>setD</cell><cell>setE</cell></row><row><cell cols="5">met6 0.2289 0.2595 0.2750 0.2758 0.2822</cell></row><row><cell cols="5">met9 0.2289 0.2608 0.2787 0.2777 0.2810</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,53.80,62.08,502.12,367.39"><head></head><label></label><figDesc>Per topic difference in MAP between met6 and met9 on the 31 TREC Terabyte topics and the various sets of relevance feedback information (a positive value indicates that met9 outperforms met6 and vice versa). The labels indicate the respective topic identifiers.</figDesc><table coords="4,53.80,62.08,485.18,367.39"><row><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell>714</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell>708</cell><cell>722</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Δ map</cell><cell>-0.05 0.05 0.15</cell><cell>722</cell><cell>840</cell><cell>706</cell><cell>708</cell><cell>742</cell><cell>746</cell><cell>738</cell><cell>782</cell><cell>800</cell><cell>776</cell><cell>850</cell><cell>828</cell><cell>704</cell><cell>792</cell><cell>806</cell><cell>768</cell><cell>846</cell><cell>808</cell><cell>790</cell><cell>788</cell><cell>766</cell><cell>796</cell><cell>836</cell><cell>772</cell><cell>760</cell><cell>764</cell><cell>714</cell><cell>804</cell><cell>820</cell><cell>814</cell><cell>726</cell><cell>Δ map</cell><cell>-0.05 0.05 0.15</cell><cell>840</cell><cell>706</cell><cell>722</cell><cell>814</cell><cell>772</cell><cell>742</cell><cell>738</cell><cell>708</cell><cell>796</cell><cell>800</cell><cell>746</cell><cell>806</cell><cell>776</cell><cell>790</cell><cell>846</cell><cell>808</cell><cell>788</cell><cell>836</cell><cell>792</cell><cell>828</cell><cell>768</cell><cell>764</cell><cell>760</cell><cell>766</cell><cell>804</cell><cell>704</cell><cell>726</cell><cell>820</cell><cell>850</cell><cell>782</cell><cell>Δ map</cell><cell>-0.05 0.05 0.15</cell><cell></cell><cell>706</cell><cell>726</cell><cell>840</cell><cell>714</cell><cell>796</cell><cell>704</cell><cell>790</cell><cell>776</cell><cell>782</cell><cell>746</cell><cell>836</cell><cell>792</cell><cell>828</cell><cell>820</cell><cell>806</cell><cell>788</cell><cell>760</cell><cell>768</cell><cell>764</cell><cell>738</cell><cell>808</cell><cell>804</cell><cell>800</cell><cell>772</cell><cell>846</cell><cell>742</cell><cell>766</cell><cell>814</cell><cell>850</cell></row><row><cell></cell><cell>-0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">(a) Set C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">(b) Set D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">(c) Set E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.1 0.2 0.3 0.4 0.6 Figure 1: 0.0 Precision 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">set A set B set C set D set E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Precision</cell><cell cols="3">0.0 0.1 0.2 0.3 0.4 0.6 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">set A set B set C set D set E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">0.0</cell><cell></cell><cell></cell><cell cols="3">0.1</cell><cell></cell><cell></cell><cell cols="3">0.2</cell><cell></cell><cell></cell><cell cols="3">0.3</cell><cell></cell><cell></cell><cell cols="3">0.4</cell><cell></cell><cell></cell><cell cols="3">0.5</cell><cell></cell><cell></cell><cell cols="2">0.6</cell><cell>0.7</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell cols="3">0.9</cell><cell></cell><cell></cell><cell cols="3">1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.0</cell><cell></cell><cell></cell><cell cols="3">0.1</cell><cell></cell><cell></cell><cell cols="2">0.2</cell><cell cols="2">0.3</cell><cell cols="2">0.4</cell><cell></cell><cell cols="3">0.5</cell><cell></cell><cell></cell><cell cols="3">0.6</cell><cell></cell><cell></cell><cell cols="3">0.7</cell><cell></cell><cell></cell><cell cols="3">0.8</cell><cell></cell><cell></cell><cell cols="3">0.9</cell><cell></cell><cell></cell><cell cols="2">1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">(a) met6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(b) met9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This research was supported by the <rs type="projectName">DuOMAn</rs> project carried out within the <rs type="programName">STEVIN programme</rs> which is funded by the <rs type="funder">Dutch and Flemish Governments</rs> (http://www. stevin-tst.org) under project number <rs type="grantNumber">STE-09-12</rs>, and by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">017. 001.190</rs>, <rs type="grantNumber">640.001.501</rs>, <rs type="grantNumber">640.002.-501</rs>, <rs type="grantNumber">612.066.512</rs>, <rs type="grantNumber">612.061.814</rs>, <rs type="grantNumber">612.061.815</rs>, <rs type="grantNumber">640.004.802</rs>, and by the <rs type="funder">Virtual Laboratory for e-Science project</rs> (http: //www.vl-e.nl), which is supported by a <rs type="grantName">BSIK grant</rs> from the <rs type="funder">Dutch Ministry of Education, Culture and Science</rs> and is part of the <rs type="programName">ICT innovation program</rs> of the <rs type="institution">Ministry of Economic Affairs</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_XMj975Q">
					<idno type="grant-number">STE-09-12</idno>
					<orgName type="project" subtype="full">DuOMAn</orgName>
					<orgName type="program" subtype="full">STEVIN programme</orgName>
				</org>
				<org type="funding" xml:id="_WRHTmXj">
					<idno type="grant-number">017. 001.190</idno>
				</org>
				<org type="funding" xml:id="_qxWwEEp">
					<idno type="grant-number">640.001.501</idno>
				</org>
				<org type="funding" xml:id="_5MxVfCu">
					<idno type="grant-number">640.002.-501</idno>
				</org>
				<org type="funding" xml:id="_kaw4J4x">
					<idno type="grant-number">612.066.512</idno>
				</org>
				<org type="funding" xml:id="_vJTHM3a">
					<idno type="grant-number">612.061.814</idno>
				</org>
				<org type="funding" xml:id="_TQcQSvx">
					<idno type="grant-number">612.061.815</idno>
				</org>
				<org type="funding" xml:id="_HhakhD9">
					<idno type="grant-number">640.004.802</idno>
					<orgName type="grant-name">BSIK grant</orgName>
				</org>
				<org type="funding" xml:id="_SExnwCf">
					<orgName type="program" subtype="full">ICT innovation program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,75.32,242.98,66.65,12.90;5,53.80,268.95,239.10,8.64;5,63.76,280.90,229.14,8.64;5,63.76,292.68,175.12,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,268.10,268.95,24.80,8.64;5,63.76,280.90,229.14,8.64;5,63.76,292.86,113.48,8.64">A few examples go a long way: constructing query models from elaborate query formulations</title>
		<author>
			<persName coords=""><forename type="first">References</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,195.72,292.68,39.22,8.59">SIGIR &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,314.06,239.10,8.64;5,63.76,325.84,229.14,8.82;5,63.76,337.97,61.71,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,146.88,314.06,146.02,8.64;5,63.76,326.02,132.82,8.64">The effect of accessing nonmatching documents on relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Dunlop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,204.84,325.84,84.56,8.59">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,359.17,239.10,8.64;5,63.76,370.95,229.14,8.82;5,63.76,382.90,229.14,8.82;5,63.76,395.04,75.54,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,117.31,359.17,158.20,8.64">New experiments in relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,137.50,370.95,155.40,8.59;5,63.76,382.90,173.81,8.59">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="337" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,416.06,239.10,8.82;5,63.76,428.02,226.94,8.82" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,137.14,416.06,155.75,8.59;5,63.76,428.02,85.07,8.59">Variations on Language Modeling for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="5,53.80,449.40,239.10,8.64;5,63.76,461.35,229.14,8.64;5,63.76,473.13,182.11,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,248.72,449.40,44.18,8.64;5,63.76,461.35,229.14,8.64;5,63.76,473.31,120.11,8.64">Better than the real thing?: iterative pseudo-query processing using cluster-based language models</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Domshlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,202.72,473.13,39.22,8.59">SIGIR &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,494.51,239.10,8.64;5,63.76,506.47,229.14,8.64;5,63.76,518.24,92.72,8.82" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,188.36,494.51,104.54,8.64;5,63.76,506.47,229.14,8.64;5,63.76,518.42,31.67,8.64">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<idno>SIGIR &apos;01</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,539.62,239.10,8.64;5,63.76,551.40,115.09,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,208.65,539.62,84.25,8.64;5,63.76,551.58,53.09,8.64">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,135.69,551.40,39.22,8.59">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,572.78,239.10,8.64;5,63.76,584.56,229.14,8.82;5,63.76,596.51,229.14,8.59;5,63.76,608.64,119.81,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,137.31,572.78,155.59,8.64;5,63.76,584.73,24.02,8.64">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,184.33,584.56,108.57,8.59;5,63.76,596.51,224.93,8.59">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,629.85,239.10,8.64;5,63.76,641.62,185.89,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,216.11,629.85,76.79,8.64;5,63.76,641.80,124.09,8.64">A study of methods for negative relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,206.49,641.62,39.22,8.59">SIGIR &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,53.80,663.00,239.10,8.64;5,63.76,674.96,229.14,8.64;5,63.76,686.91,229.14,8.64;5,63.76,698.69,229.14,8.82;5,63.76,710.82,66.40,8.64" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="5,147.70,674.96,145.21,8.64;5,63.76,686.91,229.14,8.64;5,63.76,698.87,17.93,8.64">Re-examining the effects of adding relevance information in a relevance feedback environment</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W P</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">V</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Information Processing &amp; Management, In Press</publisher>
		</imprint>
	</monogr>
	<note>Corrected Proof</note>
</biblStruct>

<biblStruct coords="5,316.81,57.28,239.10,8.64;5,326.77,69.23,229.14,8.64;5,326.77,81.01,53.96,8.82" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="5,453.64,57.28,102.28,8.64;5,326.77,69.23,225.62,8.64">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In CIKM &apos;01</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
