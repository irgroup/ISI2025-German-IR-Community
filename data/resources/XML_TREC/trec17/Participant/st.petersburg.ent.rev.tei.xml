<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.93,98.05,394.52,15.15;1,238.04,119.97,98.56,15.15">Word importance discrimination using context information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.82,178.22,110.30,12.58"><forename type="first">Danil</forename><surname>Nemirovsky</surname></persName>
							<email>danil.nemirovsky@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">St. Petersburg State University</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.92,178.22,116.86,12.58"><forename type="first">Vladimir</forename><surname>Dobrynin</surname></persName>
							<email>v.dobrynin@bk.ru</email>
							<affiliation key="aff1">
								<orgName type="institution">St. Petersburg State University</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.93,98.05,394.52,15.15;1,238.04,119.97,98.56,15.15">Word importance discrimination using context information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C09CF6EFA1C03619D2DB228FF8B7D29F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Word importance discrimination</term>
					<term>Context</term>
					<term>Clustering</term>
					<term>CDC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Word importance discrimination is a task deserving attention when one treats a topic from TREC where a topic is quite long. The goal of the process is to estimate importance of words which carry any (additional) information about user information needs. In our experiments we estimated word importance using context information of a word.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word importance discrimination is a task deserving attention when one treats a topic from TREC where a topic is quite long. The goal of the process is to estimate importance of words which carry any (additional) information about user information needs. Word importance discrimination task is strongly related to word filtering where word importance is a binary value. There were proposed several approaches addressing word filtering. Luhn <ref type="bibr" coords="1,411.11,579.40,12.36,10.48" target="#b2">[3]</ref> uses a simple filter based on frequency of term occurrence, Bookstain et al. <ref type="bibr" coords="1,438.09,593.85,12.36,10.48" target="#b0">[1]</ref> detect content-bearing words by serial clustering, Picard <ref type="bibr" coords="1,344.71,608.29,12.36,10.48" target="#b3">[4]</ref> suggests to use term similarities, and Takayama et al. <ref type="bibr" coords="1,250.38,622.74,12.36,10.48" target="#b5">[6]</ref> used SVD decomposition of co-occurrence matrix. In our experiments we estimated word importance based on context of a word, which is a probability distribution over words that can be met in the same documents as the given word. Intuitively, one can say that a word is important if it has specific meaning in a domain and occurs in the relatively small number of documents. This implies that a word has specific meaning if its context is of low entropy. We use this idea to estimate importance of both words and phrases in a document collection and perform experiments to find out what kind of word importance discrimination shows better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context Document Clustering algorithm (CDC)</head><p>Context Document Clustering algorithm is a scalable clustering algorithm which full description of can be found in <ref type="bibr" coords="2,303.02,232.33,12.36,10.48" target="#b1">[2,</ref><ref type="bibr" coords="2,316.91,232.33,8.24,10.48" target="#b4">5]</ref>.</p><p>In our TREC2008 experiment we use idea of term context which plays an important role in CDC algorithm. Let each document of a collection be presented as a probability distribution over the set of all the terms of the collection called a profile of the document. A document is presented by a probability distribution over the set of terms in the model:</p><formula xml:id="formula_0" coords="2,254.05,337.59,232.39,27.79">p(t|d) = tf t,d N d ,<label>(1)</label></formula><p>where tf t,d is the number of occurrence of term t in document d and N d is the total number of terms in document d. The occurrence of a term in a document is assumed independent from all other terms of the document. Nothing is assumed about the notion of "term" except the fact that a document consists of terms and the set of all the terms of the collection is the set of terms met in a document of the collection. A context is created for each term which is not very common (e.g. it is an upper bound for the document frequency of the term) or very rare (e.g. it is a lower bound for the document frequency of the term) in the collection. A context of a term t is a probability distribution over all the terms in the collection and the entry of the distribution is probability to meet the term t with another term in the same document.</p><p>The term contexts are used to select important words (more precisely, important terms) from the dictionary of the collection. Intuitively, one can say that a word is important if it has specific meaning in a domain and occurs in the relatively small number of documents. In terms of CDC algorithm, a word has specific meaning if its context is of low entropy. Hence, we can define importance of a term in the following way:</p><formula xml:id="formula_1" coords="2,215.29,642.65,271.16,26.77">imp(t) = 1 log(1 + df (t))H(t) .<label>(2)</label></formula><p>One can see that the lower entropy of a term is, the higher its importance, and the bigger number of documents containing the term is, the lower its importance. Also, since, for example, if a term occurs in 100 documents or in 101 makes smaller impact at the intuitive term importance than if a term occurs in 1 documents or in 2 document we applied log to document frequency of a term.</p><p>We use 2-or 3-word sequences in documents as phrases. The importance of a phrase is a sum of importance of terms it is composed of:</p><formula xml:id="formula_2" coords="3,234.87,184.59,251.58,21.26">imp 1 (p) = t∈p imp(t).<label>(3)</label></formula><p>3 Experiments</p><p>We have made four experiments. In each experiment we use the same methodology with slight changes that allows us to compare results of our experiments. All our experiments are devoted to find out a response to the following question: how useful can be phrases and important words extracted from a document collection in automatic way using context information, and, particularly, what way should be chosen using word importance discrimination defined in ( <ref type="formula" coords="3,151.26,369.67,4.99,10.48" target="#formula_1">2</ref>) and ( <ref type="formula" coords="3,192.81,369.67,4.55,10.48" target="#formula_2">3</ref>). In the first experiment, 8T 0eZ, only phrases are used to score documents over topics. In the second experiment, xLQOW , we mix two types of scores obtained by a document against a topic: score obtained with common phrases in document and topic and score obtained with common important words. In the third experiment, Krcy7, we expand the list of phrases by phrases from documents retrieved in experiment 8T 0eZ. And the fourth experiment, U 2LwQ, is the same as the first one but only "query" field is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Common part of experiments</head><p>The CSIRO document collection is parsed in the following way. First of all, we delete stop-words from the documents. In the experiments a word is defined as a string containing alphanumeric symbols and at least one letter, specifically a word satisfies the "[a-z0-9]*[a-z][a-z0-9]*" regular expression. Applying Porter stemming algorithm to words we obtain stem of words which we call terms.</p><p>The number of terms we have got is 603349. A document is presented by profile which is a probability distribution defined by <ref type="bibr" coords="3,366.20,639.61,13.66,10.48" target="#b0">(1)</ref>. We create contexts for term having document frequency greater or equal to 25 and less or equal to 10000. Term importance and phrase importance are calculated.</p><p>Parsing queries we concatenate the both "query" and "narr" fields to form a topic. We parse the topic deleting stop-words and applying stemming to words defined by the same regular expression as for documents. Terms which are not in the dictionary of the collection are ignored. Each topic contains a lot of terms having different importance which should be estimated. In experiments we test several ways to estimate word importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">8T0eZ</head><p>In the experiment, a document gains a score against a topic if the document has common phrases with the topic.</p><formula xml:id="formula_3" coords="4,160.35,247.62,326.09,21.89">score p (d|q) = p∈d,p∈q (imp 1 (p) * log (pf (p|d) + 1)) ,<label>(4)</label></formula><p>where d is a document, p is a phrase, imp 1 (p) is phrase importance, pf (p|d) is the number of occurrence of phrase in document d, and the summation is done over common phrases of a topic q and document d. We report the first thousand documents for each topic having highest score p (d|q) values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">xLQOW</head><p>Sometimes the scores applied in experiment 8T 0eZ are too strict and relaxing is required. In this experiment we mix document scores obtained with phrases and important words. </p><p>where d is a document, t is a term, imp(t) is term importance, tf (t|d) is the number of occurrence of term t in document d, and the summation is done over common term of a topic q and document d.</p><p>We mix (4) and ( <ref type="formula" coords="4,181.78,569.21,4.99,10.48" target="#formula_4">5</ref>) scores:</p><formula xml:id="formula_5" coords="4,158.42,597.51,261.54,11.51">score(d|q) = λ * score p (d|q) + (1 -λ) * score t (d|q),</formula><p>where 0 &lt; λ &lt; 1.</p><p>We optimized λ coefficient using topics and relevance judgements of TREC 2007. The optimal value of λ is 0.7.</p><p>We report the first thousand documents for each topic having highest score(d|q) values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Krcy7</head><p>In this experiment we use different kind of relaxing than in the previous one. We consider scores from experiment 8T 0eZ. Let us assume that we deal with topic q and document d which has a number of common phrases with topic q.</p><p>The common phrases of document d and topic q are called phrases of level 1.</p><p>Let us define a set of documents containing given phrase p.</p><formula xml:id="formula_6" coords="5,243.21,209.26,91.95,10.48">D(p) = {d|p ∈ d}.</formula><p>We weight phrase p by its level 1 importance with scores of documents containing phrase p against all the topics:</p><formula xml:id="formula_7" coords="5,179.41,285.41,219.54,22.33">imp 2 (p) = d∈D(p) q (imp 1 (p) * score p (d|q)),</formula><p>where q is a topic, d is a document, p is a phrase occurring in document d, imp 1 (p) is importance of phrase according (3). Hence, we get what we call "level 2 importance" of phrase. So, if a document has a phrase of level 1 against a topic it has a number of phrases of level 2 with level 2 importance. We use these phrases to expand list of phrases to search. We note that phrases of level 1 are phrases of level 2, too.</p><p>Let us define a set of documents having common phrases with a topic:</p><formula xml:id="formula_8" coords="5,219.82,444.26,138.73,10.48">L(q) = {d|∃p, p ∈ d, p ∈ q}.</formula><p>and scoring function for documents against topics is</p><formula xml:id="formula_9" coords="5,115.34,506.80,253.43,67.51">score p2 (d|q) = p∈d, p∈q (imp 1 (p) * log (pf (p|d) + 1)) + + p∈d, d∈L<label>(q)</label></formula><p>(imp 2 (p) * log (pf (p|d) + 1)) .</p><p>We report the first thousand documents for each topic having highest score p2 (d|q) values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">U2LwQ</head><p>Documents are treated as in 8T 0eZ experiment. The field "query" is used as a topic. We used all words in "query" field which were met in the collection. All the other procedures are the same as in 8T 0eZ. We report the first thousand documents for each topic having highest score(d|q) values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>The experimental results are presented in Table <ref type="table" coords="6,345.69,418.35,4.55,10.48">1</ref>. The results confirm that the scores applied in 8T 0eZ is too strict and the quality of retrieval can be improved by using important words equally with important phrases, as in experiment xLQOW . The attempt to use important phrases of level 2 does not give an advantage, see experiment Krcy7.  <ref type="table" coords="6,121.93,560.23,5.45,9.57">1</ref> Average measures over all topics. M edian is the average over of all the topics of the median measures of all the participated systems, and best is the average over of all the topics of the best achieved result among all the participated systems.</p><p>Experimental results at each topic are presented at Fig. <ref type="figure" coords="6,379.10,622.74,5.17,10.48" target="#fig_1">1</ref> and Fig. <ref type="figure" coords="6,430.46,622.74,4.85,10.48" target="#fig_2">2</ref>. One can see from left graphs of Fig. <ref type="figure" coords="6,230.00,637.18,5.17,10.48" target="#fig_1">1</ref> and Fig. <ref type="figure" coords="6,282.19,637.18,5.17,10.48" target="#fig_2">2</ref> that results are worse than median results over all the systems for most topics but in two cases for AP measure and in three cases for NDCG measure results are better than median results. Observing right graphs of Fig. <ref type="figure" coords="6,231.50,680.52,5.17,10.48" target="#fig_1">1</ref> and Fig. <ref type="figure" coords="6,283.73,680.52,5.17,10.48" target="#fig_2">2</ref> we can see that relaxed scores applied in experiment xLQOW performs better than scores of experiment 8T 0eZ in most cases, but strict scores 8T 0eZ and very relaxed scores Krcy7 can perform better than relaxed scores xLQOW in some cases. Considering all the figures and content of topics we found out that better performance of scores, like over performing median results and achieving values of AP measure higher than 0.48 and NDCG measure higher than 0.78, is reached at short topics containing almost only informative important words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,169.99,474.43,27.20,10.48;4,197.19,478.94,3.06,6.99;4,200.75,474.43,36.56,10.48;4,240.62,489.32,27.90,6.99;4,270.80,474.43,137.58,10.48"><head></head><label></label><figDesc>score t (d|q) = t∈d,t∈q (imp(t) * log(tf (t|d) + 1)) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,91.93,267.49,394.52,9.57;6,91.93,281.04,394.52,9.57;6,91.93,294.58,110.15,9.57"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. AP measure on each topic with (left) and without (right) median results over all systems. Topics ids are placed at abscissa axis, and AP measure values are places at ordinate axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,91.93,267.49,394.52,9.57;7,91.93,281.04,394.52,9.57;7,91.93,294.58,128.36,9.57"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. NDCG measure on each topic with (left) and without (right) median results over all systems. Topics ids are placed at abscissa axis, and NDCG measure values are places at ordinate axis.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,106.43,458.70,380.01,9.57;7,106.43,472.25,272.54,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,312.46,458.70,173.99,9.57;7,106.43,472.25,169.25,9.57">Detecting content-bearing words by serial clustering -extended abstract</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bookstain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Raita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,298.81,472.25,22.44,9.57">Sigir</title>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.43,494.61,311.46,9.57;7,433.71,494.61,52.74,9.57;7,106.43,508.16,380.01,9.57;7,106.43,521.71,359.01,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,433.71,494.61,52.74,9.57;7,106.43,508.16,94.16,9.57">Contextual document clustering</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Dobrynin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">W</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niall</forename><surname>Rooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,144.31,521.71,166.54,9.57">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Sharon</forename><surname>Mcdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><surname>Tait</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2997</biblScope>
			<biblScope unit="page" from="167" to="180" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.43,544.08,380.01,9.57;7,106.43,557.63,125.54,9.57" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,163.72,544.08,322.72,9.57;7,106.43,557.63,92.18,9.57">A statistical approach to the mechanized encoding and searching of literary information</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.43,579.99,380.02,9.57;7,106.43,593.54,99.46,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,177.98,579.99,253.22,9.57">Finding content-bearing terms using term similarities</title>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,453.54,579.99,26.33,9.57">EACL</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="241" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.43,615.91,380.02,9.57;7,106.43,629.46,380.02,9.57;7,106.43,643.01,155.60,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,478.27,615.91,8.18,9.57;7,106.43,629.46,313.67,9.57">A scaleable document clustering approach for large document corpora</title>
		<author>
			<persName coords=""><forename type="first">Niall</forename><surname>Rooney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">W</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mykola</forename><surname>Galushka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Dobrynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,428.21,629.46,58.23,9.57;7,106.43,643.01,38.04,9.57">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1163" to="1175" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.43,665.37,380.02,9.57;7,106.43,678.92,380.01,9.57;7,106.43,692.47,380.02,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,386.69,665.37,99.77,9.57;7,106.43,678.92,215.62,9.57">Information retrieval based on domain-specific word associations</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Flournoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,356.68,678.92,129.76,9.57;7,106.43,692.47,271.39,9.57">Proceedings of the Pacific Association for Computational Linguistics (PACLING99)</title>
		<meeting>the Pacific Association for Computational Linguistics (PACLING99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
