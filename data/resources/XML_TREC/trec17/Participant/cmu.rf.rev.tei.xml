<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.44,120.81,388.34,14.41">Extending Relevance Model for Relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.18,141.49,42.91,12.24"><forename type="first">Le</forename><surname>Zhao</surname></persName>
							<email>lezhao@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.81,141.49,84.99,12.24"><forename type="first">Chenmin</forename><surname>Liang</surname></persName>
							<email>chenminl@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.35,141.49,70.73,12.24"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.44,120.81,388.34,14.41">Extending Relevance Model for Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8C4CBC593C84C0ABC43E3B29A7C08E61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relevance feedback is the retrieval task where the system is given not only an information need, but also some relevance judgement information, usually from users' feedback for an initial result list by the system. With different amount of feedback information available, the optimal feedback strategy might be very different. In TREC Relevance Feedback task, the system is given different sets of feedback information from 1 relevant document to over 40 judgements with at least 3 relevant. Thus, in this work, we try to develop a feedback algorithm that works well on all levels of feedback by extending the relevance model for pseudo relevance feedback to include judged relevant documents when scoring feedback terms. Within these different levels of feedback, it is more difficult for the feedback algorithm to perform well when given minimal amount of feedback. Experiments show that our algorithm performs robustly in those difficult cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relevance feedback is the retrieval task where the system is given not only a user query, but also user feedback on some of the top ranked results. Feedback gives the retrieval system a chance to improve its results by exploiting the extra information through more elaborate techniques. This can be helpful in cases where the users want as many relevant results as possible. Experimenting with and failure analyses on feedback algorithms also provides us a chance to understand what are missing in users' keyword queries and how to improve these queries.</p><p>In the following, we first introduce the relevance feedback task defined by this year's track, the basis ad hoc retrieval algorithm. In Section 2, we layout the basic model for combining pseudo relevance feedback and true relevance feedback in query expansion. One difficulty in doing that is properly and effectively balancing term weights from the two sources. We give our solution. We show experiments in Section 3 and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Task definition</head><p>With this year's relevance feedback experiments, the retrieval system will be evaluated on different number of feedback documents. Specifically, for each query, set A is the baseline without any feedback information. Set B allows the system to use 1 relevant document as positive feedback; set C allows 3 relevant and 3 non-relevant; set D uses 10 judged documents (a superset of set C), and set E provides all judged documents to the retrieval algorithm, varying from 40 to 100s of judged documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Pseudo relevance feedback as basis algorithm</head><p>Since different numbers of feedback documents are possible, especially, when only given a small number of judged documents, any relevance feedback algorithm that only look at the few judged documents can have difficulty providing reliable expansion queries. This is because fewer feedback documents means larger variations of term appearances and expansion weights. The approach we propose here, is to leverage the small amount of feedback information with some pseudo relevance feedback, so that the larger number of feedback documents provide lower variance estimates of the feedback terms, stabilizing the performance of the feedback algorithm.</p><p>The motivation for using the particular pseudo relevance feedback method proposed in <ref type="bibr" coords="2,450.33,108.00,57.38,8.77;2,87.54,119.22,46.16,8.77" target="#b3">(Lavrenko and Croft 2001)</ref> is that the best runs on the two previous TREC 2006 and 2007 Terabyte tracks, <ref type="bibr" coords="2,454.25,119.22,53.41,8.77;2,87.54,130.32,22.64,8.77" target="#b0">(Metzler et al 2005)</ref> and (Li and Yan 2006), both used this particular pseudo relevance feedback method. Since the relevance feedback task uses the same collection and similar set of queries, we set the baseline retrieval method to be the same as in the two previous top runs -a dependency model to achieve higher top precision and a pseudo relevance feedback step following.</p><p>Because of its relevance to this work, we show the pseudo relevance feedback model implemented in the Indri search engine: Relevance_model_query: #weight(w 1 r 1 w 2 r 2 .. w n r n ), where, w i = P(r i | I). Final_query:</p><p>#weight( (1 -w) Relevance_model_query w Original_query )</p><p>Here, r is a concept term, which could refer to any term or phrase (we restrict ourselves to only keywords), D refers to a feedback document, and I means the information need which is expressed by the query. P(r | I) is a multinomial distribution over all concepts, of how ideally the relevant class should look like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(I | D</head><p>) is approximated with the probability for a document to generate the query, which is just the language modeling version of the relevance score. As P(I) is constant w.r.t. r, P(r | I) is easily calculated if we assume P(D) is uniform. Top weighted terms are selected for query expansion and weighted exactly as their weights, so that the expansion query ranks documents according to their KL-divergence to the relevance model P(r | I). The final query is just interpolating the expansion query with the original query. The original queries can be the keyword query titles provided by TREC or the dependency model phrase queries which we used as basis algorithm. Now we introduce our relevance feedback algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relevance feedback algorithm</head><p>The design of a feedback algorithm would be easy if the system is given complete judgments for top N results, just use the judged documents as training samples, use simple term features and train a binary classifier for relevant or not, see for example <ref type="bibr" coords="2,280.83,527.22,115.53,8.77" target="#b4">(Zhu, Zhao and Callan 2007</ref>). This will be especially effective if N is large. However, in many cases the system does not have such a luxury. In the current track, 3 out of the 4 feedback sets have N as low as 1, 6 or 10. Thus, in this section we propose a unified pseudo-relevance and true relevance feedback method based on the relevance model formalism <ref type="bibr" coords="2,222.76,577.74,103.45,8.77" target="#b3">(Lavrenko and Croft 2001)</ref>.</p><p>From equation (1) using Bayes rule, we get the original form of equation ( <ref type="formula" coords="2,394.27,594.78,3.46,8.77">1</ref> (2)</p><p>For the feedback documents that have relevance judgments, i.e. DâˆˆD T , we trust the judgments, thus we set P(D</p><formula xml:id="formula_0" coords="2,133.38,657.83,134.16,18.78">| I) = R( ) Rel( , ) Rel( , ) i i D I D I D I âˆˆ âˆ‘</formula><p>where Rel(D, I) is the judged relevance, R(I) is the set of all relevant documents for topic I. (This year, for some of the input judgments, {0, 1, 2} 3 levels of relevance is used.) Because judged irrelevant documents have relevance score 0, they will not have any effect in the model.</p><p>For the pseudo relevant documents, DâˆˆD P , we have to use Bayes rule to expand the P(D | I) term just as in (1), thus, arriving at the following decomposition, (3)</p><formula xml:id="formula_1" coords="3,132.18,137.30,293.31,67.34">T P T P D D D D ( | ) ( ) ( | ) ( | ) ( | ) ( | ) ( ) ( | ) ( | ) ( | ) ( | ) ( ) ( ) i j j j i i j D D i i j</formula><p>This final score P(r | I) will be the weight for the expansion term r in the relevance model. The above derivations are exact, except there are some unknowns in the result: P(I), P(D j ), and the total collectionrelevance C Rel( ,</p><formula xml:id="formula_2" coords="3,128.64,232.20,67.72,18.89">) i i D D I âˆˆ âˆ‘ .</formula><p>Notice that these probabilities may depend on topic, and special care must be taken to deal with these values, as they directly affect the relative importance of true relevant documents w.r.t. pseudo relevant documents. We propose a justified and effective way of dealing with this balancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Balancing weights from pseudo-and true-relevant documents</head><p>There are several key problems to solve in order to properly balance the weights. Firstly, since some of the probabilities will never be known, we want to use a hyper-parameter to weight the two sources, and tune the weight on training data. Secondly, the numbers of (true and pseudo) feedback documents can be different across topics, thus, a larger D T set would probably lead to a bias toward terms from the judged set and thus the optimal hyper parameter would vary. That's why a normalization step needs to happen so that the numbers of feedback documents -true v.s. pseudo -will not affect the relative weights of the terms from these two sources. Thirdly, note P(I | D j ) the query generation probability will have large variations across topics, as a query can contain different number of query terms which can be popular or rare. Thus, we need to reduce this variance so that the hyper parameter is less variable across topics, and it's easier to learn an effective hyper parameter value which will be applied across all topics. (5) P(D j ) is the document prior, if we assume uniform, it doesn't matter what exact value it takes, as tuning Î± will effectively cancel this constant term out. For an analogy with the empirical relevance distribution, we also use the empirical prior, 1/|D P |. These two empirical distributions act as normalizers to the two sources, ensuring the weights are normalized properly with respect to the numbers of samples in both sources.</p><p>The above solves the first and second problems outlined at the beginning of the subsection. For the last problem P(I), we don't know how exactly should it be determined, but we do know it should be dependent on the collection and on the topic, and also we expect it to provide proper balancing effect for the term P(I | D j ), which varies a lot across topics. Given the requirements, we can have several alternative assignments for P(I), a) max D âˆˆ C {P(I | D)}, b) avg Dj âˆˆ TOPN {P(I | D j )}. Intuitively, if we neglect Î± , assignment (a) means we assume that a relevant document is as good as the best scoring document in the collection, while assignment (b) means it is as good as the average score of the top N results. We avoided using the simplest P(I | C) -the query generation probability of the collection model -as an approximation for P(I), because P(I | D j ) would vary more wildly than P(I | C) depending on how well the documents in the collection are matching with the topics, while P(I | C) is ignorant of the documents. Without experiments, it is difficult to see how the two different assignments would behave because there is a regulating parameter Î± outside of the scope of the assignments that will determine how much weight to put on the two sources (true and pseudo relevance feedback). In general, we should select assignments that produce the most stable optimal regulating Î± across all topics, or more directly select the best assignment according to the final retrieval effectiveness measure (such as overall MAP etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and test datasets</head><p>We train all the parameters in the model on the training set by parameter sweeps. To create the training set, topics and assessments from the following track were used: TREC Terabyte 2004, 2005 and 2006 tracks and Million Query track 2007. We require each training topic to contain at least 40 judgments, and use 2/3 of the judgements as feedback documents and the rest 1/3 as evaluation data. For the feedback judgments, at least 4 should be relevant and 5 non relevant. Other topics are filtered out. We keep a ratio of 3:1 for MQ topics and Terabyte topics just to mimic the testing conditions and evaluate on the whole set using MAP. Total number of topics is 296, with 74 Terabyte topics. Although for training efficiency reasons we sampled a smaller subset of 113 training topics from this whole set.</p><p>The selection of the feedback documents is crucial for creating a proper training set. However, since we don't have the same results data from previous tracks as the relevance feedback track organizers do, we can only try to be as close to the test data as possible. Since the TREC judgment pool biases toward top ranked documents, although maybe not as high a bias as the feedback set for the test data, we still simply used a 2/3 random sample of the judgments from the pool as feedback and the rest as evaluation data. We fear that directly using top ranked results from the baseline run would bias the results too much, but we did not try, with the official test data now available we could try and report rankings of the feedback documents for the training sets that we may use, and compare to that of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter tuning and performance analysis</head><p>Submitted results are shown in Table <ref type="table" coords="4,258.00,564.06,3.65,8.78" target="#tab_0">1</ref>. All parameters were tuned on training set. Because of the large number of parameters to tune (especially conditioned on the different setups), we fix all the rest parameters and tune only one parameter at a time. We fix smoothing parameters first for the baseline Set A algorithm, and then keep the same parameters for feedback runs. When tuning feedback runs, we only vary the number of feedback terms and feedback documents, Î± and the weight for the original query.</p><p>From the results, we can see that the optimal Î± is quite stable across different feedback settings, meaning we have done proper balancing. Also, we did a more detailed per topic optimal Î± analysis on Set C and Set D, and results show that although the optimal Î± varies across topics, there are 1/3 of the topics where Î± can be 0-1 without changing performance; most of these topics are either too hard (MAP&lt;0.05) or too easy (MAP&gt;0.5). In 3/4 of the other topics where Î± makes a difference, optimal Î± &gt; 0.5, meaning true relevance feedback is emphasized more than the top ranking result. If we take the optimal MAP value for each topic, we get the upper bound for the performance we can get if we vary Î± , the upper bound on the training set turns out to be MAP =0.1957 for Î± in the set {0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} and with all other parameters set as runs CMURF08.C1 and D1, especially, the original query weight is set as 0.7, which restricts the effects of the expansion query.</p><p>When only with one (the top) relevant document for feedback, on the test set, our model ( Î± = 0.7) performs significantly better than relevance feedback alone ( Î± = 1), MAP 0.1034 v.s. 0.1005, with significance level p &lt; 0.004 by a paired sign test. Although it is not significantly better than pseudo relevance feedback alone ( Î± = 0, MAP=0.09918).</p><p>We also show per topic how our results compare to the median system in Figure <ref type="figure" coords="5,434.37,513.06,3.65,8.78">1</ref>. The results are worse than the median on average. And a comparison with other systems show that the baseline is much worse than other systems. However, comparing the relative gains in MAP by using more relevance data (Table <ref type="table" coords="5,117.96,546.60,3.53,8.78" target="#tab_0">1</ref>), the constructed training set and the official test data have a similar trend, and the most performance gain is in the 10% range. On the training data, this gain is achieved right at set B, while on the test data, the best performance is at set E. This suggests that a randomly sampled feedback document (what's done on the training topics) is more informative than a top ranked relevant document (for the test case). Also, this relative gain is smaller than the best systems, and it might be because of the worse baseline. More experiments need to be done to re-establish a baseline and confirm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We presented a coherent model of incorporating pseudo relevance feedback together with true relevance feedback. We showed that the difficulty lies mainly in balancing term weights coming from the two different sources. Several steps have been taken to deal with weight balancing, and reasonable alternative solutions exist. Results show that merging relevance feedback and pseudo relevance feedback is significantly better than relevance feedback alone when feedback documents are few. Randomly sampled relevant document is carrying more information than a top ranked relevant document, in terms of feedback performance. For future work, we would like to explore ways of modeling negative feedback, more finely controlled term extraction, such as restricting a window around each query term, to get more precise feedback terms. We are also interested in investigating how perturbing the feedback documents would affect results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,105.06,460.08,402.62,9.47;3,87.54,473.46,209.56,9.47;3,418.44,483.99,2.65,3.91;3,414.06,481.31,4.32,5.39;3,303.48,471.30,83.55,12.09;3,423.48,474.06,40.53,9.33;3,408.36,483.99,1.21,3.91;3,318.90,479.69,1.67,5.39;3,371.82,479.69,1.67,5.39;3,448.80,479.69,1.67,5.39;3,404.40,481.31,4.32,5.39;3,301.26,474.06,30.09,9.33;3,364.92,474.06,17.43,9.33;3,441.90,474.06,17.43,9.33;3,410.04,479.38,4.27,7.33;3,338.40,470.73,5.68,12.68;3,392.34,467.86,11.07,19.02;3,465.78,473.46,41.91,8.78;3,87.54,491.10,126.08,8.78;3,176.04,534.85,2.99,4.41;3,279.72,534.85,2.72,4.41;3,171.06,531.85,4.88,6.08;3,274.68,531.85,4.88,6.08;3,225.60,514.32,89.53,10.51;3,143.52,517.38,14.09,10.51;3,191.16,517.38,71.14,10.51;3,294.65,517.38,13.62,10.51;3,323.63,517.38,26.04,10.51;3,365.09,517.38,60.95,10.51;3,161.46,517.80,55.64,10.51;3,243.00,523.81,1.88,6.08;3,320.16,523.81,1.88,6.08;3,361.56,523.81,1.88,6.08;3,390.96,523.81,1.88,6.08;3,124.68,517.38,27.65,10.51;3,183.72,517.38,73.31,10.51;3,287.16,517.38,133.61,10.51;3,160.26,513.62,6.40,14.30;3,264.29,513.62,6.40,14.30;3,169.26,510.45,12.47,21.44;3,272.74,510.45,12.47,21.44;3,472.62,517.32,13.65,10.51;3,105.06,549.12,243.92,8.78;3,352.26,543.40,7.34,15.07;3,365.52,549.12,42.11,8.78;3,178.56,594.43,3.00,4.41;3,335.40,594.43,2.73,4.41;3,234.18,603.55,3.00,4.41;3,173.58,591.42,4.88,6.08;3,330.30,591.42,4.88,6.08;3,229.20,600.54,4.88,6.08;3,422.94,568.07,13.98,10.52;3,452.34,568.07,3.89,10.52;3,234.48,569.45,45.72,10.52;3,126.60,576.95,25.43,10.52;3,193.62,576.95,31.37,10.52;3,298.37,576.95,8.79,10.52;3,324.29,576.95,3.89,10.52;3,350.27,576.95,13.68,10.52;3,379.31,576.95,33.29,10.52;3,241.44,586.07,45.72,10.52;3,432.66,586.07,13.73,10.52;3,448.80,574.50,1.88,6.08;3,263.04,575.82,1.88,6.08;3,217.68,583.32,1.88,6.08;3,375.78,583.32,1.88,6.08;3,405.18,583.32,1.88,6.08;3,270.00,592.50,1.88,6.08;3,415.44,568.07,32.50,10.52;3,255.24,569.45,19.67,10.52;3,112.98,576.95,33.77,10.52;3,186.18,576.95,32.14,10.52;3,342.78,576.95,61.54,10.52;3,262.20,586.07,19.67,10.52;3,425.22,586.07,15.89,10.52;3,120.24,572.54,4.26,8.74;3,162.90,572.52,7.37,15.12;3,315.53,572.52,7.37,15.12;3,154.68,573.19,6.41,14.31;3,290.03,573.19,24.46,14.31;3,171.78,570.01,12.49,21.47;3,328.37,570.01,12.49,21.47;3,227.40,579.13,12.49,21.47"><head></head><label></label><figDesc>From equation (3), since we don't know the judgments of all the documents, we estimate P(D i | I) on the given samples D T , this gives us the resulting term hyper parameterÎ± , and parameterize P(r | I) in Î± as follows,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,98.04,111.48,414.21,312.50"><head>Table 1 .</head><label>1</label><figDesc>Submited results with common parameters: dependency model initial queries, phrase smoothing Dirichlet with mu=4000, and term smoothing mu=1700, #feedback terms = 50, original query weight=0.7, max assignment for P(I). Other parameters shown below: Reported test metrics MAP RR and P@10 are evaluated on the 31 Terabyte track topics, the mtc and statAP measures are evaluated on MQ topics, with 237 and 208 valid topics respectively.</figDesc><table coords="5,98.04,145.30,406.10,267.16"><row><cell>Runs: Runtag</cell><cell cols="2">Stop-words #fb</cell><cell>Î±</cell><cell>Training:</cell><cell>Official Test*: MAP</cell></row><row><cell></cell><cell></cell><cell>Docs</cell><cell></cell><cell>MAP, RR, P@10</cell><cell>RR P@10, mtc statAP</cell></row><row><cell>CMURF08.A1</cell><cell>Included</cell><cell>10</cell><cell>&lt;1</cell><cell cols="2">0.1738, 0.4374, 0.1858 0.1008, 0.4089, 0.2548</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04025, 0.1646</cell></row><row><cell>CMURF08.B1</cell><cell>Excluded</cell><cell>10</cell><cell cols="3">0.7 0.1905, 0.4376, 0.1929 0.1034, 0.3918, 0.2452</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04299, 0.1777</cell></row><row><cell>CMURF08.C1</cell><cell>Excluded</cell><cell>10</cell><cell cols="3">0.8 0.1878, 0.4323, 0.1956 0.1039, 0.4122, 0.2452</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04310, 0.1817</cell></row><row><cell>CMURF08.D1</cell><cell>Excluded</cell><cell>10</cell><cell cols="3">0.8 0.1905, 0.4438, 0.1973 0.0989, 0.3914, 0.2452</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04312, 0.1799</cell></row><row><cell>CMURF08.E1</cell><cell>Excluded</cell><cell>10</cell><cell cols="3">0.8 0.1876, 0.4631, 0.1938 0.1094, 0.4397, 0.2774</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04184, 0.1817</cell></row><row><cell>CMURF08.B2</cell><cell>Included</cell><cell>10</cell><cell cols="3">0.8 0.1898, 0.4383, 0.1885 0.0985, 0.4157, 0.2387</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04127, 0.1694</cell></row><row><cell>CMURF08.C2</cell><cell>Excluded</cell><cell>12</cell><cell cols="3">0.8 0.1883, 0.4367, 0.1947 0.1040, 0.4120, 0.2452</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04313, 0.1820</cell></row><row><cell>CMURF08.D2</cell><cell>Excluded</cell><cell>12</cell><cell cols="3">0.8 0.1907, 0.4481, 0.1965 0.0991, 0.3910, 0.2452</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.04315, 0.1804</cell></row><row><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.06,450.32,402.73,8.10;6,87.54,460.40,241.71,8.10;6,105.06,476.24,402.65,8.10;6,87.54,486.38,232.11,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,366.46,450.32,137.25,8.10;6,105.06,476.24,116.50,8.10;6,264.30,476.24,221.12,8.10">Peking University at the TREC 2006 Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,97.26,460.40,227.41,8.10;6,87.54,486.38,227.52,8.10">Proceedings of the 14th Text REtrieval Conference (TREC)</title>
		<meeting>the 14th Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 15th Text REtrieval Conference (TREC)</note>
</biblStruct>

<biblStruct coords="6,105.06,502.22,402.54,8.10;6,87.54,512.30,255.79,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,187.93,502.22,177.93,8.10">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rochio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,387.48,502.22,120.13,8.10;6,87.54,512.30,185.48,8.10">The SMART Retrieval System Experiments in Automatic Document Processing</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.06,528.20,402.54,8.10;6,87.54,538.22,420.07,8.10;6,87.54,548.30,253.08,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,330.78,528.20,176.82,8.10;6,87.54,538.22,36.34,8.10">A Study of Methods for Negative Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,149.64,538.22,357.97,8.10;6,87.54,548.30,183.58,8.10">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;08)</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.06,564.20,402.56,8.10;6,87.54,574.28,420.17,8.10;6,87.54,584.36,82.94,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,262.14,564.20,140.35,8.10">Relevance-Based Language Models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,413.16,564.20,94.46,8.10;6,87.54,574.28,420.17,8.10;6,87.54,584.36,13.37,8.10">Proceedings of the 24th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (SIGIR &apos;01)</title>
		<meeting>the 24th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (SIGIR &apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.06,600.26,402.51,8.10;6,87.54,610.28,195.51,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,291.94,600.26,139.69,8.10">Structured Queries for Legal Search</title>
		<author>
			<persName coords=""><forename type="first">Yangbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.86,600.26,58.71,8.10;6,87.54,610.28,139.12,8.10">Proceedings of the 2007 Text REtrieval Conference</title>
		<meeting>the 2007 Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
