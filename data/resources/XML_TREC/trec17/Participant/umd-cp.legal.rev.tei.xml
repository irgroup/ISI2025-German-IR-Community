<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.15,115.49,315.69,14.93">Query Expansion for Noisy Legal Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.38,149.22,58.47,10.37"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,305.41,149.22,82.90,10.37"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">College of Information Studies</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.15,115.49,315.69,14.93">Query Expansion for Noisy Legal Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">99BEE547AC0CD4A53DE8D8526ADEC31B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The vocabulary of the TREC Legal OCR collection is noisy and huge. Standard techniques for improving retrieval performance such as content-based query expansion are ineffective for such document collection. In our work, we focused on exploiting metadata using blind relevance feedback, iterative improvement from the reference Boolean run, and the effects of using terms from different topic fields for automatic query formulation. This paper describes our methodologies and results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Legal Track is designed to model a real-world challenge known as "e-discovery." In e-discovery, given a "request for production" and its associated complaint, the goal is to return all relevant documents without also returning an unreasonably large number of non-relevant documents. A request for production is a description of documents that are of interest to the requesting party. It is often broadly worded, in order to force the opposing party to produce a comprehensive set of documents that may potentially be useful in court. The two parties might then negotiate a query, which is modeled by a sequence of "Boolean" queries in the Legal Track's Ad Hoc task that lead to an agreed "reference" Boolean query.</p><p>Experience from the first two years of the Legal Track suggest that it would be productive to further explore techniques for recall enhancement. In information retrieval, people often find it difficult to anticipate all of the ways in which terms might be used in the collection. Three broad classes of vocabulary enrichment techniques have been used to bridge that gap: interactive relevance feedback <ref type="bibr" coords="1,418.39,506.87,11.81,9.46" target="#b8">[9,</ref><ref type="bibr" coords="1,433.82,506.87,7.88,9.46" target="#b7">8]</ref>, fully automatic (i.e., "blind") relevance feedback <ref type="bibr" coords="1,197.33,520.42,11.81,9.46" target="#b3">[4,</ref><ref type="bibr" coords="1,212.14,520.42,8.18,9.46" target="#b5">6,</ref><ref type="bibr" coords="1,223.31,520.42,7.88,9.46" target="#b4">5]</ref>, thesaurus expansion <ref type="bibr" coords="1,329.44,520.42,11.81,9.46" target="#b6">[7,</ref><ref type="bibr" coords="1,344.25,520.42,12.54,9.46" target="#b9">10]</ref>). Noisy OCR results pose challenges for blind relevance feedback and thesaurus expansion; however, the presence of extensive metadata in the Legal Track test collection offers some potential for overcoming these limitations, and hence one goal for the experiments reported in this papers was to begin to explore techniques for leveraging some of that metadata (specifically, the author and recipient fields). The intuition behind our approach is that people who sent or received relevant documents for which OCR text is sufficiently accurate may have also sent or received other relevant documents that cannot be found using OCR text alone. Learning these names using blind relevance feedback techniques and adding them to the query might therefore result in finding additional relevant documents. Preliminary experiments using relevance judgments from 2007 suggested that this technique could be effective.</p><p>One of the surprises from the first two years of the TREC Legal Track has been the difficulty of reliably "beating" the reference Boolean run. One possible reason for this is that ranked retrieval techniques weight query terms using techniques such as Inverse Document Frequency (IDF) that are insensitive to the content in which those terms are used in the query. The reference Boolean run, by contrast, includes query operators such as "AND" and "NOT" that can be selectively applied to more precisely convey the searcher's intent in ways that the system can productively employ. Indeed, "Boolean" is somewhat of a misnomer, since the queries for the so-called reference Boolean run also includes proximity and truncation operators. Precisely replicating the reference Boolean run would be time consuming, however, since underspecified details (e.g., tokenization) differ between systems. We therefore adopted the reference Boolean run as a starting point and sought to make iterative improvements from there. The basic idea was to rank the documents found by the reference Boolean run in a putatively best-first order, to do the same for the documents not found by that run, and then to replace the bottom documents in the Boolean ranking (i.e., those least likely to be relevant) with the top documents in the other ranking (i.e., those most likely to be relevant). Preliminary experiments using relevance judgments from 2007 suggested that this technique could yield small improvements (when averaged across all topics).</p><p>As has been observed in previous years, many of the fields in the topic description contain useful query terms, and no one field serves as an optimal selection. In order to provide the strongest possible starting point for our experiments we therefore augmented the original production request (i.e., &lt;RequestText&gt;) with all terms from the Boolean queries proposed by defendant, modified by the plaintiff and agreed as a final query. Preliminary experiments using relevance judgments from 2007 indicated that modest improvements over the use of the &lt;RequestText&gt; field alone could be achieved using this technique.</p><p>The remainder of this paper is organized as follows. In the next section, we describe the design of each of our techniques and we explain how they are used together. We then present the results of our preliminary experiments using TREC-2006 and TREC-2007 relevance judgments, including parameter settings that we learned from that data for use in all of our experiments. Section 4 then presents the results for our officially runs using the 2008 relevance judgments. The paper concludes with some remarks on additional experiments and analysis that would be useful, and some implications for future Legal Track design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>This section describes our approach of metadata-based query expansion. We also describe how document indexing, query formulation and result sets formulation were done. The process by which specific parameter values were chosen is described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>The collection consists of 6,910,192 documents, each of which includes text that was automatically generated from a document image using Optical Character Recognition (OCR) and several fields of metadata (e.g., author, recipient, and organizations). The same document collection have been used in all three years of the track <ref type="bibr" coords="2,124.15,564.97,11.81,9.46" target="#b1">[2,</ref><ref type="bibr" coords="2,138.60,564.97,12.95,9.46" target="#b10">11]</ref>. We indexed the OCR text and each metadata field separately using Indri <ref type="bibr" coords="2,474.93,564.97,11.59,9.46" target="#b0">[1]</ref>. No special processing was applied to correct OCR errors; the raw OCR text was indexed. Each term in the OCR text and the title metadata field was stemmed using the Porter stemmer; terms found in the author or recipient field were not stemmed. Indri retrieval model is used for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Initial Query Formulation</head><p>We extracted terms from the &lt;RequestText&gt;, &lt;FinalQuery&gt;, &lt;ProposalByDefendant&gt; and &lt;Rejoinder-ByPlaintiff&gt; fields to form bag-of-words queries. Functional phrases such as "All documents describing" and "Documents referring to" were automatically removed using a script. If the same word appeared more than once in the extracted text, it was only counted once. Many Boolean queries contained wildcard characters. For example, "advertis!" was intended to match all words with prefix "advertis". When the &lt;RequestText&gt; field contained a complete word that matched a prefix followed by a wildcard character, we used the full word. Otherwise, we ignored wildcard characters. This would be expected to have little effect on our results because all query terms were subsequently automatically truncated by the Porter stemmer. An exception is that during the metdata-based query expansion phase, the terms found in the author or recipient fields were not stemmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metadata-Based Query Enrichment</head><p>A substantial portion of the document collection consists of memoranda and (printed!) email for which the social network of senders and recipients might be useful. Moreover, some of the same people serve as authors for more formal documents (e.g., reports) that are also contained in the collection. This observation motivated our choice of the social network as a potentially useful source of evidence. The challenge, of course, is query formulation. Although we could, and perhaps should, ask that the names of specific people be included in the negotiated Boolean query, that has not yet been tried in the TREC Legal Track. We therefore adopted a less direct technique of learning potentially useful names-we simply assumed that people who had sent or received documents that we believed were likely to be relevant were more likely than other people to have also sent or received other relevant documents. That formulation motivated our selection of a variant of blind relevance feedback to discover these potentially useful senders and recipients.</p><p>The key idea behind blind relevance feedback is that "terms are known by the company that they keep." Specifically, we want to find useful additional terms-in this case the names of senders and recipients-that show up remarkably often in documents on some specific topic. Specifically, we look for some number of tokens that frequently occur in top-ranked documents but are rare in the entire collection. We start by performing an Indri search using the initial query described above to find the top-ranked n documents using the OCR text and the title metadata fields. We then extract terms from the author (&lt;au&gt;) and recipient (&lt;rc&gt;) metadata fields as follows. Let f denote a metadata field (f ∈ {au, rc}), let s denote the string found in f in one document. First, individual terms t are identified in s by tokenizing on punctuation and white space. An IDF weight is then assigned to each term t in s as follows:</p><formula xml:id="formula_0" coords="3,258.50,473.28,95.00,25.64">IDF f,t = log N DF f,t ,</formula><p>where N is the number of documents in the collection, and DF f,t denotes the frequency of term t in the field f in the entire collection. We then sum the IDF weights for each unique term t found in the top m documents to compute an "offer weight" w(t) for each term as follows:</p><formula xml:id="formula_1" coords="3,254.17,562.48,103.66,10.77">w(t) = T F f,t • IDF f,t ,</formula><p>where T F f,t denotes the number of occurrences of term t in field f in the top m documents. Note that this differs from the usual formulation of TF*IDF weights because the IDF is computed over individual strings, while the TF is computed over a set of strings. Finally, the terms are sorted in decreasing order of offer weight, and the n most highly ranked terms are selected. m and n are free variables that were selected using the process described in Section 3.</p><p>A new Indri query is then formed as a weighted combination of the original query and metadata search terms:</p><formula xml:id="formula_2" coords="3,239.55,681.83,132.90,10.63">#weight(λ Q i (1 -λ) Q m ),</formula><p>where Q i is the initial query described above and λ is a free parameter selected using the process described in Section 3. The Indri query operator</p><formula xml:id="formula_3" coords="4,220.41,113.57,171.18,10.63">Q m = #wsyn(w(t 1 ) t 1 . . . w(t n ) t n )</formula><p>treats the terms t i as synonyms, but allows weights w(t i ) to be assigned to each term. A weight on the term denotes its relative importance in the synonym list. For Q m , Indri operator #od1 is used to perform exact match of t i in the metadata field f ; in other words, only documents whose meta data matches t f i are returned. Stemming is not performed for metadata terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Reference boolean run</head><p>Results for the reference Boolean run were provided for each topic, ordered alphabetically by documentID. These documents match the final negotiated Boolean query. The B value, the number of documents matching the final negotiated Boolean query, was also provided (B varies by topic from a low of 170 to a high of 99,374). Since these results are intended to model present practice, one question of interest to us is how to form a set of size B that achieves better results (as measured by F 1 at B) than the reference Boolean run. Tuning Boolean queries to yield result sets that are neither too large nor too small is well known to be difficult <ref type="bibr" coords="4,109.77,311.18,11.59,9.46" target="#b2">[3]</ref>, but the choice of B as the reference value for this comparison obviated that disadvantage of Boolean queries in the context of the TREC 2007 Legal Track's evaluation framework. As a result, as of 2007, participants in the Ad Hoc task of the Legal Track have yet to reliably "beat Boolean." One possible cause for this is that no research team automatically formulated queries in as nuanced a way as the final negotiated Boolean queries. One potentially productive line of research would be to learn to automatically construct richer queries, perhaps using some combination of blind relevance feedback and rule induction. That's a fairly major undertaking, however, so we have chosen a more modest way of initially exploring the potential of this approach.</p><p>Our strategy is to replace p documents in the reference Boolean run results by an equal number of documents chosen from the rest of the collection in a way that we would expect to make them more likely to be relevant. Algorithm 1 presents the details. The inputs to Algorithm 1 are the reference boolean run R b and a ranked list for the entire collection R c created as described above. For efficiency reasons, we actually work with only the part of R c in which the documents contain at least one term from query Q (because other documents would simply be ordered arbitrarily). We tried two variants of this algorithm, one in which the same value of p was used for every topic and a second in which p was allowed to vary by topic by setting it to a constant fraction of the B value for that topic. These were free variables that were selected using the process described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary Experiments</head><p>We conducted some preliminary experiments using the full set of 43 topics from the Ad Hoc task of the TREC-2007 Legal track for which relevance judgments were available. <ref type="foot" coords="5,387.16,112.20,3.99,6.91" target="#foot_0">1</ref> We elected not to use the TREC 2006 legal track relevance judgments because the sampling strategy that year emphasized highly-ranked documents (and because only very limited additional relevance judgments for the 2006 topics were available from the 2007 relevance feedback task).</p><p>We initially used these 2007 "training" topics to explore some key alternatives in what was initially quite a broad design space. Preliminary experiments found that retaining wildcard characters could adversely affect retrieval effectiveness when the prefix was common. Retaining the full version of a query term when possible proved to be helpful, perhaps because the full word in &lt;RequestText&gt; is often the most common version of the wildcarded term found in &lt;FinalQuery&gt;, &lt;Proposal ByDefendant&gt; and &lt;RejoinderByPlaintiff&gt;. These initial experiments were not systematic, but they were sufficient to motivate the design of our wildcard character handling.</p><p>Figure <ref type="figure" coords="5,121.04,263.29,5.45,9.46" target="#fig_0">1</ref> shows some locally scored results from a few of our initial exploratory runs using the 2007 topics and relevance judgments. Run UMD-CR-C50 was produced by swapping out some low-ranked documents from the 2007 reference Boolean run with highly ranked documents from a ranked retrieval run in which terms from all three Boolean fields were used to form the query; in this case the number of swapped documents was 50. Runs UMD-AURC-P1 and UMD-AU-P5 explored different metadata expansion and swapping strategies: run UMD-AURC-P1 used blind relevance feedback on metadata to add author and recipient names to the ranked query that was used as a basis for swapping (with p = 0.01 * B); run UMD-AU-P5 did the same with just author names (in this case with p = 0.05 * B). The results of these and other early experiments led us to conclude that metadata-based expansion and our swap-based merging straregy for building up from the reference Boolean run both seemed to be viable strategies. Note that we held out no data for development test, these represent a train-on-test condition that indicate potential, but not actual, retrieval effectiveness on unseen data.</p><p>With this as motivation we conducted a more systematic evaluation of parameters for our metadatabased expansion algorithm, trying a range of reasonable values for m and n. Ultimately, we settled on the top 10 terms from the sender and recipient fields in the top 20 documents (i.e, m = 20 and n = 10). We also found that λ = 0.8 was a reasonable choice. In our exploration of this parameter space we varied one parameter at a time, selecting the optimal value for each before proceeding to the next parameter. We then conducted a sensitivity analysis around the final values to select the optimal combination.</p><p>Our most systematic exploration of the parameter space was for finding the values of p that optimized the value of F 1 at B. In the TREC 2008 Legal Track Ad Hoc task participants were allowed to select any value for K, the threshold at which their system would be evaluated. We selected K = B for each topic because comparability with the reference Boolean query was essential to our goal. We used a bag-of-words query formed from &lt;RequestText&gt;, &lt;FinalQuery&gt;, &lt;ProposalByDefendant&gt; and &lt;RejoinderByPlaintiff&gt; fields with no metadata expansion for the parameter tuning process (although the same parameters were also later used with expanded queries). The algorithm performs a hill-climbing search on a local optimal value of p, the number of lowest-ranked documents in the reference Boolean run for which highly-ranked top-p documents from the remainder of the collection were substituted. We initially tried fixed values of P from {10, 20, 30, . . . , B} in that order, stopping as soon as F 1 @B first declined. This resulted in an optimal value p = 40. This value turns out to be smaller than the minimum B for any 2008 topic, so no special handling is needed for "small" topics. We then repeated the hill climbing process with p be some fraction of B, where p ∈ {0.01 * B, 0.02 * B, . . . , 1.00 * B} with the same stopping criterion. The optimal fraction turned out to be 0.03 * B. We used these same parameters for our TREC 2008 Legal Track Ad Hoc task experiments. Both values are relatively small, so the upside potential of this approach is somewhat limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">2008 Experiments</head><p>A total of 45 topics were provided as an XML file. We submitted five runs, each containing about 100, 000 rank-ordered documents for each topic. Each run was scored twice, once using all 26 topics for which relevance judgments were available and using only the 24 documents for which highly relevant documents had been identified. In this paper we report only estimated F 1 @B because we set K = B and K h = B (K h is K value for highly-relevant) for all runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submitted Runs</head><p>1. UMD-STD<ref type="foot" coords="6,134.94,536.98,3.99,6.91" target="#foot_2">2</ref> is our standard condition run, using all terms in the "RequestText" field to form a bag-ofword query using Indri's "#combine" operator. Functional phrases such as "All documents describing" and "Documents referring to" are ignored.</p><p>2. UMD-CR-C40 is a ranked list obtained by 1) producing an initial ranked list (R c ) with a bag-of-word query Q from keywords in "RequestText", "FinalQuery", "ProposalByDefendant" and "RejoinderByPlaintiff"; 2) ranking the Boolean run by R c ; 3) replacing the bottom p = 40 documents in the ranked Boolean run with the top p = 40 documents from R c (after redundancy elimination w.r.t. the Boolean run); 4) sorting the result by R c . 3. UMD-CR-P3 is the same with UMD-CR-C40 except p was chosen to be 3% of the B value for each topic.</p><p>4. UMD-AURC-C40 is a ranked list obtained by 1) producing an initial ranked list with a bag-of-words query from all terms in "RequestText," "FinalQuery," "ProposalByDefendant" and "RejoinderByPlaintiff;" 2) extracting the top n = 10 terms from the author and recipient metadata fields of the top n = 20 documents in the initial ranked list (Section 2. 5. UMD-AURC-P3 is the same as UMD-AURC-C40 except p was chosen to be 3% of the B value for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Figure <ref type="figure" coords="7,103.48,508.77,5.45,9.46" target="#fig_2">2</ref> reports our results for the principal evaluation measure F1@B. For comparison, we also show the same result for Boolean, the reference Boolean run provided by the organizers. Selectively merging with the Boolean run improves retrieval effectiveness, when the "least likely" Boolean retrieved documents are replaced by the "most likely" documents from the Boolean complement. However, somewhat surprisingly, the runs that employ metadata-based expansion before merging with Boolean (UMD-AURC-P3 and UMD-AURC-C40) do not perform as well as the runs that simply merge with Boolean (UMD-CR-P3 and UMD-CR-C40). One possible reason is that metadata-based expansion is not content-based; it scores documents in part based on how likely their authors or recipients are "relevant". This can bring in some non-relevant documents that would adversely affect retrieval effectiveness.</p><p>Figure <ref type="figure" coords="7,120.67,630.71,5.45,9.46">3</ref> presents the same conditions using only "highly relevant" judgments. In this case, metadatabased expansion (UMD-AURC-C40) apparently improved retrieval effectiveness, although it is not yet statistically significanct. The results do seem to depend on how the p value is chosen: UMD-AURC-C40 (in which 40 documents from Boolean reference run are swapped) appears to consistently result in slightly bet-Figure <ref type="figure" coords="8,171.26,266.54,4.24,9.46">3</ref>: Official results, estimated F 1 @B, only highly relevant documents.</p><p>ter retrieval effectiveness than UMD-AURC-P3 (in which 3% of the Boolean reference run was swapped).</p><p>Averages across all topics can mask substantial variation, so Figure <ref type="figure" coords="8,392.75,326.88,5.45,9.46" target="#fig_4">4</ref> compares estimated F 1 @K between UMD-CR-C40 and the median of the 41 submitted runs from all participating teams. <ref type="foot" coords="8,478.54,338.39,3.99,6.91" target="#foot_3">3</ref> A total of 7 of the 26 topics show improvements over the median that exceed Sparck-Jones' suggested threshold of 0.1 (absolute) for an improvement that interactive users would find large enough to be "meaningful," while an adverse effect of that magnitude is present for only one of those 26 topics.</p><p>Quite a lot of analysis remains to be done, but some implications for future work are already evident. We briefly summarize some of our thoughts on that in those issues in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>The experiment results provide some evidence that we can productively leverage the presence of metadata in the TREC Legal Track test collection. It also seems that we can do at least as well as the reference Boolean run through the simple expedient of building new runs by perturbing the Boolean results rather than starting from scratch without the benefit of the Boolean query. Of course, there are a number of parameters that we could optimize. For example, it may be useful to parameterize our blind relevance feedback process differently, or to perform more than one feedback iteration. But our work is more interesting for the possibilities it opens up for more broadly exploring the design space. For example, we might try different ways of making use of the feedback, perhaps using some form of co-training rather than simply searching a unified index containing terms from multiple fields as we do now. Additional metadata fields might also prove to be useful. For example, the Bates number might provide evidence for the degree of physical proximity of documents in their original storage location, which might be connected to some degree of related meaning. With more than 100 topics now available, the TREC Legal Track test collection is a useful resource for exploring these questions, and we look forward to doing so in our future work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,80.47,473.86,212.00,9.88;4,88.94,488.46,337.26,10.85;4,88.94,502.01,186.62,10.71;4,88.94,515.63,310.09,10.77;4,88.94,529.18,262.22,10.77;4,88.94,542.73,429.69,10.63;4,88.94,556.28,16.07,10.63;4,88.94,569.83,187.10,10.77;4,88.94,583.38,341.42,10.63"><head>Algorithm 1 :</head><label>1</label><figDesc>Forming new ranked list of size B Input: The reference boolean run R b ; a ranked list of the entire collection R c Output: A new ranked list R new of size B Rank the reference boolean run R b using R c . Denote the result by R br ; Remove the bottom p documents, denoted by B p , from R br ; Select a set of top p documents from R c , such that the set contains no overlapping documents with B p ; Combine these top p documents with R br ; Optionally, sort the resulting documents by R c to from a new ranked list R new</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,177.60,272.72,256.80,10.63;6,144.00,72.00,324.00,184.66"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Preliminary 2007 experiments, estimated F 1 @B.</figDesc><graphic coords="6,144.00,72.00,324.00,184.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,159.60,266.19,292.80,10.63;7,144.00,72.00,324.00,178.13"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Official results, estimated F 1 @B, all relevant documents.</figDesc><graphic coords="7,144.00,72.00,324.00,178.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,229.05,378.87,310.95,9.46;7,72.00,392.07,468.00,10.63;7,72.00,405.62,468.00,10.63;7,72.00,419.17,204.50,10.63"><head></head><label></label><figDesc>3); 3) forming a new query combining the initial query and metadata terms (Section 2.3) denoting the resulting ranked list R c ; 4) ranking the reference Boolean run by R c , and substituting bottom p = 40 documents by the top p = 40 documents from R c (after redundancy elimination w.r.t. Boolean run); 5) sorting the result by R c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,72.00,265.66,468.00,10.63;9,72.00,279.56,81.80,9.46;9,144.00,72.00,324.01,177.60"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Difference between UMD-CR-C40 and median, estimated F 1 @K, all relevant documents. Topics are in sorted order.</figDesc><graphic coords="9,144.00,72.00,324.01,177.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,144.00,72.00,324.00,178.13"><head></head><label></label><figDesc></figDesc><graphic coords="8,144.00,72.00,324.00,178.13" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,88.14,650.96,22.92,7.77"><p>TREC</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2007" xml:id="foot_1" coords="5,135.43,650.96,404.57,7.77;5,72.00,661.92,468.00,7.77;5,72.00,672.87,37.60,7.77"><p>results were reported on 42 topics, so the results reported here are not strictly comparable with those results.Relevance judgments for one additional TREC 2007 Legal Track Ad Hoc task topic became available after official scores were computed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="6,88.14,666.69,346.57,7.77"><p>We have added dashes that were not present in the submitted run names for improved readability</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="8,88.14,651.91,300.31,8.06"><p>We show F1@K rather than F1@B because K varied for other participating teams.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,92.39,332.08,359.44,7.77" xml:id="b0">
	<monogr>
		<ptr target="http://www.lemurproject.org" />
		<title level="m" coord="9,92.39,332.08,249.12,7.77">The Lemur Toolkit for Language Modeling and Information Retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,346.86,447.61,7.94;9,92.39,357.98,20.17,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,218.05,347.02,118.10,7.77">TREC 2006 legal track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,354.99,346.86,181.08,7.73">Proceedings of the 15th Text Retrieval Conference</title>
		<meeting>the 15th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,372.76,362.82,7.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,134.48,372.76,190.69,7.73">Language and representation in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Blair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Elsevier Science Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,387.71,447.61,7.94;9,92.39,398.66,79.94,7.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,186.38,387.87,296.70,7.77">Using probabilistic models of information retrieval without relevance information</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,502.46,387.71,37.54,7.73;9,92.39,398.66,53.42,7.73">Journal of Documentation</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,413.61,447.61,7.94;9,92.39,424.57,361.50,7.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,183.13,413.77,217.74,7.77">Query expansion using local and global document analysis</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jinxi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,422.17,413.61,117.83,7.73;9,92.39,424.57,335.59,7.73">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,439.51,447.61,7.94;9,92.39,450.47,312.68,7.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,228.56,439.67,134.33,7.77">Improving automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,379.13,439.51,160.88,7.73;9,92.39,450.47,286.77,7.73">Proceedings of the 21st Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,465.41,447.61,7.94;9,92.39,476.37,223.14,7.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,162.33,465.58,111.28,7.77">Concept based query expansion</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Frei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,289.56,465.41,250.44,7.73;9,92.39,476.37,197.23,7.73">Proceedings of the 16th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 16th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,491.32,447.61,7.94;9,92.39,502.28,63.10,7.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,135.70,491.48,156.02,7.77">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,307.83,491.32,232.17,7.73;9,92.39,502.28,36.99,7.73">The SMART retrieval system experiments in automatic document processing</title>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,517.38,355.73,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,146.18,517.38,274.85,7.77">The SMART retrieval system experiments in automatic document processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,532.16,447.61,7.94;9,92.39,543.12,189.43,7.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,198.38,532.33,280.70,7.77">A cooccurrence-based thesaurus and two applications to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,497.16,532.16,42.84,7.73;9,92.39,543.12,101.80,7.73">Information Processing and Management</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="3" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.39,558.07,447.61,7.94;9,92.39,569.03,100.64,7.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,279.95,558.23,141.89,7.77">Overview of the TREC 2007 legal track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,438.23,558.07,101.77,7.73;9,92.39,569.03,74.29,7.73">Proceedings of the 16th Text Retrieval Conference</title>
		<meeting>the 16th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
