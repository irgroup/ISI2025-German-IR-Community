<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.64,100.64,338.01,19.82">UTDallas at TREC 2008 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,226.20,147.12,27.02,9.88"><forename type="first">Bin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.45,147.12,44.35,9.88"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.36,147.12,40.74,9.88"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.64,100.64,338.01,19.82">UTDallas at TREC 2008 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C61DD1C58DA610FF332EDC56EE60036</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the 2008 TREC Blog track. Our system consists of 3 components: data preprocessing, topic retrieval, and opinion finding. In the topic retrieval task, we applied Lemur IR toolkit and used various techniques for query expansion. In the opinion finding and polarization task, we employed a feature-based classification approach. Then re-ranking was performed using a linear combination of the opinionated score and the topic relevance score. Our system achieved reasonable performance in this evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">System Overview</head><p>We participated in several tasks of the 2008 TREC Blog Track. Figure <ref type="figure" coords="1,396.94,417.54,5.49,9.88">1</ref> shows the flow diagram of our system. First, data preprocessing is implemented to remove HTML tags and useless context, and extract content from the blog web pages. Second, we apply the Lemur Information Retrieval toolkit to retrieve 1000 relevant documents for each topic. Query terms are selected from the title and descriptions, and weighted according to their TFIDF values. In addition, more weight is given to topical terms and quoted expressions for each topic. Third, for opinion finding, we employ a classification framework that exploits rich linguistic features, including lexical features, polarized features, and sentimental analysis based on mutual information with predefined sentiment terms. The same method is also used for polarity task, but with different class tags in the classifiers. Re-ranking is performed using a simple linear combination of the opinion score and the relevance scores provided by the opinion analysis and the topic retrieval modules respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preprocessing</head><p>The Blog06 test collection (Ounis et al., 2006) contains more than 3 million permalinks (a total size of about 148G). In order to obtain plain text data for document retrieval and opinion finding, we preprocessed the data using two filters: html-tag filter and non-English Blog filter. This also reduced the size of the data collection and made the subsequent modules more efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>First we used an html-tag filer to remove the useless characters and tags in the data. The blog web pages in the corpus have been collected from internet in HTML format. Some HTML tags are used to describe the structure of text-based information in a document, denoting certain text as links, headings, paragraphs, lists, and so on. Examples of such structure markers are "&lt;TABLE&gt;", "&lt;TD&gt;", and "&lt;TR&gt;". Other tags are used to supplement the textual information with interactive forms, embedded images, and other objects. For example, "&lt;script&gt;, &lt;link&gt;" are used for denoting functionalities. These extra characters and tags are generally irrelevant to our topic retrieval and opinion finding tasks and thus need to be removed.</p><p>During the html-tag filtering procedure, we treated the content between different markers differently. For some useful tags that provide important information about the topic of the documents, such as "&lt;title&gt;&lt;/title&gt;", we did not remove the content between the tags. Another example is the tag "&lt;a href&gt; &lt;/a&gt;", which defines a link connection between different documents, and sometimes we cannot remove the link part. For example, in the sentence "The Buck's freshman phenom, Yi, shared his thoughts on being selected with the Milwaukee Journal Sentinel ", "Milwaukee Journal Sentinel" is a link to another related post and is a meaningful unit in this context. So in this case we only removed the tag "&lt;a href&gt; &lt;/a&gt;" and kept the content between the tags. In comparison, the content wrapped by &lt;script&gt; and &lt;/script&gt; is pure programming scripts which are irrelevant to the topic, therefore we eliminated both the tags and the content. Table 1 lists all the content-irrelevant tags that we considered in this preprocessing step. After html tag filtering, the size of the blog corpus was reduced to 28G. &lt;style&gt; &lt;script&gt; &lt;fieldset&gt; &lt;form&gt; &lt;!--comments--&gt; //comments &lt;address&gt; &lt;acronym&gt; &lt;abbr&gt; &lt;server&gt; &lt;select&gt; &lt;option&gt; &lt;strike&gt; &lt;button&gt; Table <ref type="table" coords="3,254.05,122.46,4.28,9.88">1</ref>: Content irrelevant tags.</p><p>The second part of data processing is to remove non-English blogs. We noticed that some pages in the data collection are not in English, which may cause problems in subsequent processing. In order to exclude those languages for the topic retrieval and opinion finding modules, we constructed a non-English blog filter based on a heuristic rule. In a blog, if the proportion of English characters along with regular symbols such as ".", "#", "?" is less than a predefined threshold (we used 0.5), we considered it as a non-English blog. This step further reduced the size of the data collection to 24G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topic Retrieval</head><p>Topic queries used in the Blog Track have three fields: title (T), description (D), and narrative structure (N). Each year, 50 topics were used for the blog track evaluation. Up till now, there are 150 topics in total. Before the retrieval process, we built index using the Lemur Information Retrieval Toolkit<ref type="foot" coords="3,475.08,487.74,3.51,6.32" target="#foot_0">1</ref> based on the preprocessed corpus. For each of the topic queries, the retrieval engine returns 1000 relevant documents, along with their relevance scores. We expanded queries using two approaches. For the title and description field retrieval, we applied a TFIDF-based approach to expand query terms to a proper size. For the title field only retrieval, a Google-set based query expansion method was used. We used the built-in pseudo feedback model in the Lemur toolkit. The following subsections explain in detail the query processing for the two conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query processing for title and description</head><p>A. Term weighting In our system, we expanded the query for each topic using the terms that appear in &lt;title&gt; &lt;desc&gt; fields. Each term was assigned a TFIDF-based weight indicating how significant it is to that query. TF is the number of occurrences of a term i in a query q j ,</p><formula xml:id="formula_0" coords="3,253.68,707.20,82.60,37.06">∑ = k j k j i j i n n tf , , ,</formula><p>where n i,j is the number of occurrences of the considered term in query q j , and the denominator is the number of occurrences of all the terms in query q j . IDF is the inverse document frequency, obtained by treating each query as a "document":</p><formula xml:id="formula_1" coords="4,227.94,128.66,138.56,28.19">| } : { | | | log j i j i q t q Q IDF ∈ =</formula><p>where |Q| is the total number of queries, and</p><formula xml:id="formula_2" coords="4,277.50,175.78,82.51,12.09">| } : { | j i j q t q</formula><p>∈ is the number of queries containing term t i (that is, n ij &gt; 0). The IDF is a measurement of the general importance of the term in the entire query collection. In addition, we also created a global IDF table that is computed using a large blog corpus (about 1/5 blog posts of the blog data collection). Then we chose the lower score from the two IDF tables as the IDF weight of a term. This can be thought of as a smoothing approach that helps to obtain better estimation and reduces noises caused by stop words and some frequently used terms in the blog query collection such as "find", "opinion", "comments".</p><p>B. Part-of-Speech filtering We notice that using query terms composed of only nouns achieved the highest MAP in both 2006 and 2007 blog track evaluation, so we removed all of the other non-noun words from the query terms.</p><p>C. Quote weighting For each topic, we consider the expression inside the quotation marks as the most informative unit, i.e., "March of the Penguin". Therefore, we used the quotes as is, without any part-of-speech filtering for those words. In addition, we assigned the weight for the entire term as twice of the sum of the weights of all the individual terms expanded for this query. This way Lemur will be more sensitive to such kind of quotes. For example, for query 851 as shown in the earlier example, the expanded terms with the weights are "0.5 documentary, 0.5 film, 1.0 march, 1.5 penguins", then "March of the Penguins" will have a weight of 2 * (0.5 + 0.5 + 1.0 + 1.5 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query processing for title only</head><p>Google Sets is an online search engine which predicts related items based on co-occurrence statistics by using the web as a big pool of data. For the Title field only retrieval, we took advantage of this online resource to expand the topic terms to a larger set by picking the top 15 terms for each topic.</p><p>Then we fed the new query to Lemur toolkit for the retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Opinion Finding</head><p>In our system we created a learning based classification framework to assign each topic-relevant blog a posterior probability, which indicates how likely it is opinionated. Then the re-ranking was conducted based on the combination of this posterior probability and the relevance score generated in the topic retrieval stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification framework</head><p>Since the goal of the opinion finding task is to rank those blogs higher that are more relevant and more opinionated, it is reasonable to detect an opinionated blog based on the part of it that is relevant to a specific topic rather than using the entire blog text. Motivated by this, we selected some topic specific sentences for each relevant blog for opinion analysis. This way, if one blog belongs to more than one topic, it might possibly be assigned different opinionated scores for different topics. We first split a blog into sentences based on sentence boundary detection<ref type="foot" coords="5,363.96,151.93,3.51,6.32" target="#foot_1">2</ref> , and used Lemur to retrieve top 5 relevant sentences corresponding to the topic. Then for each retrieved sentence, we also extracted its preceding and following sentences. Thus for each blog, we used a maximum 15 sentences to perform classification.</p><p>Before feature extraction, we first conducted some text normalization using regular expressions and rule-based approaches. Currently only months, weeks, numbers are normalized. For example, we replaced "September" with "MONTH", "2008" with "NUM". In addition, for the sentimental seed words (described later), we also replace them in the text with a polarized tag. For example, "good" becomes "O-POS" (positive). The features we explored are listed as follows, including words, part-of-speech, polarized features, and sentimental score statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lexical features</head><p>In addition to the bag-of-words feature, we also considered the following n-gram lexical features. Note that w i and p i is the word form and its part-of-speech tag. Combination of a word and its part-of-speech tag: w i p i Bigram words: w i-1 w i Bigram of word and the neighboring part-of-speech tag: w i-1 p i and w i p i-1</p><p>Trigram features: w i-1 w i w i+1 and p i-1 p i p i+1 Trigram syntactic patterns: w i-1 p i w i+1 and p i-1 w i p i+1 Note that when we extracted the above features, we obtained some polarized features due to the normalization of those sentimental seed words mentioned above. For instance, if the i th word is a positive sentimental seed word, we obtain polarized bigrams and trigram such as "w i-1 _O-POS", "O-POS_p i+1 ", "p i-1 _O-POS_p i+1 " and "w i-1 _O-POS_w i+1 ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sentimental features based on Mutual Information (MI) score</head><p>A Sentimental score is used to evaluate sentiment polarity of a textual context. The following describes the steps we used to compute this score. . (i) Generating sentiment terms First of all, we manually selected a group of sentimental seed words as shown in Table <ref type="table" coords="5,458.73,637.80,4.12,9.88" target="#tab_1">2</ref>. Then, based on a large review corpus <ref type="bibr" coords="5,182.88,651.13,3.51,6.32" target="#b2">3</ref> , we computed the statistics of co-occurrences between the seed words and other adjectives appearing around them (we applied a window size 3 to limit the co-occurring span of the adjective words). Adjectives that co-occur with any seed word over 10 times are considered as sentiment terms. With a further human judgment on the polarity of the generated new terms, we compiled 50 positive sentimental terms and 50 negative ones. Some examples of the new selected sentiment terms are shown in Table <ref type="table" coords="6,230.54,91.80,4.12,9.88" target="#tab_2">3</ref> (ii) Calculating MI for adjectives Once we have the sentiment terms, we will use them to compute the MI scores between each of them and an adjective. Here MI is defined as a score to evaluate the polarity strength of an adjective for both positive and negative categories. The following formula shows the MI score for an adjective word for the positive polarity:</p><formula xml:id="formula_3" coords="6,187.38,359.42,216.46,47.68">N win n w Co w MI St n i i ∑ + ∈ + = ) , ,<label>( ) (</label></formula><p>where w i refers to the target adjective; S t+ is the collection of positive sentimental terms; N is the size of S t+ ; Co is the number of times that w i co-occurs with a sentimental term n within a contextual window size of win (5 in our experiments). The co-occurrence statistics were obtained using the blog track 2006 reference collection. Similarly we calculate the MI score of the adjective word for its negative polarity using all the negative sentiment terms.</p><p>(iii) Calculating sentence-level sentiment scores After the above steps, each adjective has a positive and a negative MI score. To calculate the sentiment score for each sentence, we simply added all of its adjectives' MI scores for positive and negative respectively.</p><p>Based on the sentimental scores for each sentence, we calculated the following statistics as features for a blog for opinion classification.</p><p>Mean of the sentence sentiment scores for positive and negative, respectively. Mean of the difference between positive and negative scores among sentences. Mean of the ratio of positive and negative score among sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classifier setup</head><p>We used the annotated Blog-2006 and Blog-2007 data as training data and development data respectively. There are 4 opinion tags in the blog annotation, among which "2, 3, 4" correspond to the opinionated blogs. When using a binary classification (opinionated vs. not), "2, 3, 4" tags correspond to positive instances and "1" is negative class. We can also train a 4-way classifier based on those four tags, then we assign blogs with "2,3,4" hypotheses as opinionated blogs. A comparison between different classification paradigms will be presented later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Re-ranking</head><p>Opinion classification is applied to the relevant documents returned by the first blog retrieval module. Each blog has an associated posterior probability of being opinionated. Then we computed the final score using a linear combination of the opinionated measurement (S_opi) and the relevance score (S_rel) from topic retrieval:</p><formula xml:id="formula_4" coords="7,215.64,215.06,165.63,15.54">rel S opi S final _ * _ * ) 1 ( λ λ + - =</formula><p>where λ is a parameter to adjust the balance between being relevant and opinionated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Polarity Task</head><p>To detect the polarity of a blog, we trained a classifier using blog instances with class tags "2, 3, 4". Once we assign those tags for the relevant blogs returned by the topic retrieval module, we extracted the sentimental positive ("4") and negative ("2") ones. Then we applied the same re-ranking processing as in opinion finding, generating the ranked list of positive and negative blogs. In addition, considering some blogs classified as mixed polarity (tag "3") might also belong to the positive or negative ones, we used those mixed polarity blogs to further expand the negative and positive ranked lists based on the posterior probabilities associated with positive and negative tags. In other words, from the hypothesized mixed opinion blogs, we selected the ones with higher posterior probability for the positive tags and added them to the end of the existing positive ranked list in the order of the posterior probabilities, until we reached 400 blogs (400 is an empirical number we chose). The same approach is also used for the negative ranked list.</p><p>For the polarity task we compared three different classification strategies on the development set, and based on the results, in our final submitted runs, we chose the approach described above (i.e., the last setting below).</p><p>One stage with 4-way classification A 4-way classifier was trained to distinguish blogs as no-opinion, negative opinion, mixed opinion, and positive opinion, similar to the 4-way classification performed in opinion finding section. Directly based on the hypothesis from this classifier, we can generate a positive and negative ranked list respectively. Two-stage with successive binary classification and 3-way classification In this two-stage approach, we trained a 3-way classifier using blog instances with "2,3,4" tags and applied it only to those blogs which are recognized as opinionated ones by a binary classifier used for opinion finding. Then blogs classified as "2" and "4" were selected for the final negative and positive lists.</p><p>One stage with 3-way classification This 3-way classifier was trained with tags "2,3,4", and applied directly to the entire 1000 blogs recognized as relevant ones for each topic, generating the positive and negative lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">System Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance on previous Blog-2006 and 2007 data</head><p>A. Topic retrieval We conducted various experiments on the development set (Blog 2006 and 2007 data) to finalize our submitted system parameters. Table <ref type="table" coords="8,235.88,182.82,5.49,9.88" target="#tab_3">4</ref> shows the results for the title only (T) task using and without using Google-set based query expansion. The improved results suggest that the expanded terms produced by Google-set are helpful for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Year</head><p>T We also evaluated topic retrieval performance using TD and TDN for query expansion respectively. In Table <ref type="table" coords="8,99.23,340.80,4.12,9.88">5</ref>, results show a performance degradation when including words in the &lt;narr&gt; field (adding N) in the query terms. This might be due to some noisy terms introduced when using this field.  <ref type="table" coords="8,174.07,436.38,4.12,9.88">5</ref>. Comparison between using TD and TDN for topic retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Opinion finding</head><p>For the system development for opinion finding and polarity recognition, we used the reference relevant documents, instead of the baseline results from our topic retrieval module. Therefore the results shown below are generally better than those in the submitted runs. The experiments reported here are mainly used to finalize the parameters for the opinion and polarity finding task.</p><p>Table <ref type="table" coords="8,101.53,561.18,5.49,9.88" target="#tab_4">6</ref> shows the results for opinion finding on the Blog 2007 data using different classifiers, classification strategies, and text normalization choices. The classifier was trained using the Blog 2006 data. For the 4-way classification, we show results using SVM and the maximum entropy (Maxent) classifier. Since Maxent performs much better than SVM, we chose to use Maxent for other experiments. We can see that using the Maxent classifier, the binary classification framework outperforms the 4-way classification, especially after text normalization.  <ref type="table" coords="9,350.02,91.80,4.12,9.88" target="#tab_5">7</ref>. As described in Section 5, we tried different classification strategies on this task, among which one-stage method with a 3-way classifier obtained the best results compared to the other settings. Similar to the opinion finding task, the SVM classifier performed poorly for both opinion finding and polarity task. Both text normalization and the expansion using blogs with mixed opinion (tag "3") based on the corresponding posterior probabilities yielded performance gain on the development set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Submission performance at Blog-2008</head><p>A. Topic retrieval Table <ref type="table" coords="9,99.17,407.82,5.49,9.88">8</ref> shows the topic retrieval results of our submitted runs. "SplBaseT" denotes the run using only the topic information, and "SplBaseTD" used both topic and description information of the query. The query processing approaches for these runs are described in Section 3. Using both title and description (TD) yielded better performance, consistent with what we have observed from the Blog-2007 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunID</head><p>Map P@5 P@10 SplBaseT 0.3077 0.6000 0.5960 SplBaseTD 0.3298 0.6480 0.6380 Table <ref type="table" coords="9,212.77,534.60,4.12,9.88">8</ref>. Topic retrieval performance at Blog-2008. In addition, we employed feature scaling in our classifier, which scales each feature into the range between 0 and 1. Table <ref type="table" coords="10,176.52,76.20,10.98,9.88" target="#tab_8">10</ref> shows the MAP results with and without scaling. Overall, there seems to be slight improvement due to feature value scaling. Blog-2008 data. Finally we show the effect of the interpolation weight when combining the topic relevance score and the opinion score. Results are shown in Figure <ref type="figure" coords="10,280.29,218.58,5.49,9.88" target="#fig_2">2</ref> using different weights for different baseline systems. We notice that most baselines (3, 4 and 5) prefer the higher weight on the relevance score, indicating a more dominant role of the topic retrieval component. This is not true for all baselines due to different retrieval qualities and representation of relevance scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Opinion finding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Polarity task</head><p>For the polarity task, we submitted two results for each baseline corresponding to different combination weights for the relevance score (0.3 and 0.7). We noticed that different baselines prefer different weights. The better results between the two runs are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparisons among performance on different datasets</head><p>In Table <ref type="table" coords="11,111.62,123.00,9.15,9.88" target="#tab_10">13</ref>, we show the MAP scores of our best runs on opinion finding and polarity tasks based on different datasets for comparison (Blog06, 07, and 08). We can see that the performance on Blog-2008 is worse compared to Blog06 and Blog 07. This is as expected since our submitted system is trained on Blog-2006 and Blog-2007 reference data. How to improve the generalization of our systems will be addressed in our future work. </p><note type="other">Dataset</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we described our system in three tasks in the TREC blog track 2008: topic retrieval, opinion finding, and polarity detection. Different query expansion methods have been evaluated for topic retrieval. Our results show that query expansion on Title and Description fields with appropriate weighting can yield better performance. Extensive experiments have also been conducted on the Blog 2007 development data for opinion finding and polarization, proving that the strategies we employed in our system, such as text normalization, polarized features, sentimental features, classification mechanism, are very helpful to improve the system's performance. We will further explore more features and conduct more detailed experiments to evaluate the contribution of different features for opinion finding and polarity task. In addition, we plan to investigate a more effective and systematic combination framework between relevance score and opinionated score of each blog during the re-ranking process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,177.66,403.75,239.94,9.27;2,90.00,77.64,415.32,316.32"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System overview for TREC Blog Track 2008.</figDesc><graphic coords="2,90.00,77.64,415.32,316.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,75.06,452.10,445.17,9.88;10,236.04,467.70,123.27,9.88"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Performance curve using different combination weights for the relevance score and opinion score on different baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,144.06,91.80,307.18,152.74"><head>Table 2 .</head><label>2</label><figDesc>. Sentiment seed words.</figDesc><table coords="6,144.06,123.48,307.18,121.06"><row><cell></cell><cell>Positive</cell><cell>good, excellent, wonderful</cell></row><row><cell></cell><cell>Negative</cell><cell>bad, poor, terrible</cell></row><row><cell>Positive</cell><cell cols="2">good, excellent, wonderful, relaxing, glorious, delicious, priceless, decorated, helpful, superb, …</cell></row><row><cell></cell><cell cols="2">bad, poor, terrible, worse, absent, stupid, problematic,</cell></row><row><cell>Negative</cell><cell cols="2">boring, threatening, …</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,203.52,255.84,188.34,9.88"><head>Table 3 .</head><label>3</label><figDesc>Newly generated sentiment terms.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,110.28,245.70,378.13,58.18"><head>Table 4 .</head><label>4</label><figDesc>Topic retrieval results using title only.</figDesc><table coords="8,110.28,245.70,378.13,42.04"><row><cell></cell><cell>only (Map/P5)</cell><cell>T only, with Google-set query expansion (Map/P5)</cell></row><row><cell>2006</cell><cell>0.29/0.63</cell><cell>0.31/0.63</cell></row><row><cell>2007</cell><cell>0.33/0.64</cell><cell>0.35/0.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,127.74,655.26,321.17,90.40"><head>Table 6 .</head><label>6</label><figDesc>Opinion finding results on Blog-2007 data. C. Polarity task Polarity results on the development data are shown in Table</figDesc><table coords="8,127.74,655.26,321.17,74.32"><row><cell>Classifier</cell><cell>4-way vs. binary</cell><cell>Text normalization</cell><cell>Map/P@5</cell></row><row><cell>SVM</cell><cell>4-way</cell><cell>No</cell><cell>0.018/0.32</cell></row><row><cell>Maxent</cell><cell>4-way</cell><cell>No</cell><cell>0.384/0.536</cell></row><row><cell>Maxent</cell><cell>binary</cell><cell>No</cell><cell>0.4/0.66</cell></row><row><cell>Maxent</cell><cell>binary</cell><cell>Yes</cell><cell>0.45/0.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,84.36,201.48,424.51,138.22"><head>Table 7 .</head><label>7</label><figDesc>Results for the polarity task on Blog-2007 data.</figDesc><table coords="9,84.36,201.48,424.51,122.08"><row><cell cols="2">Classifier 2-pass vs.</cell><cell>Classification</cell><cell>Text</cell><cell>Expansion</cell><cell>Pos</cell><cell>Neg</cell></row><row><cell></cell><cell>1-pass</cell><cell>strategy</cell><cell>normalization</cell><cell>from mixed</cell><cell>(Map/P5)</cell><cell>(Map/P5)</cell></row><row><cell>SVM</cell><cell>One</cell><cell>4-way</cell><cell>No</cell><cell>No</cell><cell>.009/.29</cell><cell>.031/.73</cell></row><row><cell>Maxent</cell><cell>One</cell><cell>4-way</cell><cell>No</cell><cell>No</cell><cell>.26/.54</cell><cell>.07/.51</cell></row><row><cell>Maxent</cell><cell>Two</cell><cell>bin+3-way</cell><cell>No</cell><cell>No</cell><cell>.16/.51</cell><cell>.14/.61</cell></row><row><cell>Maxent</cell><cell>Two</cell><cell>bin+3-way</cell><cell>Yes</cell><cell>No</cell><cell>.20/.51</cell><cell>.13/.59</cell></row><row><cell>Maxent</cell><cell>One</cell><cell>3-way</cell><cell>Yes</cell><cell>No</cell><cell>.26/.47</cell><cell>.16/.58</cell></row><row><cell>Maxent</cell><cell>One</cell><cell>3-way</cell><cell>Yes</cell><cell>Yes</cell><cell>.35/.47</cell><cell>.27/.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,72.00,581.40,451.34,137.68"><head>Table 9</head><label>9</label><figDesc>shows the best result for opinion finding we obtained based on different baselines. The different performance suggests that the quality of topic retrieval has a great impact on opinion finding.</figDesc><table coords="9,238.26,628.68,113.68,90.40"><row><cell>Baseline</cell><cell>Map</cell></row><row><cell>1</cell><cell>0.3251</cell></row><row><cell>2</cell><cell>0.2789</cell></row><row><cell>3</cell><cell>0.3565</cell></row><row><cell>4</cell><cell>0.3844</cell></row><row><cell>5</cell><cell>0.3036</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,182.58,725.28,230.26,9.88"><head>Table 9 .</head><label>9</label><figDesc>Opinion finding performance atBlog-2008.   </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,72.78,123.48,449.73,58.18"><head>Table 10 .</head><label>10</label><figDesc>Comparison between feature scaling and non-scaling. Results shown are the MAP results on</figDesc><table coords="10,125.40,123.48,348.39,42.10"><row><cell>Baseline</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>No scaling</cell><cell>0.3242</cell><cell>0.2754</cell><cell>0.3565</cell><cell>0.3071</cell><cell>0.2629</cell></row><row><cell>Scaling</cell><cell>0.3251</cell><cell>0.2789</cell><cell>0.3565</cell><cell>0.3465</cell><cell>0.2874</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,150.30,545.70,354.02,218.68"><head>Table 12 .</head><label>12</label><figDesc>Table 12 for each baseline. Polarity task performance at Blog-2008.</figDesc><table coords="10,150.30,577.38,270.16,170.86"><row><cell></cell><cell>Baseline</cell><cell>Map</cell><cell>P@5</cell></row><row><cell></cell><cell>1</cell><cell>0.1110</cell><cell>0.2286</cell></row><row><cell></cell><cell>2</cell><cell>0.0760</cell><cell>0.1388</cell></row><row><cell>Positive</cell><cell>3</cell><cell>0.1120</cell><cell>0.1592</cell></row><row><cell></cell><cell>4</cell><cell>0.1350</cell><cell>0.2286</cell></row><row><cell></cell><cell>5</cell><cell>0.1108</cell><cell>0.2041</cell></row><row><cell></cell><cell>1</cell><cell>0.0815</cell><cell>0.1917</cell></row><row><cell></cell><cell>2</cell><cell>0.0719</cell><cell>0.1583</cell></row><row><cell>Negative</cell><cell>3</cell><cell>0.0888</cell><cell>0.1833</cell></row><row><cell></cell><cell>4</cell><cell>0.0962</cell><cell>0.2250</cell></row><row><cell></cell><cell>5</cell><cell>0.0746</cell><cell>0.1792</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="11,83.94,217.08,427.47,105.52"><head>Table 13 .</head><label>13</label><figDesc>Opinion finding and polarity detection results for Blog 06, 07, and 08 based on the best performing systems.</figDesc><table coords="11,129.42,217.08,341.86,73.78"><row><cell></cell><cell>Opinion</cell><cell>Positive</cell><cell>Negative</cell></row><row><cell></cell><cell>(NOpMMs4_0.3)</cell><cell>(NTrMM4_0.7)</cell><cell>(NTrMM4_0.7)</cell></row><row><cell>Blog-2006</cell><cell>0.57</cell><cell>0.19</cell><cell>0.18</cell></row><row><cell>Blog-2007</cell><cell>0.70</cell><cell>0.31</cell><cell>0.20</cell></row><row><cell>Blog-2008</cell><cell>0.35</cell><cell>0.14</cell><cell>0.10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,79.50,761.55,105.29,8.10"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,79.50,730.53,440.57,8.10;5,72.00,740.85,60.03,8.10"><p>The sentence boundary detection is done by using "mxterminator", a toolkit developed by Adwait Ratnaparkhi, University of Pennsylvania.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,79.50,751.23,428.82,8.10;5,72.00,761.55,81.49,8.10"><p>This corpus comprises of 2000 movie reviews from (Pang et al., 2002), custom reviews from (Hu and Liu, KDD-2004) and 256 hotel reviews.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,76.12,614.28,447.23,9.88;11,72.00,629.88,277.11,9.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,359.45,614.28,163.91,9.88;11,72.00,629.88,150.69,9.88">Thumbs up? Sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lilian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sharivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,241.85,629.88,100.79,9.88">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,76.12,645.48,447.34,9.88;11,72.00,661.08,72.05,9.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,239.85,645.48,194.67,9.88">Mining and summarizing customer reviews</title>
		<author>
			<persName coords=""><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,456.49,645.48,66.97,9.88;11,72.00,661.08,65.72,9.88">Proceedings of ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,76.12,676.68,447.25,9.88;11,72.00,692.28,136.43,9.88" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<title level="m" coord="11,479.98,676.68,43.39,9.88;11,72.00,692.28,131.77,9.88">Overview of the TREC-2006 Blog Track</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
