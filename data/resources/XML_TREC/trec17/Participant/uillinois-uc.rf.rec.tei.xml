<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,129.60,70.07,350.58,18.83;1,79.12,89.99,451.53,18.83">A Study of Adaptive Relevance Feedback -UIUC TREC-2008 Relevance Feedback Experiments</title>
				<funder ref="#_yWFsvx6">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_BsnBUbM #_AQtypYy">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,160.63,136.36,62.48,12.55"><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,371.65,136.36,92.34,12.55"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@cs.uiuc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,129.60,70.07,350.58,18.83;1,79.12,89.99,451.53,18.83">A Study of Adaptive Relevance Feedback -UIUC TREC-2008 Relevance Feedback Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EFCC8F3360F00BAADE8BEECCF1B23D79</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we report our experiments in the TREC 2008 Relevance Feedback Track. Our main goal is to study a novel problem in feedback, i.e., optimization of the balance of the query and feedback information. Intuitively, if we over-trust the feedback information, we may be biased to favor a particular subset of relevant documents, but undertrusting it would not take advantage of feedback. In the current feedback methods, the balance is usually controlled by some parameter, which is often set to a fixed value across all the queries and collections. However, due to the difference in queries and feedback documents, this balance parameter should be optimized for each query and each set of feedback documents.</p><p>To address this problem, we present a learning approach to adaptively predict the balance coefficient (i.e., feedback coefficient). First, three heuristics are proposed to characterize the relationships between feedback coefficient and other measures, including discrimination of query, discrimination of feedback documents, and divergence between the query and the feedback documents. Then, taking these three heuristics as a road map, we explore a number of features and combine them using a logistic regression model to predict the feedback coefficient. Experiments show that our adaptive relevance feedback is more robust and effective than the regular fixed-coefficient relevance feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Among many techniques for improving the accuracy of ad hoc information retrieval, relevance feedback is arguably one of the most effective techniques and has been shown to be effective with variety of retrieval models <ref type="bibr" coords="1,249.14,544.02,9.72,9.41" target="#b7">[7,</ref><ref type="bibr" coords="1,263.01,544.02,7.16,9.41" target="#b6">6,</ref><ref type="bibr" coords="1,274.33,544.02,7.16,9.41">8,</ref><ref type="bibr" coords="1,285.63,544.02,7.16,9.41" target="#b4">4,</ref><ref type="bibr" coords="1,53.79,554.48,10.75,9.41" target="#b10">10]</ref>. In the vector space model, feedback is usually done with the Rocchio algorithm, which forms a new query vector by maximizing its similarity to relevant documents and minimizing its similarity to non-relevant documents <ref type="bibr" coords="1,260.81,585.86,9.21,9.41" target="#b7">[7]</ref>. The feedback method in classical probabilistic models is to select expanded terms primarily based on Robertson/Sparck-Jones weight <ref type="bibr" coords="1,83.09,617.24,9.21,9.41" target="#b6">[6]</ref>. In the recently proposed language modeling approaches, relevance feedback can be implemented through estimating a query language model <ref type="bibr" coords="1,193.36,638.16,9.72,9.41" target="#b3">[3,</ref><ref type="bibr" coords="1,205.42,638.16,11.77,9.41" target="#b10">10]</ref> or relevance model <ref type="bibr" coords="1,53.79,648.63,9.72,9.41" target="#b4">[4]</ref> through exploiting a set of feedback documents.</p><p>All these existing methods show that combining feedback information with the original query typically improves the performance. However, we need to carefully balance the query and feedback information because if we over-trust the feedback information, we may be biased to favor a particular subset of relevant documents, but under-trusting it would not take advantage of feedback. In the current feedback methods, the balance is usually controlled by some parameter, which is often set to a fixed value across all the queries and collections. However due to the difference in queries and feedback documents, this balance parameter presumably should be optimized for each query and each set of feedback documents.</p><p>As far as we know, how to optimize the balance of the query and feedback information has not been well studied in previous work. Thus, in our work, we study this novel problem in relevance feedback and propose an adaptive feedback method which predicts a dynamic balance coefficient by using a learning approach. Specifically, we estimate a potentially different feedback coefficient for each query and each set of feedback documents, rather than manually set it to a fixed constant. We hypothesize that the proposed method will do better than the current fixed-coefficient approaches.</p><p>We explore a number of features potentially correlated with the feedback coefficient and classified them into three categories: (1) discrimination of query: we expect that the "clearer" (i.e., more discriminative) the query is, the less feedback we need. (2) discrimination of feedback documents: we hypothesize that clearer feedback documents can be trusted more. <ref type="bibr" coords="1,387.95,491.01,11.77,9.41" target="#b3">(3)</ref> divergence between the query and the feedback documents: if the divergence between a query and its feedback documents is large, it means that the query does not represent relevant documents well, thus we may need a larger feedback coefficient. Following these three heuristics, we explored a number of features and combined them using a logistic regression model <ref type="bibr" coords="1,446.11,553.78,9.72,9.41" target="#b2">[2]</ref> to predict the feedback coefficient.</p><p>Through preliminary experiments, we observe that, although a well-tuned fixed coefficient is not optimal for many queries, it provides a "safe" coefficient range. Compared with it, our predicted value is sometimes too extreme and thus "risky." So we also experimented with some strategies to smooth our prediction using the safe fixed coefficient value. We hypothesize that, with smoothing, our adaptive relevance feedback method would be more robust.</p><p>In our experiments, the basic retrieval method is the KLdivergence retrieval model <ref type="bibr" coords="1,429.76,668.85,9.72,9.41" target="#b3">[3]</ref> with the Dirichlet smoothing method <ref type="bibr" coords="1,368.09,679.30,9.72,9.41" target="#b9">[9]</ref> plus a generative mixture model feedback method <ref type="bibr" coords="1,351.87,689.77,13.51,9.41" target="#b10">[10]</ref>, which adopts our predicted feedback coefficient. Our proposed method has shown clear improvements in our experiments over a robust fixed-coefficient relevance feedback method; it is also observed that most features that we explore help predict the feedback coefficient. Through further analysis, we find that our adaptive relevance feedback approach is still robust and effective even if training and testing data sets are inconsistent.</p><p>In the rest of this paper, we will first introduce our basic retrieval method in Section 2. After that, we will present the adaptive relevance feedback method in Section 3. In Section 4, we will describe how to smooth the prediction value to make relevance feedback more robust. We report our experimental results in Section 5 and conclude our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RETRIEVAL METHOD</head><p>To make our algorithm clear, we break down the relevance feedback task into four steps: initial retrieval, adaptive feedback coefficient prediction, coefficient smoothing, and query language model updating. At the retrieval step, we adopt the KL-divergence retrieval model with Dirichlet smoothing method to do an initial retrieval, based on which, a couple of features are explored to predict the feedback coefficient using the logistic regression model. After that, several strategies are applied to smooth our prediction value using a tuned fixed coefficient. Finally, the smoothed value is plugged into the mixture model relevance feedback method to update the query language model.</p><p>In this section, we present our basic retrieval approaches, the KL-divergence retrieval model and the mixture model feedback method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The KL-Divergence Retrieval Model</head><p>The KL-divergence retrieval model <ref type="bibr" coords="2,206.83,396.38,9.72,9.41" target="#b3">[3]</ref> is a generalization of the query likelihood retrieval method proposed in <ref type="bibr" coords="2,265.41,406.85,9.72,9.41" target="#b5">[5]</ref> and can support feedback more naturally than the query likelihood method. In this model, all the queries and documents are represented by unigram language models, which are essentially word distributions. Assuming that these language models can be appropriately estimated, the KL-divergence retrieval model scores a document D with respect to a query Q by computing the negative Kullback-Leibler divergence between the query language model θQ and the document language model θD as follows:</p><formula xml:id="formula_0" coords="2,66.39,516.44,212.73,25.13">S(Q, D) = -D(θQ||θD) = - w∈V p(w|θQ) log p(w|θQ) p(w|θD)</formula><p>where V is the set of words in our vocabulary. Clearly, the retrieval performance of the KL-divergence would depend on how we estimate the document model θ D and the query model θ Q . The document model θ D needs to be smoothed and an effective method is Dirichlet smoothing <ref type="bibr" coords="2,245.71,589.33,9.21,9.41" target="#b9">[9]</ref>:</p><formula xml:id="formula_1" coords="2,113.26,605.27,118.98,21.29">p(w|θD) = c(w, D) + µp(w|C) |D| + µ</formula><p>where p(w|C) is the collection language model and is estimated with p(w|C) = c(w,C) w c(w,C) , and µ is a smoothing parameter and is usually set empirically. Across all of our experiments, we used the Dirichlet prior smoothing method for estimating document language models.</p><p>The query model intuitively captures what the user is interested in, thus would affect retrieval accuracy significantly. Without feedback, θ Q is often estimated as p(w|θ</p><formula xml:id="formula_2" coords="2,245.66,54.37,95.00,665.13">Q ) = p(w|Q) = c(w,Q)</formula><p>|Q| , where c(w, Q) is the count of word w in the query Q, and |Q| is the total number of words in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Mixture Model Feedback Method</head><p>The query model described above, however, is not very discriminative because a query is typically extremely short. Several different methods have been proposed to improve the estimation of θQ by exploiting documents, especially those documents that are used for relevance feedback or pseudorelevance feedback <ref type="bibr" coords="2,395.02,152.66,9.72,9.41" target="#b3">[3,</ref><ref type="bibr" coords="2,408.37,152.66,7.16,9.41" target="#b4">4,</ref><ref type="bibr" coords="2,419.18,152.66,10.75,9.41" target="#b10">10]</ref>. In <ref type="bibr" coords="2,450.91,152.66,13.51,9.41" target="#b10">[10]</ref>, it was proposed that feedback can be implemented in the KL-divergence retrieval model as updating the query model based on the feedback documents. Specifically, we can define a two-component mixture model (i.e., a fixed background language model p(w|C) estimated using the whole collection and an unknown topic language model to be estimated) and assume that the feedback documents are generated using such a mixture model. Formally, let θT be the unknown topic language model and F ⊂ C be a set of feedback documents. The log-likelihood function of the mixture model is:</p><formula xml:id="formula_3" coords="2,319.42,275.97,233.89,19.33">L(F|θ T ) = D∈F w∈V c(w, D) log[(1 -λ)p(w|θ T ) + λp(w|C)]</formula><p>where λ is a mixture noise parameter which controls the weight of the background model. Given a fixed λ (λ = 0.9 in our experiments), a standard EM algorithm can then be used to estimate parameters p(w|θ T ), which is then interpolated with the original query model p(w|Q) to obtain an improved estimation of the query model:</p><formula xml:id="formula_4" coords="2,361.35,371.09,150.02,9.81">p(w|θ Q ) = (1 -α)p(w|Q) + αp(w|θ T )</formula><p>where α is the feedback coefficient. Similarly to other existing feedback methods <ref type="bibr" coords="2,419.15,398.57,9.72,9.41" target="#b7">[7,</ref><ref type="bibr" coords="2,432.95,398.57,7.16,9.41" target="#b6">6,</ref><ref type="bibr" coords="2,444.21,398.57,6.48,9.41">8]</ref>, the parameter α in this formula is generally fixed across all queries and documents.</p><p>However, due to the difference in queries and feedback documents, the coefficient α, which indicates the balance between query and feedback, should be optimized for each query and each set of feedback documents. This motivates us to study how to optimize the balance of the query and feedback information. We view this problem as a prediction problem and propose a learning approach to solve it. Although we explore this idea in the context of the mixture model feedback method in this paper, it could be applicable to other feedback methods as well. We now present our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FEEDBACK COEFFICIENT PREDICTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Heuristics and Features</head><p>In this work, we investigate three heuristics to predict the feedback coefficient: discrimination of query, discrimination of feedback documents, and divergence between the query and the feedback documents. The three heuristics capture intrinsic characteristics of the two main components (i.e. query and feedback document set) and the relationship between these two components in a feedback process. We argue, and then show experimentally in Section 5, that the three heuristics all play important roles in predicting the feedback coefficient. Possibly, many other features can be explored by taking the three heuristics as a road map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Discrimination of Query</head><p>Intuitively, if the query itself is discriminative enough, we do not need to rely heavily on feedback documents. Hence, we expect the discrimination of query is correlated with the feedback coefficient. Several measures are proposed to quantify it.</p><p>(1) Query Length: Intuitively, for two queries</p><formula xml:id="formula_5" coords="3,53.79,108.44,239.10,20.28">Q 1 and Q 2 , if Q 1 is longer than Q 2 (i.e</formula><p>. Q 1 has more terms than Q2), Q1 is usually discriminative than Q2. Therefore, the query length could be a characteristic of the discrimination of a query. To capture this intuition, we introduce query length |Q| as our first feature. Formally, it is defined as:</p><p>|Q|: the number of terms in query Q.</p><p>(2) Entropy of Query: It is known that more entropy means more randomness and less discrimination. Therefore, we could adopt such a concept to measure how discriminative a query is. To compute the entropy, we need to estimate the query language model first, which, however, involves again an interpolation between the original query model θQ and the pseudo feedback document model θ F as well as the setting of a feedback coefficient. (Note that we have used a slightly different notation θ F for relevance feedback document model, and throughout this paper we estimate pseudo feedback models by using the top 50 documents.) To avoid this problem, in this paper, we do not estimate an entropy for the interpolated query model directly, instead, two entropy scores respectively for θ Q and θ F are computed, allowing the training system to weigh them, which is expected to get an appropriate approximation. Assume that each query term only appears once in a query, the entropy of θ Q is defined as:</p><formula xml:id="formula_6" coords="3,53.79,390.88,219.80,39.97">QEnt A1 = w∈Q -p(w|θQ) log 2 p(w|θQ) = log 2 |Q| Where θ Q is estimated as p(w|θ Q ) = c(w,Q) |Q| = 1</formula><p>|Q| . We can see that QEnt A1 is a negative logarithm transformation of query length |Q|. Thus in effect, we just have another query length feature.</p><p>Similarly, we defined the entropy of θ F as follows:</p><formula xml:id="formula_7" coords="3,90.46,479.97,165.79,19.72">QEnt A2 = w∈F -p(w|θ F ) log 2 p(w|θ F )</formula><p>where p(w|θ F ) is estimated as p(w|θ F ) = c(w,F ) w c(w,F ) . (3) Relative Entropy of Query:</p><p>In the definition above, query entropy is affected significantly by common terms (e.g., 'the', 'and', ...). This problem can be addressed by using a mixture model to separate the topic model from the background model <ref type="bibr" coords="3,148.86,563.24,13.51,9.41" target="#b10">[10]</ref>. However, they both are quite time-consuming. So, we adopt a similar idea of "relative entropy of query" as proposed in <ref type="bibr" coords="3,191.51,584.17,9.72,9.41" target="#b1">[1]</ref> to compute the query clarity score, which measures the coherence of the language usage in query language models as compared to the collection model. The "query clarity" has been shown an intrinsic feature of queries and has an important impact on the retrieval performance <ref type="bibr" coords="3,137.21,636.47,9.21,9.41" target="#b1">[1]</ref>. Therefore, we expect that it can also predict the feedback coefficient.</p><p>In the definition, the clarity of a query is the Kullback-Leibler divergence of the query model from the collection model. Similar to the computation of query entropy, an important role in this definition is the estimation of a query model. To avoid it, we use the same strategy by computing two clarity scores for θ Q and θ F respectively.</p><p>To further reduce the effect of common terms, θ F is smoothed with the collection language model using Jelinek-Mercer smoothing method with a λ of 0.7 <ref type="bibr" coords="3,423.38,77.07,10.31,9.41" target="#b9">[9]</ref>. Following <ref type="bibr" coords="3,482.50,77.07,9.21,9.41" target="#b1">[1]</ref>, we define relative entropy QEnt R1 and QEnt R2 as follows:</p><formula xml:id="formula_8" coords="3,357.87,104.13,155.79,64.85">QEnt R1 = w∈Q p(w|θQ) log p(w|θ Q ) p(w|C) QEnt R2 = w∈F p(w|θ F ) log p(w|θ F ) p(w|C)</formula><p>where p(w|C) is the collection language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Discrimination of Feedback Documents</head><p>Intuitively, if feedback documents are more discriminative, it means that they focus more on the relevant topic and far away from noise. Therefore, discriminative feedback documents can be trusted more in the feedback process.</p><p>(1) Feedback Length: For a query Q and two possible relevant judgment sets F1 and F2, if F1 has more documents than F 2 , usually F 1 contains more intensive relevant information than F 2 ; thus, F 1 could be discriminative than F 2 in describing relevant information. Therefore, the number of feedback documents, which we define as feedback length, can be taken as a characteristic of the discrimination of feedback document set. Formally, feedback length |F | is defined as follows:</p><p>|F |: the number of documents in F .</p><p>(2) Entropy of Feedback Documents: Feedback length, as described above, captures the discrimination of feedback documents on the document level, whereas the entropy of feedback documents, which measures the term distribution, is on the term level. Usually, more entropy means a more random term distribution; thus it is not clear which topic the feedback documents talk about. Similarly to the computation of query entropy, the entropy of feedback model θ F is defined as:</p><formula xml:id="formula_9" coords="3,356.16,477.51,160.41,19.33">F BEnt A = w∈F -p(w|θ F ) log 2 p(w|θ F )</formula><p>where p(w|θ F ) is estimated as p(w|θ F ) = c(w,F ) w c(w,F ) . (3) Relative Entropy of Feedback Documents: Similar to Query Entropy QEnt A2, the computation of feedback document entropy F BEnt A is also affected severely by common terms. So, we follow the same idea to smooth θF using Jelinek-Mercer smoothing method and then compute the "relative entropy of feedback documents" as an alternative feature, which is defined as follows:</p><formula xml:id="formula_10" coords="3,360.56,598.59,150.41,25.14">F BEnt R = w∈F p(w|θ F ) log p(w|θ F ) p(w|C)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Divergence between Query and Feedback Documents</head><p>The motivation of divergence between query and feedback documents is that, we have to rely on feedback more, if the query does not represent relevant information well (i.e., the divergence between the query and its feedback documents is large.) Below, we list two measures to quantify it.</p><p>(1) Absolute Divergence:</p><p>A direct and intuitive way to estimate the divergence is computing the divergence between query model θ Q and feedback model θF using the KL-divergence formula. It is clear that θF is easily estimated using the Maximum Likelihood estimator: p(w|θ F ) = c(w,F ) w c(w,F ) . We simply use pseudo feedback document model θ F instead of θ Q to compute the divergence, which is defined below:</p><formula xml:id="formula_11" coords="4,92.82,143.66,159.88,25.14">QF BDiv A = w∈F p(w|θF ) log p(w|θ F ) p(w|θ F )</formula><p>To prevent zero probability, θ F is smoothed using the collection language model as p(w|θ F ) = c(w,F )+µp(w|C) w c(w,F )+µ where µ is set to 1500.</p><p>We call this divergence "absolute divergence" in contrast to the relative divergence to be defined below.</p><p>(2) Relative Divergence:</p><p>With the above absolute divergence, it is often difficult to say that a large divergence value means a bad query, because the absolute divergence only relies on θ F and θ Q but does not take other useful factors into consideration, e.g., the divergence between query model and negative feedback model. In fact, if the divergence between query and negative feedback is much larger than that between query and positive feedback documents, we can say that the query represents relevant information well, no matter what is the absolute divergence value.</p><p>To address this problem, we propose another feature to capture a relative divergence. Considering a scenario: in a searching process, if document D is judged as a relevant document but its rank in the result document list is very low, it also shows that the query does not represent the feedback documents well. Hence, intuitively, the rank of a document also measures the divergence between query and feedback documents, and such a measure seems more comparable among different queries. Because there are sometimes more than one feedback documents, we adopt an average rank in our predicting system, as follows:</p><formula xml:id="formula_12" coords="4,125.59,459.67,94.33,25.19">QF BDiv R1 = d∈F r d |F |</formula><p>Where r d is the rank of document d, e.g., the rank of the first document is 1 and the second one is 2 ...; |F | is feedback length as described before.</p><p>In the formula above, a large QF BDiv R1 value means a low rank. Intuitively, we would like QF BDiv R1 to positively contribute to the measure of the divergence between query and feedback documents, which simply says that a higher QF BDiv R1 implies a larger divergence. However, we would like the contribution from a rank measure to drop quickly when the QF BDiv R1 is low and become nearly constant as it becomes higher. The rationale of this heuristic is the following: a low rank of feedback documents (i.e., large QF BDiv R1) often implies large divergence between query and feedback documents, thus we should take consideration of such a measure when computing the divergence; however, when QF BDiv R1 is very large (i.e., the feedback documents are ranked very low), the contribution of rank should not be so sensitive to the difference in ranks as when it is small. The heuristic suggests a concave curve for QF BDiv R1 and the query-feedback divergence as shown in Figure <ref type="figure" coords="4,95.60,699.23,3.58,9.41" target="#fig_0">1</ref>. To capture such a heuristic, we propose another measure by taking a logarithm transformation on the average rank to approximate the divergence div(Q, F ), as follows:</p><formula xml:id="formula_13" coords="4,381.88,321.31,107.78,25.19">QF BDiv R2 = log d∈F r d |F |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Algorithm</head><p>Based on our heuristics and features, we hope to utilize some learning technique to obtain equations which predict feedback coefficients.</p><p>We propose to use the logistic regression model <ref type="bibr" coords="4,517.44,406.44,9.21,9.41" target="#b2">[2]</ref>, which appears to model our problem well: it can take any value from negative infinity to positive infinity as an input, whereas the output is confined to values between 0 and 1.</p><p>Logistic regression models are also called maximum entropy models in some communities. In particular, logistic regression models are of the form:</p><formula xml:id="formula_14" coords="4,395.90,483.66,79.73,21.29">f (z) = 1 1 + exp(-z)</formula><p>where the variable z represents the exposure to some set of features, while f (z) represents the probability of a particular outcome, given that set of features. The variable z is a measure of the total contribution of all the features used in the model, which is usually defined as z = wx. Specifically, x is a vector of numeric values representing the features, for instance, our features might include query length |Q|, the entropy of feedback documents, etc. And w represents a set of weights, which indicate the relative weights for each feature. A positive weight means that the corresponding feature increases the probability of the outcome, while a negative weight means that its corresponding feature decreases the probability of that outcome; a large weight means that the feature strongly influences the probability of that outcome; while a near-zero weight means that the feature has little influence on the probability of that outcome.</p><p>Typically, we learn these weights using training data. For our problem, the training data would consist of feature values along with the corresponding optimal feedback coefficient. To construct such a training data set, we exhaust the feedback coefficient space for each query-feedback pair to find its optimal coefficient (more details are given in Section 5), where each query-feedback pair together with its optimal coefficient form our training data. Because logistic regression models have a global optimum, the choice of learning algorithm is usually of little importance. In our study, we use the statistical package R 1 to train our model.</p><p>Once the weight vector w of the equation have been derived for a particular data set (training data), these weights may be used to predict feedback coefficients for new queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FEEDBACK COEFFICIENT SMOOTHING</head><p>One advantage of our adaptive feedback algorithm is that we can naturally incorporate many features as evidence to improve our estimation of the feedback coefficient. However, through preliminary experiments, we observe that, although a fixed coefficient may not be optimal for many queries, it provides a "safe" coefficient range; compared with it, our predicted dynamic value, though optimized based on many features, is sometimes too extreme and thus "risky". To address this problem, we experimented with some strategies to smooth our prediction using the safe fixed coefficient value. We hypothesize that, with smoothing, our adaptive relevance feedback method would be more robust.</p><p>Formally, let α f be the fixed feedback coefficient and α d be the dynamically predicted coefficient. Our task here is to estimate a more robust feedback coefficient, which we denote by α c , obtained by smoothing α d using α f . We now describe several different smoothing strategies.</p><p>(1) Linear Interpolation: Our first idea is to linearly interpolate the two feedback coefficients (i.e., α f and α d ) to obtain the final coefficient α c , which is defined below.</p><formula xml:id="formula_15" coords="5,129.06,402.99,88.09,9.87">αc = (1 -β)α f + βα d</formula><p>where β ∈ [0, 1] is a parameter to control the weight on each coefficient. If β = 0, α c simplifies to α f ; If β = 1, α c simplifies to α d ; otherwise α c is between α f and α d . In our experiments, β is experimentally set to 0.5.</p><p>(2) Range Normalization: Suppose there is a safe range for feedback coefficient [α f -δ l , α f +δ r ], and we would like to restrict α c in the safe range. If we have some prior knowledge about δ l and δ r , we can obtain the safe range easily. However, most of the time, we do not have such knowledge and have to approximate δ l and δr. In this work, they are empirically approximated as:</p><formula xml:id="formula_16" coords="5,151.44,524.24,141.46,9.88">δ l = γ(α f -0) and δ r = γ(1 -α f ),</formula><p>where we use a γ to indicate the breadth of safe range. Based on these assumptions, we propose to use the following formula to obtain the final feedback coefficient.</p><formula xml:id="formula_17" coords="5,53.79,572.27,239.09,36.99">α c = 2δα d + α f -δ where, if α d &lt; α f , then δ = δ l ; otherwise, δ = δ r . And it is clear that, αc is restricted to [α f -δ l , α f + δr].</formula><p>In our experiments, γ is experimentally set to 0.5.</p><p>(3) Pivoted Interpolation: Intuitively, if the feedback coefficient is not very large, the performance of relevance feedback will be at least as good as that of the original query; however, if we use a large coefficient, we have a relative higher possibility to hurt the retrieval performance. To strike a balance between exploration and exploitation, we suggest a preservative strategy to take advantage of the predicted coefficient but at the same time not to be involved </p><formula xml:id="formula_18" coords="5,316.81,267.42,168.28,27.50">α c = (1 -β)α f + βα d where if α d &lt; α f , β = 1; otherwise β = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Preprocessing</head><p>We employ the Lemur toolkit (version 4.5) and Indri search engine (version 2.5)<ref type="foot" coords="5,401.99,355.57,3.66,6.28" target="#foot_0">2</ref> in our experiments. Below, we describe how we pre-process the data collection and how to prepare the training data.</p><p>First, due to the large size of the GOV2 data collection, we decide to do the retrieval experiments on a subset of the collection, instead of the whole 426G data. To make the retrieval experiments on our working sets equally to that on the whole data set, we use Indri to construct 5 working sets, each for one task. Specifically, we first build an index on the whole Gov2 collection, then we retrieve result documents for each query (for task B-E, we do relevance feedback based on the corresponding positive judgments), and finally these result documents are extracted to construct our working sets. The related parameters we adopt to construct working sets is shown in Table <ref type="table" coords="5,387.19,503.29,3.58,9.41" target="#tab_0">1</ref>, other parameters are the same as Indri's default setting.</p><p>Furthermore, we pre-compute a global collection model θC using the whole data set, and in the following experiments, whenever we need to access a local collection language model (i.e., language model of a specified working set) to smooth document models, we just use θ C instead. To very if our working sets work appropriately, we try some retrieval experiments on them and observe that the retrieval performance is almost the same to that on the whole data set.</p><p>After constructing working sets, we build a separate index for each working set. Throughout this paper, when building any index, we only stem words using the Porter algorithm, without any other preprocessing.</p><p>To train our adaptive relevance feedback model, we need some training data. In our study, we use the Terabyte topics (701-850, excluding those included in this year's test set) as the training queries. There are 100 topics in total, of which  one topic has no relevant documents and another fails to return any relevant documents. So, finally, we adopt 98 topics as training data. Then, for each query, we randomly select some top relevant documents to simulate "judgments". Specifically, for each query, with a probability of 0.3 we will select only 1 relevant document for feedback, and with probabilities of 0.4, 0.1, 0.1 and 0.1 we will select 3, 4, 5 and 6 relevant documents respectively. The distribution over these different numbers of relevant documents is heuristically fixed to {0.3, 0.4, 0.1, 0.1, 0.1} to approximate the numbers of relevant documents used for feedback in the official tasks B, C, and D.</p><p>After that, we also construct a working set for training data using the same parameter setting as task C. Finally, with Lemur toolkit, we adopt the KL-Divergence retrieval model with mixture model feedback to do relevance feedback experiments (related parameters are shown in Table <ref type="table" coords="6,53.79,437.79,3.58,9.41" target="#tab_1">2</ref>); through trying different feedback coefficients (0.0, 0.1, ..., 1.0), we get the optimal coefficient value for each query. The 4th column of Table <ref type="table" coords="6,155.94,458.71,4.61,9.41" target="#tab_3">3</ref> gives some examples of the optimal coefficients. We use this data set to learn the prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sensitivity of Feedback Coefficient</head><p>As we have discussed in Section 2, relevance feedback is controlled by a coefficient α. When α = 0, we are only using the original query model (i.e., no feedback), while if α = 1, we ignore completely the original query and rely only on the feedback model. To show the sensitivity of α, we plot the MAP of some queries (Terabyte topics 757, 776, and 793) in relevance feedback experiments by varying α from 0 to 1. The results are shown in Figure <ref type="figure" coords="6,184.02,589.43,3.58,9.41" target="#fig_2">2</ref>. We can observe that the setting of feedback coefficient α can affect the performance significantly, and the optimal coefficients for different queries could be quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Preliminary Experiment</head><p>We find that Relative Query Entropy QEnt R1, Relative Entropy of Feedback Document F BEnt R, Absolute Divergence QF BDiv A, and Relative Divergence QF BDiv R2 have the most significant influence on the ability to predict accurate feedback coefficients. Thus, we keep the above four features in our final prediction model, and the following co-  Where we use the absolute value of each feature. Then, we can predict feedback directly as below:</p><formula xml:id="formula_19" coords="6,383.04,379.61,105.46,21.29">α = f (z0) = 1 1 + exp(-z0)</formula><p>From the above formulas, we can see clearly that QF BDiv A and QF BDiv R2 play the similar roles as we discussed in Section 3. However, Relative Query Entropy QEnt R1 increases the feedback coefficient, which means that, if a query is more discriminative, we can use a higher feedback coefficient. It is in contrast to our initial expectation. One explanation is that, a more discriminative query (i.e., with a high clarity score) is more drifting-tolerant, and thus it is safe to use a large feedback coefficient in this case. Also F BEnt R is negatively correlated to the feedback coefficient α, and it is also in contrast to our intuition. One possible explanation is that, we do not need a large feedback coefficient if the feedback is too discriminative, since a discriminative feedback can easily drift the original query away.</p><p>With the prediction formula, we can compute potentially different feedback coefficients for different queries. Note that, the four features are all computed efficiently, because we only use the Maximum Likelihood method to estimate related language models. We evaluate our method on the training data by using 10-fold cross validation. Some examples of our prediction values are shown in the second column of Table <ref type="table" coords="6,511.98,626.01,3.58,9.41" target="#tab_3">3</ref>.</p><p>We compare our adaptive feedback method with our baseline system, i.e., the fixed feedback coefficient approach (where we use a fixed coefficient 0.6, since it brings the best performance). Before evaluation, the judged documents are removed from the results, and the result document list is then restricted to only contain the top 1000 documents. Besides, we also compute the Mean Absolute Error (MAError) to indicate how far off the coefficients used in the two</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,316.81,251.24,239.01,9.41;4,316.81,261.71,197.15,9.41"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Approximate relation between the queryfeedback divergence and the average rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,53.79,205.50,239.00,9.41;6,53.79,215.96,131.31,9.41"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The sensitivity to feedback coefficient of some Terabyte query topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,54.26,53.70,501.57,665.40"><head>Table 1 :</head><label>1</label><figDesc>Parameters in Constructing Workingset. The row "Docs per Query" means that we use top 2500 documents of each query to construct working sets</figDesc><table coords="5,54.26,708.42,110.66,10.69"><row><cell>1 http://www.r-project.org/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,316.81,199.79,239.10,59.41"><head>Table 2 :</head><label>2</label><figDesc>Parameters in Relevance Feedbackin too much risk. Specifically, if α d &lt; α f , we use α d as the feedback coefficient; otherwise, we use α f . Formally, this Pivoted Interpolation is defined as follows.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,316.81,260.43,230.23,84.79"><head>Table 3 :</head><label>3</label><figDesc>Samples of Prediction Values efficients are then derived from our training algorithm. z 0 = -0.93265 + 0.09890 * QEnt R1 -1.45937 * F BEnt R + 0.28350 * QF BDiv A + 0.32427 * QF BDiv R2</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="5,321.42,709.69,122.63,9.41"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENT</head><p>This material is based upon work supported by the <rs type="funder">National Science Foundation</rs> under Grant Numbers <rs type="grantNumber">IIS-0347933</rs>, <rs type="grantNumber">IIS-0713581</rs>, and <rs type="grantNumber">FIBR-0425852</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BsnBUbM">
					<idno type="grant-number">IIS-0347933</idno>
				</org>
				<org type="funding" xml:id="_AQtypYy">
					<idno type="grant-number">IIS-0713581</idno>
				</org>
				<org type="funding" xml:id="_yWFsvx6">
					<idno type="grant-number">FIBR-0425852</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>methods and the optimal coefficients. The comparison of performances is shown in Table <ref type="table" coords="7,184.02,249.45,3.58,9.41">4</ref>, which indicates that our approach outperforms the fixed coefficient approach clearly. Next, we perform several experiments to show the contributions of individual features. Table <ref type="table" coords="7,226.50,280.83,4.61,9.41">5</ref> shows the results, where every time one feature is removed singly. Below we give the derived formula to predict coefficient without QEnt R1 as an example. From Table <ref type="table" coords="7,216.25,312.22,3.58,9.41">5</ref>, we can see that each feature plays an important role.</p><p>Also, we design some experiments to evaluate the proposed smoothing methods (Section 4). The results are shown in Table <ref type="table" coords="7,88.60,405.20,3.58,9.41">6</ref>. It indicates that Range Normalization (Norm) ≥ Linear Interpolation (Linear) ≥ Pivot Interpolation (Pivot), however, no smoothing method outperforms our basic adaptive feedback method. Maybe it is because we did not tune the parameters of these smoothing methods, which will be further studied in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Official Run Results</head><p>We submitted two runs for each task, in which various techniques we designed are applied. These runs are described in Table <ref type="table" coords="7,127.07,510.73,3.58,9.41">7</ref>. UIUC.B1, UIUC.C1, UIUC.D1 and UIUC.E1 use the proposed adaptive relevance feedback plus the range normalization smoothing method; UIUC.B2 and UIUC.C2 also adopt our adaptive feedback but respectively use pivot and linear interpolation as the smoothing method; UIUC.D2 only tests the adaptive feedback without any smoothing method; in UIUC.E2, we add a pseudo relevance feedback on top of the adaptive relevance feedback. Table <ref type="table" coords="7,288.19,583.95,4.61,9.41">8</ref> shows the performance of these official runs.</p><p>After our official runs were submitted we discovered that   our implementation of the range normalization was not quite accurate and we had left out the query feedback documents divergence feature QF BDiv A. So, we decided to re-compute our runs. Note that we did not change anything related to our algorithm but just the implementation. We also generated some additional runs to compare different techniques we proposed. Table <ref type="table" coords="7,397.75,376.19,4.61,9.41">9</ref> and 10 show the performance of these runs.</p><p>Comparing to our preliminary experimental results, there are two significant changes: feature QFBDiv R2 hurts the performance; our smoothing methods, especially the Range Normalization, improve the performance. From Table <ref type="table" coords="7,548.67,428.49,3.58,9.41">9</ref>, we can see that "No QFBDiv R2" outperforms "All Features" all the time, and that "No QFBDiv R2" always beats the baseline system; with Range Normalization, the performances of all methods are improved.</p><p>One possible explanation of the two significant changes is that, the training data and the testing data are quite inconsistent. In our training data, we use top relevant documents to simulate judged documents, because in real world, users would like to judge top documents; however, in the testing data, the judged documents are often ranked very low (or even do not occur in the top 2500 result documents). Our feature QFBDiv R2 measures the rank of feedback documents and thus is very sensitive across two data sets. However, on the other hand, it also shows that our prediction  Another reason to explain the changes may be the sparseness of our training data. We only utilize 98 training queries to train 4 features, which have already led to a robust relevance feedback method. It would be interesting to see whether a large number of queries would lead to a more effective logistic regression approach to predict feedback coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In summary, we studied a novel problem in feedback, i.e., optimization of the balance of the query and feedback information, in this year's relevance feedback task, and proposed an adaptive relevance feedback approach to dynamically predict feedback coefficient. Our experiment results show that the our proposed method is robust and effective, which outperforms the fixed-coefficient relevance feedback, even when training and testing data sets are not consistent.</p><p>Besides, we also designed three smoothing strategies to smooth our predicted coefficients to make them more robust. Among the three smoothing methods, Range Normalization is the most effective one; smoothing our prediction value can make it more robust, especially when training and testing data sets are inconsistent.</p><p>Among our features, we find that Relative Query Entropy QEnt R1, Relative Entropy of Feedback Document F BEnt R, Absolute Divergence QF BDiv A, and Relative Divergence QF BDiv R2 are the most effective ones. However, QF BDiv R2 is very sensitive to the data set and should be used carefully.</p><p>There is still much room to explore in the future work. We should study more effective and robust features in the future. And also, we hope to apply our method to other feedback models, e.g. Rocchio Feedback, to show its performance. In addition, it will be interesting to explore how to adaptively predict feedback coefficients in pseudo and implicit relevance feedback models.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,321.29,108.89,96.80,12.55" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,124.59,215.17,9.41;8,335.63,135.05,208.26,9.41;8,335.63,145.50,83.97,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,362.52,135.05,118.18,9.41">Predicting query performance</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,499.70,135.05,40.49,9.41">SIGIR &apos;02</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,156.97,185.47,9.41;8,335.63,167.43,193.38,9.41;8,335.63,177.88,213.95,9.41;8,335.63,188.35,110.55,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,379.30,167.43,145.53,9.41">The Elements of Statistical Learning</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,335.63,177.88,109.73,9.41">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer New York Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,199.80,205.24,9.41;8,335.63,210.26,219.49,9.41;8,335.63,220.73,183.57,9.41;8,335.63,231.19,58.85,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,499.54,199.80,41.34,9.41;8,335.63,210.26,219.49,9.41;8,335.63,220.73,96.06,9.41">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,449.89,220.73,40.49,9.41">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,242.64,220.19,9.41;8,335.63,253.10,214.74,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,490.38,242.64,65.45,9.41;8,335.63,253.10,64.52,9.41">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,419.14,253.10,40.49,9.41">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,264.56,189.00,9.41;8,335.63,275.02,216.89,9.41;8,335.63,285.48,101.84,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,479.35,264.56,45.29,9.41;8,335.63,275.02,172.57,9.41">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,526.43,275.02,26.09,9.41;8,335.63,285.48,11.09,9.41">SIGIR &apos;98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,296.94,192.21,9.41;8,335.63,307.40,207.73,9.41;8,335.63,317.86,171.85,9.41;8,335.63,328.33,82.39,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,335.63,307.40,143.74,9.41">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karen</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,486.70,307.40,56.67,9.41;8,335.63,317.86,167.81,9.41">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,339.78,196.29,9.41;8,335.63,350.24,187.26,9.41;8,335.63,360.70,196.80,9.41;8,335.63,371.16,160.64,9.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,393.84,339.78,138.08,9.41;8,335.63,350.24,32.05,9.41">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,397.86,350.24,125.03,9.41;8,335.63,360.70,192.78,9.41">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<imprint>
			<publisher>Prentice-Hall Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,382.62,216.60,9.41;8,335.63,393.08,202.15,9.41;8,335.63,403.54,171.85,9.41;8,335.63,414.00,82.39,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,474.88,382.62,77.35,9.41;8,335.63,393.08,138.23,9.41">Improving retrieval performance by relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,481.11,393.08,56.67,9.41;8,335.63,403.54,167.81,9.41">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,425.46,205.92,9.41;8,335.63,435.92,218.70,9.41;8,335.63,446.38,186.62,9.41;8,335.63,456.84,58.85,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,498.78,425.46,42.78,9.41;8,335.63,435.92,218.70,9.41;8,335.63,446.38,99.12,9.41">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.94,446.38,40.49,9.41">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,468.29,213.86,9.41;8,335.63,478.76,190.15,9.41;8,335.63,489.22,212.67,9.41;8,335.63,499.68,14.33,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,498.78,468.29,50.72,9.41;8,335.63,478.76,190.15,9.41;8,335.63,489.22,81.97,9.41">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,435.79,489.22,22.41,9.41">CIKM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
