<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.44,80.11,447.19,15.24">Distributed EDLSI, BM25, and Power Norm at TREC 2008</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,133.32,107.48,86.27,9.86"><forename type="first">April</forename><surname>Kontostathis</surname></persName>
							<email>akontostathis@ursinus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Ursinus College</orgName>
								<address>
									<postCode>19426</postCode>
									<settlement>Collegeville</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.24,148.40,61.74,9.86"><forename type="first">Andrew</forename><surname>Lilly</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Saskatchewan</orgName>
								<address>
									<postCode>S7N 5C9</postCode>
									<settlement>Saskatoon</settlement>
									<region>SK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,174.00,189.20,92.04,9.86"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Spiteri</surname></persName>
							<email>spiteri@cs.usask.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Saskatchewan</orgName>
								<address>
									<postCode>S7N 5C9</postCode>
									<settlement>Saskatoon</settlement>
									<region>SK</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.44,80.11,447.19,15.24">Distributed EDLSI, BM25, and Power Norm at TREC 2008</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">446C38CD96544D2903EB088FF715EAEF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the TREC Legal competition in 2008. Our first set of experiments involved the use of Latent Semantic Indexing (LSI) with a small number of dimensions, a technique we refer to as Essential Dimensions of Latent Semantic Indexing (EDLSI). Because the experimental dataset is large, we designed a distributed version of EDLSI to use for our submitted runs. We submitted two runs using distributed EDLSI, one with k = 10 and another with k = 41, where k is the dimensionality reduction parameter for LSI. We also submitted a traditional vector space baseline for comparison with the EDLSI results. This article describes our experimental design and the results of these experiments. We find that EDLSI clearly outperforms traditional vector space retrieval using a variety of TREC reporting metrics.</p><p>We also describe experiments that were designed as a followup to our TREC Legal 2007 submission. These experiments test weighting and normalization schemes as well as techniques for relevance feedback. Our primary intent was to compare the BM25 weighting scheme to our power normalization technique. BM25 outperformed all of our other submissions on the competition metric (F1 at K) for both the ad hoc and relevance feedback tasks, but Power normalization outperformed BM25 in our ad hoc experiments when the 2007 metric (estimated recall at B) was used for comparison.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the 2007 TREC Legal competition, our submissions were designed to test the effectiveness of a variety of simple normalization schemes on a large dataset. We had planned a comparison with some of the state-of-theart systems for weighting and normalization, but these activities were not completed in time for the TREC 2007 submissions. Thus, in 2008 one of our primary goals was to compare power normalization <ref type="bibr" coords="1,258.96,677.32,11.60,8.97" target="#b8">[9]</ref> to the BM25 weighting scheme <ref type="bibr" coords="1,175.80,689.32,15.34,8.97" target="#b12">[13]</ref>. We submitted five ad hoc runs and six relevance feedback runs that compare these algorithms.</p><p>We have extensive experience with Latent Semantic Indexing (LSI) <ref type="bibr" coords="1,377.76,306.76,10.60,8.97" target="#b4">[5]</ref>, <ref type="bibr" coords="1,396.24,306.76,10.60,8.97" target="#b6">[7]</ref>, <ref type="bibr" coords="1,414.84,306.76,10.60,8.97" target="#b7">[8]</ref>, <ref type="bibr" coords="1,433.44,306.76,15.24,8.97" target="#b14">[15]</ref>, <ref type="bibr" coords="1,456.96,306.76,15.24,8.97" target="#b15">[16]</ref>, <ref type="bibr" coords="1,480.48,306.76,15.34,8.97" target="#b10">[11]</ref>. Thus we were also eager to see how LSI would work on the IIT Complex Document Information Processing (IIT CDIP) test collection, which contains approximately 7 million documents (57 GB of uncompressed text). Specifically, we wanted to see if the Essential Dimensions of Latent Semantic Indexing (EDLSI) <ref type="bibr" coords="1,427.80,378.40,11.72,8.97" target="#b4">[5]</ref> approach would scale to this large collection and what the optimal k value would be. We have used SVD updating, folding-in, and foldingup in previous work <ref type="bibr" coords="1,403.08,414.28,15.34,8.97" target="#b14">[15]</ref>, <ref type="bibr" coords="1,426.96,414.28,15.34,8.97" target="#b15">[16]</ref>, <ref type="bibr" coords="1,450.84,414.28,15.24,8.97" target="#b10">[11]</ref>, and it appeared that these techniques would be useful for handling a collection of this size. This year, teams participating in the TREC Legal task were required to indicate the K and K h value for each query. K is the threshold at which the system believes the competing demands of recall and precision are best balanced (using the F1@K measure shown in Equation <ref type="formula" coords="1,312.00,513.28,3.59,8.97">1</ref>), and K h is the corresponding threshold for highly relevant documents. Assessors assigned a value of highly relevant when judging documents for the first time this year. Much of our preparatory work for the competition centered on determining appropriate ways to set K and K h for each query.</p><p>F 1@K = (2 * P @K * R@K)/(P @K + R@K) <ref type="bibr" coords="1,528.36,607.23,11.72,8.97" target="#b0">(1)</ref> This paper is organized as follows: Sections 2, 3, and 4 describe the methodologies used. Section 5 discusses our approach for finding the optimal K and K h values for each query. Section 6 describes our experiments and results when EDLSI was used in the ad hoc task. Section 7 details our power normalization and BM25 ad hoc experiments and results. Section 8 discusses our relevance feedback submissions with power normalization </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ESSENTIAL DIMENSIONS OF LSI</head><p>In this section we first describe LSI. We then discuss EDLSI and how it improves upon LSI. We also detail the distributed EDLSI approach we used for our TREC Legal 2008 submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Semantic Indexing</head><p>LSI is a well-known approach to information retrieval that is believed to reduce the problems associated with polysemy and synonymy <ref type="bibr" coords="2,179.40,384.16,10.60,8.97" target="#b2">[3]</ref>. At the heart of LSI is a matrix factorization, the singular value decomposition (SVD), which is used to express the t Ã— d term-bydocument matrix as a product of three matrices, a term component (T), a "diagonal" matrix of nonnegative singular values in non-increasing order (S), and a document component (D). This is shown pictorially in Figure <ref type="figure" coords="2,292.56,455.80,3.71,8.97" target="#fig_0">1</ref>, taken from <ref type="bibr" coords="2,121.68,467.80,10.60,8.97" target="#b1">[2]</ref>. The original term-by-document matrix A is then given by A = T SD T . In LSI, these matrices are truncated to k â‰ª min(t, d) dimensions by zeroing elements in the singular value matrix S. In practice, fast algorithms exist to compute only the required k dominant singular values and the associated vectors in T and D <ref type="bibr" coords="2,285.96,527.56,10.60,8.97" target="#b0">[1]</ref>. The primary problem with LSI is that the optimal number of dimensions k to use for truncation is dependent upon the collection <ref type="bibr" coords="2,170.52,563.44,10.60,8.97" target="#b3">[4]</ref>, <ref type="bibr" coords="2,187.08,563.44,15.34,8.97" target="#b9">[10]</ref>. Furthermore, as shown in <ref type="bibr" coords="2,82.56,575.32,10.60,8.97" target="#b7">[8]</ref>, LSI does not always outperform traditional vector space retrieval, especially on large collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EDLSI</head><p>To address this problem, Kontostathis developed an approach to using LSI in combination with traditional vector space retrieval called EDLSI. EDLSI uses a convex combination of the resultant vector from LSI and the resultant vector from traditional vector space retrieval. Early results showed that this fusion approach provided retrieval performance that was better than either LSI or vector space retrieval alone <ref type="bibr" coords="2,221.88,713.20,10.69,8.97" target="#b4">[5]</ref>. Moreover, this approach is not as sensitive to the k value, and in fact, it performs well when k is small (10 -20). In addition to providing better retrieval performance, keeping k small provides significant runtime benefits. Fewer SVD dimensions need to be computed, and memory requirements are reduced because fewer columns of the dense D and T matrices must be stored.</p><p>The computation of the resultant vector w using EDLSI is shown in Equation <ref type="formula" coords="2,433.92,338.08,3.77,8.97" target="#formula_0">2</ref>, where x is a weighting factor (0 â‰¤ x â‰¤ 1) and k is kept small. In this equation, A is the original term-by-document matrix, A k is the term-by-document matrix after truncation to k dimensions, and q is the query vector.</p><formula xml:id="formula_0" coords="2,358.80,409.94,181.28,11.78">w = (x)(q T A k ) + (1 -x)(q T A)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distributed EDLSI</head><p>Although the runtime requirements, specifically the memory, for EDLSI are reduced over LSI, they are still significant, especially for a corpus the size of IIT CDIP.</p><p>After indexing, we produced a term-by-document matrix that contained 486,654 unique terms and 6,827,940 documents. Our indexing system used log-entropy weighting and OCR error detection. Details can be found in <ref type="bibr" coords="2,509.40,553.24,10.60,8.97" target="#b8">[9]</ref>, <ref type="bibr" coords="2,525.84,553.24,10.69,8.97" target="#b5">[6]</ref>.</p><p>In order to process this large collection, we decided to sub-divide it into 81 pieces, each of which was about 300 MB in size. Each piece contained approximately 85,000 documents and approximately 100,000 unique terms (although the term-by-document matrix for each piece was approximately 85K by 487K). Each piece was treated as a separate collection (similar to <ref type="bibr" coords="2,484.68,639.16,15.02,8.97" target="#b13">[14]</ref>), and the 2008 queries were run against each piece using EDLSI. The result vectors were then combined so that the top 100,000 scoring documents overall could be submitted to TREC. The architecture model appears in Figure <ref type="figure" coords="2,527.16,687.04,3.77,8.97" target="#fig_1">2</ref>.</p><p>In Section 6 we discuss our 2008 TREC submissions using distributed EDLSI. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">POWER NORM</head><p>A variety of well-known and effective term weighting schemes can be used when forming the document and query vectors <ref type="bibr" coords="3,128.04,510.40,15.24,8.97" target="#b16">[17]</ref>. Term weighting can be split into three types: A local weight based on the frequency within the document, a global weight based on a term's frequency throughout the dataset, and a normalizing weight that negates the discrepancies of varying document lengths. The entries in the document vector are computed by multiplying the global weight for each term by the local weight for the document-term pair. Normalization may then be applied to the document and query vectors.</p><p>Our power normalization studies employed logentropy weighting. The purpose of the log-entropy weighting scheme is to reduce the relative importance of high-frequency terms while giving words that distinguish the documents in a collection a higher weight. Once the term weight is computed for each document, we then normalize the document vectors using Equation 3. In this equation dtw is the document term weight, qtw is the query term weight, dc is the number of terms in the document, qc is the number of terms in the query, p is the normalization parameter, and the sum is over all terms in the query.</p><formula xml:id="formula_1" coords="3,378.72,536.49,161.36,27.42">w d = tâˆˆQ dtw dc p qtw qc p<label>(3)</label></formula><p>Power normalization is designed to reduce, but not completely eliminate, the advantage that longer documents have within a collection. Without normalization, long documents would dominate the top ranks because they contain so many terms. In contrast, cosine normalization ensures in all documents have exactly the same weight for retrieval purposes. Experiments from the TREC Legal competition in 2007 showed that power normalization using p = 1/3 and p = 1/4 results in retrieval performance improvements over cosine normalization for the 2006 and 2007 query sets <ref type="bibr" coords="3,482.64,713.20,10.60,8.97" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BM25</head><p>The BM25 algorithm was introduced at TREC 3 <ref type="bibr" coords="4,280.92,93.64,15.34,8.97" target="#b12">[13]</ref>. It combines an inverse collection frequency weight with collection specific scaling for documents and queries. The weight function for a given document (d) and query (Q) appears in Equation <ref type="formula" coords="4,175.92,141.40,3.77,8.97" target="#formula_2">4</ref>. In this equation, idf t refers to the inverse document frequency weight for a given term (Equation <ref type="formula" coords="4,136.92,165.28,3.59,8.97" target="#formula_4">5</ref>), K is given by Equation <ref type="formula" coords="4,253.20,165.28,3.71,8.97" target="#formula_5">6</ref>, tf is the number of times term t appears in document d, qtf is the number of times term t appears in the query Q, N is the number of documents in the collection, n is the number of documents containing t, dl is the document length (we measured this in words), adl is the average document length for the corpus (also measured in words), and b, k1, and k3 are tuning parameters.</p><formula xml:id="formula_2" coords="4,108.60,268.05,187.57,27.43">w d = tâˆˆQ idf t (k1 + 1)tf (K + tf ) (k3 + 1)qtf (k3 + qtf ) (<label>4</label></formula><formula xml:id="formula_3" coords="4,296.17,275.44,3.91,8.97">)</formula><formula xml:id="formula_4" coords="4,146.88,303.09,153.20,23.52">idf t = N -n + .5 n + .5<label>(5)</label></formula><formula xml:id="formula_5" coords="4,129.72,336.69,170.36,23.52">K = k1 (1 -b) + b dl adl<label>(6)</label></formula><p>The full BM25 weighting scheme is slightly more complicated than this and requires two additional tuning parameters, but BM25 reduces to the above formula when relevance feedback is not used and when those two parameters take their standard values. Interested readers will find details in <ref type="bibr" coords="4,150.72,427.36,15.34,8.97" target="#b12">[13]</ref>.</p><p>We recently compared BM25 to power normalization and determined that BM25 outperforms power normalization at top ranks, but power normalization outperforms BM25 in terms of recall after rank 300 <ref type="bibr" coords="4,266.52,475.36,10.69,8.97" target="#b5">[6]</ref>. Our submissions using BM25 and power normalization for TREC Legal 2008 are described in Sections 7 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DEFINING K AND K h</head><p>We ran a variety of experiments to determine how to identify the optimal K and K h for each query. Our approach centered on finding a scoring threshold to use for determining optimal K and K h values. After several false starts using the judged documents from 2006 and 2007, we determined that the collection size for judged only was insufficient for training. Thus, we used the entire collection, in conjunction with the 2006 and 2007 queries and judgment data, for training purposes.</p><p>Initially we ran the 2006 and 2007 queries against the IIT CDIP corpus and developed a pseudo submission file containing the top 100,000 scoring documents and their scores for each query. We then used these files as input and measured F1 at a variety of cutoff threshold levels. If a document score was greater than the threshold T hres K the document was considered 'relevant'; otherwise it was considered 'not relevant'. We then measured precision, recall, and F1 for each query and computed the average across all queries. We used the TREC2006 and TREC2007 relevance judgment files for computing precision, recall, and F1. All unjudged documents were considered not relevant. Again, using only judged documents did not provide sufficient training data, in our opinion. This approach provided data that were more consistent across runs.</p><p>Two methods were used for determining the optimal T hres K . The first method determined the optimal F1 for each query individually by incrementing K from 1 to 100,000 and measuring F1. The document score at the optimal K was determined to be T hres K for a given query. The average T hres K for each query was used to identify the final T hres K for a query set.</p><p>The second approach involved iterating through thresholds to identify which threshold gave us the optimal F1 for the entire query set. For power normalization, thresholds from 3 to .01 were tried (in increments of .02); for BM25, thresholds of 400 to 25 were tried (in increments of 5). The maximum F1 determined the optimal T hres K value. We measured F1 to 3 significant digits. When ties for the best F1 were found, maximum recall was used as a tie breaker to determine the optimal T hres K .</p><p>Both techniques for determining T hres K were used on the queries for 2006 and 2007 separately and then for 2006 and 2007 combined, for both power normalization and BM25. The optimal threshold values appear in Table <ref type="table" coords="4,338.40,447.64,3.71,8.97" target="#tab_0">1</ref>. Because two different approaches were used, they always produced different optimal values (although sometimes only slightly different). The lower of these was used as the K threshold, and the higher was used as the K h threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">AD HOC EXPERIMENTS WITH DIS-TRIBUTED EDLSI</head><p>In this section we discuss our distributed EDLSI experiments for the TREC 2008 competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Design</head><p>Our main focus during these experiments was the development and implementation of the system. Initially we hoped to treat the entire collection as a single LSI space. To accomplish this, we planned to implement EDLSI with the folding-up algorithm <ref type="bibr" coords="4,433.32,653.44,15.24,8.97" target="#b10">[11]</ref>. Folding-up combines folding-in <ref type="bibr" coords="4,356.88,665.32,10.60,8.97" target="#b2">[3]</ref>, <ref type="bibr" coords="4,375.96,665.32,11.60,8.97" target="#b1">[2]</ref> with SVD updating <ref type="bibr" coords="4,480.12,665.32,15.34,8.97" target="#b11">[12]</ref>, <ref type="bibr" coords="4,504.24,665.32,15.34,8.97" target="#b17">[18]</ref>, <ref type="bibr" coords="4,528.36,665.32,11.60,8.97" target="#b1">[2]</ref> to maintain the integrity of the LSI space (with simple folding-in the columns of T and D generally become less orthogonal with every added term and document, respectively). We initially sub-divided the collection into We then attempted to integrate the remaining 80 pieces via the folding-up process. Unfortunately, the folding-up process is memory intensive, and we estimated that the process would run approximately 35 days; we did not have sufficient time for it to complete. Our next approach combined distributed EDLSI with folding-up. Our design included distributing EDLSI across 8 processors, each processing about 10 chunks of data. The first chunk would perform the SVD, the remaining 9 would be folded-up. We estimated that this process would take about 10 days, and once again we did not have sufficient time.</p><p>Finally we decided to run a fully distributed EDLSI system with no folding-up. In this system, we performed the SVD on each of the 81 pieces separately, used LSI to run the queries against the SVD space, and combined the LSI results with the traditional vector space retrieval results (EDLSI). Interestingly, we were able to run this algorithm on an IBM x3755 8-way with 4 AMD Opteron 8222 SE 3.0GHz dual-core processors and 128GB RAM running 64-bit RedHat Linux in approximately 4 hours.</p><p>We used only the request text portion of each query for all runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Choosing the k Parameter</head><p>For any LSI system, the question of which k to use must be addressed. To consider this question, we tested our distributed EDLSI with folding-up system on a subset of the IIT CDIP corpus using the TREC 2007 queries. In this training run, we extracted all documents that were judged for any of the 2007 queries from the full IIT CDIP collection. This collection contained approximately 22,000 documents and 85,000 terms. After preliminary testing to ensure that the results for TREC Legal 2007 were similar to other collections, we tested values for k from 15 to 49 (in increments of 2) and values for x from .16 to .22 (in increments of .02). The optimal values were k = 41 and x = .2. These values are consistent with the values identified in the early EDLSI experiments described in <ref type="bibr" coords="5,176.88,713.20,10.69,8.97" target="#b4">[5]</ref>. The distributed EDLSI system (without folding-up) with k = 41 and x = .2 was run against the 2008 queries and was submitted for judging as UCEDLSIa.</p><p>One of the most interesting things about EDLSI is that it works with very few dimenions of the SVD space; thus we decided to submit a second run with an even lower k value. We expected a lower k value to provide improved results because keeping k constant while increasing the number of divisions (over the space we used for the training run) resulted in a greater number of values being used in the T k , S k , and D k matrices. For instance, at k = 41, there are a total of 4.4 Ã— 10 8 numbers being stored in T k , S k , and D k for 8 partitions, but there are 18.8 Ã— 10 8 numbers being stored in T k , S k and D k for 80 partitions.</p><p>However, with 80 partitions and k = 10, there are 4.6 Ã— 10 8 numbers in T k , S k and D k . We speculate that this may indicate that roughly the same amount of noise has been eliminated from the system as with 8 partitions and k = 41. Therefore we decided to use k = 10 and x = .2 for our second submission run for 2008 (UCEDLSIb). All experiments to date with EDLSI seem to indicate that .2 is a suitable weighting factor, and <ref type="bibr" coords="5,312.00,505.24,11.60,8.97" target="#b4">[5]</ref> shows that performance does not vary much with x, even on fairly large collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baseline Run</head><p>Because comparison to a complete LSI run was not feasible for a collection this size, we chose instead to compare our distributed EDLSI system to the traditional vector space system that forms a part of EDLSI. Our hypothesis is that EDLSI will elicit enough term relationship information from LSI to boost the performance of the vector space model. Our vector space baseline was submitted as UrsinusVa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Defining K and K h for EDLSI</head><p>For UCEDLSIa, the K and K h thresholds were set using the TREC 2007 training run via the process described in Section 5. This was not ideal, as explained above, Initially we planned to use these values for UCEDL-SIb run also, but a glance at the output indicated that the thresholds were too low to yield meaningful results. Almost all queries for UCEDLSIb had K and K h values close to 100,000. As a result, we raised these thresholds, to T hres K = .300 and T hres K h = .350 for UCEDLSIb because they seemed to produce more reasonable K and K h values. We should have trusted the original data however (the top scoring runs at TREC Legal all had average K values at or near 100,000). A follow-up run which set K = 100, 000 for all queries, resulted in an Estimate F 1 at K that was 11 times larger than the submitted run (.1168 vs .0094); this would have made UCEDLSIb our top scoring run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results</head><p>Figure <ref type="figure" coords="6,100.92,641.44,4.98,8.97" target="#fig_2">3</ref> shows a comparison of the two EDLSI submissions and the vector baseline using a variety of metrics. EDLSI clearly outperforms vector space retrieval across the board. Furthermore, the performance of EDLSIa, the optimized EDLSI run, is not dramatically better than the performance of EDLSIb, the baseline EDLSI run (k = 10), although it is slightly better on most metrics. The exception is estimated precision at F1, where EDLSIb outperforms EDLSIa. This is a direct result of the average K value submitted for the two runs. Average K for EDLSIa was 15,225; for EDLSIb it was only 1535. Interestingly, the average K for the vector baseline was even lower (802), but it still did not come close to matching the performance of either EDLSI run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">AD HOC EXPERIMENTS USING BM25 AND POWER NORMALIZATION</head><p>In this section we describe our submitted runs for BM25 and power normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Design</head><p>Our experiments in 2007 focused on comparing power normalization to cosine normalization. We discovered that power normalization significantly outperformed cosine normalization, and we also identified the optimal power parameter for the 2006 and 2007 queries <ref type="bibr" coords="6,525.84,605.56,10.69,8.97" target="#b8">[9]</ref>. Subsequently, we ran some experiments that compared power normalization to BM25 <ref type="bibr" coords="6,434.76,629.56,11.60,8.97" target="#b5">[6]</ref> and learned that BM25 has better retrieval metrics (recall and precision) at top ranks, but power normalization has better recall as rank increases. Furthermore, power normalization overtakes BM25 in terms of recall at about rank 300. Thus, for 2008 we wanted to compare BM25 to power normalization. UrsinusBM25a and UrsinusPwrA are our baseline runs for BM25 and power normalization.</p><p>As noted in Section 4, BM25 requires a large number of tuning parameters. For our UrsinusBM25a baseline run we set b = .6, k1 = 2.25 and k3 = 3. These are the average of the optimal values for the TREC Legal 2006 and TREC Legal 2007 query sets. We used number of terms to represent document size and the average was 435; the total number of indexed documents was 6,827,940.</p><p>In section 3 we noted that power normalization also requires a parameter, the normalization factor, p. The TREC 2006 normalization factor was used for our Ursi-nusPwrA submission. The reason for this is described in Section 7.2.</p><p>In addition to comparing BM25 and power normalization, we wanted to see if automatic query expansion could be used to improve retrieval performance. Thus we ran training experiments using the TREC Legal 2007 queries. In these experiments we extracted the most highly weighted 5 and 10 terms from the top-ranked document, as well as the top three documents, and added these terms to the original query before rerunning the search. For both BM25 and power normalization, we required the document to have minimal weight before terms to add. Multiple document thresholds were tested, and between 0 and 30 terms were added to each query.</p><p>Results from the automatic relevance feedback tests were submitted as runs UrsinusBM25b (5 terms from the top document, with a minimum score of 200), UrsinusPwrB (5 terms from the top document with a minimum score of 1.0), and UrsinusPwrC (5 terms from the top 3 documents with a minimum score of 1.4).</p><p>We used only the request text portion of each query for all runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Defining K and K h for Power Normalization and BM25</head><p>Optimal K and K h values were identified as described in Section 5. For BM25, this resulted in a T hres K = 215 and T hres Kh = 239. These values were identified by combining the TREC Legal 2006 and 2007 values into a single run of the optimization loop. This threshold was used for both BM25 runs.</p><p>Training using both the 2006 and 2007 query sets for power normalization to find the optimal the T hres k values for the 2008 queries led to K values that were very small (we never let K = 0, but there were many queries with K = 1). Therefore, we decided to use the T hres K and T hres Kh values that were identified by training on the 2006 queries only, instead of using the combined values, because this produced more reasonable results for K. This leads us to conclude that the 2008 queries are more like the 2006 queries than the 2007 queries (or some combination of the two), so we also used the optimal p for the 2006 query set (p = .36) rather than an average of the optimal p for 2006 and 2007 (p = .32). The corresponding T hres K = .655 and T hres Kh = .800 values were used for the power normalization runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Results</head><p>Figure <ref type="figure" coords="7,342.96,163.72,4.98,8.97">4</ref> shows a comparison of the BM25 run with relevance feedback and the BM25 baseline run. The relevance feedback run outperformed the baseline run on every statistic except estimated precision at K. This was not surprising as the average K for the relevance feedback run was almost three times the average K for the baseline run <ref type="bibr" coords="7,402.72,235.48,71.34,8.97">(10,168 vs. 3157</ref>) and precision decreases as additional documents are retrieved.</p><p>Figure <ref type="figure" coords="7,352.32,259.12,4.98,8.97">5</ref> shows the results for the three power normalization runs. The results for power norm are more mixed. Although all three runs resulted in similar retrieval performance across the statistics shown, there is no run that consistently outperforms the other two. The baseline run outperforms the relevance feedback runs for estimated recall at B, estimated precision at F1, recall at B, and precision at B. PowerC adds in the top 5 terms from the top 3 documents, if the term weight is greater than 1.4 and performs nearly identically to the baseline run (PowerA) across the board. PowerC is usually less than PowerA, only slightly outperforming PowerA when the estimated precision at B is used. This suggests that PowerC is adding terms that are slightly unhelpful. PowerB does significantly better than PowerA on the competition metric (estimated F1 at K). It outperforms both PowerA and PowerC for estimated precision at B, estimated recall at K, estimated F1 at K, mean average precision (MAP), and MAP using judged documents only. The average K for PowerB is 3292, for PowerA it is 905, and for PowerC it is 850. Interestingly, although PowerB retrieves approximately three times as many documents at K on average, the precision at K is only slightly less than precision at K for PowerA and PowerC. Recall is again naturally much better due to the larger K value.</p><p>Finally, Figure <ref type="figure" coords="7,388.32,569.68,4.98,8.97">6</ref> compares the performance of the optimal BM25 to the two best Power normalization schemes. BM25 is the clear winner. It soundly outperforms power normalization on the track metric of estimated F1 at K. There is again a three-to-one advantage in terms of documents retrieved (BM25b retrieves 10,168 on average, PowerB retrieves 3292), but BM25b manages to retrieve this large number of documents without sacrificing precision. Indeed, BM25b outperforms the power normalization schemes on all three F1 metrics. In fact, the only algorithm that happens to beat BM25b on any of the at-K based metrics is EDLSIa, which outperforms all other metrics in estimated recall at K. EDLSIa </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELEVANCE FEEDBACK RUNS USING BM25 AND POWER NORMALIZATION</head><p>In this section we describe our submissions and results for the relevance feedback task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Experimental Design</head><p>Our relevance feedback submissions leveraged our work on automatic relevance feedback. We decided to look at the top weighted terms from all documents that were judged relevant. Terms were added to the query only if their global term weight met a certain weight threshold. Some minimal training was done to determine the optimal number of terms and optimal term weight before we ran out of time. We used only the request text portion of each query for all runs.</p><p>Our relevance feedback runs are summarized below:</p><p>â€¢ UCRFBM25BL: BM25 baseline using the same parameters as UrsinusBM25a â€¢ UCRFPwrBL: Power Norm baseline using the same parameters as UrsinusPwrA The T hres K and T hres K h values from the ad hoc experiments were also used for the relevance feedback experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Results</head><p>The first interesting feature that we noted is the use of the T hres K and T hres K h values from the ad hoc experiments resulted in a far larger value of K for the relevance feedback runs. The basic power normalization run in the ad hoc submissions resulted in an average K value of 905; for the relevance feedback runs, the average K value was 1547. More strikingly, after the top 5 terms were added, the average K jumped to 11,880. When ten terms were added, the value increased to 13,233. For the BM25 submission the average K rose even faster. The baseline relevance feedback run had a K value of 564, which was significantly lower than the ad hoc average K (3157), but when five terms were added, this jumped Figure <ref type="figure" coords="10,112.56,617.56,4.98,8.97" target="#fig_5">7</ref> shows a comparison of the results for the top scoring BM25 and Power normalization relevance feedback runs. BM25 is clearly outperforming power normalization. It will be interesting to optimize the T hres K and T hres K h values in post-hoc testing, to see if BM25 is getting its advantage by merely retrieving more documents. The strong showing on last year's metric (Estimate Recall at B) leads us to believe that BM25 will likely to continue to outperform power normalization after additional optimizations are run.</p><p>Figures <ref type="figure" coords="10,354.48,605.56,4.98,8.97">8</ref> and<ref type="figure" coords="10,379.20,605.56,4.98,8.97" target="#fig_6">9</ref> show the comparison for each weighting/normalization scheme as additional terms are added. We now detect a distinct difference between the power normalization runs and the relevance feedback runs. The power norm consistently does better as more terms are added. Adding five terms outperforms the baseline; adding ten terms outperforms adding only five terms. We plan post-hoc testing to take this strategy further to determine where the performance peaks and begins to degrade. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS</head><p>Our experiments show that the Distributed EDLSI approach is promising, but it is not yet fully mature. It soundly outperformed the vector baseline approach, but it could not improve upon a basic BM25 system. Generally BM25 was the most consistent performing algorithm across all of the techniques we tested. BM25b outperformed power normalization on both the ad hoc and the relevance feedback tasks, and it outperformed Distributed EDLSI on the ad hoc task. Post analysis will be done to optimize the parameter settings for all three approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,78.17,145.33,9.22"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Truncation of SVD for LSI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.00,78.17,112.21,9.22"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2. Distributed EDLSI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,72.00,78.17,188.94,9.22"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Distributed EDLSI Result Summary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,72.00,106.37,261.54,9.22"><head>Fig. 4 .Fig. 6 .</head><label>46</label><figDesc>Fig. 4. BM25 Baseline vs. Automated Relevance Feedback</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,321.96,377.08,218.17,8.97;9,332.04,389.08,91.50,8.97;9,321.96,401.08,218.17,8.97;9,332.04,412.96,91.50,8.97;9,321.96,424.96,218.24,8.97;9,332.04,436.96,122.10,8.97;9,321.96,448.84,218.24,8.97;9,332.04,460.84,98.70,8.97;9,312.00,487.53,106.98,9.96;9,419.04,487.85,120.98,9.95"><head>â€¢</head><label></label><figDesc>UCBM25T5Th5: BM25 with 5 terms over weight 5 added to each query â€¢ UCBM25T10Th5: BM25 with 10 terms over weight 5 added to each query â€¢ UCPwrT5Th5: Power Norm with 5 terms over weight 5 added to each query â€¢ UCPwrT10Th5: Power Norm with 10 terms over weight 5 for each query 8.2 Defining K and K h for Relevance Feedback</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,72.00,78.17,277.70,9.22"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Power vs. BM25 Relevance Feedback Run Comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,72.00,78.17,345.42,9.22"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. BM25 Relevance Feedback Run Comparison -Adding additional terms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,72.05,437.58,166.99"><head>TABLE 1 Determining</head><label>1</label><figDesc>Optimal K and K h</figDesc><table coords="5,72.00,101.32,437.58,137.73"><row><cell>Weighting</cell><cell>Query</cell><cell cols="2">Method 1</cell><cell></cell><cell cols="2">Method 2</cell></row><row><cell>Scheme</cell><cell>Set</cell><cell>T hres K</cell><cell cols="3">F1 recall T hres K</cell><cell>F1 recall</cell></row><row><cell>BM25</cell><cell>TREC 2006</cell><cell cols="2">162.4 .104</cell><cell>.177</cell><cell cols="2">190 .044</cell><cell>.123</cell></row><row><cell>BM25</cell><cell>TREC 2007</cell><cell cols="2">307.9 .130</cell><cell>.164</cell><cell cols="2">280 .054</cell><cell>.234</cell></row><row><cell>BM25</cell><cell>Combined 2006/2007</cell><cell cols="2">238.7 .117</cell><cell>.170</cell><cell cols="2">215 .040</cell><cell>.236</cell></row><row><cell>Power normalization</cell><cell>TREC 2006</cell><cell cols="2">.655 .050</cell><cell>.187</cell><cell cols="2">.800 .026</cell><cell>.075</cell></row><row><cell>Power normalization</cell><cell>TREC 2007</cell><cell cols="2">1.997 .135</cell><cell>.171</cell><cell cols="2">1.720 .061</cell><cell>.259</cell></row><row><cell cols="2">Power normalization Combined 2006/2007</cell><cell cols="2">1.358 .095</cell><cell>.179</cell><cell cols="2">1.700 .032</cell><cell>.139</cell></row><row><cell cols="3">81 pieces and used traditional SVD on the first piece.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,90.24,696.68,209.53,7.17;11,90.24,705.56,209.47,7.17;11,90.24,714.56,82.15,7.17" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,156.84,696.68,142.93,7.17;11,90.24,705.56,14.46,7.17">Large-scale sparse singular value computations</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,111.24,705.56,185.25,7.17">The International Journal of Supercomputer Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="49" />
			<date type="published" when="1992">Spring 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,366.44,209.59,7.17;11,330.24,375.44,209.80,7.17;11,330.24,384.44,97.51,7.17" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,330.24,375.44,183.55,7.17">Using linear algebra for intelligent information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gavin</forename><forename type="middle">W</forename><surname>O'brien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,521.88,375.44,18.16,7.17;11,330.24,384.44,21.05,7.17">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="595" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,395.00,209.47,7.17;11,330.24,404.00,209.79,7.17;11,330.24,413.00,209.70,7.17;11,330.24,421.88,139.99,7.17" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,499.56,404.00,40.47,7.17;11,330.24,413.00,78.38,7.17">Indexing by latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,419.40,413.00,120.53,7.17;11,330.24,421.88,63.66,7.17">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,432.56,209.79,7.17;11,330.24,441.44,209.78,7.17;11,330.24,450.44,209.77,7.17;11,330.24,459.44,129.91,7.17" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<title level="m" coord="11,401.76,432.56,118.54,7.17;11,394.20,441.44,145.82,7.17;11,330.24,450.44,209.77,7.17;11,330.24,459.44,51.96,7.17">The First Text REtrieval Conference (TREC-1), National Institute of Standards and Technology Special Publication 500-207</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="137" to="152" />
		</imprint>
	</monogr>
	<note>LSI meets TREC: A status report</note>
</biblStruct>

<biblStruct coords="11,330.24,470.00,209.58,7.17;11,330.24,479.00,209.58,7.17;11,330.24,488.00,209.71,7.17;11,330.24,496.88,149.23,7.17" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,400.56,470.00,139.26,7.17;11,330.24,479.00,57.77,7.17">Essential Dimensions of Latent Semantic Indexing (EDLSI)</title>
		<author>
			<persName coords=""><forename type="first">April</forename><surname>Kontostathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,407.16,479.00,132.66,7.17;11,330.24,488.00,183.66,7.17">Proceedings of the 40th Annual Hawaii International Conference on System Sciences (CD-ROM)</title>
		<meeting>the 40th Annual Hawaii International Conference on System Sciences (CD-ROM)<address><addrLine>Kona, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Society Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,507.56,209.30,7.17;11,330.24,516.44,209.53,7.17;11,330.24,525.44,209.47,7.17;11,330.24,534.44,141.91,7.17" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,447.36,507.56,92.19,7.17;11,330.24,516.44,88.01,7.17">The Effect of Normalization when Recall Really Matters</title>
		<author>
			<persName coords=""><forename type="first">April</forename><surname>Kontostathis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Kulp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,433.44,516.44,106.33,7.17;11,330.24,525.44,206.05,7.17">Proceedings of the 2008 International Conference on Information and Knowledge Engineering</title>
		<meeting>the 2008 International Conference on Information and Knowledge Engineering<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CSREA Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,545.00,209.54,7.17;11,330.24,554.00,209.47,7.17;11,330.24,563.00,201.67,7.17" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,495.24,545.00,44.54,7.17;11,330.24,554.00,205.98,7.17">A framework for understanding Latent Semantic Indexing (LSI) performance</title>
		<author>
			<persName coords=""><forename type="first">April</forename><surname>Kontostathis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">M</forename><surname>Pottenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,330.24,563.00,132.60,7.17">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="73" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,573.56,209.59,7.17;11,330.24,582.56,209.78,7.17;11,330.24,591.44,209.59,7.17;11,330.24,600.44,209.62,7.17;11,330.24,609.44,100.03,7.17" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,330.24,582.56,193.84,7.17">Identification of critical values in latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">April</forename><surname>Kontostathis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">M</forename><surname>Pottenger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,330.24,600.44,184.36,7.17">Foundations of Data Mining and Knowledge Discovery</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ohsuga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Liau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Tsumoto</surname></persName>
		</editor>
		<imprint>
			<publisher>Spring-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="333" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,620.00,209.58,7.17;11,330.24,629.00,209.65,7.17;11,330.24,637.88,209.71,7.17;11,330.24,646.88,186.55,7.17" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,451.20,620.00,88.62,7.17;11,330.24,629.00,163.04,7.17">On Retrieving Legal Files: Shortening Documents and Weeding Out Garbage</title>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Kulp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">April</forename><surname>Kontostathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,511.20,629.00,28.69,7.17;11,330.24,637.88,205.83,7.17">Proceedings of the Sixteenth Text REtrieval Conference (TREC2007)</title>
		<meeting>the Sixteenth Text REtrieval Conference (TREC2007)<address><addrLine>Bethesda, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="500" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,657.44,209.43,7.17;11,330.24,666.44,209.47,7.17;11,330.24,675.44,80.59,7.17" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,462.84,657.44,76.83,7.17;11,330.24,666.44,127.84,7.17">Large-scale information retrieval with latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">Todd</forename><forename type="middle">A</forename><surname>Letsche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,468.72,666.44,67.76,7.17">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="105" to="137" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,686.00,209.53,7.17;11,330.24,695.00,209.70,7.17;11,330.24,704.00,185.11,7.17" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,457.32,686.00,82.45,7.17;11,330.24,695.00,129.04,7.17">A New Adaptive Foldingup Algorithm for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jane</forename><forename type="middle">E</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Spiteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,477.96,695.00,61.98,7.17;11,330.24,704.00,91.20,7.17">Proceedings of the 2008 Text Mining Workshop</title>
		<meeting>the 2008 Text Mining Workshop<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,330.24,714.56,209.55,7.17;12,90.24,78.56,209.47,7.17;12,90.24,87.44,53.95,7.17" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,396.96,714.56,142.83,7.17;12,90.24,78.56,107.87,7.17">Information management tools for updating an svd-encoded indexing scheme</title>
		<author>
			<persName coords=""><forename type="first">Gavin</forename><forename type="middle">W</forename><surname>O"brien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Knoxville, TN, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="12,90.24,96.44,209.53,7.17;12,90.24,105.44,209.79,7.17;12,90.24,114.44,153.55,7.17" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,235.32,105.44,49.23,7.17">Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aarron</forename><surname>Gull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marianna</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,90.24,114.44,83.49,7.17">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.24,123.32,209.78,7.17;12,90.24,132.32,209.71,7.17;12,90.24,141.32,209.80,7.17;12,90.24,150.20,209.42,7.17;12,90.24,159.20,199.87,7.17" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,290.28,123.32,9.75,7.17;12,90.24,132.32,206.31,7.17">On scaling latent semantic indexing for large peer-to-peer systems</title>
		<author>
			<persName coords=""><forename type="first">Chunqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandhya</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhichen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,99.12,141.32,200.92,7.17;12,90.24,150.20,209.42,7.17;12,90.24,159.20,25.88,7.17">SIGIR &apos;04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="112" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.24,168.20,209.77,7.17;12,90.24,177.20,209.59,7.17;12,90.24,186.08,135.31,7.17" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,221.40,168.20,78.61,7.17;12,90.24,177.20,175.36,7.17">Updating the partial singular value decomposition in latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">Jane</forename><forename type="middle">E</forename><surname>Tougas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Spiteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,272.76,177.20,27.07,7.17;12,90.24,186.08,59.30,7.17">Comput. Statist. Data Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.24,195.08,209.70,7.17;12,90.24,204.08,209.47,7.17;12,90.24,212.96,139.03,7.17" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,218.16,195.08,81.78,7.17;12,90.24,204.08,206.20,7.17">Two uses for updating the partial singular value decomposition in latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">Jane</forename><forename type="middle">E</forename><surname>Tougas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Spiteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,90.24,212.96,62.74,7.17">Appl. Numer. Math</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="510" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.24,221.96,209.77,7.17;12,90.24,230.96,160.03,7.17" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,168.48,221.96,70.31,7.17">Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.24,239.96,209.79,7.17;12,90.24,248.84,209.47,7.17;12,90.24,257.84,17.83,7.17" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,216.84,239.96,83.19,7.17;12,90.24,248.84,77.80,7.17">On updating problems in latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horst</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,175.20,248.84,68.91,7.17">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="782" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
