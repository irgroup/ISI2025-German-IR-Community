<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.76,72.44,286.09,16.59">DCU at the TREC 2008 Blog Track</title>
				<funder ref="#_gxYHz4K">
					<orgName type="full">Enterprise Ireland Commercialisation Fund</orgName>
				</funder>
				<funder ref="#_FEuae5K">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.52,118.02,102.24,11.06"><forename type="first">Adam</forename><surname>Bermingham</surname></persName>
							<email>abermingham@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">CLARITY: Centre for Sensor Web Technologies and Centre for Digital Video Processing Dublin City University Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.28,118.02,74.93,11.06"><forename type="first">Alan</forename><surname>Smeaton</surname></persName>
							<email>asmeaton@computing.dcu.ie</email>
							<affiliation key="aff1">
								<orgName type="department">CLARITY: Centre for Sensor Web Technologies and Centre for Digital Video Processing Dublin City University Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,152.40,188.22,78.98,11.06"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
							<email>jfoster@computing.dcu.ie</email>
							<affiliation key="aff2">
								<orgName type="department">National Centre for Language Technology Dublin City University Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.96,188.22,77.57,11.06"><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
							<email>dhogan@computing.dcu.ie</email>
							<affiliation key="aff3">
								<orgName type="department">National Centre for Language Technology Dublin City University Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.76,72.44,286.09,16.59">DCU at the TREC 2008 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A6790789DB9BFB223501D2C3C15BC5C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our system, experiments and results from our participation in the Blog Track at TREC 2008. Dublin City University participated in the adhoc retrieval, opinion finding and polarised opinion finding tasks. For opinion finding, we used a fusion of approaches based on lexicon features, surface features and syntactic features. Our experiments evaluated the relative usefulness of each of the feature sets and achieved a significant improvement on the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This was Dublin City University's first year participating in the Blog Track at TREC and this marks our return to TREC participation after a layoff of several years 1 . We aimed to evaluate the effectiveness of combining three different approaches to opinion finding. Machine learning has been used successfully in the field of Sentiment Analysis <ref type="bibr" coords="1,247.63,440.27,9.11,8.97" target="#b2">[3]</ref>, <ref type="bibr" coords="1,264.18,440.27,9.63,8.97" target="#b8">[9]</ref> and each of our approaches uses machine learning techniques to generate sentiment scores for relevant blog entries.</p><p>Our system consists of:</p><p>• A Lexicon module which evaluates the sentiment orientation of a blog entry by aggregating the sentiment scores for its consituent words in a sentiment lexicon, SentiWordNet.</p><p>• A Surface module which scores documents based on textual features which are obtained without any parsing or syntactic understanding of the sentence structure.</p><p>• A Syntactic module which scores documents based on features derived from part-of-speech tagging and parsing the text.</p><p>We fuse the scores from each of the modules using weighted 1 We have been participants in TREC from the early 90s to early 00s combsum late fusion <ref type="bibr" coords="1,407.14,258.84,13.37,8.97" target="#b10">[11]</ref>. We have also submitted runs which do not weight the various sources, as a comparison.</p><p>Three baselines were used in our experiments: a topic only retrieval run, a topic and description run and "baseline4" as distributed by TREC. We chose baseline4 as time did not permit us to perform experiments on more than one distributed baseline and it gave a higher MAP on the previous two years' queries than the other four distributed baselines.</p><p>The rest of this paper is organized as follows: The system is described in Section 2. The runs submitted and results are discussed in Section 3. Conclusions are then presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM DESIGN</head><p>As is common in previous years' opinion retrieval submissions <ref type="bibr" coords="1,339.11,442.79,9.11,8.97" target="#b7">[8]</ref>, <ref type="bibr" coords="1,354.22,442.79,9.11,8.97">[5]</ref>, we favoured a design based on a two-stage system. The first stage concerns retrieving and ranking topic relevant blog entries. In the second stage the results are re-ranked according to opinion scores (or polarised opinion scores for the polarity task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevant Blog Retrieval</head><p>The open source retrieval platform Terrier <ref type="bibr" coords="1,494.25,522.23,9.63,8.97" target="#b6">[7]</ref> was used to index the collection and retrieve query-relevant blog entries. The retrieval model used was the Okapi BM25 model. We submitted two relevance baselines: DCUCDVPtbl, our title only run, and DCUCDVPtdbl, our title and description run.</p><p>During system development we experimented with various different query expansion models available on the Terrier platform before settling on the Divergence From Randomness Bo1 (Bose-Einstein 1) model. We tuned the query expansion parameters based on retrieval MAP scores for the topics from the Blog Track 2006 and 2007. For the titleonly run we found that setting the number of documents to look for query expansion to 2, and the maximum number of query terms to 7, gave optimum results. For the topic and description baseline, we did not find that adding additional terms to the query improved performance. We did however observe that improved performance could be obtained for this baseline by using the query expansion model to re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>For each topic, a ranked list of 1000 documents is returned to the feature extraction stage. Firstly, the HTML documents referred to by the results lists are parsed using HTMLParser [1] to extract the natural language from the blog entry. The documents are separated into text sections corresponding to blocks of text delimited by HTML elements which are deemed to break the flow of the text by the parser. Certain sections are then judged noisy if they have a high link-totext ratio (e.g. advertisements, link lists) or if they have a high non-alphabetic character to alphabetic character ratio (e.g. code citation, date). These sections are removed before feature extraction.</p><p>From the text we then extract three types of features which are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Surface Features</head><p>We chose to use a set of surface features as we wanted to evaluate features which may be derived easily and which offer information not necessarily contained in a standard bag-ofwords model. The vector of surface features is comprised of four different types of feature:</p><p>• A number of document measurements. Looking at logical lengths in the document such as section length or word length, we derived a number of metrics such as "average section length in characters" or "document length in words".</p><p>• The frequency of a small manually-created list of words often associated with sentiment including pronouns, obscenities and some simple emotive verbs eg. "think", "feel", "like", "hate".</p><p>• Non-word characters and character sequences such as punctuation and emoticons.</p><p>• Regex patterns to detect unusual word and punctuation structures. These include words containing 3 or more of the same character in a row ("arrrrgh"), excessive punction ("?!?!"), sequences of full stops ("....") and sequences of astrisks which might denote a censored word. For pattern or word counts, the vector items were max-min normalised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Syntactic Features</head><p>We carried out experiments with a set of syntactic features in order to investigate whether knowledge of the relationships between the words in a sentence is useful in the opinion detection task. The text from the blog documents were parsed using the Charniak and Johnson re-ranking parser <ref type="bibr" coords="2,517.42,647.87,9.11,8.97" target="#b1">[2]</ref>, a constituency parser which achieves state-of-the-art performance on the Wall Street Journal section of the Penn Treebank <ref type="bibr" coords="2,543.58,668.75,9.11,8.97" target="#b5">[6]</ref>. This parser also performs part-of-speech tagging on the text.</p><p>Parsing the blog entries for a retrieval run requires a significant amount of processing power. We used a high-end computing cluster maintained by the Irish Centre for High End Computing. This cluster consists of 31 CPUs (AMD Opteron 250, 2.4 GHz single core). We only had time to parse our title only baseline run, so only runs based on that baseline make use of syntactic features.The marginal computing cost of parsing additional baselines is dependent on the amount of overlap between baselines.</p><p>Two kinds of features were extracted from the parsed data: part-of-speech n-grams and features related to the types of phrases occurring in the data:</p><p>• The 50 most discriminative part-of-speech unigrams, bigrams and trigrams were chosen, resulting in 150 part-of-speech n-gram features. The discriminativeness of an n-gram was determined by comparing the normalised count of the particular n-gram in the neutral dataset to the normalised count of the same ngram in the opinionated dataset.</p><p>• The second set of features consisted of normalised document counts for each of the Penn Treebank phrasal types (S, NP, ADJP, etc.), for each of these phrasal types appearing as the root of a parse tree, and for miscellaneous parse tree structures which we thought might be more likely to reflect opinionated language, e.g. the number of occurrences of a subordinate clause within a verb phrase (e.g. thought that...) or an adverbial inside an adjectival phrase (e.g. much more useful ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Lexicon Features</head><p>Our lexicon-based features were based on aggregate scores derived from looking up words in the lexicon SentiWordNet <ref type="bibr" coords="3,53.76,423.35,9.11,8.97" target="#b3">[4]</ref>. SentiWordNet is a lexicon which assigns negative and positive scores to each sense of each synset in WordNet based on a semi-supervised classification of WordNet synsets. It has been used in the past for opinion finding by participants in the Blog Track to varying degrees of success <ref type="bibr" coords="3,239.37,465.11,13.37,8.97" target="#b9">[10]</ref>, <ref type="bibr" coords="3,258.68,465.11,13.37,8.97" target="#b11">[12]</ref>. For our experiments, we considered the positive SentiWordNet score for a word w to be the mean of the positive scores for all the word senses of that word:</p><formula xml:id="formula_0" coords="3,96.60,511.19,196.31,27.15">spos (w) = 1 n n i=0 1 m m k=0 P osSwn i,k<label>(1)</label></formula><p>where n is the number of synsets the word appears in, m is the number of word senses in the synset for that word and P osSwn i,k is the positivity score for word sense k in synset i for word w. The positive score for a document is the mean spos (w) for all words in the document and is given by:</p><formula xml:id="formula_1" coords="3,114.24,603.11,178.67,27.39">Scorepos (d) = 1 p p i=0 spos (wi)<label>(2)</label></formula><p>for a document d with p words. The negative score is calculated similarly giving a feature vector of length two.</p><p>We also submitted runs where we did not use a classifier for lexicon scores. As the positive and negative values so closely resemble what the classifier score represents ("positivity", "negativity"), we thought it would be interesting to simply use the raw features as scores. For these runs we used the positive score from equation 2 for positive opinion retrieval runs and the negative score from the same equation for negative opinion retrieval runs. In this way, positive runs will not take into account negative scores and vice versa, as is the case with the classifier.</p><p>For opinion retrieval we used a weighted sum of the positive and negative scores to give an opinion score. Tuning the weights based on MAP from a combination of the 2006 and 2007 topics, we found a weighting of 0.7 for the positive score and 0.3 for the negative score to be a good approximate optimization. This might suggest that positive word orientation is more indicative of opinionate but could also simply reflect the larger volume of positive posts than negative posts in the training corpus. Whether each run uses a classifier for generating the lexicon module score is noted in Table <ref type="table" coords="3,342.11,224.40,4.61,8.97" target="#tab_1">1</ref> and Table <ref type="table" coords="3,392.99,224.40,3.56,8.97" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document Scoring</head><p>For the opinion retrieval task, we employed three logistic regression classifiers, one for each of the feature sets mentioned in Section 2.2. For the opinion retrieval task, each of the classifiers is a binary classifier which classifies documents as opinionated or non-opinionated. For the polarised opinion retrieval task we used the same system except the Sen-tiWordNet classifier was retrained as a binary positive/nonpositive classifier for positive blog retrieval and as a binary negative/non-negative classifier for negative blog retrieval. It was found during development that this performed significantly better than training either or both of the classifiers in the surface or syntactic module to detect polarity rather than opinion. This suggests that the linguistic characteristics do not differ as significantly between positive and negative blogs as between opinionated and non-opinionated. We found that using the surface and syntactic classifiers as prior subjectivity scores helped re-enforce the scores as designated by the polarity lexicon module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Training</head><p>We used the qrels from the Blog Track in TREC 2006 and TREC 2007 as training for the classifiers in our system. Opinionated posts were considered to be those that were judged mixed, positive, or negative. Non-opinionated posts were considered to be those judged relevant. We considered negative posts to be those judged negative and non-negative posts were considered to be those judged mixed, relevant and positive. We considered positive posts to be those judged positive and non-positive posts were considered to be those judged mixed, relevant and negative. It is noted that the inclusion of mixed posts as non-relevant for polarised opinion retrieval is contentious as mixed posts by definition contain both negative and positive text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Scoring</head><p>The relevant documents returned from Terrier are scored for opinion relevance in each of the modules as well as for negative and positive opinion in the lexicon classifier. The scores recorded are the probabilities from the probability distribution as determined by the logistic classifiers, except for the lexicon module which for some runs does not use a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fusion</head><p>The scores in the system are fused through a multi-stage weighted CombSum late fusion <ref type="bibr" coords="4,179.62,78.48,13.37,8.97" target="#b10">[11]</ref>. For comparison we also submitted two unweighted fusion runs for the opinion retrieval run for baseline4. In the first stage, the scores from the three modules for a document d are added according to weights which sum to one:</p><formula xml:id="formula_2" coords="4,87.36,137.03,205.55,9.02">Scoreop (d) = w1.s lex + w2.s surf + w3.ssyn<label>(3)</label></formula><p>where:</p><formula xml:id="formula_3" coords="4,137.52,170.27,71.69,8.97">w1 + w2 + w3 = 1</formula><p>For the runs based on baseline4 and our title and description baseline we omitted syntactic features so the fused opinion score becomes:</p><formula xml:id="formula_4" coords="4,108.12,224.39,184.79,9.02">Scoreop (d) = w1.s lex + w2.s surf<label>(4)</label></formula><p>where:</p><formula xml:id="formula_5" coords="4,148.56,257.51,49.60,8.97">w1 + w2 = 1</formula><p>For polarity runs, the fused polarity score is calculated similarly, using the polarity scores from the lexicon module in place of the opinion lexicon score. This score is then fused in a similar manner with the relevance score Score rel as determined by Terrier to give the overall relevant opinion score Score relop :</p><formula xml:id="formula_6" coords="4,85.92,343.07,206.99,9.02">Score relop (d) = w4.Scoreop + (1 -w4) .s rel<label>(5)</label></formula><p>where:</p><p>w4 &lt;= 1</p><p>The weights we used for the runs we submitted are listed in Table <ref type="table" coords="4,79.07,413.51,4.61,8.97" target="#tab_1">1</ref> and Table <ref type="table" coords="4,129.95,413.51,3.56,8.97" target="#tab_2">2</ref>.</p><p>These were obtained by grid searching in the parameter space [w1, w2, w3, w4] ([w1, w2, w4] for the runs without syntactic features) that produced optimizations of MAP for the combined 2006 and 2007 topics. It is interesting to note that whenever syntactic features are used, they appear to subsume the surface features, as optimizations result in the surface module weight tending towards 0.</p><p>The Terrier results for each topic are then re-ranked according to the combined relevance and opinion score Score relop to give the final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS AND ANALYSIS</head><p>The results for our opinion retrieval runs are detailed in Table 3. DCU's best run based on a non-distributed baseline was DCUCDVPtol. This run is based on our topic-only baseline and used syntactic features and a classifier-based lexicon module. This baseline performed better than the median on 29 out of the 50 new topics in 2008 as shown in Figure <ref type="figure" coords="4,83.40,637.31,3.56,8.97" target="#fig_1">2</ref>. The highest scoring opinion finding run we submitted based on a distributed baseline was DCUCDVPgoo which performs better than median on 44 out of the 50 new topics this year as shown in Figure <ref type="figure" coords="4,197.98,668.75,3.56,8.97" target="#fig_2">3</ref>. This run did not use syntactic features and used a classifier-based lexicon module.</p><p>Our results are consistent with the observations in last year's overview paper <ref type="bibr" coords="4,120.10,710.63,9.11,8.97">[5]</ref>, that the higher the opinion MAP for the baseline, the more difficult it is to achieve a percentage increase above the baseline for opinion finding.</p><p>In the graph, MAP is shown in descending order alongside the median MAP for that topic. For the run based on our topic-only baseline, there is a steady degradation in MAP and no sudden jumps or spikes in the graph. There is however a number of topics that perform significantly below baseline. On closer inspection, most of these topics also performed significantly worse that the median in the baseline run for topic retrieval. Some examples of low-performing topics include "System of a Down" and "I Walk the Line" which would have benefitted from word grouping or phrasing techniques. Such a pattern is not evident in the runs based on baseline4 where the retrieval baseline is much stronger and there is no topic where the median performs a significant amount better.</p><p>For polarised opinion detection our run based on our topic only baseline perfomed better than our run based on the topic and description baseline for both positive and negative opinion finding. This reflects a similar pattern to the relevance MAP results.</p><p>Our best polarity runs on baseline4 were for the unoptimized configuration for negative opinion finding and the optimized configuration for positive finding.</p><p>Our non-optimized fusion runs performed significantly worse than their optimized counterparts. Whether the lexicon module uses a classifier or not seems to have little effect. This is the only difference, for example, between DCUCD-VPtol and DCUCDVPtolnc whose performance differs only by 0.0003 in terms of MAP.</p><p>For the polarity task our highest scoring run based on a non-distributed baseline was DCUCDVPtpl for both positive and negative opinion, which was based on our title only baseline. Regarding the distributed baseline, DCUCD-VPgpo performed best for positive opinion, and DCUCD-VPgp 2 for negative opinion. Comparing across the different baselines highlights surprising results -the runs based on    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>We developed a system this year which evaluated three different approaches to opinion finding and polarised opinion finding. We also investigated the advantage in weighting different approaches in a fusion process.</p><p>We found that our most successful runs made use of syntactic features. Our best-performing runs also benefitted from using weighted scores when combining sources rather than unweighted combinations.</p><p>Inspection of our performance on a per topic basis shows that our opinion retrieval performance was hampered in several topics due to a low retrieval precision. Our runs based on baseline4, a much stronger baseline than our own, demonstrate a much more consistent performance.</p><p>ficially distributed results. The original submission was a based on an erroneous configuration.</p><p>It is also noted that subjectivity detection is a very important part of polarity detection and that with two of our three modules tuned to opinion finding, regardless of polarity, we achieved a significant improvement on baseline for the opinion finding task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,218.76,366.08,172.05,8.08;2,156.00,53.95,297.52,297.53"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: DCU's system architecture</figDesc><graphic coords="2,156.00,53.95,297.52,297.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,53.76,673.04,502.29,8.08;5,53.76,683.48,89.72,8.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Per-topic MAP for the best-performing opinion run based on a non-distributed baseline, DCU's topic only baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,53.76,275.36,502.37,8.08"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Per-topic MAP for DCU's best-performing opinion run based on a distributed baseline, baseline4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,322.20,64.16,239.19,178.44"><head>Table 3 :</head><label>3</label><figDesc>Baseline Results for Opinion Retrieval</figDesc><table coords="4,322.20,74.28,239.19,168.33"><row><cell>Run</cell><cell>Type</cell><cell></cell><cell cols="3">MAP R-prec P@10</cell></row><row><cell>DCUCDVPtbl</cell><cell>Title</cell><cell></cell><cell>0.2875</cell><cell>0.328</cell><cell>0.556</cell></row><row><cell cols="4">DCUCDVPtdbl Title + Desc 0.264</cell><cell>0.3145</cell><cell>0.55</cell></row><row><cell>baseline4</cell><cell cols="2">Distributed</cell><cell cols="3">0.3822 0.4284 0.616</cell></row><row><cell cols="5">Table 4: Opinion Retrieval Results</cell></row><row><cell>Run</cell><cell>MAP</cell><cell cols="4">R-prec P@10 % over b/l</cell></row><row><cell>DCUCDVPto</cell><cell cols="3">0.3216 0.3706</cell><cell>0.61</cell><cell>11.86</cell></row><row><cell>DCUCDVPtol</cell><cell cols="4">0.3299 0.3679 0.636</cell><cell>14.75</cell></row><row><cell>DCUCDVPtolnc</cell><cell>0.3296</cell><cell></cell><cell>0.3673</cell><cell>0.632</cell><cell>14.64</cell></row><row><cell>DCUCDVPtdo</cell><cell>0.2927</cell><cell></cell><cell>0.3439</cell><cell>0.594</cell><cell>10.87</cell></row><row><cell>DCUCDVPgo</cell><cell>0.4064</cell><cell></cell><cell>0.4392</cell><cell>0.676</cell><cell>6.33</cell></row><row><cell>DCUCDVPgonc</cell><cell>0.4052</cell><cell></cell><cell>0.4418</cell><cell>0.662</cell><cell>6.02</cell></row><row><cell>DCUCDVPgoo</cell><cell cols="3">0.4155 0.4479</cell><cell>0.68</cell><cell>8.71</cell></row><row><cell cols="5">DCUCDVPgoonc 0.4125 0.4491 0.674</cell><cell>7.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,67.44,89.96,471.72,114.24"><head>Table 1 :</head><label>1</label><figDesc>Opinion Retrieval Configuration</figDesc><table coords="5,67.44,100.08,471.72,104.13"><row><cell>Run</cell><cell>Baseline</cell><cell>Relevance Weight</cell><cell cols="3">Feature Weights Lexicon Surface Syntactic</cell><cell>Classifier for Lexicon?</cell></row><row><cell>DCUCDVPto</cell><cell>Title</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>n/a</cell><cell>yes</cell></row><row><cell>DCUCDVPtol</cell><cell>Title</cell><cell>0.5</cell><cell>0.4</cell><cell>0</cell><cell>0.6</cell><cell>yes</cell></row><row><cell>DCUCDVPtolnc</cell><cell>Title</cell><cell>0.5</cell><cell>0.4</cell><cell>0</cell><cell>0.6</cell><cell>no</cell></row><row><cell>DCUCDVPtdo</cell><cell>Title + Description</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>n/a</cell><cell>yes</cell></row><row><cell>DCUCDVPgo</cell><cell>baseline4</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>yes</cell></row><row><cell>DCUCDVPgonc</cell><cell>baseline4</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>no</cell></row><row><cell>DCUCDVPgoo</cell><cell>baseline4</cell><cell>0.6</cell><cell>0.4</cell><cell>0.6</cell><cell>n/a</cell><cell>yes</cell></row><row><cell>DCUCDVPgoonc</cell><cell>baseline4</cell><cell>0.6</cell><cell>0.4</cell><cell>0.6</cell><cell>n/a</cell><cell>no</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,99.72,275.36,407.42,115.20"><head>Table 2 :</head><label>2</label><figDesc>Polarised Opinion Retrieval Configuration</figDesc><table coords="5,99.72,285.48,407.42,105.09"><row><cell>Run</cell><cell>Polarity</cell><cell cols="2">Baseline Relevance Weight</cell><cell cols="3">Feature Weights Lexicon Surface Syntactic</cell></row><row><cell>DCUCDVPtpl</cell><cell>Title Title</cell><cell>positive negative</cell><cell>0.5 0.5</cell><cell>0.3 0.6</cell><cell>0.25 0.05</cell><cell>0.45 0.35</cell></row><row><cell>DCUCDVPtdp</cell><cell cols="2">Title + Description positive Title + Description negative</cell><cell>0.35 0.5</cell><cell>0.35 0.65</cell><cell>0.65 0.35</cell><cell>n/a n/a</cell></row><row><cell>DCUCDVPgp</cell><cell>baseline4 baseline4</cell><cell>positive negative</cell><cell>n/a n/a</cell><cell>n/a n/a</cell><cell>n/a n/a</cell><cell>n/a n/a</cell></row><row><cell>DCUCDVPgpo</cell><cell>baseline4 baseline4</cell><cell>positive negative</cell><cell>0.5 0.75</cell><cell>0.35 0.55</cell><cell>0.65 0.45</cell><cell>n/a n/a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,53.76,311.72,247.24,199.08"><head>Table 5 :</head><label>5</label><figDesc>Polarised Opinion Retrieval ResultsDCUCDVPgpopos 0.1644 0.1924 0.2041 5.18 neg 0.1505 0.1747 0.1813 17.12 the distributed baseline performed much better for negative opinion finding and the runs based on our topic only baseline performed better on the positive opinion finding task in terms of percentage MAP above baseline. The reason for this is not entirely clear at this stage. It is worth noting that if the opinion finding runs are assessed for polarity, a similar pattern is observed.</figDesc><table coords="6,59.52,321.84,241.48,72.69"><row><cell>Run</cell><cell>Pol</cell><cell>MAP</cell><cell>R-prec</cell><cell>P@10</cell><cell>%</cell></row><row><cell>DCUCDVPtpl</cell><cell cols="5">pos 0.1109 0.1494 neg 0.1111 0.1357 0.175 0.1408 13.39 5.71</cell></row><row><cell>DCUCDVPtdp</cell><cell cols="5">pos 0.1079 0.1507 0.1469 11.12 neg 0.0932 0.1208 0.1417 5.19</cell></row><row><cell>DCUCDVPgp</cell><cell cols="4">pos 0.1642 0.2074 0.2028 neg 0.1528 0.1714 0.1708</cell><cell>5.05 19</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the <rs type="institution">Irish Centre for High-End Computing</rs> for the use of their computer facilities. We would also like to thank <rs type="person">Joachim Wagner</rs> for the time and effort he spent parsing the document set. This work was supported by <rs type="funder">Science Foundation Ireland</rs> under grant number <rs type="grantNumber">07/CE/I1147</rs> and by <rs type="funder">Enterprise Ireland Commercialisation Fund</rs> under grant number <rs type="grantNumber">CFTD/2007/229</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FEuae5K">
					<idno type="grant-number">07/CE/I1147</idno>
				</org>
				<org type="funding" xml:id="_gxYHz4K">
					<idno type="grant-number">CFTD/2007/229</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.29,463.55,96.59,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,486.95,209.88,8.97;6,335.64,497.39,213.84,8.97;6,335.64,507.83,197.20,8.97;6,335.64,518.27,201.39,8.97;6,335.64,528.71,42.88,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,489.09,486.95,56.42,8.97;6,335.64,497.39,209.77,8.97">Course-to-fine n-best-parsing and MaxEnt discriminative reranking</title>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,347.04,507.83,185.80,8.96;6,335.64,518.27,16.14,8.96">Proceedings of the 43rd Annual Meeting of the ACL</title>
		<meeting>the 43rd Annual Meeting of the ACL<address><addrLine>Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,540.23,220.31,8.97;6,335.64,550.67,202.74,8.97;6,335.64,561.11,211.14,8.97;6,335.64,571.55,220.12,8.96;6,335.64,582.11,200.18,8.97;6,335.64,592.55,20.80,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,335.64,550.67,202.74,8.97;6,335.64,561.11,164.85,8.97">Mining the peanut gallery: opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName coords=""><forename type="first">Kushal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,519.12,561.11,27.66,8.96;6,335.64,571.55,220.12,8.96;6,335.64,582.11,80.55,8.96">WWW &apos;03: Proceedings of the twelfth international conference on World Wide Web</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,603.95,152.41,8.97;6,335.64,614.39,198.82,8.97;6,335.64,624.95,119.29,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,335.64,614.39,198.82,8.97;6,335.64,624.95,114.88,8.97">SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,636.35,196.94,8.97;6,335.64,646.79,194.68,8.97;6,335.64,657.23,204.90,8.96;6,335.64,667.79,72.27,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,335.64,646.79,160.30,8.97">Overview of the TREC-2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,514.80,646.79,15.52,8.96;6,335.64,657.23,204.90,8.96;6,335.64,667.79,44.14,8.96">The Sixteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>TREC 2007) Proceedings</note>
</biblStruct>

<biblStruct coords="6,335.63,679.19,220.01,8.97;6,335.64,689.63,218.31,8.97;6,335.64,700.07,177.57,8.97;6,335.64,710.63,131.08,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,398.50,689.63,155.45,8.97;6,335.64,700.07,111.24,8.97">Building a Large Annotated Corpus of English: the Penn Treebank</title>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,454.32,700.07,58.89,8.96;6,335.64,710.63,42.05,8.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,57.00,209.06,8.97;7,72.60,67.56,210.45,8.97;7,72.60,78.00,209.77,8.97" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,255.70,67.56,27.35,8.97;7,72.60,78.00,118.78,8.97">Terrier information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,89.40,198.00,8.97;7,72.60,99.84,196.54,8.97;7,72.60,110.40,213.92,8.97;7,72.60,120.84,178.83,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,205.06,99.84,64.08,8.97;7,72.60,110.40,79.12,8.97">Overview of the trec-2006 blog track</title>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,170.04,110.40,116.48,8.96;7,72.60,120.84,150.70,8.96">The Fifteenth Text REtrieval Conference (TREC 2007) Proceedings</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,132.24,218.76,8.97;7,72.60,142.68,212.97,8.97;7,72.60,153.24,220.24,8.97;7,72.60,163.67,209.86,8.96;7,72.60,174.11,203.18,8.97;7,72.60,184.55,220.21,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,72.60,142.68,212.97,8.97;7,72.60,153.24,75.64,8.97">Thumbs up ?: sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,166.08,153.24,126.76,8.96;7,72.60,163.67,209.86,8.96;7,72.60,174.11,76.34,8.96">EMNLP &apos;02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,195.95,206.91,8.97;7,72.60,206.51,207.06,8.97;7,72.60,216.95,218.67,8.96;7,72.60,227.39,20.80,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,129.36,206.51,135.02,8.97">DUTIR at TREC 2007 blog track</title>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daming</forename><surname>Tang Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongfei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,72.60,216.95,213.51,8.96">Proceedings of the Text Retrieval Conference (TREC)</title>
		<meeting>the Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,238.79,214.11,8.97;7,72.60,249.35,190.63,8.97;7,72.60,259.79,217.59,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,224.38,238.79,62.33,8.97;7,72.60,249.35,67.16,8.97">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,158.40,249.35,104.83,8.96;7,72.60,259.79,125.82,8.96">Proceeding of the 3rd Text Retrieval Conference (TREC-3)</title>
		<meeting>eeding of the 3rd Text Retrieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,271.19,205.84,8.97;7,72.60,281.63,189.90,8.97;7,72.60,292.19,146.67,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,189.47,271.19,88.96,8.97;7,72.60,281.63,78.51,8.97">UCSC on TREC 2006 blog opinion mining</title>
		<author>
			<persName coords=""><forename type="first">Ethan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,169.92,281.63,92.58,8.96;7,72.60,292.19,117.63,8.96">Proceedings of the Text Retrieval Conference (TREC)</title>
		<meeting>the Text Retrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
