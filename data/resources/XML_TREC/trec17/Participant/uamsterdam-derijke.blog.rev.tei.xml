<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.08,83.76,337.56,15.48">External Query Expansion in the Blogosphere</title>
				<funder ref="#_xbkVc4M">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_KjYa3aj #_s4Dn8JK #_RduAhhb #_czKy764 #_uzkaqRB #_QQqfgqu #_Bz36yxP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_KJqztnc">
					<orgName type="full">Dutch and Flemish Governments</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.48,116.28,97.60,10.75"><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.04,116.28,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.08,83.76,337.56,15.48">External Query Expansion in the Blogosphere</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E783A077C1139A2535961F00F9CD15CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of the University of Amsterdam's ILPS group in the blog track at TREC 2008. We mainly explored different ways of using external corpora to expand the original query. In the blog post retrieval task we did not succeed in improving over a simple baseline (equal weights for both the expanded and original query). Obtaining optimal weights for the original and the expanded query remains a subject of investigation. In the blog distillation task we tried to improve over our (strong) baseline using external expansion, but due to differences in the run setup, comparing these runs is hard. Compared to a simpler baseline, we see an improvement for the run using external expansion on the combination of news, Wikipedia and blog posts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We describe our participation in this year's TREC Blog track. Like last year, the blog track consists of two separate tasks: blog post retrieval and blog distillation. Besides the task of finding topically relevant blog posts, the blog post retrieval task has two further tasks: finding blog posts that contain an opinion on the given topic and determining the polarity of the opinion. To test the opinion-ranking capabilities of participants' systems, participants were asked to rerank five baseline runs based on opinionatedness, besides submitting four full opinion retrieval runs. Our main interest this year lies with the topical retrieval of both blog posts and blogs. We did not participate in the polarity determination and only submitted very basic opinion finding runs.</p><p>The remainder of this paper introduces our retrieval approaches for both tasks in Section 2, and explains the way we incorporated external sources in query modeling in Section 3. We then zoom in on the runs for both tasks and their results: post retrieval in Section 5 and blog distillation in Section 6. Finally, we conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Approaches</head><p>In the blog post retrieval task we use an out-of-the-box implementation of Indri.<ref type="foot" coords="1,406.60,222.91,3.69,6.39" target="#foot_0">1</ref> Results of previous years showed good overall performance of Indri compared to other systems and besides, it allows for easy use of query models (queries consisting of weighted terms).</p><p>In the blog distillation task we use our in-house expert retrieval model <ref type="bibr" coords="1,371.93,284.62,75.79,8.64" target="#b0">(Balog et al., 2006)</ref>, which we translated to fit the task of blogger retrieval <ref type="bibr" coords="1,431.67,296.58,78.30,8.64" target="#b1">(Balog et al., 2008;</ref><ref type="bibr" coords="1,513.01,296.58,42.91,8.64;1,316.81,308.53,45.16,8.64">Weerkamp et al., 2008)</ref>. The main reason for using this model is that we believe blog distillation should be solved using a post index (as opposed to a full blog index). Although last year's blog track showed good performance of blog indexes, we stick to a post index for three reasons: (i) a post index allows for easy incremental updating, (ii) posts are a natural unit for result presentation to the user, and most importantly, (iii) only one index is needed for both post retrieval and blog distillation.</p><p>We estimate the probability of a blog blog generating query Q as follows: Q) .</p><formula xml:id="formula_0" coords="1,371.36,435.37,120.38,20.70">P(Q|θ blog ) = ∏ t∈Q P(t|θ blog ) n(t,</formula><p>(1)</p><p>Next, we smooth the probability of a term given a blog with the background probabilities:</p><formula xml:id="formula_1" coords="1,340.24,502.57,215.68,9.81">P(t|θ blog ) = (1 -λ blog ) • P(t|blog) + λ blog • P(t). (2)</formula><p>Finally, we estimate P(t|blog) as follows:</p><formula xml:id="formula_2" coords="1,329.07,546.17,226.84,18.64">P(t|blog) = ∑ post∈blog P(t|post, blog) • P(post|blog). (3)</formula><p>We assume that the post and the blog are conditionally independent, thus P(t|post, blog) = P(t|post), and approximate P(t|post) with the standard maximum likelihood estimate. In Section 6 we detail our choices for estimating p(post|blog).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Modeling</head><p>For both tasks we experimented with query models using external corpora. In short, we assume that documents in the target collection (the blog collection) are too noisy to generate good query models based on blind relevance feedback. Instead, we use different, less noisy external corpora for expanding our original query. As much of what goes on in the blogosphere is determined by news events, we use a contemporary news corpus AQUAINT-2<ref type="foot" coords="2,204.43,115.12,3.69,6.39" target="#foot_1">2</ref> as our external corpus. Besides this, many queries directed towards blogs and blog posts contain named entities (persons, locations, organizations, products) or general concepts (especially in blog distillation). For this we also look at Wikipedia as an external corpus, since this source contains focused information on many general concepts and named entities.</p><p>For two post retrieval runs we use Lavrenko's relevance model 2 <ref type="bibr" coords="2,91.99,212.69,114.82,8.64" target="#b2">(Lavrenko and Croft, 2001)</ref> to select the top 10 terms from the top 10 external documents. After selecting weighted new terms, we construct the final query model P(t|θ Q ) by combining this new query P(t| θQ ) with the original query P(t|Q) using:</p><formula xml:id="formula_3" coords="2,93.46,275.51,199.44,11.78">P(t|θ Q ) = λ Q P(t| θQ ) + (1 -λ Q )P(t|Q).<label>(4)</label></formula><p>In two opinion retrieval runs and two blog distillation runs we use an experimental approach to query expansion. We estimate the probability of a expansion term t given the query Q and set of external corpora C:</p><formula xml:id="formula_4" coords="2,104.60,347.86,188.30,25.26">P(t|Q,C) = ∑ c∈C P(t|c, Q) • P(c|Q) ∑ c ∈C p(c |Q) .<label>(5)</label></formula><p>We estimate p(t|c, Q) based on the probability of document d given the query and corpus, and the probability of term t given the document: </p><formula xml:id="formula_5" coords="2,87.99,422.40,45.22,8.74">P(t|c, Q) =</formula><p>Next, we estimate P(D|Q, c), the probability of document D given corpus c and query Q:</p><formula xml:id="formula_7" coords="2,88.36,479.22,200.67,27.05">P(D|Q, c) = ∏ q∈Q P(q|D) + n(Q, D) • |Q| -1 |D| . (<label>7</label></formula><formula xml:id="formula_8" coords="2,289.03,488.13,3.87,8.64">)</formula><p>where n(Q, D) is the count of phrase Q in document D and P(q|d) = n(q, D)•|D| -1 . Finally, we estimate the probability of corpus c given query Q:</p><formula xml:id="formula_9" coords="2,79.38,556.56,213.52,25.69">P(c|Q) = ∑ D∈c;P(D|Q,c)&gt;0 P(D|Q, c) |D ∈ c; p(D|Q, c) &gt; 0| . (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Metrics and Significance</head><p>In this paper we report on mean average precision (MAP), precision at 5 and 10 documents (P5, P10), and mean reciprocal rank (MRR). We use the Wilcoxon signed-rank test to test for significant differences between runs. We report on significant increases (or drops) for p &lt; .01 using (and ) and for p &lt; .05 using (and ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Blog Post Retrieval</head><p>As explained in the introduction to this section, we use an out-of-the-box implementation of Indri as our retrieval system. Runs are evaluated on two topic sets: the new 2008 topics alone and the full set of 150 topics <ref type="bibr" coords="2,483.12,115.52,22.48,8.64">(2006)</ref><ref type="bibr" coords="2,505.60,115.52,4.50,8.64">(2007)</ref><ref type="bibr" coords="2,510.10,115.52,22.48,8.64">(2008)</ref>.</p><p>We submitted 6 runs:</p><p>(A) uams08n1o1 the baseline run uses a news corpus for query expansion with λ Q = 0.5 (i.e. equal weights to expanded and original query) and assigns priors to posts based on credibility. (B) uams08n1o1sp identical to previous run, but with "opinionatedness prior". (C) uams08class query expansion using both a news corpus and Wikipedia; λ Q trained on 2006 and 2007 topics, and priors based on credibility indicators. (D) uams08clspr identical to the previous run, but with "opinionatedness prior". (E) uams08qm4it1 query expansion following Section 3 on a news corpus and Wikipedia. (F) uams08qm4it2 identical to previous run, but with the blog post corpus as additional source.</p><p>We experiment with estimating λ Q based on old topics: for each of the old (2006/2007) topics we know the performance of various parameter settings (weights of different corpora) in terms of MAP. We use this information in the following way: for each unseen topic t we assing a similarity score to seen topics (t) based on overlapping documents in the result lists. Next, we multiply this overlap score by the MAP performance of each mixture setting and determine the "optimal" mixture weights this way. This method is used in runs uams08class and uams08clspr. Four of our runs (A-D) also use credibility priors: based on a combination of 6 credibility indicators <ref type="bibr" coords="2,492.71,477.90,63.20,8.64;2,316.81,489.85,59.16,8.64">(Weerkamp and de Rijke, 2008)</ref>, we estimate the prior probability of the blog post being relevant. Since all runs use the same priors, we cannot determine its effectiveness here, but it has proven successful before <ref type="bibr" coords="2,388.18,525.72,123.05,8.64">Weerkamp and de Rijke (2008)</ref>.</p><p>Looking at opinion retrieval, we explore the use of an "opinionatedness prior." To construct this prior we use strongly opinionated terms from the OpinionFinder system<ref type="foot" coords="2,551.73,559.65,3.69,6.39" target="#foot_2">3</ref> and calculate for each post the ratio of opinionated terms to the total number of terms. We use this prior on top of our two baseline runs uams08n1o1 and uams08class, to come to runs uams08n1o1sp and uams08clspr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results and Discussion</head><p>From the results in Tables <ref type="table" coords="2,419.91,657.27,4.98,8.64">1</ref> and<ref type="table" coords="2,443.19,657.27,4.98,8.64">2</ref> we have three initial observations: (i) The runs using the method for combining external corpora introduced in Section 3 (i.e., uams08qm4it1 and uams08qm4it2) perform significantly worse than runs using  <ref type="table" coords="3,78.21,247.78,3.88,8.64">1</ref>: Opinion results on the blog post retrieval task. Significance of uams08clspr and uams08n1o1sp tested against their baselines, other runs tested against the first run, uams08n1o1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>relevance models and a linear combination of the expanded query and original query (uams08n1o1 and uams08class).</p><p>(ii) Looking at the runs using relevance models to construct query models (uams08n1o1 and uams08class), we see that estimating the relative importance of the original query is not easy: the simple baseline approach (λ = 0.5) outperforms the slightly more advanced per-topic estimation. (iii) The runs using opinion priors (uams08n1o1sp and uams08clspr) significantly outperform their baseline counterparts in terms of MAP, not only on opinion retrieval, but also on topical retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Blog Distillation</head><p>Our blog distillation model allows for the estimation of the importance of individual posts to a blog, i.e., estimating association strengths between posts and their blog (P(post|blog) in Eq. 3).</p><p>Based on previous experiments <ref type="bibr" coords="3,81.20,543.45,99.64,8.64">(Weerkamp et al., 2008)</ref> and additional tests on the 2007 topics we use a combination of blog features to estimate this association strength: post length, recency, and number of comments. On top of this, we noticed that using information from the post title is an important indicator of relevance in the blog distillation task. To be able to use this information, we perform a linear combination between runs on the full post index and runs on a title-only index. This run is our baseline run, uams08bl.</p><p>We again experiment with expansion on external corpora using the novel method introduced in Section 3. In run uams08nw we use the news corpus and Wikipedia, in run uams08pnw we also use the post index as external corpus. The difference with the baseline is that we do not use the combination with the title-only index: for this submission Run MAP P5 P10 MRR All topics uams08n1o1 0.4350 0.7680 0.7480 0.8464 uams08n1o1sp 0.4366 0.7667 0.7473 0.8419 uams08class 0.4313 0.7507 0.7493 0.8439 uams08clspr 0.4332 0.7520 0.7473 0.8441 uams08qm4it1 0.3627 0.6800 0.6713 0.7780 uams08qm4it2 0.2745 0.5760 0.5740 0.6869 2008 topics uams08n1o1 0.4644 0.8040 0.7620 0.8892 uams08n1o1sp 0.4661 0.8000 0.7620 0.8892 uams08class 0.4494 0.7680 0.7480 0.8358 uams08clspr 0.4513 0.7720 0.7500 0.8408 uams08qm4it1 0.3734 0.6720 0.6600 0.8052 uams08qm4it2 0.2606 0.5480 0.5380 0.6981 Table <ref type="table" coords="3,341.45,247.78,3.88,8.64">2</ref>: Topical results on the blog post retrieval task. Significance of uams08clspr and uams08n1o1sp tested against their baselines, other runs tested against the first run, uams08n1o1.</p><p>we would like to look at the influence of the query expansion and scores of the two runs (using query expansion and the title-only run) are in a very different range, calling for other, more suitable ways of combining these scores.</p><p>The final run we submitted, uams08nonr is a highly experimental run: an important aspect of the blog distillation task is to return not just blogs that mention this topic, but mention it quite often. In that sense, we do not only want to determine the relevance of the blog for a given topic, but also the non-relevance for that topic (i.e. relevant regarding different topics). We tried to estimate this by looking at the performance of blogs on the 2007 topics and use this as indicator of non-relevance (assuming the 2008 topics are different from the 2007 topics); the relevance score of a blog (Eq. 1) is divided by the average relevance score of that blog on all 2007 topics. A blog with a high relevance score and low relevance scores on other topics will get a score (and rank) boost.</p><p>Summarizing, we submitted the following runs, and added one extra baseline run to our results table: We submitted 6 runs:</p><p>(A) uams08bl P(post|blog) based on number of comments, post length, and recency; combination of ti-tle+body run and title-only run. (B) baseline identical to previous run, but without the combination with a title-only run. (C) uams08nw identical to previous run, but with query expansion following Section 3 on a news corpus and Wikipedia. (D) uams08pnw identical to previous run, but with the blog post corpus as additional external corpus. (E) uams08nonr ratio of relevance to non-relevance of a blog.</p><p>Run MAP P5 P10 MRR baseline 0.2567 0.4480 0.4180 0.7298 uams08bl 0.2638 0.4600 0.4200 0.7294 uams08nonr 0.0257 0.1000 0.0900 0.2393 uams08nw 0.2489 0.4080 0.3660 0.6515 uams08pnw 0.2620 0.4080 0.3900 0.6303 Table <ref type="table" coords="4,78.97,139.39,3.88,8.64">3</ref>: Results on the blog distillation task. Significance tested against baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results and Discussion</head><p>The results of our submitted runs, plus the evaluation of one additional run are presented in Table <ref type="table" coords="4,231.21,228.94,3.74,8.64">3</ref>. The results show some interesting things: (i) The experimental run using "non-relevance" fails completely, indicating we need different ways of incorporating this notion of non-relevance. (ii) Our baseline ( <ref type="formula" coords="4,127.70,276.76,36.31,8.64">uams08bl</ref>) is a pretty strong baseline and cannot be beaten by the other runs (except on MRR by baseline). (iii) Query expansion can improve over the absolute baseline in terms of MAP, but still performs less than the combination with the titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this year's participation in the blog track we mainly explored different ways of using external corpora to expand the original query. In the blog post retrieval task we did not succeed in improving over a simple baseline (equal weights for both the expanded and original query) and we need a thorough analysis to find out why this did not work. For the same task, further investigation is needed to determine the effectiveness of the credibility priors and to see what happens when the opinion prior is applied.</p><p>In the blog distillation task we tried to improve over our (strong) baseline using external expansion. Since this baseline also uses information from the title explicitly, it is hard to determine why the expanded runs do not improve over the baseline. Compared to a baseline without the title component, we see an improvement for the run using expansion on the combination of news, Wikipedia and blog posts. For this task, further research into the combination of title and full post components is needed, as well as the combination with expanded queries. The run that tried to capture nonrelevance of a blog failed, but exploring this area further could lead to significant improvements over a baseline that looks only at "relevance."</p><p>Finally, looking at the two tasks combined, we see that query expansion on the blog distillation task is much more effective than on the blog post retrieval task. Further analysis is needed to find out why this difference occurs.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,331.16,712.87,134.14,5.63"><p>http://www.lemurproject.org/indri</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,68.14,703.41,191.04,5.63;2,53.80,712.87,101.62,5.63"><p>http://trec.nist.gov/data/qa/2007_qadata/qa.07. guidelines.html#documents</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,331.16,712.87,113.81,5.63"><p>http://www.cs.pitt.edu/mpqa/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This research was supported by the <rs type="projectName">DuOMAn</rs> project carried out within the <rs type="programName">STEVIN programme</rs> which is funded by the <rs type="funder">Dutch and Flemish Governments</rs> (http://www. stevin-tst.org) under project number <rs type="grantNumber">STE-09-12</rs>, and by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">640.001.-501</rs>, <rs type="grantNumber">640.002.501</rs>, <rs type="grantNumber">612.066.512</rs>, <rs type="grantNumber">612.061.814</rs>, <rs type="grantNumber">612.061.815</rs>, and <rs type="grantNumber">640.004.802</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KJqztnc">
					<idno type="grant-number">STE-09-12</idno>
					<orgName type="project" subtype="full">DuOMAn</orgName>
					<orgName type="program" subtype="full">STEVIN programme</orgName>
				</org>
				<org type="funding" xml:id="_xbkVc4M">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_KjYa3aj">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_s4Dn8JK">
					<idno type="grant-number">640.001.-501</idno>
				</org>
				<org type="funding" xml:id="_RduAhhb">
					<idno type="grant-number">640.002.501</idno>
				</org>
				<org type="funding" xml:id="_czKy764">
					<idno type="grant-number">612.066.512</idno>
				</org>
				<org type="funding" xml:id="_uzkaqRB">
					<idno type="grant-number">612.061.814</idno>
				</org>
				<org type="funding" xml:id="_QQqfgqu">
					<idno type="grant-number">612.061.815</idno>
				</org>
				<org type="funding" xml:id="_Bz36yxP">
					<idno type="grant-number">640.004.802</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,338.33,193.44,66.65,12.90;4,316.81,219.12,239.10,8.64;4,326.78,231.07,229.14,8.64;4,326.78,242.85,40.67,8.59" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,539.11,219.12,16.81,8.64;4,326.78,231.07,209.51,8.64">Formal models for expert finding in enterprise corpora</title>
		<author>
			<persName coords=""><forename type="first">References</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,326.78,242.85,36.15,8.59">SIGIR&apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,316.81,262.95,239.10,8.64;4,326.78,274.73,118.52,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,533.22,262.95,22.69,8.64;4,326.78,274.91,57.10,8.64">Bloggers as experts</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,402.14,274.73,39.22,8.59">SIGIR &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,316.81,294.83,239.10,8.64;4,326.78,306.61,115.09,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,471.66,294.83,84.25,8.64;4,326.78,306.79,53.09,8.64">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,398.70,306.61,39.22,8.59">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,316.81,326.72,239.10,8.64;4,326.78,338.49,190.12,8.82" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,524.91,326.72,31.00,8.64;4,326.78,338.67,124.66,8.64">Finding key bloggers, one post at a time</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,316.81,358.60,239.10,8.64;4,326.78,370.37,216.76,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,494.86,358.60,61.05,8.64;4,326.78,370.55,129.33,8.64">Credibility improves topical blog post retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,474.00,370.37,65.60,8.59">HLT/NAACL &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
