<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.20,71.96,313.29,16.59;1,57.60,91.88,494.41,16.59;1,226.08,111.80,157.58,16.59">University of Glasgow at TREC 2008: Experiments in Blog, Enterprise, and Relevance Feedback Tracks with Terrier</title>
				<funder ref="#_67GTYYY">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.92,157.07,33.71,10.76"><forename type="first">Ben</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,179.30,157.07,80.63,10.76"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.52,157.07,50.68,10.76"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.37,157.07,37.73,10.76"><forename type="first">Jie</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.50,157.07,99.27,10.76"><forename type="first">Rodrygo</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
							<email>rodrygo@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.20,71.96,313.29,16.59;1,57.60,91.88,494.41,16.59;1,226.08,111.80,157.58,16.59">University of Glasgow at TREC 2008: Experiments in Blog, Enterprise, and Relevance Feedback Tracks with Terrier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08F94CA017EC6A7ADD7EAE8397FBBE21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In TREC 2008, we participate in the Blog, Enterprise, and Relevance Feedback tracks. In all tracks, we continue the research and development of the Terrier platform<ref type="foot" coords="1,196.68,303.36,2.99,5.38" target="#foot_0">1</ref> centred around extending state-of-the-art weighting models based on the Divergence From Randomness (DFR) framework <ref type="bibr" coords="1,170.33,326.16,13.70,8.07" target="#b25">[26]</ref>. In particular, we investigate two main themes, namely, proximity-based models, and collection and profile enrichment techniques based on several resources.</p><p>In the Blog track, we aim to improve our opinion detection techniques and to integrate various new blog-specific features into our Voting Model <ref type="bibr" coords="1,106.49,378.36,13.70,8.07" target="#b17">[18]</ref>. For the baseline ad-hoc task, we aim to build strongly performing baselines by applying two different techniques. The first one boosts documents in which query terms co-occur in a given window size, and the second one applies query expansion using collection enrichment. Non-English documents are also removed from the retrieved results.</p><p>In the opinion-finding task, we experiment with two main opinion detection approaches. The first one improves our TREC 2007 dictionary-based approach by automatically building an internal opinion dictionary from the collection itself. We measure the opinionated discriminability of each term using an information-theoretic divergence measure based on the relevance assessments of previous years. The second approach is based on the OpinionFinder tool, which identifies subjective sentences in text. In particular, we introduce a novel method to measure the informativeness of query terms occurring in close proximity to subjective sentences.</p><p>In the blog distillation task, we have two research themes. Firstly, we aim to extend our Voting Model with a component to focus on a balanced and neutral retrieval that does not favour prolific bloggers. The Voting Model is based on the intuition that a relevant blogger will post repeatedly around a topic area. By treating each relevant post as a vote for that blog to be relevant, we can infer a ranking of blogs. This approach is based on voting techniques inspired by electoral social choice theory and data fusion <ref type="bibr" coords="1,224.32,618.96,13.70,8.07" target="#b17">[18]</ref>. Neutrality is an important concept in an election -each candidate should have an equal chance of getting elected. Similarly, bloggers should have an equal chance of getting retrieved for a query, regardless of how many posts they have made. With this in mind, we investigate the application of normalisation techniques in this task.</p><p>In our participation in the first Relevance Feedback track, we aim to develop new techniques on top of our DFR query expansion models. First, we expand the query by measuring the divergence of a term's distribution in a relevance set to its distribution in the whole collection using the Kullback-Leibler (KL) divergence measure. This relevance set can be either pseudo, which consists of the top returned documents, or explicit, which consists of the judged relevant documents as in the provided feedback sets B to E.</p><p>Second, we expand the query from surrogates of the documents, instead of all their text content. These document surrogates are created by a low-cost, syntactically-based information processing model, which uses surface-syntactic evidence to automatically identify informative content and to reduce the noise from any textual input. We use the surface-syntactic approach to prune the feedback documents before selecting the query expansion terms, allowing (noisy) terms carried in unusual syntactic structures to be ignored.</p><p>In the Enterprise track, we participate in both the document and the expert search tasks. In keeping with one of the central themes for our TREC 2008 participation, we investigate the application of suitable external evidence in both tasks.</p><p>In the document search task, we investigate how external resources can be used to enhance the retrieval performance through a collection enrichment approach. Our external resources are obtained from the top-ranked results produced by different commercial search engines. Furthermore, we test how the selective application of collection enrichment can provide further improvements.</p><p>In the expert search task, we have two aims. Firstly, to extend and further test our novel Voting Model and, secondly, to use external evidence of expertise. The Voting Model takes into account various sources of evidence of candidate's expertise, by examining the ranking of documents with respect to the query, and inferring votes for candidates to be relevant. The model is expanded to take into account several input rankings of documents. In particular, we use document rankings produced by Web search engines as an external evidence of the expertise of the candidates.</p><p>The remainder of this paper is structured as follows. Section 2 describes the DFR models we use in TREC 2008. Section 3 details our indexing specifications. Section 4 describes our approaches in each of the Blog track post retrieval tasks along with their corresponding evaluation. Section 5 details our participation in the Enterprise document search task, while Section 6 discusses the results of our first participation in the Relevance Feedback track. Sections 7 and 8 cover our participation in the Enterprise track expert search task and on the Blog track blog distillation task, respectively. Finally, Section 9 presents our final remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODELS</head><p>Following from previous years, our research in Terrier cent res on extending the Divergence From Randomness framework (DFR) <ref type="bibr" coords="2,280.20,81.00,9.69,8.07" target="#b0">[1]</ref>. In Section 2.1, we present existing DFR weighting models we experimented with in TREC 2008, while, in Section 2.2, we present our existing DFR model that captures terms dependence and proximity. Section 2.3 presents the Bo1 and KL DFR term weighting models for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Divergence From Randomness Weighting Models</head><p>Document structure (i.e. fields, such as titles) has been shown to be useful when ranking documents. A field-based weighting model is one that considers the occurrences of query terms in different fields. Robertson et al. <ref type="bibr" coords="2,136.27,207.84,14.87,8.07" target="#b30">[31]</ref> observed that the linear combination of scores, which has been the approach mostly used for the combination of fields, is difficult to interpret due to the non-linear relation between the scores and the term frequencies in each of the fields. In addition, Hawking et al. <ref type="bibr" coords="2,157.18,249.72,14.87,8.07" target="#b9">[10]</ref> showed that the length normalisation that should be applied to each field depends on the nature of the field. Zaragoza et al. <ref type="bibr" coords="2,158.30,270.60,14.87,8.07" target="#b35">[36]</ref> introduced a field-based version of BM25, called BM25F, which applies length normalisation and weighting of the fields independently. Macdonald et al. <ref type="bibr" coords="2,250.13,291.47,14.87,8.07" target="#b24">[25]</ref> also introduced Normalisation 2F in the DFR framework for performing independent term frequency normalisation and weighting of fields.</p><p>In this work, we use a field-based model from the DFR framework, namely PL2F. Using the PL2F model, the relevance score of a document d for a query Q is given by:</p><formula xml:id="formula_0" coords="2,57.36,358.43,235.55,24.26">score(d, Q) = X t∈Q qtw • 1 tf n + 1 `tf n • log 2 tf n λ<label>(1)</label></formula><formula xml:id="formula_1" coords="2,127.20,387.67,367.51,11.39">+(λ -tf n) • log 2 e + 0.5 • log 2 (2π • tf n) ẃhere</formula><p>λ is the mean and variance of a Poisson distribution, given by λ = F/N . F is the frequency of the query term t in the whole collection, and N is the number of documents in the whole collection. The query term weight qtw is given by qtf /qtfmax: qtf is the query term frequency, and qtfmax is the maximum query term frequency among all query terms. In PL2F, tf n corresponds to the weighted sum of the normalised term frequencies tf f for each used field f , a technique known as Normalisation 2F <ref type="bibr" coords="2,227.88,478.92,13.90,8.07" target="#b24">[25]</ref>:</p><formula xml:id="formula_2" coords="2,63.00,494.11,220.67,24.79">tf n = X f " w f • tf f • log 2 " 1 + c f • avg l f l f «« , (c f &gt; 0)</formula><p>(2) where tf f is the frequency of term t in field f of document d, l f is the length in tokens of field f in document d, and avg l f is the average length of the field across all documents. c f is a hyperparameter for each field, which controls the term frequency normalisation; the importance of the term occurring in field f is controlled by the weight w f . Note that the classical DFR weighting model PL2 can be generated by using Normalisation 2 instead of Normalisation 2F for tf n in Equation (1) above. Normalisation 2 is given by:</p><formula xml:id="formula_3" coords="2,95.16,630.07,194.27,21.37">tf n = tf • log 2 " 1 + c • avg l l « , (c &gt; 0) (<label>3</label></formula><formula xml:id="formula_4" coords="2,289.43,637.08,3.48,8.07">)</formula><p>where tf is the frequency of term t in the document d, l is the length of the document in tokens, and avg l is the average length of all documents. c is a hyper-parameter that controls the term frequency normalisation with respect to the document length.</p><p>Another weighting model used in our participation in TREC is the InLB model, which is applied in the Blog track opinion-finding task. This model applies the Inverse Document Frequency and Laplace succession for document weighting <ref type="bibr" coords="2,475.46,68.16,9.51,8.07" target="#b0">[1]</ref>, as well as BM25's term-frequency normalisation function <ref type="bibr" coords="2,459.54,78.60,13.70,8.07" target="#b31">[32]</ref>. In InLB, for a given document d and query Q, the relevance score is given by:</p><formula xml:id="formula_5" coords="2,322.56,104.28,233.39,24.26">score(d, Q) = X t∈Q w(d, t) = X t∈Q qtw • tf n tf n + 1 log 2 N + 1 df + 0.5<label>(4)</label></formula><p>where the query term weight qtw is given by qtf /qtfmax, qtf is the query term frequency, and qtfmax is the maximum query term frequency among all query terms. N is the number of documents in the collection, and df is the number of documents containing the query term t. The normalised term frequency tf n is given by BM25's normalisation function <ref type="bibr" coords="2,431.64,190.08,14.87,8.07" target="#b31">[32]</ref> as follows:</p><formula xml:id="formula_6" coords="2,388.08,204.72,167.87,23.66">tf n = tf (1 -b) + b • l avg l (5)</formula><p>where tf is the within-document term frequency, l is the document length, and avg l is the average document length in the whole collection. b is a free parameter. In this paper, we set b to 0.2337 after training on the 100 topics from the TREC 2006 and 2007 Blog track opinion-finding tasks, numbered 851 to 950.</p><p>The last weighting model used in this work is the DPH model, also derived from the DFR framework. Using DPH, the relevance score of a document d for a query Q is given by <ref type="bibr" coords="2,489.71,309.36,9.70,8.07" target="#b1">[2]</ref>:</p><formula xml:id="formula_7" coords="2,316.80,324.44,392.23,41.26">score(d, Q) = X t∈Q qtw(1 -F ) 2 tf + 1 • `tf • log 2 (tf • avg l l N T F ) +0.5 • log 2 (2π • tf • (1 -F ))<label>(6)</label></formula><p>where F is given by tf /l, tf is the within-document frequency, and l is the document length in tokens. avg l is the average document length in the collection, N is the number of documents in the collection, and TF is the term frequency in the collection. Note that DPH is a parameter-free model. All variables in its formula can be directly obtained from the collection statistics. No parameter tuning is required to optimise DPH, and we can rather focus on studying query expansion. qtw is the query term weight and is given by qtf /qtfmax, where qtf is the query term frequency and qtfmax is the maximum query term frequency among all query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Terms Dependence in the Divergence From Randomness Framework</head><p>We believe that taking into account the dependence and proximity of query terms in documents can increase the retrieval effectiveness. To this end, we extend the DFR framework with models for capturing the dependence of query terms in documents. Following <ref type="bibr" coords="2,330.84,552.60,9.51,8.07" target="#b2">[3]</ref>, the models are based on the occurrences of pairs of query terms that appear within a given number of terms of each other in the document. The introduced weighting models assign scores to pairs of query terms, in addition to the single query terms. The score of a document d for a query Q is given as follows:</p><formula xml:id="formula_8" coords="2,327.36,610.75,228.58,21.82">score(d, Q) = X t∈Q qtw • score(d, t) + X p∈Q 2 score(d, p)<label>(7)</label></formula><p>where score(d, t) is the score assigned to a query term t in the document d, p corresponds to a pair of query terms, and Q2 is the set that contains all possible combinations of two query terms.</p><p>In Equation <ref type="bibr" coords="2,362.20,672.00,9.51,8.07" target="#b6">(7)</ref>, P t∈Q qtw • score(d, t) can be estimated by any DFR weighting model, with or without fields. The score(d, p) of a pair of query terms in a document is computed as follows:</p><formula xml:id="formula_9" coords="2,364.44,710.63,191.50,10.38">score(d, p) = -log 2 (Pp1) • (1 -Pp2)<label>(8)</label></formula><p>where Pp1 corresponds to the probability that there is a document in which a pair of query terms p occurs a given number of times. Pp1 can be computed with any randomness model from the DFR framework, such as the Poisson approximation to the Binomial distribution. Pp2 corresponds to the probability of seeing the query term pair once more, after having seen it a given number of times. Pp2 can be computed using any of the after-effect models in the DFR framework. The difference between score(d, p) and score(d, t) is that the former depends on counts of occurrences of the pair of query terms p, while the latter depends on counts of occurrences of the query term t. This year, we applied the pBiL2 randomness model <ref type="bibr" coords="3,251.25,172.68,13.70,8.07" target="#b16">[17]</ref>, which does not consider the collection frequency of pairs of query terms. It is based on the binomial randomness model, and computes the score of a pair of query terms in a document as follows:</p><formula xml:id="formula_10" coords="3,53.76,219.83,240.27,48.30">score(d, p) = 1 pf n + 1 • " -log 2 (avg w -1)! + log 2 pf n! + log 2 (avg w -1 -pf n)! -pf n log 2 (pp)<label>(9)</label></formula><formula xml:id="formula_11" coords="3,163.20,273.00,124.31,11.70">-(avg w -1 -pf n) log 2 (p ′ p )</formula><p>"</p><p>where avg w = T -N(ws-1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>is the average number of windows of size ws tokens in each document in the collection, N is the number of documents and T is the total number of tokens in the collection. pp = 1 avg w-1 , p ′ p = 1 -pp, and pf n is the normalised frequency of the tuple p, as given by Normalisation 2:</p><formula xml:id="formula_12" coords="3,53.76,340.80,239.13,19.41">pf n = pf • log 2 (1 + cp • avg w-1</formula><p>l-ws ). In Normalisation 2, pf is the number of windows of size ws in document d in which the tuple p occurs, l is the length of the document in tokens, and cp &gt; 0 is a hyper-parameter that controls the normalisation applied to the pf frequency with respect to the number of windows in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Term Weighting Models for Query Expansion</head><p>Terrier implements a list of DFR-based term weighting models for query expansion. The basic idea of these term weighting models is to measure the divergence of a term's distribution in a pseudo-relevance set from its distribution in the whole collection. The higher this divergence is, the more likely the term is related to the query topic. Among the term weighting models implemented in Terrier, Bo1 is one of the best-performing ones <ref type="bibr" coords="3,233.19,500.04,9.51,8.07" target="#b0">[1]</ref>.</p><p>The Bo1 term weighting model is based on the Bose-Einstein statistics. Using this model, the weight of a term t in the exp doc top-ranked documents is given by:</p><formula xml:id="formula_13" coords="3,94.92,547.56,197.86,20.85">w(t) = tfx • log 2 1 + Pn Pn + log 2 (1 + Pn)<label>(10)</label></formula><p>where exp doc usually ranges from 3 to 10 <ref type="bibr" coords="3,209.92,575.88,9.51,8.07" target="#b0">[1]</ref>. Another parameter involved in the query expansion mechanism is exp term, the number of terms extracted from the exp doc top-ranked documents. exp term is usually larger than exp doc <ref type="bibr" coords="3,202.92,607.20,9.51,8.07" target="#b0">[1]</ref>. Pn is given by F N , F is the frequency of the term t in the collection, and N is the number of documents in the collection. tfx is the frequency of the query term t in the exp doc top-ranked documents.</p><p>Terrier employs a parameter-free function to determine the query term weight qtw (see Equation (1)), which is given as follows:</p><formula xml:id="formula_14" coords="3,71.04,676.32,221.74,20.90">qtw = qtf qtfmax + w(t) lim F →tfx w(t)<label>(11)</label></formula><p>= Fmax log 2 1 + Pn,max Pn,max</p><formula xml:id="formula_15" coords="3,199.32,706.32,76.31,10.50">+ log 2 (1 + Pn,max)</formula><p>where lim F →tfx w(t) is the upper bound of w(t). Pn,max is given by Fmax/N . Fmax is the frequency F of the term with the maximum w(t) in the top-ranked documents. If a query term does not appear among the most informative terms from the top-ranked documents, its query term weight remains equal to the original one.</p><p>Another term weighting model employed by Terrier is based on the KL divergence measure. Using the KL model, the weight of a term t in the feedback document set D is given by <ref type="bibr" coords="3,499.19,130.92,9.70,8.07" target="#b0">[1]</ref>:</p><formula xml:id="formula_16" coords="3,377.88,146.51,177.94,20.85">w(t) = p(t|D) • log 2 p(t|D) p(t|Coll)<label>(12)</label></formula><p>where p(t|D) = tfx/c(D) is the probability of observing the term t in the feedback document set D, tfx is the frequency of the term t in the set D and c(D) is the number of tokens in this set. p(t|Coll) = T F/c(Coll) is the probability of observing the term t in the whole collection, T F is the frequency of t in the collection, and c(Coll) is the number of tokens in the collection. In our experiments, the feedback document set contains the exp doc topranked documents, from which the exp term most weighted terms by KL are then extracted.</p><p>Using KL, the query term weight qtw is also determined by Equation <ref type="bibr" coords="3,351.48,280.20,13.70,8.07" target="#b10">(11)</ref>, while the upper bound of w(t) is given by: lim</p><formula xml:id="formula_17" coords="3,372.24,296.51,179.87,24.45">F →tfx w(t) = Fmax • log 2 c(Coll) lx lx (<label>13</label></formula><formula xml:id="formula_18" coords="3,552.11,306.48,3.72,8.07">)</formula><p>where Fmax is the collection frequency F of the term with the maximum w(t) in the top-ranked documents, lx is the length of the feedback documents, and c(Coll) is the number of tokens in the whole collection. Note that the DFR query expansion framework is similar to Rocchio's relevance feedback method <ref type="bibr" coords="3,503.96,369.84,13.70,8.07" target="#b32">[33]</ref>. The difference is that the former considers the whole feedback document set as a bag of words, while the latter averages term weights over single feedback documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INDEXING</head><p>In TREC 2008, we participate in the Blog, Enterprise, and Relevance feedback tracks. The test collection for the Blog track is the TREC Blogs06 collection <ref type="bibr" coords="3,425.61,460.32,13.70,8.07" target="#b22">[23]</ref>, which is a crawl of 100k blogs over an 11-week period. During this time, the blog posts (permalinks), feeds (RSS, XML, etc.) and homepages of each blog were collected. In our participation in the Blog track, we index only the permalinks component of the collection. There are approximately 3.2 million documents in the permalinks component. For the Relevance feedback track, the test collection is the large-scale .GOV2 collection, which has an uncompressed size of 426G. For indexing purposes, we treat the above two collections in the same way. Using the Terrier IR platform <ref type="bibr" coords="3,417.40,554.52,13.70,8.07" target="#b25">[26]</ref>, we create content-based indices, including the document content and the titles.</p><p>For the Enterprise track, we use the CSIRO Enterprise Research Collection (CERC), which is a crawl of the csiro.au domain (370k documents). CSIRO is a real Enterprise-sized organisation. To support the field-based weighting models, we index separate fields of the documents, namely the content, the title, and the anchor text of the incoming hyperlinks.</p><p>For all the three collections, each term is stemmed using Porter's English stemmer, and normal English stopwords are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BLOG TRACK: BLOG POST RETRIEVAL TASKS</head><p>Following the TREC guidelines, in the Blog post retrieval tasks, namely, baseline ad-hoc, opinion-finding, and polarity tasks, we submit runs based on all 150 topics developed so far. In this section, however, unless otherwise stated, we report our results on the new topics only, i.e., topics 1001-1050. Additionally, all our runs use only the title of the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Ad-hoc Retrieval Task</head><p>Following the Blog track guidelines, in the baseline ad-hoc retrieval task, we submit two runs solely aimed at retrieving topicrelevant documents, i.e., with no opinion feature enabled. Our first baseline, uogBLProx, applies the InLB document weighting model and the pBiL2 term proximity model, as described in Equations ( <ref type="formula" coords="4,285.95,162.24,3.48,8.07" target="#formula_5">4</ref>) and ( <ref type="formula" coords="4,73.49,172.68,3.17,8.07" target="#formula_10">9</ref>), respectively. In addition, we remove non-English blog posts from the returned results as language filtering was shown to be beneficial in previous Blog tracks <ref type="bibr" coords="4,190.52,193.56,14.12,8.07" target="#b23">[24,</ref><ref type="bibr" coords="4,207.68,193.56,10.59,8.07" target="#b26">27]</ref>. On top of uogBL-Prox, our second baseline, uogBLProxCE, applies the Bo1 term weighting model for query expansion on an external collection, namely, the Aquaint2 collection, a timely news resource.</p><p>Table <ref type="table" coords="4,84.48,235.44,4.48,8.07" target="#tab_0">1</ref> summarises the retrieval performance of our two submitted baseline runs in terms of both topic-relevance (rel) and opinionfinding (op). The median of the participating groups in this task for the 2008 topics, 1001-1050, is also shown. From the table, we can see that our baselines markedly outperform the TREC median performance, both in terms of topic-relevance and opinion-finding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Opinion-Finding Task</head><p>In the opinion-finding task, we experiment with two main approaches for detecting opinionated documents. The first approach improves our TREC 2007 dictionary-based approach by automatically building an internal opinion dictionary from the collection itself. The second approach is based on the OpinionFinder tool, which identifies subjective sentences in text. In particular, we introduce a novel method to measure the informativeness of query terms occurring in a close proximity to subjective sentences.</p><p>In our first opinion detection approach <ref type="bibr" coords="4,202.90,491.52,13.70,8.07" target="#b11">[12]</ref>, a dictionary of subjective terms is automatically derived from the target collection without requiring any manual effort. In particular, from the list of all terms in the collection ranked by their within-collection frequency in descending order, a skewed query model is applied to filter out those that are too frequent or too rare <ref type="bibr" coords="4,228.84,543.84,9.51,8.07" target="#b4">[5]</ref>. This aims to remove terms with too little or too specific information and which thus cannot be interpreted as general opinion indicators for different queries. Using the Bo1 term weighting model (Equation ( <ref type="formula" coords="4,278.37,575.16,7.21,8.07" target="#formula_13">10</ref>)) and a training set comprising the 100 topics for the TREC 2006 and 2007 Blog track opinion-finding tasks, 851-950, the remaining terms from the list are weighted based on the divergence of their distribution in the set D(opRel) of relevant and opinionated documents retrieved for these topics against that in the set D(rel) of relevant documents retrieved for the same set of topics.</p><p>To compare with the dictionary derived from the collection itself, we also manually generate a dictionary compiled from various linguistic resources such as OpinionFinder <ref type="bibr" coords="4,213.90,669.36,13.70,8.07" target="#b34">[35]</ref>. This dictionary contains around 12,000 English words, mostly adjectives, adverbs and nouns, which are supposed to be subjective. In this paper, we denote the manually edited dictionary by the external dictionary, and the automatically derived one by the internal dictionary.</p><p>We submit the 100 highly weighted terms from either dictionary as a query Qopn and assign an opinion score to the retrieved documents using the InLB DFR weighting model (Equation ( <ref type="formula" coords="4,527.33,78.60,3.14,8.07" target="#formula_5">4</ref>)). For each retrieved document d for a given new query Q, we combine its topic-relevance score -given by a retrieval baseline, which is independent of any expressed opinion in the document -with its opinion score to produce the final document ranking. We have experimented with two combination methods. The first method applies the following linear combination: Our second combination method maps each opinion score to the maximum likelihood of the probability P (opn|d, Qopn) of being opinionated as follows:</p><formula xml:id="formula_19" coords="4,320.04,156.23,232.66,19.23">scorecom(d, Q) = (1 -α) • score(d, Qopn) + α • score(d, Q)<label>(14</label></formula><formula xml:id="formula_20" coords="4,357.96,254.75,194.15,27.74">P (opn|d, Qopn) = score(d, Qopn) P d∈Coll score(d, Qopn) (<label>15</label></formula><formula xml:id="formula_21" coords="4,552.11,261.12,3.72,8.07">)</formula><p>where Coll is the entire document collection. Since a high probability is supposed to indicate a high degree of opinion expressed in the document, we would like to have a combined score that is an increasing function of P (opn|d, Qopn). Therefore, such a probability P (opn|d, Qopn) is combined with the initial relevance score using a logarithmic function as follows:</p><formula xml:id="formula_22" coords="4,325.20,354.84,230.62,22.38">scorecom(d, Q) = -k log 2 P (opn|d, Qopn) + score(d, Q) (16)</formula><p>where k is a free parameter, also set by training.</p><p>Both score combination methods use the stored opinion scores of all documents, computed during indexing. Therefore, there is only a negligible additional overhead during retrieval.</p><p>Our second approach in this task <ref type="bibr" coords="4,447.40,423.72,14.87,8.07" target="#b33">[34]</ref> uses OpinionFinder <ref type="bibr" coords="4,538.38,423.72,13.93,8.07" target="#b34">[35]</ref>, a Natural Language Processing-based subjectivity analysis system, to classify the subjectiveness of every sentence in the Blogs06 collection. After the whole collection is parsed, we index it by considering the sentence tags generated by OpinionFinder as special position markers, so that we can record the positions of every index term with respect to the sentences in which it occurs within a given document. We then boost the scores of the retrieved documents, as given by the InLB weighting model (Equation ( <ref type="formula" coords="4,493.65,507.48,3.14,8.07" target="#formula_5">4</ref>)), based on the proximity between the query terms and the subjective sentences identified in each of these documents according to the equation:</p><formula xml:id="formula_23" coords="4,317.16,543.07,238.67,30.56">scorecom(d, Q) = (1 -β) X p∈Q×S score(d, p) + β • score(d, Q)<label>(17)</label></formula><p>where the pair p = t, s comprises a query term t from the query Q and a subjective sentence s from the set S of all subjective sentences identified in document d. In order to compute the proximity score score(d, p), we apply the pBiL randomness model <ref type="bibr" coords="4,528.18,607.44,13.70,8.07" target="#b16">[17]</ref>, as given by Equation ( <ref type="formula" coords="4,387.29,617.88,3.17,8.07" target="#formula_10">9</ref>), except that the proximity windows are measured in terms of number of sentences instead of number of tokens and Normalisation 2 is not applied, since it did not show significant improvements in our experiments. The linear combination parameter β is trained on the 2006 and 2007 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experiments</head><p>In 2008, the TREC Blog track organisers provided five strongly performing, yet statistically different baselines. Each of these comprises a list of retrieved documents produced by a "black box" search engine that retrieves as many topic-relevant documents as possible without applying any specific opinion-finding feature. On top of each of our two baselines, namely, uogBLProx and uogBL-ProxCE, and the 5 standard baselines provided by the Blog track organisers for TREC 2008, we submitted four runs as follows. The first run applies our dictionary-based approach using either the internal or the external opinion dictionary. The second run applies our TREC 2007 OpinionFinder-based approach <ref type="bibr" coords="5,217.17,130.92,13.70,8.07" target="#b10">[11]</ref>, while the third run integrates the new proximity of query terms to subjective sentences approach. The last run combines the proximity to subjective sentences feature with our dictionary-based approach. Table <ref type="table" coords="5,85.32,324.36,4.48,8.07" target="#tab_3">3</ref> summarises the retrieval performance of our submitted runs in terms of topic-relevance (rel) and opinion-finding (op) over the 7 baselines. In this table, an asterisk (*) indicates a significant difference (p ≤ 0.05) from the corresponding baseline run according to the Wilcoxon matched-pairs signed-ranks test. We find that all our 4 approaches provide statistically significant improvement over 5 out of the 7 baselines. As for the remaining baselines, our proximity-based approaches significantly improve over baseline 2, while the approaches that do not employ proximity significantly improve over baseline 4. As a whole, these results show that both of our approaches are effective in finding opinionated documents.</p><p>In order to further investigate the robustness of our approaches, Table <ref type="table" coords="5,75.96,449.88,4.48,8.07" target="#tab_6">4</ref> shows the opinion MAP performances of each of our runs over each of the 5 standard baselines as well as the average performance of these runs across the baselines. For the sake of legibility, the 4 approaches deployed over each baseline are generically referred to as Dict, OpinionFinder, ProxSent, and Dict+ProxSent. For each baseline, we also show the median performance of the 21 TREC runs that were deployed over all the standard baselines. The median of the average improvements of these runs is also shown. From the table, we can see that, besides improving over the baselines, our approaches outperform the TREC medians in most settings. Moreover, on average, all of our techniques provide improvements across the 5 baselines, what further attests their robustness.</p><p>Additionally, Table <ref type="table" coords="5,134.64,575.40,4.48,8.07" target="#tab_8">5</ref> shows the performance results of our best opinion-finding runs for each of the 7 baselines in terms of topicrelevance and opinion-finding MAP across the topics for each of the 3 years the opinion-finding task was run, as well as for the combined topics for the 3 years. From Table <ref type="table" coords="5,198.54,617.28,3.34,8.07" target="#tab_8">5</ref>, we can observe that, for baselines 1, 3, and 4, our opinion-finding performance increases over the three years. Interestingly, in terms of topic-relevance performance, the best results are observed for the TREC 2007 topics across all baselines, which suggests that this set of topics was relatively easier when compared to the 2006 and 2008 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Polarity Task</head><p>In the polarity task, we apply our dictionary-based approach once more, with the exception that the dictionaries used for retrieving  positive and negative blog posts are respectively extracted from the strong positive and negative words in OpinionFinder's dictionary. The negative dictionary comprises a total of 2,547 words while the positive one comprises 1,153 words in total. Table <ref type="table" coords="5,349.32,501.96,4.48,8.07" target="#tab_10">6</ref> shows the performance of our polarity runs in terms of both topic-relevance and opinion-finding across all baselines. Overall, none of our approaches significantly differs from their respective baselines according to the Wilcoxon matched-pairs signedranks test with p ≤ 0.05. In fact, we can observe minor improvements for retrieving negative documents and a slight degradation for retrieving positive documents.</p><p>Analogously to the analysis conducted in the previous section, Table <ref type="table" coords="5,338.88,585.72,4.48,8.07" target="#tab_12">7</ref> shows the negative and positive MAP performances of our deployed approach for the polarity task (DictOF, in the table) over each of the 5 standard baselines as well as the average performances of these runs across the baselines. For each baseline, the median performance of all 10 TREC runs that were deployed over all 5 baselines is also shown. From Table <ref type="table" coords="5,453.26,637.92,3.34,8.07" target="#tab_12">7</ref>, we can see that, although having decreased the baseline performances on average, our approach stands well above the median. These very low median values, in turn, attest the difficulty of this task.</p><p>Overall, our participation in the TREC 2008 Blog track was very successful. Our two submitted baseline runs performed well above the median performance of all participants. In the opinion-finding task, most of our approaches provided improvements over all the  baselines and the TREC median performance. Moreover, all of them showed to be robust, as demonstrated by their average improvement across the standard baselines. Finally, although not providing a significant improvement over the baselines, our polarity approach performed fairly above the median performance of the participants in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ENTERPRISE TRACK: DOCUMENT SEARCH TASK</head><p>In our participation in the Enterprise track document search task, we aim to investigate how external resources, such as the Google and Yahoo! Web search engines, can be used to enhance the retrieval performance through a collection enrichment approach. Furthermore, we test how the selective application of collection enrichment can further improve retrieval effectiveness. Section 5.1 describes the selective application of collection enrichment. Section 5.2 presents our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Selective Application of Collection Enrichment</head><p>For Enterprise document search, query expansion may fail as the Enterprise intranets are often created by a small number of individuals, which lead to the Enterprise collection having limited use of alternative lexical representations. In particular, this could lead to pseudo-relevance sets of poor quality. In this case, it can be beneficial to use collection enrichment, which expands the initial query by taking into account a pseudo-relevance set based on larger and higher-quality external resources <ref type="bibr" coords="6,436.40,453.48,13.70,8.07" target="#b13">[14]</ref>.</p><p>We experiment using five different external resources, namely Wikipedia, Yahoo! Web, Google Web, Google Scholar, and Google Books. The Wikipedia website provides the procedures about how to download the Wikipedia data<ref type="foot" coords="6,430.56,493.44,2.99,5.38" target="#foot_1">2</ref> . For the external resources of Yahoo! Web, Google Web, Google Scholar, and Google Books, we submit queries to each of these search engines and then download their returned results. In particular, we discriminate the Yahoo! Web resource into Yahoo! Web ANY and Yahoo! Web PDF according to the restriction on the type of the retrieved documents for a given query. For example, Yahoo! Web ANY means that we do not apply any restriction on the retrieved documents when we submit a query to the Yahoo! Web search engine, while Yahoo! Web PDF means that we restrict the retrieved documents to be PDF files only. We make the same kind of discrimination for Google Web and Google Scholar as well.</p><p>We hypothesise that not all queries benefit equally from the application of collection enrichment. Therefore, we use query performance predictors to selectively apply collection enrichment on a per-query basis. Various query performance predictors have been studied in <ref type="bibr" coords="6,356.16,662.63,14.87,8.07" target="#b12">[13]</ref> and shown to be useful and low-cost. In our experiment, we use two of these predictors, namely, the γ2 and the Average Inverse Collection Term Frequency (AvICTF) predictors.  The definition of γ2 is given as follows:</p><formula xml:id="formula_24" coords="7,147.96,182.88,144.83,20.85">γ2 = idfmax idfmin<label>(18)</label></formula><p>where idfmax and idfmin are the maximum and minimum idf among the query terms in the query Q, respectively. The idf of each query term t is computed as follows:</p><formula xml:id="formula_25" coords="7,119.40,246.24,173.39,22.38">idf (t) = log 2 (N + 0.5)/Nt log 2 (N + 1)<label>(19)</label></formula><p>where Nt is the number of documents in which the query term t appears and N is the number of documents in the whole collection. The definition of AvICT F is given as follows:</p><formula xml:id="formula_26" coords="7,113.64,310.07,175.43,24.81">AvICT F = log 2 Q Q token coll tf coll ql (<label>20</label></formula><formula xml:id="formula_27" coords="7,289.07,320.40,3.72,8.07">)</formula><p>where ql is the query length, tf coll is the number of occurrences of a query term in the whole collection and token coll is the number of tokens in the whole collection.</p><p>Our decision mechanism is given as follows:</p><p>1. Expand the initial query on the local resource if and only if the prediction score obtained from the local resource is higher than a threshold score and the prediction score obtained from the external resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Expand the initial query on the external resource if and only</head><p>if the prediction score obtained from the external resource is higher than a threshold score and the prediction score obtained from the internal resource.</p><p>3. Disable the expansion on the initial query if and only if the prediction scores obtained from the external and internal resources are all lower than a threshold score.</p><p>In addition, our decision mechanism is summarised in Table <ref type="table" coords="7,280.51,527.27,3.42,8.07" target="#tab_15">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments</head><p>We submitted four runs, all of which apply the PL2F DFR fieldbased weighting model. More details about this weighting model can be found in Section 2.1. Three document fields, namely, body, title, and anchor text of incoming hyperlinks are used. The details of our submitted runs are given below, while Table <ref type="table" coords="7,242.94,600.59,4.48,8.07" target="#tab_14">9</ref> summarises their salient features.</p><p>• Run uogTrEDbl tests how effective the application of query terms proximity in the DFR framework is by using the pBiL2 randomness model. More details about query terms proximity in the DFR framework can be found in Section 2.2.</p><p>• Run uogTrEDQE tests how effective the uniform application of query expansion to all queries is by using the Bo1 term weighting model. More details about the Bo1 term weighting model can be found in Section 2.3.</p><p>• As we hypothesise that not all queries benefit equally from the application of collection enrichment, we propose to use a query performance predictor to selectively apply collection enrichment on a per-query basis. Run uogTrEDSelW investigates how effective the selective application of collection enrichment is. The external resource in this case is Wikipedia, and we use γ2 as our predictor, as it performed better than the AvICT F predictor during our training process.</p><p>• Run uogTrEDSE2 also investigates how effective the selective application of collection enrichment is. In particular, we combine the results from several external resources.  Table <ref type="table" coords="7,349.20,432.36,8.92,8.07" target="#tab_18">10</ref> summarises the results of our official and unofficial runs on the final 63 judged queries. The table shows that the query terms proximity technique makes a marginal improvement on the retrieval performance over the PL2F baseline (ID0 vs. ID1). In addition, we observe that the runs with the application of query expansion or collection enrichment outperform the PL2F baseline (ID0 vs. ID2/ID5). Moreover, the selective application of collection enrichment makes a marked improvement on retrieval performance (ID3 vs. ID2/ID5). In particular, the improvement is significant (p &lt; 0.05) in terms of MAP according to the Wilcoxon matched-pairs signed-ranks test. We also find that it is very important to choose an appropriate external resource (ID6 vs. ID7/ID8) and an appropriate predictor (ID3 vs. ID6) before selectively applying collection enrichment. Finally, we notice that the combination of external resources makes a slight improvement over any single external resource (ID4 vs. ID7/ID8).</p><p>Overall, in the Enterprise track document search task, we have shown that using query performance predictors to selectively apply collection enrichment on a per-query basis can enhance the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELEVANCE FEEDBACK TRACK</head><p>In the first Relevance Feedback track, we aim to test the effectiveness of the DFR query expansion framework in various relevance feedback settings. In addition, we apply a novel technique that expands queries on surrogates of the feedback documents, instead of the raw documents themselves.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Document Surrogates Creation</head><p>We expand the query from document surrogates, instead of all text content in the documents. The document surrogates are created by a low-cost, syntactically-based information processing model, which uses surface-syntactic evidence in order to automatically identify informative content and to reduce the noise from any textual input. We use the surface-syntactic approach to prune the feedback documents before selecting the query expansion terms, allowing (noisy) terms carried in unusual syntactic structures to be ignored.</p><p>We use part-of-speech (POS) n-grams <ref type="bibr" coords="8,208.52,400.20,9.68,8.07" target="#b3">[4,</ref><ref type="bibr" coords="8,222.32,400.20,11.87,8.07" target="#b15">16]</ref> to detect noise in the indexed documents. POS n-grams are n-grams of parts-ofspeech, which are extracted from a POS-tagged sentence in a recurrent and overlapping way. For example, for a sentence ABCDEFG, where parts-of-speech are denoted by the single letters A, B, C, D, E, F, G, and where POS n-grams have length n = 4, the POS ngrams extracted are ABCD, BCDE, CDEF, and DEFG. The order in which the POS n-grams occur in the sentence is ignored. For each sentence, all possible POS n-grams are extracted.</p><p>Our technique is based on the fact that high-frequency POS ngrams correspond mostly to sequences of words that include relatively little noise, whereas low-frequency POS n-grams correspond mostly to sequences of words that include relatively more noise <ref type="bibr" coords="8,278.51,525.72,13.70,8.07" target="#b15">[16]</ref>. The only resources needed are a POS tagger and a collection of documents. This can be any collection of documents of a reasonable size, not necessarily the target collection, i.e., the collection from which we retrieve documents <ref type="bibr" coords="8,160.84,567.60,13.70,8.07" target="#b14">[15]</ref>.</p><p>Our methodology is as follows. We extract POS n-grams from a collection of documents and count their frequency. We refer to these POS n-grams as global POS n-grams. We rank these global POS n-grams according to their frequency in the collection (in decreasing order). We refer to this ranked list as global list. We empirically set a cutoff threshold θ of POS n-gram rank in the global list and we assume that everything below this threshold corresponds to estimated noise (Figure <ref type="figure" coords="8,150.98,651.24,3.23,8.07" target="#fig_1">1</ref>). We then extract POS n-grams from the text we wish to process. For each POS n-gram drawn from the text, we determine its position in the global list. Whenever this rank is below the threshold, we remove the POS n-gram and its corresponding sequence of words from the document, regardless of any other POS n-grams that overlap it. We remove POS n-grams in a uniform way, i.e., by setting θ to the same value for all documents. We use the POS n-grams extracted from WT10G to reduce noise from the index of .GOV2. In particular, we use TreeTagger<ref type="foot" coords="8,433.56,493.32,2.99,5.38" target="#foot_2">3</ref> for the POS tagging of WT10G. The POS n-grams extracted from WT10G provide us with a global list of POS n-grams. Overall, we extract 25,070 POS n-grams from WT10G with θ = 17, 070.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments</head><p>In our participation in the Relevance Feedback track, we apply the DPH model (see Equation ( <ref type="formula" coords="8,432.49,568.80,3.36,8.07" target="#formula_7">6</ref>)) for document ranking, and the KL model (Equations ( <ref type="formula" coords="8,404.35,579.24,7.43,8.07" target="#formula_16">12</ref>) &amp; ( <ref type="formula" coords="8,433.13,579.24,7.15,8.07" target="#formula_17">13</ref>)) for query expansion. Moreover, we take into account the proximity between the original query terms by applying the pBiL randomness model <ref type="bibr" coords="8,494.84,600.12,14.87,8.07" target="#b16">[17]</ref> as given by Equation ( <ref type="formula" coords="8,355.37,610.56,3.17,8.07" target="#formula_10">9</ref>), except that Normalisation 2 is not applied. We use these three parameter-free models for experimentation so that we focus on studying the query expansion aspect.</p><p>As the applied weighting models and the query expansion model are parameter-free, the only parameters that we need to fix are the number of expanded terms (exp term) extracted from the exp doc top-ranked documents. For training purposes, we use the 65 oddnumbered Terabyte track ad-hoc topics for which there are at least 4 relevant documents in the top 50 documents returned by DPH. We scan a wide range of possible values of exp doc and exp term, namely every exp doc value within 2 ≤ exp doc ≤ 10, and every exp term value within 10 ≤ exp term ≤ 100 with an interval of 5, in order to maximise MAP. We obtain exp doc = 3 and exp term = 35, which are used in our submitted runs.</p><p>We submitted two lists of runs for each set of feedback documents as follows. The first list, namely runs uogRF08.{A-E}1, applies query expansion on surrogates of the positive documents for the feedback sets B-E, and on surrogates of the top-3 returned documents for the feedback set A. The second list, namely runs uogRF08.{A-E}2, applies query expansion on positive documents for sets B-E, and on the top-3 returned documents for set A.</p><p>Table <ref type="table" coords="9,86.28,183.12,8.92,8.07" target="#tab_20">11</ref> provides the retrieval performance of our submitted runs as given by three different evaluation measures: Top10 AP, MCT AP, and Stat AP. From Table <ref type="table" coords="9,180.38,204.12,7.41,8.07" target="#tab_20">11</ref>, we conclude the following:</p><p>1. It is beneficial to use the positive feedback documents for query expansion. The retrieval performance for sets B-E is markedly higher than that of set A.</p><p>2. Query expansion on document surrogates has a better retrieval performance in terms of Top10 AP than query expansion on the raw documents.  We have also conducted additional experiments to test the effectiveness of the techniques applied in our participation using the 31 topics for which the top-10 returned documents were judged by assessors. We obtain the following results:</p><p>• First, we evaluate the usefulness of pseudo-query expansion compared with the first-pass retrieval. We obtain MAP 0.1368 for first-pass retrieval, and MAP 0.1982 for pseudo-query expansion using the top-3 returned documents for relevance feedback. Pseudo-query expansion provides a 44.88% statistically significant improvement over the first-pass retrieval, which attests the effectiveness of this technique.</p><p>• Second, we test if the use of positive feedback documents (Pos QE) provides a better retrieval performance than pseudoquery expansion (Pseudo-QE). From Table <ref type="table" coords="9,237.20,630.96,7.41,8.07" target="#tab_21">12</ref>, we can see that positive feedback documents do bring useful information, particularly for the settings which have a relatively larger number of feedback documents (see results for sets D and E).</p><p>• Third, we evaluate if query expansion on document surrogate improves the retrieval performance. Table <ref type="table" coords="9,249.82,690.24,8.92,8.07" target="#tab_22">13</ref> provides the related results. In general, we find no obvious difference between query expansion on the whole documents and query expansion on the document surrogates on the 31 topics used.</p><p>No statistically significant difference is found between their corresponding MAP values.  Overall, we have shown that expanding queries on positive documents is markedly better than pseudo-relevance feedback. We have also shown that it is beneficial to expand the queries over a refined representation of the feedback documents, namely, the document surrogates. In addition, we have investigated the usefulness of negative feedback documents for query expansion based on an adaptation of Rocchio's relevance feedback algorithm <ref type="bibr" coords="9,497.48,397.32,14.87,8.07" target="#b32">[33]</ref> to the DFR query expansion framework. We have not found the negative documents to be useful for relevance feedback, in line with the findings of other participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ENTERPRISE TRACK: EXPERT SEARCH TASK</head><p>We participate in the expert search task of the TREC 2008 Enterprise track with the aim of continuing to test and develop our novel Voting Model <ref type="bibr" coords="9,370.49,501.48,13.70,8.07" target="#b21">[22]</ref>. In the expert search task, systems are asked to rank candidate experts with respect to their predicted expertise about a query, using documentary evidence of expertise found in the collection.</p><p>In our participation in the expert search task this year, we follow our central themes of proximity and enrichment. In particular, we use an advanced proximity extension to the Voting Model, which uses an information-theoretic DFR model to calculate the informativeness of a candidate's name occurring in close proximity to the terms of the query. Moreover, we enrich the profiles of the candidate experts to obtain better evidence of their expertise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Voting Model</head><p>In expert search, the expertise areas of the candidates are represented to the system by documentary evidence of expertise, known as candidate profiles. In our Voting Model for expert search, instead of directly ranking candidates using these profiles, we consider the ranking of documents with respect to the query Q, which we denote R(Q). We propose that the ranking of candidates can be modelled as a voting process from the retrieved documents in R(Q) to the profiles of candidates: every time a document is retrieved and is associated with a candidate, then this is a vote for that candidate to have relevant expertise to Q. The votes for each candidate are then appropriately aggregated to form a ranking of candidates, taking into account the number of voting documents for that candidate, and the relevance score of the voting documents. Our Voting Model is extensible and general, and is not collection or topics dependent.</p><p>In <ref type="bibr" coords="10,74.08,120.36,13.70,8.07" target="#b21">[22]</ref>, we defined twelve voting techniques for aggregating votes for candidates, adapted from existing data fusion techniques. In this work, we apply only the robust and effective expCombMNZ voting technique for ranking candidates. expCombMNZ ranks candidates by considering the sum of the exponential of the relevance scores of the documents associated with each candidate's profile. Moreover, it includes a component which takes into account the number of documents in R(Q) associated to each candidate, hence explicitly modelling the number of votes made by the documents for each candidate. In expCombMNZ, the score of a candidate C's expertise to a query Q is given by:</p><formula xml:id="formula_28" coords="10,92.88,241.91,199.90,34.58">score(C, Q) = |R(Q) ∩ prof ile(C)| • X d ∈ R(Q)∩ prof ile(C) exp(score(d, Q))<label>(21)</label></formula><p>where |R(Q) ∩ prof ile(C)| is the number of documents from the profile of candidate C that are in the ranking R(Q). Some types of documents can have many topic areas and many occurrences of candidate names (e.g., the minutes of a meeting). In such documents, the closer a candidate's name occurrence is to the query terms, the more likely that the document is a high quality indicator of expertise for that candidate <ref type="bibr" coords="10,202.26,349.19,9.68,8.07" target="#b5">[6,</ref><ref type="bibr" coords="10,215.34,349.19,10.59,8.07" target="#b28">29]</ref>. To this end, we define a voting technique, based on expCombMNZ, which takes into account the proximity of each candidate's name occurrence to the query terms in the documents <ref type="bibr" coords="10,175.60,380.51,13.70,8.07" target="#b18">[19]</ref>. We measure this proximity using the DFR term proximity model defined in Section 2.2. This model is designed to measure the informativeness of a pair of query terms occurring in close proximity in a document. We adapt this to the expert search task and into the expCombMNZ voting technique (Equation ( <ref type="formula" coords="10,94.91,432.83,6.71,8.07" target="#formula_28">21</ref>)), by measuring the informativeness of a query term occurring in close proximity to a candidate's name. The adapted voting technique, expCombMNZProx, is given as follows:</p><formula xml:id="formula_29" coords="10,73.44,470.75,219.34,63.14">score(C, Q) = (22) |R(Q) ∩ prof ile(C)| • X d ∈ R(Q)∩ prof ile(C) exp " score(d, Q) + X p=name(C)×t∈Q score(d, p) "</formula><p>Here, p = t, C is a tuple of a term t from the query and the full name of candidate C. score(d, p) can be calculated using any DFR weighting model <ref type="bibr" coords="10,119.04,564.71,13.90,8.07" target="#b16">[17]</ref>; for efficiency reasons, we use the pBiL2 model (Equation ( <ref type="formula" coords="10,120.14,575.15,3.14,8.07" target="#formula_10">9</ref>)), as it does not consider the frequency of the tuple p in the collection but only in the document. Hence, in this way, we are able to use the same weighting model to count and weight candidate occurrences in close proximity to query terms as that used in other TREC tracks (Blog, Relevance Feedback) to weight query term occurrences in close proximity. Note that this approach does not remove evidence of expertise for a candidate where the candidate's name does not occur near a query term, as this may result in a relevant candidate not being retrieved for a difficult query (i.e., the relevant candidate had only sparse evidence of expertise). Instead, candidate with names occurring in close proximity to query terms are given stronger votes in the Voting Model, and hence should be ranked higher in the final ranking. Some voting techniques in the Voting Model can suffer from be-ing biased towards candidates with large candidate profiles (many associated documents). To neutralise this effect, we apply a normalisation function that is called Norm2D <ref type="bibr" coords="10,470.21,78.59,13.90,8.07" target="#b20">[21]</ref>:</p><formula xml:id="formula_30" coords="10,316.80,95.23,242.87,21.38">score(C, Q) = score(C, Q) • log " 1 + cpro • avg l pro |prof ile(C)| « (<label>23</label></formula><formula xml:id="formula_31" coords="10,559.67,102.12,3.72,8.07">)</formula><p>where cpro is a free parameter (cpro &gt; 0), |prof ile(C)| is the size of the profile of candidate C, measured as the number of documents, and avg l pro is the average size of the profile of all candidates. This is inspired by the Normalisation 2 from the DFR framework (Equation ( <ref type="formula" coords="10,378.04,168.36,3.14,8.07" target="#formula_3">3</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Enriching Candidate Profiles</head><p>In keeping with our TREC theme this year, we investigate how enterprise data can be enriched by an external source of evidence. In <ref type="bibr" coords="10,327.51,222.84,13.70,8.07" target="#b29">[30]</ref>, Serdyukov &amp; Hiemstra proposed the use of external evidence in expert search, in particular, by using queries submitted to commercial Web search engines. In this work, we follow their suggestion for identifying useful external evidence. However, we develop more refined methods for ranking the experts. In particular, we actually download and rank all of the expertise evidence derived from a given source.</p><p>In order to identify expertise evidence for the candidates on the Web, we build new queries, which we call "evidence identification queries". These evidence identification queries involve both the actual expert search query (from the TREC 2007 Expert search task), and the name of a candidate. We then submit these evidence identification queries to the APIs of a major Web search engine, which will allow Web documents specific to the query and to the candidate to be retrieved. In particular, each query contains:</p><p>• the quoted full name of the person: e.g., "craig macdonald",</p><p>• the name of the organisation: e.g., csiro,</p><p>• query terms without any quotations: e.g., genetic modification,</p><p>• a directive prohibiting any results from the actual organisation Web site: -site:csiro.au.</p><p>The use of the name of the organisation helps in name disambiguation, to prevent the matching of any content not related to the candidate expert in question. However, this will also prevent the matching of evidence for a candidate from a previous employer. The prohibitive -site directive, in turn, ensures that the acquired expertise evidence does not overlap with the intranet collection.</p><p>For each of the 50 topics in the TREC 2008 expert search task, we submit the evidence identification queries to the Yahoo! Web search engine, for the top 100 candidates suggested by our baseline expert search engine. From the search listing results, we extract a list of URLs associated to each candidate. For each expert identification query, a maximum of 24 results are extracted, and the corresponding Web pages downloaded. These pages form the profiles of the candidates. Note that these new profiles are query-biased, as only documents which are related to query topic(s) are associated to each candidate.</p><p>To create the runs, we rank all downloaded documents which have been returned by the Yahoo! Web search engine using the PL2 document weighting model (Equations ( <ref type="formula" coords="10,484.83,690.24,3.48,8.07" target="#formula_0">1</ref>) &amp; (3)). Then, a voting technique is applied to convert this ranking of documents into a ranking of candidates using the query-biased profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Experiments Setup, Runs, and Results</head><p>We submitted four runs to the expert search task of the Enterprise track. Along with the unsubmitted baselines, these are:</p><p>• uogTrEXFeMNZ is our baseline run (unsubmitted). It applies the PL2F DFR document weighting model (see Equations ( <ref type="formula" coords="11,100.19,119.40,3.48,8.07" target="#formula_0">1</ref>) &amp; ( <ref type="formula" coords="11,123.59,119.40,3.36,8.07">2</ref>)) to generate the underlying ranking of documents from the CERC collection, combined with the exp-CombMNZ (Equation ( <ref type="formula" coords="11,160.83,140.27,7.15,8.07" target="#formula_28">21</ref>)) voting technique to rank experts.</p><p>• uogTrEXFeMNZP is a baseline run (unsubmitted), which improves upon the baseline run by applying query-term proximity (Equation ( <ref type="formula" coords="11,138.35,177.95,3.36,8.07" target="#formula_10">9</ref>)) before expCombmMNZ.</p><p>• uogTrEXfeNP improves upon uogTrEXFeMNZP by applying candidate size normalisation on top of expCombMNZ.</p><p>• uogTrEXfePC improves upon the baseline run by applying candidate-query term proximity, expCombMNZProx (Equation ( <ref type="formula" coords="11,95.75,242.87,6.71,8.07">22</ref>)), instead of expCombMNZ.</p><p>• uogTrEXfeNPC combines the previous two runs, by applying expCombMNZProx, and candidate size normalisation.</p><p>• uogTrEXeY is a baseline run (unsubmitted), which applies the PL2 DFR document weighting model on the externally obtained Yahoo! document index.</p><p>• uogTrEXmix combines uogTrEXfeNPC and uogTrEXeY by a linear mixture combination.</p><p>The salient features of the runs are described in Table <ref type="table" coords="11,260.00,352.43,7.41,8.07" target="#tab_25">14</ref>. Note that, for all runs using the CERC collection, we use the candidate profiles identified during our TREC 2007 participation <ref type="bibr" coords="11,251.41,373.31,9.51,8.07" target="#b8">[9]</ref>. Table <ref type="table" coords="11,84.48,383.75,8.92,8.07" target="#tab_23">15</ref> presents the retrieval performance of the runs described above, as well as the per-topic median and best runs from all TREC participants. From the results, we draw the following observations: all of our submitted runs were above median; our best performing run was uogTrEXfeNPC, which applied expCombMNZProx and normalisation; applying query term proximity did not appear to benefit retrieval performance for MAP, but did improve MRR (uogEXFeMNZ vs. uogEXFeMNZP); candidate normalisation improved retrieval performance (uogEXFeMNZP vs. uogTrEXfeNP and uogTrEXfePC vs. uogTrEXfeNPC); candidate-query term proximity was more effective than query-term proximity, or the baseline (uogTrEXfePC vs. uogEXFeMNZ and uogEXFeMNZP), while applying normalisation on top was more beneficial (uogTrEXfePC vs. uogTrEXfeNPC); lastly, the usefulness of Yahoo! for expertise evidence mining was disappointing (uogTrEXeY), and hindered retrieval performance when combined with uogTrEXfeNPC. Overall, we conclude that we successfully participated in the TREC 2008 expert search task of the Enterprise track. All of our submitted runs were above median, and our normalisation and candidate query-term proximity features were successful at increasing baseline retrieval performance. The low performance of the Webenriched candidate profiles requires further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">BLOG TRACK: BLOG DISTILLATION TASK</head><p>In TREC 2008, we also participate in the blog distillation task of the Blog track, where we aim to test the applicability of our Voting Model <ref type="bibr" coords="11,343.49,159.72,14.87,8.07" target="#b21">[22]</ref> to this task. Firstly, in the blog distillation task, the aim of each system is to identify the blogs that have a principle recurring interest in the query topic <ref type="bibr" coords="11,446.74,180.72,13.70,8.07" target="#b23">[24]</ref>. We believe that this task can be seen as a voting process: a blogger with an interest in a topic will blog regularly about the topic, and these blog posts will be retrieved in response to a query topic. Each time a blog post is retrieved about a query topic, that can be seen as a vote for that blog to have an interest in the topic area. In <ref type="bibr" coords="11,474.55,233.04,10.43,8.07" target="#b8">[9]</ref> &amp; <ref type="bibr" coords="11,496.51,233.04,13.70,8.07" target="#b19">[20]</ref>, we showed that the task can be successfully modelled using the Voting Model. With this in mind, many of the techniques we apply in this task are described in Section 7 above: for each candidate expert C, read blog; for each document d, read blog post. The set of posts of each blog forms the blog's "candidate profile".</p><p>We also investigate the use of a feature which ascertains if the retrieved posts in a given blog for a topic are spread across the time span of the collection. If a blogger has an interest in a topic area, it is likely that he or she will continue to blog about the topic area repeatedly and frequently. Indeed, the definition for a relevant blog in the blog distillation task gives a clue that the timing of on-topic posts by a blog may have an impact on the overall relevance of the blog. In particular, we believe that a relevant blog will continue to post relevant posts throughout the timescale of the collection.</p><p>With this in mind, we break the 11-week period of the Blogs06 collection into a series of DI equal intervals (where DI is a parameter). Then, for each blog, we measure the proportion of its posts in each time interval that were retrieved in response to a query. We call this evidence recurring interests (Dates), and define a QscoreDates(C, Q) for each blog C as follows <ref type="bibr" coords="11,513.12,442.20,13.90,8.07" target="#b19">[20]</ref>:</p><formula xml:id="formula_32" coords="11,343.56,457.55,208.55,8.97">QscoreDates(C, Q) = (<label>24</label></formula><formula xml:id="formula_33" coords="11,361.44,458.16,194.38,40.22">) DI X i=1 1 + |R(Q) ∩ dateIntervali(posts(C))| 1 + |dateIntervali(posts(C))|</formula><p>where dateIntervali(posts(C)) is the number of posts of blog C in the ith date interval. Note that we smooth this probability distribution using Laplace smoothing to combat sparsity problems (e.g., when a blog had no posts in a date interval). We integrate the QscoreDates(B, Q) evidence as:</p><formula xml:id="formula_34" coords="11,329.04,560.63,226.79,10.77">score(C, Q) = score(C, Q) × QscoreDates(C, Q) ω<label>(25)</label></formula><p>where ω &gt; 0 is a free parameter. We use DI = 3, which approximates the month where the post was made (the corpus time span is 11 weeks), and ω = 0.48. Initial experiments found that using higher values for DI does not change the results, due to the time span of the corpus. Finally, note that, as this evidence requires knowledge of the ranking of posts for a query, it has to be calculated during the retrieval phase, but without adding high overheads.</p><p>We submitted 4 runs to the blog distillation task of the TREC 2008 Blog Track, which test our hypotheses for this task. Below, we describe our submitted and unsubmitted runs:</p><p>• uogTrBDfe is our baseline run. It uses the PL2F weighting model together with the expCombMNZ voting technique to score the predicted relevance of blogs to the query topic.  • uogTrBDfeN is an unsubmitted baseline run. In addition to PL2F and expCombMNZ, it applies Norm2D to remove any bias towards prolific bloggers.</p><p>• uogTrBDfeNP improves on the baseline run, in two ways: Firstly, by boosting the rank of documents in the document ranking where the query terms occur in close proximity using the pBiL2 DFR terms dependence model (Equation ( <ref type="formula" coords="12,263.73,237.36,3.19,8.07" target="#formula_10">9</ref>)); secondly, we use the Norm2D normalisation technique (Equation ( <ref type="formula" coords="12,95.27,258.35,6.71,8.07" target="#formula_30">23</ref>)), which has been shown to be useful on this task <ref type="bibr" coords="12,278.63,258.35,13.70,8.07" target="#b19">[20]</ref>.</p><p>• uogTrBDfeNPD investigates the use of the recurring interests quality evidence, compared to uogTrBDfeNP.</p><p>• uogTrBDfeNWD investigates the use of Wikipedia for collection enrichment, compared to uogTrBDfeNP, similarly to our collection enrichment approach in the document search task (Section 5). In particular, to enrich the topics, we apply the Bo1 term weighting model (Equation ( <ref type="formula" coords="12,240.40,345.83,7.15,8.07" target="#formula_13">10</ref>)) on the full content of a Wikipedia index from 2008, and using a pseudorelevance set of size exp doc = 30 documents, we expand each query with exp term = 10 additional terms.</p><p>Table <ref type="table" coords="12,85.80,396.84,8.92,8.07" target="#tab_27">16</ref> summarises the salient features of the submitted and unsubmitted runs. Table <ref type="table" coords="12,146.90,407.28,8.92,8.07" target="#tab_28">17</ref> presents the results of our submitted runs in the blog distillation task of the Blog track. The evaluation measures in this task are mean average precision (MAP), mean reciprocal rank (MRR), and precision at rank 10 (P@10).    <ref type="bibr" coords="12,73.70,648.12,4.98,8.07">)</ref>, and precision at 10 (P@10) of our Blog track blog distillation task runs, as well as that achieved by all participants. MRR and P@10 achieved by all participants is not available.</p><p>From the results, we note that applying any of Norm2D, pBiL2 (proximity), Dates or Wikipedia-based collection enrichment results in an increase in retrieval effectiveness in comparison to the baseline run. Moreover, the incremental combination of these techniques brings further improvements, suggesting that each of them may address a different dimension of the blog distillation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSIONS</head><p>In TREC 2008, we have participated in three tracks, namely the Blog track, the Enterprise track, and the Relevance Feedback track. In particular, we have investigated the effectiveness of our proximity-based models in different tasks as well as the usefullness of external resources for collection and profile enrichment.</p><p>In the Blog track opinion-finding task, our main conclusion is that our proposed approaches have significantly improved over all but one of the 7 baselines (our proximity-based approaches do not improve over baseline 4, while the approaches that do not use proximity do not improve over baseline 2). Moreover, the opinionfinding performance of our best-performing runs for all but two baselines has increased across the topics for the three years of the Blog opinion-finding task, with our best topic-relevance performances observed for the 2007 topics over all baselines.</p><p>In the Blog track blog distillation task, we have shown that all of our approaches individually improve over the baseline and also over the median of the participating groups. Additionally, the combination of these individual approaches improves even further.</p><p>In our participation in the first Relevance Feedback track, we have shown that query expansion on the set of positive feedback documents markedly improves over the first-pass retrieval baseline. Furthermore, the application of query expansion on the document surrogates rather than the raw documents has shown improved performance in terms of Top10 AP.</p><p>For the Enterprise track, we have investigated the application of suitable external resources. In the document search task, we have shown that using external resources through collection enrichment to enhance the retrieval performance is very effective. In particular, the selective application of collection enrichment according to the query performance makes a significant improvement in MAP. Moreover, it is very important to choose an appropriate external resource and an appropriate query performance predictor before applying the collection enrichment approach.</p><p>In the Enterprise track expert search task, we have successfully applied our Voting Model to rank candidate experts. Moreover, we have investigated the application of candidate size normalisation, candidate query-term proximity, and the enrichment of candidate profiles using a commercial Web search engine. Both candidate size normalisation and candidate-query term proximity have been shown to improve the retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,552.11,167.40,3.72,8.07;4,316.80,177.23,240.39,8.97;4,316.80,187.67,239.07,8.97;4,316.80,198.72,239.08,8.07;4,316.80,209.16,127.54,8.07"><head></head><label></label><figDesc>) where score(d, Qopn) and score(d, Q) are scaled by dividing them by the maximum score(d, Qopn) and score(d, Q), respectively. α is the free parameter of the linear combination, set after training on the 100 topics from 2006 and 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,350.04,433.56,172.76,8.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: POS n-grams ranked by frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,53.76,308.00,239.23,65.71"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,73.08,308.00,200.04,34.89"><row><cell>Run</cell><cell cols="4">MAP rel P@10 rel MAPop P@10op</cell></row><row><cell>TREC median</cell><cell>0.3529</cell><cell>0.6960</cell><cell>0.2890</cell><cell>0.5700</cell></row><row><cell>uogBLProx</cell><cell>0.4141</cell><cell>0.6840</cell><cell>0.3464</cell><cell>0.5820</cell></row><row><cell cols="2">uogBLProxCE 0.4219</cell><cell>0.7060</cell><cell>0.3531</cell><cell>0.6100</cell></row></table><note coords="4,90.00,355.20,202.99,8.07;4,53.76,365.64,124.04,8.07"><p>Results of submitted runs in the baseline ad-hoc retrieval task for topics 1001-1050.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.76,162.24,239.20,149.43"><head>Table 2 : Techniques applied in the submitted runs in the Blog track opinion-finding task.</head><label>2</label><figDesc>Table 2 describes the nomenclature used by our submitted runs.</figDesc><table coords="5,61.32,192.44,223.78,88.29"><row><cell>Code</cell><cell>Techniques</cell></row><row><cell>OPb1</cell><cell>Run based on baseline uogBLProx</cell></row><row><cell>OPb2</cell><cell>Run based on baseline uogBLProxCE</cell></row><row><cell cols="2">OP1-5 Runs based on standard baselines 1-5</cell></row><row><cell>ext</cell><cell>Dictionary-based approach (external dictionary)</cell></row><row><cell>int</cell><cell>Dictionary-based approach (internal dictionary)</cell></row><row><cell>of</cell><cell>OpinionFinder-based approach</cell></row><row><cell>Pr</cell><cell>Proximity to OpinionFinder's classified subjective sentences</cell></row><row><cell>L</cell><cell>Logarithmic combination</cell></row><row><cell>l</cell><cell>Linear combination</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,316.80,393.96,239.10,29.37"><head>Table 3 : Results of submitted runs in the opinion-finding task over 7 different baselines for topics 1001-1050. An asterisk</head><label>3</label><figDesc></figDesc><table /><note coords="5,316.80,414.36,205.94,8.97"><p>(*) indicates a significant difference (p ≤ 0.05) from</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,316.80,414.96,239.14,28.95"><head>the cor- responding baseline run according to the Wilcoxon matched- pairs signed-ranks test.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,53.76,128.76,502.29,18.81"><head>Table 4 : Opinion MAP over 5 standard baselines and average improvement for topics 1001-1050. An asterisk (*) indicates a signifi- cant difference</head><label>4</label><figDesc></figDesc><table /><note coords="6,112.92,138.60,60.27,8.97"><p>(p ≤ 0.05) from</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,131.64,139.20,391.87,165.05"><head>the corresponding baseline run according to the Wilcoxon matched-pairs signed-ranks test.</head><label></label><figDesc></figDesc><table coords="6,131.64,159.32,346.34,144.93"><row><cell></cell><cell cols="2">2006 (851-900)</cell><cell cols="2">2007 (901-950)</cell><cell cols="2">2008 (1001-1050)</cell><cell cols="2">All 150 topics</cell></row><row><cell>Run</cell><cell cols="8">MAP rel MAPop MAP rel MAPop MAP rel MAPop MAP rel MAPop</cell></row><row><cell>uogBLProx</cell><cell>0.3366</cell><cell>0.2224</cell><cell>0.4373</cell><cell>0.3265</cell><cell>0.4141</cell><cell>0.3464</cell><cell>0.3960</cell><cell>0.2984</cell></row><row><cell>uogOPb1ofL</cell><cell>0.3307</cell><cell cols="2">0.2341* 0.4375</cell><cell cols="2">0.3520* 0.4281*</cell><cell cols="2">0.3665* 0.3988</cell><cell>0.3175*</cell></row><row><cell cols="2">uogBLProxCE 0.3459</cell><cell>0.2351</cell><cell>0.4507</cell><cell>0.3393</cell><cell>0.4219</cell><cell>0.3531</cell><cell>0.4062</cell><cell>0.3091</cell></row><row><cell>uogOPb2ofL</cell><cell>0.3449</cell><cell cols="2">0.2428* 0.4529</cell><cell cols="2">0.3584* 0.4342*</cell><cell cols="2">0.3709* 0.4106*</cell><cell>0.3240*</cell></row><row><cell>baseline 1</cell><cell>0.3004</cell><cell>0.1905</cell><cell>0.4043</cell><cell>0.2758</cell><cell>0.4032</cell><cell>0.3239</cell><cell>0.3693</cell><cell>0.2634</cell></row><row><cell cols="2">uogOP1PrintL 0.3326*</cell><cell cols="2">0.2595* 0.4609*</cell><cell cols="2">0.3513* 0.4120*</cell><cell cols="2">0.3564* 0.4019*</cell><cell>0.3224*</cell></row><row><cell>baseline 2</cell><cell>0.3156</cell><cell>0.2296</cell><cell>0.3881</cell><cell>0.3034</cell><cell>0.3107</cell><cell>0.2639</cell><cell>0.3381</cell><cell>0.2656</cell></row><row><cell cols="2">uogOP2PrintL 0.3082</cell><cell cols="2">0.2410* 0.4069*</cell><cell cols="2">0.3415* 0.3045</cell><cell cols="2">0.2692* 0.3399*</cell><cell>0.2839*</cell></row><row><cell>baseline 3</cell><cell>0.3768</cell><cell>0.2545</cell><cell>0.4619</cell><cell>0.3489</cell><cell>0.4343</cell><cell>0.3564</cell><cell>0.4244</cell><cell>0.3199</cell></row><row><cell>uogOP3ofL</cell><cell>0.3769</cell><cell cols="2">0.2705* 0.4657</cell><cell cols="2">0.3665* 0.4419*</cell><cell cols="2">0.3728* 0.4282*</cell><cell>0.3366*</cell></row><row><cell>baseline 4</cell><cell>0.4300</cell><cell>0.3022</cell><cell>0.5303</cell><cell>0.3784</cell><cell>0.4724</cell><cell>0.3822</cell><cell>0.4776</cell><cell>0.3543</cell></row><row><cell>uogOP4intL</cell><cell>0.4240</cell><cell cols="2">0.3134* 0.5428*</cell><cell cols="2">0.3959* 0.4750</cell><cell cols="2">0.3964* 0.4806</cell><cell>0.3686*</cell></row><row><cell>baseline 5</cell><cell>0.4046</cell><cell>0.2632</cell><cell>0.5465</cell><cell>0.3805</cell><cell>0.3745</cell><cell>0.2988</cell><cell>0.4419</cell><cell>0.3141</cell></row><row><cell cols="2">uogOP5PrintL 0.4138</cell><cell cols="2">0.3012* 0.5510</cell><cell cols="2">0.4104* 0.3915*</cell><cell cols="2">0.3345* 0.4521*</cell><cell>0.3487*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,53.76,316.56,502.11,18.93"><head>Table 5 : Results of our best submitted runs for each baseline on the 2008 topics across 4 sets of topics. An asterisk (*) indicates a significant difference</head><label>5</label><figDesc></figDesc><table /><note coords="6,135.84,326.52,60.27,8.97"><p>(p ≤ 0.05) from</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="6,67.20,327.12,479.23,162.53"><head>the corresponding baseline run according to the Wilcoxon matched-pairs signed-ranks test.</head><label></label><figDesc></figDesc><table coords="6,67.20,354.20,211.81,135.45"><row><cell>Run</cell><cell cols="4">MAPneg P@10neg MAPpos P@10pos</cell></row><row><cell>uogBLProx</cell><cell>0.1218</cell><cell>0.1320</cell><cell>0.1376</cell><cell>0.1800</cell></row><row><cell>uogPLb11</cell><cell>0.1225</cell><cell>0.1440</cell><cell>0.0866</cell><cell>0.1260</cell></row><row><cell cols="2">uogBLProxCE 0.1176</cell><cell>0.1400</cell><cell>0.1388</cell><cell>0.1760</cell></row><row><cell>uogPLb21</cell><cell>0.1176</cell><cell>0.1420</cell><cell>0.1372</cell><cell>0.1700</cell></row><row><cell>baseline1</cell><cell>0.1175</cell><cell>0.1700</cell><cell>0.1364</cell><cell>0.1860</cell></row><row><cell>uogPL11</cell><cell>0.1076</cell><cell>0.1400</cell><cell>0.1272</cell><cell>0.1820</cell></row><row><cell>baseline2</cell><cell>0.0865</cell><cell>0.1420</cell><cell>0.0951</cell><cell>0.1400</cell></row><row><cell>uogPL21</cell><cell>0.0867</cell><cell>0.1420</cell><cell>0.0942</cell><cell>0.1340</cell></row><row><cell>baseline3</cell><cell>0.1266</cell><cell>0.1520</cell><cell>0.1376</cell><cell>0.1680</cell></row><row><cell>uogPL31</cell><cell>0.1203</cell><cell>0.1440</cell><cell>0.1345</cell><cell>0.1800</cell></row><row><cell>baseline4</cell><cell>0.1288</cell><cell>0.1600</cell><cell>0.1532</cell><cell>0.1980</cell></row><row><cell>uogPL41</cell><cell>0.1301</cell><cell>0.1580</cell><cell>0.1394</cell><cell>0.1700</cell></row><row><cell>baseline5</cell><cell>0.1085</cell><cell>0.1680</cell><cell>0.1229</cell><cell>0.1780</cell></row><row><cell>uogPL51</cell><cell>0.1067</cell><cell>0.1700</cell><cell>0.1179</cell><cell>0.1440</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="6,53.76,502.08,239.16,18.51"><head>Table 6 : Results of submitted runs in the polarity task for topics 1001-1050 over 7 different baselines.</head><label>6</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="7,95.04,138.48,419.35,8.07"><head>Table 7 : Negative and positive MAP over 5 standard baselines and average improvement for topics 1001-1050.</head><label>7</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="7,336.48,281.39,219.30,104.22"><head></head><label></label><figDesc>After training, we chose to combine the Yahoo! Web ANY and Yahoo! Web PDF external resources and the AvICT F predictor.</figDesc><table coords="7,336.48,342.20,199.39,43.41"><row><cell>Run</cell><cell>Techniques</cell></row><row><cell>uogTrEDbl</cell><cell>PL2F + proximity</cell></row><row><cell>uogTrEDQE</cell><cell>PL2F + query expansion</cell></row><row><cell cols="2">uogTrEDSelW PL2F + selective CE on a single resource</cell></row><row><cell>uogTrEDSE2</cell><cell>PL2F + selective CE on combined resources</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="7,316.80,398.04,239.14,18.51"><head>Table 9 : Techniques applied in the submitted runs in the En- terprise track document search task.</head><label>9</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="8,53.76,54.87,445.81,55.37"><head>Table 8 : The decision mechanism of the selective application of collection enrichment</head><label>8</label><figDesc></figDesc><table coords="8,202.56,54.87,204.29,34.58"><row><cell>score L &gt; T</cell><cell cols="3">score E &gt; T score L &gt; score E Decision</cell></row><row><cell>True</cell><cell>True or False</cell><cell>True</cell><cell>local</cell></row><row><cell>True or False</cell><cell>True</cell><cell>False</cell><cell>external</cell></row><row><cell>False</cell><cell>False</cell><cell>True or False</cell><cell>disabled</cell></row></table><note coords="8,380.96,101.28,118.61,8.97"><p>. score L and score E denote</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="8,53.76,101.88,502.26,29.25"><head>the prediction scores on the local and external resources, respectively. T is a threshold score, which needs an appropriate setting. We used</head><label></label><figDesc></figDesc><table /><note coords="8,522.71,111.72,33.18,8.97;8,53.76,122.16,196.86,8.97"><p>T = 0 in the submitted runs. local, external and disabled in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="8,53.76,122.16,502.15,128.81"><head>the column Decision indicate expanding the initial query on the local resource, external resource and disabling the expansion, respectively.</head><label></label><figDesc></figDesc><table coords="8,163.56,153.44,282.62,97.53"><row><cell cols="2">ID Run</cell><cell>Technique</cell><cell>Predictor</cell><cell>MAP</cell><cell>NDCG</cell></row><row><cell>0</cell><cell>-</cell><cell>PL2F</cell><cell>-</cell><cell cols="2">0.3590 0.5454</cell></row><row><cell>1</cell><cell>uogTrEDbl</cell><cell>+ Proximity</cell><cell>-</cell><cell cols="2">0.3623 0.5489</cell></row><row><cell>2</cell><cell>uogTrEDQE</cell><cell>+Query Expansion</cell><cell>-</cell><cell cols="2">0.3718 0.5568</cell></row><row><cell>3</cell><cell cols="2">uogTrEDSelW + Selective CE (wiki)</cell><cell>γ1</cell><cell cols="2">0.3891 0.5660</cell></row><row><cell>4</cell><cell>uogTrEDSE2</cell><cell>+ Selective CE (YWA + YWP)</cell><cell>AvICTF</cell><cell cols="2">0.3658 0.5587</cell></row><row><cell>0</cell><cell>-</cell><cell>PL2F</cell><cell>-</cell><cell cols="2">0.3590 0.5454</cell></row><row><cell>5</cell><cell>-</cell><cell>+ CE (wiki)</cell><cell>-</cell><cell cols="2">0.3648 0.5638</cell></row><row><cell>6</cell><cell>-</cell><cell>+ Selective CE (wiki)</cell><cell>AvICTF</cell><cell cols="2">0.3677 0.5579</cell></row><row><cell>7</cell><cell>-</cell><cell>+ Selective CE (YWA)</cell><cell>AvICTF</cell><cell cols="2">0.3576 0.5418</cell></row><row><cell>8</cell><cell>-</cell><cell>+ Selective CE (YWP)</cell><cell>AvICTF</cell><cell cols="2">0.3650 0.5534</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="8,53.76,263.40,502.01,18.51"><head>Table 10 : The results of our official and unofficial runs in the Enterprise track document search task. The highest value in each column is highlighted in bold.</head><label>10</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="9,53.76,447.60,239.22,18.51"><head>Table 11 : Results of submitted runs in the Relevance Feedback track.</head><label>11</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="9,316.80,109.28,239.18,170.13"><head>Table 12 : The MAP values obtained by (pseudo) query expan- sion (QE), and by positive query expansion (Pos QE). A statis- tically significant difference between the two MAP values at the 0.05 confidence level is marked with a star.</head><label>12</label><figDesc></figDesc><table coords="9,373.80,109.28,125.06,170.13"><row><cell cols="3">Set Pseudo-QE Pos QE</cell><cell>diff.</cell></row><row><cell>B</cell><cell>0.1982</cell><cell>0.2153</cell><cell>8.63</cell></row><row><cell>C</cell><cell>0.1982</cell><cell>0.2240</cell><cell>13.02</cell></row><row><cell>D</cell><cell>0.1982</cell><cell>0.2330</cell><cell>17.56*</cell></row><row><cell>E</cell><cell>0.1982</cell><cell>0.2343</cell><cell>18.21*</cell></row><row><cell cols="3">Set QE/Pos QE Sur QE</cell><cell>diff.</cell></row><row><cell>A</cell><cell>0.1982</cell><cell>0.2001</cell><cell>0.96</cell></row><row><cell>B</cell><cell>0.2153</cell><cell>0.2187</cell><cell>1.58</cell></row><row><cell>C</cell><cell>0.2240</cell><cell>0.2275</cell><cell>1.56</cell></row><row><cell>D</cell><cell>0.2330</cell><cell>0.2376</cell><cell>1.97</cell></row><row><cell>E</cell><cell>0.2343</cell><cell cols="2">0.2272 -3.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="9,316.80,291.72,239.21,29.07"><head>Table 13 : The MAP values obtained by pseudo/positive query expansion (QE/Pos QE), and by query expansion on document surrogates (Sur QE).</head><label>13</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="11,53.76,560.00,239.19,130.03"><head>Table 15 : Retrieval performances of the our Enterprise track expert search task runs, and also the TREC per-topic best and median runs.</head><label>15</label><figDesc></figDesc><table coords="11,76.20,560.00,194.18,88.77"><row><cell>Run Name</cell><cell>Submitted</cell><cell>MAP</cell><cell>MRR</cell><cell>P@10</cell></row><row><cell>TREC best</cell><cell>-</cell><cell cols="2">0.6844 0.9909</cell><cell>-</cell></row><row><cell>TREC median</cell><cell>-</cell><cell cols="2">0.3491 0.7829</cell><cell>-</cell></row><row><cell>uogEXFeMNZ</cell><cell>"</cell><cell cols="3">0.3484 0.6550 0.3130</cell></row><row><cell>uogEXFeMNZP</cell><cell>"</cell><cell cols="3">0.3444 0.7072 0.3148</cell></row><row><cell>uogTrEXfeNP</cell><cell></cell><cell cols="3">0.3535 0.7079 0.3218</cell></row><row><cell>uogTrEXfePC</cell><cell></cell><cell cols="3">0.3969 0.7259 0.3636</cell></row><row><cell>uogTrEXfeNPC</cell><cell></cell><cell cols="3">0.4126 0.7611 0.3727</cell></row><row><cell>uogTrEXeY</cell><cell>"</cell><cell cols="3">0.2428 0.5868 0.2436</cell></row><row><cell>uogTrEXmix</cell><cell></cell><cell cols="3">0.3748 0.7600 0.3473</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25" coords="12,163.80,137.76,282.19,8.07"><head>Table 14 : Salient features of our Enterprise track expert search task runs.</head><label>14</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27" coords="12,53.76,523.68,239.22,101.57"><head>Table 16 : Salient features of our Blog track blog distillation task submitted and unsubmitted runs.</head><label>16</label><figDesc></figDesc><table coords="12,96.60,563.48,153.50,61.77"><row><cell>Run Name</cell><cell>MAP</cell><cell>MRR</cell><cell>P@10</cell></row><row><cell>TREC median</cell><cell>0.2224</cell><cell>-</cell><cell>-</cell></row><row><cell>uogTrBDfe</cell><cell cols="3">0.2028 0.6595 0.3560</cell></row><row><cell>uogTrBDfeN</cell><cell cols="3">0.2337 0.6948 0.3720</cell></row><row><cell>uogTrBDfeNP</cell><cell cols="3">0.2395 0.7267 0.3800</cell></row><row><cell>uogTrBDfeNPD</cell><cell cols="3">0.2437 0.7310 0.3740</cell></row><row><cell cols="4">uogTrBDfeNWD 0.2521 0.7425 0.4040</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28" coords="12,53.76,637.56,239.29,18.63"><head>Table 17 : The mean average precision (MAP), Reciprocal Rank (MRR</head><label>17</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.32,712.68,139.89,8.07;1,53.76,722.79,153.58,6.26"><p>Information on Terrier can be found at: http://ir.dcs.gla.ac.uk/terrier/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,321.24,703.35,187.18,6.26;6,316.80,712.35,79.54,6.26"><p>http://en.wikipedia.org/wiki/Wikipedia: Database download</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,321.24,711.24,225.56,8.07"><p>Details on the parameters and tagset used can be found in<ref type="bibr" coords="8,529.51,711.24,13.83,8.07" target="#b15">[16]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Alasdair Gray</rs> and <rs type="person">Richard McCreadie</rs> for assisting us in our TREC assessment workload this year. The work within the Relevance Feedback track is funded by the SIMAP: Simulation modelling of the MAP kinase pathway, <rs type="projectName">EC</rs> project <rs type="grantNumber">2006-2009</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_67GTYYY">
					<idno type="grant-number">2006-2009</idno>
					<orgName type="project" subtype="full">EC</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,73.20,70.80,205.67,8.07;13,73.20,81.24,218.97,8.07;13,73.20,91.68,161.84,8.07" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="13,111.12,70.80,167.75,8.07;13,73.20,81.24,141.81,8.07">Probabilistic Models for Information Retrieval based on Divergence From Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Dept.of Computing Science, Univ. of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="13,73.20,103.20,198.80,8.07;13,73.20,113.64,214.75,8.07;13,73.20,124.08,199.88,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,110.04,113.64,177.91,8.07;13,73.20,124.08,84.01,8.07">FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ambrosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,173.04,124.08,96.06,8.07">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,135.60,172.10,8.07;13,73.20,146.04,179.35,8.07;13,73.20,156.48,100.04,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,222.30,135.60,23.00,8.07;13,73.20,146.04,166.32,8.07">Italian Monolingual Information Retrieval with Prosit</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,73.20,156.48,78.09,8.07">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,167.88,211.60,8.07;13,73.20,178.44,192.49,8.07;13,73.20,188.88,217.52,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,123.38,178.44,142.31,8.07;13,73.20,188.88,33.25,8.07">Class-Based n-Gram Models of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,113.04,188.88,94.17,8.07">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,200.28,205.27,8.07;13,73.20,210.72,219.38,8.07;13,73.20,221.28,134.72,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,219.45,200.28,59.02,8.07;13,73.20,210.72,219.38,8.07;13,73.20,221.28,57.47,8.07">A Case Study of Distributed Information Retrieval Architectures to Index One Terabyte of Text</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cacheda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,136.56,221.28,20.13,8.07">IP&amp;M</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,232.68,207.78,8.07;13,73.20,243.12,202.75,8.07;13,73.20,253.56,20.00,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,178.78,232.68,102.20,8.07;13,73.20,243.12,108.39,8.07">Research on expert search at enterprise track of TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,198.00,243.12,77.95,8.07;13,73.20,253.56,16.00,8.07">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,265.07,206.96,8.07;13,73.20,275.52,202.12,8.07;13,73.20,285.95,100.52,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,73.20,275.52,188.42,8.07">Relevance weighting for query independent evidence</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,73.20,285.95,96.50,8.07">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,297.47,217.66,8.07;13,73.20,307.92,132.20,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,181.68,297.47,109.19,8.07;13,73.20,307.92,16.69,8.07">Overview of TREC-2004 Web track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,105.36,307.92,96.06,8.07">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,319.32,198.77,8.07;13,73.20,329.87,212.33,8.07;13,73.20,340.31,212.11,8.07;13,73.20,350.75,20.00,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,73.20,329.87,212.33,8.07;13,73.20,340.31,118.42,8.07">University of Glasgow at TREC2007: Experiments in Blog and Enterprise tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,207.36,340.31,77.95,8.07;13,73.20,350.75,16.00,8.07">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,362.15,201.72,8.07;13,73.20,372.59,194.24,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,222.12,362.15,52.81,8.07;13,73.20,372.59,77.41,8.07">Towards better Weighting of Anchors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Upstill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,166.92,372.59,78.16,8.07">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,384.11,204.28,8.07;13,73.20,394.55,205.24,8.07;13,73.20,404.99,20.00,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,202.63,384.11,74.85,8.07;13,73.20,394.55,111.06,8.07">Ranking opinionated blog posts using OpinionFinder</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,200.28,394.55,78.16,8.07">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,416.51,196.36,8.07;13,73.20,426.95,186.62,8.07;13,73.20,437.39,100.52,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,225.62,416.51,43.94,8.07;13,73.20,426.95,173.64,8.07">An effective statistical approach to blog post opinion retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,73.20,437.39,78.54,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,448.91,179.10,8.07;13,73.20,459.35,102.08,8.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,132.14,448.91,106.84,8.07">Query performance prediction</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,73.20,459.35,98.10,8.07">Proceedings of SPIRE 2004</title>
		<meeting>SPIRE 2004</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,470.75,197.74,8.07;13,73.20,481.19,215.31,8.07;13,73.20,491.75,160.88,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,73.20,481.19,215.31,8.07;13,73.20,491.75,44.58,8.07">TREC-7 Ad-Hoc, High Precision and Filtering Experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,134.04,491.75,96.06,8.07">Proceedings of TREC 1998</title>
		<meeting>TREC 1998</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,503.15,189.74,8.07;13,73.20,513.59,213.80,8.07;13,73.20,524.03,43.88,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,157.95,503.15,104.99,8.07;13,73.20,513.59,182.71,8.07">A Syntactically-Based Query Reformulation Technique for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,261.84,513.59,20.13,8.07">IP&amp;M</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,535.55,214.86,8.07;13,73.20,545.99,213.80,8.07;13,73.20,556.43,47.60,8.07" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,157.95,535.55,130.11,8.07;13,73.20,545.99,153.77,8.07">Examining the Content Load of Part of Speech Blocks for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,242.64,545.99,44.36,8.07;13,73.20,556.43,25.50,8.07">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,567.95,213.89,8.07;13,73.20,578.39,218.89,8.07;13,73.20,588.83,177.11,8.07;13,73.20,599.27,100.04,8.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,106.60,578.39,185.49,8.07;13,73.20,588.83,164.10,8.07">University of Glasgow at TREC 2006: Experiments in Terabyte and Enterprise Tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,73.20,599.27,78.07,8.07">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.20,610.79,205.34,8.07;13,73.20,621.23,218.39,8.07" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="13,128.52,610.79,127.83,8.07">The Voting Model for People Search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computing Science, Univ. of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="13,73.20,632.63,190.45,8.07;13,73.20,643.19,144.44,8.07" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,174.93,632.63,88.73,8.07;13,73.20,643.19,30.87,8.07">High quality evidence of expertise</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,119.64,643.19,94.01,8.07">Proceedings of ECIR 2008</title>
		<meeting>ECIR 2008</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,57.59,207.52,8.07;13,336.24,68.15,152.84,8.07" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,437.97,57.59,105.79,8.07;13,336.24,68.15,36.53,8.07">Key blog distillation: ranking aggregates</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,388.56,68.15,78.54,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,79.55,188.52,8.07;13,336.24,89.99,210.04,8.07;13,336.24,100.43,42.82,8.07" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,437.97,79.55,86.80,8.07;13,336.24,89.99,126.09,8.07">Searching for Expertise: Experiments with the Voting Model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,469.08,89.99,64.05,8.07">Computer Journal</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="13,336.24,111.95,218.68,8.07;13,336.24,122.39,193.38,8.07;13,336.24,132.83,102.92,8.07" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,437.97,111.95,116.95,8.07;13,336.24,122.39,179.87,8.07">Voting for Candidates: Adapting Data Fusion Techniques for an Expert Search Task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,336.24,132.83,78.54,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,144.35,213.87,8.07;13,336.24,154.79,186.88,8.07;13,336.24,165.23,210.10,8.07;13,336.24,175.67,137.26,8.07;13,336.24,186.59,158.86,6.90;13,336.24,197.66,172.78,6.26" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<ptr target="http://www.dcs.gla.ac.uk/∼craigm/publications/macdonald06creating.pdf" />
		<title level="m" coord="13,437.97,144.35,112.14,8.07;13,336.24,154.79,186.88,8.07">The TREC Blog06 Collection : Creating and Analysing a Blog Test Collection DCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="13,336.24,208.07,202.98,8.07;13,336.24,218.51,199.88,8.07" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,481.46,208.07,57.76,8.07;13,336.24,218.51,84.01,8.07">Overview of the TREC 2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,436.08,218.51,96.06,8.07">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,230.03,218.96,8.07;13,336.24,240.47,209.96,8.07;13,336.24,250.91,211.84,8.07;13,336.24,261.35,109.76,8.07" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,336.24,240.47,209.96,8.07;13,336.24,250.91,207.50,8.07">University of Glasgow at WebCLEF 2005: Experiments in Per-Field Normalisation and Language Specific Stemming</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,345.96,261.35,96.07,8.07">Proceedings of CLEF 2005</title>
		<meeting>CLEF 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,272.87,218.44,8.07;13,336.24,283.31,189.94,8.07;13,336.24,293.75,200.61,8.07;13,336.24,304.19,91.64,8.07" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="13,374.64,283.31,151.54,8.07;13,336.24,293.75,109.45,8.07">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,461.52,293.75,75.33,8.07;13,336.24,304.19,69.33,8.07">Proceedings of OSIR Workshop at SIGIR</title>
		<meeting>OSIR Workshop at SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,315.71,191.08,8.07;13,336.24,326.15,201.14,8.07;13,336.24,336.59,100.04,8.07" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="13,379.74,326.15,144.03,8.07">Overview of the TREC 2006 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,336.24,336.59,78.07,8.07">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,347.99,205.66,8.07;13,336.24,358.55,208.16,8.07" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="13,412.99,347.99,128.92,8.07;13,336.24,358.55,94.54,8.07">Combination of Document Priors in Web Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,446.40,358.55,94.01,8.07">Proceedings of ECIR 2007</title>
		<meeting>ECIR 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,369.95,207.34,8.07;13,336.24,380.39,218.52,8.07;13,336.24,390.83,20.00,8.07" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="13,423.86,369.95,119.73,8.07;13,336.24,380.39,124.33,8.07">Hierarchical language models for expert finding in enterprise corpora</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Petkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,476.76,380.39,78.00,8.07">Proceedings of ICTAI</title>
		<meeting>ICTAI</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,402.35,206.46,8.07;13,336.24,412.79,199.72,8.07;13,336.24,423.23,205.53,8.07;13,336.24,433.67,91.64,8.07" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="13,448.98,402.35,93.73,8.07;13,336.24,412.79,199.72,8.07;13,336.24,423.23,107.60,8.07">Being Omnipresent To Be Almighty: The Importance of Global Web Evidence for Organizational Expert Finding</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,459.84,423.23,81.93,8.07;13,336.24,433.67,69.33,8.07">Proceedings of fCHER Workshop at SIGIR</title>
		<meeting>fCHER Workshop at SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,445.19,204.84,8.07;13,336.24,455.63,206.56,8.07;13,336.24,466.07,44.60,8.07" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="13,490.54,445.19,50.54,8.07;13,336.24,455.63,137.68,8.07">Simple BM25 Extension to Multiple Weighted Fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,489.24,455.63,53.56,8.07;13,336.24,466.07,22.55,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,477.59,216.10,8.07;13,336.24,488.03,216.56,8.07" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="13,388.71,488.03,60.96,8.07">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,466.08,488.03,83.42,8.07">Proceedings of TREC 4</title>
		<meeting>TREC 4</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,499.43,216.80,8.07;13,336.24,509.99,200.20,8.07;13,336.24,520.43,172.04,8.07;13,336.24,530.87,178.88,8.07" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="13,378.64,499.43,161.09,8.07">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,336.24,509.99,200.20,8.07;13,336.24,520.43,76.90,8.07">The Smart Retrieval system -Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Salton, G., Ed; Prentice-Hall Englewood Cliffs, N.J., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,542.27,185.24,8.07;13,336.24,552.83,191.42,8.07;13,336.24,563.27,171.32,8.07" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="13,336.24,552.83,191.42,8.07;13,336.24,563.27,57.94,8.07">Integrating proximity to subjective sentences for blog opinion retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,409.44,563.27,94.13,8.07">Proceedings of ECIR 2009</title>
		<meeting>ECIR 2009</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,574.67,192.18,8.07;13,336.24,585.11,210.44,8.07;13,336.24,595.67,188.64,8.07;13,336.24,606.11,126.32,8.07" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="13,336.24,595.67,175.35,8.07">OpinionFinder: a system for subjectivity analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,336.24,606.11,104.63,8.07">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.24,617.51,180.84,8.07;13,336.24,627.95,209.77,8.07;13,336.24,638.51,171.77,8.07" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="13,386.94,627.95,159.06,8.07;13,336.24,638.51,42.53,8.07">Microsoft Cambridge at TREC 13: Web and Hard Tracks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,394.68,638.51,91.27,8.07">Proceedings of the TREC</title>
		<meeting>the TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
