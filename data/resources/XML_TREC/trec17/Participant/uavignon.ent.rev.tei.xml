<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.95,164.85,243.34,15.49;1,193.53,186.76,224.19,15.49">Universities of Avignon &amp; Lyon III at TREC 2008: Enterprise Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,274.25,219.26,62.76,10.76;1,337.00,217.47,1.41,7.32"><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
							<email>eric.sanjuan@univ-avignon.fr</email>
						</author>
						<author>
							<persName coords="1,269.05,253.23,73.14,10.76;1,342.19,251.45,1.88,7.32"><forename type="first">Nicolas</forename><surname>Flavier</surname></persName>
							<email>nicolas.flavier@univ-avignon.fr</email>
						</author>
						<author>
							<persName coords="1,248.02,287.21,115.21,10.76;1,363.23,285.43,1.88,7.32"><forename type="first">Fidelia</forename><surname>Ibekwe-Sanjuan</surname></persName>
						</author>
						<author>
							<persName coords="1,273.01,321.20,65.23,10.76;1,338.24,319.41,1.88,7.32"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@univ-avignon.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA &amp; IUT STID</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LIA &amp; CERI</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ELICO</orgName>
								<orgName type="institution" key="instit2">University of Lyon 3</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">LIA &amp; CERI</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.95,164.85,243.34,15.49;1,193.53,186.76,224.19,15.49">Universities of Avignon &amp; Lyon III at TREC 2008: Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">29C218CDB9D2F203C2D0A1917E2844D9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Enterprise track of TREC 2008 comprised of the same two tasks as in the previous years: an ad-hoc document search and an expert search.</p><p>• The document search consisted in retrieving documents that best matched reallife queries submitted by users to the CSIRO corporation. Systems were allowed to retrieve and rank up to a 1000 documents.</p><p>• The expert search consisted in locating the CSIRO staff who is best able to respond to the query formulated by the users.</p><p>This year was our first participation in TREC-ENT. We explored three major approaches to information retrieval using various existing methods and systems. These approaches ranged from domain knowledge mapping <ref type="bibr" coords="1,465.87,567.94,11.62,8.97" target="#b1">[2]</ref> to QA <ref type="bibr" coords="1,160.88,579.90,10.58,8.97" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document search</head><p>Three document runs were submitted in this task. Each run tested a different search methodology, ranging from SOMs using a general ontology, to question-answering and passage retrieval, and then to manual query expansion based on relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General ontologies for knowledge organization and domain mapping based on Self Organized Maps</head><p>Two runs were carried out using this strategy.</p><p>• In LiaIIcAuto run, a small set of documents was extracted and concatenated using Lemur and the query title field.</p><p>• In LiaIcAuto run, all query fields were concatenated.</p><p>• In both cases, resulting texts were projected onto the knowledge map previously built on the whole data. Documents were then ranked by similarity. The runs were completely automatic. There was no human intervention on the ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question-Answering based on SIAC4QA segmenter</head><p>This run was also completely automatic. Question Answering systems aim at retrieving precise answers to questions expressed in natural language. Questions are mainly factual questions and answers are pieces of text extracted from a collection (such as newspaper article compilation). They have been particularly studied since 1999 and the first large scale QA evaluation campaign held as a track of the Text REtrieval Conference.</p><p>Typical QA system architecture involves at least these main steps (most often pipelined):</p><p>• Question Analysis, to extract semantic type(s) of the expected answer;</p><p>• Document Retrieval to restrict the amount of processed data by further components;</p><p>• Passage Retrieval to choose the best answering passages from documents;</p><p>• and final Answer Extraction Strategies to determine the best answer candidate(s) drawn from the previously selected passages.</p><p>We employed the Passage Retrieval component in TREC Enterprise as an Indri post-processing. Applied to TREC Enterprise data, the inputs are the title fields of the topics and the sets of documents, and the outputs are some ranked lists of retrieved passages.</p><p>Since our first TREC QA participation <ref type="bibr" coords="2,304.49,630.08,10.58,8.97" target="#b0">[1]</ref>, our passage retrieval approach changed from a cosine based similarity to a density measure. For QA, our passage retrieval component sees a question as a set of several kinds of items : words, lemmas, POS tags, Named Entity tags, and expected answer types. For experiments, items were the lemmas of the topic only (the empty words were filtered according to their POS tags) and the maximum size of a retrieved passage has been limited to three sentences.</p><p>First, a density score s is computed for each occurrence o w of each topic lemma w in a given document d. This score measures how much the words of the topic are far away from the other ones. It allows to point at the centers of the document areas where the words of the topic are most present. It takes into account the number of different lemmas |w| in the topic, the number of topic lemmas |w, d| occurring in the currently processed document d and a distance µ(o w ) that equals the average number of words from o w to the other topic lemmas in d (in case of multiple occurrences of a lemma, only the nearest occurrence to o w is considered).</p><p>Let s(o w , d) be the density score of o w in document d:</p><formula xml:id="formula_0" coords="3,210.11,267.77,169.91,23.53">s(o w , d) = log [µ(o w ) + (|w| -|w, d|) .p] |w|</formula><p>where p is an empirically fixed penalty aimed to prefer or to not prefer few common words with the topic that are close to each other or many words that are distant to each other.</p><p>Secondly, a score is computed for each sentence S in a document d. The score of a sentence is the maximum density score of the topic lemmas it contains:</p><formula xml:id="formula_1" coords="3,247.34,371.32,96.64,14.04">s(S, d) = max ow∈S s(o w , d)</formula><p>At the end of the process, the score of a document is the linear combination of the original INDRI score with the passage retrieval score. This resulted in the in the LIAIndriSiac run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiword term incremental query expansion using relevance feedback</head><p>From the observation that the topics in TREC-ent were real life complex queries that would normally involve humans somewhere in the loop in order to "construct" the answer. Indeed, a manual inspection showed that often, the answer was not readily available on the retrieved web pages. It needed to be "constructed" from reading several potentially relevant web pages. Topics of the type "How can I do Y about X?" would typically have pages containing some information about X but not necessarily the real answer ("how to do Y"). These topics particularly relevance feedback techniques in order to expand the queries with more adequate terms. The query expansion strategy consisted in submitting an initial query to Indri using terms from the title field. Additional multiword terms were manually gathered from an exploration of the top 20 documents ranked by Indri. These terms were then used to expand the initial set of query terms and resubmitted to Indri. The final set of query terms was submitted to the Indri engine using:</p><p>• proximity operators (#3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• belief operators (#combine).</head><p>This run is named LiaIndriMan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Preliminary results for document search</head><p>For this corpus our baseline run consisted in submitting the content of the title field to Indri. This baseline attained a high average score: infAP=0.3167, infNDCG=0.5008 on queries. We observed a similar performance TREC-ENT 2007 data. Only the LI-AIndriSiac run attained a higher average score: infAP=0.3191 infNDCG=0.5078.</p><p>The run LiaIndriMan using manually expanded multiword terms obtained a quite lower score: infAP=0.2379, infNDCG=0.3951. This score can be improved by relaxing the NP structure of the multiword terms and allowing the insertion of more words into MTW (We added 2 to all #n indri operators). It is also improved using automatic query expansion. The manual run finally obtains the following average scores: infAP=0.2734, infNDCG=0.4461, that still remain under the baseline.</p><p>The average score of other two runs are even lower (under 0.1 for infAP and 0.2 for infNDCG). This could be explained by the gap between the knowledge base we used (specialized scientific domains and economic vocabulary) and the common vocabulary in CSIRO web pages.</p><p>However, when we look at the performance of our runs, query by query, we find out that each system works better on some type of query. Figure <ref type="figure" coords="4,400.35,372.18,4.98,8.97" target="#fig_0">1</ref> shows the results query by query. The left bar represents the median score of all participants. It clearly appears that LIAIndriSiac is often over the median score but when the median score is low, then the run based on manually extracted terms performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Expert search</head><p>We carried out a baseline search using Indri and a manual search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic baseline run</head><p>This consisted in generating multi-document summaries for each e-mail address occurring in the corpus. These summaries were indexed using Lemur and addresses were ranked based on indri #combine operator applied to titles without any preprocessing. This run is labeled LiaExp08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Manual Run</head><p>RunID: LiaIcExp08. This run was carried out following these steps:</p><p>1. creating an expert sub-collection using the query terms "dr" and "professor" in html title fields.</p><p>2. an automatic search was then done by similarity of concepts with query and narrative fields just copied into the search mask. Concept similarity relies on a general ontology and a domain map built on the sub-collection. • When relevance was above a user defined threshold, documents were opened for selection and/or query refinement.</p><p>• Otherwise the query was run on the entire corpus and same process using the corresponding larger domain map.</p><p>3. If relevance was again too low, the query was reduced to traditional keyword list by deletion of meaningless words. Then a search by synonyms was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>It appeared that the user did not find more than four experts per query with an average of 2.44. This is in contrast with the resulting qrels established by participants where there is an average of 10.36 experts per query. Therefore the map score of LiaIcExp08 is only 0.2513. However, ircl prn.0.00 is 0.8576 and ircl prn.0.10 is 0.7806 in average on all queries. Still, even on these qrels, the manual run significantly outperforms our baseline that has a map score of 0.1841 with ircl prn.0.00=0.5906 and ircl prn.0.10=0.5393.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,133.77,635.38,343.71,8.97;5,133.77,647.33,67.23,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Inferred Average Precision for Lia runs and TREC 2008 median score on document search</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,150.36,374.51,327.10,8.97;6,150.36,386.47,327.10,8.97;6,150.36,398.42,327.11,8.97;6,150.36,410.37,141.40,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,410.80,374.51,66.67,8.97;6,150.36,386.47,327.10,8.97;6,150.36,398.42,62.61,8.97">Coupling named entity recognition, vector-space model and knowledge bases for trec-11 questionanswering track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Crestan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El-Bèze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>De Loupy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,233.21,398.42,216.36,8.97">The Eleventh Text REtrieval Conference (TREC 2002)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,150.36,430.30,327.09,8.97;6,150.36,442.26,327.09,8.97;6,150.36,454.21,75.99,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,294.11,430.30,145.82,8.97">From term variants to research topics</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ibekwe-Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,446.73,430.30,30.72,8.97;6,150.36,442.26,327.09,8.97;6,150.36,454.21,12.57,8.97">Journal of Knowledge Organization (ISKO), special issue on Human Language Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
