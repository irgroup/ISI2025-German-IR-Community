<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.24,115.96,336.88,12.62;1,215.13,133.89,185.09,12.62">FUB, IASI-CNR and University of Tor Vergata at TREC 2008 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.69,171.91,88.13,8.74"><forename type="first">Giambattista</forename><surname>Amati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Ugo Bordoni</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.37,171.91,79.55,8.74"><forename type="first">Giuseppe</forename><surname>Amodeo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Ugo Bordoni</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.48,171.91,63.54,8.74"><forename type="first">Marco</forename><surname>Bianchi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Istituto di Analisi dei Sistemi ed Informatica &quot;A. Ruberti&quot;-CNR</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.57,171.91,63.86,8.74"><forename type="first">Carlo</forename><surname>Gaibisso</surname></persName>
							<email>carlo.gaibisso@iasi.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Istituto di Analisi dei Sistemi ed Informatica &quot;A. Ruberti&quot;-CNR</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.21,183.87,73.84,8.74"><forename type="first">Giorgio</forename><surname>Gambosi</surname></persName>
							<email>gambosi@mat.uniroma2.it</email>
							<affiliation key="aff2">
								<orgName type="department">Mathematics Department</orgName>
								<orgName type="institution">University of Tor Vergata</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.24,115.96,336.88,12.62;1,215.13,133.89,185.09,12.62">FUB, IASI-CNR and University of Tor Vergata at TREC 2008 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01835C262190987337798B174FEDB429</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We take part in the opinion and polarity retrieval tasks of the blog track.</p><p>A test collection, called Blog06, was created for the blog track in 2006 <ref type="bibr" coords="1,470.08,331.22,10.52,8.74" target="#b3">[4]</ref> with three main different components: feeds, permalinks and home-pages. The collection contains spam as well as possibly no blogs and no english pages. For our experimentation only permalinks have been taken into consideration, consisting of 3.2 million of Web pages for a total of 88.8GB, each one containing a post and its related comments.</p><p>The evaluation metrics are precision/recall based <ref type="bibr" coords="1,364.39,403.30,9.96,8.74" target="#b3">[4]</ref>, the Mean Average Precision (MAP) and R-Precision (RPrec), but we also focused on Precision at 10 (P@10), due to its relevance in evaluating the effectiveness of Web search engines <ref type="bibr" coords="1,134.77,439.17,10.52,8.74" target="#b4">[5]</ref>  <ref type="bibr" coords="1,148.60,439.17,9.96,8.74" target="#b2">[3]</ref>.</p><p>As in 2007, we based our approch on the costruction of ad-hoc weighted dictionaries, containing terms assumed to be used to express a sentiment. The weight is a measure of how much sentiment the term expresses.</p><p>To automatically construct our dictionaries, we assumed that "opinion-bearing" words distribute more randomly in the set of opinionated documents than semanticbearing terms, but less randomly than not-informative terms.</p><p>As a consequence, we relyed on two theoretic measures. The first of them was based on a Divergence From Randomness (DFR) model and defined the weight of each term within an opinionated document, conseguently identifing the set of terms candidate to appear in the vocabularies. The other one, was based on entropy maximization in the set of all relevant and opinionated documents and defined the final content of the dictionaries and the weights of their terms.</p><p>By these dictionaries, we first reranked the set of documents relevant to a topic on the basis of the quantity of opinion they express, and then extract two new rankings according to the polarity of the expressed sentiment.</p><p>All these phases are detailed described in Sections 2, 3, 4, 5 and 6. Finally, in Section 7 we report and discuss on the experimentation activity and results. Finally, a brief analysis of our results is present in 8.</p><p>As in 2007 <ref type="bibr" coords="2,181.62,141.09,9.96,8.74" target="#b4">[5]</ref>, data preprocessing mainly consisted in trying to remove no english documents from the collection through LingPipe <ref type="bibr" coords="2,354.15,153.04,9.96,8.74" target="#b0">[1]</ref>. In our intention the tool should also succed in detecting some of the spam. A deeper analysis, than those conducted in 2007, of the effectiveness of this approch, revealed that a consistent fraction of relevant documents were wrongly identified as written in a language other than english.</p><p>Unfortunately we hadn't enough time to test alternative training modalities of the LingPipe or to evaluate complete different approches to solve the problem. Thus, we have been forced to deal with the original collection, spam and no english documents included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topic relevance retrieval</head><p>For the retrieval of the documents relevant to a topic, we basically followed the same approch adopted in 2007, with only few exceptions: we did not rely on the distributed implementation of Terrier <ref type="bibr" coords="2,301.17,324.75,10.52,8.74" target="#b1">[2]</ref> to build our indexes, while DFRee1, a parameter free retrieval model, has been adopted instead of DPH. The stemmimg modalities and the choice of the parametric PL2 model, with c set to 9, stayed unaffected.</p><p>Table <ref type="table" coords="2,178.43,372.57,4.98,8.74">1</ref> shows the values of the MAP, the R-Prec and the P@10 for the topic relevance retrieval baselines we submitted to TREC 2008 (BL DFRee and BL PL2c9 respectively for the DFRee1 and PL2 retrieval models). Together with the same values for the baselines provided by NIST (BL1, BL2, BL3, BL4, BL5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatic costruction of ad-hoc dictionaries</head><p>Our approch is based on the costruction of three ad-hoc weighted dictionaries: one for the opinion retrieval, OpinD, and the other two for the polarity detection, P osD and N egD. Before entering the details of the construction, let us introduce a little bit of notation. Let:</p><p>-C denote the collection of documents; -R ⊆ C denote the set of documents relevant to a topic; -O ⊆ R denote the set of documents relevant to the same topic, expressing an opinion on it;</p><p>In automatically constructing OpinD <ref type="bibr" coords="2,301.54,567.19,10.52,8.74" target="#b2">[3]</ref> [5], we assumed that:</p><p>content-bearing words maximize the probability P of observing the posterior probability of occurrence in O, given the prior probability of occurrence in R.</p><p>opinion-bearing words, instead, minimize the same probability. The weight of an opinion-bearing word is provided by a DFR model, and, as a consequence, a word is assumed to express an opinion iff it maximizes the value of the computed divergence.</p><p>best opinion-bearing words also maximize the entropy in O. In our assumption, the approach we adopted to maximize entropy is to select the terms with highest divergence, that, at the same time, belong to a large enough number of opinionated documents.</p><p>Starting from our assumptions, R has been identified as the set of documents recognized as relevant by TREC 2006-2007, those labeled 1, 2, 3 or 4 in the provided qrels; while O as the set of opinionated ones, those labeled 2, 3 or 4 in the same qrels <ref type="bibr" coords="3,200.49,210.46,9.96,8.74" target="#b3">[4]</ref>.</p><p>The DFree1 DFR has been adopted to identify the set of opinion-bearing words. The set of best opinion-bearing words is then obtained as follows: a sequence of candidate dictionaries</p><formula xml:id="formula_0" coords="3,134.77,246.33,345.83,44.60">D 1 ⊇ D 2 ⊇ • • • ⊇ D k , with D 1 coincid- ing with the set of opinion-bearing words, has been computed such that ∀i = 1, . . . , n D i = {t ∈ D 1 ∧ df t ≥ i} where df t is the document frequency of term t in O [3] [5].</formula><p>As result, a generic k level dictionary contains all opinion-bearing terms occurring in at least k documents in O. Our final goal was to find the maximum value of k, say k, that keeps the retrieval performance stable, when compared with those obtained by D k , with k ≤ k; and maintains the dictionary size small engough to be computationally effective. The value of k best fitting our needs has been tentatively fixed to 1, 000.</p><p>P osD and N egD respectively are analogously determined: all the above assumptions and considerations still hold if R is substituted by O, and O by O + and O -, respectively, where O + (resp. O -) denotes the set of documents expressing a positive (resp., negative) opinion.</p><p>This time O has been identified as the set of documents recognized as positively and negatively opinionated by TREC 2006-2007, those labeled 2 or 4 in the provided qrels. The value of k best fitting our needs has been tentatively fixed to 500 for P osD, and to 100 for N egD. Since weights assigned to terms appeared to be significantly dissimilar between the two dictionaries, the weights of each dictionary have been normalized to the highest value inside of the dictionary itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Opinionated relevance retrieval</head><p>Opinionated and relevant documents was ranked, for each query q, in three steps:</p><p>1. a topic retrieval step was accomplished, as decribed in section 3: a new rank, say content rank(d||q), was assigned to each document d, depending on the score, say content score(d||q), assigned to it by the adopted DFR model; 2. a new query, maden by all the terms in OpinD, weighted by their respective weights, was submitted: a new score was obtained for each document d, say opinion score(d||OpinD). A new rank, say opinion rank(d||q), for each document d was then obtained on the basis of opinion score(d||q) = opinion score(d||OpinV)/content rank(d||q);</p><p>3. the final ranking was obtained by furtherly boosting the rank assigned to each document d, say content score + (d||q), as follows: content score + (d||q) = content score(d||q)/opinion rank(d||q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Polarity Recognition</head><p>Polarity recognition is accomplished with an approach similar to that adopted for the opinionated relevance retrieval. The starting point is the opinion ranking determined according to the modalities described in section 5. The computation of the polarity rank is based on the weights assigned to the terms in P osD and in N egD. The final polarity score of a document is obtained by subtracting to its positive polarity score its negative one. If the final score is greater than zero, the document is considered as expressing a positive opinion; a negative opinion, otherwise. Finally if this score is close to zero, we consider the document as not sufficently polarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Tests and results</head><p>We first of all generate our baselines, one for each topic of interest, by ranking the documents according to the content they bear: table <ref type="table" coords="4,389.47,357.24,4.98,8.74">1</ref> shows the mean of the values of the topic relevance MAP, R-Prec and P@10 for our baselines, rows BL DFRee and BL PL2c9 for the DFRee1 and PL2 retrieval models, together with the same values for the baselines provided by the NIST. As shown by the table, in no case we succeeded to improve the baselines of reference.</p><p>Next, each baseline is re-ranked according to the quantity of opinion its documents bear. These new rankings will be referred to as opinion based rankings.</p><p>To asses the effectiveness of our approach, we first of all investigate its impact on the baselines: we compared the MAP and the R-Precision values of table 1, with the corrisponding values for the opinion based rankings generated using the DFRee1 and PL2 models, shown by tables 2 and 3, respectively. These tables also show the results of the comparison.</p><p>We then compared the MAP and the R-Prec values for the opinion relevance of the baselines, shown by table 4, with the corrisponding values for the opinion based rankings generated using the DFRee1 and PL2 models, shown by tables 5 and 6, respectively. Table <ref type="table" coords="4,257.17,536.57,4.98,8.74">7</ref> shows the results of this comparison.</p><p>Furthermore, for each topic we have been given the medians of the opinion relevance MAP and R-Prec for all the runs submitted by all the participants. We compute the means of this values, 0.3050 for the MAP and 0.3651 for the R-Prec, and compare them with our results, as shown by tables 5 and 6.</p><p>Finally, documents in each of the opinion based rankings are filtered according to the positive (resp., negative) polarity of the opinion they bear, resulting in two new rankings, one of documents expressing a positive opinion, the other of documents expressing a negative one. It is worth noting that the sets of documents in these rankings, do not intersect and are a subset of the documents appearing in the original opinion based ranking. This implies that the MAP and R-Prec values for these rankings can not be directly compared with those for the opinion based rankings.</p><p>As a consequence we limited ourselves to investigate the effectiveness of our approach by comparing the medians of the polarity relevance MAP, i.e. 0.1151, and R-Prec, i.e. 0.1624, for the runs submitted by all participans with our results, as shown by tables 8 and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>By our experiments we confirmed that our approach to the opinion retrieval is really effective and robust, also in absence of ad hoc solutions for the detection of spam and of no english documents. The Dfree1 model has proved itself to overperform the PL2 one. As concerns the polarity detection, we failed in achieving acceptable results. We think that the main motivation of this failure has reference to the scarce effectiveness of our sentimental dictionaries in properly classifying documents. May be the adoption of an approach based on passage retrieval could be the proper solution to this problem. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,241.47,552.44,132.42,102.26"><head>Table 9 .</head><label>9</label><figDesc></figDesc><table coords="5,241.47,552.44,132.42,102.26"><row><cell>Run</cell><cell>MAP R-Prec P@10</cell></row><row><cell cols="2">BL DFR 0.3195 0.3756 0.6307</cell></row><row><cell cols="2">BL PL2c9 0.3287 0.3838 0.6313</cell></row><row><cell>BL1</cell><cell>0.3701 0.4156 0.7307</cell></row><row><cell>BL2</cell><cell>0.3382 0.3831 0.7000</cell></row><row><cell>BL3</cell><cell>0.4244 0.4573 0.7220</cell></row><row><cell>BL4</cell><cell>0.4776 0.5092 0.7867</cell></row><row><cell>BL5</cell><cell>0.4424 0.4868 0.7793</cell></row><row><cell></cell><cell>Table 1.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,369.93,310.86,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,179.39,369.93,115.86,7.86">Lingpipe named entity tagger</title>
		<author>
			<persName coords=""><surname>Alias-I</surname></persName>
		</author>
		<ptr target="http://www.alias-i.com/lingpipe/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,380.66,342.24,7.86;5,146.91,391.62,333.68,7.86;5,146.91,402.58,333.68,7.86;5,146.91,413.54,20.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,450.57,380.66,30.02,7.86;5,146.91,391.62,266.32,7.86">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,434.71,391.62,45.88,7.86;5,146.91,402.58,329.49,7.86">Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)</title>
		<meeting>ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,424.27,342.25,7.86;5,146.91,435.23,333.68,7.86;5,146.91,446.19,174.29,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,415.01,424.27,65.59,7.86;5,146.91,435.23,258.95,7.86">Automatic Construction of an Opinion-Term Vocabulary for Ad Hoc Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ambrosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,423.44,435.23,57.16,7.86;5,146.91,446.19,43.70,7.86">Proceedings of ECIR 2008</title>
		<title level="s" coord="5,198.23,446.19,24.38,7.86">LNCS</title>
		<meeting>ECIR 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4956</biblScope>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,456.93,342.25,7.86;5,146.91,467.89,333.68,7.86;5,146.91,478.84,20.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,248.56,456.93,232.04,7.86;5,146.91,467.89,22.81,7.86">University of Glasgow Overview of the TREC 2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,188.38,467.89,288.02,7.86">Proceedings of The Sixteenth Text REtrieval Conference (TREC 2007)</title>
		<meeting>The Sixteenth Text REtrieval Conference (TREC 2007)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,489.58,342.24,7.86;5,146.91,500.54,333.68,7.86;5,146.91,511.50,231.81,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,438.48,489.58,42.10,7.86;5,146.91,500.54,237.88,7.86">IASI-CNR and University of Tor Vergata at TREC 2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ambrosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,403.60,500.54,77.00,7.86;5,146.91,511.50,203.56,7.86">Proceedings of The Sixteenth Text REtrieval Conference (TREC 2007)</title>
		<meeting>The Sixteenth Text REtrieval Conference (TREC 2007)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
