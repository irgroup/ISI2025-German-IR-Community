<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.47,160.66,371.07,15.15">York University at TREC 2008: Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.99,195.24,73.22,7.86"><forename type="first">Mladen</forename><surname>Kovacevic</surname></persName>
							<email>mladen@cse.yorku.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.99,195.24,58.87,7.86"><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
							<email>jhuang@yorku.ca</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.47,160.66,371.07,15.15">York University at TREC 2008: Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C640339AE4054981671A0D67944AD8D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>York University participated in the TREC 2008 Blog track, by introducing two opinion finding features. By initially focusing solely on the sentiment terms found in a document, using two different methods, and not considering the topic relevance, we saw an overall negative impact in the final evaluations. Post-TREC improvements show the necessity of combining opinion weights with the topic relevance scores. A tunable combination function is used, which revealed some interesting findings about the opinion finding features noting the strengths and weaknesses. We also introduce the Compass Information Retrieval system, which is based on Okapi, as the driver by which these experiments were conducted.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper discusses two opinion finding features, sentiment term frequency and sentiment tf-idf term weight. They are implemented by the newly developed Compass Information Retrieval (IR) engine, used in the Blog retrieval task at TREC 2008. Compass IR is a java based application built on the Okapi Basic Search System (BSS), which provides capabilities in preparing data sets, running experiments, and the extendible ability of easily implementing information retrieval techniques. Two baselines and two opinion finding runs were submitted for the TREC 2008 Blog Track. Several more experiments were conducted after the TREC conference deadline, and we make note of those results here as well. By making use of a pre-determined sentiment word list, the opinion finding techniques were used on provided baseline runs to provide a new ranking. As overall results show us, not considering any topical relevance in the re-ranking algorithms is detrimental to the final results. Notable improvements can be seen using combination functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>The Compass IR system was concurrently in the process of development and used in preparing the run submissions to TREC 2008 for the Blog track. Okapi BSS is the back-end engine of Compass IR, being integrated for applying the BM25 weighting function <ref type="bibr" coords="1,378.07,631.99,10.52,8.74" target="#b0">[1]</ref> to documents, and providing ranked, relevant results. Okapi is an information retrieval system based on probabilistic models of Robertson and Sparck Jones <ref type="bibr" coords="1,232.27,655.90,9.96,8.74" target="#b8">[9]</ref>, which has been the primary system used in numerous TREC conferences, in various applications and organizations. This includes being used as a platform for contextual information retrieval processes <ref type="bibr" coords="1,277.54,679.81,9.96,8.74" target="#b3">[4]</ref>, to Chinese text retrieval experiments <ref type="bibr" coords="1,463.18,679.81,9.96,8.74" target="#b1">[2]</ref>, and early TREC conferences dating back to 1995 <ref type="bibr" coords="1,264.92,691.76,14.61,8.74" target="#b9">[10]</ref>. A search term is assigned weight based on its withindocument term frequency and query term frequency. The BM25 weighting function applies document weight as shown in equation 1.</p><p>w = (k1 + 1) * tf K + tf * log (r + 0.5)/(R -r + 0.5) (n -r + 0.5)/(N -n -R + r + 0.5)</p><formula xml:id="formula_0" coords="2,338.24,132.09,183.76,16.47">* (k3 + 1 ) * qtf k3 + qtf ⊕ k2 * nq * (avdl -dl) (avdl + dl) (1)</formula><p>where N is the number of indexed documents in the collection, n is the number of documents containing a specific term, R is the number of documents known to be relevant to a specific topic, r is the number of relevant documents containing the term, tf is within-document term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the average document length, nq is the number of query terms, the k i s are tuning constants (which depend on the database and possibly on the nature of the queries and are empirically determined), K equals to k 1 * ((1b) + b * dl/avdl), and ⊕ indicates that its following component is added only once per document, rather than for each term <ref type="bibr" coords="2,204.68,238.86,9.96,8.74" target="#b2">[3]</ref>.</p><p>This was the weighting method we used in developing the primary baseline runs for the Blog track.</p><p>The opinion finding features presented in this paper did not incorporate the BM25 function although we leave this open for future work when using Compass IR with Okapi as the ranking system.</p><p>3 Blog Tasks: Baselines to Opinion Rankings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>There were several tasks associated with the Blog track at TREC 2008. The primary tasks were the opinion finding task and the blog distillation task. Overall, submissions were based on four key tasks.</p><p>• Baseline adhoc (blog post) retrieval task</p><p>• Opinion finding (blog post) retrieval task</p><p>• Polarized opinion finding (blog post) retrieval task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Blog finding distillation task</head><p>This year, we participated in the first two of the above tasks, namely the baseline adhoc and opinion finding retrieval tasks. The Blog collection <ref type="bibr" coords="2,283.42,486.76,10.52,8.74" target="#b4">[5]</ref> was the same one initially used in the TREC 2006 Blog track <ref type="bibr" coords="2,139.08,498.71,9.96,8.74" target="#b7">[8]</ref>, and all of our runs were based solely on permalink documents provided in the data set.</p><p>The assessment procedure involved assessors judging the 50 newly added 2008 topics, while the judgments from the previous years, 2006 and 2007, were reused for the first given 100 topics. A total of 150 queries of various topics were provided for the tasks. Pools for judgments were created from participating group run submissions. The documents in the pools were then assigned a value according to the following scale: 0 -not relevant 1 -topic relevant, no opinion 2 -topic relevant, negative opinion 3 -topic relevant, both negative and positive opinion 4 -topic relevant, positive opinion When the final results given were evaluated, two sets of evaluations were provided for each run. The first, topic relevance, would only measure effectiveness according to all those labelled 1 through 4. Meanwhile, the opinion relevance evaluation would consider only those labelled from 2 to 4. The polarity tasks went even further to determine the differences between the orientation of the opinion noted by labels 2, 3, 4 as required.</p><p>The goal of the baseline adhoc retrieval task was to provide rankings of blog posts which contained relevant information about the given target, ensuring that all opinion-finding techniques were turned off.</p><p>The opinion retrieval task re-ranks the results from the baseline adhoc retrieval task, by further placing emphasis on those documents expressing an opinion about a given topic target. Being more of a subjective task, the target needed not be the topic of the post, rather opinions about the target needed to be present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submission Requirements</head><p>There were 150 query topics provided by TREC, and a total of 2 topic-relevance baselines were permitted to be submitted. One of which had to be an automatic, title-only run. We submitted runs york08bb1 and york08bb2, both being automatic, title-only runs.</p><p>The opinion finding task accepted up to 24 total possible submissions. Four runs were allowed to be based on the two previously submitted baseline runs. TREC also provided 5 standard topicrelevance baseline runs submitted by various groups, for which up to 4 runs were accepted for each baseline. The more submissions that could be evaluated, the more accurate it was in determining the validity of any conclusions. Due to the concurrent development of Compass IR, we were only able to complete two submissions, york08bo1a and york08bo3b, both being automatic, title-only runs. They were based on re-evaluating baselines 1 and 3 provided by TREC, and used two different opinion finding features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Opinion Retrieval Features 4.1 Experiments</head><p>Our experiments were conducted on a single-processor Intel(R) Pentium (R) 4 CPU 3.40GHz server with 1GB of memory. The server ran Fedora 8 Linux distribution, on the 2.6.23 kernel.</p><p>Two adhoc retrieval, automatic, title-only runs and two opinion retrieval, automatic, title-only runs were provided. The adhoc retrieval was based on the BM25 weighting function, while the opinion ranking methods incorporated, (A) Sentimental term frequency and (B) Sentimental term tf-idf weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Cleaning and Indexing</head><p>The permalink documents of the blog posts were given as raw HTML content which needed to be parsed and cleaned into a state which could be indexed by the Okapi BSS. A java based TREC Preprocessor engine was written, as part of the Compass IR package, to perform this work. Integrated into the TREC Preprocessor is the HTML Parser <ref type="bibr" coords="3,307.87,542.59,9.96,8.74" target="#b6">[7]</ref>, which automatically removed all HTML tags, and provided us with the body text of each document. Since all queries were in English, documents were further cleaned by stripping away any non-ASCII characters, and limiting the final document size at approximately 3MB. A vast majority of documents were under the 100 KB mark, however, there were still a fair amount that were quite large. Because of this, some documents were even left out of the indexing process. The final, cleaned version of the permalink documents amounted to approximately 25GB.</p><p>This was a unique data set for Okapi, since in other TREC tracks which used Okapi, although data sets may have been large, on a per-document level, the index entries were normally much smaller <ref type="bibr" coords="3,90.00,650.18,9.96,8.74" target="#b2">[3]</ref>. In fact, this large data set required us to break up the data into smaller textual databases, for which a distributed index was then generated. After collecting all the statistics for the associated terms based on this distributed index, we later needed to perform merging to get the final weights and rankings.</p><p>Two Okapi indexes were generated from the result set. The first index was used to quickly return the content of a document, given its document (permalink) number. The second index was used for generating ranking document lists, based on the BM25 weight function, for a given TREC query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Adhoc Retrieval</head><p>The baseline adhoc retrieval approach we took was applying BM25 on the data set provided by the TREC queries. Each query was read in by Compass IR, and passed to Okapi for ranking. The query terms were extracted, and gathered into a list. Stemming and stop-word removal was applied, and a count of the occurrences of each term is kept. Duplicates were thus removed. We only submitted automatic, title-only runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Opinion Retrieval Techniques</head><p>We conducted two key opinion retrieval techniques to re-rank the baseline adhoc retrieval results. They were both simple, and were designed specifically to provide baseline opinion retrieval results to which many more techniques can then be compared against. Resource and time constraints prevented us from providing submitting additional runs and implementing the methods we were aiming for. Post-TREC, these baseline runs were in fact improved on, and a tunable combination function was implemented. The combination function did the following:</p><formula xml:id="formula_1" coords="4,227.24,316.06,291.36,6.12">w = a(opinionscore) + (1 -a)(topicrelscore) (<label>2</label></formula><formula xml:id="formula_2" coords="4,518.60,316.06,3.40,6.12">)</formula><p>where a is a tunable parameter between 0 and 1. The higher the value, the more weight is assigned to the opinion score and the lower the value to the topic relevance score. Also, the opinion score and the topicrel score were the normalized values for the given document over the maximum opinion or topic relevance scores. Thus, for those baselines that do not provide a ranking weight that is based on some scoring method, rather simply go in descending order sequentially by constant value, most likely would not yield beneficial results from using this combination method.</p><p>The general approach taken to apply the opinion weight for both the sentiment term frequency as well as the sentiment term tf*idf frequency, can be described as:</p><p>Input: a baseline run submission Output: a re-ranked baseline determined by opinion retrieval algorithm Method:</p><p>(1) Read in the 1000 documents for each query (2) Apply an opinion ranking algorithm and set new weights for each document (3) Re-rank the documents according to new weights For each of the given queries, However, we did not make use of the query term information for the opinion retrieval techniques. Rather, we applied the following method for each document in the baseline .</p><p>The two methods of calculating the new document weights were extremely limited because we only incorporated the opinionated feature of the documents, and discarded the topic relevance. This intuitively was not expected to produce the best overall results, but by comparing the two methods, we see drastic differences in overall results as well. By applying the combination function (2), improvements could be seen. Both approaches make use of detecting sentiment words in the given documents. This was done by using a predetermined, static list of sentiment terms and detecting them during the statistics gathering stage described in Figure <ref type="figure" coords="4,241.65,703.48,8.71,8.74" target="#fig_2">-2</ref>.  The first basic weight function was a basic sentiment term frequency function counting the number of sentiment terms found in the document and normalizing the result over the total number of terms in the document.</p><p>The first run, york08bo1a, used this method against baseline1 provided by TREC. It was lacking in two ways. First, it did not take into consideration the terms found locally in the document normalized with the entire document set. Second, it did not consider any target references in relation to the sentiment terms themselves. Since there were a total of 1000 documents returned per query, and those 1000 are re-ranked blindly according to sentiment word appearance, clearly, those documents that are not relevant but opinionated will potentially be placed higher in the rankings than other relevant (opinionated) documents.</p><p>Future work for this method could address the above noted problems by having a way to rank the terms globally considering the entire data set and extending it to make use of sentiment term proximity relative to target terms. Several proximity based methods were used in previous years as well <ref type="bibr" coords="5,110.24,474.29,9.96,8.74" target="#b5">[6]</ref>. Our improved approach post-TREC relied on the topic relevance score being incorporated by equation-2, thus the need for involving the query terms themselves was not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Sentiment term tf-idf weight</head><p>The second run, york08bo3b, was run against baseline3 provided by TREC. This approach only looked at sentiment terms as well, applying the tf-idf formula to the sentiment terms found in the document, setting the tf-idf weight to be the new score.</p><p>The tf-idf weighting function can be defined as follows:</p><formula xml:id="formula_3" coords="5,266.89,582.76,251.71,21.40">weight = n X i=0 tfi * idfi (<label>3</label></formula><formula xml:id="formula_4" coords="5,518.60,590.12,3.40,6.12">)</formula><p>where tf is the sentimental term frequency in the document and is calculated as,</p><formula xml:id="formula_5" coords="5,285.20,627.26,233.40,16.47">tfi = stfi mstf (<label>4</label></formula><formula xml:id="formula_6" coords="5,518.60,632.37,3.40,6.12">)</formula><p>where stf is the frequency of the sentiment word in the document and mstf is the maximum frequency of any sentiment word found in the document.</p><p>The idf (inverse document frequency), is defined as:</p><formula xml:id="formula_7" coords="5,286.72,689.42,235.28,16.47">idf = ln N dt<label>(5)</label></formula><p>where N, is the total number of documents in the collection and dt is the number of documents in which the sentimental term is found. This method extends the first by considering the sentiment terms found over the entire collection. It is still lacking as it does not take into account the target terms from the query whatsoever, nor does it consider the provided topic relevance baseline runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The two approaches taken in the experiments, Sentimental term frequency and Sentimental term tf-idf weight, provided interesting results, however, it was unfortunate that we did not provide a direct head-to-head comparison. Instead, we look at the percentage differences that were achieved from the baselines provided compared with the opinion retrieval techniques and compare those.</p><p>Two runs were submitted for the baseline runs, york08bb1 and york08bb2. Unfortunately, york08bb1 provided incorrect results due to incorrect system configuration settings. With the york08bb2 submission, some system configuration settings were changed, yielding slightly better results, however, the settings had still been incorrect, and do not give a good representation of the system. We repaired these configurations post-TREC and developed a few more notable baseline runs. These runs worked with a single large Okapi index. Runs york08bb3 through to york08bb8 adjusted various tuning parameters in Okapi's BM25 algorithm, thus we found york08bb7 and york08bb8 runs to be our top baseline runs based solely on adhoc topic retrieval. It is interesting how across all three measures of the opinion scores for these runs, is that york08bb7 edges out york08bb8. The key difference between these two runs is that york08bb8 takes into account the adjacency order of query terms provided in the query. As an example, if "brown fox" was the query, york08bb8 run mandated that "brown" always came before "fox" to determine relevancy. We can see that when looking at the topical relevance, that york08bb8 performs better. We believe that one of the reasons these values were still lower than expected is because our investigation showed that Okapi indeed did not successfully index all of the documents.</p><p>Two runs were submitted for the opinion finding task, york08bo1a and york08bo3b. Run york08bo1a applied opinion finding feature (A), sentiment term frequency, against baseline1 provided by TREC. Meanwhile york08bo3b applied opinion finding feature (B), sentiment term tf-idf weight opinion finding feature on baseline3 provided by TREC.  TREC provided results according to topic relevance (runs appended with .topicrel in Table -2) and according to opinion relevance (runs appended with .opinion in Table-2). The average, MAX, Min and Med ∆ MAP represent the MAP changes between the baseline and submitted opinion finding run in order to see the effectiveness of the opinion finding features. Topic relevance was assessed against documents which were relevant, regardless of whether they contained opinions or not. Opinion relevance had to have both topic relevance and opinionated nature in measuring effectiveness. The results indicate the lack of taking the target into account in the re-ranking of the opinionated documents. The worst change in MAP seen for a given query was -99.52%, while the greatest increase in MAP was 380.00%. The greatest average change in MAP over the 150 queries was -62.54%. All three of these largest changes were attributed to the york08bo3b run, which incorporated the sentiment term tf-idf weight algorithm, solely for the opinionated terms found in the documents.</p><p>The greatest improvement of our opinion finding features of 268.59% and 380% came from queries 920 and 898. These queries are defined as "andrew coyne" and "Business Intelligence Resources" respectively. It is not yet clear as to why these particular queries yielded the best results, and further analysis needs to be performed.  Table-3 shows us the queries which yielded the best and worst results for our opinion finding features. We make note of what the queries themselves were for these results. Query 1031 was "Sew Fast Sew Easy", query 1002 was "Wikipedia primary source", query 1043 was "A Million Little Pieces" and finally, query 1030 was "System of a Down". We only looked at the best/worst queries from the new 2008 provided queries and not queries from previous years. For each of the queries listed, the results provided equal or lower values from the baseline runs.</p><p>Post-TREC results yielded a noticeable improvement over the submitted opinion finding runs. The difference is attributed to using a combination function between the opinion and topic relevance initial ranking, to come up with a new score for the given document. Table-4 shows these results using the term frequency method on baseline1 provided by TREC. The first row shows how the given baseline performed for a baseline comparison. We can see by using our combination function, that opinion retrieval was noticeably increased over the original submitted opinion finding run which did not combine the topic and opinion relevance. We can also see how york08bo2br has improved the opinion score over the baesline run itself. Since york08bo2br only assigned 25% weight on the opinion score, it looks like that was enough to give the boost to the opinionated documents to improve performance. Table-6 describes how all the runs are actually tuned and configured.</p><p>When re-running the combination function on the sentiment term TF*IDF opinion finding method, we only ran it against baseline1 and so we do not compare the results to the initially submitted runs. Instead we directly compare against the baseline we ran it against.  It seems evident that the sentiment term TF*IDF approach was not very effective in re-ranking documents from the initial adhoc topic retrieval run. The less weight that was given to the opinion score generated by this opinion finding technique, the better the score. This represents the fact that the baseline score itself was better than this approach. When comparing the two approaches, the simple sentiment term frequency method, in combination with the topic relevance runs, was able to improve on the baseline adhoc opinion retrieval method. Although the improvement was quite small, only about 1.3% improvement, it did not worsen the result like the other methods have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opinion Run Description york08bo1a</head><p>Term frequency calculation along with no combination methods york08bo2r</p><p>Term frequency with combination function, using a = 0.75 york08bo2br</p><p>Term frequency with combination function, using a = 0.25 york08bo2cr</p><p>Term frequency with combination function, using a = 0.50 york08bo3b</p><p>Sentiment term TF*IDF with no combination methods york08bo3r</p><p>Sentiment term TF*IDF with combination function, using a = 0.75 york08bo3ar</p><p>Sentiment term TF*IDF with combination function, using a = 0.25 york08bo3br</p><p>Sentiment term TF*IDF with combination function, using a = 0.50 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The contributions of our work are as follows. First, we implemented a robust, easy to extend and use information retrieval system, Compass IR, which uses the Okapi Basic Search System as the primary ranking engine. Second, we incorporated several techniques into Compass IR to provide clean baseline runs with which future adhoc retrieval and opinion finding techniques can be implemented. Adhoc retrieval baseline runs submitted were poor, however, later enhancements provided mediocre results using the BM25 weight function of Okapi. The sentiment term TF*IDF opinion retrieval technique degraded the overall opinion ranking results, meanwhile, the simpler sentiment term frequency method improved opinion retrieval in conjunction with a combination score function.</p><p>Future work should continue building on the Compass IR infrastructure to incorporate various new research methods. It is important to keep the task requirements clear; to find opinionated documents that are relevant for the given topic. To achieve this, proximity functions can be used to find out how many of the sentiment terms are in close range to the target terms themselves.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,202.46,542.97,207.08,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General approach to opinion retrieval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,108.84,126.57,179.25,7.89;5,108.84,137.53,202.36,7.89;5,108.84,159.45,39.39,7.89;5,123.02,170.43,263.00,7.86;5,123.02,181.39,200.85,7.86;5,137.19,192.35,152.67,7.86;5,137.19,203.31,151.44,7.86;5,137.19,214.27,173.98,7.86;5,137.19,225.23,357.10,7.86;5,123.02,236.19,227.44,7.86"><head>Input:</head><label></label><figDesc>Document number from baseline file Output: Document with newly calculated weight Method: (1) Lookup raw document text from one of distributed databases (2) Collect statistics about the document, namely (3) Number of terms in the document (4) Number of sentiment terms found (5) Number of target terms from the query (6) Positioning information collected of where sentiment/target terms reside positionally (7) Calculate new document weight given collected stats</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,146.76,260.06,318.48,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Gathering statistics used in determining final document weight</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,174.39,332.01,263.23,104.28"><head>Table 1 :</head><label>1</label><figDesc>Baseline topic relevance runs using BM25 function</figDesc><table coords="6,174.39,332.01,263.23,83.03"><row><cell>Baseline</cell><cell cols="2">MAP topicrel opinion</cell><cell cols="2">R-prec topicrel opinion</cell><cell cols="2">P@10 topicrel opinion</cell></row><row><cell>york08bb1</cell><cell>0.0509</cell><cell>0.0319</cell><cell>0.0973</cell><cell>0.0560</cell><cell>0.1207</cell><cell>0.0660</cell></row><row><cell>york08bb2</cell><cell>0.1830</cell><cell>0.1333</cell><cell>0.2660</cell><cell>0.2090</cell><cell>0.5547</cell><cell>0.3733</cell></row><row><cell>york08bb3</cell><cell>0.1041</cell><cell>0.0662</cell><cell>0.1622</cell><cell>0.0975</cell><cell>0.1907</cell><cell>0.0960</cell></row><row><cell>york08bb4</cell><cell>0.1639</cell><cell>0.1100</cell><cell>0.2500</cell><cell>0.1609</cell><cell>0.2373</cell><cell>0.1320</cell></row><row><cell>york08bb5</cell><cell>0.2257</cell><cell>0.1588</cell><cell>0.3118</cell><cell>0.2273</cell><cell>0.4607</cell><cell>0.2793</cell></row><row><cell>york08bb6</cell><cell>0.2704</cell><cell>0.1991</cell><cell>0.3463</cell><cell>0.2672</cell><cell>0.5620</cell><cell>0.3713</cell></row><row><cell>york08bb7</cell><cell>0.2855</cell><cell>0.2203</cell><cell>0.3530</cell><cell>0.2865</cell><cell>0.6053</cell><cell>0.4327</cell></row><row><cell>york08bb8</cell><cell>0.2976</cell><cell>0.2168</cell><cell>0.3626</cell><cell>0.2853</cell><cell>0.5917</cell><cell>0.4138</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,103.27,678.89,401.02,8.74"><head>Table 2 :</head><label>2</label><figDesc>MAP differences comparing Opinion ranking features vs. adhoc retrieval baselines.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,174.34,364.59,258.90,8.74"><head>Table 3 :</head><label>3</label><figDesc>Opinion feature results for best and worst queries.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,90.00,539.93,432.00,93.12"><head>Table 4 :</head><label>4</label><figDesc>Opinion finding feature results using Term Frequency approach on baseline1 over all 150 queries</figDesc><table coords="7,170.74,539.93,270.53,57.92"><row><cell>Baseline</cell><cell cols="2">MAP topicrel opinion</cell><cell cols="2">R-prec topicrel opinion</cell><cell cols="2">P@10 topicrel opinion</cell></row><row><cell>baseline1</cell><cell>0.3701</cell><cell>0.2639</cell><cell>0.4156</cell><cell>0.3189</cell><cell>0.7307</cell><cell>0.4753</cell></row><row><cell>york08bo1a</cell><cell>0.2994</cell><cell>0.2175</cell><cell>0.3738</cell><cell>0.2808</cell><cell>0.4867</cell><cell>0.3433</cell></row><row><cell>york08bo2r</cell><cell>0.3429</cell><cell>0.2543</cell><cell>0.4109</cell><cell>0.3177</cell><cell>0.5400</cell><cell>0.4127</cell></row><row><cell>york08bo2br</cell><cell>0.3657</cell><cell>0.2674</cell><cell>0.4206</cell><cell>0.3239</cell><cell>0.6680</cell><cell>0.5000</cell></row><row><cell>york08bo2cr</cell><cell>0.3572</cell><cell>0.2624</cell><cell>0.4187</cell><cell>0.3209</cell><cell>0.6067</cell><cell>0.4653</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,90.00,232.26,431.99,20.69"><head>Table 5 :</head><label>5</label><figDesc>Opinion finding feature results using Sentiment Term TF*IDF approach on baseline1 over all 150 queries</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,196.09,459.30,219.81,8.74"><head>Table 6 :</head><label>6</label><figDesc>Descriptions of Opinion Finding Features</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We would like to thank <rs type="person">Miao Wen</rs>, <rs type="person">Damon Sotoudeh-Hosseini</rs>, <rs type="person">Xiaoshi Yin</rs> and <rs type="person">Andrew MacFarlane</rs> for their support and generous suggestions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,212.37,411.52,8.74;9,110.48,224.32,255.23,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,482.73,212.37,39.27,8.74;9,110.48,224.32,33.92,8.74">Okapi at TREC-5</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,166.92,224.32,98.97,8.74">Proceedings of TREC-5</title>
		<meeting>TREC-5</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="143" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,244.25,411.52,8.74;9,110.48,256.20,183.75,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,238.74,244.25,239.43,8.74">Okapi Chinese Text Retrieval Experiments at TREC-6</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roberton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,502.89,244.25,19.11,8.74;9,110.48,256.20,83.93,8.74">Proceedings of TREC-6</title>
		<meeting>TREC-6</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,276.13,411.52,8.74;9,110.48,288.08,411.52,8.74;9,110.48,300.04,384.73,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,377.80,276.13,144.19,8.74;9,110.48,288.08,68.22,8.74">York University at TREC 2007: Genomics Track</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sotoudeh-Hosseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rohian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,395.37,288.08,126.63,8.74;9,110.48,300.04,163.37,8.74">The Sixteenth Text REtrieval Conference Proceedings (TREC 2007)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="500" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,319.96,411.52,8.74;9,110.48,331.92,304.60,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,319.17,319.96,202.83,8.74;9,110.48,331.92,71.52,8.74">A Platform for Okapi-based Contextual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,203.07,331.92,181.39,8.74">Proc. of the 29th ACM SIGIR Conference</title>
		<meeting>of the 29th ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,351.85,411.52,8.74;9,110.48,363.80,411.52,8.74;9,110.48,375.76,124.47,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,242.83,351.85,279.17,8.74;9,110.48,363.80,65.44,8.74">The TREC Blog06 Collection: Creating and Analysing a Blog Test Collection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,202.03,363.80,20.25,8.74">DCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,110.48,395.68,411.52,8.74;9,110.48,407.64,411.52,8.74;9,110.48,419.59,213.50,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,238.02,395.68,174.68,8.74">Overview of the TREC 2007 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,219.82,407.64,297.64,8.74">The Sixteenth Text REtrieval Conference Proceedings (TREC 2007)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="500" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,439.52,306.44,8.74;9,448.98,439.52,73.02,8.74;9,110.48,451.47,148.83,8.74" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Raha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Macfarlane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Walters</surname></persName>
		</author>
		<ptr target="http://htmlparser.sourceforge.net" />
		<title level="m" coord="9,448.98,439.52,68.68,8.74">HTML Parser</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,471.40,411.52,8.74;9,110.48,483.35,411.52,8.74;9,110.48,495.31,411.52,8.74;9,110.48,507.26,22.69,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,398.12,471.40,123.87,8.74;9,110.48,483.35,47.26,8.74">Overview of the TREC-2006 Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,392.06,483.35,129.94,8.74;9,110.48,495.31,167.00,8.74">The Fifteenth Text REtrieval Conference Proceedings (TREC 2006)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,527.19,411.52,8.74;9,110.48,539.14,333.36,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,235.23,527.19,160.12,8.74">Relevance Weighting of Search Terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sparck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,416.19,527.19,105.81,8.74;9,110.48,539.14,136.17,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,559.07,411.52,8.74;9,110.48,571.02,65.72,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,385.67,559.07,76.39,8.74">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,487.05,559.07,34.95,8.74;9,110.48,571.02,34.03,8.74">Proc. of TREC-3</title>
		<meeting>of TREC-3</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
