<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.26,148.86,346.47,15.15;1,210.88,170.78,181.24,15.15">TREC 2008 at the University at Buffalo: Legal and Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.06,203.31,82.43,10.48"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Library and Information Studies</orgName>
								<orgName type="institution">The State University of New York at Buffalo Buffalo</orgName>
								<address>
									<postCode>14260</postCode>
									<region>NY</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.14,203.31,47.80,10.48"><forename type="first">Ying</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Library and Information Studies</orgName>
								<orgName type="institution">The State University of New York at Buffalo Buffalo</orgName>
								<address>
									<postCode>14260</postCode>
									<region>NY</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.14,287.00,77.71,10.48"><forename type="first">Omar</forename><surname>Mukhtar</surname></persName>
							<email>omukhtar@buffalo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center of Excellence for Document Analysis and Recognition</orgName>
								<orgName type="institution">The State University of New York at Buffalo Buffalo</orgName>
								<address>
									<postCode>14260</postCode>
									<region>NY</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.52,287.00,72.35,10.48"><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
							<email>rohini@cedar.buffalo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center of Excellence for Document Analysis and Recognition</orgName>
								<orgName type="institution">The State University of New York at Buffalo Buffalo</orgName>
								<address>
									<postCode>14260</postCode>
									<region>NY</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.26,148.86,346.47,15.15;1,210.88,170.78,181.24,15.15">TREC 2008 at the University at Buffalo: Legal and Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">52FCCCA16154C7461981946E477086B3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the TREC 2008, the team from the State University of New York at Buffalo participated in the Legal track and the Blog track. For the Legal track, we worked on the interactive search task using the Web-based Legacy Tobacco Document Library Boolean search system. Our experiment achieved reasonable precision but suffered significantly from low recall. These results, together with the appealing and adjudication results, suggest that the concept of document relevance in legal e-discovery deserve further investigation. For the Blog distillation task, our official runs were based on a reduced document model in which only text from several most content-bearing fields were indexed. This approach indeed yielded encouraging retrieval effectiveness while significantly decreasing the index size. We also studied query independence/dependence and link-based features for finding relevant feeds. For the Blog opinion and polarity tasks, we mainly investigated the usefulness of opinionated words contained in the SentiGI lexicon. Our experiment results showed that the effectiveness of the technique is quite limited, indicating other more sophisticated techniques are needed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Interactive Legal Task</head><p>For the 2008 TREC Legal track, the University at Buffalo (UB) team participated in the interactive search task. The high level goal of the TREC Legal track is to advance computer technology on the effective search of electronic business records in the legal domain, a problem generally known as "legal e-discovery." The documents used in the track, the IIT Complex Document Information Processing (CDIP) Test Collection, contains nearly seven million business documents including correspondences, memos, publications, and regulations of tobacco products that are released under the "Master Settlement Agreement" (MSA). Two features of the collection are of particular interest: rich metadata fields that were produced by human catalogers and noisy text of full documents that were generated from some Optical Character Recognition (OCR) software. The search topics are several hypothetical "complaints" that are typically filed in court. Each complaint lays out the background of a legal case, including factual assertions and causes of action, as well as some specific requests for production of responsive documents. In practice, the main responsibility of a searcher, usually an e-discovery firm, is for each request for production to find as many responsive documents as possible while minimizing the number of false hits. For the Legal track search tasks, each participating team is free to use any of the document fields and any information contained in each complaint, thus leading to a variety of potential retrieval techniques and strategies. More information of the test collection can be found in the 2006 track overview paper <ref type="bibr" coords="2,117.98,147.89,9.96,8.74" target="#b0">[1]</ref>.</p><p>To focus research effort on different important aspects of the legal e-discovery task, the Legal track has three separate tasks: a routine ad hoc task, a feedback task, and an interactive task <ref type="bibr" coords="2,494.73,171.80,14.61,8.74" target="#b12">[13]</ref>. The UB team chose to work on the interactive search task for this year. In this paper, we describe our research interests, our search system and strategies, search outcomes, and the official evaluation including appealing and adjudication results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Areas of Interest</head><p>The challenges of legal e-discovery have been well-noticed in both the research and practice communities. However, the greatest success of search technology, namely the ranked retrieval algorithms and techniques, has not been widely applied to legal e-discovery, for which the typical approach is the Boolean search. The goal of our focus on the interactive Legal e-discovery task is two-fold: to gain a better understanding of the Boolean e-discovery practice and to develop better ranked retrieval techniques based on this understanding. In addition, we would also like to learn more about the document collection, something usually not easy to achieve with batch retrieval in which the human searcher is not directly involved. An interesting feature of this year's interactive Legal task is the inclusion of Topic Authorities (TAs). Specifically, each participating team had chances to clarify its understanding of document relevance for a topic, and TAs adjudicated the initial relevance judgments of documents that a participating team disagreed. Therefore, this year's interactive task is one step closer to model the real life practice of legal e-discovery, thus more interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Search System and Topic</head><p>The small size of our team and the time constraint prevented us from building a full-blown interactive search system of our own for the task. Therefore we studied several existing systems and eventually chose a Web-based Boolean search engine maintained by the Legacy Tobacco Document Library team at the University of California at San Francisco (UCSF)<ref type="foot" coords="2,403.29,477.91,3.97,6.12" target="#foot_0">1</ref> . The system contains a superset of the documents used in the Legal track. Some of the most important features of the system include:</p><p>• Three levels of search Users can select from basic search, advanced search, or expert search mode. The basic search mode is for search with one term on the entire document or the searchable fields (Title, Author, Bates Number, Document Type, Entire Record, Metadata, and Text body). The advanced search mode supports Boolean query formulation with up to six search terms. With the Expert Search interface users can create more complex Boolean queries using mixed operators and wildcards while searching on any field in the collection. We used the expert search interface to generate all the queries and the corresponding official runs reported in this paper.</p><p>• Search on individual fields or the whole document Such flexibility turned out to be very useful. For example, searchers can easily construct queries that will return or exclude documents that contain or do not contain certain terms in their Title field. Also, this makes it easy to search based on dates.</p><p>• Search with Wildcards This feature makes it easy to specify variants of query terms and proximity search.</p><p>• Full document image Full documents in PDF format and TIFF format are provided by the system, which facilitates the review of retrieved documents.</p><p>• Search history and bookbag The system automatically keeps search history (queries and the number of retrieved documents for each query) for a login session, and the search history can be easily downloaded to a local computer. The "bookbag" keeps documents that a searcher has selected. The searcher can also write notes about any document in the bookbag. The bookbag needs to be downloaded before a search session ends.</p><p>We used all these features of the system quite frequently. Initially we encountered one problem with the "bookbag" feature. Since the default Web interface of the UCSF search system allows to display up to only 50 documents per page, marking relevant documents (so that their document IDs can be saved in the bookbag) can be very time-consuming if the number of retrieved documents is large. Upon our request, the system staff provided an API command that allows to change the number of documents per page up to 10,000. Given the number of documents we retrieved, it only requires a few pages to display them.</p><p>Three topics, together with the complaint that these topics are based on, were provided for the interactive search task. The complaint describes a hypothetical legal case in which a company is accused of providing false information to its security customers. Specifically, the company is suspected to have been involved in illegal domestic retail marketing practice of cigarettes and payment to foreign officials. Consequently, the three requests for production seek documents that talk about (1) marketing or advertising restrictions, (2) retail marketing campaigns for cigarettes, and (3) payments to foreign government officials. Due to constraints on the task schedule and the availability of our searchers, we were able to work only on Topic 103 for our official submission, which is the topic of retail marketing campaigns for cigarettes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Query Formulation</head><p>The request for production of Topic 103 states that relevant documents should "describe, refer to, report on, or mention any "in-store," "on-counter," "point of sale," or other retail marketing campaigns for cigarettes." At the beginning we thought this is an easy search request but as we worked on it we realized it is indeed difficult. One of the reasons is that a relevant document may not use any of the terms in the request, except "cigarette." Our understanding is that in-store, on-counter, and point-of-sale retail marketing of cigarettes are just examples of relevant retail marketing activities -there are other forms of retail marketing campaigns that can be relevant, such as during concerts, sports, and even in streets. For example, a document that describes sales representatives of a tobacco company distribute free cigarette samples on a car racing event is deemed to be relevant, while it never uses any of the other words in the request for production. Our clarification with the TA confirmed our speculation. Therefore, terms like "in-store," "oncounter," "point of sale," and "retail marketing" should be ORed with "cigarette" if they are to appear in a Boolean query.</p><p>On the other hand, documents mentioning those words in the topic statement may not be relevant. For example, a document would be non-relevant if it discusses only the qualification of some retail stores for becoming a member of the Retail Partners Program of a tobacco company without mentioning the consumer. The TA explained that "I think the communication would need to refer or relate or imply some distribution to customers and a particular program or campaign to be responsive."</p><p>Many documents that we initially found describe federal, state, local, and company rules, codes, regulations, etc. that apply to retail marketing of cigarettes. However, it is somewhat difficult to judge the relevance of these documents. The TA clarified that the relevance of such documents should be decided based on whether they are general to all retail marketing activities or specifically about some individual events. She explained that "I think as a general matter, if the document was generated by the government and discusses rules or regulations relating to free samples of cigarettes, I would not consider this alone to be responsive to the Topic 103 request for documents relating to retail marketing campaigns. If the document was generated by a cigarette manufacturer and said that in connection with providing free samples of Merit or Pall Mall, we must follow the attached rules, I think it would be responsive. If the document was sent internally at Philip Morris to the entire Marketing Department and said "Attached are the new government rules regarding free cigarette samples," I would probably err on the side of deeming that responsive." From these conversations, it seems that we should excluded documents of general rules, codes, regulations, and their like, but the fine line is hard to maintain.</p><p>For the submitted run, two faculty members worked on the search task in two weeks, of course with other teaching and service responsibilities going on at the time. We did not accurately keep track of the time spent on the task. Our plan was that each of us focused on different types of potentially relevant documents and then merged the search results. Here are the five queries that led to our official runs:</p><p>1. UBQ01: (cigarette* AND ("off package"∼2 OR "off carton"∼2 OR "off pack"∼2) NOT (ebay)) The first four queries are generated by one team member with the expectation to retrieve potentially relevant documents mentioning some specific types of retail campaign events. The types of events focused in the queries are: store sale, giveaways, and gifts with purchase. Each query has three clauses: (1) cigarette and its spelling variants; (2) expressions carrying the meaning of one or two types of retail campaign events; and (3) a not clause to exclude some false positive documents. For each type of event, the searcher came up with possible expressions and only the expressions that have proved to be present in the collection are kept in the final queries. For the purpose of accessing query qualities, the queries are not defined neatly as one query per type of event, but quite ad-hoc with a general rule that the searcher feels comfortable to manipulate the number of documents retrieved. The NOT clause is added after looking at a small sample of documents retrieved by a query containing only the first two clauses. The TA was consulted to verify some sample documents.</p><p>We found a large amount of scanned coupons in the collection with very few texts in them. However the texts follow the general pattern as "certain amount off per/every/a package/carton/pack." The TA gave a positive answer to such documents. Query UBQ01 is generated to catch such documents. Query UBQ03 is about free gift or sample giveaway events. A lot of efforts were made to exclude non-responsive documents with this query. Query UBQ04 uses the general concept terms for store sale and gift with purchase events. The query is used to target documents talking about the events in general, such as a marketing report or general store sale instructions. Query UBQ02 is also about the store sale events where "discount" is mentioned. We limited the appearance of the term "discount" somewhere near "campaign" or "promotion" based on our initial understanding that the responsive documents should mentioned some specific events. However, a last-minute contact with the TA proved that the understanding is incorrect, which may be one of the reasons leading to our low recall results. Query UBQ05 was generated by another team member. It requires the presence of the words "distribute," "cigarette," and either of the words "sample" and "coupon" (or their variants). In addition, it requires relevant documents not to be regulations and their likes, as restricted through the "document type" (dt) field, or document titles not to contain these words. The proximity operator ∼ specifies the window size for words to appear together in a document. We tried several different window sizes and felt the window size of five can bring in a reasonable number of relevant documents. Of course, this heuristics is only based on some sample documents. Given the low recall of our results, we suspect that we should have probably set the window size larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query ID Rtrd Sampled Rel(I) Non Rel(I) A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Official Evaluation Results</head><p>Table <ref type="table" coords="5,118.92,416.02,4.98,8.74" target="#tab_1">1</ref> shows the statistics of our submitted run, which merges documents retrieved by the five queries. We can see that among all the five queries, UBQ01 and UBQ05 retrieved about the same number of documents, which is much more than the other three queries. It's worth mentioning that there is very little overlap between these two sets of documents, suggesting the queries pulled out documents from different categories or types. Our merged (official) run contains 67,334 documents. Only less than 7% of these documents were sampled for official review. The effectiveness of our run is 4.7% for recall, 63.4% for precision, and 8.7% for F measure (before appealing and adjudication). Both the balanced F measure and recall are very low for our run, indicating we missed lots of relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Results after Appealing and Adjudication</head><p>An interesting and important feature of this year's interactive search task is appealing and adjudication after the initial relevance assessment results were released. Each participating team was given an opportunity to question the relevance judgment of any of the sampled documents that the team disagreed. The TA would further examine the appealed documents and made a final assessment of their relevance. Appealing and adjudication was done after the TREC conference. Our team looked particularly at those documents that were initially judged as non-relevant and selected 40 of them that we felt representing different types of documents that we have retrieved. In other words, there are more documents that we wished to be adjudicated, but we believed that once we had the adjudicated results of these 40 documents we would have a better understanding of the true relevance of other documents that are similar to the adjudicated documents. Of course, another reason for us to appeal only a relatively small set of documents for adjudication is we did not want the TA to spend too much time on similar documents.</p><p>Table <ref type="table" coords="5,132.80,701.40,4.98,8.74" target="#tab_1">1</ref> also shows, for each of the five queries, the number of documents that were initially judged as non-relevant and we appealed for adjudication. There were six appealed documents that were retrieved by both UBQ01 and UBQ05, hence 40 unique documents for adjudication. As we can see, among these 40 documents (again, initially judged as non-relevant), 35 were adjudicated by the TA as relevant, showing there was some noticeable discrepancy of relevance judgments between the assessor and the topic authority. Adjusting the initial official evaluation results by taking into consideration the appealing and adjudication results, the estimated recall, precision, and F measure of our submitted run are boosted to 6.1%, 71.6%, and 11.3%, respectively.</p><p>We believe that the track overview paper will provide a more detailed analysis of the process and results of appealing and adjudication. From our perspective, however, the results, together with our experience of interacting with the topic authority, make us believe that relevance assessment of documents for legal e-discovery is a very challenging task. The challenge could be due to several factors. One possible factor is that the concept of document "relevance" in the legal e-discovery domain does not seem to be quite the same as in other domains (e.g., with newswires stories).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Conclusions for the Interactive Legal Task</head><p>At the first glance, our interactive legal search results do not seem to be very encouraging. However, we have learned a lot than what these recall, precision, and F measures show. Through our participation in the task, we have become more familiar with the actual legal e-discovery process. The involvement of the topic authority in this experimental evaluation significantly helped us to understand that process as well as the concept of document relevance in the legal e-discovery domain. We also learned the usefulness of utilizing certain document features such as document types and specific fields, and query formulation techniques such as proximity search, wildcards, and relevance feedback to improve the search performance. Our immediate next step would be to study how these techniques can be automated so that they become part of some text analysis, indexing, and searching algorithms. We feel that the concept of document relevance in legal ediscovery deserves further investigation and understanding. One possible direction is to look at the different types of relevance <ref type="bibr" coords="6,231.45,397.40,9.96,8.74" target="#b7">[8]</ref>. Therefore, we look forward to continuing our effort in the TREC 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Blog Distillation Task</head><p>Blog Distillation or "feed search" is the task of finding blog feeds that have a principal recurring interest in a topic X, where X is an information need expressed as a short query (title), a short sentence(description) and a short paragraph(narrative). The goal is to return a ranked list of feeds that are most relevant to the expressed information need.</p><p>The document collection contains three sets of documents: 88.8 GB permalink documents, 28.2GB homepages, and 38.6GB RSS and ATOM feeds. We used the permalink collection which is organized as 3.2 million documents. In the previous two years' Blog track, groups have looked at the so-called large document model in which all documents from a feed are used to make a single document that represents that feed <ref type="bibr" coords="6,251.16,559.76,14.61,8.74" target="#b10">[11]</ref>. An alternative model is to execute the query against the individual pages and then use a ranking method that scores feeds based on the ranking of the individual pages, hence, the small document model <ref type="bibr" coords="6,332.90,583.67,9.96,8.74" target="#b3">[4]</ref>. We choose the large document model because it has shown to work better than the small document model. We also looked at the reduced document model in which text between certain tags is indexed (described below). Another approach we tried was to integrate the PageRank of a feed with its query likelihood score. For all our experiments we used the Indri retrieval engine as well as the Lemur toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus Processing</head><p>A FEEDNO tag is included in the DOCHDR section of each permalink document, indicating which feed it belongs to. With that information, we were able to aggregate all documents with the same FEEDNO in the permalink collection into one document. There are about 100K individual feeds so we created an index with about 100K documents. We initially built two indices, one with stop-words removed and the other without. Our preliminary experiments showed that removing stop words seemed to help improve retrieval effectiveness. Therefore, our official runs were based on document indices with stop words removed.</p><p>For the reduced documents model, we built three separate indices: (1) Title (T) only index, by indexing only the text in the TITLE field, (2) Title and Heading (TH) index, by taking only the text in the TITLE and the heading fields(H1, H2, and H3), and (3) Title, Heading and Paragraph (THP) index, by taking the text in the TITLE, H1, H2, H3, and P fields. We did not remove any stop words in the reduced document model but did use Krovetz stemming.</p><p>We did not use any technique to remove non-English or spam blogs. According to statistics of the 2007 Blogosphere<ref type="foot" coords="7,199.31,206.09,3.97,6.12" target="#foot_1">2</ref> , only 36% of the blogs are English. Our motivation for not removing non-English blogs is that we want to see how robust our approaches are in dealing with English Blogs mixed with non-English ones. We note, however, that excluding non-English and spam blogs could improve performance on Blog Distillation <ref type="bibr" coords="7,325.97,243.53,14.61,8.74" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Construction</head><p>A retrieval engine will perform only as well as the quality of the words that are used to express an information need. Query expansion techniques such as relevance feedback (including pseudo relevance feedback), WordNet association, and Wikipedia term association are all ways to enhance the quality and clarity of the information need. We performed experiments on different methods of combining words in title, description and narrative. We used last year's relevance judgments and Mean Average Precision(MAP) as the metric to test the performance of our techniques.</p><p>For query construction, our baseline run was produced with queries containing only the words from the title field. We looked at the MAP obtained from running the full title, description and narrative and their combinations. In our experiments removing all stop words from the title, description and narrative improved performance. More complicated techniques such as, automatically extracting the most important words by finding the capitalized words, phrases such as "University at Buffalo" where "at" is a connective between two capitalized words, showed further improvement. The addition of lower case words did not improve MAP or R-Precision but we did get the highest P@10 when we take the title, capital words and lower case words. (see Table <ref type="table" coords="7,117.23,457.18,3.87,8.74" target="#tab_2">2</ref>. The pre-processing we did is similar to query expansion in that we were trying to clarify the information need of the user. The similarity between our approach and query expansion is that our aim is to add words to the short original query to clarify the information need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Documenting Ranking Techniques</head><p>We tried three document ranking techniques in our experiments, namely, query likelihood and PageRank, support vector machine, and reduced document model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Query Likelihood and PageRank</head><p>We first looked at whether combining the query likelihood and a popularity metric improves performance on the blog distillation task. The popularity metric we used is PageRank <ref type="bibr" coords="7,444.93,595.56,9.96,8.74" target="#b8">[9]</ref>. Specifically, we used the link structure within the document collection to calculate the PageRank. To calculate the PageRank values of the feeds we used Lemur's PageRank utility.</p><p>We first calculated the PageRank of the individual feeds. Then we used a linear combination of the PageRank and the query likelihood score to rank the feeds. There were two parameters that we needed to learn, the relative weights to be given to the query likelihood and the PageRank of the feeds. Another interesting thing we found was that only 27K blogs out of 100K blogs had a PageRank i.e. 72K feeds had a PageRank of zero. The distribution of the PageRank's of the feeds are given in table 3. We tried different combinations of these scores but found that using the PageRank degraded performance unless the weight of the PageRank was set so low (less than 0.01) so that it would be almost ignored. In conclusion, we found that using the PageRank did not improve performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Support Vector Machine</head><p>Another system we built was based on Support Vector Machines(SVM). For this purpose we used the LibSVM toolkit <ref type="bibr" coords="8,178.72,471.71,9.96,8.74" target="#b2">[3]</ref>. We took the qrels from last year, extract features for each feed, and then use SVM to classify the feeds. The features that we used to train our SVM include: PageRank of a feed within the permalink collection, the number of documents in the feed, the length of the feed document, the average document length in the feed, the number of times, each word in query title is found, and the number of times, the title of the query is found (Phrase Matches). As we can see, some of the features rely on the relevance judgments from last year's track.</p><p>The performance metric we used was the percentage of correctly identified feeds in the testing set. The distribution of relevant/irrelevant feeds in the relevance judgment file is skewed in favor of irrelevant feeds. To overcome this problem, we penalized our SVM whenever it misclassified a relevant feed. Our best performance using SVM and the features itemized above was 62%. If one looks at documents belonging to a feed, there is a certain structure to them. The Blog (Feed) title is repeated on each page, there are links to similar blogs (blog roll), there is an archive of previous posts, there is the main "post" and finally the "comments." The archive, blog roll, and feed title do not change much and are almost static, while the "post" and the "comment" changes in each document. So there are static (or almost static elements) and dynamic content on each documents belonging to a feed. For the distillation task, our goal is to find out whether posts or comments to a post from a feed have a "principal recurring interest in X (information need)."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Reduced Document</head><p>We used the reduced document model to filter out the non-relevant elements of a feed. Instead of labeling different HTML tags from a feed as relevant/irrelevant, we used a heuristic method in which texts within certain HTML tags is indexed while texts within other tags is simply ignored. The tags we chose to index were the TIELE, H1, H2, H3, and P tags. Our approach is much simpler than the approach taken by <ref type="bibr" coords="9,216.50,255.48,10.52,8.74" target="#b5">[6]</ref> and <ref type="bibr" coords="9,250.59,255.48,9.96,8.74" target="#b6">[7]</ref>. The former group's approach is based on VIPS (Vision based Page Segmentation) algorithm <ref type="bibr" coords="9,256.50,267.44,10.52,8.74" target="#b1">[2]</ref> for page segmentation. They used layout features and language features to identify important content blocks. The latter group used content extraction on the Opinion track but their approach is also based on page segmentation and uniqueness of text within tags. <ref type="bibr" coords="9,167.74,303.30,15.50,8.74" target="#b13">[14]</ref> also preprocessed the permalink collection for the opinion retrieval task to remove the script and style information in documents, which led to a reduction of 80% in page size (from 88GB to about 18GB).</p><p>The motivation for the reduced document model is to see the effect of index reduction on performance and to simplify the filtering of document elements. Both techniques (VIPS and Uniqueness of text) are computation-intensive, and might not be suitable for real time filtering. Another motivation is to mimic what people do when they subscribe to blogs. People seldom read every single document from a feed. Instead, they perform a search based on their information need and look at the titles and snippets to decide if the feed might be relevant and worth subscribing to. Table <ref type="table" coords="9,117.61,410.90,4.98,8.74" target="#tab_4">4</ref> shows the reduction in index as well as the performance penalty of index size reduction. We used the queries and relevance judgments from last year to measure performance. We achieved a reduction in index size to 0.01% with a performance penalty of -18%. However, we received a 10% boost in P@10. For the heading and paragraph indices we observed similar trends but no such improvement in P@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experiments and Results</head><p>Since we got disappointing results from the combination of query likelihood and PageRank feature, we did not submit any run based on this technique. With SVM, we used classification errors as our metric. Therefore, all our submissions are based on query likelihood on the feed index as well as the reduced document model. For our baseline, we used the title only query on the feed index. For other submissions, we built the query from the title and the capital words as explained in the query construction section above. All evaluations are on the 50 new queries proposed this year. 5 of those were proposed by our group.</p><p>The MAP, R-Precision and P@10 for our submitted runs are given in Table <ref type="table" coords="9,444.25,588.68,3.87,8.74" target="#tab_5">5</ref>. Our baseline run is UBDist1 which is a title only run on the full index. Our second run is UBDist2, for this run, we built the query from title, the index we used is the title, heading and paragraph index. We see a drop in MAP, R-Precision and P@10 although the performance was not as severe as observed on the queries from last year (Table <ref type="table" coords="9,234.97,636.50,3.87,8.74" target="#tab_4">4</ref>). This could be because of the change in nature or the queries themselves. Our third run is UBDist3, which is produced with queries built from the title words, capital and lower case words and index with the title, heading and paragraph. For our final run UBDist4, we built the query from title and capital words and again used the title, heading and paragraph index. We achieved the best performance with this run, which is an improvement of 9% in MAP, 8% in R-precision and 2% in P@10 over our baseline run. The Opinion retrieval task involves locating blog posts that express an opinion about a given target. It can be summarized as, "What do people think about "target" where the target is an information need." Thus, the Opinion task is a subjective task. Each search topic in this task is about a person, a location, an organization, a product, or an event. The topic of a post does not necessarily have to be the target, but an opinion about the target must be present in the post or one of the comments to the post in order for the blog to be considered relevant. The goal of the Polarity task is, for each topic, all positive opinionated documents and negative opinionated documents should be ranked and retrieved. This task is very similar to the Opinion task with a small difference: in the Polarity task no documents that have both negative and positive opinion about a target should be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Since both tasks are subjective, we need some human judgments on words that are considered opinionated (positive or negative or objective). That is, a lexicon with words labeled as positive, negative or objective is needed. In our experiment, we used SentiGI lexicon <ref type="bibr" coords="10,414.57,446.50,9.96,8.74" target="#b4">[5]</ref>, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lexicon</head><p>The SentiGI lexicon is based on the General Inquirer (GI) lexicon <ref type="bibr" coords="10,384.03,492.78,14.61,8.74" target="#b11">[12]</ref>. The GI lexicon contains positive and negative words. The SentiGI lexicon adds a new category of objective words. The SentiGI positive and negative word lists are made from 1,612/1,982 terms obtained by two original GI sets of 1,915/2,291 terms after removing 17 terms appearing in both categories (e.g. deal). For the Objective category, the 7,582 GI terms that are not labeled as either Positive or Negative, are labeled as Objective. Then all the multiple entries of a single term caused by multiple senses are reduced to a single entry. After this reduction we have 5,009 objective terms. We used this list of positive, negative and objective words in our experiment. We trained our neural network based on the relative word distribution on last years judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Opinion Task</head><p>To train our neural network we used the following features: (1) document length, (2) number of positive words, (3) number of negative words, (4) number of objective words, (5) ratio of positive words to total number of words in document (after stop word removal, same below), (6) ratio of negative words to total number of words in document, and (7) ratio of objective words to total number of words in document.</p><p>After training, we first ran the title-only query and got an initial set of documents. Since for this task, 1000 documents had to be returned and some documents may not have any opinion or might be rejected, our initial set was made of the top 2000 documents returned for a query. We then used the neural network to obtain a class label (Opinionated or Unopinionated) for the document. For the Opinion task the class labels were Positive, Negative, Mixed and Unopinionated. If a UB Runs MAP R-Prec P@10 UBpol1 (positive) 0.0666 0.0785 0.0906 UBneg1 (negative) 0.0529 0.0686 0.0725 document is judged to be positive, negative or objective, we take a linear combination of its initial relevance and the probability assigned to the label (positive, negative or objective). We finally reranked the documents based on this modified score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Polarity Task</head><p>Unlike the Opinion task, documents of mixed opinion should be excluded from the returned set. Thus we did not need to retrain our neural network and needed to make only some minor changes. The class labels were Positive, Negative, Mixed or Objective and Unopinionated. If a document was judged to have mixed opinion, we rejected it. All documents ranked positive were separated from the the documents ranked negative. We then output the final ranked set of positive and negative documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiments and Results</head><p>For testing we used the Indri query language; our query looks like the following #1(M archof theP enguins)#uw(M archP enguins)</p><p>where "#(March of the Penguins)" was taken from the title of the query and executed as a phrase query, and the words "March" and "Penguins" are extracted from title after removing stop words from the query itself. We use Indri's built-in Language Modeling (query likelihood) ranking method to rank the documents.</p><p>We found that there was a high correlation between relevant blogs and opinionated blogs. Indeed, applying opinion mining degraded the performance (see Table <ref type="table" coords="11,398.01,556.29,3.87,8.74" target="#tab_8">8</ref>). Such degradation after processing was also observed by a few other teams in last year Opinion Mining task (e.g., <ref type="bibr" coords="11,490.86,568.24,14.76,8.74" target="#b9">[10]</ref>). Our hypothesis for the drop in performance is that most blogs are opinionated, thus this kind of shallow opinion mining could have filtered out some relevant blogs. In that table, UB is our baseline submission, for which we used the title as our query and the default query likelihood model in Indri to rank documents. For UBop1, we used the title as our query, got an initial set of documents, and used our neural network to rerank these documents.</p><p>Table <ref type="table" coords="11,131.98,639.97,4.98,8.74" target="#tab_7">7</ref> shows the result for the polarity task. From the results we see that the performance of our system is better on positive opinionated documents than on negative opinionated documents. We used the title as our query, got an initial set of documents, and used our neural network trained to filter out blogs with mixed opinions. The reranked list of positive opinionated blogs was submitted as UBpol1 and the list of negative opinionated blogs was submitted as UBneg1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Conclusions for the Blog tasks</head><p>For the Feed Distillation task, we applied link-based feature PageRank in combination with query likelihood model but the combination did not work well. We also studied query independent features on an Support Vector Machine classifier. Our official submission, however, was based on the reduced document model in which text between certain tags was indexed. We got encouraging results and improved on our baseline while achieving a reduction in index size.</p><p>For the Opinion and Polarity task, we used the SentiGI lexicon to find opinionated words in a document. We trained a Bayesian Neural Network using the relevance judgments from the last two years. The primary feature we used was the distribution of the positive, negative and objective words in opinionated documents. Our results are not very encouraging, suggesting that simple word distribution doesn't capture opinions about an information need. The same can be said about ranking documents that are positive or negative about an information need, suggesting more sophisticated techniques based on deeper lexical and semantical analysis of documents need to be considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,110.82,423.00,152.14"><head>Table 1 :</head><label>1</label><figDesc>Run statistics: Initial and Post-Adjudication. Rtrd : the number of documents retrieved by each query; Sampled: the number of retrieved documents that were sampled for the official relevance judgment; Rel(I): the number of our sampled documents that were initially judged as relevant; Non Rel(I): the number of our sampled documents that were initially judged as non-relevant; A A: the number of Non Rel(I) that we appealed for adjudication; A A Rel : the number of A A that were adjudicated as relevant.</figDesc><table coords="5,397.24,110.82,56.07,8.74"><row><cell>A A A Rel</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,110.79,423.00,285.62"><head>Table 2 :</head><label>2</label><figDesc>The effect of query construction on different indices. Evaluation was on the Blog Distillation Relevance Judgments from last year. T is Title, C is capital words, H is heading, P is paragraph, S is stop words, wd is words , D is description, N is narrative , L is lower case words, x is except.</figDesc><table coords="8,95.98,110.79,414.15,285.62"><row><cell>Query Fields</cell><cell>Index used</cell><cell cols="3">Processing</cell><cell></cell><cell cols="3">MAP R-Precision P@10</cell></row><row><cell>T</cell><cell>T</cell><cell>none</cell><cell></cell><cell></cell><cell></cell><cell>0.2012</cell><cell>0.2705</cell><cell>0.4541</cell></row><row><cell>T</cell><cell>T &amp; H</cell><cell>none</cell><cell></cell><cell></cell><cell></cell><cell>0.1837</cell><cell>0.2626</cell><cell>0.4054</cell></row><row><cell>T</cell><cell>T, H &amp; P</cell><cell>none</cell><cell></cell><cell></cell><cell></cell><cell>0.2075</cell><cell>0.2887</cell><cell>0.3895</cell></row><row><cell>T</cell><cell>Full</cell><cell>none</cell><cell></cell><cell></cell><cell></cell><cell>0.2430</cell><cell>0.3167</cell><cell>0.4103</cell></row><row><cell>T &amp; C wd</cell><cell>T only</cell><cell cols="3">T, C from D &amp; N</cell><cell></cell><cell>0.2432</cell><cell>0.3225</cell><cell>0.4711</cell></row><row><cell>T &amp; C wd</cell><cell>T &amp; Hd</cell><cell cols="3">T, C from D &amp; N</cell><cell></cell><cell>0.2249</cell><cell>0.3027</cell><cell>0.4156</cell></row><row><cell>T &amp; C wd</cell><cell>T, Hd &amp; P</cell><cell cols="3">T, C from D &amp; N</cell><cell></cell><cell>0.2416</cell><cell>0.3229</cell><cell>0.4244</cell></row><row><cell>T &amp; C wd</cell><cell>Full</cell><cell cols="3">T, C from D &amp; N</cell><cell></cell><cell>0.2915</cell><cell>0.3700</cell><cell>0.4400</cell></row><row><cell>T ,C &amp; L</cell><cell>Full</cell><cell cols="5">T, C from D &amp; N, S 0.2890</cell><cell>0.3594</cell><cell>0.4636</cell></row><row><cell>T ,C &amp; L</cell><cell>T</cell><cell cols="4">T, C from D &amp; N,</cell><cell>0.2483</cell><cell>0.3333</cell><cell>0.4955</cell></row><row><cell></cell><cell></cell><cell cols="2">all wd x S</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T ,C &amp; L</cell><cell>T &amp; H</cell><cell cols="4">T, C from D &amp; N,</cell><cell>0.2171</cell><cell>0.2856</cell><cell>0.4455</cell></row><row><cell></cell><cell></cell><cell cols="2">all wd x S</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T ,C &amp; L</cell><cell>T, H &amp; P</cell><cell cols="4">T, C from D &amp; N,</cell><cell>0.2477</cell><cell>0.3239</cell><cell>0.4705</cell></row><row><cell></cell><cell></cell><cell cols="2">all wd x S</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PageRank 10 9 8 7</cell><cell>6</cell><cell>5</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell cols="9">Frequency 1 3 9 25 69 189 517 1,415 3,866 17,303 77,252</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,234.14,409.51,134.72,8.77"><head>Table 3 :</head><label>3</label><figDesc>PageRank of Feeds</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,90.00,605.17,423.00,131.11"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table coords="8,111.61,605.17,379.78,85.34"><row><cell></cell><cell cols="2">Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fields Indexed</cell><cell>Size</cell><cell cols="4">Avg. Doc. Length MAP R-Precision P@10</cell></row><row><cell>Full</cell><cell>24.3GB</cell><cell>16,000</cell><cell>0.2430</cell><cell>0.3167</cell><cell>0.4103</cell></row><row><cell>T</cell><cell>236MB</cell><cell>235</cell><cell>0.2012</cell><cell>0.2795</cell><cell>0.4541</cell></row><row><cell>TH</cell><cell>922MB</cell><cell>1,356</cell><cell>0.1837</cell><cell>0.2626</cell><cell>0.4054</cell></row><row><cell>THP</cell><cell>6.58GB</cell><cell>7,634</cell><cell>0.2075</cell><cell>0.2887</cell><cell>0.3895</cell></row></table><note coords="8,128.87,703.63,384.12,8.74;8,90.00,715.59,423.00,8.74;8,90.00,727.54,111.55,8.74"><p>Index Sizes. T is Title-only index created from words inside TITLE field of a webpage, H is heading index created from words inside H1, H2, or H3 fields, and P is paragraph index created from words inside P field.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,149.17,110.79,304.66,125.47"><head>Table 5 :</head><label>5</label><figDesc>Performance of official runs on Feed Distillation task</figDesc><table coords="10,207.99,110.79,187.01,125.47"><row><cell></cell><cell cols="3">MAP R-Prec P@10</cell></row><row><cell cols="2">UBDist1 0.2410</cell><cell>0.2916</cell><cell>0.3720</cell></row><row><cell cols="2">UBDist2 0.2348</cell><cell>0.2949</cell><cell>0.3640</cell></row><row><cell cols="2">UBDist3 0.2304</cell><cell>0.2951</cell><cell>0.3460</cell></row><row><cell cols="4">UBDist4 0.2633 0.3160 0.3820</cell></row><row><cell>UB Runs</cell><cell cols="3">MAP R-Prec P@10</cell></row><row><cell cols="2">UB (baseline) 0.1700</cell><cell>0.2267</cell><cell>0.3793</cell></row><row><cell>UBop1</cell><cell>0.1570</cell><cell>0.1925</cell><cell>0.3093</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,90.00,249.36,324.65,41.44"><head>Table 6 :</head><label>6</label><figDesc>Performance on Opinion Mining task 3 Opinion and Polarity Task</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,176.66,157.36,249.67,55.25"><head>Table 7 :</head><label>7</label><figDesc>Performance on Polarity task</figDesc><table coords="11,176.66,179.14,249.67,33.47"><row><cell>Method</cell><cell cols="4">QRel MAP R-Prec P@10</cell></row><row><cell>A</cell><cell>OQ</cell><cell>0.3476</cell><cell>0.3803</cell><cell>0.6250</cell></row><row><cell>A with O</cell><cell>AQ</cell><cell>0.2698</cell><cell>0.3471</cell><cell>0.6208</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,90.00,225.74,423.00,32.65"><head>Table 8 :</head><label>8</label><figDesc>Performance on Opinion Mining Task with and without opinion mining. A means Ad-hoc retrieval techniques used, O means Opinion mining technique used, OQ means 2007 Opinion qrel, AQ means 2006 Ad-hoc retrieval.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,747.00,111.56,6.99"><p>http://legacy.library.ucsf.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,105.24,746.00,187.41,6.99"><p>http://www.sifry.com/alerts/archives/000493.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,110.48,285.61,402.52,8.74;12,110.48,297.57,348.31,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,366.88,285.61,141.63,8.74">TREC 2006 legal track overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,122.93,297.57,206.98,8.74">The Sixteenth Text REtrieval Conference TREC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,315.81,402.52,8.74;12,110.48,327.77,350.84,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,362.66,315.81,150.34,8.74;12,110.48,327.77,69.71,8.74">Vips: a vision-based page segmentation algorithm</title>
		<author>
			<persName coords=""><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno>MSR-TR-2003-79</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="12,110.48,346.01,402.52,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,279.92,346.01,202.60,8.74">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,364.25,402.52,8.74;12,110.48,376.21,270.03,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,414.12,364.25,98.88,8.74;12,110.48,376.21,217.32,8.74">Retrieval and feedback models for blog distillation. Proceedings of TREC</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,394.45,402.52,8.74;12,110.48,406.40,402.52,8.74;12,110.48,418.36,106.54,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,173.23,394.45,339.77,8.74;12,110.48,406.40,393.74,8.74">Automatic Generation of Lexical Resources for Opinion Mining: Models, Algorithms and Applications. PhD in Information Engineering, PhD School &quot;Leonardo da Vinci</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of Pisa</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,436.60,402.52,8.74;12,110.48,448.55,92.70,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,336.27,436.60,176.73,8.74;12,110.48,448.55,40.00,8.74">Nlpr in trec 2007 blog track. Proceedings of TREC</title>
		<author>
			<persName coords=""><forename type="first">Liu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang</forename><surname>Gen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Xianpei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhao</forename><surname>Jun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,466.80,402.52,8.74;12,110.48,478.75,402.52,8.74;12,110.48,490.71,22.69,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,147.01,478.75,338.78,8.74">Experiments in trec 2007 blog opinion task at cas-ict. Proceedings of TREC</title>
		<author>
			<persName coords=""><forename type="first">Xiangwen</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Songbro</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,508.95,402.53,8.74;12,110.48,520.90,402.52,8.74;12,110.48,532.86,287.58,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,356.74,520.90,156.26,8.74;12,110.48,532.86,132.97,8.74">Overview of the CLEF-2006 crosslanguage speech retrieval track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dagobert</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoli</forename><surname>Soergel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shafran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,264.47,532.86,102.25,8.74">Proceedings of CLEF&apos;06</title>
		<meeting>CLEF&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,551.10,402.52,8.74;12,110.48,563.06,402.52,8.74;12,110.48,575.01,60.77,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,415.87,551.10,97.12,8.74;12,110.48,563.06,151.25,8.74">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Stanford Digital Library Technologies Project</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="12,110.48,593.25,402.52,8.74;12,110.48,605.21,286.39,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,420.76,593.25,92.24,8.74;12,110.48,605.21,233.68,8.74">Trec 2007 blog track experiments at kobe university. Proceedings of TREC</title>
		<author>
			<persName coords=""><forename type="first">Kazuhiro</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshihiro</forename><surname>Kino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shohei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,623.45,402.53,8.74;12,110.48,635.41,80.99,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,265.99,623.45,176.41,8.74">Umass at trec 2007 blog distillation task</title>
		<author>
			<persName coords=""><forename type="first">Jangwon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,451.48,623.45,61.53,8.74;12,110.48,635.41,50.36,8.74">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,653.65,402.52,8.74;12,110.48,665.60,312.93,8.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,458.71,653.65,54.30,8.74;12,110.48,665.60,229.17,8.74">The General Inquirer: A Computer Approach to Content Analysis</title>
		<author>
			<persName coords=""><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dexter</forename><forename type="middle">C</forename><surname>Dunphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marshall</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ogilvie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,683.84,402.52,8.74;12,110.48,695.80,402.52,8.74;12,110.48,707.76,296.10,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,459.93,683.84,53.07,8.74;12,110.48,695.80,119.61,8.74">Overview of the 2007 TREC legal track</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,254.44,695.80,258.56,8.74;12,110.48,707.76,168.81,8.74">The Sixteenth Text REtrieval Conference TREC. National Institutes of Standards and Technology</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.48,726.00,402.52,8.74;12,110.48,737.95,208.21,8.74" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,365.19,726.00,147.81,8.74;12,110.48,737.95,155.50,8.74">Fdu at trec 2007: opinion retrieval of blog track. Proceedings of TREC</title>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
