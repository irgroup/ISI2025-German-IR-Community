<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.96,75.04,418.29,15.49">RMIT University at TREC 2008: Relevance Feedback Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,185.16,107.55,83.86,10.76"><forename type="first">Yohannes</forename><surname>Tsegay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.92,107.55,60.50,10.76"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.40,107.55,67.52,10.76"><forename type="first">Simon</forename><surname>Puglisi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.96,75.04,418.29,15.49">RMIT University at TREC 2008: Relevance Feedback Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BFD6B9448680435FCD11547FEC6F10CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report outlines <ref type="bibr" coords="1,171.24,219.08,133.02,8.97">TREC-2008 Relevance Feedback</ref> Track experiments done at RMIT University.</p><p>Relevance feedback in text retrieval systems is a process where a user gives explicit feedback on an initial set of retrieval results returned by a search system. For example, the user might mark some of the items as being relevant, or not relevant, to their current information need. This feedback can be used in different ways; one approach is query expansion, where terms from the relevant documents are added to the original query, with the aim of improving retrieval effectiveness.</p><p>This report describes the the query expansion methods that we explored as part of TREC 2008. Our results demonstrate that high weight terms are not always necessarily useful for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Term Selection</head><p>The 2008 Relevance Feedback Track provided a set of relevance judgements for participants to use. These judgements are based on data from previous TREC tracks <ref type="bibr" coords="1,319.92,370.16,24.33,8.97" target="#b0">(2004</ref><ref type="bibr" coords="1,344.25,370.16,24.33,8.97">-2006</ref><ref type="bibr" coords="1,370.20,370.16,151.14,8.97;1,90.72,382.16,52.84,8.97">Terabyte Tracks, and the 2007 Million Query Track)</ref>. Different runs for the Relevance Feedback Track made use of varying numbers of relevant and non relevant documents (see Section 4 for details).</p><p>Based on the set of available documents with known relevance judgements, a query expansion scheme aims to identify the set of terms that, when added to the original query, is most likely to be able to boost retrieval performance. For all the experiments reported in this paper, queries were expanded using only terms which occur in the documents provided for expansion. No new terms were introduced from external sources.</p><p>Let R be the set of documents in the collection that are known to be relevant for the current query (that is, the set of relevant documents provided as part of the Track framework). To expand a query, a set of candidate expansion terms S is first established. In our experiments, we explore two approaches for the construction of the candidate term set, S. In the first, we combine all the terms from the provided relevant documents R into a single term-pool. Treating the available expansion documents as a single unit provides a means of selecting expansion terms which are signature to the set of relevant documents as a whole. Furthermore, the question of, how expansion terms from different documents should be combined can be avoided. We call this approach METHOD1.</p><p>In the second approach, a set S d is constructed separately for each relevant document, d. Here, term weights are first calculated for each set S d independently, using one of the weighting schemes described below. The top ranked terms from each set are then added to the original query by selecting the top terms in an interleaved fashion. Preference is given to terms that occur in over half the expansion documents. This approach ensures that the expansion terms are sourced from a variety of documents; in the first method, it is possible for terms from a small subset of relevant documents to dominate. We call this approach METHOD2.</p><p>Once the candidate term sets are constructed, term weighting approaches are used to rank and select the final expansion terms. In our submitted runs, we make use of a TF × IDF approach. Here, a term's weight is calculated as the product of its occurrence frequency within the set S, and its inverse document frequency (the reciprocal of the count of documents which contain the term) in the collection. While there are several variations for the calculation of TF and IDF weights, we base our formulation on the logarithmically smoothed approach <ref type="bibr" coords="2,171.00,62.12,100.03,8.97" target="#b3">(Zobel and Moffat, 1998)</ref>:</p><formula xml:id="formula_0" coords="2,208.68,85.33,194.68,10.26">T F × IDF = log(f S,t + 1) × (log(N/f t ) + 1)</formula><p>where f S,t is the number of times that term t occurs in the set S, N is the count of documents in the collection, and f t is the number of documents in the collection in which t occurs. Selected terms are thus those which occur frequently in the set of known relevant documents, but occur rarely across the collection.</p><p>An alternative term weighting scheme is based on language modeling, where the Kullback-Leibler divergence <ref type="bibr" coords="2,116.28,151.16,116.00,8.97" target="#b1">(Kullback and Leibler, 1951)</ref> -the relative entropy of the model of the set S against the model of the collection C -is used to estimate the weight of terms in S: KL(t, S, C) = P (t|S model ) × log P (t|S model ) P (t|C model )</p><p>where P (t|M ) is the probability of observing t in a model M , and S model and C model are the unigram language models of the set S and the collection as a whole, respectively. Lastly, to minimise the possible confounding effect of query drift, where queries are expanded with terms that lead the query away from relevant answers, the contribution of the expansion terms to the overall similarity score is adjusted relative to the original query terms. Expansion terms are down-weighted by a factor of one-third compared to the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Preprocessing</head><p>Prior to term selection, each document was parsed and terms were extracted. A term was defined as a sequence of alphanumeric characters delimited by whitespace <ref type="bibr" coords="2,340.44,340.88,106.89,8.97" target="#b2">Williams and Zobel (2005)</ref>. Terms were then case-folded and stemmed. We excluded from this set all stopwords 1 , URLs, and floating point numbers.</p><p>To avoid repetition, expansion terms identical to the original query terms were removed from consideration. Terms were then scored using either of the schemes presented in the previous section. Each query was then expanded using the top 25 unique terms <ref type="bibr" coords="2,271.56,388.76,114.08,8.97" target="#b0">(Billerbeck and Zobel, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs</head><p>The gov2 collection was indexed using the ZETTAIR Search engine 2 . Terms were stemmed using the Porter stemmer. The expanded queries were processed using a Dirichlet-smoothed language model.</p><p>The Track framework made available a set of training queries (the odd-numbered Million query track topics and corresponding relevance judgments). Based on initial experiments, we found that the TF × IDF approach performed marginally better than the Kullback-Leibler divergence approach. Due to a limited number of runs that could be officially submitted, we opted to use the former term weighting method only, and to explore the effect of forming the candidate term-pool S with METHOD1 and METHOD2 (these correspond to the official submitted runs named RMIT08.*2 and RMIT08.*1).</p><p>The track contained five runs, A-E described below:</p><p>• Run A: Baseline retrieval, no relevance information was provided and queries are run with no expansion.</p><p>• Run B: For each query, a single relevant document was provided for expansion.</p><p>• Run C: Three relevant documents and three not relevant documents were provided.</p><p>• Run D: Ten judged document were provided where at least 3 were relevant and 3 were not. • Run E: A larger number of judged documents (40-800) documents was provided.</p><p>For our runs, the non-relevant documents provided in the above sets were disregarded, and only information in relevant documents was utilised.</p><p>A single variant, METHOD1, was submitted for runs A and B. For runs C -E, where several expansion documents were available, METHOD1 and METHOD2 runs were submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The effectiveness of the query expansion strategies is shown in Tables <ref type="table" coords="3,382.32,410.36,4.98,8.97" target="#tab_0">1</ref> and<ref type="table" coords="3,408.60,410.36,3.77,8.97" target="#tab_1">2</ref>. Adding expansion terms negatively impacts on performance as measured by P@10, P@20, P@100, MAP and BPREF. Run C yields the lowest effectiveness for both methods, leading to a relative decrease in P@20 of 62%. The losses are even greater for MAP and BPREF. Failure analysis shows that while expansion did help for some queries, a larger number were harmed by the expansion process (of the 31 judged queries, 5 have shown an increase in performance, while the performance of 15 queries was decreased). The two queries which have shown the best improvements due to expansion and the two queries whose performance decreased most after expansion are shown in Figure <ref type="figure" coords="3,172.32,494.00,4.98,8.97">1</ref> and 2 respectively.</p><p>Run D demonstrates slight improvements from run C and these are more pronounced in METHOD2. This can be attributed to the fact that precedence is given to terms which occur in the largest number of expansion documents. In METHOD1 a term's weight is computed based on its frequency in the term-pool S, and therefore it does not make any distinction between a high weight term sourced from a single expansion document and another that occurs in a large number of the expansion documents.</p><p>The above observation are consistent with run E, where for METHOD2 it is the only run to have performed better than the baseline in P@10 and P@20. Although more effective than runs C and D, the MAP and BPREF scores for run E are still worse than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>As part of the 2008 Relevance Feedback Track, we experimented with two TF × IDF based query expansion approaches. While some queries benefited form both approaches, the techniques failed to lead to consistent improvements. In the case of run E, even though improvements are observed in early precision scores, only 42% of the queries had shown improvements in MAP. We plan to conduct further failure analysis to try and identify the properties of queries for which the approach harmed performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,90.72,45.85,430.63,9.96;4,90.72,58.52,147.93,8.97;4,143.88,71.11,57.67,8.37;4,143.88,83.11,303.67,8.37;4,143.88,95.11,324.55,8.37;4,143.88,106.99,313.99,8.37;4,143.88,118.99,220.15,8.37"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Terabyte track topics 782 and 772 which have yielded the best improvements. The * next to a term indicates that it is an expansion term. Topic: 782 orange varieties seasons fruit* valencia* season* october* midseason* florida* lemons* navel* nfc* concentrated* oranges* economic* growers* exporter* juice* crop* grapefruit* fresh* citrus* conditions* september* tangerines*</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,101.52,647.32,377.14,18.29"><head>Table 1 :</head><label>1</label><figDesc>METHOD1 runs, where the expansion terms were selected from a single pool of terms from all relevant document, ordered by decreasing TF × IDF weight.</figDesc><table coords="2,101.52,647.32,377.14,18.29"><row><cell>1 The list of stopwords used is available at http://www.csse.unimelb.edu.au/ ∼ jz/resources/stopping.zip</cell></row><row><cell>2 http://www.seg.rmit.edu.au/zettair/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,90.72,247.33,430.58,21.63"><head>Table 2 :</head><label>2</label><figDesc>METHOD2 runs, where a terms in each document are ranked separately by decreasing TF × IDF order, and the top ranked terms are intersected.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,90.72,502.36,430.60,8.07;4,100.68,513.28,407.48,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,224.88,502.36,278.19,8.07">Questioning query expansion: an examination of behaviour and parameters</title>
		<author>
			<persName coords=""><forename type="first">Bodo</forename><surname>Billerbeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,100.68,513.28,206.95,8.07">Proceedings of the 15th Australasian database conference</title>
		<meeting>the 15th Australasian database conference<address><addrLine>Darlinghurst, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACS, Inc</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,90.72,532.24,430.28,8.07;4,100.68,543.06,209.84,8.27" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,208.56,532.24,112.27,8.07">On information and sufficiency</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2236703" />
	</analytic>
	<monogr>
		<title level="j" coord="4,329.88,532.24,138.53,8.07">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,90.72,562.12,430.40,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,191.76,562.12,101.37,8.07">Searchable words on the web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,300.00,562.12,147.01,8.07">International Journal of Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="105" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,90.72,581.08,355.40,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,211.44,581.08,106.33,8.07">Exploring the similarity space</title>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,324.72,581.08,45.97,8.07">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="34" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
