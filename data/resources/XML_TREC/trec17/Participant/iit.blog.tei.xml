<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.65,105.69,348.69,18.14">IIT Kharagpur at TREC 2008 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-10-25">25 Oct, 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,216.90,145.49,65.58,12.58"><forename type="first">Robin</forename><surname>Anil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kharagpur</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,293.18,145.49,101.91,12.58"><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
							<email>sudeshna@cse.iitkgp.ernet.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Kharagpur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.65,105.69,348.69,18.14">IIT Kharagpur at TREC 2008 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-10-25">25 Oct, 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">34917F01996968B1622FFC6283F2C52E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our opinion retrieval system for TREC 2008 blog track. We focused on five different aspects of the system. The first module is focussed on extracting the blog content out from junk html and thereby decreasing the noise in the indexed content. The second module aims at removing various kind of spam content from real blogs. The third module aimed at retrieving the relevant documents. The fourth module filters out opinionated documents and the fifth one calculated the polarity of the sentiments in the document. The final ranked retrieval runs were based on various combination of settings in each module so as to study the effect of each. For classification of subjectivity and polarity, the predictions we done using a complementary naive bayes classifier</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Web logs (blogs) are a fast growing phenomenon on the World Wide Web (Web) as they allow people to publish their thoughts and opinions on any topic they choose. In July 2006 a blog tracking company, Technorati, Inc., reported that it was tracking 48.8 million blogs worldwide (currently tracking 48.8 million sites and 2.7 billion links 2006), up from 4.2 million in October 2004. There were over 100 Million Blogs in 2007 and growing. Blogs are an informal form of communicating. Blogs created by organizations such as newspapers or those addressing political or popular topics receive thousands of hits per day, however most personal blogs have a far smaller audience. The author of a blog is free use any language required to express their thoughts and opinions. The length of blogs varies from one paragraph to pages of rants, the paragraphs within the blogs also vary considerably. On the other hand, the length of a news article is dictated by the space available. However, one paragraph articles are uncommon. These articles have trained and experienced authors and an editor to ensure high quality writing techniques are used and words are not used out of context or with ambiguous meaning.</p><p>Blog posts are often informally written, poorly structured, and filled with spelling and grammatical errors, and feature non-traditional content. Performing linguistic analysis on blogs is plagued by two additional problems: (i) the presence of spam blogs and spam comments and (ii) extraneous non-content including blog-rolls, link-rolls, advertisements and sidebars. TREC Blog Track 2006 and 2007 saw some good amount of interest in this field. This problem was again posed in the TREC 2008 Blog track with a few modifications.</p><p>For the purpose of the TREC evaluation, the participants were working on the Blog06 test collection. The permalinks of documents was used as a retrieval unit. The number of such permalinks was around 3.2 million. Our system of retrieving the documents was made using the Apache Lucene search engine. Lucene was able to index the whole Blog06 <ref type="bibr" coords="2,462.34,135.55,15.77,10.48" target="#b5">[6]</ref> dataset and retrieved the documents very quickly(Ëœ0.1s). Inorder to decrease the size of the index it was necessary to remove a lot of noise in the HTML. A lot of the documents had malformed html which was corrected using the HTML Tidy utility. We used the qrels of the Blog Track of TREC 2006 and 2007 to train the sentence level subjectivity and polarity classifiers. In the following sections we detail how we structured the whole system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preprocessing -Content Extraction</head><p>Web pages are cluttered with distracting features around the body of a blog post which distract the user from the content block. These features range from pop-up ads, flashy banner advertisements, unnecessary images, or links scattered around the screen. These objects are quite unimportant and thus add noise to the main content. The noise in the content may create errors while doing document retrieval thus drastically reducing the precision of retrieval. In order to improve the quality of opinion extraction results, we extracted the title and content of the blog post for indexing because the scoring functions and Lucene indexing engine cannot differentiate between text present in the links and sidebars of the blog post. Automatically extracting the actual content poses an interesting challenge for us. Blogs are a bit more structured than random html pages. The parts we are interested are the title, post and comments area of the page. But the random nature and structure of different blog softwares make it quite difficult for simple rule based content extraction. Our approach to html extraction was based on the text to link ratio principle.</p><p>The core idea is that a content rich part of the blog will have a higher text/link ratio than the side bars, header, footer and other advertisements. We did a DFS traversal of the HTML tree and calculated the ratio for the whole set of nodes. After this is done the second step starts from the body tag and goes down the tree in breadth first fashion. We do a hill climbing search to find the node with the highest text to link ratio. Since blogs are simpler than other html pages on the internet, we were able to extract the required content in almost all of the cases barring a few(&lt;0.001%). In most of the cases the document was retrieved with none or few extraneous content(1-2 sentences long). We also tried using a SVM classifier to predict the content blocks based on the link ratio, tags and html attributes. The result was quite similar to the hill climbing heuristic, but it skipped many important blocks in some of the cases. We needed to index most of the content, so indexing the content with partial noise was preferred to the one where some content blocks are unrecognized. Figure <ref type="figure" coords="2,501.54,620.69,5.85,10.48" target="#fig_1">1</ref> shows the screenshot of a Blog and the extracted content on a sample blog post. Curtain, which is about how Steve Jobs is preparing for his keynote speech at Macworld Expo next week in San Francisco. From the article: When Apple announces something new, people pay attention. This is due, in large measure, to Steve and the way he delivers Apple's messages. His preferred method of making major product announcements is at one of his public presentations, or "keynotes" as they are called inside the company. If you want to see Jobs in action, and believe me it's worth it, you can watch his keynote from Macworld 2005 here. This isn't your father's PowerPoint. PermaLink posted by Frank @ 7:20 PM  <ref type="bibr" coords="3,398.31,262.65,13.52,10.48" target="#b0">[1]</ref>. We analysed how much the models will be able to minimize misclassification of authentic blogs while maintaining accuracy of the splog classification. Finally we determined that using a bag of words , model provided a decent set of features that did so. All of the models are based on Complement Naive Bayes classifier(Implementation in Apache Mahout <ref type="bibr" coords="3,365.18,320.43,17.07,10.48" target="#b1">[2]</ref>) using Bag-of-words, bigrams and trigrams as features. A few shortcomings we faced in this classifier are that the dataset wasn't complete in its coverage of spam. So In order to facilitate better classification, we increased the dataset by manually annotating some splog in the Blog06 dataset itself. Finally we did filtering of offensive content. A major chunk of the Blog06 dataset (Ëœ18%) was identified as spam or offensive and was hence removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Splog Detection</head><p>The Blog06 dataset also contained a lot of non-english blogs. Since our system only dealt with english language opinions it made no sense to keep the non english ones. They were removed by a simple heuristic comprising of the number of occurence of non english characters and the articles(a, an, the etc.) in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Document Indexing and Retrieval</head><p>Lucene <ref type="bibr" coords="3,101.79,516.67,15.46,10.48" target="#b4">[5]</ref>, an open-source search engine is a powerful search framework capable of indexing several gigabytes of document data and quickly performing complex searches on that data. Lucene can also process data beyond raw text of the document content. Typically this includes data about the documents that are being indexed, for example, title information, document author, feed url etc. Lucene provides a scoring algorithm that includes this additional data to find best matches to document queries. The default scoring algorithm is fairly complex and considers such factors as the frequency of a particular query term with individual documents and the frequency of the term in the total population of documents.</p><p>After pre-processing, blog posts are indexed using Lucene. Lucene internally constructs an inverted index of the documents by representing each document as a vector of terms. Given a query term, Lucene uses standard Term Frequency (TF) and Inverse Document Frequency (IDF) normalization to compute similarity. In addition, the scoring formula can also be tuned to perform document length normalization and term specific boosting. The default parameters were modified while searching the index. In our system we maintained two different indices, one at the sentence level and the other at the document level. We Figure <ref type="figure" coords="4,245.21,387.00,4.55,10.48">2</ref>: The Overview of the system ran our opinion detection and polarity detection module through the whole Blog06 dataset and split a document into 4 lists. The lists had only objective, positive, neutral or negative sentences from the given document. We indexed these four group of sentences as a field as well as the text of the whole document. Using the given TREC queries, we queried on each of these 4 fields/indexes as well as the whole document (content field) as shown in figure <ref type="figure" coords="4,532.02,481.00,4.55,10.48">2</ref>. The final score of the retrieved document was based on the BaseScore(Score from the query on the content field) and the OpinionScore(Score from positive, negative and neutral field). Score from the objective index was given lesser weight. This ensured that the opinionated documents come on top in the ranked retrieval result.</p><p>Once the documents are retrieved they are run through a reranking module which does the reranking on the basis of the output for opinion classification and polarity detection modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Opinion Filtering and Polarity Detection</head><p>The Opinion Filtering and Polarity Detection module aims at improving the opinion finding accuracy of the system. This module takes in the ranked result and re-ranks it based on the topic relevance and opinion relevance with respect to the given topic keywords.</p><p>The Opinion finding system primarily builds on the works of Pang &amp; Lees EMNLP 2002 <ref type="bibr" coords="4,525.81,691.68,15.33,10.48" target="#b3">[4]</ref> and Pang &amp; Lees ACL 2004 <ref type="bibr" coords="4,211.81,706.12,14.63,10.48" target="#b2">[3]</ref>. Pang &amp; <ref type="bibr" coords="4,280.82,706.12,51.84,10.48">Lee (2002)</ref> <ref type="bibr" coords="4,332.67,706.12,14.96,10.48" target="#b3">[4]</ref> examined whether it suffices to treat sentiment classification simply as a special case of topic-based categorization (with two topics representing different polarities). They conducted sentimental classification using three standard classification techniques (Nave Bayes, Maximum Entropy, Support Vector Machine). They also examined several factors that make sentimental classification more challenging such as the thwarted expectation problem mentioned in the prior section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data for Subjectivity and Polarity Classification</head><p>For the purpose of classification we looked at various datasets available today. Movie reviews are typically subjective and can be classified as either positive or negative opinions. They are thus suitable for our classification evaluation. Our experiments use Pang &amp; Lees movie review datasets <ref type="bibr" coords="5,110.22,206.41,14.76,10.48" target="#b2">[3]</ref> from http://www.cs.cornell.edu/People/pabo/moviereview-data/. But the size of the movie dataset( 10k sentences) had less coverage over the opinions found in the Blog06 dataset. So we had to augment that system with data from various sources. The final dataset for polarity and opinion finding tasks were extracted from the following sources.</p><p>Sentiment Polarity Datasets (v 2.0) The data source was gathered from the Internet Movie Database IMDB archive of rec.arts.movies.reviews. There are 1000 positive and 1000 negative full text movie reviews. The ground truths (actual polarity ratings in this case) were assigned based on Pang &amp; Lees heuristic scripts extracting the first review score from each review such as Thumbs Up/ Thumbs Down and Star Ratings.</p><p>Subjectivity Datasets (v 1.0) The data source contains 5000 objective sentences and 5000 subjective sentences. Objective sentences were extracted from Internet Movie Databases plot summaries while the subjective sentences are from Rotten Tomatoes review snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minekey Opinion Dataset</head><p>We were able to obtain the database of posted opinions from Minekey (www.minekey.com), an opinion discussion network. The database had over 100K sentences, which are very opinionated. We took these sentences extracted the topic and created a parallel dataset of objective sentences by fetching sentences from wikipedia on the same topic.</p><p>Extracting parallel sentences from previous Qrels For each of the opinionated documents in the qrels for the year 2006 and 2007, we extracted all the sentences which has the query keyword(which is relevant to the topic in question) in it. We then created a parallel polarity and subjective-objective dataset by grouping these topic relevant sentences into positive/negative and subjective/objective sets. To minimize the bias towards the topic qrels, we removed the query words from the dataset before running the trainer of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Topic Relevance and Opinion Search</head><p>In the following paragraphs, we detail how the Mean Average Precision(MAP) of each run compares with respect to the different techniques we have implemented. Even though we will be referencing the opinion finding MAP, the trend in topic relevance MAP follows the former with some degree of correlation. The output of the search engine with no postprocessing was the run IITKGPTITLE1. All the other runs are built on top of this base system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">IITKGPNOSPAM</head><p>The effect of how spam and offensive content was affecting the precision of the retrieved result was being tested from the baseline. The spam was switched on in IITKGPNOSPAM. This resulted the MAP value to increase in both opinion relevance and topic relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">KGPOP</head><p>In KGPOP, we tested the effect of the movie review opinion dataset in subjectivity and polarity classification. When a document is fetched, we select the sentences which refer to the topic in question. Then each of these sentences are classified as being subjective or objective. Further we calculated the opinion score based on the number of positive and negative sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">KGPPOS1</head><p>In the Opinion finding task, the best opinion finding MAP (0.30 in KGPPOS1) on the new topics was achieved from a classifier trained using the opinions from minekey.com as the dataset. This classifier used also POS tags as a feature for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">KGPPOS2</head><p>Another major problem we test was how well we could find and classify the sentences in the document which actually talked about the topic. In KGPPOS16.1.3 we used only the sentences from the document where the query keywords appeared. In KGPPOS2 we also considered the next sentence and tried classifying its as opinionated or not. We saw a decline in the MAP for that run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">KGPFILTER</head><p>A bigger gain in MAP was made on removing offensive/pornographic content which plagued the dataset. Along with the filtering we used the dataset of parallel sentences from the qrels and wikipedia for opinion classification in the run KGPFILTER. The overall MAP for that run was 0.47. But the MAP value of the new topics was hovering around 0.28. This performance hit on the new topics suggested that there can still can be more improvements in subjectivity classification methods using that parallel sentences corpora. In the run KGP-BASE4, we used the given baseline-4 and used the same rerank module as in KGPFILTER.   In this run, we relied on the classification of polarity using movie review dataset. In the classifier we used the bigrams, part of speech tags as features. We also added functionality to identify single and double negations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">KGPPOL2</head><p>In this run, we relied on the classification of polarity using parallel sentences extracted from the previous year's qrels. The classifier used the same features as in KGPPOL16.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Blog Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">FEEDKGP</head><p>Our feed distillation extraction was based on the opinion extraction module. We took the top thousand retrieved posts from our system and reranked them on the basis of the count of documents from each feed. This run was based on opinion run using movie review data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">FEEDKGP1</head><p>This run was based on the same system as in the run KGPFILTER. The ranking was based on the log of the count of documents from each feed.</p><p>Run ID MAP R-prec P@5 P@10 P@20 FEEDKGP 0.1539 0.1330 0.3240 0.2680 0.2570 FEEDKGP1 0.1720 0.2484 0.3600 0.3220 0.2770 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Comparing the median scores achieved by other teams in all topics, our score in the new topics is slightly below the overall median(hovering around 0.28). Some of the topics have MAP&gt;0.4(in around 24 topics/50) where as some have a low score. Thus the average came out at 0.28 for these 50 new topics. The primary reason for a low map value is the failure of the retrieval engine to fetch the documents in difficult queries. For eg. when asked for articles "citing wikipedia as the primary source", the lucene was unable fetch relevant documents just based on the keywords. For those topics which returned a good amount of documents, the rerank module was able to pick out relevant as well as the opinionated ones correctly. The MAP value was consistently above 0.4 for them. 20 out of these 50 topics did not retrieve enough relevant articles. These topics haven't been search engine friendly so the lack of documents(in a automated retrieval scenario) from our system became the reason for the low MAP value. The average P@5 was near 0.34 for all topics whose MAP was below 0.25(around 30/150). Many of these topics had less than 30 relevant documents retrieved(out of thousand) while some(around 3) had around 80 documents. These precision scores give us some confidence in the classification accuracy of our opinion filtering module. Our focus for the coming year would be to be able to identify the topic of the query in a much more precise manner. The results blog distillation task depended on the output of the opinion filtering system. That too showed the increase in performance due to the improvement in the opinion filtering module.</p><p>In the Polarity task, polarity phrasal features extracted from the previous qrels helped to increase the performance. Using a split index for positive, negative and neutral sentences did proved to be faster way to get decent quality results instead completely relying on post retrieval filter. Polarity runs showed the lack of the classification accuracy of the polarity classifier. We need to find more descriptive features which identitfies the polarity of a sentence. Further more, complex negated statements requires a lot more processing for us to be able to find its polarity correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgement</head><p>This work is in conjuction with Minekey Inc. We would like to show our gratitude for allowing us to use their opinion data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,70.87,701.36,470.26,10.48;2,70.87,715.80,470.27,10.48;3,308.60,78.15,235.13,5.24;3,308.60,85.12,80.27,5.24;3,308.60,92.10,235.13,5.24;3,308.60,99.07,235.13,5.24"><head></head><label></label><figDesc>Spam filtering was done based on a seed data set (Kolari, Finin, &amp; Joshi 2006) of 700 positive (splogs) and 700 negative (authentic blog) examples. Each one is the entire HTML content of Title: iFractal: Steve Jobs Keynote at Macworld: Black T-Shirt, Jeans, and a Lot of Preparation Post: Steve Jobs Keynote at Macworld: Black T-Shirt, Jeans, and a Lot of Preparation. Scott McNulty at TUAW pointed me to Behind the Magic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,138.32,197.55,335.36,10.48;3,70.87,59.53,180.01,126.11"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example show HTML content extraction from blogs</figDesc><graphic coords="3,70.87,59.53,180.01,126.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,126.00,59.53,360.01,315.57"><head></head><label></label><figDesc></figDesc><graphic coords="4,126.00,59.53,360.01,315.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,76.84,61.96,489.10,65.56"><head>Table 1 :</head><label>1</label><figDesc>The Precision Scores for Baseline runs</figDesc><table coords="6,76.84,61.96,489.10,40.82"><row><cell>Run ID</cell><cell cols="4">Topic MAP(all) Topic MAP(50 new) Opinion MAP(all) Opinion MAP(50 new)</cell></row><row><cell>KGPTITLE1</cell><cell>0.3562</cell><cell>0.3576</cell><cell>0.2671</cell><cell>0.2935</cell></row><row><cell>KGPNOSPAM</cell><cell>0.3612</cell><cell>0.3560</cell><cell>0.2720</cell><cell>0.2988</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,100.25,157.68,411.51,67.71"><head>Table 2 :</head><label>2</label><figDesc>The Precision Scores for Opinion Finding runs</figDesc><table coords="7,100.25,184.57,411.51,40.82"><row><cell>Run ID</cell><cell cols="4">Pos MAP(all) Pos MAP(50 new) Neg MAP(all) Neg MAP(50 new)</cell></row><row><cell>KGPPOL1</cell><cell>0.1270</cell><cell>0.1330</cell><cell>0.0952</cell><cell>0.0990</cell></row><row><cell>KGPPOL2</cell><cell>0.2416</cell><cell>0.0982</cell><cell>0.3287</cell><cell>0.0587</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,70.87,239.65,355.87,68.87"><head>Table 3 :</head><label>3</label><figDesc>The Precision Scores for Polarity runs</figDesc><table coords="7,70.87,274.19,239.62,34.33"><row><cell>6.2 Ranked Retrieval of Polarity</cell></row><row><cell>6.2.1 KGPPOL1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,163.53,680.48,284.93,10.48"><head>Table 4 :</head><label>4</label><figDesc>The Precision Scores for Blog Distillation runs</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,89.08,597.63,452.05,10.48;8,89.08,612.08,452.05,10.48;8,89.08,626.52,452.06,10.48;8,89.08,640.97,104.80,10.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,497.08,597.63,44.05,10.48;8,89.08,612.08,194.03,10.48">Blogvox: Separating blog wheat from blog chaff</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anupam</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akshay</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranam</forename><surname>Kolari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,308.05,612.08,233.08,10.48;8,89.08,626.52,452.06,10.48;8,89.08,640.97,99.18,10.48">Proceedings of the Workshop on Analytics for Noisy Unstructured Text Data, 20th International Joint Conference on Artificial Intelligence (IJCAI-2007)</title>
		<meeting>the Workshop on Analytics for Noisy Unstructured Text Data, 20th International Joint Conference on Artificial Intelligence (IJCAI-2007)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,89.08,665.38,402.19,10.48" xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://lucene.apache.org/mahout" />
	</analytic>
	<monogr>
		<title level="j" coord="8,89.08,665.38,216.54,10.48">Machine Learning Library Apache Mahout</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,89.08,689.78,452.05,10.48;8,89.08,704.23,348.18,10.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,208.20,689.78,332.93,10.48;8,89.08,704.23,199.90,10.48">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName coords=""><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,314.26,704.23,116.16,10.48">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,89.08,63.32,452.05,10.48;9,89.08,77.76,177.81,10.48" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,201.21,63.32,339.92,10.48;9,89.08,77.76,30.44,10.48">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,145.04,77.76,114.24,10.48">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,89.08,102.17,365.04,10.48" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,175.07,102.17,140.44,10.48">Apache software foundation</title>
		<author>
			<persName coords=""><forename type="first">Apache</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,89.08,126.58,452.06,10.48;9,89.08,141.03,452.06,10.48;9,89.08,155.47,193.03,10.48" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,273.03,126.58,268.11,10.48;9,89.08,141.03,107.78,10.48">The trec blog06 collection : Creating and analysing a blog test collection</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">DCS Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
