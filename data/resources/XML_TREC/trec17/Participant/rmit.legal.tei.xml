<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.08,77.92,316.05,15.49">RMIT University at TREC 2008: Legal Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.72,111.87,56.86,10.76"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.48,111.87,60.62,10.76"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.08,111.87,73.30,10.76"><forename type="first">Andrew</forename><surname>Turpin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and IT</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.08,77.92,316.05,15.49">RMIT University at TREC 2008: Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A8E0EEF5120F422FA86F08EB18C4B5E3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper reports on the participation of RMIT university in the 2008 TREC Legal Track Ad Hoc task. OCR errors can corrupt the document view formed by an information retrieval system, and substantially hinder the successful retrieval of relevant documents for user queries. In previous research, the presence of errors in OCR text was observed to lead to unstable and unpredictable retrieval effectiveness. In this study, we investigate the effects of OCR error minimization -through de-hyphenation of terms, and the removal of corrupted or "noise" terms -on retrieval performance. Our results indicate that removing noise terms can lead to significant savings in terms of index size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OCR Error Minimization</head><p>Printed hard-copy documents can be converted into electronically-editable form using Optical Character Recognition (OCR) technology. However, this technology generally does not achieve an accuracy of 100%. That is, errors are commonly introduced as a result of the conversion process, and any hyphenation of words occurring at the end of lines will be transferred directly from the source to the target format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise term removal</head><p>A major problem that arises when using OCR text with keyword-based information retrieval systems is that the OCR process introduces errors. That is, previously valid terms may become corrupted, introducing noise into the collection. The presence of many such "noise terms" within the text, and subsequently in the index, has two detrimental effects: first, keyword searches based on correctly formulated terms will not match with the corrupted term, even if the original version of the term would have been a valid match. Second, the presence of many noise terms may lead to unreliable term weighting in documents, which may have detrimental effects on the similarity functions used in the retrieval system.</p><p>The key challenge is to determine which terms are "noise". In our experiments below, we treat those terms with collection frequency ≤ 2 and document frequency ≤ 2 as noise terms.</p><p>Once noise terms are identified, there are two ways of dealing with them: first, error-correcting algorithms may be applied, to attempt to transform the corrupted version of the word back to its source. We investigated OCR-spell,<ref type="foot" coords="2,254.16,36.60,3.99,7.17" target="#foot_0">1</ref> an an OCR-based spell-checking tool that extends ispell, proposed by <ref type="bibr" coords="2,148.08,52.11,117.26,9.82" target="#b0">Taghva and Stofsky (2001)</ref>. However, initial experiments indicated some limitations of this approach for the TREC Legal Track environment: the tool did not correct some commonlyoccurring errors (for example, most errors that incorporated an erroneous punctuation symbol in the middle of a term were not identified). Moreover, the data used in the Legal Track collection includes many proper names (for example, names of companies and individuals), and technical terms (for example, from chemical or medical analyses), which were not identified. Due to these problems, we did not investigate OCR-spell further.</p><p>A second approach is to simply remove the noise terms from the collection. While this does not help the "missed match" problem, where a query keyword no longer matches the corrupted version of a term that would originally have been a match, we hypothesise that this process should remove some of the noise that is introduced into the collection term statistics, and therefore lead to better behaviour of ranking functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text de-hyphenation</head><p>Hyphenated words that span lines in the printed source document will be converted into the target electronically-editable format in the same way. Such word occurrences will therefore not match standard keyword searches for the non-hyphenated form of the word. One steps to reduce term mismatch due to the OCR process is therefore to remove the end-of-line hyphens, and re-assemble the word fragments.</p><p>When a term token that occurs at the end of a line ends with a hyphen, we remove the hyphen and join this token with the first token that occurs on the subsequent line. This new joined token is checked against the ispell dictionary.<ref type="foot" coords="2,266.40,344.88,3.99,7.17" target="#foot_1">2</ref> If the term is found in the dictionary, the original two fragments are replaced with the new, joined term. If the term is not found, the original fragments are retained in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The collection used for the 2008 TREC Legal Track is the IIT Complex Document Processing Information (CDIP) Test Collection <ref type="bibr" coords="2,239.04,447.39,101.87,9.82" target="#b1">(Tomlinson et al., 2007)</ref>. It contains 6, 910, 192 metadata records from US tobacco companies; 6, 794, 895 of the records included document text of varying quality from an optical character reader. 45 new topics were created for the 2008 track; each topic contains a RequestText (a natural language description of the request, typically one-sentence), a ProposalByDefendant (an initial boolean query proposed by the defendant), a RejoinderByPlaintiff (a rejoinder boolean query from the plaintiff, and a FinalQuery (the final boolean query from the negotiations).</p><p>For our experiments, we used the Zettair search engine developed by the Search Engine Group at RMIT University. <ref type="foot" coords="2,177.36,553.80,3.99,7.17" target="#foot_2">3</ref> The similarity function is based on a Dirichlet-smoothed language model <ref type="bibr" coords="2,500.52,555.75,23.55,9.82;2,90.72,569.31,82.67,9.82" target="#b2">(Zhai and Lafferty, 2004)</ref>. In line with the track guidelines, each submitted retrieval run consisted of up to 100,000 documents. </p><formula xml:id="formula_0" coords="3,247.80,37.47,5.45,9.82">#</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing</head><p>We created three separate indexes as follows:</p><p>Original text (p1): For each record in the collection, we indexed the following fields from the metadata and the OCR document: &lt;au&gt;, &lt;ca&gt;, &lt;no&gt;, &lt;cr&gt;, &lt;np&gt;, &lt;rc&gt;, &lt;pc&gt;, and &lt;tp&gt;. HTML entities were converted into their characters.</p><p>De-hyphened text (p2): Hyphens occurring in line breaks have been removed from the original text, and the trailing part of the word has been joined to the preceding line, if it was found in the ispell lexicon. This de-hyphened text collection is re-indexed as p2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise-removed text (p3):</head><p>We removed the "noise terms" from the de-hyphened text collection and re-indexed the new collection as p3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Run descriptions</head><p>Our retrieval experiments consist of six official runs on three different indexes using the search request as stated in the RequestedText and FinalQuery fields, respectively. For RequestedText runs (RMITRP1, RMITRP2, RMITRP3), all query terms are used to conduct a bag-of-words ranked search. The matching algorithm used was a Dirichlet-smoothed language model.</p><p>FinalQuery runs (RMITBP1, RMITBP2, RMITBP3) were also run as bag-of-words searches. However, special string-matching and Boolean operators that are not natively supported in the Zettair search engine were expanded as follows:</p><p>• Parentheses or proximity operators were removed from the query text.</p><p>• Wildcards were expanded to all the possible variations that appear in the ispell lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Table <ref type="table" coords="3,118.32,557.43,5.45,9.82" target="#tab_0">1</ref> shows the number of unique terms and total that were indexed by our search engine for each of the described pre-processing approaches. The index size is also shown. De-hyphenation, while leading to an increase in the number of unique terms, actually results in a reduced index size overall. Removing noise terms (p3) significantly reduces the index size: p3 is only 59% of the size of the original collection, p1. The main effectiveness measure for the 2008 Legal Track is F1@K ("est K-F1")<ref type="foot" coords="4,459.24,179.28,3.99,7.17" target="#foot_3">4</ref> , defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><formula xml:id="formula_1" coords="4,210.24,202.34,189.50,25.79">F 1@K = 2 * P recision@K * Recall@K (P recision@K + Recall@K)</formula><p>where K is an integer between 0 and 100, 000 inclusive, representing the threshold at which the system believes the competing demands of recall and precision are best balanced. For each topic, we determined the value of K as the total number of documents with a similarity value greater than 0.5. Secondary measures for the track are F1@R ("est R-F1"). For comparison with previous years, Recall@B ("est RB") is also reported. Reportedly, the sampling approach favored depths 5 and 100000, so P@5 ("est P5") and R@100000 ("est R100000") are shown. Results for our ad hoc runs are shown in Table <ref type="table" coords="4,311.28,321.39,4.06,9.82" target="#tab_1">2</ref>. The row labelled "Median" shows the median results of all 2008 Legal Track participants. Our baseline ranked retrieval approach RMITRP1, using RequestedText with no pre-processing of the collection, performed well. De-hyphenation (RMITRP2) let to marginal improvements for the F1@K, F1@R, P@5 and R@100000 measures. The improvements are not statistically significant at the 95% confidence level based on the Wilcoxon signed-rank test.</p><p>Removing noise terms, on the other hand, led to a slight drop in performance for all effectiveness measures (RMITRP3). However, again, none of these differences are significant at the 95% confidence level. Our techniques can be used to decrease the size of the index by over 40%, with no significant fall in retrieval effectiveness.</p><p>Our runs based on the final boolean query from the negotiations, FinalQuery, perform much worse than those using the RequestedText. We believe that this may be in part due to overexpansion of wildcard matches when transforming the Boolean requests into ranked requests. In contrast to the previous results, for the FinalQuery runs our de-hyphenation approach harms performance for all effectiveness measures (RMITBP2). Noise-term removal, on the other hand, leads to improvements on all measures except F1@K compared to using the original collection (RMITBP3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we have investigated two simple collection pre-processing approaches, aiming to overcome some of the errors introduced into the TREC Legal Track collection by OCR processes.</p><p>Our results show that, for a standard ranking approach based on using natural language request text as a query, de-hyphenation can offer some small increments in retrieval performance.</p><p>We hypothesised that the removal of noise terms might improve retrieval performance by dampening interference in the term distribution statistics that are used to calculated ranked retrieval similarity scores. However, our approach of simply removing noise terms, defined by scarcity of occurrence in individual documents and the collection as a whole, did not improve retrieval performance (the changes in effectiveness were not statistically significant). However, noise term removal led to a significant saving in terms of resources: the inverted index for the collection with noise terms removed takes up only 60% of the space of the original index. We therefore recommend the removal of noise terms.</p><p>In future work, we plan to investigate other OCR-based error-correction algorithms, so that instead of simply removing noise terms, these can be mapped back to their original form.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.72,37.47,430.48,87.34"><head>Table 1 :</head><label>1</label><figDesc>of unique terms # of total terms index size (MB) Number of unique indexed terms, number of total indexed terms, and the size of index for different pre-processing approaches.</figDesc><table coords="3,131.64,51.39,348.53,36.94"><row><cell>Original text (p1)</cell><cell>130,531,969 8,183,835,310</cell><cell>24,573</cell></row><row><cell>De-hyphened text (p2)</cell><cell>156,548,835 9,446,100,292</cell><cell>23,534</cell></row><row><cell>Noise-removed text (p3)</cell><cell>26,001,570 6,325,243,280</cell><cell>14,506</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,121.80,37.47,367.38,128.38"><head>Table 2 :</head><label>2</label><figDesc>Results for theLegal Track 2008.    </figDesc><table coords="4,121.80,37.47,367.38,105.46"><row><cell></cell><cell>EST K-F1</cell><cell>EST R-F1</cell><cell>EST RB</cell><cell>EST P5</cell><cell>EST R100000</cell></row><row><cell>Median</cell><cell>0.0702</cell><cell>0.1109</cell><cell>0.4073</cell><cell>0.1959</cell><cell>0.2805</cell></row><row><cell>RMITRP1</cell><cell>0.1578</cell><cell>0.2158</cell><cell>0.2622</cell><cell>0.5615</cell><cell>0.4337</cell></row><row><cell>RMITRP2</cell><cell>0.1586</cell><cell>0.2173</cell><cell>0.2628</cell><cell>0.5808</cell><cell>0.4472</cell></row><row><cell>RMITRP3</cell><cell>0.1129</cell><cell>0.1777</cell><cell>0.2172</cell><cell>0.3846</cell><cell>0.3500</cell></row><row><cell>RMITBP1</cell><cell>0.0704</cell><cell>0.1481</cell><cell>0.2148</cell><cell>0.4038</cell><cell>0.4016</cell></row><row><cell>RMITBP2</cell><cell>0.0646</cell><cell>0.1367</cell><cell>0.2071</cell><cell>0.3962</cell><cell>0.3766</cell></row><row><cell>RMITBP3</cell><cell>0.0681</cell><cell>0.1583</cell><cell>0.2186</cell><cell>0.4692</cell><cell>0.4182</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,106.80,603.64,405.51,8.17"><p>We used version 1.0 of the OCR-spell software, available from http://www.isri.unlv.edu/ISRI/Software</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,106.80,615.34,215.91,7.62"><p>http://www.gnu.org/software/ispell/ispell.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,106.80,625.96,323.31,8.17"><p>Zettair is available under a BSD license from http://www.seg.rmit.edu.au/zettair</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,106.80,633.16,414.47,8.07;4,90.72,644.20,290.07,8.17"><p>The evaluation measures are defined in the TREC 2008 Legal Track: Ad Hoc and Relevance Feedback Task Guidelines, available at http://trec-legal.umiacs.umd.edu/adhocRF08b.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,90.72,244.16,430.65,8.97;5,101.64,256.04,307.77,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,233.54,244.16,280.56,8.97">Ocrspell: an interactive spelling correction system for ocr errors in text</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Taghva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,101.64,256.04,246.18,8.97">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="125" to="137" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,90.72,277.04,430.69,8.97;5,101.64,288.92,419.70,8.97;5,101.64,300.92,126.45,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,378.12,277.04,143.29,8.97;5,101.64,288.92,18.78,8.97">Overview of the TREC 2007 legal track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,143.48,288.92,223.86,8.97">The Sixteenth Text REtrieval Conference (TREC 2007)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,90.72,321.80,430.86,8.97;5,101.64,333.80,282.81,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,225.06,321.80,296.52,8.97;5,101.64,333.80,32.57,8.97">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,143.76,333.80,173.92,8.97">ACM Transactions On Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
