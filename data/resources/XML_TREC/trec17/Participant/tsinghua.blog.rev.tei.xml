<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.36,76.31,206.81,12.58;1,405.24,78.76,3.00,5.40">THUIR at TREC2008: Blog Track 1</title>
				<funder ref="#_RDZ6rxt">
					<orgName type="full">Chinese National Key Foundation Research &amp; Development Plan</orgName>
				</funder>
				<funder ref="#_YfrA7CP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_UvXdy3d #_PM4gr4r #_ThUgyCX">
					<orgName type="full">Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,206.10,100.17,37.63,8.74"><forename type="first">Tong</forename><surname>Zhu</surname></persName>
							<email>zhutong000@gmail.com</email>
						</author>
						<author>
							<persName coords="1,250.87,100.17,42.52,8.74"><forename type="first">Min</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName coords="1,300.55,100.17,39.80,8.74"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName coords="1,346.93,100.17,53.57,8.74"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.36,76.31,206.81,12.58;1,405.24,78.76,3.00,5.40">THUIR at TREC2008: Blog Track 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">60F3B4A29EE77BA33047F8E526071C85</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the second time we participate in TREC Blog Track. There are three main tasks in the track, relevant finding task, opinion finding task and polarity task. In this year, we use multi-field relevance ranking in relevant finding task; and in opinion finding task, we focused on the combination of relevance score and opinionate score use a unified generation model; in polarity task, we develop two new methods to find out positive and negative blogs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the second year that the IR groups of Tsinghua University participated in TREC Blog Track. Different from the previous track, TREC introduced a new task, the polarity finding task. So, we focus on 3 main tasks this year. The opinion retrieval task involves locating blog posts that express an opinion about a given target. The target can be a "traditional" named entity --a name of a person, location, or organization --but also a concept (such as a type of technology), a product name, or an event. The topic of the post does not necessarily have to be the target, but an opinion about the target must be present in the post or one of the comments to the post. The polarity task is to locate blog posts that express an idea either positive or negative about a target.</p><p>For relevant task, a multi-field relevance ranking based on probabilistic retrieval model has been used. Both feed content and permalink content are used. Two kinds of information fusion have been experimented. One is the result combination on both parts. Another is to combine the two corpus in the weighting phase with improved algorithms. Experimental results on training set showed that both methods are proved to be effective and the second way seemed to be more stable.</p><p>For opinion finding tasks, the combination of relevance score and opinionate score use a unified generation model is emphasized. The final score of one document is a quadratic combination of sentiment score given by an opinion generation model and the relevance score given by document generation model. HowNet has been used as the sentimental lexicon.</p><p>For polarity task, several algorithms on using sentiment words co-occurrence frequency are implemented. The selection of the sentiment dictionaries and the effectiveness of co-occurrence window size are studied. The approach of using polarity words as query terms on first-step relevance results is also performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relevant Finding Task</head><p>The task of relevant finding is defined to retrieve those blogs which are relevant to the given query. To do this task, we need to make some pretreatment to the original corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-Processing</head><p>The main purpose of processing permalinks components is to remove noisy data from corpus. We found blog posts are written in various languages, and some blog posts are spam. So cleaning the corpus is very necessary for further work. We processed the permalinks components in the following two aspects.</p><p>One is to remove blogs written in languages other than English. We did this by examining the letter of the blog posts. By checking the content of set, we removed more than forty thousand blogs from permalinks components. we also removed strings which contained more than 20 letters. This value is fixed through experience. The other aspect is that we removed some obviously useless tags from the corpus. These tags include &lt;script&gt; &lt;link&gt; &lt;dochdr&gt; &lt;style&gt; et al.</p><p>For feed extract, a feed parser is designed to extract information from feed items, in this year, we use this part of data, only for multi-field search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval Process</head><p>Two kinds of information fusion have been experimented. One is the result combination on both parts. Another is to combine the two corpus in the weighting phase with improved algorithms. Experimental results on our training set showed that both methods are proved to be effective and the second way seemed to be more stable. Hence we used the second way (named as multi-field search) to merge permalink and feed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submitted Relevant Finding results</head><p>This task requests each group to submit 2 runs which regard as baseline in Blog track. In this task, we just use BM2500 formula and some expanded features such as word pair. The TMiner search engine, from IR group of Tsinghua University, is used as our text retrieval system. Two runs are submitted. One is retrieved in only one field (permalinks field), while the other is multi-field search (permalinks field &amp; feeds field). Table <ref type="table" coords="2,483.82,319.03,5.25,9.16">1</ref> shows the map of these two runs. The results are similar and no obvious improvement can be made.</p><p>Table1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Opinion Finding Task</head><p>The task of opinion finding is defined to retrieve those blogs which are opinionated to given query. Relevant finding task is the foundation of this task. In this task, each participating group was required to submit a compulsory automatic run, using only the title field of the topics, with all opinion finding features of the retrieval system turned off <ref type="bibr" coords="2,171.89,504.61,11.53,9.16" target="#b0">[1]</ref>. Four runs are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Opinion Finding retrieval process</head><p>In opinion finding task, users want to find the documents that is both relevant and with subjective opinions. Thus to the retrieval system, it is to find the document with the high probability of p(d|q,s). For simplicity, when we discuss the lexicon-based sentiment analysis, the latent variable s is assumed to be a pre-constructed bag-of-word sentiment thesaurus, and all sentiment words s i are uniformly distributed. Then the prior probability that the document d contains relevant opinions to query q is given by <ref type="bibr" coords="2,402.70,610.21,11.42,9.16" target="#b1">[2]</ref>:</p><formula xml:id="formula_0" coords="2,148.38,626.30,321.43,22.11">∑ ∑ ∝ = i i i i i d p d q p q d s p S s s p s q d p s q d p ) ( ) | ( ) , | ( | | 1 ) , ( ) , | ( ) , | ( (<label>1</label></formula><formula xml:id="formula_1" coords="2,469.81,629.17,4.09,9.16">)</formula><p>where |s| is the number of words in sentiment thesaurus s.</p><formula xml:id="formula_2" coords="2,82.50,673.38,432.11,21.60">Define ) ( ) | ( ) , ( , ) , | ( | | 1 ) , , ( ), , ( ) , , ( ) , | ( d p d q p q d I q d s p S s q d I where q d I s q d I s q d p rel i i op rel op ≡ ≡ = ∑ (2)</formula><p>where I op (d,q,s) is the opinion generation probability, and the I rel <ref type="bibr" coords="2,331.38,703.81,20.12,9.16">(d,q)</ref> is the document relevance probability. For relevance score, we use BM 25 ranking function <ref type="bibr" coords="2,284.90,719.41,11.41,9.16" target="#b3">[4]</ref>:</p><formula xml:id="formula_3" coords="3,63.12,77.01,414.20,36.39">) ) , ( ) , ( ) 1 ( ) , ( | | ) 1 ( ) , ( ) 1 ( 5 . 0 ) ( 5 . 0 ) ( (ln ) , ( 3 3 1 1 q w c k q w c k d w c avdl d b b k d w c k w df w df N q d ScoreI d q w rel + × + × + + - × + × + + - = ∑ ∩ ∈ (3)</formula><p>For opinion score, totally two heuristic ranking functions have been used in our experiments:</p><formula xml:id="formula_4" coords="3,61.50,139.86,196.68,21.87">1. λ λ + ⋅ - = ∑ ∈d S i op i W d q c W q s co s q d ScoreI | | ) , ( ) | , ( ) 1 ( ) , , (</formula><p>(for details, see <ref type="bibr" coords="3,348.91,147.43,11.85,9.16" target="#b1">[2]</ref>)</p><p>where co(s i ,q|W) is the frequency of sentiment word s i which is co-occurred with query q within window W, c(q,d) is the query term frequency in the document.</p><p>2. Use sentiment words as query terms, searching on top returned documents and get the opinion score. i.e. </p><formula xml:id="formula_6" coords="3,127.08,217.41,350.60,29.28">+ × + × + + - × + × + + - = ∑ ∩ ∈ (5)</formula><p>where s means the sentiment words in the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter settings</head><p>In our experiments, λ is set to 1.35.</p><p>And the sentiment dictionary we used is HowNet <ref type="bibr" coords="3,289.15,339.91,15.15,9.16" target="#b4">[5]</ref>. it is a knowledge database of Chinese, and some of the words in the dictionary have properties of positive or negative. We use the English translation of those sentiment words provided by HowNet. Finally there are 4621 English sentiment words selected.</p><p>For co-occurrence window size design, best performance is got when window size is full text according to training result on Blog07 data. A possible explanation is that the majority of authors in a blog article on only one thing to express their notions, so generally the topic diversity is much smaller than ordinary web pages. In all the experiment, we fixed the window size to full text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submitted Opinion Finding results</head><p>In this year opinion finding task, we submitted 4 runs which are listed in table 2. The results show the effectiveness of our runs.</p><p>Table2. The official results of opinion finding runs Run Tag Description Relevance Baseline Run in task1 MAP THUopnTwpGen Use co-occurrence MLE of senti-word and query term (Eq. 4). Weight on permalinks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 1 0.3155</head><p>THUopnTwpRRM Re-search by using sentiment words as query (Eq 5).</p><p>Weight on permalinks.</p><p>Run 1 0.3169 THUopnTmfRQ Use co-occurrence MLE of senti-word and query term (Eq. 4). Weight on permalinks and feeds.</p><p>Run 2 0.3120 THUopnTmfRmf Re-search by using sentiment words as query (Eq 5).</p><p>Weight on permalinks and feeds. Run 2 0.3283</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Polarity Task</head><p>The task of opinion finding is defined to retrieve those blogs which are opinionated whether positive or negative.</p><p>The polarity should be identified. This task was introduced in TREC 2008 as a natural extension of the opinion finding task, and it required 2 runs from each group.[3]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Polarity Task retrieval process</head><p>The retrieval process likes the process of opinion finding task. We also computed two scores of each blog posts. One is positive score, the other is negative score. The calculation is the same as we did in opinion finding task, i.e. the co-occurrence MLE based opinion score, and the re-search based score. Different with the previous task, the sentiment dictionary is divided into two polarity ones: positive dictionary and negative dictionary. For combination of relevance score and polarity score, three algorithms are implemented in this task. Aussume pos is positive score, neg is negative score, A is the const threshold. Then the three algorithms are:</p><p>Alg. 1</p><p>Alg. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alg. 3</head><p>Comparative experiments have been made on the training set we constructed on Blog 06 &amp; 07 topics with the three algorithms. Table <ref type="table" coords="4,177.26,665.83,5.25,9.16">3</ref> shows the differences between these algorithms on the training sets. Then it is negative; For Other conditions It is mixed, and neither in the positive set nor in the negative set.</p><p>In the table, all the polarity scores are got by co-occurrence MLE approach. And the choice of window size was the same as opinion finding task. It's found that the Alg. 3 gets best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submitted Polarity Task results</head><p>In this year polarity task, we submitted 2 runs which are listed in table 4. In relevant finding task, we will use more blog-specific features in blog data.</p><p>In opinion finding and polarity finding task, we will make further analysis on different algorithms, and a classify of query can be taken into account.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,212.04,681.43,181.85,9.16;4,229.08,697.34,58.37,9.45;4,329.08,697.34,58.37,9.45;4,169.68,713.59,26.26,9.16;4,229.08,713.59,23.67,9.16;4,329.04,713.59,28.90,9.16;4,169.68,729.73,26.25,9.16;4,229.08,729.73,28.89,9.16;4,329.04,729.73,28.89,9.16;4,169.68,745.81,26.26,9.16;4,229.08,745.59,28.89,9.45;4,329.04,745.59,28.89,9.45;4,99.84,396.85,209.14,9.16;4,110.40,412.45,78.43,9.16;4,99.84,428.05,209.26,9.16;4,110.34,443.65,80.77,9.16;4,99.84,459.25,87.51,9.16;4,110.34,474.85,271.51,9.16;4,99.84,537.25,195.07,9.16;4,110.40,552.85,78.43,9.16;4,99.84,568.45,201.48,9.16;4,110.34,584.05,80.77,9.16;4,99.84,599.65,87.51,9.16;4,110.34,615.25,271.51,9.16;4,102.84,256.57,167.37,9.16;4,113.40,272.17,78.43,9.16;4,102.84,287.77,167.95,9.16"><head></head><label></label><figDesc>pos -neg &gt; A + α ) or (pos &gt; 0) and (neg = 0)) Then it is positive; If ((pos -neg &lt; A -α ) or (neg &gt; 0) and (pos = 0)) Then it is negative; For Other conditions It is mixed, and neither in the positive set nor in the negative set. If ((pos -neg &gt; A ) or (pos &gt; 0) and (neg = 0)) Then it is positive; If ((pos / neg &lt; 1/A ) or (neg &gt; 0) and (pos = 0)) Then it is negative; For Other conditions It is mixed, and neither in the positive set nor in the negative set. If ((pos &gt; A) or (pos &gt; 0) and (neg = 0)) Then it is positive; If ((neg &gt; A) or (neg &gt; 0) and (pos = 0))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,213.36,167.41,179.94,9.16;5,85.68,183.49,36.52,9.16;5,222.12,183.49,49.04,9.16;5,355.78,183.49,82.56,9.16;5,370.50,199.09,53.13,9.16;5,453.48,183.49,33.33,9.16;5,502.02,183.49,22.77,9.16;5,452.34,215.23,75.34,9.16;5,67.20,215.23,68.84,9.16;5,151.50,215.23,190.28,9.16;5,151.50,230.83,185.83,9.16;5,352.56,215.23,25.42,9.16;5,452.34,231.31,75.34,9.16;5,452.34,247.39,75.34,9.16;5,67.20,247.39,274.63,9.16;5,151.50,262.99,123.38,9.16;5,352.56,247.39,25.42,9.16;5,452.34,263.53,75.34,9.16;5,61.50,300.62,156.79,10.80"><head>Table4</head><label></label><figDesc>Use co-occurrence MLE of polarity words and query terms (Eq.4). Combination Alg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,132.30,350.23,272.48,57.46"><head></head><label></label><figDesc>. The official results of relevant finding runs</figDesc><table coords="2,132.30,366.15,270.07,41.54"><row><cell>Run No.</cell><cell>Run Tag</cell><cell>MAP</cell></row><row><cell>1</cell><cell>THUrelTwp , baseline</cell><cell>0.3909</cell></row><row><cell>2</cell><cell>THUrelTwpmf, multi-filed search</cell><cell>0.3988</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>1 Supported by the <rs type="funder">Chinese National Key Foundation Research &amp; Development Plan</rs> (<rs type="grantNumber">2004CB318108</rs>), <rs type="funder">Natural Science Foundation</rs> (<rs type="grantNumber">60621062</rs>, <rs type="grantNumber">60503064</rs>, <rs type="grantNumber">60736044</rs>) and <rs type="programName">National 863 High Technology Project</rs> (<rs type="grantNumber">2006AA01Z141</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RDZ6rxt">
					<idno type="grant-number">2004CB318108</idno>
				</org>
				<org type="funding" xml:id="_UvXdy3d">
					<idno type="grant-number">60621062</idno>
				</org>
				<org type="funding" xml:id="_PM4gr4r">
					<idno type="grant-number">60503064</idno>
				</org>
				<org type="funding" xml:id="_ThUgyCX">
					<idno type="grant-number">60736044</idno>
					<orgName type="program" subtype="full">National 863 High Technology Project</orgName>
				</org>
				<org type="funding" xml:id="_YfrA7CP">
					<idno type="grant-number">2006AA01Z141</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,65.45,397.69,468.47,9.16;5,79.50,412.69,186.52,9.16" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<title level="m" coord="5,461.27,397.69,72.65,9.16;5,79.50,412.69,181.88,9.16">Overview of the TREC-2006 Blog Track. TREC Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,65.45,427.69,468.43,9.16;5,79.50,442.69,454.41,9.16;5,79.50,457.69,92.98,9.16" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,194.19,427.69,339.69,9.16;5,79.50,442.69,69.29,9.16">A generative model to unify topic relevance and lexicon-based sentiment for opinion retrieval</title>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xingyao</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,155.82,442.69,298.16,9.16">The 31st Annual International ACM SIGIR Conference (SIGIR 2008)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="411" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,65.45,472.69,468.51,9.16;5,79.50,487.69,51.05,9.16" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<title level="m" coord="5,311.59,472.69,222.37,9.16;5,79.50,487.69,46.41,9.16">Overview of the TREC2007 Blog Track. TREC Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,65.45,502.69,325.82,9.16" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,219.62,502.69,114.14,9.16">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno>TREC-8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,65.45,517.69,196.90,9.16" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hownet</surname></persName>
		</author>
		<ptr target="http://www.HowNet.org" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
