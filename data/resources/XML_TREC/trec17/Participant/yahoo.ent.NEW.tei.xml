<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.20,120.93,440.41,14.37">Blind Relevance Feedback with Wikipedia: Enterprise Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.24,155.03,61.09,12.19"><forename type="first">Yefei</forename><surname>Peng</surname></persName>
							<email>ypeng@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Yahoo! Labs Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.52,155.03,57.96,12.19"><forename type="first">Ming</forename><surname>Mao</surname></persName>
							<email>ming.mao@sap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">SAP Research</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.20,120.93,440.41,14.37">Blind Relevance Feedback with Wikipedia: Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F947BA1145A1D91675C950F799C629F0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this year's Enterprise track experiment, we focused on testing Blind Relevance Feedback, especially using online Wikipedia as query expansion collection. We demonstrated that using Wikipedia as query expansion collection returns better infNDCG than not using it.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This year's TREC Enterprise track is using the same corpus as last year. The document collection is a crawl of the publicly available web pages from the *.csiro.au domain, known as the CSIRO Enterprise Research Collection (CERC or .CSIRO). But the topics are different in 2008. They are real information requests from users via telephone and email. The enquiries staffers need to search on CSIRO web to find answers for these requests <ref type="bibr" coords="1,306.72,368.89,92.20,10.42">(Soboroff et al. 2008)</ref>. The enquiries cover a wide range of topics. The answers could be very specific or very general.</p><p>Because of the wide coverage of topics, CSIRO website may not have enough details of each of them. This led us to looking for a source of knowledge which covers a wide range of topics with as much details as possible.</p><p>Query expansion based on Blind Relevance Feedback (BRF) has been demonstrated to be an effective technique for improving retrieval results. The documents selected from initial search should contain reasonable number of relevant documents. Normally top k documents will be selected. Most frequent terms from these top documents will be extracted to form query expansion. The expanded query should be able to get more relevant documents if the selected documents share similar genre with target relevant documents.</p><p>There are two types of BRF-based query expansion. BRF Type 1 (BRFT1) is the original version of BRF, where query expansion is performed on the BRF information extracted from top N documents selected from an initial search on the same collection that the target documents are in <ref type="bibr" coords="1,70.44,597.25,116.33,10.42">(Evans &amp; Lefferts 1994)</ref>. This collection is called "target collection" in this paper. BRF Type 2 (BRFT2) has been explored as an alternative to BRFT1. The query expansion is performed based on the BRF information of the top N documents selected from the initial search on a DIFFERENT collection <ref type="bibr" coords="1,119.77,637.57,85.38,10.42">(He &amp; Peng 2006)</ref>. Such a collection is called "expansion collection" in this paper. The expanded query is then used to search on the target collection to find the relevant documents.</p><p>Wikipedia 1 is an online encyclopedia written collaboratively by its readers. There are several reasons for which we chose Wikipedia as our expansion collection. First, like other encyclopedias, it covers a wide range of topics. Second, it has a large size. As of Jan 2009, there are 2,679,000 articles in Wikipedia. Third, its content is up to date. People continuously contribute content to it.</p><p>In the remaining of this report, we will first talk about our approach in detail, then present the experiment design including the runs that we submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Approach</head><p>CSIRO corpus was index with Indri to form our target collection. A snapshot of Wikipedia at July 2008 was used as query expansion collection. We only focus on English version of Wikipedia. There are totally 6,996,745 articles. All these articles are index with Indri 2.7<ref type="foot" coords="2,434.04,243.70,3.92,6.98" target="#foot_1">2</ref> to form our query expansion collection.</p><p>Indri 2.7 is our retrieval system. We modified Indri's BRF module so that it can perform both BRFT1 and BRFT2 using the same default BRF model. The parameters for BRF were defined as selecting top 20 terms from top 20 documents.</p><p>In total, we submitted 4 different runs. They are:</p><p>• TitDes: In the Indri query, both the title and the description were used, and each word was treated as a term in the query. No Blind Relevance Feedback is used.</p><p>• TitBrf: Only the title is used in original query. Target collection (CSIRO) is used as expansion collection. Expansion collection (Wikipedia) is not used. This is traditional BRF method, mentioned earlier as BRF1.</p><p>• TitExp: Only the title is used in original query. Wikipedia collection is used as expansion collection. This method is different from traditional BRF method, mentioned earlier as BRF2.</p><p>• TitExpBrf57: Only the title is used in original query. Wikipedia collection is used as expansion collection. The new query is issued in target collection (CSIRO) again with Blind Relevance Feedback. BRF is used twice here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>Results of the four runs are shown in Figure <ref type="figure" coords="2,278.89,569.65,4.40,10.42">1</ref>. As expected, TitDes has lowest performance since no blind relevance feedback is used. TitBrf has best infAP, while TitExp has best infNDCG. It shows that using target collection itself or another collection (Wikipedia) both improves performance. In this case, they give similar results.</p><p>As of last run TitExpBrf57, BRF is used twice. The first time, original query is issued on Wikipedia collection, expanded query is combined with original query with weight of 0.5 and 0.5. Then new query is issued on target collection (CSIRO), expanded query is combined with issued query (which is an expanded query) with weight 0.3 and 0.7 respectively. Then final query is issued on target collection. Similar idea could be found in <ref type="bibr" coords="2,343.20,677.17,86.80,10.42">(He &amp; Peng, 2006)</ref>, but their results</p><p>showed similar twice BRF method returned better results than BRF1 and BRF2. Further experiments and analysis needs to be done to find out the reason.</p><p>Figure <ref type="figure" coords="3,237.00,373.21,4.34,10.42">1</ref>. infAP and infNDCG of the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we are talking about the studies we conducted in the participation to TREC 2008 in the task of document search. The goal of this study is to examine the effects of different Blind Relevance Feedback methods, and using Wikipedia as query expansion collection. Our experiment results demonstrate that using target collection as expansion collection (BRF1) and using Wikipedia as expansion collection both get better results than no BRF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Reference</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.44,581.41,454.57,10.42;3,105.48,594.85,171.39,10.42;3,70.44,608.29,455.56,10.42;3,105.48,621.73,242.32,10.42;3,70.44,635.17,464.05,10.42;3,105.47,648.61,250.06,10.42"><head>I</head><label></label><figDesc>. Soboroff, A. de Vries (2008). Overview of the TREC 2008 Enterprise Track, in Proceedings of TREC 2008. Gaithersburg MD USA. D. Evans and R. Lefferts. Design and evaluation of the clarit-trec-2 system. In Proceedings of the Second Text REtrieval Conference (TREC-2), 1994. Daqing He, Yefei Peng. Comparing Two Blind Relevance Feedback Techniques. In proceedings of Annual Conference of SIGIR, Seattle, WA USA 2006</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.08,727.72,100.74,8.65"><p>http://www.wikipedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,76.08,727.72,135.80,8.65"><p>http://www.lemurproject.org/indri/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
