<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.47,137.95,317.07,15.12">Overview of the TREC 2008 Enterprise Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.26,170.43,79.88,10.48"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>k.balog@uva.nl</email>
						</author>
						<author>
							<persName coords="1,380.41,170.43,62.75,10.48"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
						</author>
						<author>
							<persName coords="1,199.54,218.13,68.18,10.48"><forename type="first">Paul</forename><surname>Thomas</surname></persName>
							<email>paul.thomas@csiro.au</email>
						</author>
						<author>
							<persName coords="1,361.75,218.13,63.08,10.48"><forename type="first">Peter</forename><surname>Bailey</surname></persName>
							<email>pbailey@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NIST</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Microsoft</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Nick Craswell MSR</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Arjen P. de Vries CWI</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.47,137.95,317.07,15.12">Overview of the TREC 2008 Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">374D337965290EAF955ACD9B4C5ECDB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the enterprise track is to conduct experiments with enterprise data that reflect the experiences of users in real organizations. This year, we continued with the CERC collection introduced in TREC 2007 <ref type="bibr" coords="1,229.42,380.93,88.48,8.74" target="#b0">(Bailey et al., 2007)</ref>. Topics were developed in conjunction with CSIRO Enquiries, who field email and telephone questions about CSIRO research from the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection</head><p>The CERC corpus (CSIRO Enterprise Research Collection, http://es.csiro.au/cerc/) represents the public-facing web of the Australian Commonwealth Scientific and Industrial Research Organisation (CSIRO). Here, we summarize the main characteristics of this corpus; a complete description of the collection is given by <ref type="bibr" coords="1,281.66,495.48,82.13,8.74" target="#b0">Bailey et al. (2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The collection consists of all the *.csiro.au (public) websites as they appeared in March 2007. The resulting data set consists of 370 715 documents, with total size 4.2 gigabytes. The web crawler visited the outward-facing pages of CSIRO in a fashion similar to the crawl used in CSIRO's own search engine. In fact, the same crawler technology that CSIRO uses was used to gather the CSIRO documents (http://www.funnelback.com/). The corpus contains approximately 7.9 million hyperlinks, and 95% of pages have one or more outgoing links containing anchor text. One participant extracted email addresses of 3678 individuals, with 38% of documents containing at least one mailto field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Users</head><p>When the CERC corpus was developed, a conscious decision was made to work with CSIRO employees to develop topics and make relevance judgments whenever possible. In 2007, this role was filled by science communicators. Science communicators read and create the outwardfacing web pages of CSIRO as part of their job to interact with industry, government agencies, professional groups, the media, and the public to promote the work of CSIRO.</p><p>This year, our users were staffers for CSIRO Enquiries. Enquiries staffers receive requests for information about CSIRO work, primarily via telephone and email. The "contact us" links on the bottom of most CERC pages lead to someone in Enquiries. Enquiries staffers need to search the CSIRO web to find the information needed to fulfill the request. Additionally, expert search could help them locate experienced CSIRO researchers to fill out gaps in what they find on the CSIRO web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tasks and Topics</head><p>The tasks this year are the same as in 2007: document search and expert search, although the goal of the user is somewhat different this year. An employee in CSIRO Enquiries is responding to an email request for information about something at CSIRO. To do this, they search the public-facing web for answers and resources. Additionally, they look for subject experts who can help them by providing in-depth information relating to the enquiry.</p><p>The topics have been extracted from a log of real email enquiries from January to March 2007, the same date range as the CERC crawl. They are not a random sample, but have been chosen to illustrate a range of requests. Each was answered with reference to at least one page on CSIRO's public web site, so each has at least one relevant page in the corpus. There are a total of 77 topics, numbered CE-051 to CE-127.</p><p>Each topic has the original email (stripped of any identifying information, and of any greetings etc), and a short form which is a two-or three-word query created by track coordinator Paul Thomas but which Enquiries staff confirmed is very similar to one they'd issue to a search engine.</p><p>Here is an example topic: &lt;top&gt; &lt;num&gt;CE-053&lt;/num&gt; &lt;query&gt;selenium soil&lt;/query&gt; &lt;narr&gt; Can you please provide a current e-mail address, or failing that can you please put me in contact with the group responsible for the research into the use of selenium as an additive to soils, to promote sheep productivity/health. There were some trials conducted in WA and I am looking for aditional information on these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/narr&gt; &lt;/top&gt;</head><p>The query field is the short form, as might be typed to a search engine; the narr field is the substantive part of the original email.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Assessments</head><p>This year, no CSIRO resources were available for making relevance judgments, so both document and expert search tasks were judged by participants.</p><p>Analysis of last years document judgment data indicated that participant judges needed additional resources if their judgments were to be comparable to those made by CSIRO "insiders" <ref type="bibr" coords="2,129.03,675.81,86.20,8.74" target="#b1">(Bailey et al., 2008)</ref>. To that end, in addition to the topic text which includes the email sent to Enquiries, we provided judges with the final response sent by Enquiries, as well as a link to any CSIRO URL included in the response email. For expert search, the judges also received links to highly-relevant document search results for that same topic.</p><p>For the document search task, stratified sampling was used to select a subset of the pool to judge. The initial pool was the top 75 retrieved documents from two runs per group (selected according to priorities given by each group when the run was submitted). We then uniformly drew 100% of documents retrieved at ranks 1-3, 20% of documents ranked 4-25, and 10% of documents ranked 25-75. The principal measures for document search are mean inferred average precision ("infAP") and inferred NDCG ("infNDCG") <ref type="bibr" coords="3,345.78,184.75,87.00,8.74" target="#b16">(Yilmaz et al., 2008)</ref>, which estimate AP and NDCG given the sample.</p><p>Topics were assigned to three different groups to study assessor effects. Participants judged the pools through the CSIRO assessment system (adapted from the assessment system used in the Million Query track).</p><p>The guidelines instructed the assessors to read the query and narrative, and optionally carry out a Web search to learn more about the subject. Relevance judgments were made on a three-point scale: 2: Highly likely to be a 'key page', containing an answer to the enquiry.. 1: Possibly a 'key page'. 0: Not a 'key page', because, e.g., not relevant, off-topic, not an important page on the topic, on-topic but out-of-date, not the right kind of navigation point, or too informal or too narrow an audience.</p><p>For expert search, we drew a standard pool to a depth of 5 candidates from all submitted runs, along with the top 5 submitted supporting documents for each pooled candidate. The candidates and response email were compiled into an HTML file along with links to the supporting documents and highly relevant judged documents. Participants were asked to edit the file to indicate whether each candidate was or was not an expert. This simplified the process by not requiring an assessment platform (only some way to retrieve the linked documents), at the expense of some errors that may have crept in by editing the file by hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document search task</head><p>For the document search tasks, participants were asked to return up to 1000 documents from the corpus in response to each topic. Each group was allowed to submit up to four runs. One run was required to be an automatic run using the query field.</p><p>Fourteen groups submitted a total of 56 document search runs. Of those, 39 were automatic runs using only the query field. 13 automatic runs used the narr field (email text) in addition to the query. There were four manual runs.</p><p>Figure <ref type="figure" coords="3,154.64,551.80,4.98,8.74">1</ref> shows the range of infAP scores for all runs, ordered by mean infAP. The box for each run extends from the first to the third quartile of the infAP scores for each topic; the whiskers extend to include topics not more than 1.5 times the interquartile distance; outlier topics are shown as circles. The run names along the x-axis include "(n)" if the run used the narr field, and "(M)" indicates a manual run.</p><p>Table <ref type="table" coords="3,151.09,611.58,4.98,8.74" target="#tab_0">1</ref> shows the mean infAP and infNDCG scores for the top run from each group (by mean infAP). The top runs from each group were nearly always query-only automatic runs. Of the seven groups that submitted runs using the narr section in addition to the query, two groups -the University of Avingon and St. Petersburg State University (SPSU) -had narr runs perform better than their query-only runs. In all other cases, runs adding the narr field performed roughly the same, or otherwise much worse, than those using the query field only.</p><p>As a quick guide to papers by TREC participants appearing in the proceedings, we offer the following brief descriptions of each group's approaches.</p><formula xml:id="formula_0" coords="4,115.41,136.11,388.45,225.31">infAP 0.0 0.2 0.4 0.6 0.8 1.0 uogTrEDSelW ICTI3Sdoc03 uogTrEDQE uogTrEDSE2 ICTI3Sdoc01 uogTrEDbl THUFmfS ICTI3Sdoc02 ICTI3Sdoc04 THUFaAS THUFS UvA08DSall ucl01 UvA08DSexp UvA08DSbfb ucl03 FDUBase LIAIndriSiac (n) FDUUrl ucl02 FDUExpand (n) UvA08DSbl ucl04 ualr08e01 DERIrun6 RmitDocQ RmitDQCombLO DERIrun5 RmitDQRerank RmitDQExp THUFsimAncL (n) LiaIndriMan (M) FDUEmail TitBrf ualr08e02 TitExp pristask103 TitExpBrf57 pristask101 ualr08e04 (M) pristask104 (M) pristask102 (M) TitDes (n) ycbLS DERIrun8 (n) TOmUW (n) xLQOW (n) DERIrun7 (n) LiaIIcAuto ualr08e03 LiaIcAuto (n) 8T0eZ (n) Krcy7 (n) U2LwQ 4FvfI (n) Rkylv (n)</formula><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Figure 1: Box-and-whisker plots of infAP scores in the document search task, ordered by the run's mean infAP score across all topics. The box for each run extends from the first to the third quartile of the per-topic infAP scores; the whiskers extend to include topics not more than 1.5 times the interquartile distance; outliers are shown as circles. UGlasgow looked at query expansion using external resources. The resources included blind feedback from web search engine results, and Wikipedia. <ref type="bibr" coords="5,383.05,113.02,71.12,8.74" target="#b4">(He et al., 2008)</ref> CAS used BM25 and language models with blind feedback. <ref type="bibr" coords="5,374.52,132.94,80.26,8.74" target="#b12">(Shen et al., 2008)</ref> Tsinghua investigated link analysis methods in the CERC collection, as well as selecting queryindependent key pages based on outlinks and anchors. <ref type="bibr" coords="5,372.54,164.82,76.66,8.74" target="#b14">(Xue et al., 2008)</ref> UAmsterdam developed a novel language model that mixes document models with expert profile models, as a collection enrichment technique. <ref type="bibr" coords="5,363.60,196.70,117.06,8.74" target="#b2">(Balog and de Rijke, 2008)</ref> UC-London uses document search as one component of their expert search system. Their approach uses language models, and they investigate the use of anchor texts and in-degree counts. <ref type="bibr" coords="5,167.01,240.54,50.64,8.74" target="#b17">(Zhu, 2008)</ref> (Fudan do not describe their document search approach in their paper.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>UAvingon tried a number of different approaches; their top run employs a passage retrieval method that comes from their work in QA. <ref type="bibr" coords="5,319.63,292.34,98.32,8.74" target="#b10">(SanJuan et al., 2008)</ref> (UArkansas did not submit a final TREC proceedings paper as of this writing.)</p><p>NUI-Galway developed a term-weighting scheme based on BM25 that incorporates expert candidate profiles in determining the weights. <ref type="bibr" coords="5,335.09,344.15,143.57,8.74" target="#b3">(Cummins and O'Riordan, 2008)</ref> RMIT investigated using out-degree of pages within the results list ("local outdegree") to rerank. <ref type="bibr" coords="5,167.00,376.03,74.17,8.74" target="#b13">(Wu et al., 2008)</ref> Sebir investigated blind relevance feedback using Wikipedia as the expansion collection. <ref type="bibr" coords="5,496.68,395.96,25.32,8.74;5,132.91,407.91,68.36,8.74" target="#b9">(Peng and Mao, 2008)</ref> (BUPT did not submit a final TREC proceedings paper as of this writing.)</p><p>INRIA investigated weighted PageRank variants, in particular first clustering the collection and differentially weighting links within and between clusters. <ref type="bibr" coords="5,397.41,459.72,134.61,8.74;5,132.91,471.67,23.80,8.74" target="#b7">(Nemirovsky and Avrachenkov, 2008)</ref> SPSU looked at term and phrase weighting models based on entropy. <ref type="bibr" coords="5,426.67,491.60,90.03,8.74;5,132.91,503.55,58.42,8.74" target="#b8">(Nemirovsky and Dobrynin, 2008)</ref> As stated above, each topic was assigned to three participant groups for relevance assessment. In the end, a total of 67 out of the full set of 77 topics were judged. 10 topics were judged by three groups, 33 topics by two groups, and 24 topics by only a single group. The first group assigned was labeled as the primary assessor, and the judgments of the primary assessor were used in the official results. Four topics <ref type="bibr" coords="5,276.64,573.29,94.00,8.74">(51, 74, 108, and 116)</ref> had no relevant documents judged by the primary assessor; these topics were not used in the official evaluation.</p><p>We also created two sets of relevance judgments using the other assessors. The first used the judgments of the second assessor, unless no such assessor existed, in which case the primary assessor's judgments were used. The second used the judgments of the third assessor where such existed (otherwise falling back to the second or primary assessor as available). We dropped the four topics where no relevant documents were judged by the primary assessor, as well as topic 63 which had no relevant documents judged by the secondary assessor. We computed mean infAP for all systems using these "secondary" and "tertiary" relevance judgments, and computed the Kendall's τ rank correlation between the order of systems by the official, secondary, and tertiary sets. The τ value was 0.92 between the official judgments and both the secondary and tertiary The vertical line at 0.92 is the τ between the primary judge and both the secondary and tertiary sets. The line at 0.98 is the τ between the secondary and tertiary sets.</p><p>(95% confidence interval 0.65-0.98 in both cases), and 0.98 between the secondary and tertiary rankings themselves (interval 0.75-0.999) <ref type="bibr" coords="6,295.54,439.08,128.64,8.74" target="#b6">(Kendall and Gibbons, 1990)</ref>. We do not see any reason to believe there is a difference in correlations when judgement sets are changed. Lastly, we also constructed fifty sets of relevance judgments choosing a judge (primary, secondary, or tertiary) at random for each topic, and compared the resulting rankings among each other. The lowest τ between two of these rankings was 0.89, and the highest was 0.99. The mean τ among all pairs of rankings was 0.95. Figure <ref type="figure" coords="6,344.83,498.85,4.98,8.74" target="#fig_0">2</ref> shows the distribution of the τ values, along with the τ s between the primary, secondary, and tertiary judgments for comparison. From this we conclude that although differences do exist among the relevance judgments, this does not have a large effect on the document search rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Expert search task</head><p>For the expert search tasks, participants were asked to return email addresses of up to 100 candidate experts. Like in previous year, no canonical list of candidate experts was made available, email addresses were to be extracted from the data. Each group was allowed to submit up to four runs. Eleven groups submitted a total of 42 expert search runs. Of those, 32 were automatic runs using only the query field; 7 automatic runs used the narr field in addition to the query. Two groups submitted manual runs. Interestingly, the manual run LiaIcExp08 by SanJuan et al. ( <ref type="formula" coords="6,192.94,661.22,18.45,8.74">2008</ref>) not only involved multiple iterations of manual query reformulations, but was created entirely manually by a paid search professional. CAS focused on identifying authoritative persons by constructing a recommendation network of persons, then applying the PageRank algorithm on this network. In addition, different weights were assigned to various types of person occurrences. <ref type="bibr" coords="7,402.98,407.21,80.26,8.74" target="#b12">(Shen et al., 2008)</ref> UGlasgow applied a proximity-based variation of their Voting Model. They also investigated expanding candidate profiles with Web evidence. <ref type="bibr" coords="7,348.84,438.11,71.12,8.74" target="#b4">(He et al., 2008)</ref> Fudan introduced two methods to judge whether a person is more likely to be an expert. One method is to determine the roles of a person by the context of pages; the other is to judge the authority of a person by exploiting the structure of specific document types. <ref type="bibr" coords="7,501.52,480.97,20.47,8.74;7,132.91,492.93,52.03,8.74" target="#b15">(Yao et al., 2008)</ref> Tsinghua investigated the combination of profile-based and document-based methods. Link analysis and homepage detection were performed to identify high quality documents. They also experimented with automatic query type identification. <ref type="bibr" coords="7,397.30,535.78,76.66,8.74" target="#b14">(Xue et al., 2008)</ref> Wuhan developed a model that considers the probability of query generation separately for different expert identifiers; the ambiguity of abbreviated person names was also addressed. Additionally, they adopted a method to detect phrases in the query. <ref type="bibr" coords="7,433.61,578.64,82.61,8.74" target="#b5">(Jiang et al., 2008)</ref> UTwente combined the intranet-based ranking (produced using their infinite random walk based expert finding method) with various rankings obtained from the Web using search engine APIs. <ref type="bibr" coords="7,191.77,621.50,104.36,8.74" target="#b11">(Serdyukov et al., 2008)</ref> UC-London uses a document-centric generative approach, and investigates the use of anchor texts and in-degree counts. Associations between candidates and query terms are captured using a combination of windows of different sizes. <ref type="bibr" coords="7,351.78,664.36,50.65,8.74" target="#b17">(Zhu, 2008)</ref> NUI-Galway used genetic programming to find ranking functions, both for profiles-based and for document-based approaches. <ref type="bibr" coords="7,275.87,695.26,143.57,8.74" target="#b3">(Cummins and O'Riordan, 2008)</ref> UAvingon carried out both automatic and manual search. The automatic method ranks summaries corresponding to email addresses using baseline Indri retrieval. The manual run employed multiple iterations of query refinement. <ref type="bibr" coords="8,351.12,124.97,96.44,8.74" target="#b10">(SanJuan et al., 2008)</ref> (BUPT did not submit a final TREC proceedings paper as of this writing.)  Although the track did not solicit explicit baseline runs, we make some general observations of contrasting runs within several groups. Two groups (UAvignon and BUPT) submitted both automatic and manual runs; for both teams, their best performing submission was a manual run. Two groups (Tsinghua and NUI-Galway) had both query-only runs and runs using the narr field as well; the query-only runs performed better in both cases. Finally, two groups (UAmsterdam and UGlasgow) had submissions both with and without using external resources (Web search engine APIs). In one case (UAmsterdam) using external resources resulted in improvements, while in the other (UGlasgow) it did not. As described in Section 2.4, relevance assessments were created by participants. Based on the judgments made, different sets of qrels could be created, depending on how agreement between assessors is handled. In addition, we also consider the manual run LiaIcExp08 by <ref type="bibr" coords="9,460.67,220.61,61.33,8.74;9,108.00,232.57,27.67,8.74" target="#b10">SanJuan et al. (2008)</ref> as an alternative. Consequently, four different sets of relevance judgments were obtained; see Table <ref type="table" coords="9,151.50,244.52,3.87,8.74" target="#tab_4">4</ref>.</p><p>Table <ref type="table" coords="9,150.03,256.48,4.98,8.74" target="#tab_3">3</ref> displays the MAP and MRR scores for all submitted runs, using the different sets of ground truth. Runs are ordered by their MAP scores according to the official set of qrels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg. #experts Qrels Description per topic Majority</head><p>A person is considered to be an expert 10.4 if most assessors said so (tie votes taken as relevant). This was used as the official set of qrels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lenient</head><p>A person is considered to be an expert 12.6 if at least one assessor said so. Unanimous A person is considered to be an expert 4.8 if all assessors agreed. LiaIcExp08 Judgments performed by an independent, 2.4 external search professional <ref type="bibr" coords="9,317.82,409.90,94.79,8.74" target="#b10">(SanJuan et al., 2008)</ref>. To compare the rankings of systems using the different qrels sets we used Kendall's τ correlation. The systems defined by their runs, are ordered by some metric (MAP or MRR) for each qrels set, and the two rankings are compared. The run LiaIcExp08 was ignored when using it as qrels. Table <ref type="table" coords="9,177.27,635.49,4.98,8.74" target="#tab_5">5</ref> reports the Kendall τ correlation given each qrels set against the other. We found strong correlation between the rankings of systems using the ground truths obtained from community judging (Majority, Lenient, and Unanimous). The LiaIcExp08 qrels set showed moderate correlation against the others. One reason for that is that the number of experts identified for each topic is much lower than for the other qrels sets; in fact, according to the majority qrels, LiaIcExp08 had the lowest recall of all runs. We also note that the professional's judgement is possibly more demanding than a participant's; and that the latter know how systems make ranking decisions and may themselves think similarly. We leave further examination and analysis to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>The fourth year of the enterprise track has featured the same tasks and collection as in the 2007 edition: document and expert search on the CERC corpus. Topics have been extracted from a log of real email enquiries. The only difference compared to the previous year is that both tasks were judged by participants. Although disagreements between assessors do exist, these do not have a large effect on the rankings of systems for either of the tasks. Common themes for this year's document search task included query expansion using external sources <ref type="bibr" coords="10,172.30,251.14,71.58,8.74" target="#b4">(He et al., 2008;</ref><ref type="bibr" coords="10,247.72,251.14,93.03,8.74" target="#b9">Peng and Mao, 2008)</ref>, exploiting expertise profiles <ref type="bibr" coords="10,473.47,251.14,48.54,8.74;10,108.00,263.09,66.29,8.74" target="#b2">(Balog and de Rijke, 2008;</ref><ref type="bibr" coords="10,178.45,263.09,140.55,8.74" target="#b3">Cummins and O'Riordan, 2008)</ref>, and leveraging link-structure in the form of in-degree <ref type="bibr" coords="10,150.35,275.05,48.68,8.74" target="#b17">(Zhu, 2008)</ref>, out-degree <ref type="bibr" coords="10,254.98,275.05,71.61,8.74" target="#b13">(Wu et al., 2008)</ref>, or PageRank <ref type="bibr" coords="10,393.04,275.05,74.65,8.74" target="#b14">(Xue et al., 2008;</ref><ref type="bibr" coords="10,470.72,275.05,51.28,8.74;10,108.00,287.00,104.47,8.74" target="#b7">Nemirovsky and Avrachenkov, 2008)</ref>. The best performing document search run employed a query performance predictor mechanism to selectively apply collection enrichment (i.e., query expansion) based on Wikipedia on a per-query basis; retrieval was performed using the Divergence From Randomness framework <ref type="bibr" coords="10,214.85,322.87,69.46,8.74" target="#b4">(He et al., 2008)</ref>.</p><p>As to expert search, methods and approaches employed this year included special treatment of different types of person occurrences <ref type="bibr" coords="10,282.52,346.78,79.52,8.74" target="#b12">(Shen et al., 2008;</ref><ref type="bibr" coords="10,365.48,346.78,71.22,8.74" target="#b15">Yao et al., 2008;</ref><ref type="bibr" coords="10,440.13,346.78,77.44,8.74" target="#b5">Jiang et al., 2008)</ref>, link analysis <ref type="bibr" coords="10,167.13,358.74,78.57,8.74" target="#b14">(Xue et al., 2008;</ref><ref type="bibr" coords="10,250.02,358.74,46.11,8.74" target="#b17">Zhu, 2008)</ref>, proximity-based techniques <ref type="bibr" coords="10,429.04,358.74,92.96,8.74;10,108.00,370.69,22.69,8.74" target="#b2">(Balog and de Rijke, 2008;</ref><ref type="bibr" coords="10,134.60,370.69,67.91,8.74" target="#b4">He et al., 2008;</ref><ref type="bibr" coords="10,206.42,370.69,45.70,8.74" target="#b17">Zhu, 2008)</ref>, the use of external evidence <ref type="bibr" coords="10,387.88,370.69,118.31,8.74" target="#b2">(Balog and de Rijke, 2008;</ref><ref type="bibr" coords="10,510.10,370.69,11.90,8.74;10,108.00,382.65,52.45,8.74" target="#b4">He et al., 2008;</ref><ref type="bibr" coords="10,164.54,382.65,101.12,8.74" target="#b11">Serdyukov et al., 2008)</ref>, and the combination of candidate-and document-based methods <ref type="bibr" coords="10,148.51,394.60,117.25,8.74" target="#b2">(Balog and de Rijke, 2008;</ref><ref type="bibr" coords="10,269.40,394.60,72.09,8.74" target="#b14">Xue et al., 2008)</ref>. The best performing expert search run used a Language Modeling framework to combine three models: a proximity-based candidate model, a document-based model, and a Web-based variation of the candidate model <ref type="bibr" coords="10,474.31,418.51,47.69,8.74;10,108.00,430.47,64.07,8.74" target="#b2">(Balog and de Rijke, 2008)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,108.00,371.39,414.00,8.74;6,108.00,383.34,414.00,8.74;6,108.00,395.30,410.35,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Distribution of τ values taken between all pairs of rankings in the randomly-selectedjudge experiment. The vertical line at 0.92 is the τ between the primary judge and both the secondary and tertiary sets. The line at 0.98 is the τ between the secondary and tertiary sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,108.00,472.13,414.00,220.29"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,153.06,472.13,323.88,176.51"><row><cell></cell><cell>Group</cell><cell cols="4">Type Fields Mean infAP infNDCG</cell></row><row><cell cols="2">uogTrEDSelW UGlasgow</cell><cell>auto</cell><cell>q</cell><cell>0.3891</cell><cell>0.5660</cell></row><row><cell>ICTI3Sdoc03</cell><cell>CAS</cell><cell>auto</cell><cell>q</cell><cell>0.3760</cell><cell>0.5393</cell></row><row><cell>THUFmfS</cell><cell>Tsinghua</cell><cell>auto</cell><cell>q</cell><cell>0.3612</cell><cell>0.5578</cell></row><row><cell>UvA08DSall</cell><cell cols="2">UAmsterdam auto</cell><cell>q</cell><cell>0.3306</cell><cell>0.4909</cell></row><row><cell>ucl01</cell><cell>UC-London</cell><cell>auto</cell><cell>q</cell><cell>0.3246</cell><cell>0.5175</cell></row><row><cell>FDUBase</cell><cell>Fudan</cell><cell>auto</cell><cell>q</cell><cell>0.3204</cell><cell>0.4985</cell></row><row><cell>LIAIndriSiac</cell><cell>UAvingon</cell><cell>auto</cell><cell>qn</cell><cell>0.3191</cell><cell>0.5078</cell></row><row><cell>ualr08e01</cell><cell>UArkansas</cell><cell>auto</cell><cell>q</cell><cell>0.3024</cell><cell>0.4838</cell></row><row><cell>DERIrun6</cell><cell>NUI-Galway</cell><cell>auto</cell><cell>q</cell><cell>0.3018</cell><cell>0.4791</cell></row><row><cell>RmitDocQ</cell><cell>RMIT</cell><cell>auto</cell><cell>q</cell><cell>0.2975</cell><cell>0.5045</cell></row><row><cell>TitBrf</cell><cell>Sebir</cell><cell>auto</cell><cell>q</cell><cell>0.2252</cell><cell>0.4035</cell></row><row><cell>pristask103</cell><cell>BUPT</cell><cell>auto</cell><cell>q</cell><cell>0.2216</cell><cell>0.4046</cell></row><row><cell>ycbLS</cell><cell>INRIA</cell><cell>auto</cell><cell>q</cell><cell>0.1879</cell><cell>0.3785</cell></row><row><cell>xLQOW</cell><cell>SPSU</cell><cell>auto</cell><cell>qn</cell><cell>0.1300</cell><cell>0.3057</cell></row></table><note coords="4,147.76,671.73,374.25,8.74;4,108.00,683.69,66.03,8.74"><p>The top run from each group by mean infAP, showing the mean infAP and infNDCG scores for each.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,122.94,685.13,399.06,8.74"><head>Table 2 :</head><label>2</label><figDesc>The top run from each group by mean AP, showing the mean AP and mean RR scores for each. Reported results use the official qrels.</figDesc><table coords="7,163.49,122.34,303.03,140.64"><row><cell>Run</cell><cell>Group</cell><cell>Type</cell><cell cols="2">Fields MAP</cell><cell>MRR</cell></row><row><cell>UvA08ESweb</cell><cell cols="2">UAmsterdam auto</cell><cell>q</cell><cell cols="2">0.4490 0.8721</cell></row><row><cell>ICTI3Sexp01</cell><cell>CAS</cell><cell>auto</cell><cell>q</cell><cell cols="2">0.4214 0.7241</cell></row><row><cell>uogTrEXfeNPC</cell><cell>UGlasgow</cell><cell>auto</cell><cell>q</cell><cell cols="2">0.4126 0.7611</cell></row><row><cell>FDURoleRes</cell><cell>Fudan</cell><cell>auto</cell><cell>qn</cell><cell cols="2">0.4114 0.7516</cell></row><row><cell>THUPDDlchrS</cell><cell>Tsinghua</cell><cell>auto</cell><cell>q</cell><cell cols="2">0.3846 0.7419</cell></row><row><cell cols="2">WHU08NOPHR Wuhan</cell><cell>auto</cell><cell>q</cell><cell cols="2">0.3826 0.6770</cell></row><row><cell>utqurl</cell><cell>UTwente</cell><cell>auto</cell><cell>q</cell><cell cols="2">0.3728 0.7647</cell></row><row><cell>UCLex04</cell><cell>UC-London</cell><cell>auto</cell><cell>q</cell><cell cols="2">0.3476 0.6759</cell></row><row><cell>DERIrun3</cell><cell>NUI-Galway</cell><cell>auto</cell><cell>q</cell><cell cols="2">0.2619 0.6212</cell></row><row><cell>LiaIcExp08</cell><cell>UAvingon</cell><cell cols="2">manual qn</cell><cell cols="2">0.2513 0.8545</cell></row><row><cell>pristask204</cell><cell>BUPT</cell><cell cols="2">manual qn</cell><cell cols="2">0.0977 0.2343</cell></row></table><note coords="6,122.94,685.13,399.06,8.74;7,108.00,101.06,244.17,8.74;7,108.00,328.45,414.00,8.77;7,132.91,340.44,389.09,8.74;7,132.91,352.39,389.09,8.74;7,132.91,364.35,221.53,8.74"><p>Table 2 shows the MAP and MRR scores for the top run from each group (by MAP). Below, we present a brief summary of participants' approaches. UAmsterdam used a combination of multiple approaches; a proximity-based version of their candidate model (Model 1B), the document-based model (Model 2), and a Web-based variation of Model 1B (to bring in external evidence). Additionally, they applied profilebased query expansion. (Balog and de Rijke, 2008)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,108.00,681.13,414.00,20.69"><head>Table 3 :</head><label>3</label><figDesc>All submitted runs, ordered by official MAP scores. MAP and MRR scores using different sets of qrels are also shown; highest scores for each are typeset in boldface.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,161.67,441.72,306.66,113.59"><head>Table 4 :</head><label>4</label><figDesc>Alternative qrels sets for the expert finding task.</figDesc><table coords="9,161.67,474.44,306.66,80.87"><row><cell>Qrels</cell><cell cols="4">Metric Majority Lenient Unanimous LiaIcExp08</cell></row><row><cell>Majority</cell><cell>MAP</cell><cell>0.8722</cell><cell>0.8420</cell><cell>0.5804</cell></row><row><cell></cell><cell>MRR</cell><cell>0.9070</cell><cell>0.8072</cell><cell>0.6487</cell></row><row><cell>Lenient</cell><cell>MAP</cell><cell></cell><cell>0.7653</cell><cell>0.5560</cell></row><row><cell></cell><cell>MRR</cell><cell></cell><cell>0.8257</cell><cell>0.6634</cell></row><row><cell cols="2">Unanimous MAP</cell><cell></cell><cell></cell><cell>0.5926</cell></row><row><cell></cell><cell>MRR</cell><cell></cell><cell></cell><cell>0.6243</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,236.11,578.40,157.77,8.74"><head>Table 5 :</head><label>5</label><figDesc>Kendall τ rank correlation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,108.00,430.47,414.00,207.14"><head>Table 6 :</head><label>6</label><figDesc>. Tasks and number of participating groups at the TREC Enterprise Track.The Enterprise Track was introduced in 2005, and after four successful years, it came to an end in 2008. Since its introduction, the track, and especially the expert finding task, has generated a lot of interest within the research community, with rapid progress being made in terms of algorithms, modeling, and evaluation. Table6lists the tasks featured at the Enterprise track throughout the years. The Entity Search Track, implemented at TREC 2009 can be seen as a continuation of the expert search task, extending it along two dimensions: type (from peopleonly to multiple types of entities) and scale (from Intranet to Web).</figDesc><table coords="10,368.54,452.51,18.29,8.74"><row><cell>year</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,108.00,683.31,413.99,8.74;10,117.96,695.26,169.93,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,344.55,683.31,177.44,8.74;10,117.96,695.26,15.94,8.74">The CSIRO enterprise search test collection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,142.31,695.26,57.90,8.74">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="42" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,101.06,414.00,8.74;11,117.96,113.02,404.04,8.74;11,117.96,124.97,335.00,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,462.54,101.06,59.46,8.74;11,117.96,113.02,244.87,8.74">Relevance assessment: are judges exchangeable and does it matter?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,385.29,113.02,136.71,8.74;11,117.96,124.97,165.23,8.74">Proceedings of the 31st Annual International ACM SIGIR Conference</title>
		<meeting>the 31st Annual International ACM SIGIR Conference<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="667" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,144.36,414.00,8.74;11,117.96,156.31,404.04,8.74;11,117.96,168.27,45.52,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,283.19,144.36,238.81,8.74;11,117.96,156.31,25.45,8.74">Combining candidate and document models for expert search</title>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,166.64,156.31,286.88,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,187.65,414.00,8.74;11,117.96,199.61,384.59,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,285.69,187.65,194.79,8.74">DERI at TREC 2008 enterprise search track</title>
		<author>
			<persName coords=""><forename type="first">Ronan</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Colm</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Riordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,502.89,187.65,19.11,8.74;11,117.96,199.61,267.94,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,219.00,414.00,8.74;11,117.96,230.95,176.72,8.74;11,322.19,230.95,199.81,8.74;11,117.96,242.91,404.03,8.74;11,117.96,254.86,45.52,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,464.86,219.00,57.14,8.74;11,117.96,230.95,176.72,8.74;11,322.19,230.95,199.81,8.74;11,117.96,242.91,28.41,8.74">University of Glasgow at TREC 2008: Experiments in enterprise, and relevance feedback tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,168.54,242.91,285.26,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,274.25,414.00,8.74;11,117.96,286.20,404.03,8.74;11,117.96,298.16,139.33,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,296.25,274.25,225.75,8.74;11,117.96,286.20,135.25,8.74">CSIR at TREC 2008 expert search task: Modeling expert evidence in expert search</title>
		<author>
			<persName coords=""><forename type="first">Jiepu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haozhen</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,271.70,286.20,250.29,8.74;11,117.96,298.16,22.67,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,317.55,414.00,8.74;11,117.96,329.50,105.44,8.74" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Maurice</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><forename type="middle">Dickinson</forename><surname>Gibbons</surname></persName>
		</author>
		<title level="m" coord="11,321.68,317.55,109.81,8.74">Rank correlation methods</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>5th edition</note>
</biblStruct>

<biblStruct coords="11,108.00,348.89,413.99,8.74;11,117.96,360.84,404.03,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,325.61,348.89,192.02,8.74">Weighted PageRank: cluster-related weights</title>
		<author>
			<persName coords=""><forename type="first">Danil</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantin</forename><surname>Avrachenkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,129.68,360.84,277.57,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,380.23,414.00,8.74;11,117.96,392.18,404.04,8.74;11,117.96,404.14,45.52,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,295.38,380.23,226.62,8.74;11,117.96,392.18,28.47,8.74">Word importance discrimination using context information</title>
		<author>
			<persName coords=""><forename type="first">Danil</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Dobrynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,169.05,392.18,284.82,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,423.53,414.00,8.74;11,117.96,435.48,399.62,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,237.67,423.53,262.83,8.74">Blind relevance feedback with Wikipedia: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Yefei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,117.96,435.48,282.97,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,454.87,414.00,8.74;11,117.96,466.82,404.04,8.74;11,117.96,478.78,226.81,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,438.21,454.87,83.79,8.74;11,117.96,466.82,214.45,8.74">Universities of Avignon &amp; Lyon III at TREC 2008: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Flavier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fidelia</forename><surname>Ibekwe-Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,353.19,466.82,168.82,8.74;11,117.96,478.78,110.15,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,498.16,414.00,8.74;11,117.96,510.12,404.04,8.74;11,117.96,522.08,318.14,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,342.03,498.16,179.97,8.74;11,117.96,510.12,304.59,8.74">University of Twente at the TREC 2008 enterprise track: Using the global web as an expertise evidence source</title>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,443.96,510.12,78.04,8.74;11,117.96,522.08,201.49,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,541.46,414.00,8.74;11,117.96,553.42,404.04,8.74;11,117.96,565.37,71.56,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,389.46,541.46,132.54,8.74;11,117.96,553.42,50.08,8.74">Research on enterprise track of TREC 2008</title>
		<author>
			<persName coords=""><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenjing</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,189.82,553.42,283.66,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,584.76,414.00,8.74;11,117.96,596.71,404.03,8.74;11,117.96,608.67,45.52,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,324.70,584.76,197.30,8.74;11,117.96,596.71,20.78,8.74">RMIT University at TREC 2008: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Mingfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,163.39,596.71,289.66,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,628.06,414.00,8.74;11,117.96,640.01,404.03,8.74;11,117.96,651.97,139.33,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,475.65,628.06,46.35,8.74;11,117.96,640.01,124.92,8.74">THUIR at TREC2008: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guichun</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,264.43,640.01,257.57,8.74;11,117.96,651.97,22.67,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.00,671.35,413.99,8.74;11,117.96,683.31,404.04,8.74;11,117.96,695.26,108.79,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,274.94,671.35,247.06,8.74;11,117.96,683.31,88.67,8.74">Using role determination and expert mining in the enterprise environment</title>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,230.43,683.31,287.03,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,101.06,413.99,8.74;12,117.96,113.02,404.04,8.74;12,117.96,124.97,161.84,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,289.95,101.06,232.05,8.74;12,117.96,113.02,64.73,8.74">A simple and efficient sampling method for estimating AP and NDCG</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,209.12,113.02,308.27,8.74">Proceedings of the 31st Annual International ACM SIGIR Conference</title>
		<meeting>the 31st Annual International ACM SIGIR Conference<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.00,144.90,414.00,8.74;12,117.96,156.85,334.69,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,169.20,144.90,271.27,8.74">The University College London at TREC 2008 enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Jianhan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,460.86,144.90,61.15,8.74;12,117.96,156.85,218.03,8.74">Proceedings of the 2008 Text REtrieval Conference (TREC 2008)</title>
		<meeting>the 2008 Text REtrieval Conference (TREC 2008)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
