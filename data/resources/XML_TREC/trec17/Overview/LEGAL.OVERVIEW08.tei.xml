<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.68,112.00,346.64,15.15">Overview of the TREC 2008 Legal Track</title>
				<funder ref="#_WwzxfXD">
					<orgName type="full">TREC Legal Track</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.14,145.86,85.35,8.77"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
						</author>
						<author>
							<persName coords="1,236.66,201.65,61.21,8.77"><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
							<email>bhedin@h5.com</email>
						</author>
						<author>
							<persName coords="1,199.57,243.50,93.63,8.77"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
							<email>stomlins@opentext.com</email>
						</author>
						<author>
							<persName coords="1,211.10,285.34,75.67,8.77"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
							<email>jason.baron@nara.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Information Studies and Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">H5</orgName>
								<address>
									<addrLine>71 Stevenson St</addrLine>
									<postCode>94105</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Waterloo (UWIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.68,112.00,346.64,15.15">Overview of the TREC 2008 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">75A2EB217B7F6380B13BC8285109F250</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>â€¢ RMIT University: OCR error minimization</term>
					<term>noise term removal</term>
					<term>text de-hyphenation</term>
					<term>ispell dictionary</term>
					<term>Zettair search engine</term>
					<term>Dirichlet-smoothed language model Thresholding a Ranked List</term>
					<term>score-distributional threshold optimization (s-d)</term>
					<term>Probability Thresholds</term>
					<term>Bayes&apos; rule</term>
					<term>Truncated Normal-Exponential Model</term>
					<term>Theoretical Truncation</term>
					<term>Technical Truncation</term>
					<term>Expectation Maximization</term>
					<term>Apache&apos;s Lucene Lucene StandardAnalyzer</term>
					<term>Okapi-BM25</term>
					<term>query expansion</term>
					<term>wildcard expansions</term>
					<term>pseudo-relevance feedback</term>
					<term>WordNet</term>
					<term>weighted CombSum method</term>
					<term>Reference Run boost : fusion IR methods</term>
					<term>stepwise logistic regression</term>
					<term>Wumpus search engine</term>
					<term>cover density ranking</term>
					<term>Okapi BM25</term>
					<term>character 4-grams</term>
					<term>MultiText</term>
					<term>CombMNZ combination method</term>
					<term>linear and logarithmic transfer functions.</term>
					<term>Ursinus College: Latent Semantic Indexing (LSI)</term>
					<term>Essential Dimensions of Latent Semantic Indexing (EDLSI)</term>
					<term>Distributed EDLSI</term>
					<term>BM25 weighting</term>
					<term>power normalization technique</term>
					<term>singular value decomposition (SVD)</term>
					<term>log-entropy weighting</term>
					<term>OCR error detection</term>
					<term>automatic query expansion. 2.4 Reference runs tot=10</term>
					<term>hrel=4</term>
					<term>orel=1</term>
					<term>non=3</term>
					<term>gr=2 tot=10</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=10</term>
					<term>gr=0 28 (2006-C-4) tot=10</term>
					<term>hrel=9</term>
					<term>orel=0</term>
					<term>non=1</term>
					<term>gr=0 tot=10</term>
					<term>hrel=3</term>
					<term>orel=1</term>
					<term>non=6</term>
					<term>gr=0 31 (2006-C-7) tot=10</term>
					<term>hrel=6</term>
					<term>orel=3</term>
					<term>non=1</term>
					<term>gr=0 tot=10</term>
					<term>hrel=1</term>
					<term>orel=1</term>
					<term>non=8</term>
					<term>gr=0 36 (2006-D-3) tot=10</term>
					<term>hrel=0</term>
					<term>orel=7</term>
					<term>non=3</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=10</term>
					<term>gr=0 47 (2006-E-6) tot=6</term>
					<term>hrel=0</term>
					<term>orel=2</term>
					<term>non=4</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=2</term>
					<term>non=8</term>
					<term>gr=0 60 (2007-A-9) tot=10</term>
					<term>hrel=2</term>
					<term>orel=3</term>
					<term>non=1</term>
					<term>gr=4 tot=10</term>
					<term>hrel=1</term>
					<term>orel=2</term>
					<term>non=7</term>
					<term>gr=0 73 (2007-B-5) tot=10</term>
					<term>hrel=1</term>
					<term>orel=0</term>
					<term>non=9</term>
					<term>gr=0 tot=10</term>
					<term>hrel=1</term>
					<term>orel=3</term>
					<term>non=6</term>
					<term>gr=0 79 (2007-C-1) tot=10</term>
					<term>hrel=4</term>
					<term>orel=3</term>
					<term>non=3</term>
					<term>gr=0 tot=10</term>
					<term>hrel=1</term>
					<term>orel=2</term>
					<term>non=7</term>
					<term>gr=0 80 (2007-C-2) tot=10</term>
					<term>hrel=0</term>
					<term>orel=8</term>
					<term>non=2</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=2</term>
					<term>non=8</term>
					<term>gr=0 83 (2007-C-5) tot=10</term>
					<term>hrel=0</term>
					<term>orel=4</term>
					<term>non=6</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=10</term>
					<term>gr=0 85 (2007-C-7) tot=10</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=10</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=1</term>
					<term>non=9</term>
					<term>gr=0 89 (2007-D-1) tot=10</term>
					<term>hrel=2</term>
					<term>orel=7</term>
					<term>non=1</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=1</term>
					<term>non=9</term>
					<term>gr=0 Totals tot=116</term>
					<term>hrel=28</term>
					<term>orel=38</term>
					<term>non=44</term>
					<term>gr=6 tot=120</term>
					<term>hrel=7</term>
					<term>orel=15</term>
					<term>non=98</term>
					<term>gr=0</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC 2008 was the third year of the Legal Track, which focuses on evaluation of search technology for discovery of electronically stored information in litigation and regulatory settings. The track included three tasks: Ad Hoc (i.e., single-pass automatic search), Relevance Feedback (two-pass search in a controlled setting with some relevant and nonrelevant documents manually marked after the first pass) and Interactive (in which real users could iteratively refine their queries and/or engage in multi-pass relevance feedback). This paper describes the design of the three tasks and presents the official results.</p><p>search string for purposes of a baseline Boolean search were thereafter conducted among selected Sedona Conference members. The final topic set contained 45 topics, numbered 102 to 151, of which topics 102-104 were used in the Interactive task. Those three topics were run by Ad Hoc task participants, but were not sampled or scored as part of the Ad Hoc task. An XML formatted version of the topics (fullL08.xml) was created for (potentially automated) use by the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Participation</head><p>Participating teams were allowed to submit up to 8 runs; additional runs could be scored locally. A total of 10 research teams submitted 64 runs for this year's Ad Hoc task. The teams experimented with a wide variety of techniques including the following:</p><p>â€¢ Centro Nazionale per l'Informatica nella Pubblica Amministrazione (CNIPA): Terrier (TERabyte Re-trIEveR) Information Retrieval platform, DFRee model, Bo1 (Bose-Einstein statistics) term weighting models, Boolean re-rank, query lexicon, query performance prediction, Z-Score.</p><p>â€¢ Open Text Corporation: negotiated Boolean queries, defendant Boolean, rank-based merging of vector results with the reference Boolean results, blind feedback, fusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The use of information retrieval techniques in law has traditionally focused on providing access to legislation, regulations, and judicial decisions. Searching business records for information pertinent to a case (or "discovery") has also been important, but searching records in electronic form was until recently the exception rather than the norm. The goal of the Legal Track at the Text Retrieval Conference (TREC) is to assess the ability of information retrieval technology to meet the needs of the legal community for tools to help with retrieval of business records, an issue of increasing importance given the vast amount of information stored in electronic form to which access is increasingly desired in the context of current litigation. Ideally, the results of a study of how well comparative search methodologies perform when tasked to execute types of queries that arise in real litigation will serve to better educate the legal community on the feasibility of automated retrieval as well as its limitations. The TREC Legal Track was held for the first time in 2006, when 6 research teams participated in an Ad Hoc retrieval task. In 2007, 13 research teams participated in at least one of the track's three tasks (Ad Hoc, Interactive, and Relevance Feedback). This year, there were a total of 15 participating research teams.</p><p>The key goal of the TREC Legal Track is to develop and apply objective criteria for comparing methods for searching large heterogeneous collections using topics that approximate how real lawyers would go about propounding discovery in civil litigation, and to create a large, representative (unstructured and heterogeneous) test collection. Important aspects of this task include a focus on returning sets of documents for subsequent human review (rather than ranked lists), the need to accommodate topics that return relatively large result sets (which necessitates sampling for assessment), the importance of recall in those result sets (since in real settings many requests for production state that "all" such evidence is to be produced), and the importance of precision in those result sets (to reduce unnecessary review costs).</p><p>The 2008 Legal Track includes the same three tasks as in 2007, but with some changes to each. For the Ad Hoc task, the most significant changes were the introduction of a new "highly relevant" category, the use of the balanced F measure as a way of simultaneously reflecting the importance of recall (for exhaustiveness) and precision (for timeliness and affordability), and a new requirement that participating teams estimate the optimal rank threshold for their system (in 2007, all systems had been compared at the number of documents returned by a "reference Boolean run"). The same changes were also made for the Relevance Feedback task. The Interactive task was completely redesigned to more closely model actual practice in e-discovery settings.</p><p>The increased visibility of the TREC Legal Track and its importance to the greater legal community were in evidence in 2008. The introduction of a completely redesigned Interactive task this year was accompanied by a signed open letter to the legal profession from Ellen Voorhees and the leadership of The Sedona Conference, urging participation this year by legal service providers and other interested parties <ref type="bibr" coords="2,495.14,230.58,14.61,8.74" target="#b9">[11]</ref>. Also, for the first time, in May 2008 the TREC Legal Track was expressly discussed in a U.S. federal court opinion involving the failure of a party to use an adequate search protocol in connection with filtering out potentially privileged documents in litigation. Judge Grimm, writing in Victor Stanley v. Creative Pipe <ref type="bibr" coords="2,483.58,266.44,14.61,8.74">[13]</ref>, went on to make this rather extraordinary set of observations about discovery of Electronically Stored Information (ESI):</p><p>"[T]here is room for optimism that as search and information retrieval methodologies are studied and tested, this will result in identifying those that are most effective and least expensive to employ for a variety of ESI discovery tasks. Such a study has been underway since 2006, when the National Institute of Standards and Technology (NIST), an agency within the U.S. Department of Commerce, embarked on a cooperative endeavor . . . to evaluate the effectiveness of a variety of search methodologies. This project, known as the Text Retrieval Conference (TREC) . . . Legal Track, [is] a research effort aimed at studying the e-discovery review process to evaluate the effectiveness of a wide array of search methodologies. This evaluative process is open to participation by academics, law firms, corporate counsel and companies providing ESI discovery services. . . . The goal of the project is to create industry best practices for use in electronic discovery. This project can be expected to identify both cost effective and reliable search and information retrieval methodologies and best practice recommendations, which, if adhered to, certainly would support an argument that the party employing them performed a reasonable ESI search, whether for privilege review or other purposes."</p><p>Whether or not the results of the TREC Legal Track to date can be said to meet the judiciary's expectations, it is nevertheless the case that the opinion in Victor Stanley is only one published decision in a growing body of court precedent acknowledging greater sophistication in information retrieval techniques, and calling for parties to collaborate over appropriate search protocols. (We also note that, in another recent opinion, Judge Scheindlin cited data from the 2006 Legal Track in support of her decision on one question at dispute in the lawsuit <ref type="bibr" coords="2,181.16,505.54,14.76,8.74" target="#b8">[10]</ref>). These cases have arisen in the aftermath of the changes to the Federal Rules of Civil Procedure involving "electronically stored information" that went into effect in December 2006.</p><p>The remainder of this paper is organized as follows. Section 2 describes the Ad Hoc task, Section 3 describes the Relevance Feedback task, Section 4 describes the Interactive task, and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ad Hoc Task</head><p>In the Ad Hoc task, the participants were given requests to produce documents, herein called "topics," and a set of documents to search. The following sections provide more details, but an overview of the differences from the previous year is as follows:</p><p>â€¢ The main evaluation measure this year was F 1 @K, where K was specified by the participating system for each topic. This new requirement gave systems the opportunity to show that they could produce a closer set to the optimal set of R relevant documents than the reference Boolean run (for which K=B, where B is the number of documents matched by the final negotiated Boolean query). It also modeled a real operational requirement of e-discovery systems to return a set of documents, not just an unbounded ranked list.</p><p>â€¢ Participating teams were allowed to submit up to 100,000 documents for each topic (up from 25,000 in 2007). The maximum B value (the number of documents matched by the final negotiated Boolean query) was likewise increased to 100,000 this year.</p><p>â€¢ The concept of "highly relevant" documents as a third category for purposes of assessment was introduced (in addition to last year's "relevant" and "not relevant"). This was an experiment for investigating the problem of isolating a set of "hot" or "material" documents for use in later phases of discovery (e.g., depositions) and at trial from a large set of potentially merely tangentially relevant documents, which remains a key concern for the legal profession. Participating systems could specify a different K h value than K value for each topic for targeting a set of just highly relevant documents.</p><p>â€¢ Some topics had longer negotiation histories (i.e., a greater number of Boolean queries) than last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Collection</head><p>The 2008 Legal Track used the same collection as the 2006 and 2007 Legal Tracks, the IIT Complex Document Information Processing (CDIP) Test Collection, version 1.0 (referred to here as "IIT CDIP 1.0") which is based on documents released under the tobacco "Master Settlement Agreement" (MSA). The University of California San Francisco (UCSF) Library, with support from the American Legacy Foundation, has created a permanent repository, the Legacy Tobacco Documents Library (LTDL), for tobacco documents <ref type="bibr" coords="3,503.88,336.63,9.96,8.74" target="#b7">[9]</ref>. The IIT CDIP 1.0 collection is based on a snapshot, generated between November 2005 and January 2006, of the MSA subcollection of the LTDL. The IIT CDIP 1.0 collection consists of 6,910,192 document records in the form of XML elements. See the 2006 TREC Legal Track overview paper for additional details about the IIT CDIP 1.0 collection <ref type="bibr" coords="3,160.83,384.45,9.96,8.74" target="#b3">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topics</head><p>Topic development in 2008 continued to be modeled on U.S. civil discovery practice. In the litigation context, a "complaint" is filed in court, outlining the theory of the case, including factual assertions and causes of action representing the legal theories of the case. In a regulatory context, often formal letters of inquiry serve a similar purpose by outlining the scope of the proposed investigation. In both situations, soon thereafter one or more parties create and transmit formal "requests for the production of documents" to adversary parties, based on the issues raised in the complaint or letter of inquiry. See the TREC 2006 Legal Track overview for additional background <ref type="bibr" coords="3,229.00,502.46,9.96,8.74" target="#b3">[5]</ref>.</p><p>For the TREC 2008 Legal Track, three new hypothetical complaints were created by members of the Sedona Conference R Working Group on Electronic Document Retention and Production, a nonprofit group of lawyers who play a leading role in the development of professional practices for e-discovery. These complaints described: (1) a combined wrongful death, negligence, and medical malpractice action against a corporate owner of a factory making fire-resistant products, and the hospital at which the fictional worker died; (2) a fictional U.S. regulatory agency's investigation into a variety of incidents in the Asian trade market involving violations of binding trade agreements between the United States and its Asian trading partners, and (3) a shareholder class action suit alleging securities fraud advertising in connection with a fictional tobacco company's "We're Smokin"' campaign. As in the past two years of the Legal Track, in using fictional names and jurisdictions, the track coordinators attempted to ensure that no third party would mistake the academic nature of the TREC Legal Track for an actual lawsuit involving real-world companies or individuals, and any would-be link or association with either past or present real litigation was entirely unintentional.</p><p>For each complaint, a set of topics (formally, "requests to produce") were initially created by the creator of the complaint, and revised by the track coordinators. "Boolean negotiations" to arrive at a consensus â€¢ refL08B: final negotiated Boolean query (from the FinalQuery field of the .xml topic file) Note that the participants were provided with the refL08B run at the time of topic release, but the other 3 Boolean reference runs (xref runs) were not available in time for participants to use them.</p><p>Also note that in some cases, the xrefL08P and xrefL08C runs included more than 100,000 documents for a topic (which was not allowed for participant runs). The most was 1,194,522 matches for the plaintiff query of topic 133. Also, in some cases, the xrefL08D run matched 0 documents, in which case the first document (aaa00a00) was submitted as a placeholder (it was always judged non-relevant).</p><p>The 5th reference run, called "randomL08", consisted of, for each topic, 100,000 randomly chosen documents from the set of documents not submitted nor in another reference run for the topic. (Last year's random reference run only included 100 documents per topic.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation</head><p>Affordable evaluation of large result sets requires sampling and estimation. The deep sampling method we used last year (the "L07 Method" <ref type="bibr" coords="5,227.51,248.36,15.50,8.74" target="#b12">[15]</ref>) turned out to be very similar to the "statAP" method evaluated by Northeastern University in the TREC 2007 Million Query Track <ref type="bibr" coords="5,379.25,260.31,9.96,8.74" target="#b2">[4]</ref>. (The common ancestor was the (original) "infAP" method <ref type="bibr" coords="5,190.72,272.27,14.61,8.74" target="#b13">[16]</ref>, which also came from Northeastern.) Both methods associate a probability with each document judgment. Our approach used deeper sampling and more judgments per topic, but used many fewer topics than the Million Query Track. The methods also assigned sampling probabilities differently and targeted different measures.</p><p>Like last year, we chose the sampling probabilities to support evaluation of both early precision and deep recall measures, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Pooling</head><p>Like last year, we formed a pool of documents for each topic consisting of all of the documents submitted by any run. Each of the 64 runs submitted for the Ad Hoc task run included as many as 100,000 documents, sorted in a putative best-first order, for each of the topics. The 4 Boolean reference runs were also fully included in the pool, even the plaintiff Boolean run that sometimes matched more than 1 million documents for a topic. The random reference run was created after the other runs were pooled, then itself added to the pool (an additional 100,000 documents). The final pool sizes, before sampling, ranged from 618,756 (for topic 119) to 1,634,012 (for topic 141).</p><p>Note that, like traditional TREC pooling, our deep sampling method still implicitly assumes that documents not included in the pool are not relevant for purposes of the recall calculation. (The random reference run allows us to separately analyze the accuracy of this assumption.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Sampling</head><p>Our deep sampling method had a minor adjustment this year in how the sampling probabilities were chosen. A topic's B value was not a factor in the formula this year in order to provide better coverage across the range of K values that a participant might choose (as this year participants could choose a K value that differed from B). The floor on the probability was 5/100000 this year (instead of 5/25000) to compensate for the deeper submission limit this year.</p><p>The probability of judging each document d in the pool for a topic was:</p><formula xml:id="formula_0" coords="5,82.46,606.09,277.21,20.25">If (hiRank(d) &lt;= 5) { p(d) = 1.0; } Else { p(d) = min(1.0, ((5/100000)+(C/hiRank(d)))); }</formula><p>where hiRank(d) is the highest (i.e., best) rank at which any included run retrieved document d, and C is chosen so that the sum of all p(d) (for all submitted documents d) was the number of documents that could be judged (typically 500).</p><p>Note: for the 4 Boolean reference runs, which were unranked, the applicable rank was set to the number of documents retrieved (e.g., if 75,000 documents were retrieved for a topic, then all documents for that topic were considered to be of rank 75,000 for that run; of course, if some other participant run retrieved one of the documents at a higher rank (e.g., 15) the hiRank would be 15 instead of 75,000 for that document). The random reference run was treated as an ordinary ranked run.</p><p>The above formula caused the first judging bin of 500 documents to contain the top-5 documents from each run, and it caused measures at depth 100,000 to have the accuracy of approximately 5+C simple random sample points. Measures at depth K have the accuracy of approximately (at least) (5K/100000)+C simple random sample points. The C values this year ranged from 1.70 for topic 113 to 4.41 for topic 105. (The final C values for each topic are listed in the Appendix of these proceedings.) These C values are fairly low, indicating that substantial estimation errors are possible on individual topics. Mean scores (over 24 or 26 topics) should be somewhat more reliable than the estimates for individual topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Binning</head><p>Like last year, to allow for the possibility that some assessors could judge more than 500 documents, the sampling process was enhanced to have a first bin of approximately 500 documents and 5 additional bins of approximately 100 documents each, using the following approach. The C values were set so that the p(d) values would sum to 1,000, and an initial draw of approximately 1000 documents was done. Then the C values were set so that the p(d) values would sum to 900, and approximately 900 documents were drawn from the initial draw of 1000 (using the ratio of the probabilities); the approximately 100 documents that were not drawn became "bin 6". This process was repeated to create "bin 5", "bin 4", "bin 3" and "bin 2". The approximately 500 documents drawn in the last step became "bin 1".</p><p>When the judgments were received from the assessors (as described in the next section), the final p(d) values were based on how many bins the assessor had completed (e.g., if 3 bins had been completed, then the p(d) values from choosing C so that the p(d) sum to 700 were used). If there had been partial judging of deeper bins, the judged documents from these bins were also kept, but with their p(d) reset to 1.0. Note that if the 1st bin was not completed, the topic had to be discarded. For each completed topic, the final number of assessed documents and corresponding C values are listed in the Appendix of these proceedings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Relevance Judgments</head><p>As in 2007, we primarily sought out second-year and third-year law students who would be willing to volunteer as assessors in order to fulfill a law school requirement or expectation to perform some form of pro bono service to the larger community. A total of 34 Ad Hoc task topics were assigned to assessors, but judgments for 7 of those topics were not available in time for use in the evaluation, so the number of assessed topics was 27 (and one of these could not be used for evaluation because no relevant documents were found for it). Most of the assessors were law students from at least 17 returning and new institutions to the TREC Legal track, with the largest contingent, for the second year running, representing Loyola-LA law school. In addition, participants included several recent graduates of law schools, as well as experienced paralegals and litigation specialists. <ref type="foot" coords="6,161.00,522.80,3.97,6.12" target="#foot_0">1</ref>As in 2007, the assessors used a Web-based platform developed by NIST that was developed by Ian Soboroff and hosted at the University of Maryland to view scanned documents and to record their relevance judgments. Each assessor was given a set of approximately 500 documents to assess, which was labeled "Bin 1." Additional bins 2 through 6, each consisting of 100 documents, were available for optional additional assessment, depending on willingness and time. (It turned out that 5 Ad Hoc task assessors completed at least one of the optional bins, with one completing all five optional bins.) In total, 14,771 judgments were produced for the 27 topics.</p><p>As in 2007, we provided the assessors with a"How To Guide" that explained that the project was modeled on the ways in which lawyers make and respond to real requests for documents, including in electronic form. Assessors were told to assume that they had been requested by a senior partner, or hired by a law firm or another company, to review a set of documents for "relevance." No special, comprehensive knowledge of the matters discussed in each complaint was expected (e.g., no need to be an expert in federal election law, product liability, etc.). The heart of the exercise was to look for relevant and nonrelevant documents within a topic. Relevance was to be defined broadly. Special rules were to be applied for any document of over 300 pages. The same process was used for assessment for the interactive and relevance feedback tasks (which had different topics, as described below). See the TREC 2006 Legal Track overview for additional background (including measurement of inter-assessor agreement for that year's topics) <ref type="bibr" coords="7,397.72,146.89,9.96,8.74" target="#b3">[5]</ref>.</p><p>This year, for the first time, we asked assessors to identify some documents as "highly relevant." Each reviewed document was judged highly relevant, judged relevant, judged non-relevant, or left as "gray." (Our "gray" category includes all documents that were presented to the assessor, but for which a judgment could not be determined. Among the most common reasons for this were documents that were too long to review (more than 300 pages, according to our "How To Guide") or for which there was a technical problem with displaying the scanned document image.)</p><p>The survey returns reveal a considerable variance reported among assessors in their ability to distinguish between "relevant" and "highly relevant" documents: many reported "no" difficulty in so distinguishing; one said "the highly relevant documents came few and far between ... as such, they jumped off the page when I saw them"; others reported comments such as: "it was difficult to decipher the scientific language in order to determine how relevant the information was to my topic"; "there were one or two documents where I wasn't entirely sure ... [so] I erred on the side of inclusiveness (highly relevant)"; "this was the most challenging aspect of the project"; "It takes time to gather a feel for the documents ... If I were working with a team of attorneys, I would create document samples ... so that 'key' or 'highly relevant' documents could more easily [be] identified".</p><p>Another difference from 2007 is that the posted Word files with the background complaints and requests for the assessors did not include a copy of the Boolean negotiations this year, to reduce the chance that knowledge of the Boolean strings might somehow influence the assessing. This change was suggested by <ref type="bibr" coords="7,526.71,362.08,9.96,8.74" target="#b4">[6]</ref>.</p><p>A qrelsL08.normal file was created in the common trec eval qrels format. Its 4th column was a 2 (judged highly relevant), 1 (judged relevant), 0 (judged non-relevant), -1 (gray) or -2 (gray). (In the assessor system, -1 was "unsure" (the default setting for all documents) and -2 was "unjudged" (the intended label for gray documents).) A qrelsL08.probs file was also created, which was the same as qrelsL08.normal except that there was a 5th column which listed the p(d) for the document (i.e., the probability of that document being selected for assessment from the pool of all submitted documents). qrelsL08.probs can be used with the l07 eval utility to estimate a run's scores (such as F 1 , precision and recall) from the judged samples.</p><p>We asked assessors to record how much time they spent on their task. Past review rates averaged to 25 documents per hour in 2006 and 20 documents per hour in 2007. Based on partial survey results for 2008, assessors reported that it took a collective 631.15 hours to review 13,543 documents, or a rate of 21.5 documents per hour. As this result is in line with past years, it appears that the new highly relevant category did not substantially affect the assessment rate.</p><p>Overall, survey returns contained uniformly positive reviews for the experience of being a volunteer assessor in 2008, including such statements as "the project was a lot of fun"; "interesting and educational"; "enjoyed participating"; "[the coordinators were] very helpful, courteous, and gracious at all times, which made this sometimes tedious project seem much more engaging, exciting, and purposeful"; "a great way to get [pro bono] hours for evening students with limited availability"; "Once in a lifetime opportunity to help with a fascinating e-Discovery project."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Computing Evaluation Measures</head><p>The formulas for estimating the number of relevant, non-relevant and gray documents in the pool for each topic, and also for estimating precision and recall, were the same as last year <ref type="bibr" coords="7,411.20,635.51,14.61,8.74" target="#b12">[15]</ref>.</p><p>The new F 1 measure this year was estimated as follows:</p><p>Define estF 1 @k to be the estimated F 1 of S at depth k:</p><formula xml:id="formula_1" coords="8,216.74,83.07,323.26,22.31">estF 1 @k = 2 * estP rec@k * estRecall@k estP rec@k + estRecall@k<label>(1)</label></formula><p>Note: we define estF 1 @k as 0 if both estP rec@k and estRecall@k are 0. The K and B values are integers and hence can be substituted for k in the above formulas. R, however, can be fractional, hence we provide the following additional definition:</p><p>Define F 1 @R = F 1 @R ceil where R ceil is the ceiling of R (i.e., the smallest integer greater than or equal to R). For runs that did not contribute to the pools, the same estimation process can be used, albeit with the same limitations as in traditional TREC pooling (in particular, the assumption that unpooled documents are not relevant), and possibly larger sampling errors if the run would have influenced the hiRank() settings that were used to set the sampling probabilities of the documents. Note that the Interactive task, described below, did more detailed sampling of the entire collection for 3 test topics, providing another option for evaluating a novel technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Results</head><p>For the Ad Hoc task, 27 topics were assessed. However, one topic (#130) had no relevant judgments, leaving 26 useful topics. Furthermore, 2 topics had no "highly relevant" judgments (topics 136 and 142) leaving 24 useful topics for measures just counting "highly relevant" documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.1">Number of Relevant Documents</head><p>Applying the cited formulae, the estimated number of relevant documents in the pool, on average per topic, was 82,403, almost 5x more than last year <ref type="bibr" coords="8,265.68,357.35,16.97,8.74" target="#b13">(16,</ref><ref type="bibr" coords="8,282.65,357.35,16.97,8.74">904)</ref>. The number varied considerably by topic, from 110 (for topic 137) to 658,399 (for topic 131). Unfortunately, six of the topics had more than 100,000 relevant documents, i.e., more relevant documents than the participant runs were allowed to retrieve for a topic. (Last year, the most relevant documents for a topic was 77,467 (topic 71).) Perhaps one should remove these six topics for future training (though further study is needed to determine how best to deal with this issue). For this paper we have used all of the available topics in the scoring.</p><p>An explanation offered for the increase in the number of relevant documents is that the topic formulators had been instructed in previous years to try to keep the requests narrow because of concerns about the shallow pooling traditionally used at TREC. With the deeper sampling approach now in use, this concern went away, resulting in more broadly worded topics. However, if this is what happened, it was not a planned change, and the participants were not advised that this year's topics might tend to be broader (though the higher B values of the reference Boolean run this year (see next section) may have been a tip-off).</p><p>Over the 24 topics with highly relevant judgments, the estimated number of highly relevant documents in the pool, on average per topic, was 11,542. This number ranged from 1 (for topic 109) to 51,313 (for topic 145). Hence 100% recall of highly relevant documents was possible within the constraint of retrieving at most 100,000 documents for each topic. A concern though is the small numbers of highly relevant documents for some topics (e.g., if a topic has just 1 highly relevant document, then recall for that topic can only be 0% or 100%). Perhaps one should remove topics of small numbers of highly relevant documents for future training, but for this paper we have used all of the available topics in the scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.2">Boolean Negotiation Results</head><p>The average B value was 40,402 for the 26 evaluation topics. For the subset of 24 topics that have highly relevant judged documents, the average B value was almost the same (39,930). These values are about eight times larger than last year's average B value of 5,004 (i.e., the final negotiated Boolean query matched about eight times as many documents this year, on average).</p><p>This year, the average recall of the final negotiated Boolean query was 24%, close to last year's average recall of 22%. Once again, the recall varied widely by topic, ranging from 0.1% (for topic 150) to 88% (for  topic 148). For highly relevant documents, the average recall of the final negotiated Boolean query was 33%. It ranged from 0% (for topics 109, 147 and 150) to 100% (for topics 128 and 137).</p><p>The average F 1 of the final negotiated Boolean query was 16%, close to last year's average F 1 of 14%. The F 1 varied by topic, ranging from 0.2% (for topic 150) to 38% (for topic 128). For highly relevant documents, the average F 1 of the final negotiated Boolean query was 9%. It ranged from 0% (for topics 109, 147 and 150) to 38% (for topic 145).</p><p>Table <ref type="table" coords="9,114.82,568.83,4.98,8.74" target="#tab_1">1</ref> lists the mean scores of the negotiated Boolean queries 2 . It shows that the initial proposal by the defendant produced a relatively small set with relatively high precision, but had relatively low recall. In contrast, the rejoinder by the plaintiff produced a relatively large set with relatively high recall (57% of highly relevant documents on average), but more modest precision. The original consensus of the negotiators decreased the recall but did not gain much in precision on average. Final adjustments needed for some queries 2 While F 1 must always be between P and R for individual topics (since it is a harmonic mean, and all means lie between the extreme values), Table <ref type="table" coords="9,158.30,645.05,4.23,6.99" target="#tab_1">1</ref> shows that the arithmetic mean of F 1 is not the same as the F 1 of the arithmetic mean of P and the arithmetic mean of R. This is because the computation of F 1 is nonlinear. Consider the following two-topic example: Topic 1 (P =0.01, R=0.99, F 1 =0.02), Topic 2 (P =0.99, R=0.01, F 1 =0.02). F 1 is always close to the smaller value, and in this example the mean F 1 is 0.02, but, mean P is 0.5, as is mean R. So we have the mean F 1 on the per-topic values outside the bounds of the mean P and the mean R. to put B in the 100 to 100,000 range likewise decreased the recall without much gain in precision. On balance, the F 1 scores were highest with the original consensus of the negotiators, though the plaintiff query produced a similar mean F 1 score. Before trying to draw firm conclusions about the effect of different stages in the Boolean negotiation history, we will want to look at the negotiation results on a topic-by-topic basis.</p><p>Figure <ref type="figure" coords="10,118.35,392.46,4.98,8.74" target="#fig_0">1</ref> shows graphically that many of the highly relevant documents that are estimated to exist were not found by the consensus Boolean run (xrefL08C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.3">Participant Results</head><p>Table <ref type="table" coords="10,99.51,448.70,4.98,8.74" target="#tab_2">2</ref> shows the estimated F 1 at K for each submitted participant run (along with the 5 reference runs). Several participating teams submitted runs that achieved higher mean estimated F 1 values (across all topics) than the reference Boolean run.</p><p>Unlike last year, several participant runs also had a higher Recall@B than the reference Boolean run, including some runs that are known to have used the same techniques as last year. It is not yet clear whether this results from topics that are different in important ways (as the larger Boolean result sets might suggest) or from some other factor (e.g., subtle system improvements, or differences in the way the final Boolean query was constructed).</p><p>The highest scoring run in mean F 1 @K (wat7fuse) just set K=100,000 (the maximum allowed) for all topics, which probably isn't a generally applicable thresholding approach. The number of relevant documents (more than 80,000 on average per topic, including several topics of more than 100,000) was unexpectedly much larger than last year, so the submission cutoff of 100,000 may not have allowed enough flexibility to really test the thresholding ability of the systems.</p><p>The 100,000 cutoff issue would not seem to affect evaluation on highly relevant documents as much since the number of highly relevant documents was less than 52,000 for every topic. However, the top-scoring run in F 1 @K h (wat6fuse in Table <ref type="table" coords="10,202.04,628.03,4.43,8.74" target="#tab_3">3</ref>) just set K h to a constant 12,500 for all topics (close to the average number of highly documents per topic <ref type="bibr" coords="10,206.57,639.98,16.83,8.74" target="#b9">(11,</ref><ref type="bibr" coords="10,223.40,639.98,16.83,8.74">542)</ref>). It should be noted though that the participants did not have any training data for the highly relevant category, so this year's results may not represent what could be done with further study.</p><p>(A glossary for Tables <ref type="table" coords="10,186.24,675.85,4.98,8.74" target="#tab_2">2</ref> and<ref type="table" coords="10,213.92,675.85,4.98,8.74" target="#tab_3">3</ref> appears in Section 2.8.7.)  </p><formula xml:id="formula_2" coords="11,98.09,85.14,388.78,6.12">Run Fields Ret. Avg. K (P@K, R@K) F1@K F1@R S1J, P5 R@B,</formula><formula xml:id="formula_3" coords="12,274.20,85.14,221.42,6.79">(P@K h , R@K h ) F1@K h F1@R h S1J, P5 R@B,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.4">Estimated Gray Percentages</head><p>As previously mentioned, "gray" documents are those documents that were presented to the assessor but could not be judged non-relevant nor relevant (including highly relevant). The assessors were not expected to read through documents longer than 300 pages (though they were still asked to search such documents for relevant passages, in which case the document could be judged relevant). Sometimes there was a technical problem displaying the document in the assessor system. At depth B, the reference Boolean run had the highest percentage of gray documents (averaged over the 26 topics) at 2.5%. At depth K, only the RMITrp3 run had a higher gray percentage than the reference Boolean run (and it still rounded to 2.5%). At depth 5, runs UrsinusVa and UCEDLSIa had the highest gray percentages at 5.4%. At depth K h (averaged over 24 topics), the reference Boolean run had the highest gray percentage (2.7%). These gray percentages seem low enough to not adversely affect the comparability of mean precision, recall and F 1 estimates in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.5">Random Run Results</head><p>Like last year, we created a "random run" (this year named randomL08) in hopes of estimating the number of relevant documents that may have been outside of the pooled results of participating systems. By design, the randomL08 run had no overlap with any of the participant runs; for each topic, it was a random sample of just the unsubmitted documents.</p><p>The number of assessed random run documents varied by topic depending on the size of the pool, the number of bins judged by the assessor, and the randomness inherent in the sampling process. On average, 35 random run documents were assessed per topic, ranging from as low as 20 (for topics 113 and 118) to as high as 55 (for topic 124). More than three times as many random run documents were judged per topic this year as in 2007 (when 10 random run documents were assessed on average per topic).</p><p>One topic had several more random run documents judged relevant than the others. Topic 131 had 11 relevant judgments for its 33 random run documents. No other topic had more than 2 relevant judgments for the random run documents. We consider the judgments of this outlier topic to be suspect (in particular, none of its relevant documents that we (some of the track coordinators) have reviewed have looked relevant to us). For the rest of this subsection, we exclude topic 131.</p><p>Over the remaining 25 topics, there were 889 random run documents assessed. Of these, 8 were judged relevant (including 1 judged highly relevant), 864 were judged non-relevant, and 17 were left as gray. So, like last year, approximately 1% of the unsubmitted documents were judged relevant. For the new "highly relevant" category, only 0.1% of the unsubmitted documents were judged highly relevant.</p><p>Last year, when we reviewed the 3 random run documents that were judged relevant, they did not appear to be relevant to us, suggesting that the 1% number may actually be an assessor false positive rate rather than the percentage of unsubmitted documents that are relevant. This year, we have just reviewed the one random run document that was judged highly relevant (document nhx30c00 of topic 126). Again, it did not appear to be relevant to us (let alone highly relevant). Last year, an analysis of "assessor blunders" by a participant concluded that "the number of assessing disagreements due to blunders is still much less than the number of assessing disagreements due to scope of relevance" <ref type="bibr" coords="13,355.64,543.84,9.96,8.74" target="#b4">[6]</ref>. The Interactive task (described below) looked at the impact of adjudicating judgments this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.6">Marginal Precision Rates to Depth 100,000</head><p>Table <ref type="table" coords="13,100.21,599.62,4.98,8.74" target="#tab_4">4</ref> shows how precision falls with retrieval depth for the Ad Hoc task runs. The table includes the median and highest estimated marginal precision rates of the Ad Hoc task runs for depths 1-25,000, 25,001-50,000, 50,001-75,000 and 75,001-100,000. The median run was still maintaining more than 10% precision at the deepest stratum (depths 75,001-100,000), indicating that depth 100,000 was likely not deep enough to cover all of the relevant documents that a run could potentially find. This result seems consistent with the earlier finding that 6 topics had more than 100,000 estimated relevant documents. Perhaps it would be better for reusability to discard these 6 topics, but additional analysis will be needed before we can draw firm conclusions.  <ref type="table" coords="14,114.89,297.21,4.98,8.74" target="#tab_4">4</ref> also shows that the percentage of relevant documents judged highly relevant also tends to fall with retrieval depth. For the median runs, approximately 27% of the relevant documents were judged highly relevant in the highest stratum (depths 1-25,000) while just 13% of the relevant documents were judged highly relevant in the deepest stratum (depths 75,001-100,000). This is another indicator that the collection may have better coverage of highly relevant documents than relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.7">Table Glossary</head><p>The following glossary explains the codes used in Tables <ref type="table" coords="14,320.88,389.32,4.98,8.74" target="#tab_2">2</ref> and<ref type="table" coords="14,348.54,389.32,3.87,8.74" target="#tab_3">3</ref>.</p><p>"Fields": The topic fields used by the run: 'b' Boolean query (final negotiated), 'C' complaint, 'd' defendant Boolean (initial proposal), 'i' instructions and definitions, 'p' plaintiff Boolean (rejoinder query), 'o' other negotiation history (Defendant2, Plaintiff2, etc.), 'c' original consensus Boolean (or final Boolean if the Consensus1 field was not used), 'r' request text, 'v' B value, 'm' metadata fields were indexed, 'B' reference Boolean run was used, 'M' manual processing was involved, 'F' feedback run (old relevance assessments were used, applicable to RF task only).</p><p>"Ret.": The Average Number of Documents Retrieved per Topic. "Avg. K": The Average K value. "P@K" and "R@K": Estimated Precision and Recall at Depth K. "F 1 @K": Estimated F 1 at Depth K. "F 1 @R": Estimated F 1 at Depth R (where R is the estimated number of relevant documents). "S1J": Success of the First Judged Document. "P5": Estimated Precision at Depth 5. "R@B": Estimated Recall at Depth B. "R@ret": Estimated Recall of the full retrieval set. "K h ": K value when just counting Highly relevant documents as relevant. "R h ": Estimated number of Highly relevant documents. "Ret r ": The Average Number of "Residual" Documents Retrieved per Topic (RF task only). "K r ": The Average "Residual" K value (RF Task only).</p><p>Table <ref type="table" coords="14,115.10,628.42,4.98,8.74" target="#tab_2">2</ref> counts all relevant documents as relevant (averaged over 26 topics). Table <ref type="table" coords="14,460.39,628.42,4.98,8.74" target="#tab_3">3</ref> shows the mean scores when just counting highly relevant documents as relevant (averaged over 24 topics).</p><p>Parentheses are used for the 2 reference runs (xrefL08C and xrefL08P) which sometimes retrieved more than 100,000 documents for a topic (which was not allowed for participant runs).</p><p>For the 4 reference Boolean runs, only measures at the retrieval depth are shown since a specific ordering of Boolean results is not defined.</p><p>The Appendix of these proceedings lists more detailed information for each topic, including median and high F 1 scores for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Feedback Task</head><p>The objective in the Relevance Feedback task was to automatically discover previously unknown relevant documents by augmenting the evidence available from the topic description with evidence available from a limited number of existing relevance assessments. This task provides a simple and well controlled model for assessing the utility of a two-pass search process. The 2007 Relevance Feedback task relied on 2006 relevance assessment pools that had (generally) been drawn from near the top of submitted ranked retrieval runs. For the 2008 Relevance Feedback task, relevance assessments sampled from throughout the runs submitted in 2007 were available. We therefore selected some Ad Hoc topics from each year for use in the 2008 Relevance Feedback task. Teams could use positive and/or negative judgments in conjunction with the metadata for and/or full text from the judged documents to refine their models.</p><p>The same document collection was used in the Ad Hoc and Relevance Feedback tasks, so participation in both tasks did not require indexing a second collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Selection</head><p>40 topics were selected from among those used in 2006 and 2007. These topics were chosen by the track coordinators based on a variety of factors, as follows:</p><p>Topics were rejected if any of the following applied: the residual B r value was less than 100 (where B r is the number of documents matching the final negotiated Boolean query after documents judged in previous years were omitted); the residual B r value was greater than 100,000; or the topic had been used in last year's Relevance Feedback task.</p><p>The To get to 40, we chose to balance the number from each complaint of a given year, which led to choosing 3 topics from each complaint of 2006 (a total of 15 from 2006) and 6 or 7 topics from each complaint of 2007 (7 from A, 6 from the others, to make a total of 25 from 2007). From each complaint, the topics were chosen randomly.</p><p>At least the first bin for 12 of those topics was assessed by volunteers. (The assessed topics are listed in the Appendix of these proceedings.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Participation</head><p>Participating teams were allowed to submit up to 8 runs; additional runs could be scored locally. A total of 5 research teams submitted 29 runs for this year's Relevance Feedback task. The teams experimented with a variety of techniques including the following:</p><p>â€¢ Open Text Corporation: baseline runs, relevance feedback, pure feedback run, ranked-based fusion, sampling-based thresholds.</p><p>â€¢ Sabir Research, Inc: basic Rocchio feedback, all judged docs, add 40 terms, SMART ltu Lnu vector run, all Boolean query negotiation terms.</p><p>â€¢ University of Iowa (Srinivasan): relstrings, WEKA API, WEKA SMO, Platt's sequential minimal optimizing algorithm, support vector machine, polynomial kernel, logistic regression, relevance probability estimates, classifier models.</p><p>â€¢ University of Missouri-Kansas City: VSM, BM25, LM, Expand15, CombMNZ.</p><p>â€¢ Ursinus College: BM25 baseline, Power Norm baseline, 5 terms over weight 5 added, 10 terms over weight 5 added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>29 Relevance Feedback runs were submitted by 5 research teams. Participating teams were allowed to submit up to 101,000 documents per topic. "Residual evaluation" was used for the Relevance Feedback task. Hence, before pooling, any documents that were already judged (of which there were at most 1000 per topic) were removed from the Relevance Feedback runs. Also, any documents past 100,000 residual documents retrieved were discarded before pooling. The pools were then enriched before judgment with four additional runs:</p><p>â€¢ refRF08B (the final negotiated Boolean query results)</p><p>â€¢ randomRF08 (100,000 randomly selected residual documents from the unpooled documents for each topic)</p><p>â€¢ oldrel08 (10 (or as many as available) randomly chosen relevant documents from past judging of the topic)</p><p>â€¢ oldnon08 (10 (or as many as available) randomly chosen non-relevant documents from past judging of the topic).</p><p>The p(d) formula for the Relevance Feedback task was the same as for the Ad Hoc task except that the p(d) was set to 1.0 for all of the documents in oldrel08 and oldnon08 (small assessor-consistency study). Also, the first bin to judge was typically just 400 documents instead of 500 (because fewer documents needed to be judged to maintain the same accuracy (C value) as in the Ad Hoc task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relevance Assessment</head><p>Relevance assessments for the Relevance Feedback task were performed using exactly the same process as for the Ad Hoc task. A total of 12 topics were completed (3 of those 12 assessors completed at least one additional bin, 2 of those assessors completed all 5 additional bins).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>Of the 12 topics for which assessments are available, all 12 had some judgments of "relevant," but 3 topics had no "highly relevant" judgments (topics 36, 47 and 83). Thus there are 9 useful topics for measures that focus on "highly relevant" documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Number of Relevant Documents</head><p>The estimated number of (residual) relevant documents in the pool, on average per topic, was 23,536. The number varied considerably by topic, from 107 (for topic 14) to 101,197 (for topic 73).</p><p>Table <ref type="table" coords="16,115.52,597.01,4.98,8.74" target="#tab_5">5</ref> compares the estimated number of relevant documents for the 7 topics assessed in both the 2007 Ad Hoc task and this year's 2008 Relevance Feedback task. The estimates vary considerably; it's not immediately clear how much of the differences are from assessor inconsistency, or sampling error, or differences in the participating runs, or differences in the pooling depth (this year's runs were pooled 4x deeper, 100,000 vs. 25,000).</p><p>Over the 9 topics with highly relevant judgments, the estimated number of highly relevant documents in the pool, on average per topic, was 2,640. This number ranged from 22 (for topic 85) to 12,246 (for topic 73). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Baseline vs. Feedback Results</head><p>Table <ref type="table" coords="17,98.77,462.36,4.98,8.74" target="#tab_6">6</ref> compares the scores of the reference Boolean run, the median of 10 participant baseline runs, and the median of 19 participant feedback runs. Whether counting all relevant or just highly relevant documents, the mean F 1 score was higher for the reference Boolean run than for either the median baseline or feedback run. Furthermore, the median feedback run actually scored lower in mean F 1 than the median baseline run.</p><p>However, the number of topics is small. Table <ref type="table" coords="17,115.87,522.14,4.98,8.74" target="#tab_8">8</ref> shows the results for the 29 Relevance Feedback runs (and 2 reference runs). The highest mean F 1 @K r score came from the reference Boolean run. Several runs had a higher mean F 1 @R r than the Boolean run's mean F 1 , suggesting that thresholding the retrieval set remains a challenge. In last year's R@B r measure, few runs scored a higher mean R@B r than the Boolean run.</p><p>Table <ref type="table" coords="17,114.59,569.96,4.98,8.74" target="#tab_9">9</ref> shows the results just counting highly relevant documents. A few runs did have a higher mean F 1 @K hr than the reference Boolean run, but (as per the medians) the majority did not. We should note that the groups did not have any training data for the highly relevant category this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Assessor Consistency Results</head><p>Assessor agreement on the (up to) 10 documents judged relevant and 10 documents judgment non-relevant can shed some light on the cause of the larged observed differences in our estimates. Table <ref type="table" coords="17,479.90,650.11,4.98,8.74" target="#tab_7">7</ref> shows these results. The "Previously Judged Relevant" column shows how this year's assessor judged the (up to) 10 documents that were judged relevant when the topic was used in the 2006 or 2007 Ad Hoc task (as per the oldrel08 run). The "Prev. Judged Non-relevant" column shows the same information for 10 documents previously judged non-relevant (as per the oldnon08 run). The labels are "tot" for total judged (which was 10 except when less than 10 documents were judged relevant previously), "hrel" for highly relevant, "orel" for other relevant, "non" for non-relevant, and "gr" for gray. We see that just 58% of previously judged relevant documents were judged relevant again this year; almost half of these were judged highly relevant (note that in previous years, the "highly relevant" category was not available). 18% of previously judged non-relevant documents were judged relevant this year; note that these non-relevant documents may have been rated highly by past search engines (which boosted their chance of being in the previous judging pool in the first place). For the previously judged relevant documents, we do not see perfect agreement for any topic. For the previously judged non-relevant documents, there are 3 topics for which both assessors agreed that all 10 documents were non-relevant. There were 2 topics (73 and 85) for which this year's assessor found that more of the previously judged non-relevant documents were relevant than of the previously judged relevant documents.</p><p>Past assessor agreement studies typically have found a lot of assessor disagreements, but generally retrieval systems are rated similarly regardless of which assessor's judgments are used <ref type="bibr" coords="18,424.32,492.59,9.96,8.74" target="#b6">[8]</ref>. We have not to date attempted to quantify whether our levels of disagreement are more or less than the norm. Note that none of these double-assessed documents were used in this year's evaluation (as residual evaluation excludes previously judged documents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interactive Task</head><p>In 2008, the Legal Track introduced a completely redesigned Interactive Task. The purpose of the redesign was to arrive at a task that modeled more completely and accurately the objectives and conditions of ediscovery in the real world. It was hoped that featuring such a task would further advance the Legal Track toward its goal of fostering greater communication and collaboration among the legal, scientific, and ediscovery communities. In the following, we (1) review key features of the design of the task, (2) provide a description of the procedures whereby task submissions were evaluated, (3) review specific parameters that defined this year's exercise, (4) summarize the results obtained, and (5) provide some further analysis of the results of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Fields</head><p>Retr Kr (P@Kr, R@Kr) F1@Kr F1@Rr S1J, P5 R@Br, R@retr </p><formula xml:id="formula_4" coords="19,84.99,94.98,38.38,6.14">refRF08B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Design</head><p>The goal of the Interactive Task is to model as accurately as possible the real-world conditions in which companies and law firms, and the e-discovery firms they engage, must meet their document-retrieval objectives and obligations. Pursuant to that goal, four key features were introduced into the 2008 design of the task: (1) the designation of a single individual (an attorney) to act as the authority for defining the intent and scope of a topic; (2) a provision that allowed participants to engage with that authority for purposes of clarifying relevance to a topic; (3) the specification of the task objective to be, for each topic, a binary assessment (relevant, not relevant) of all documents in the target collection; and (4) the provision for an appeal and adjudication process as a corrective on possible errors in sample assessments. In the following, we elaborate on each of these features. <ref type="foot" coords="20,240.15,187.61,3.97,6.12" target="#foot_1">3</ref>Topic Authority. When the lead attorney on a matter oversees a document production, he or she will have formed, or be in the process of forming, an opinion as to what is responsive to the requests for production and what is not. In forming that conception of responsiveness, the attorney will take into account both considerations of substantive relevance and considerations related to case strategy (e.g., whether to take a broad view of responsiveness, thereby minimizing the risk of being challenged for underproduction or to take a narrow view, thereby restricting what is produced to just that which has to be produced). When that attorney employs the products or services of an e-discovery firm, or, for that matter, the services of a traditional manual-review team, he or she does so with the goal of efficiently applying that conception of responsiveness across the full document population implicated by the matter. The review team is not asked to consider, weigh, and resolve differences between all possible conceptions of relevance; the review team is asked to replicate, across the document population, one conception of responsiveness, that of the senior attorney who has hired the firm and who bears ultimate responsibility for the validity of the production. The goal, therefore, of the review team or e-discovery firm engaged to assist in a document production effort is to replicate the responsiveness assessments the senior litigator in the matter would make, if he or she had the time and leisure needed to review for responsiveness every document in the population.</p><p>In order to model this aspect of real-world e-discovery, we introduced a new role into the Interactive task, that of the "Topic Authority." The role that the Topic Authority plays in the Interactive task is modeled on that played by the lead attorney in a lawsuit, the attorney who must form a conception of what is and is not responsive to a request for production and who must then communicate that conception to the team or vendor who will be asked to replicate that conception of responsiveness across the target document population. In the Interactive task, it is the role of the Topic Authority to define what is and is not relevant to a topic and it is the objective of the teams participating in the task to retrieve documents that match the Topic Authority's definition of relevance.</p><p>In keeping with this role, the Topic Authority is essential to the execution of three key elements of the task. The first is topic clarification. While teams are going about their efforts to retrieve documents relevant to a topic, it is the role of the Topic Authority to give the teams guidance when they seek clarification as to the intent and scope of the topic. The second is review oversight. In order to be able to obtain valid measures of effectiveness, it is essential that the samples reviewed for purposes of evaluation be assessed in accordance with the Topic Authority's conception of relevance; it is the role of the Topic Authority to provide assessors with guidance as to what is and is not relevant. The third is final adjudication. As an additional measure to ensure the quality of the assessments in the evaluation samples, we provide teams with the opportunity to appeal any sample assessments they believe were made in error; it is the role of the Topic Authority to render final judgment on all appealed assessments.</p><p>Topic clarification. If it is the Topic Authority who defines the target (i.e., who determines what should and should not be considered relevant to a topic), it is essential that provision be made for teams to be able to interact with the Topic Authority in order to gain better understanding of the Topic Authority's conception of relevance. In the 2008 Interactive task, this provision took the following form. Each team could ask for up to 10 hours of a Topic Authority's time for purposes of clarifying a topic. A team could call upon a Topic Authority at any point in the exercise, from the kickoff of the task to the deadline for the submission of results. How a team made use of the Topic Authority's time was largely unrestricted: a team could ask the Topic Authority to pass judgment on example documents; a team could submit questions to the Topic Authority by email; a team could arrange for conference calls to discuss aspects of the topic. The one constraint (apart from the 10-hour maximum) we did place on communication between the teams and their designated Topic Authorities was introduced in order to minimize the sharing of information developed by one team with another; while we instructed the Topic Authorities to be free in sharing the information they had about their topics, we also asked that they avoid volunteering to one team specific information that was developed only in the course of interaction with another team.</p><p>Submission of results. When an attorney vouches for the validity of a document production, he or she is vouching for the accuracy of a binary classification of the document population implicated by the litigation, a classification into the subset of the population that is responsive to the requests for production and the subset that is not. When an e-discovery firm supports an attorney in this effort, it must make a similar relevance determination. The 2008 Interactive task, modeling this requirement, specified that each team's final deliverable be a binary classification of the full population for relevance to each target topic. Teams were of course free to use relevance ranking as a means to arrive at their result sets, but the final deliverable was a single binary classification (relevant/not relevant) of the full population of documents.</p><p>Appeal and adjudication. Assessors can make errors; as an additional quality-control check on the sample assessments, we introduced an appeal and adjudication phase to the task. Once sample review was complete, participating teams were given access to sample results, allowing them to review any mismatches between their assessments and those of the assessors; teams were not, at this stage, given access to the results submitted by any other team. Teams were permitted to appeal any assessments they believed were directly and specifically contradicted by information given them by the Topic Authority in the course of their communications regarding the topic. Teams were not permitted to appeal assessments that represented differences in interpretation. The "court of appeal" and the final arbiter was the Topic Authority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>The metrics used to gauge the effectiveness of each team's efforts were recall, precision, and F 1 . In this section, we briefly describe the sampling, assessment, and measurement procedures whereby estimates of the target metrics were obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sampling</head><p>A separate evaluation sample was drawn for each topic targeted in the Interactive Task. The sampling design was fairly straightforward, its salient features being results-based stratification, fairly large sample sizes, and moderately disproportionate representation of strata. Specifics of the sampling design are as follows.</p><p>Stratification. For purposes of drawing each sample, the document population was partitioned into strata, with strata being defined by the cross-classification of results submitted by each of the teams whose performance was to be measured via the sample. In the case of a topic for which three teams submitted results for evaluation, for example, the collection would be partitioned into eight strata, one for each of the possible combinations of binary assessments (R/R/R, R/R/N, R/N/R, R/N/N, and so on). The full evaluation sample was created by drawing samples of documents from each of the resulting strata.</p><p>Sample Size. The samples drawn for each topic were fairly large (ranging from 2,500 documents to 6,500 documents). The provision for larger sample sizes made it possible to draw a sufficient number of documents from each possible result combination (stratum) and to obtain fairly precise estimates of the target metrics (even in the case of low-yielding topics). The drawing of larger samples was in part enabled by the fact that the Interactive Task, being in other regards a fairly time-intensive exercise, targets a relatively small number of topics, thereby allowing a greater amount of assessor resources to be concentrated on each topic.</p><p>Allocation. In constructing the sample, strata were largely represented in proportion to their fullpopulation proportions. The exception to the rule is that very large strata (such as the "All-N" stratum, the stratum containing documents no team assessed as relevant), though represented in larger numbers than the smaller strata, were not represented in the numbers strict proportionality would have dictated. This departure from strict proportionality enabled the inclusion in the sample of a greater number of each team's positive assessments, and, in particular, cases in which one team's assessments were at variance with those of all other teams.</p><p>The actual results of constructing evaluation samples in accordance with this sampling design are detailed below (section 4.4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Assessment</head><p>Once the samples were drawn, the documents they contained were reviewed for relevance to the target topics. Document assessment followed a two-step procedure, whereby volunteer assessors made a first-pass review of the sample and then the participating teams, after reviewing the results of the assessors' first pass, had the opportunity to appeal initial assessments to the Topic Authority for final adjudication.</p><p>First-Pass Assessment. The task of the volunteer assessors was twofold. In the first instance, each assessor had to make a threshold decision as to the assessability of each document that had been assigned to him or her. <ref type="foot" coords="22,119.91,237.42,3.97,6.12" target="#foot_2">4</ref> Primary reasons for a document's having been deemed not assessable were (1) length (in excess of 300 pages), ( <ref type="formula" coords="22,138.94,250.95,4.24,8.74">2</ref>) substantial non-English content, and (3) failure to load properly into the review platform; the vast majority (98.8%) of documents in the samples met the threshold criterion of assessability.</p><p>Documents that met the assessability threshold were then reviewed for relevance to the target topic. For purposes of making relevance assessments, assessors were provided with topic-specific guidelines that documented the criteria the Topic Authority wanted to be applied in assessing the relevance of documents to the target topic; these guidelines were essentially compilations of all the guidance that the Topic Authority had given teams in the course of the exercise to that point. When assessors encountered a document the status of which was insufficiently determined by these topic-specific guidelines, they had the opportunity to seek further guidance (via email) from the appropriate Topic Authority. The final topic-specific guidelines (including additional guidance given by the Topic Authorities to the assessors) can be found on the Legal Track Home Page <ref type="bibr" coords="22,152.70,370.50,9.96,8.74" target="#b1">[3]</ref>.</p><p>Appeal &amp; Adjudication. Recognizing that, although the topic-specific guidelines and the opportunity for further clarification could be expected to go some way down the path to ensuring that sample assessments were aligned with the Topic Authority's conception of relevance, those provisions in themselves could not be expected to eliminate all scope for error, the coordinators included, as a further corrective on sample assessments, a provision for appeal and adjudication of the first-pass assessments.</p><p>The mechanism for submitting an appeal of a first-pass assessment was fairly straightforward. Once the first-pass review of the samples had been completed, teams were provided both with their initial (preadjudication) recall, precision, and F 1 scores and with lists recording all assessable documents in the sample, the assessor's assessment of each of those documents, and their own submitted assessment of the same documents. Teams did not have access, at this stage, to the assessments submitted by any other team. Teams were also provided with each document's probability of selection into the sample (information that a team could use to prioritize its appeal efforts). As a further aid to their review of the assessments, teams were given the topic-specific guidance that had been provided to the assessors (supplemented with the additional guidance the Topic Authority had provided the assessors in the course of the sample review).</p><p>Upon comparing the first-pass assessments with their own submitted assessments, a team could decide to appeal initial assessments it believed the manual assessor had made in error. The circumstances in which a team could lodge an appeal were not unconstrained; a team could lodge an appeal only in one of the following circumstances.</p><p>â€¢ An appeal could be made in cases in which a team believed that a sample assessment was directly contradicted by specific guidance already provided by the Topic Authority.</p><p>â€¢ An appeal could be made in cases in which a team believed that it was immediately apparent that one sample assessment was inconsistent with another (e.g., a set of duplicate documents that had been inconsistently assessed).</p><p>A team could not appeal a case in which previously obtained guidance was insufficiently specific to decide between competing assessments (the task made it the responsibility of the team to obtain, through interaction with the Topic Authority, guidance of the specificity required to decide such cases).</p><p>The number of documents a team could appeal was unrestricted (although teams were encouraged, in the interest of efficiency, to be judicious in selecting the documents they wished to appeal).</p><p>Teams were asked to consolidate all their appeals into a single document, including the following information (where appropriate) for each document the assessment of which was being appealed:</p><p>â€¢ document ID;</p><p>â€¢ current assessment (that of the first-pass assessor);</p><p>â€¢ proposed revised assessment;</p><p>â€¢ specific reason for the revision;</p><p>â€¢ excerpt(s) from the document supporting the case for revision; and</p><p>â€¢ additional notes or comments.</p><p>The appeals documents were not anonymized before being submitted to the Topic Authority for final adjudication (meaning the Topic Authority did know which appeals had been made by which teams). Teams were encouraged to make their appeals documentation as complete as possible, but were permitted, if the Topic Authority agreed, to arrange time to discuss their appeals by telephone, if they believed that that would make the process more efficient.</p><p>The final decision on all appeals rested with the Topic Authority. There was no second round of appeals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Metrics</head><p>Once we have final assessments for all documents (that is, once we have completed a first-pass review of all sampled documents, allowed teams to appeal any first-pass assessments they wish to dispute, and obtained the Topic Authority's final judgment on all appealed documents), we are in a position to obtain estimates of the metrics by which we will gauge each team's effectiveness in performing the task (recall, precision, and F 1 ). The procedures for obtaining those estimates are largely a matter of (i) obtaining stratified estimates of the inputs to the target metrics, then (ii) combining those inputs in the appropriate manner to obtain estimates of the metrics themselves. The specifics of these estimation procedures are reviewed in Appendix A to this document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Task Specifics</head><p>With the posting of the final guidelines for the 2008 Interactive Task (on June 22, 2008), teams were able to begin their work. In this section, we describe some of the specific elements that defined this year's running of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Document Collection</head><p>The document collection used for the Interactive task was be the same as that used for the Ad Hoc and Relevance Feedback tasks, the IIT Complex Document Information Processing (CDIP) Test Collection, version 1.0. For more on the document collection, see section 2.1 above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Topics</head><p>Three topics were selected as the retrieval targets for the Interactive task (the resource-intensive nature of the task, both for teams and for Topic Authorities, constrained the number of topics we could accommodate).</p><p>A participating team was free to take on one, two, or all three topics, as it chose.</p><p>All three topics were associated with the same mock complaint (a modified version of a complaint used in the 2006 Legal Track). Two of the topics (102, 103) were entirely new for 2008; one (104) was used in a prior years (in the 2006 Ad Hoc task and in the 2007 Relevance Feedback task). Even the previously-used topic, however, was essentially "new," due to the fact that modifications to the complaint and the addition of the Topic Authority's guidance effectively reoriented the topic.</p><p>The specific topics are as follows.</p><p>â€¢ Topic 102. Documents referring to marketing or advertising restrictions proposed for inclusion in, or actually included in, the Master Settlement Agreement ("MSA"), including, but not limited to, restrictions on advertising on billboards, stadiums, arenas, shopping malls, buses, taxis, or any other outdoor advertising.</p><p>â€¢ Topic 103. All documents which describe, refer to, report on, or mention any "in-store," "on-counter," "point of sale," or other retail marketing campaigns for cigarettes.</p><p>â€¢ Topic 104. All documents discussing or referencing payments to foreign government officials, including but not limited to expressly mentioning "bribery" and/or "payoffs."</p><p>Topics were selected with an eye to representing the sorts of challenges typically encountered in real-world document discovery and were, as is typical of real-world document requests, underspecified as to scope and intent. Among the questions left open by the statement of Topic 103, for example, include what set of marketing practices constitute a "campaign" and where to draw the boundaries around specifically "retail" marketing campaigns. Topic 104 raises the question of whether just illegitimate payments are in-scope or legitimate payments are to be considered within the scope of the request as well. Each of the topic statements, upon further inspection, will be found to raise a number of such questions, questions the answers to which will depend on the outlook the producing party (represented, in our task, by the Topic Authority) has on the issues being litigated, the specific request for production, and his or her discovery obligations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Topic Authorities</head><p>To guide the teams in addressing these sorts of questions, a single Topic Authority was assigned to each of the topics. The Topic Authorities for the 2008 Interactive Task were as follows.</p><p>â€¢ Topic 102. Joe Looby (of FTI Consulting).</p><p>â€¢ Topic 103. Maura Grossman (of Wachtell, Lipton, Rosen &amp; Katz).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Topic 104. Conor Crowley (of Daley Crowley LLP).</head><p>Archives of the topic-clarification guidance the Topic Authorities provided both to the teams and to the relevance assessors over the course of the exercise have been maintained and will be made available by NIST for future use by researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Participating Teams</head><p>Four teams submitted results for the Interactive task. The teams and the topics for which they submitted results are as follows.</p><p>â€¢ University at Buffalo ("UB"). Submitted results for Topic 103.</p><p>â€¢ Clearwell Systems ("CS"). Submitted results for Topics 102, 103, and 104.</p><p>â€¢ H5 ("H5"). Submitted results for Topic 103.</p><p>â€¢ University of Pittsburgh ("UP"). Submitted results for Topics 102 and 103.</p><p>As can be seen from the list, participants in the task included both teams from academic institutions and teams from industry. While the task has been designed to be fair and accessible to all participants, it is also fair to recognize, in light of the fact that there is a mix of academic and industry participants (a mix that is very welcome), that different teams will bring different resources to the task.</p><p>In addition to the results submitted by these teams, who participated in all phases of the task, results were also submitted, for all three topics, by participants in the Ad Hoc task, who, however, did not interact with the Topic Authorities in preparing their submissions. For evaluation purposes, we created an additional benchmark result set, the Ad Hoc Pool ("AH"), formed by pooling each of the 64 Ad Hoc submissions, to a maximum depth of 100,000, along with the results of each of the 4 associated Boolean queries (for more on the Boolean reference runs, see Section 2.4 above). The random run was not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Task guidelines and topics were released on June 22, 2008. Teams submitted their results on or before September 12, 2008. The evaluation protocol outlined above was carried out in the weeks following. In this section, we review the results of the Interactive Task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Team-TA Interaction</head><p>As noted above, teams were permitted to call on up to 10 hours of a Topic Authority's time for purposes of clarifying the scope and intent of a topic. Figure <ref type="figure" coords="25,282.74,351.03,4.98,8.74" target="#fig_2">2</ref> summarizes the participants' use of the Topic Authorities' time for each topic. In the diagram, each bar represents the total time allowed for team-TA interaction (600 minutes for each team for each topic); the grey portion of the bar represents the amount of the permitted time that was actually used by a team (with the number of minutes used indicated to the left of each bar). The contributors to the Ad Hoc Pool did not participate in the topic-clarification phase of the task, and so recorded zero minutes of Topic Authority time. A few initial observations can be made regarding these data. First, there is considerable range in the amount of time teams spent engaging with their Topic Authorities. For Topic 103, for example, one team (Pittsburgh) used just 5 of their permitted 600 Topic Authority minutes, while another (H5) used 485 minutes. Second, apart from the H5 team, participants generally used only a small portion of their permitted Topic Authority time; on average, teams (apart from H5) used about 60 minutes of a Topic Authority's time for purposes of clarifying the definition of a target topic (about 10% of the time allowed for this purpose). In analyzing the results, we are therefore interested in seeing whether there is a correlation between up-front time spent with the Topic Authority and retrieval effectiveness (as measured by recall, precision, and F 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Submissions</head><p>Submissions for each topic consisted of the submissions of teams fully participating in the task as well as the pooled submissions of the Ad Hoc participants. Table <ref type="table" coords="26,334.12,179.22,9.96,8.74" target="#tab_11">10</ref> summarizes the total number of documents submitted by each team for each topic.  As can be seen from the table, there was, even within the same topic, considerable variation in the numbers of documents participants identified as relevant. For Topic 103, for example, setting aside the set of pooled Ad Hoc results, submissions ranged from a low of 25,816 documents (Pittsburgh) to a high of 608,807 documents (H5). The goal of evaluation would be to see how these submissions lined up with the Topic Authorities' conceptions of relevance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Sampling &amp; Assessment</head><p>The evaluation protocol described above (Section 4.2) was followed, with samples being drawn, assessed, and adjudicated. The results of the sample assessment process are summarized in ; in the tables, the column labels are defined as follows: N = total documents in the stratum; n = total documents sampled from the stratum; a = total sampled documents observed to be assessable; r 1 = total sampled documents observed to be assessable and relevant (pre-adjudication); r 2 = total sampled documents observed to be assessable and relevant (post-adjudication).</p><p>A few further notes on the sampling and assessment data follow. Sample sizes. As can be seen from the tables, larger samples were drawn for some topics than were drawn for others. Generally speaking, the greater the number of participants who submitted results for a topic, the larger the sample that was drawn; this was done to ensure that, even for a topic with several participants, we would be able to sample a meaningful number of documents from each stratum. Topic 103, for example (see Table <ref type="table" coords="26,177.36,633.81,8.30,8.74" target="#tab_2">12</ref>), saw participation by five teams (including the Ad Hoc Pool), making for 32 possible strata (as defined by cross-classifying team submissions), and so called for a larger sample (6,500 documents); Topic 104, on the other hand (see Table <ref type="table" coords="26,299.22,657.72,8.30,8.74" target="#tab_3">13</ref>), saw participation by two teams (again including the Ad Hoc Pool), making for four possible strata, and so was covered with a smaller sample (2,500 documents). should also be borne in mind as we look ahead to the 2009 running of the task. The data we have been considering suggests that making effective use of the Topic Authority's time (or, translating to the real-world task being modeled, the time of the senior litigator responsible for a document production) is useful for retrieving the set of documents the Topic Authority wants retrieved. The importance of this interaction is also underlined by the 2008 Topic Authorities themselves, who, in their reflections on the 2008 Interactive Task, note the following <ref type="bibr" coords="32,270.23,404.90,9.96,8.74" target="#b5">[7]</ref>.</p><p>The successful outcome of an information retrieval task is highly dependent on the amount of time -and the quality use of the time -spent with the person or persons tasked with the ultimate responsibility for defining relevance. It is not possible to replicate subjective judgment calls without spending time with the subjects who are ultimately responsible for making those determinations.</p><p>A good lesson for teams looking ahead to 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Assessment &amp; Adjudication</head><p>As noted above (Section 4.4.3), of the 13,339 sampled documents (aggregating across all three topics) that were found to be assessable, and so received a first-pass Relevant or Non-Relevant assessment, 966 were appealed; of the 966 first-pass assessments that were appealed, nearly 80% were in fact overturned by the Topic Authority. Changes in assessments on that order obviously can have a substantial impact on results. In this section, we take a closer look at some of the effects of the appeal and adjudication process.</p><p>Distribution across topics. We note, first, that the appeals were not evenly distributed across the three topics. There were many more appeals of Topic 103 assessments than there were of either Topic 102 or Topic 104 assessments. Now, Topic 103 had the greatest number of participants and had the largest evaluation sample; even allowing for such considerations, however, it remains true that the appeal/adjudication mechanism was utilized more heavily for Topic 103 than it was for the other two topics, and it is on the results for Topic 103 that we would expect the mechanism to have the strongest effect.</p><p>Impact on F 1 scores. As expected, the appeal and adjudication mechanism had little effect on the F 1 scores of participants in Topics 102 or 104. For these topics, the greatest Pre-to Post-Adjudication difference was a 0.4% drop in the F 1 score reported for the Ad Hoc Pool; the appeals for these topics were simply too few to have a large impact. The appeal and adjudication mechanism did have an effect on the F 1 scores of participants in Topic 103. All active participants (and the Ad Hoc Pool) saw an improvement in their F 1 scores as a result of the appeals process. For the active participants, the Pre-to Post-Adjudication differences ranged, in absolute terms, from 0.013 (Pittsburgh) to 0.277 (H5). Expressed as a proportion of their Pre-Adjudication scores, however, the differences were more equal; all active participants saw their F 1 scores improve by 28% to 38% relative to their Pre-Adjudication values. The Ad Hoc Pool, on the other hand, realized a smaller gain, a 6% gain on its Pre-Adjudication value.</p><p>Impact on ranking of results. For no topic did the results of the appeal and adjudication process affect the ranking of submissions on F 1 . Even for Topic 103, where, as we have just seen, the appeal and adjudication process had an impact on absolute F 1 values, the Pre-to Post-Adjudication changes in scores did not affect the relative ordering of participants on the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Generalizability of Results -the Impact of OCR Quality</head><p>The Legal Track's Interactive Task, like all TREC tasks, is intended to serve a number of purposes. It is a research exercise, intended to advance our understanding both of the increasingly challenging text retrieval needs of the legal community and of the retrieval methods and capabilities best suited to meet those needs. It is a forum for the development of task and evaluation designs that will answer the questions the research, legal, and vendor communities have about the application of search technologies to e-discovery problems. It is a resource to which the legal community can turn to gain a better understanding of the sorts of approaches that are most likely to prove effective at performing the e-discovery tasks they most urgently need assistance with.</p><p>With an eye to the latter purpose (serving as a resource for attorneys looking to evaluate approaches), we have tried to make the Interactive Task as realistic as possible. While some compromise on realism is always going to be required if a task is to enable valid approach-to-approach comparisons, we have endeavored, in this year's Interactive Task, to model as closely as possible the conditions and objectives of real-world responsive review. There is, however, one component of the task that is decidedly anomalous to what one could expect in a typical responsive review of 2009: the document collection, or, more specifically, the quality of the OCRing of the document collection.</p><p>The limited accuracy of the OCR text in the IIT CDIP test collection has been a subject of conversation since we started using the collection in the first year of the Legal Track. The document text that, together with the document metadata, teams rely on in conducting their retrieval efforts, is the result of an OCR process applied to images of the original documents; the output of that process is often faulty (i.e., a garbled rendering of the content of the original document), either because the source image is not susceptible to accurate OCRing (e.g., is characterized by poor resolution, small type, or predominantly handwritten content) or because the OCR process that was used was less accurate than might now be achievable. This aspect of the collection is atypical of what would be expected in most current collections that are likely to be the focus of litigation, in that the latter would be sourced almost entirely from born-digital files and so would have minimal OCR issues.</p><p>In order to enable the results of the task to be more generalizable to conditions typically encountered in the real world, we would like to be able to take into account the possible impact of the state of the OCR in the test collection. While it would be beyond the scope of this overview to address all the questions that would have to be addressed if we were to attempt to arrive at a precise quantification of the OCR effect, we provide the following analysis as a simple but informative gauge of the possible impact on performance of the state of the OCR.</p><p>OCR Score. To begin with, we need some method of objectively quantifying the quality of the OCR. While a number of measures are discussed in the literature, the non-stopword accuracy measure advocated by researchers at the Information Science Research Institute (ISRI) at University of Nevada, Las Vegas <ref type="bibr" coords="34,524.50,242.53,15.50,8.74" target="#b10">[12]</ref> is a reasonably straightforward measure that will suit our purposes. Under this approach, a document's OCR score is, generally stated, the proportion of correctly rendered lexical words out of the sum of correctly and incorrectly rendered lexical words. The higher the OCR score (the closer to 1.0) the better the quality of the OCRing; the lower the OCR score (the closer to 0.0) the worse the quality of the OCRing.</p><p>More specifically, for purposes of the calculation, a correctly rendered word is a token that is found in a reference dictionary; an incorrectly-rendered word is a token that is not found in the dictionary. Certain types of tokens are ignored for the purposes of the calculation (e.g., stopwords, proper names), making the score a function primarily of lexical words likely to be found in a dictionary.</p><p>The specific token types we exclude from the calculation are the following:</p><p>â€¢ any token likely to be a proper noun or acronym;</p><p>â€¢ any token likely to be a stopword;</p><p>â€¢ any token of 3 characters or less; and</p><p>â€¢ any token that contains numerics;</p><p>These exclusions are the same as those excluded by the ISRI team. The one addition we made was the exclusion of numeric strings; the accuracy of the rendering of these (which occur with great frequency in the test collection) cannot be tested by reference to a dictionary, so they were excluded from the calculation. The reference dictionary we used was the single-word component of the Moby Words wordlist, a list of 354,984 English single words that includes archaic words and variant spellings <ref type="bibr" coords="34,378.48,497.57,9.96,8.74" target="#b0">[2]</ref>. Counted as a token likely to be a proper noun or acronym was any string beginning with an upper case alphabetic. Counted as a stopword was any word on the list of 319 stopwords made available by the Information Retrieval Group at the University of Glasgow [1]. Retrieved vs. Not-Retrieved Documents. The intuition that prompts a closer look at the effect of OCR is that teams will find it more difficult to retrieve documents with poorly rendered text than they will documents with accurately rendered text; to be sure, in some cases of poor quality OCR, metadata values can be drawn on, but, given the subtlety and complexity of the target topics, metadata alone will, in the majority of cases, not be able to compensate for the inaccurate rendering of the document text. Because the manual assessors who review the evaluation samples base their assessments on the source images of the documents, rather than on the OCR text, it would not be surprising if there were some number of documents that the assessors found relevant but, due to the poor state of the OCR, the teams were, collectively, unable to retrieve.</p><p>With our evaluation samples and our rough-and-ready OCR score at hand, it is fairly easy to test whether the intuition is borne out by the data. Using the evaluation samples, we compare, for each topic, the mean OCR score of relevant documents retrieved by at least one team to the mean OCR score of relevant documents not retrieved by any team. If the motivating intuition is correct, we should find that the latter score is lower than the former. As can be seen from the table, the data corroborate our intuition. The mean OCR scores are evidence that, for all three topics, the OCR quality of relevant documents that have been found by at least one team tends to be better than the OCR quality of relevant documents that have been missed by all teams.</p><p>OCR-Adjusted Scores. If, on the one hand, the OCR effect is real (i.e., affecting the likelihood with which a document will be successfully retrieved), and if, on the other hand, the collections of documents that typically figure in current litigation are, unlike the test collection, largely free from OCR issues, we would like to have some way to control for the OCR effect, so that those seeking to generalize from task conditions and results to real-world conditions and results would be better equipped to do so.</p><p>The approach we take is reasonably straightforward: we confine our view to those parts of the collection characterized by more accurate OCRing (as indicated by OCR scores) and see how participants performed on just those parts of the collection. More specifically, what we do is select certain threshold values for the OCR-accuracy score, then, relying on our already-adjudicated samples, obtain estimates of the recall, precision, and F 1 achieved by each participant on documents at or above those threshold values.</p><p>In selecting thresholds at which to obtain these adjusted metrics, we begin with an OCR-accuracy threshold of â‰¥ 0.95 (the minimum level of non-stopword accuracy deemed acceptable for the IR application that is the focus of the ISRI study noted above), then drop down to three additional lower-accuracy thresholds: â‰¥ 0.85, â‰¥ 0.75, and â‰¥ 0.50. The estimated proportions of the full collection (or manually-assessable part of it; see Section 4.2.2 above) and proportions of actually relevant documents included at each threshold are summarized in Table <ref type="table" coords="35,166.45,420.80,8.49,8.74" target="#tab_15">18</ref>.  From Table <ref type="table" coords="35,141.05,683.31,8.49,8.74" target="#tab_15">18</ref>, we see that, although the collection, as a whole, is characterized by poor quality OCR, there is a sizeable subset that is characterized by more accurate rendering of the source text: nearly 20% of the collection is estimated to have an OCR-accuracy score of 0.95 or better and nearly 45% (nearly 3 million documents) a score of 0.85 or better; there is some topic-to-topic variation in the proportions due to that fact that these are sample-based estimates of total assessable documents and the fact that different manual assessors may have had slightly different interpretations of the assessability criteria. We also see that, as with assessable documents in general, so with relevant documents in particular, a substantial proportion are included in the subsets characterized by high-quality OCR: generalizing across topics, about 25% of relevant documents have an OCR score of 0.95 or better and 50% a score of 0.85 or better. Table <ref type="table" coords="36,114.26,170.80,9.96,8.74" target="#tab_17">19</ref> summarizes, for each topic, the estimates of recall, precision, and F 1 we obtain for each participant at each OCR-accuracy threshold; of course, if we dropped the threshold all the way to 0.00, we would obtain the unadjusted post-adjudication metrics already reported (Table <ref type="table" coords="36,392.18,194.71,8.30,8.74" target="#tab_5">15</ref>).  From the table, we see that precision is for the most part not affected by the quality of the OCR; increasing the number of poorly-OCRed documents in the test set does not appear to result in a higher proportion of false positives. Recall, on the other hand, is affected: as you include more poorly OCRed documents (and those of increasingly poor quality) into the set on which you measure performance, you find that recall tends to decrease, for all participants and all topics (and continues to drop, down to the unadjusted post-adjudication numbers). Looked at another way, if you confine your attention to the part of the collection characterized by higher-quality OCRing (that part most like a collection likely to be targeted in contemporary litigation), you find that all participants are estimated to have achieved higher levels of recall than they are estimated to have achieved when the poorly OCRed documents are included in the test set; indeed, for the team that had the highest unadjusted post-adjudication recall (H5), the OCR-based adjustments point to the achievement of recall in the neighborhood of 80%. We see, finally, that the ordering of results, in terms of F 1 , is unaffected by the thresholding; the changes to collection characteristics brought about by thresholding on OCR scores result in across-the-board upward adjustments to recall, so the relative rankings based on F 1 do not change.</p><p>In this section, we have taken a brief look at the possible effect of the state of the OCR in the test collection on participants' results. Our analysis has been a deliberately rough-and-ready one, and it could certainly be followed up with additional research and analysis. We could, for example, look more narrowly at specific OCR intervals and see whether there is a particular point at which the state of the OCR begins to have a strong negative effect on recall (and whether that point differs for different approaches). For the purpose of this section, however, which is to provide additional perspective for those seeking to generalize from task conditions and results to real-world conditions and results, the analysis suffices to provide evidence (a) that the state of the OCR in the test collection has a real (depressing) effect on recall and (b) that, if you adjust for that effect, you will see increases, sometimes substantial, in the levels of recall reported for all participants. We conclude the section with a visual; Figure <ref type="figure" coords="37,337.93,87.11,4.98,8.74" target="#fig_5">5</ref> plots each team's adjusted scores (assuming an OCR-accuracy threshold of 0.85) on precision-recall diagrams.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Conclusions from the 2008 Interactive Task</head><p>We wrap up the paper with some general observations on the 2008 Legal Track (Section 5). Before turning to that conclusion, however, we summarize a few of the key lessons learned from this year's Interactive Task.</p><p>â€¢ The Interactive Task is intended to model the conditions and objectives of real-world responsive review as accurately as possible. To that end, we introduced a number of new elements to the design of the task, the most important of these being the role of the Topic Authority. While there are always challenges to implementing a new design, and this task was no exception, the design fostered needed communication between the attorneys and researchers who participated in the task and yielded interesting and meaningful results.</p><p>â€¢ If one engages with that authority effectively, one can bring about the conditions for effective document retrieval; the one participant in this year's exercise who took full advantage of the opportunity to interact with their designated Topic Authority was able to achieve, simultaneously, high recall and high precision (in the neighborhood of 80% on both metrics, if you adjust for the state of the OCR in the test collection).</p><p>â€¢ This year's exercise taught us (track coordinators, topic authorities, participating teams, and others who contributed their support) a great deal, lessons that we will build on in what we expect to be a still more productive running of the task in 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In the three years of the TREC Legal Track we have constructed a unique test collection that we expect to have enduring value. The collection contains OCR text and metadata for nearly 7 million documents (with the corresponding document images also available) and relevance assessments for a total of 109 richly structured topics. Moreover, we have iterated to what we believe to be relatively stable evaluation designs for the Ad Hoc, Relevance Feedback, and Interactive tasks. A community of 22 research teams from 21 institutions in seven countries that did not exist three years ago now has experience with one or more of those tasks, and a broad range of results have been reported. This year's track yielded several important results. Shifting the principal evaluation measure in the Ad Hoc task from recall to F 1 and shifting the reference point from one specified by us (B) to one specified by each participating team (K) resulted in greater task fidelity, modeling the real operational requirement of e-discovery systems to return a set of documents, not just an unbounded ranked list. Five of the 10 participating research teams submitted a run of higher mean F 1 @K than the reference Boolean run, an encouraging result given that, in the previous year (2007), no team had outscored the reference Boolean run in that year's main measure (mean Recall@B).</p><p>However, we cannot say that the additional flexibility of setting K was the reason for the automated approaches outperforming the negotiated Boolean queries in 2008 because 7 of the 10 participating research teams actually submitted a run of higher mean Recall@B than the reference Boolean run in 2008. Furthermore, the highest mean Recall@B was from a run that used the same automated approach as a run from 2007 which scored lower in Recall@B than the reference Boolean run of 2007. More analysis is needed to understand this result. A possible contributing factor is that, compared to the previous year, this year's topics were found to average five times as many relevant documents (i.e., they were "broader"). The result sets for the final Boolean queries averaged eight times larger than last year, which is also consistent with broader topics. Another possibility is simply natural variation in human performance; the Boolean negotiations that we use as a reference are themselves a human activity that naturally can vary somewhat from session to session.</p><p>A second major change this year was the introduction of a "highly relevant" category for relevance assessment, modeling the legal concept of evidence being "material" rather than merely relevant. It is not yet clear how useful this new category will be, in part because we have yet to study inter-annotator agreement for this assessment task. Nonetheless, we found that the consensus Boolean query found 42% of the highly relevant documents, on average per topic, which is better coverage than its recall of all relevant documents (33% this year). However, this result still implies that, on average per topic, 58% of the "highly relevant" documents were not found by the consensus Boolean query, indicating that it is not just tangentially relevant documents that are being missed by the negotiated Boolean approach.</p><p>The Relevance Feedback task attracted fewer participants than we had hoped this year. The participants' feedback runs did not generally outperform the baseline runs, which may reflect the difficulty of automatically making use of feedback in the presence of OCR errors. An intriguing observation from this year's task, which re-used topics from previous years, is that the estimated numbers of relevant documents for a topic often differed dramatically depending on which year's assessments were used. This suggests that we should devote some attention to characterizing the extent to which differing in assessor opinion may be a confounding factor when comparing results obtained through sampling and estimation.</p><p>The completely redesigned Interactive task attracted research teams from two companies and two universities, a number sufficient to wring out the new evaluation design. No participant undertook a completely manual review (although that would be permissible). The approaches used in this task varied widely, as did the time and resources invested, and the number of topics completed (which varied between 1 and 3). For the one topic completed by all participants, the submitted result sets all had similar precision (71%-81%), but recall varied substantially (from less than 3% to more than 60%). Two of the four submissions substantially outperformed the consensus Boolean query on this topic (which had a precision of 76% and recall of 13%). Relatively rich sampling was done for the topics of this task (up to 6,500 assessments for one topic). We expect that the far richer sampling for these topics will serve as a useful reference point when designing cost-effective sampling strategies in the future. The participants also had the opportunity to appeal the original assessments in this task, further increasing the quality of this resource. As we approach the limit of what relevance assessment by volunteers can support, cost-effective sampling is a matter of increasing urgency.</p><p>Although there is surely more to be learned by continuing to work with the IIT CDIP v1.0 test collection, we are nearing the point of diminishing returns beyond which further investment in this one collection may no longer yield new insights with importance that is in line with the costs to participants and assessors. That is not to say that further work with this test collection would not be justified. Quite to the contrary, a lot of work remains to be done. But the test collection needed to support that work is now for the most part in place, and for that reason we are now considering turning our attention to other collections with new characteristics that would help to extend our investigation of this important problem space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,135.24,455.31,341.52,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Highly relevant documents not found by the consensus Boolean run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,107.87,403.35,432.13,8.74;15,72.00,415.31,468.00,8.74;15,72.00,427.26,373.32,8.74"><head></head><label></label><figDesc>above criteria left 64 topics to choose from. Grouped by complaint, remaining were 4 topics from 2006-A, 7 topics from 2006-B, 5 topics from 2006-C, 4 topics from 2006-D, 4 topics from 2006-E, 13 topics from 2007-A, 9 topics from 2007-B, 8 topics from 2007-C, and 10 topics from 2007-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="25,223.71,617.61,164.58,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Team-TA Interaction Time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="33,208.54,249.11,194.91,9.65"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F 1 vs. Team-TA Interaction Time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="37,123.56,329.83,364.88,8.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Recall &amp; Precision -OCR-Adjusted (OCR-Accuracy Threshold = 0.85).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,117.75,127.51,384.68,264.20"><head>24 Topics Estimated Highly Relevant Documents</head><label></label><figDesc></figDesc><table coords="9,128.59,127.51,373.84,251.98"><row><cell>60,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Ranked Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Boolean</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>109</cell><cell>149</cell><cell>150</cell><cell>137</cell><cell>148</cell><cell>118</cell><cell>132</cell><cell>128</cell><cell>147</cell><cell>138</cell><cell>139</cell><cell>131</cell><cell>105</cell><cell>119</cell><cell>110</cell><cell>129</cell><cell>127</cell><cell>120</cell><cell>121</cell><cell>124</cell><cell>133</cell><cell>113</cell><cell>126</cell><cell>145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,72.00,76.92,468.00,234.66"><head>Table 1 :</head><label>1</label><figDesc>Mean scores of the Negotiated Boolean Queries and Median Mean Scores of the Participant Runs.</figDesc><table coords="10,142.44,76.92,310.61,209.81"><row><cell>All Relevant (26 topics)</cell><cell cols="2">Retrieved Precision</cell><cell>Recall</cell><cell>F 1</cell></row><row><cell>Defendant</cell><cell>3,180</cell><cell>0.41</cell><cell>0.04</cell><cell>0.05</cell></row><row><cell>Plaintiff</cell><cell>219,606</cell><cell>0.23</cell><cell>0.43</cell><cell>0.19</cell></row><row><cell>Consensus1</cell><cell>93,190</cell><cell>0.24</cell><cell>0.33</cell><cell>0.20</cell></row><row><cell>Final</cell><cell>40,402</cell><cell>0.28</cell><cell>0.24</cell><cell>0.16</cell></row><row><cell></cell><cell>Avg. K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Median (23 request runs)</cell><cell>14,363</cell><cell>0.26</cell><cell>0.12</cell><cell>0.10</cell></row><row><cell>Median (41 other runs)</cell><cell>40,402</cell><cell>0.28</cell><cell>0.25</cell><cell>0.16</cell></row><row><cell cols="2">Highly Relevant only (24 topics)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Defendant</cell><cell>3,445</cell><cell>0.14</cell><cell>0.06</cell><cell>0.06</cell></row><row><cell>Plaintiff</cell><cell>234,016</cell><cell>0.08</cell><cell>0.57</cell><cell>0.09</cell></row><row><cell>Consensus1</cell><cell>97,259</cell><cell>0.07</cell><cell>0.42</cell><cell>0.09</cell></row><row><cell>Final</cell><cell>39,930</cell><cell>0.08</cell><cell>0.33</cell><cell>0.09</cell></row><row><cell></cell><cell>Avg. K h</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Median (23 request runs)</cell><cell>5,838</cell><cell>0.10</cell><cell>0.22</cell><cell>0.05</cell></row><row><cell>Median (41 other runs)</cell><cell>19,965</cell><cell>0.09</cell><cell>0.34</cell><cell>0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,98.09,85.14,413.13,586.07"><head>Table 2 :</head><label>2</label><figDesc>Mean scores for submitted Ad Hoc task runs, using All Relevant documents.</figDesc><table coords="11,489.55,85.14,21.67,6.12"><row><cell>R@ret</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,88.33,85.14,432.66,586.07"><head>Table 3 :</head><label>3</label><figDesc>Mean scores for submitted Ad Hoc task runs, using only Highly Relevant documents.</figDesc><table coords="12,498.30,85.14,21.67,6.12"><row><cell>R@ret</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,86.94,76.95,435.28,229.00"><head>Table 4 :</head><label>4</label><figDesc>Median and Highest Estimated Marginal Precision RatesTable</figDesc><table coords="14,289.91,76.95,218.71,8.74"><row><cell>Depths</cell><cell>Depths</cell><cell>Depths</cell><cell>Depths</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="17,121.04,76.95,369.92,308.01"><head>Table 5 :</head><label>5</label><figDesc>Comparison of Estimated Numbers of Relevant Documents (2007 vs. 2008).</figDesc><table coords="17,125.12,76.95,358.44,308.01"><row><cell>Topic</cell><cell cols="5">Judged Rel. in 2007 Est. Rel. in 2007 Est. Resid. Rel. in 2008</cell></row><row><cell>60 (2007-A-9)</cell><cell>10</cell><cell></cell><cell>83.2</cell><cell></cell><cell>36,821.0</cell></row><row><cell>73 (2007-B-5)</cell><cell>72</cell><cell></cell><cell>31,894.5</cell><cell></cell><cell>101,196.9</cell></row><row><cell>79 (2007-C-1)</cell><cell>35</cell><cell></cell><cell>1,486.6</cell><cell></cell><cell>56,162.3</cell></row><row><cell>80 (2007-C-2)</cell><cell>391</cell><cell></cell><cell>38,649.9</cell><cell></cell><cell>46,094.8</cell></row><row><cell>83 (2007-C-5)</cell><cell>44</cell><cell></cell><cell>13,987.5</cell><cell></cell><cell>830.6</cell></row><row><cell>85 (2007-C-7)</cell><cell>96</cell><cell></cell><cell>3,890.7</cell><cell></cell><cell>746.7</cell></row><row><cell>89 (2007-D-1)</cell><cell>78</cell><cell></cell><cell>6,083.6</cell><cell></cell><cell>11,660.8</cell></row><row><cell>Avg.</cell><cell>104</cell><cell></cell><cell>13725.1</cell><cell></cell><cell>36216.2</cell></row><row><cell cols="4">All Relevant (12 topics) Retrieved Precision</cell><cell>Recall</cell><cell>F 1</cell></row><row><cell cols="2">Reference Boolean</cell><cell>3,488</cell><cell>0.37</cell><cell>0.23</cell><cell>0.14</cell></row><row><cell></cell><cell cols="2">Avg. K r</cell><cell></cell><cell></cell></row><row><cell cols="2">Median (10 baseline runs)</cell><cell>2,965</cell><cell>0.22</cell><cell>0.18</cell><cell>0.09</cell></row><row><cell cols="2">Median (19 feedback runs)</cell><cell>3,519</cell><cell>0.23</cell><cell>0.12</cell><cell>0.06</cell></row><row><cell cols="3">Highly Relevant only (9 topics)</cell><cell></cell><cell></cell></row><row><cell cols="2">Reference Boolean</cell><cell>3,870</cell><cell>0.11</cell><cell>0.34</cell><cell>0.12</cell></row><row><cell></cell><cell cols="2">Avg. K hr</cell><cell></cell><cell></cell></row><row><cell cols="2">Median (10 baseline runs)</cell><cell>1,714</cell><cell>0.08</cell><cell>0.23</cell><cell>0.05</cell></row><row><cell cols="2">Median (19 feedback runs)</cell><cell>3,894</cell><cell>0.04</cell><cell>0.24</cell><cell>0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,107.98,401.63,396.04,8.74"><head>Table 6 :</head><label>6</label><figDesc>Mean scores (Boolean and Participant Medians) for the Relevance Feedback task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="18,72.00,270.92,468.00,20.69"><head>Table 7 :</head><label>7</label><figDesc>Consistency of Previous and New Judgments for the 12 RF Topics (tot=total, hrel=highly relevant, orel=other relevant, non=non-relevant, gr=gray).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="19,82.08,94.98,444.49,559.48"><head>Table 8 :</head><label>8</label><figDesc>Mean scores for submitted Relevance Feedback task runs, using All Relevant documents.</figDesc><table coords="19,82.08,94.98,444.49,559.48"><row><cell></cell><cell>bvmBM</cell><cell>3488</cell><cell>3488</cell><cell>(0.367, 0.228)</cell><cell>0.142</cell><cell></cell><cell></cell><cell></cell><cell>0.228, 0.228</cell></row><row><cell>otRF08fb</cell><cell>bvmBM</cell><cell>3488</cell><cell>3488</cell><cell>(0.367, 0.228)</cell><cell>0.142</cell><cell>0.151</cell><cell></cell><cell>9/12, 0.656</cell><cell>0.228, 0.228</cell></row><row><cell>otRF08rvl</cell><cell>rmM</cell><cell>100000</cell><cell>87738</cell><cell>(0.126, 0.607)</cell><cell>0.134</cell><cell>0.185</cell><cell></cell><cell>6/12, 0.450</cell><cell>0.096, 0.640</cell></row><row><cell>UMKCTL08RF6</cell><cell>F-bBM</cell><cell>100000</cell><cell>3476</cell><cell>(0.367, 0.212)</cell><cell>0.131</cell><cell>0.309</cell><cell></cell><cell>8/12, 0.667</cell><cell>0.212, 0.690</cell></row><row><cell>otRF08fv</cell><cell>bmM</cell><cell>100000</cell><cell>70255</cell><cell>(0.099, 0.495)</cell><cell>0.119</cell><cell>0.202</cell><cell></cell><cell>8/12, 0.533</cell><cell>0.103, 0.596</cell></row><row><cell>otRF08frw</cell><cell>brmBM</cell><cell>100000</cell><cell>39726</cell><cell>(0.140, 0.549)</cell><cell>0.116</cell><cell>0.307</cell><cell cols="2">10/12, 0.800</cell><cell>0.236, 0.659</cell></row><row><cell>UMKCTL08RF3</cell><cell>F-bM</cell><cell>100000</cell><cell>859</cell><cell>(0.377, 0.142)</cell><cell>0.109</cell><cell>0.239</cell><cell></cell><cell>7/12, 0.583</cell><cell>0.154, 0.662</cell></row><row><cell>IowaSL08RF3B</cell><cell></cell><cell>50663</cell><cell>2442</cell><cell>(0.249, 0.207)</cell><cell>0.092</cell><cell>0.209</cell><cell></cell><cell>8/12, 0.650</cell><cell>0.199, 0.545</cell></row><row><cell>UMKCTL08RF2</cell><cell>F-bM</cell><cell>100000</cell><cell>880</cell><cell>(0.371, 0.108)</cell><cell>0.091</cell><cell>0.206</cell><cell></cell><cell>8/12, 0.667</cell><cell>0.120, 0.626</cell></row><row><cell>UMKCTL08RF1</cell><cell>F-bM</cell><cell>100000</cell><cell>875</cell><cell>(0.327, 0.124)</cell><cell>0.088</cell><cell>0.225</cell><cell></cell><cell>8/12, 0.629</cell><cell>0.148, 0.669</cell></row><row><cell>otRF08fbF</cell><cell>F-bmBM</cell><cell>100000</cell><cell>36620</cell><cell>(0.118, 0.392)</cell><cell>0.084</cell><cell>0.261</cell><cell></cell><cell>8/12, 0.633</cell><cell>0.261, 0.472</cell></row><row><cell>UMKCTL08RF5</cell><cell>F-bBM</cell><cell>100000</cell><cell>3519</cell><cell>(0.348, 0.149)</cell><cell>0.083</cell><cell>0.237</cell><cell></cell><cell>8/12, 0.667</cell><cell>0.144, 0.670</cell></row><row><cell>otRF08fbFR</cell><cell>F-bmBM</cell><cell>100000</cell><cell>8776</cell><cell>(0.266, 0.261)</cell><cell>0.082</cell><cell>0.261</cell><cell></cell><cell>8/12, 0.633</cell><cell>0.261, 0.472</cell></row><row><cell>IowaSL08RF1B</cell><cell></cell><cell>75148</cell><cell>2442</cell><cell>(0.287, 0.150)</cell><cell>0.082</cell><cell>0.204</cell><cell></cell><cell>8/12, 0.589</cell><cell>0.135, 0.694</cell></row><row><cell>otRF08F</cell><cell>F-mM</cell><cell>100000</cell><cell>36716</cell><cell>(0.109, 0.327)</cell><cell>0.075</cell><cell>0.156</cell><cell></cell><cell>7/12, 0.600</cell><cell>0.118, 0.446</cell></row><row><cell>SabL08rf1</cell><cell>F-bdporm</cell><cell>99609</cell><cell>3547</cell><cell>(0.318, 0.145)</cell><cell>0.075</cell><cell>0.248</cell><cell></cell><cell>9/12, 0.717</cell><cell>0.145, 0.606</cell></row><row><cell>IowaSL08RF3A</cell><cell>F</cell><cell>50663</cell><cell>2442</cell><cell>(0.227, 0.148)</cell><cell>0.062</cell><cell>0.150</cell><cell></cell><cell>3/12, 0.383</cell><cell>0.163, 0.545</cell></row><row><cell>SabL08rfbase</cell><cell>bdporm</cell><cell>100000</cell><cell>3517</cell><cell>(0.313, 0.111)</cell><cell>0.060</cell><cell>0.167</cell><cell></cell><cell>5/12, 0.422</cell><cell>0.110, 0.579</cell></row><row><cell>IowaSL08RFTr</cell><cell>F-bdpr</cell><cell>100000</cell><cell>2302</cell><cell>(0.214, 0.116)</cell><cell>0.058</cell><cell>0.160</cell><cell></cell><cell>6/12, 0.650</cell><cell>0.118, 0.617</cell></row><row><cell>IowaSL08RF1A</cell><cell>F</cell><cell>75148</cell><cell>2354</cell><cell>(0.150, 0.121)</cell><cell>0.058</cell><cell>0.197</cell><cell></cell><cell>5/12, 0.367</cell><cell>0.132, 0.689</cell></row><row><cell>IowaSL08RF2B</cell><cell></cell><cell>15944</cell><cell>2288</cell><cell>(0.231, 0.104)</cell><cell>0.053</cell><cell>0.099</cell><cell></cell><cell>6/12, 0.517</cell><cell>0.085, 0.300</cell></row><row><cell>IowaSL08RF2A</cell><cell>F</cell><cell>15944</cell><cell>2344</cell><cell>(0.209, 0.098)</cell><cell>0.051</cell><cell>0.084</cell><cell></cell><cell>4/12, 0.283</cell><cell>0.080, 0.300</cell></row><row><cell>UCBM25T10Th5</cell><cell>F-r</cell><cell>100000</cell><cell>21406</cell><cell>(0.257, 0.086)</cell><cell>0.048</cell><cell>0.126</cell><cell></cell><cell>3/12, 0.392</cell><cell>0.069, 0.395</cell></row><row><cell>otRF08FR</cell><cell>F-mM</cell><cell>100000</cell><cell>8826</cell><cell>(0.203, 0.129)</cell><cell>0.047</cell><cell>0.156</cell><cell></cell><cell>7/12, 0.600</cell><cell>0.118, 0.446</cell></row><row><cell>UCPwrT10Th5</cell><cell>F-r</cell><cell>100000</cell><cell>13233</cell><cell>(0.172, 0.113)</cell><cell>0.036</cell><cell>0.092</cell><cell></cell><cell>3/12, 0.267</cell><cell>0.066, 0.445</cell></row><row><cell>IowaSL08RF2C</cell><cell>F</cell><cell>15944</cell><cell>2351</cell><cell>(0.176, 0.086)</cell><cell>0.034</cell><cell>0.091</cell><cell></cell><cell>4/12, 0.333</cell><cell>0.083, 0.300</cell></row><row><cell>UCBM25T5Th5</cell><cell>F-r</cell><cell>100000</cell><cell>6517</cell><cell>(0.240, 0.037)</cell><cell>0.021</cell><cell>0.121</cell><cell></cell><cell>6/12, 0.433</cell><cell>0.070, 0.377</cell></row><row><cell>UCPwrT5Th5</cell><cell>F-r</cell><cell>100000</cell><cell>11880</cell><cell>(0.110, 0.101)</cell><cell>0.020</cell><cell>0.079</cell><cell></cell><cell>2/12, 0.267</cell><cell>0.062, 0.442</cell></row><row><cell>UCRFPwrBL</cell><cell>r</cell><cell>100000</cell><cell>1547</cell><cell>(0.113, 0.034)</cell><cell>0.017</cell><cell>0.089</cell><cell></cell><cell>1/12, 0.117</cell><cell>0.054, 0.420</cell></row><row><cell>UCRFBM25BL</cell><cell>r</cell><cell>100000</cell><cell>564</cell><cell>(0.200, 0.014)</cell><cell>0.013</cell><cell>0.128</cell><cell></cell><cell>3/12, 0.283</cell><cell>0.074, 0.432</cell></row><row><cell>randomRF08</cell><cell></cell><cell>100000</cell><cell>20000</cell><cell>(0.005, 0.002)</cell><cell>0.003</cell><cell>0.002</cell><cell></cell><cell>0/12, 0.000</cell><cell>0.002, 0.018</cell></row><row><cell>Run</cell><cell>Fields</cell><cell>Retr</cell><cell>K hr</cell><cell>(P@K hr , R@K hr )</cell><cell>F1@K hr</cell><cell cols="2">F1@R hr</cell><cell>S1J, P5</cell><cell>R@Br, R@retr</cell></row><row><cell>UMKCTL08RF2</cell><cell>F-bM</cell><cell>100000</cell><cell>879</cell><cell>(0.109, 0.259)</cell><cell>0.136</cell><cell>0.126</cell><cell></cell><cell>3/9, 0.222</cell><cell>0.181, 0.607</cell></row><row><cell>otRF08frw</cell><cell>brmBM</cell><cell>100000</cell><cell>2392</cell><cell>(0.124, 0.336)</cell><cell>0.129</cell><cell>0.145</cell><cell></cell><cell>2/9, 0.289</cell><cell>0.347, 0.647</cell></row><row><cell>UMKCTL08RF3</cell><cell>F-bM</cell><cell>100000</cell><cell>857</cell><cell>(0.121, 0.269)</cell><cell>0.128</cell><cell>0.158</cell><cell></cell><cell>2/9, 0.222</cell><cell>0.237, 0.699</cell></row><row><cell>UMKCTL08RF6</cell><cell>F-bBM</cell><cell>100000</cell><cell>3857</cell><cell>(0.117, 0.358)</cell><cell>0.124</cell><cell>0.204</cell><cell></cell><cell>4/9, 0.311</cell><cell>0.359, 0.694</cell></row><row><cell>UMKCTL08RF1</cell><cell>F-bM</cell><cell>100000</cell><cell>872</cell><cell>(0.098, 0.286)</cell><cell>0.121</cell><cell>0.157</cell><cell></cell><cell>1/9, 0.222</cell><cell>0.250, 0.693</cell></row><row><cell>refRF08B</cell><cell>bvmBM</cell><cell>3870</cell><cell>3870</cell><cell>(0.112, 0.336)</cell><cell>0.118</cell><cell></cell><cell></cell><cell></cell><cell>0.336, 0.336</cell></row><row><cell>otRF08fb</cell><cell>bvmBM</cell><cell>3870</cell><cell>3870</cell><cell>(0.112, 0.336)</cell><cell>0.118</cell><cell>0.148</cell><cell></cell><cell>2/9, 0.259</cell><cell>0.336, 0.336</cell></row><row><cell>UMKCTL08RF5</cell><cell>F-bBM</cell><cell>100000</cell><cell>3894</cell><cell>(0.083, 0.239)</cell><cell>0.092</cell><cell>0.146</cell><cell></cell><cell>3/9, 0.222</cell><cell>0.239, 0.694</cell></row><row><cell>IowaSL08RF3B</cell><cell></cell><cell>55146</cell><cell>1035</cell><cell>(0.105, 0.284)</cell><cell>0.084</cell><cell>0.123</cell><cell></cell><cell>6/9, 0.378</cell><cell>0.346, 0.540</cell></row><row><cell>IowaSL08RFTr</cell><cell>F-bdpr</cell><cell>100000</cell><cell>923</cell><cell>(0.146, 0.157)</cell><cell>0.076</cell><cell>0.080</cell><cell></cell><cell>4/9, 0.422</cell><cell>0.159, 0.583</cell></row><row><cell>IowaSL08RF1A</cell><cell>F</cell><cell>79742</cell><cell>946</cell><cell>(0.057, 0.187)</cell><cell>0.066</cell><cell>0.093</cell><cell></cell><cell>1/9, 0.111</cell><cell>0.243, 0.794</cell></row><row><cell>IowaSL08RF1B</cell><cell></cell><cell>79742</cell><cell>1035</cell><cell>(0.094, 0.169)</cell><cell>0.065</cell><cell>0.104</cell><cell></cell><cell>6/9, 0.393</cell><cell>0.223, 0.795</cell></row><row><cell>UCBM25T10Th5</cell><cell>F-r</cell><cell>100000</cell><cell>18905</cell><cell>(0.052, 0.257)</cell><cell>0.054</cell><cell>0.096</cell><cell></cell><cell>0/9, 0.111</cell><cell>0.117, 0.607</cell></row><row><cell>otRF08fbFR</cell><cell>F-bmBM</cell><cell>100000</cell><cell>10150</cell><cell>(0.096, 0.400)</cell><cell>0.050</cell><cell>0.202</cell><cell></cell><cell>3/9, 0.267</cell><cell>0.367, 0.661</cell></row><row><cell>SabL08rfbase</cell><cell>bdporm</cell><cell>100000</cell><cell>3896</cell><cell>(0.064, 0.146)</cell><cell>0.049</cell><cell>0.053</cell><cell></cell><cell>1/9, 0.148</cell><cell>0.144, 0.686</cell></row><row><cell>otRF08fbF</cell><cell>F-bmBM</cell><cell>100000</cell><cell>15305</cell><cell>(0.032, 0.516)</cell><cell>0.044</cell><cell>0.202</cell><cell></cell><cell>3/9, 0.267</cell><cell>0.367, 0.661</cell></row><row><cell>IowaSL08RF2B</cell><cell></cell><cell>16229</cell><cell>912</cell><cell>(0.096, 0.120)</cell><cell>0.044</cell><cell>0.093</cell><cell></cell><cell>4/9, 0.222</cell><cell>0.147, 0.459</cell></row><row><cell>SabL08rf1</cell><cell>F-bdporm</cell><cell>99628</cell><cell>3926</cell><cell>(0.034, 0.176)</cell><cell>0.042</cell><cell>0.103</cell><cell></cell><cell>1/9, 0.200</cell><cell>0.175, 0.703</cell></row><row><cell>IowaSL08RF3A</cell><cell>F</cell><cell>55146</cell><cell>1035</cell><cell>(0.040, 0.204)</cell><cell>0.037</cell><cell>0.095</cell><cell></cell><cell>2/9, 0.244</cell><cell>0.287, 0.540</cell></row><row><cell>UCPwrT10Th5</cell><cell>F-r</cell><cell>100000</cell><cell>8934</cell><cell>(0.032, 0.174)</cell><cell>0.032</cell><cell>0.041</cell><cell></cell><cell>0/9, 0.000</cell><cell>0.186, 0.728</cell></row><row><cell>otRF08rvl</cell><cell>rmM</cell><cell>100000</cell><cell>39342</cell><cell>(0.018, 0.418)</cell><cell>0.029</cell><cell>0.036</cell><cell></cell><cell>3/9, 0.200</cell><cell>0.133, 0.718</cell></row><row><cell>otRF08FR</cell><cell>F-mM</cell><cell>100000</cell><cell>10213</cell><cell>(0.016, 0.311)</cell><cell>0.028</cell><cell>0.089</cell><cell></cell><cell>2/9, 0.244</cell><cell>0.237, 0.658</cell></row><row><cell>IowaSL08RF2A</cell><cell>F</cell><cell>16229</cell><cell>956</cell><cell>(0.029, 0.110)</cell><cell>0.027</cell><cell>0.046</cell><cell></cell><cell>1/9, 0.089</cell><cell>0.126, 0.459</cell></row><row><cell>IowaSL08RF2C</cell><cell>F</cell><cell>16229</cell><cell>963</cell><cell>(0.020, 0.115)</cell><cell>0.025</cell><cell>0.078</cell><cell></cell><cell>1/9, 0.133</cell><cell>0.131, 0.459</cell></row><row><cell>otRF08fv</cell><cell>bmM</cell><cell>100000</cell><cell>22248</cell><cell>(0.018, 0.371)</cell><cell>0.025</cell><cell>0.085</cell><cell></cell><cell>1/9, 0.156</cell><cell>0.185, 0.686</cell></row><row><cell>UCPwrT5Th5</cell><cell>F-r</cell><cell>100000</cell><cell>4557</cell><cell>(0.029, 0.044)</cell><cell>0.020</cell><cell>0.031</cell><cell></cell><cell>0/9, 0.067</cell><cell>0.095, 0.737</cell></row><row><cell>otRF08F</cell><cell>F-mM</cell><cell>100000</cell><cell>15390</cell><cell>(0.009, 0.444)</cell><cell>0.015</cell><cell>0.089</cell><cell></cell><cell>2/9, 0.244</cell><cell>0.237, 0.658</cell></row><row><cell>UCBM25T5Th5</cell><cell>F-r</cell><cell>100000</cell><cell>3980</cell><cell>(0.022, 0.025)</cell><cell>0.015</cell><cell>0.079</cell><cell></cell><cell>0/9, 0.133</cell><cell>0.109, 0.609</cell></row><row><cell>UCRFPwrBL</cell><cell>r</cell><cell>100000</cell><cell>265</cell><cell>(0.021, 0.008)</cell><cell>0.007</cell><cell>0.019</cell><cell></cell><cell>0/9, 0.022</cell><cell>0.069, 0.505</cell></row><row><cell>UCRFBM25BL</cell><cell>r</cell><cell>100000</cell><cell>323</cell><cell>(0.051, 0.013)</cell><cell>0.006</cell><cell>0.026</cell><cell></cell><cell>1/9, 0.100</cell><cell>0.106, 0.568</cell></row><row><cell>randomRF08</cell><cell></cell><cell>100000</cell><cell>10000</cell><cell>(0.000, 0.000)</cell><cell>0.000</cell><cell>0.000</cell><cell></cell><cell>0/9, 0.000</cell><cell>0.000, 0.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="19,73.65,666.96,464.70,8.74"><head>Table 9 :</head><label>9</label><figDesc>Mean scores for submitted Relevance Feedback task runs, using only Highly Relevant documents.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="26,242.09,318.49,127.83,8.74"><head>Table 10 :</head><label>10</label><figDesc>Submitted Results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="32,160.65,87.85,284.55,226.52"><head>Table 16 :</head><label>16</label><figDesc>Individual Ad Hoc and Boolean Runs.</figDesc><table coords="32,160.65,87.85,284.55,204.02"><row><cell>Topic</cell><cell>Run</cell><cell>Retrieved</cell><cell>R</cell><cell>P</cell><cell>F 1</cell></row><row><cell></cell><cell>High Individual Ad Hoc</cell><cell>100,000</cell><cell cols="3">0.121 0.710 0.207</cell></row><row><cell>Topic 102</cell><cell>Boolean-Defendant Boolean-Plaintiff</cell><cell>980 113,796</cell><cell cols="3">0.002 0.930 0.003 0.113 0.640 0.192</cell></row><row><cell></cell><cell>Boolean-Final</cell><cell>86,742</cell><cell cols="3">0.099 0.693 0.173</cell></row><row><cell></cell><cell>Full Collection</cell><cell>6,910,192</cell><cell cols="3">1.000 0.083 0.153</cell></row><row><cell></cell><cell>High Individual Ad Hoc</cell><cell>100,000</cell><cell cols="3">0.103 0.762 0.181</cell></row><row><cell>Topic 103</cell><cell>Boolean-Defendant Boolean-Plaintiff</cell><cell>35,290 280,383</cell><cell cols="3">0.030 0.634 0.058 0.218 0.649 0.326</cell></row><row><cell></cell><cell>Boolean-Final</cell><cell>80,225</cell><cell cols="3">0.074 0.731 0.135</cell></row><row><cell></cell><cell>Full Collection</cell><cell>6,910,192</cell><cell cols="3">1.000 0.117 0.209</cell></row><row><cell></cell><cell>High Individual Ad Hoc</cell><cell>100,000</cell><cell cols="3">0.096 0.045 0.061</cell></row><row><cell>Topic 104</cell><cell>Boolean-Defendant Boolean-Plaintiff</cell><cell>16 2,682</cell><cell cols="3">0.000 0.571 0.000 0.002 0.085 0.004</cell></row><row><cell></cell><cell>Boolean-Final</cell><cell>2,680</cell><cell cols="3">0.002 0.085 0.004</cell></row><row><cell></cell><cell>Full Collection</cell><cell>6,910,192</cell><cell cols="3">1.000 0.007 0.013</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="35,148.16,87.11,296.66,95.87"><head>Table 17 :</head><label>17</label><figDesc>Table 17 presents the results. Mean OCR Scores -Relevant Documents.</figDesc><table coords="35,161.04,123.09,283.77,37.40"><row><cell>Collection Subset</cell><cell>Topic 102</cell><cell>Topic 103</cell><cell>Topic 104</cell></row><row><cell>Strata other than All-N Stratum</cell><cell>0.868</cell><cell>0.730</cell><cell>0.857</cell></row><row><cell>All-N Stratum</cell><cell>0.617</cell><cell>0.462</cell><cell>0.533</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="35,172.09,648.07,267.83,8.74"><head>Table 18 :</head><label>18</label><figDesc>Thresholding the Collection Based on OCR Scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="36,231.97,399.03,148.06,8.74"><head>Table 19 :</head><label>19</label><figDesc>OCR-Adjusted Metrics.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,87.24,662.86,452.75,6.99;6,72.00,672.32,467.99,6.99;6,72.00,681.79,365.32,6.99"><p>Completed topics were received from individuals representing the following law schools and law firms: Boston U., Cleveland-Marshall, Florida Coastal, Golden Gate, Indiana U-Indianapolis, U. of Alabama, U. of Baltimore, U.C. Hastings, U. of Dayton, U. of Maine, Williamette, Baudino Law Group, Bullivant Houser Bailey, and McCarter &amp; English.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="20,87.24,674.37,384.95,6.99"><p>For a full description of the task protocol, see the task guidelines posted on the Legal Track website<ref type="bibr" coords="20,460.91,674.37,8.47,6.99" target="#b1">[3]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="22,87.24,638.50,452.76,6.99;22,72.00,647.96,468.00,6.99;22,72.00,657.43,468.00,6.99;22,72.00,666.89,291.49,6.99"><p>Completed review batches were received from individuals representing the following law schools and non-academic institutions: U. of Baltimore, Georgetown, Loyola Law School Los Angeles, U. of Maine, Rutgers School of Law-Camden, Texas Wesleyan, Anchors Smith Grimsley, Parker, Bunt &amp; Ainsworth, Redgrave Daley Ragan &amp; Wagner, Stafford Frey Cooper, Chevron Corporation, and the National Archives and Records Administration.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This track would not have been possible without the efforts of a great many people. Our heartfelt thanks go to <rs type="person">Ian Soboroff</rs> for creating the relevance assessment system; to the dedicated group of pro bono relevance assessors and the pro bono coordinators at the participating law schools; once again to <rs type="person">Conor Crowley</rs> (<rs type="affiliation">Daley Crowley LLP</rs>), <rs type="person">Joe Looby</rs> and <rs type="person">Ryan Bilbrey</rs> (<rs type="affiliation">FTI Consulting</rs>), and the team from <rs type="grantNumber">H5</rs> (<rs type="person">Todd Elmer</rs>, <rs type="person">Jim Donahue</rs>, <rs type="person">Misti Gerber</rs>, and others) for their invaluable assistance with the Ad Hoc task (complaint drafting, topic formulation, and participating in Boolean negotiations); also to <rs type="person">Conor Crowley</rs>, <rs type="person">Maura Grossman</rs>, and <rs type="person">Joe Looby</rs> for their service in the role as Topic Authorities; to <rs type="person">Julie Hoff</rs> and <rs type="person">Dana Novak</rs> (<rs type="person">Redgrave Daley Ragan &amp; Wagner LLP</rs>) for valuable support in the sample assessment phase of the project; and finally, to <rs type="person">Richard Braman</rs>, Executive Director of The Sedona Conference R , for his continued support of the <rs type="funder">TREC Legal Track</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WwzxfXD">
					<idno type="grant-number">H5</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Allocation among strata. As noted above (Section 4.2.1), samples were composed by sampling from each stratum, with stratum-specific sample sizes being largely proportionate to the stratum's size in the full collection; an exception was made in the case of very large strata, from which fewer documents were drawn than strict proportionality would dictate, so as to ensure that even small strata would have some representation. For Topic 103, for example, the "All-N" stratum (the stratum containing documents no team considered relevant) contained, in the full collection, 5,708,286 documents, or 82.6% of the collection; from this stratum, we sampled a large number of documents (1,625) but a number smaller than the fullcollection proportion would dictate (the 1,625 represented 25.0% of the 6,500-document sample). This under-representation of the "all-N" stratum enabled us to bring more positively assessed documents into the sample and to obtain a clearer view of where teams differed from each other, while still obtaining a good measure of the rate at which relevant documents were missed by all participants collectively.</p><p>First-Pass Assessments. A total of 22 volunteer assessors participated in the first-pass review of the evaluation samples. Most of the assessors were students at law schools; others were practicing attorneys or, in some cases, paralegals. Documents were reviewed in 500-document batches; most assessors completed a single batch, although some took on additional batches after completing their first. In carrying out their task, all assessors were, as described above (Section 4.2.2), supported by guidance provided by the Topic Authority. In the tables, the column labeled a reports, for each stratum, the number of documents the reviewers found to be assessable (as noted above (Section 4.2.2), nearly 99% of sampled documents were found to be assessable); the column labeled r 1 reports the number of documents the reviewers found to be both assessable and relevant.</p><p>Appeal &amp; Adjudication. The column labeled r 2 reports the post-adjudication counts of relevant documents, that is, for each stratum, the number of documents found to be both assessable and relevant after teams had had the opportunity to appeal any first-pass assessments they believed were inconsistent with the Topic Authority's guidance and after the Topic Authority had rendered a final assessment on those appealed documents (see Section 4.2.2). In all, 966 assessments were appealed (aggregating across all three topics). Of these, the Topic Authority agreed with the appealing team (or teams; there were some cases of overlapping appeals) on 762 (78.9%); the Topic Authority denied the appeal (maintained the first-pass assessment) on 204 (21.1%). The topic that saw the most appeals was Topic 103 (950 appealed assessments, compared to 10 for Topic 102 and 6 for Topic 104), and this is the topic for which the appeal and adjudication mechanism had the greatest impact on results. We provide some further analysis of the appeal and adjudication process below (Section 4.5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Metrics</head><p>Once the sampling/assessment/adjudication process had been completed, it was possible to obtain, via the estimation procedures described in Appendix A to this document, estimates both of the yield of (actually) relevant documents for each topic and of the effectiveness of each of the participating teams in retrieving  those documents.</p><p>Yields. Estimates (post-adjudication) of the full-collection yields of relevant documents for each topic are summarized in Table <ref type="table" coords="28,184.14,515.30,8.49,8.74">14</ref>. Reported in the table are, for both count and percentage, the point estimate of the yield and the 95% confidence interval associated with the estimate.</p><p>Metrics. Estimates (post-adjudication) of the participants' effectiveness (as measured by recall, precision, and F 1 ), in retrieving those relevant documents are summarized in Table <ref type="table" coords="28,425.01,551.16,8.49,8.74">15</ref>. Reported in the table are, for each metric, the point estimate of the metric and the 95% confidence interval associated with the estimate.</p><p>With regard to the results reported in the table, a few initial observations are possible. We see, first, that the set formed by aggregating the 64 Ad Hoc submissions (to a depth of 100,000) along with the results of the four Boolean reference queries (the Ad Hoc Pool), generally was able to achieve recall and precision scores in the 0.30 -0.40 (30% -40%) range. The one exception to the rule is the low precision achieved on Topic 104; this exception, however, is likely an effect of the manner in which the set was constructed (going to a depth of 100,000, where possible, for each run) combined with the fact that Topic 104 was very low-yielding. These recall and precision numbers were sufficient to allow the Ad Hoc Pool, on two topics (102 and 104), to rank highest in overall effectiveness (as measured by F 1 ).</p><p>We see, second, that the one team that fully availed itself of the opportunity to engage with the Topic  Authority, the team from H5 (which submitted results for Topic 103 only), was able to achieve substantially higher recall than the other entrants (including the Ad Hoc Pool) while at the same time achieving high precision (in a statistical tie with the Pittsburgh team for highest precision). This result enabled the H5 team, on the topic for which it submitted results, to rank highest in overall effectiveness (as measured by F 1 ), and that by a considerable margin (over 0.30 (30 percentage points) higher than the next-highest entry, that of the Ad Hoc Pool). We see, third, that the other participants, who took less advantage of the opportunity to engage with the Topic Authority, generally submitted results that scored high on precision but low on recall. Whether this result is to be attributed (a) to incomplete topic clarification, (b) to a drawback of the retrieval methods applied, or (c) to a combination of both, is a question for further analysis; the participants' own papers on this year's task will undoubtedly add further information on this question.</p><p>In the next section, we provide some further analysis of these results. Before turning to that analysis, however, we first provide a visual summary of the immediate post-adjudication results; Figure <ref type="figure" coords="29,487.96,472.60,4.98,8.74">3</ref> plots each team's post-adjudication scores on precision-recall diagrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Analysis</head><p>We have seen the immediate post-adjudication metrics. There remain, however, a number of important questions that merit further study if we are to gain a proper understanding of the significance of the task results. In this section, we provide some further analysis (i) of the performance of individual Ad Hoc runs; (ii) of the correlation between Team-TA interaction and effectiveness; (iii) of the impact of the adjudication process on assessments; and (iv) of the effect of the state of the OCR in the test collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Individual Ad Hoc Runs</head><p>To this point in our discussion, we have included reference to the Ad Hoc submissions, but only as an aggregated reference set. It is also possible to assess the effectiveness of individual Ad Hoc runs (as well as the Boolean reference runs); in this section, we take a look at some of the more interesting of these results.</p><p>Table <ref type="table" coords="29,114.75,658.81,9.96,8.74">16</ref> summarizes the effectiveness (as measured by recall, precision, and F 1 ) attained by select Ad Hoc and Boolean runs. The table does not provide results for all 64 ad hoc runs; these can be found in the Appendix to the Proceedings. Instead, the table focuses on the following runs:  â€¢ the highest scoring (as measured by F 1 ) of the individual Ad Hoc runs ("High Individual Ad Hoc");</p><p>â€¢ Defendant's proposed Boolean query ("Boolean-Defendant");</p><p>â€¢ Plaintiff's proposed Boolean query ("Boolean-Plaintiff");</p><p>â€¢ the final negotiated Boolean query ("Boolean-Final"); and</p><p>â€¢ the full test collection ("Full Collection").</p><p>For each run, the table presents the total number of documents retrieved by the run and the estimated recall, precision, and F 1 attained by that set of retrieved documents. Scores are based on post-adjudication assessments.</p><p>The data in the table occasion a few observations. With regard to Topic 102, recall that, for this topic, the set that, in our earlier analysis, proved most effective was the Ad Hoc Pool (attaining an F 1 score of 0.321), with the two active participants in the topic scoring somewhat lower. We see from Table <ref type="table" coords="30,507.78,456.16,9.96,8.74">16</ref> that a few of the individual runs now under consideration (High Individual Ad Hoc (in this case, Open Text's otL08frw), Boolean-Plaintiff, and Boolean-Final) retrieved sets that attained F 1 scores higher than those attained by the active participants' submissions (but still lower than the score attained by the Ad Hoc Pool); and the High Individual Ad Hoc could well have achieved a higher score, had it not been subject to the 100,000-document submission constraint. In this instance, it appears that the active participants were not able to take full advantage of the opportunity to engage with the Topic Authority as a means of improving their effectiveness (and it should be noted that they utilized only a small portion of the time available to them for this purpose).</p><p>With regard to Topic 103, recall that the set that, in our earlier analysis, proved most effective was that turned in by one of the active participants (H5), a set that attained an F 1 score of 0.705. None of the individual runs currently under consideration were able to approach that level of effectiveness, the closest being Boolean-Plaintiff which attained an F 1 score of 0.326. As can also be seen from the table, for Topic 103, as for Topic 102, a rather sizeable F 1 score can be attained simply by submitting the entire collection (0.209 for Topic 103; 0.153 for Topic 102); the high yields of these topics make for a high "floor" for precision (and thereby for F 1 ) for a full-collection submission.</p><p>With regard to Topic 104, recall that, in our earlier analysis, it was the Ad Hoc Pool that scored highest on the F 1 metric (0.043), but that neither set that was evaluated achieved particularly high scores. Of the runs reviewed in Table <ref type="table" coords="30,176.60,671.35,8.49,8.74">16</ref>, one, the High Individual Ad Hoc (in this case, Waterloo's wat7fuse), was able to achieve a higher F 1 (0.061) than that attained by the Ad Hoc Pool. For this topic, however, all runs,  including the High Individual Ad Hoc just mentioned, were found to have attained very low scores, indicating that the participants' conceptions of relevance were not well aligned with the Topic Authority's conception of what should be retrieved for the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Team-TA Interaction</head><p>Earlier (Section 4.4.1), we saw that there was considerable variation in the amount of time teams chose to spend with the Topic Authorities for the purpose of clarifying the intent and scope of the target topics; times ranged from five minutes in one instance to 485 minutes in another. Such variation naturally prompts the question of whether there is a correlation between the amount of time spent with a Topic Authority and retrieval effectiveness.</p><p>Figure <ref type="figure" coords="31,119.36,451.27,4.98,8.74">4</ref> plots retrieval effectiveness (as measured by post-adjudication F 1 scores) against time spent with the Topic Authority on the topic-clarification portion of the task. Results for all participating teams and topics are represented on the diagram; we also include the results we get for the Ad Hoc Pool (which, of course, represents the aggregated results of 68 runs, none of which made use of Topic Authority time).</p><p>From the diagram, we see that, while there are not a large number of data points (10, if we include the results for the Ad Hoc Pool), and while there are no data points that correspond to the "middle" segment of the time range, there does appear to be a correlation between effectiveness and time spent with the Topic Authority. Submissions that resulted in low F 1 scores tend to have come from approaches that made little use of the Topic Authority's time; the team that made the most use of the Topic Authority's time achieved a very high F 1 score.</p><p>The impression is borne out by correlation measures. Looking just at the results turned in by the active participants in the task (i.e., setting aside the results of the Ad Hoc Pool), we obtain a Pearson productmoment correlation coefficient of 0.927 with a 95% confidence interval of (0.577, 0.989). Even including the results of the Ad Hoc Pool, we find evidence of a positive correlation: r = 0.699 (0.124, 0.923).</p><p>If there is a correlation between retrieval effectiveness and time spent interacting with the Topic Authority, the next question is whether there are some modes of interaction that are more effective than others. After all, it would be surprising if effectiveness were simply a function of time spent with the Topic Authority, regardless of how that time was used; we would expect that there are some approaches to gathering the required information that work better than others. This is a question that merits further study. The participants' papers on their approaches to this year's task may shed some light on the question; the question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Estimation of Metrics -Interactive Task</head><p>As described more fully above (Section 4.2), evaluation in Interactive Task follows a four-step protocol, whereby (i) stratified samples are drawn for each of the target topics, (ii) a first-pass manual review is conducted of each of the samples, (iii) those first-pass assessments are, if a participating team chooses, appealed and adjudicated, and (iv) estimates of each team's performance, as measured by recall, precision, and F 1 , are obtained. In this appendix, we review the specifics of the last step, the procedures whereby estimates of our target metrics are obtained.</p><p>Arriving at estimates of the target metrics is itself a two-step process. The first step is to obtain estimates of three input components: (i) total documents both assessable and relevant in the full collection; (ii) total documents assessable in the set submitted by the team whose run is being evaluated; and (iii) total documents both assessable and relevant in the set submitted by the team whose run is being evaluated. The second step is to combine the elements in the appropriate way to obtain estimates of Recall, Precision, and F 1 . The specifics are as follows (most of the formulae cited below will be found in any standard discussion of stratified sampling, such as that in <ref type="bibr" coords="40,226.95,240.44,76.99,8.74" target="#b11">Thompson (2002)</ref>  <ref type="bibr" coords="40,307.26,240.44,14.76,8.74" target="#b11">[14]</ref>).</p><p>Notation. We begin with some notation.</p><p>N = total number of documents in the full collection.</p><p>Ï„ a = total number of assessable documents in the full collection.</p><p>Ï„ r = total number of assessable and relevant documents in the full collection.</p><p>L = total number of strata into which the full collection has been partitioned.</p><p>Ï„ a(h) = total number of assessable documents in stratum h. Ï„ r(h) = total number of assessable and relevant documents in stratum h.</p><p>p a(h) = proportion of documents in stratum h that are assessable.</p><p>p r(h) = proportion of documents in stratum h that are assessable and relevant.</p><p>n h = total number of documents sampled from stratum h. a h = of documents sampled from stratum h, total number assessable.</p><p>r h = of documents sampled from stratum h, total number assessable and relevant.</p><p>L (A) = total number of strata into which the documents Team A submitted as relevant were partitioned.</p><p>Ï„ a(A) = of documents Team A submitted as relevant, total number assessable.</p><p>Ï„ r(A) = of documents Team A submitted as relevant, total number assessable and relevant.</p><p>Estimation of inputs. Now, we review the procedures used to obtain estimates of the inputs to the metrics. As noted above, there are three inputs we require: (i) full-collection estimate of total assessable and relevant; (ii) estimate of total assessable in a team's submission; and (iii) estimate of total assessable and relevant in a team's submission.</p><p>Component 1: Full-Collection Estimate of Total Assessable and Relevant. We obtain the first component by finding the stratified estimate of the total documents both assessable and relevant in the full population. To obtain this estimate, we first find within-stratum estimates then find full-collection estimates.</p><p>Within-stratum estimates are obtained as follows.</p><p>1. Obtain estimate of within-stratum proportion of documents both assessable and relevant.</p><p>2. Obtain estimate of within-stratum total number of documents both assessable and relevant.</p><p>3. Obtain estimate of within-stratum sample variance.</p><p>4. Obtain estimate of variance of within-stratum total estimator.</p><p>Full-collection estimates are obtained as follows.</p><p>1. Obtain estimate of full-collection total number of documents both assessable and relevant.</p><p>2. Obtain estimate of variance of full-collection total estimator.</p><p>Component 2: Estimate of Total Assessable in a Team's Submission. We obtain the second component by finding the stratified estimate of the total documents assessable in the part of the population that a team identified as relevant. Obtaining this estimate is again a matter of finding the value of a stratified estimator; in this case, however, the strata that figure in the estimate are just those that contain the team's positive assessments. More specifically, the steps (for a team we'll call Team A) are the following.</p><p>Within-stratum estimates are obtained as follows.</p><p>1. Obtain estimate of within-stratum proportion of documents that are assessable.</p><p>2. Obtain estimate of within-stratum total number of documents that are assessable.</p><p>3. Obtain estimate of within-stratum sample variance.</p><p>4. Obtain estimate of variance of within-stratum total estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>var(Ï„</head><p>Full-submission estimates are obtained as follows.</p><p>1. Obtain estimate of full-submission total number of documents that are assessable.</p><p>2. Obtain estimate of variance of full-submission total estimator.</p><p>Component 3: Estimate of Total Assessable and Relevant in a Team's Submission.</p><p>We obtain the third component by finding the stratified estimate of the total documents assessable and relevant in the part of the population that a team identified as relevant. Obtaining this estimate is again a matter of finding the value of a stratified estimator; as with the second component, the strata that figure in the estimate are just those that contain the team's positive assessments. More specifically, the steps (for "Team A") are the following.</p><p>Within-stratum estimates are obtained as follows.</p><p>1. Obtain estimate of within-stratum proportion of documents that are both assessable and relevant.</p><p>2. Obtain estimate of within-stratum total number of documents that are both assessable and relevant.</p><p>Ï„r(h) = N h pr(h)</p><p>3. Obtain estimate of within-stratum sample variance.</p><p>4. Obtain estimate of variance of within-stratum total estimator.</p><p>Full-submission estimates are obtained as follows.</p><p>1. Obtain estimate of full-submission total number of documents that are both assessable and relevant. Estimation of metrics. Finally, we review the procedures used to obtain estimates of the metrics themselves. With estimates of the three inputs in hand, we combine the elements to obtain estimates of Recall, Precision, and F 1 . Variances for the estimates of the metrics are obtained by propagating the variances of their component elements (in accordance with the principles of Gaussian Error Propagation).</p><p>Recall. We obtain estimates and 95% confidence intervals for recall as follows.</p><p>1. Obtain estimate of recall (for "Team A"). Precision. We obtain estimates and 95% confidence intervals for precision as follows.</p><p>1. Obtain estimate of precision (for "Team A"). </p><p>F 1 . We obtain estimates and 95% confidence intervals for F 1 as follows.</p><p>1. Obtain estimate of F 1 (for "Team A"). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="45,91.31,110.62,284.80,7.86" xml:id="b0">
	<monogr>
		<ptr target="http://icon.shef.ac.uk/Moby/mwords.html" />
		<title level="m" coord="45,91.31,110.62,102.40,7.86">Moby Project Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,125.56,272.98,7.86" xml:id="b1">
	<monogr>
		<ptr target="http://trec-legal.umiacs.umd.edu/" />
		<title level="m" coord="45,91.31,125.56,123.90,7.86">TREC Legal Track Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,140.51,448.69,7.86;45,91.32,151.47,448.68,7.86;45,91.32,162.43,105.78,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="45,511.59,140.51,28.41,7.86;45,91.32,151.47,110.65,7.86">Million Query Track 2007 Overview</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blagovest</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Dachev</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kanoulas</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="45,221.69,151.47,270.84,7.86">The Sixteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
	<note>TREC 2007) Proceedings</note>
</biblStruct>

<biblStruct coords="45,91.31,177.37,448.69,7.86;45,91.32,188.33,369.79,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="45,326.20,177.37,138.02,7.86">TREC-2006 Legal Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="45,485.30,177.37,54.70,7.86;45,91.32,188.33,213.35,7.86">The Fifteenth Text REtrieval Conference (TREC 2006) Proceedings</title>
		<imprint>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,203.27,448.68,7.86;45,91.32,214.23,369.79,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="45,153.74,203.27,311.38,7.86">Examining Overfitting in Relevance Feedback: Sabir Research at TREC 2007</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="45,484.94,203.27,55.06,7.86;45,91.32,214.23,213.35,7.86">The Sixteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
	<note>TREC 2007) Proceedings</note>
</biblStruct>

<biblStruct coords="45,91.31,229.18,448.69,7.86;45,91.32,240.13,140.62,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Conor</forename><forename type="middle">R</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Looby</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/" />
		<title level="m" coord="45,333.20,229.18,146.27,7.86">Reflections of the Topic Authorities</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,255.08,448.69,7.86;45,91.32,266.04,291.97,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="45,172.64,255.08,111.24,7.86">The TREC Test Collections</title>
		<author>
			<persName coords=""><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="45,511.05,255.08,28.95,7.86;45,91.32,266.04,210.26,7.86">TREC: Experiment and Evaluation in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="21" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,280.98,448.69,7.86;45,91.32,291.94,355.47,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="45,242.76,280.98,137.58,7.86">Building digital tobacco document</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Butter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of California, San Francisco Library/Center for Knowledge Management. D-Lib Magazine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,306.88,272.04,7.86" xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName coords=""><surname>Sec V</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Collins &amp; Aikman Corp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="45,273.00,306.88,15.23,7.86">WL</title>
		<imprint>
			<biblScope unit="volume">94311</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,321.83,412.92,7.86" xml:id="b9">
	<monogr>
		<ptr target="http://trec-legal.umiacs.umd.edu/" />
		<title level="m" coord="45,91.31,321.83,127.36,7.86">Sedona Conference Open Letter</title>
		<imprint>
			<date type="published" when="2008-05-22">May 22. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="45,91.31,336.77,448.69,7.86;45,91.32,347.73,345.16,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="45,139.53,336.77,259.35,7.86">Measuring and delivering 95% non-stopword document accuracy</title>
		<author>
			<persName coords=""><surname>Staff</surname></persName>
		</author>
		<idno>2003-04</idno>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
			<pubPlace>Las Vegas</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information Science Research Institute, University of Nevada</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="45,91.31,377.62,222.38,7.86" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sampling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct coords="45,91.31,392.56,448.69,7.86;45,91.32,403.52,448.69,7.86;45,91.32,414.48,80.71,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="45,439.50,392.56,100.50,7.86;45,91.32,403.52,69.82,7.86">Overview of the TREC 2007 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="45,185.90,403.52,279.67,7.86">The Sixteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
	<note>TREC 2007) Proceedings</note>
</biblStruct>

<biblStruct coords="45,91.31,429.42,448.69,7.86;45,91.32,440.38,448.68,7.86;45,91.32,451.34,58.87,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="45,241.88,429.42,293.52,7.86">Estimating Average Precision with Incomplete and Imperfect Judgments</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="45,102.34,440.38,407.93,7.86">Proceedings of the 15th International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 15th International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
