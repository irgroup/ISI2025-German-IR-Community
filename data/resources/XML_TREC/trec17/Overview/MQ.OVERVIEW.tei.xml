<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.48,114.94,247.04,15.12">Million Query Track 2008 Overview</title>
				<funder>
					<orgName type="full">Microsoft Live Labs</orgName>
				</funder>
				<funder ref="#_5EGyNkz">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
				<funder ref="#_cuwacNQ #_rK3DwfP">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.43,148.84,62.97,10.48;1,151.41,147.22,1.41,6.99"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,163.29,148.84,81.19,10.48;1,244.48,147.22,1.88,6.99"><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.89,148.84,76.58,10.48;1,332.47,147.22,1.88,6.99"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark, Delaware</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.88,148.84,61.94,10.48;1,405.83,147.22,1.88,6.99"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.24,148.84,102.07,10.48;1,519.30,147.22,1.88,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.48,114.94,247.04,15.12">Million Query Track 2008 Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">98C91012D8C9E560134A2C5AF9F439EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Million Query (1MQ) track ran for the second time in TREC 2008. The track is designed to serve two purposes: first, it is an exploration of ad-hoc retrieval over a large set of queries and a large collection of documents; second, it investigates questions of system evaluation, in particular whether it is better to evaluate using many shallow judgments or fewer thorough judgments.</p><p>As with the 2007 track [ACA + 07], participants ran 10,000 queries against a collection of 25 million documents. The 2008 track differed in the following ways:</p><p>1. Queries were assigned to one of four categories.</p><p>2. Each query was assigned a target of 8, 16, 32, 64, or 128 judgments.</p><p>3. Assessors could judge documents "not relevant but reasonable".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Section 1 describes how the corpus and queries were selected, the query classes, details of the submission formats, and a brief description of each submitted run. Section 2 provides an overview of the judging process, including a sketch of how it alternated between two methods for selecting the small set of documents to be judged. Sections 3.1 and 3.2 provide an overview of those two selection methods, developed at UMass and NEU, respectively.</p><p>In Section 4 we present statistics collected during the judging process, including the total number of queries judged, how many judgments were served by each approach, and so on, along with the overall results of the track. We present additional results and analysis in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Phase I: Running Queries</head><p>The first phase of the track required that participating sites submit their retrieval runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Corpus</head><p>The 1MQ track used the so-called "terabyte" or "GOV2" collection of documents. This corpus is a collection of Web data crawled from Web sites in the .gov domain in early 2004. The collection is believed to include a large proportion of the .gov pages that were crawlable at that time, including HTML and text, plus the extracted text of PDF, Word, and PostScript files. Any document longer than 256Kb was truncated to that size at the time the collection was built. Binary files are not included as part of the collection, though were captured separately for use in judging.</p><p>The GOV2 collection includes 25 million documents in 426 gigabytes. The collection was made available by the University of Glasgow, distributed on a hard disk that was shipped to participants for an amount intended to cover the cost of preparing and shipping the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Queries</head><p>Topics for this task were drawn from a large collection of queries that were collected by a large Internet search engine. Each of the chosen queries is likely to have at least one relevant document in the GOV2 collection because logs showed a clickthrough on one page captured by GOV2. Obviously there is no guarantee that the clicked page is relevant, but it increases the chance of the query being appropriate for the collection.</p><p>These topics are short, title-length (in TREC parlance) queries. In the judging phase, they were developed into full-blown TREC topics.</p><p>Ten thousand (10,000) queries were selected for the official run. Queries were categorized based on length and number of clicks on documents in the .gov domain. Half of the 10,000 had no more than six words (designated "short") and half more than six words (designated "long"); half had more than three clicks in the .gov domain ("govheavy") and half three or fewer ("govslant"). The exact distribution is shown in Table <ref type="table" coords="2,247.35,355.65,4.24,9.57">1</ref>. (The numbers in the table does not add to 10,000 because 264 of the queries were designated for use with the Relevance Feedback track and excluded from Million Query judging.) short long govslant 2,434 2,434 govheavy 2,434 2,434 Table <ref type="table" coords="2,101.97,458.02,4.24,9.57">1</ref>: Distribution of queries by category: length at most six words ("short") versus more than six ("long"), and at most three clicks on GOV2 documents ("govslant") versus more than three ("govheavy").</p><p>No quality control was imposed on the 10,000 selected queries. The hope was that most of them would be good quality queries, but it was recognized that some were likely to be partially or entirely non-English, to contain spelling errors, or even to be incomprehensible to anyone other than the person who originally created them.</p><p>The queries were distributed in a text file where each line has the format "N:query word or words". Here, N is the query number, is followed by a colon, and immediately followed by the query itself. For example, the line "32:barack obama internships" means that query number 32 is the 3-word query "barack obama internships". All queries were provided in lowercase and with no punctuation. Query categories were not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Submissions</head><p>Sites were permitted to submit up to five runs. Every submitted run was included in the judging pool and all were treated equally. In addition, there were five "baseline" runs reflecting standard retrieval models.</p><p>A run consisted of up to the top 1,000 documents for each of the 10,000 queries. The submission format was a standard TREC format of exactly six columns per line with at least one space between the columns. For example: 100 Q0 ZF08-175-870 1 9876 mysys1 100 Q0 ZF08-306-044 2 9875 mysys2 where:</p><p>1. The first column is the topic number.</p><p>2. The second column is unused but must always be the string "Q0" (letter Q, number zero).</p><p>3. The third column is the official document number of the retrieved document, found in the &lt;DOCNO&gt; field of the document.</p><p>4. The fourth column is the rank of that document for that query.</p><p>5. The fifth column is the score this system generated to rank this document.</p><p>6. The six column was a "run tag," a unique identifier for each group and run.</p><p>If a site would normally have returned no documents for a query, it instead returned the single document "GX000-00-0000000" at rank one. Doing so maintained consistent evaluation results (averages over the same number of queries) and did not break any evaluation tools being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Submitted runs</head><p>The following is a brief summary of some of the submitted runs, from the summary information provided by the submitting sites.</p><p>ARSC/University of Alaska Fairbanks submitted five runs.</p><p>lsi150dyn The documents from each unique host name were grouped and indexed as separate collections. Each host collection was represented as a "big document" in a vector space of approximately 400,000 terms. Each topic was projected into the term vector space. The host collections were ranked relative to each topic-vector using a 150-rank LSIreduced host-by-term matrix. The topic was run against the top 50 ranked hosts with Lucene and the ranked results from each host were merged using a standard result set merge algorithm.</p><p>lsi150stat The documents from each unique host name were grouped and indexed as separate collections. Each host collection was represented as a "big document" in a vector space of approximately 400,000 terms. The 10,000 topics were represented as a "big topic" in the term space. The host collections were ranked relative to this static "big topic" using an 150-rank LSI-reduced host-by-term matrix and the 10,000 topics were run against the top 50 most relevant hosts and the results merged into a single ranked list.</p><p>vsmdyn The documents from each unique host name were grouped and indexed as separate collections. Each host collection was represented as a "big document" in a vector space of approximately 400,000 terms. Each topic was projected into the term vector space. The host collections were ranked relative to each topic-vector using vector space model cosines with the host-by-term matrix. The topic was run against the top 50 ranked hosts with Lucene and the ranked results from each host were merged using a standard result set merge algorithm.</p><p>vsmstat The documents from each unique host name were grouped and indexed as separate collections. Each host collection was represented as a "big document" in a vector space of approximately 400,000 terms. The 10,000 topics were represented as a "big topic" in the term space. The host collections were ranked relative to this static "big topic" using standard vector space model (VSM) cosines with the host-by-term matrix; the 10,000 topics were run against the top 50 most relevant hosts and the results merged into a single ranked list.</p><p>vsmstat07 This run is to test the sensitivity of the static "big topic" vector space ranking (restriction) of host collections searched. In short, the hosts were chosen by ranking them with respect to a "big topic" from the 10000 TREC 2007 MQ topics using the vector space model as in vsmstat. Then the TREC 2008 MQ topics were run against this collection of hosts (that was tuned for the 2007 topics).</p><p>I3S Group of ICT submitted two runs.</p><p>dxrun We use Wikipedia as a resources to identify entities in a query, then add term dependency features for each query. The term dependency features are actually ordered phrases. Indri search engine is used for index and retrieval.</p><p>txrun We use Wikipedia as a resources to identify entities in a query, then expand each query with ten terms(if there are any) based on Wikipedia. The term selection procedure makes use of semantic similarity measure proposed by resnik <ref type="bibr" coords="4,416.64,450.10,28.43,9.57">(1996)</ref>. Terms are ranked in descending order according to the similarity between a term and the who query. Indri search engine is used for indexing and retrieval. indri25DM08 Dependence model approach fielded during the Terabyte track two years ago and the 1MQ track last year.</p><p>In addition, there were four "baseline" runs of standard retrieval algorithms, all using the Lemur implementations: a vector-space cosine similarity run (000cos), a language modeling run (000klabs), an Okapi formula run (000okapi), a BM25 run with tf-idf weighting (000tfidfBM25), and a log tf-idf run (000tfidfLOG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Phase I: Relevance judgments and judging</head><p>After all runs were submitted, a subset of the topics were judged. The goal was to provide a small number of judgments for a large number of topics. For TREC 2008, 782 queries were judged, a large increase over the more typical 50 queries judged by other tracks in the past, though a significant decrease from the 1700 judged for TREC 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Judging overview</head><p>Judging was done by assessors at NIST. Participating sites were not asked to provide judgments this year. The process looked roughly like this from the perspective of someone judging:</p><p>1. The assessment system presented 10 queries randomly selected from one of the four categories (short-govslant, short-govheavy, long-govslant, or long-govheavy). The category from which queries were drawn rotated in a round-robin fashion by assessor to ensure that each category would be benefit from roughly the same judging effort.</p><p>2. The assessor either selected one of those ten queries to judge, or refreshed the display to see a new list of 10 from the same category. Once a query was chosen, all others that were seen were returned to the pool.</p><p>3. The assessor provided the description and narrative parts of the query, creating a full TREC topic. This information was used by the assessor to keep focus on what is relevant.</p><p>4. The system presented a GOV2 document (Web page) and asked whether it was relevant to the query. Judgments were on a four-way scale: highly relevant, relevant, not relevant but reasonable, or not relevant. Consistent with past practice, the distinction between the first two was up to the assessor.</p><p>The "not relevant but reasonable" judgment is new to the 2008 track. It was a concession to the fact that a short query is vague, and after seeing retrieved documents, assessors may realize that there are other possible topic definitions that are as reasonable as the one they submitted. Assessors used this judgment to indicate a document that was not relevant to the topic they submitted, but would be relevant to a reasonable alternative.</p><p>5. The assessor was required to continue judging until a pre-defined stopping point was reached. Each query was judged to a target of 8, 16, 32, 64, or 128 judgments. The target was selected by a scheduling algorithm that ensured that the total amount of assessor effort at each target would be roughly equal. For every query with a target of 128, there were two with a target of 64, four with a target of 32, eight with a target of 16, and 16 with a target of 8. Because of overlap between the two methods (see below), the total number of judgments obtained could be less than the intended target.</p><p>The system for carrying out those judgments was built at UMass on top of the Drupal content management platform<ref type="foot" coords="6,177.34,531.81,4.23,6.99" target="#foot_0">1</ref> . The same system was used for the Relevance Feedback track and as the starting point for relevance judgments in the Enterprise track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Selection of documents for judging</head><p>Two approaches to selecting documents were used:</p><p>Minimal Test Collection (MTC) method. In this method, documents are selected by how much they inform us about the difference in mean average precision given all the judgments that were made up to that point <ref type="bibr" coords="6,254.39,646.73,36.86,9.57" target="#b8">[CAS06]</ref>. Because average precision is quadratic in relevance judgments, the amount each relevant document contributes is a function of the total number of judgments made and the ranks they appear at. Nonrelevant documents also contribute to our knowledge: if a document is nonrelevant, it tells us that certain terms in the quadratic expansion cannot contribute anything to average precision. We quantify how much a document will contribute if it turns out to be relevant or nonrelevant, then select the one that we expect to contribute the most. This method is further described below in Section 3.1.</p><p>Statistical evaluation (statMAP) method. This method draws and judges a specific random sample of documents from the given ranked lists and produces unbiased, low-variance estimates of average precision, R-precision, and precision at standard cutoffs from these judged documents <ref type="bibr" coords="7,153.85,192.87,30.52,9.57" target="#b2">[AP07]</ref>. Additional (non-random) judged documents may also be included in the estimation process, further improving the quality of the estimates. This method is further described below in Section 3.2.</p><p>Each query was judged by alternating between the two methods until each had selected half of the target number of judgments. For example, if the target was eight, the first document might be selected by MTC, the second by statAP, the third by MTC, and so on. (The method to select the first document was determined by a coin flip.) The two methods had no knowledge of judgments to each other's documents during this phase. If a document was selected by one method after it had previously been selected by the other and judged by the assessor, it was not judged a second time.</p><p>Instead, the judging system supplied the same judgment that had previously been made. Thus it was possible that fewer documents could be judged than were originally targeted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>For a full description of the two methods, we refer the reader to the 2007 Million Query Track overview [ACA + 07] and the original work cited below. The descriptions below focus on estimates of R-precision and precision at rank thresholds that are new to the 2008 track, in addition to estimates of mean average precision that we have used in previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Minimal Test Collections</head><p>The Minimal Test Collections (MTC) method works by identifying documents that will be most informative for understanding performance differences between systems by some evaluation measure (in this case average precision). Details on the workings of MTC can be found elsewhere [CPK + 08, CAS06, ACA + 07]. Here we focus on MTC estimates of evaluation measures. First, we consider each document i to have a distribution of relevance p(X i ). If the document has been given judgment j = 0 or 1 (nonrelevant or relevant), then p(X i = j) = 1; otherwise the probability that the document is relevant is p(X i = 1) = p i . Then we consider a measure to be a random variable expressible as a function of document relevances X i . For example, precision can be expressed as a random variable that is a sum of document relevance random variables for those documents at ranks 1 through k.</p><p>If the measure is a function of document random variables, then the measure has a distribution over possible assignments of relevance to unjudged documents. Note that this applies to measures averaged over queries as well as to measures for a single query.</p><p>Abstracting even higher, this produces a distribution over possible rankings of systems. There is some probability that system 1 is better than system 2, which in turn is better than system 3; some probability that system 1 is better than system 3, which in turn is better than system 2, and so on. It can be shown that the maximum a posteriori ranking of systems is that in which systems are ranked by the expected values of the evaluation measure of interest over the possible relevance assignments.</p><p>Calculating the expectation of an evaluation measure is fairly simple. Given the probability that document i is relevant p i = p(X i = 1), we define:</p><formula xml:id="formula_0" coords="8,217.90,162.42,175.58,146.27">Eprec@k = 1 k k i=1 p i ER-prec ≈ 1 ER ER i=1 p i EAP ≈ 1 ER n i=1 p i /i + j&gt;i p i p j /j and ER = n i=1 p i .</formula><p>Note that there is an assumption that document relevances are independent, or conditionally independent given features used to estimate the distribution p(X i ).</p><p>Though MTC is designed for ranking systems (i.e. making decisions about relative differences in performance between systems), In this work we largely present expectations of evaluation measures for individual systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">statAP</head><p>In statistical terms, average precision can be thought of as the mean of a population: the elements of the population are the relevant documents in the document collection and the population value of each element is the precision at this document for the list being evaluated. This principle is the base for several recently proposed evaluation techniques [YA06, APY06, AP, ACA + 07]. StatAP is a sample-and-estimate technique defined by the following two choices.</p><p>Stratified Sampling, as developed by Stevens <ref type="bibr" coords="8,327.42,487.26,32.88,9.57" target="#b4">[BH83,</ref><ref type="bibr" coords="8,365.04,487.26,16.97,9.57" target="#b10">Ste]</ref>, is very straightforward for our application. Briefly, it consists of bucketing the documents ordered by a chosen prior distribution and then sampling in two stages: first sample buckets with replacement according to cumulative weight; then sample documents inside each bucket without replacement according to selection at the previous stage.</p><p>Generalized ratio estimator. Given a sample S of judged documents along with inclusion probabilities, in order to estimate average precision, statAP adapts the generalized ratio estimator for unequal probability designs <ref type="bibr" coords="8,222.22,582.10,34.45,9.57" target="#b11">[Tho92]</ref>.(very popular on polls, election strategies, market research etc). For our problem, the population values are precisions at relevant ranks; so for a given query and a particular system determined by ranking r(.) we have (x d denotes the relevance judgment of document d) : </p><formula xml:id="formula_1" coords="8,224.12,644.95,151.82,29.73">statAP = 1 R d∈S x d • prec@r(</formula><formula xml:id="formula_2" coords="9,203.58,209.33,203.15,22.30">R = d∈S x d π d ; prec@k = 1 k d∈S,r(d)≤k x d π d</formula><p>are estimates the total number of relevant documents and precision at rank k, respectively, both using the Horwitz-Thompson unbiased estimator <ref type="bibr" coords="9,308.67,256.16,34.47,9.57" target="#b11">[Tho92]</ref>.</p><p>Confidence intervals. We can compute the inclusion probability for each document (π d ) and also for pairs of documents (π df ); therefore we can calculate an estimate of variance, var(statAP ), from the sample, using the ratio estimator variance formula found in <ref type="bibr" coords="9,413.62,296.81,34.47,9.57" target="#b11">[Tho92]</ref>, pp. 78 (see [AP, ACA + 07] for details). Assuming the set of queries Q is chosen randomly and independently, and taking into account the weighting scheme we are using to compute the final MAP (see Results), we compute an estimator for the MAP variance</p><formula xml:id="formula_3" coords="9,187.40,357.79,237.20,29.64">var(statM AP ) = 1 ( q w q ) 2 q∈Q w 2 q • var(statAP q )</formula><p>where w q is distribution weight proportional with the number of judgments made on query q.</p><p>Assuming normally distributed statM AP values, a 95% confidence interval is given by ±2std or ±2 var(statM AP ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Queries and judgments</head><p>In total we obtained 15,211 judgments for 784 queries. Of these, 747 (5%) were "not relevant but reasonable", 2,001 (13%) were "relevant", and 931 (6%) were "highly relevant"; the rest were not relevant.</p><p>As described above, queries were partitioned into four categories (overlapping) and assigned a target number of judgments to ensure roughly equal judging effort for each category and each target. Table <ref type="table" coords="9,136.99,574.27,5.45,9.57" target="#tab_1">2</ref> shows the distribution of judged documents broken out by category and by judgment target. Multiplying the number of queries by the number of judgments per query shows that there are roughly the same number of judgments for each category and each target. It is true that the number of judgments at each target decreases as the target increases; this is because the probability of collisions increases with the target, from 2.3% (= 100 • 7.81/8) at target 8 to 9.3% (= 100 • 116.08/128) at target 128. Nevertheless, the number of collisions is quite low, suggesting that the two methods are identifying very different sets of documents as important.</p><p>Table <ref type="table" coords="9,120.43,669.11,5.45,9.57" target="#tab_2">3</ref> shows the proportion of documents marked relevant broken out by query category and maximum number of judgments. Note that the "govheavy" categories had a significantly greater proportion of documents judged relevant than the "govslant" categories. The difference between "short" and "long" is not as clear, but we can see from the table that the increase from "short-govslant" to "short-govheavy" is quite a bit larger than the increase from "long-govslant" to "long-govheavy".</p><p>The length of judged documents varied by category. Measuring length by number of characters (a loose measure that also includes HTML tags, Javascript, metadata, and more that would not be visible to the user), documents judged for short queries had an average length of 38,730 characters, while those judged for long queries had an average length of 43,900 characters. There is a smaller difference for govslant and govheavy: an average length of 40,456 characters for the former, and 42,175 for the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Assessor Time</head><p>We measured the elapsed time assessors spent performing various interactions with the judging system. Assessor time was mainly spent in three areas:</p><p>1. selecting a query 2. backfitting the query to a topic definition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">judging documents</head><p>When selecting a query, assessors were allowed to refresh the page to see a new list of 10. They could do this as many times as they wanted. How many times did they refresh? How long did they look at each list of 10? How long did they look at the final list of 10 before selecting one? And then how long did they spend developing it into a topic?</p><p>These numbers are shown in Table <ref type="table" coords="10,254.51,528.54,5.45,9.57">4</ref> along with the time spent on topic definition. Note that all numbers are median seconds-the mean is inappropriate because the distribution is heavily skewed by breaks. On average, the assessors looked at 2.413 pages of queries before selecting one-meaning they picked roughly one out of every 24.13 queries they saw. Three of the assessors looked at about two pages of queries every time (on average). One assessor looked at an average of more than 7 pages of queries! This suggests that assessors were willing to spend some time looking for queries that they felt comfortable with.</p><p>Assessors looked at each page of 10 queries for 22 seconds (median). There was a large amount of variance in this number, with one assessor looking at each page for only 13 seconds, and three looking at each page for about 35 seconds. Before choosing a query, they looked at the last page of 10 queries for 29 seconds (median), slightly longer than the average look. After selecting a query, they took 76 seconds (median) to develop it into a topic. It is possible that the looking time at the last page of queries was slightly longer because they began mentally formulating a topic before clicking on a query. It is very clear from the table that long queries resulted in more time spent selecting a query and defining a topic. This may explain why long queries seemed to produce more stable rankings: the topic definitions may have been more carefully constructed and relevance judgments were more consistent. There is a hint that less time was spent on govheavy queries than govslant.The number of refreshes does not vary a great deal by category. Time to make judgments is also shown in Table <ref type="table" coords="11,326.84,308.17,4.24,9.57">4</ref>. Here too time varied by query type, and somewhat surprisingly, by the total number of judgments. The fact that each judgment was made faster when 128 were requested suggests that assessors may have some "ramp-up" time on a topic, after which they can make judgments much faster, though it may also suggest that they become less careful in their judging over time.</p><p>Did certain types of judgments take longer than others? We calculated the median number of seconds to make a "nonrelevant", "not relevant but reasonable", "relevant" and "highly relevant" judgment. Respectively, it took 10 seconds, 41 seconds, 23 seconds, and 27 seconds. Recall that assessors could reformulate their topic in between judgments. As it turns out, the proportion of "reasonable" judgments that were immediately preceded by a topic reformulation was greater than that for any other type of judgment: 6.2% for reasonable compared to 5.7% for relevant, 5.1% for highly relevant, and 1.9% for nonrelevant. In addition, assessors spent more time on the reformulations that were made before a reasonble judgment than they did for any other type. A working hypothesis is that certain documents compelled assessors to "tighten" their definition of relevance, and that these occurred more often closer to the boundary between relevant and not relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We evaluated each system by MTC and statAP measures over all queries; in addition, each system was evaluated over each category individually and each judgment target individually:</p><p>1. all queries, all judgments 2. queries with [8|16|32|64|128] judgments 3. long/short or govslant/govheavy queries weighted MAP. Both evaluation methods estimate average precision, for each run, for each query. Our overall baseline measurement is mean average precision (MAP); however, since the queries have various number of documents judged, we compute a weighted MAP over all queries for each run r (separately for each evaluation method): wM AP (r) = q AP q (r)w q where w q is proportional with the number of documents judged for query q, and q w q = 1. The weighted MAP formula essentially counts each judgment made uniformly, (or equivalently, each of the 5 level-judgment classes uniformly), because it can be written as the average of straight MAP for each judgment level class c wM AP (r) = 1 5</p><formula xml:id="formula_4" coords="12,271.15,207.82,143.93,33.71">5 j=1 M AP j = 1 5 5 j=1 1 Q j q∈j AP q</formula><p>where M AP j is averaged over all queries with target j (= 2 j+2 target judgments) and Q j is the number of queries at target j. The weighted MAP is a compromise between the TREC ad-hoc tracks setup, where few queries are judged heavily, and the Million Query 2007 track, where many queries were judged lightly.  In the absence of any traditional evaluation baseline (where a lot more documents are judged for each query), the best indication that our rankings are close to the "true" ranking of systems is the correlation of the two evaluation methodologies. Their mechanisms for estimation are fundamentally different, so any correlation is much more likely to be due to correct estimation than any other reason. Figure <ref type="figure" coords="12,175.54,669.11,5.45,9.57" target="#fig_2">1</ref>  The confidence in statAP MAP estimate is given as a 95% confidence interval (column 5); the confidence of MTC (column 6) is the probability that the system's performance is lower than the one of the next system. each evaluation method) with straight MAP vs weighted MAP: for MTC the rankings are correlated with τ = 0.97 and for statAP the correlation is τ = .99 (Figure <ref type="figure" coords="13,419.20,535.72,4.24,9.57" target="#fig_3">2</ref>). We can get a deeper sense of the agreement by breaking queries out into subsets by category and judgment count. The number of queries in each category is roughly the same (see Table <ref type="table" coords="13,393.17,562.81,4.24,9.57" target="#tab_1">2</ref>). MTC and statAP continue to agree well, with τ correlations of 0.93, 0.82, 0.89, and 0.92 for short-govslant, short-govheavy, long-govslant, and long-govheavy, respectively. The two methods continue to agree highly even when evaluated over only the documents they selected (τ = 0.87), and even when evaluated over only the documents selected by the other method (τ = 0.91)-despite very little overlap between the methods. Showing that the two methods agree very highly on the ranking of systems even when evaluating over disjoint query/judgment sets provides very strong evidence that these methods are finding something close to the "true" ranking of systems, and possibly the actual true ranking modulo assessor disagreement. Judgment targets. We calculated the MAP separately for each set of queries judged to a certain target <ref type="bibr" coords="14,140.84,318.86,12.73,9.57">(8,</ref><ref type="bibr" coords="14,157.40,318.86,13.94,9.57">16,</ref><ref type="bibr" coords="14,175.17,318.86,13.94,9.57">32,</ref><ref type="bibr" coords="14,192.93,318.86,13.94,9.57">64,</ref><ref type="bibr" coords="14,210.70,318.86,18.91,9.57">128)</ref>, in order to investigate both stability of the evaluation methodologies and the performance of retrieval systems. The results are presented in Table <ref type="table" coords="14,476.63,332.40,4.24,9.57" target="#tab_4">6</ref>; we provide a detailed analysis in the next section. Due to the differences between the two evaluation technologies, a different number of queries contribute to the MAP for each level, as indicated on the table.</p><p>Clearly, the evaluation methods are slightly biased upwards when the number of document judged is very small (i.e. 8); there is a consistent trend (for middle and good systems) to have the MAP decreasing as the number of judgments increases. One of the conclusions of this study is that a reliable evaluation will need at least a dozen documents judged per query.</p><p>Query categories. Results in the form of weighted MAP are presented in Table <ref type="table" coords="14,468.13,440.80,4.24,9.57" target="#tab_5">7</ref>. Consistently across systems, short and gov-heavy queries are the "easiest" (systems have highest performance on these categories). This may be because there are more relevant documents containing query terms that are therefore easier to find, though that may in turn be because short queries are more open to interpretation and thus produce more relevant documents. According to MTC, the hardest category for systems appears to be the short and gov-slant queries; statAP performance varies across systems without indicating a clear hardest category of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Other measures</head><p>To test whether the other measures do a fair job of ranking systems we correlated system rankings by one measure over a set of queries/judgments to system rankings by another measure over the same set of queries/judgments (Table <ref type="table" coords="14,251.97,598.60,4.24,9.57" target="#tab_6">8</ref>). They tend to correlate very well (τ ≈ .9), suggesting that even though our judgments were acquired to rank systems by MAP estimates, they can be used for other measures reliably. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>The above results are based on a large sample of 784 sparsely-judged queries distributed uniformly over four categories. The next step is to determine the extent to which the number of queries and judgments can be reduced, and how to sample the categories to achieve similar results with less overall effort.</p><p>Our aim is to answer the following question: what is the number of queries needed for different levels of relevance incompleteness to guarantee that, when systems are run over this many queries, their MAP scores reflect their actual performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of variance studies</head><p>When systems are run over different sets of queries, MAP scores (and consequently the ranking of these systems) may vary. The amount of variability that occurs in MAP scores (as measured by variance) across all sets of queries and all systems can be decomposed into three components: (a) variance due to actual performance differences among systems -system variance, (b) variance due to the relative difficulty of a particular set of queries -query set variance, and (c) variance due to the fact that different systems consider different set of queries hard (or easy) -systemquery set interaction variance. The variance due to other sources of variability, such as sampling of documents, is confounded with the later variance component, i.e. variance due to system-query set interaction.</p><p>Ideally, one would like the total variance in MAP scores to be due to the actual performance differences between systems, as opposed to the other two sources of variance. In such a case, having the systems run over different sets of queries would result into each system obtaining identical MAP scores over all sets of queries, and thus MAP scores over a single set of queries would be 100% reliable in evaluating the quality of the systems. The percentage of variance attributed to the system effect is a function of the size of the query set.</p><p>Note that among the three variance components, only the variance due to system and systemquery set interaction affects the ranking of systems -it is these two components that can alter the relative differences among MAP scores, while the query set variance will affect all systems equally, reflecting the overall difficulty of the set of queries.</p><p>In practice, retrieval systems are run over a single given set of queries. The decomposition of the total MAP variance into the aforementioned components in this case can be realized by fitting MTC statAP run Rprec prec@10 prec@30 prec@100 Rprec prec@10 prec@30 prec@100 hedge0 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Stability of wMAP scores and induced rankings</head><p>We ran two separate variance decomposition studies; one over the MAP scores estimated by the MTC method and one over the MAP scores estimated by the statAP method. In both cases systems were run over the same set of 784 queries<ref type="foot" coords="17,302.63,556.59,4.23,6.99" target="#foot_1">2</ref> and each one of the methods utilized all available judgments per query in the estimation of MAP scores. As mentioned before, first, MAP scores were computed over each of the judgment targets separately (MAP j ), and then averaged to produce the final MAP scores (wMAP). The variance in wMAP is a function of the variance of the MAP within each query class and the covariance of the MAP values among query classes. Thus, instead of fitting a single ANOVA model in AP values over all queries [BOZ99, BL07], we used a Multivariate Analysis of Variance (MANOVA) <ref type="bibr" coords="17,503.15,639.84,32.24,9.57" target="#b7">[Bre01]</ref>.</p><p>The variance of MAP within each query class was decomposed into the aforementioned variance component, while the covariance of the MAPs among the query classes was solely attributed to system effects, since the query classes are disjoint.</p><p>For both studies, we report (a) the stability levels of the wMAPs (the ratio of the variance due to system and the total variance) and (b) the stability levels of the systems rankings (the ratio of the variance due to system and the variance components that affect the relative MAP scores, i.e. the ranking of systems), both as a function of the total number of queries in the query set. The results of the two studies are illustrated in Figure <ref type="figure" coords="18,312.57,170.35,4.24,9.57" target="#fig_5">3</ref>.</p><p>The solid lines correspond to stability levels of wMAP values which express how fast (in terms of number of queries) we reach stable wMAP values over different sets of queries of the same size. As the figure indicates, statAP reaches a stability level of 0.95 with a set of 129 queries, while MTC reaches the same level with 204 queries (not observed in the figure). <ref type="foot" coords="18,401.46,222.60,4.23,6.99" target="#foot_2">3</ref>The dashed lines correspond to stability levels of systems ranking which expresses how fast (in terms of number of queries) we reach stable system rankings. MTC reaches a stability level of 0.95 with a set of 83 queries, while statAP reaches the same level with 102 queries.</p><p>These results support the claims that the statAP method, by design, aims to estimate the actual MAP scores of the systems, while the MTC method, by design, aims to infer the proper ranking of systems.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Stability for different levels of relevance incompleteness</head><p>To illustrate how stable the MAPs returned by the two methods are with respect to different levels of relevance incompleteness, we ran ANOVA studies for each one of the query classes separately. Figure <ref type="figure" coords="19,106.12,426.20,5.45,9.57" target="#fig_6">4</ref> demonstrate the stability levels for both methods when 16, 32, 64, and 128 judgments are available, respectively.</p><p>MTC leads to both more stable MAPs and induced rankings than statAP when 16 or 32 relevance judgments are available per query, while the opposite is true when 64 or 128 relevance judgments are available.</p><p>Note that the stability of the MAP scores returned by statAP degrades with the relevance incompleteness, as expected. On the other hand, the opposite is true for MTC. For the estimation of MAP scores, MTC is employing a prior distribution of relevance which is calculated by combining information from all queries, which violates the query independence assumption ANOVA makes. The fewer the relevance judgments, the larger the weight on the prior distribution, and thus the more the assumption is violated. Consequently, MAP scores seem to be more stable than they should.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Stability for different query categories</head><p>Furthermore, to illustrate how stable the wMAP values returned by the two methods are with respect to the different query categories (i.e. short, long, govslant and govheavy), we also ran MANOVA studies for each one of the query categories separately in the same manner as in Subsection 5.1.1. Figure <ref type="figure" coords="19,173.15,665.30,5.45,9.57" target="#fig_7">5</ref>   than for short queries (top plots) and for govslant than for govheavy queries (bottom plots). Furthermore, the stability of MTC results appears to be affected more from the different query categories than the stability of statAP results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Many queries; categories. We put in practice two recently developed evaluation techniques that, unlike standard evaluation, scale easily and allow many more experiments and analyses. We experimented with 25 submitted systems over 10000 natural queries, evaluating 784 of them with only about 15000 judgments.</p><p>We investigated system performance over pre-assigned query categories. There is some evidence that over-sampling some types of queries may result in cheaper (if not substantially more efficient) evaluation. We have presented more such evidence in a separate work [CPK + 09].</p><p>Evaluation stability. The setup also allowed an analysis of evaluation stability with fewer judgments or queries. Using ANOVA, we concluded that MTC (ranking optimized) needs about 83 queries with approximately 1700 total judgments for a reliable ranking, while statAP (score optimized) needs about 129 queries with approximately 2650 total judgments for a reliable estimate of MAP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,378.68,74.43,73.99,9.57;11,100.21,88.38,40.33,9.57;11,156.10,88.38,151.50,9.57;11,333.50,88.38,5.45,9.57;11,359.40,88.38,10.91,9.57;11,390.74,88.38,10.91,9.57;11,422.09,88.38,89.69,9.57;11,100.21,102.33,24.33,9.57;11,168.65,102.33,52.86,9.57;11,252.62,102.33,19.39,9.57;11,288.21,102.33,144.80,9.57;11,450.41,102.33,13.94,9.57;11,492.40,102.33,19.39,9.57;11,100.21,115.88,20.00,9.57;11,168.65,115.88,52.86,9.57;11,252.62,115.88,19.39,9.57;11,288.21,115.88,176.14,9.57;11,492.40,115.88,19.39,9.57;11,100.21,129.82,39.15,9.57;11,168.65,129.82,52.86,9.57;11,252.62,129.82,19.39,9.57;11,288.21,129.82,113.45,9.57;11,419.07,129.82,45.28,9.57;11,492.40,129.82,19.39,9.57;11,100.21,143.37,43.94,9.57;11,168.65,143.37,52.86,9.57;11,252.62,143.37,19.39,9.57;11,288.21,143.37,144.80,9.57;11,450.41,143.37,13.94,9.57;11,492.40,143.37,19.39,9.57;11,100.21,157.32,35.48,9.57;11,168.65,157.32,52.86,9.57;11,252.62,157.32,19.39,9.57;11,288.21,157.32,144.80,9.57;11,450.41,157.32,13.94,9.57;11,492.40,157.32,19.39,9.57;11,72.00,180.32,468.00,9.57;11,72.00,193.87,277.76,9.57"><head></head><label></label><figDesc>Average number of query lists viewed and median seconds spent viewing each list, viewing the final list, and defining the topic for the selected query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,175.24,590.04,261.51,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MTC and statAP weighted-MAP correlation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="14,156.35,272.30,299.31,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MAP VS weighted MAP for each evaluation method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="18,72.00,619.26,468.00,9.57;18,72.00,632.81,162.25,9.57"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Stability level of MAP scores and induced ranking of systems for statAP and MTC as a function of the number of queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="19,72.00,331.96,468.00,9.57;19,72.00,345.50,342.51,9.57"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Stability levels of MAP scores and induced ranking for statAP and MTC as a function of the number of queries for different levels of relevance incompleteness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="20,72.00,388.26,468.00,9.57;20,72.00,401.81,80.12,9.57"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Stability levels of wMAP scores and induced ranking for statAP and MTC for different query categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,644.95,314.69,47.28"><head>Table 2 :</head><label>2</label><figDesc>Number of queries judged per category and target, with number of judgments per query in parentheses.</figDesc><table coords="8,347.47,644.95,39.21,25.64"><row><cell>d)</cell></row><row><cell>π d</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,143.27,74.43,325.46,101.11"><head>Table 3 :</head><label>3</label><figDesc>Percent of documents judged relevant.</figDesc><table coords="10,143.27,74.43,325.46,78.11"><row><cell>category</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>total</cell></row><row><cell>short-govslant</cell><cell cols="6">0.1872 0.1214 0.2028 0.1399 0.0298 0.1459</cell></row><row><cell>long-govslant</cell><cell cols="6">0.2021 0.1702 0.1734 0.1201 0.1369 0.1597</cell></row><row><cell cols="7">short-govheavy 0.2462 0.3081 0.3037 0.2338 0.3739 0.2832</cell></row><row><cell cols="7">long-govheavy 0.2887 0.2039 0.2226 0.1361 0.1600 0.1958</cell></row><row><cell>total</cell><cell cols="6">0.2313 0.1928 0.2251 0.1524 0.1575 0.1928</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,72.00,669.11,468.01,23.12"><head>Table 5 :</head><label>5</label><figDesc>illustrates the weighted MAP scatterplot, with a Kendall τ =.93. It turns out that for this experiment in particular, there is virtually no difference in rankings obtained (for MTC and statAP estimate of MAP. Numbers are weighted averages of AP query estimates.</figDesc><table coords="13,97.34,74.43,417.32,348.70"><row><cell>run</cell><cell cols="5">%unjudged statAP wMAP MTC wMAP statAP CI MTC conf</cell></row><row><cell>hedge0</cell><cell>0.9971</cell><cell>0.0004</cell><cell>0.0041</cell><cell>±0.0003</cell><cell>1.000</cell></row><row><cell>vsmstat07</cell><cell>0.9536</cell><cell>0.0039</cell><cell>0.0062</cell><cell>±0.0078</cell><cell>0.869</cell></row><row><cell>vsmstat</cell><cell>0.9549</cell><cell>0.0043</cell><cell>0.0063</cell><cell>±0.0078</cell><cell>0.911</cell></row><row><cell>lsi150stat</cell><cell>0.9571</cell><cell>0.0025</cell><cell>0.0064</cell><cell>±0.0029</cell><cell>1.000</cell></row><row><cell>sabmq08a1</cell><cell>0.9758</cell><cell>0.0069</cell><cell>0.0094</cell><cell>±0.0128</cell><cell>1.000</cell></row><row><cell>lsi150dyn</cell><cell>0.9684</cell><cell>0.0342</cell><cell>0.0190</cell><cell>±0.0179</cell><cell>1.000</cell></row><row><cell>000cos</cell><cell>0.9596</cell><cell>0.0533</cell><cell>0.0236</cell><cell>±0.0096</cell><cell>0.529</cell></row><row><cell>vsmdyn</cell><cell>0.9642</cell><cell>0.0493</cell><cell>0.0247</cell><cell>±0.0184</cell><cell>1.000</cell></row><row><cell>000tdfLOG</cell><cell>0.9606</cell><cell>0.0896</cell><cell>0.0322</cell><cell>±0.0163</cell><cell>1.000</cell></row><row><cell>000tdfBM25</cell><cell>0.9579</cell><cell>0.1106</cell><cell>0.0373</cell><cell>±0.0148</cell><cell>1.000</cell></row><row><cell>000klabs</cell><cell>0.9446</cell><cell>0.1259</cell><cell>0.0428</cell><cell>±0.0159</cell><cell>1.000</cell></row><row><cell>000okapi</cell><cell>0.9412</cell><cell>0.1443</cell><cell>0.0441</cell><cell>±0.0160</cell><cell>1.000</cell></row><row><cell>sabmq08b1</cell><cell>0.9630</cell><cell>0.1398</cell><cell>0.0487</cell><cell>±0.0200</cell><cell>0.994</cell></row><row><cell>LucDet</cell><cell>0.9399</cell><cell>0.1739</cell><cell>0.0492</cell><cell>±0.0150</cell><cell>1.000</cell></row><row><cell>indriLowMu08</cell><cell>0.9371</cell><cell>0.1861</cell><cell>0.0548</cell><cell>±0.0133</cell><cell>0.997</cell></row><row><cell>mpiimq0801</cell><cell>0.9498</cell><cell>0.1855</cell><cell>0.0549</cell><cell>±0.0204</cell><cell>1.000</cell></row><row><cell>neuMSRF</cell><cell>0.9665</cell><cell>0.1925</cell><cell>0.0583</cell><cell>±0.0198</cell><cell>1.000</cell></row><row><cell>neumslt</cell><cell>0.9517</cell><cell>0.2266</cell><cell>0.0746</cell><cell>±0.0239</cell><cell>1.000</cell></row><row><cell>neustbl</cell><cell>0.9384</cell><cell>0.2268</cell><cell>0.0717</cell><cell>±0.0207</cell><cell>1.000</cell></row><row><cell>dxrun</cell><cell>0.9334</cell><cell>0.2744</cell><cell>0.0798</cell><cell>±0.0191</cell><cell>0.547</cell></row><row><cell>txrun</cell><cell>0.9345</cell><cell>0.2854</cell><cell>0.0802</cell><cell>±0.0200</cell><cell>0.560</cell></row><row><cell>indriQLST08</cell><cell>0.9285</cell><cell>0.2716</cell><cell>0.0803</cell><cell>±0.0191</cell><cell>0.951</cell></row><row><cell>ind25QLnST08</cell><cell>0.9312</cell><cell>0.2810</cell><cell>0.0812</cell><cell>±0.0204</cell><cell>0.583</cell></row><row><cell>LucLpTfS</cell><cell>0.9412</cell><cell>0.2815</cell><cell>0.0823</cell><cell>±0.0224</cell><cell>1.000</cell></row><row><cell>indri25DM08</cell><cell>0.9330</cell><cell>0.3114</cell><cell>0.0849</cell><cell>±0.0210</cell><cell>NA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="15,72.00,74.23,468.01,408.91"><head>Table 6 :</head><label>6</label><figDesc>MTC and statAP estimation of MAP separately for sets of queries judged at 8, 16, 32, 64, 128 level. The column header indicates the number of queries judged at a specific level( for example, for first column, 403 queries with 8 documents judged). MTC outputs an estimate for all queries judged, while statAP estimates only queries with at least one judged relevant document; therefore there are less queries per level of judgment reported for statAP.</figDesc><table coords="15,77.48,74.23,457.07,331.91"><row><cell></cell><cell></cell><cell></cell><cell>MTC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>statAP</cell><cell></cell></row><row><cell>run</cell><cell cols="9">403x8 204x16 102x32 50x64 25x128 271x8 138x16 89x32 43x64 23x128</cell></row><row><cell>hedge0</cell><cell>0.003</cell><cell>0.004</cell><cell>0.004</cell><cell>0.005</cell><cell>0.006</cell><cell>0.002</cell><cell>0.000</cell><cell>0.000 0.000</cell><cell>0.000</cell></row><row><cell>lsi150stat</cell><cell>0.005</cell><cell>0.006</cell><cell>0.006</cell><cell>0.007</cell><cell>0.008</cell><cell>0.004</cell><cell>0.002</cell><cell>0.002 0.001</cell><cell>0.003</cell></row><row><cell>vsmstat</cell><cell>0.005</cell><cell>0.006</cell><cell>0.006</cell><cell>0.007</cell><cell>0.008</cell><cell>0.006</cell><cell>0.000</cell><cell>0.001 0.003</cell><cell>0.010</cell></row><row><cell>vsmstat07</cell><cell>0.005</cell><cell>0.006</cell><cell>0.006</cell><cell>0.007</cell><cell>0.009</cell><cell>0.007</cell><cell>0.000</cell><cell>0.001 0.000</cell><cell>0.010</cell></row><row><cell>sabmq08a1</cell><cell>0.009</cell><cell>0.009</cell><cell>0.010</cell><cell>0.010</cell><cell>0.009</cell><cell>0.010</cell><cell>0.001</cell><cell>0.007 0.011</cell><cell>0.005</cell></row><row><cell>lsi150dyn</cell><cell>0.019</cell><cell>0.022</cell><cell>0.025</cell><cell>0.016</cell><cell>0.011</cell><cell>0.023</cell><cell>0.057</cell><cell>0.044 0.031</cell><cell>0.020</cell></row><row><cell>vsmdyn</cell><cell>0.027</cell><cell>0.026</cell><cell>0.031</cell><cell>0.024</cell><cell>0.016</cell><cell>0.054</cell><cell>0.071</cell><cell>0.058 0.046</cell><cell>0.025</cell></row><row><cell>000cos</cell><cell>0.027</cell><cell>0.024</cell><cell>0.035</cell><cell>0.017</cell><cell>0.014</cell><cell>0.064</cell><cell>0.058</cell><cell>0.081 0.035</cell><cell>0.033</cell></row><row><cell>000tfidfLOG</cell><cell>0.036</cell><cell>0.032</cell><cell>0.038</cell><cell>0.031</cell><cell>0.024</cell><cell>0.119</cell><cell>0.080</cell><cell>0.105 0.103</cell><cell>0.048</cell></row><row><cell>000tfidfBM25</cell><cell>0.042</cell><cell>0.036</cell><cell>0.045</cell><cell>0.033</cell><cell>0.030</cell><cell>0.144</cell><cell>0.095</cell><cell>0.145 0.118</cell><cell>0.057</cell></row><row><cell>000klabs</cell><cell>0.053</cell><cell>0.044</cell><cell>0.053</cell><cell>0.033</cell><cell>0.032</cell><cell>0.161</cell><cell>0.164</cell><cell>0.139 0.097</cell><cell>0.086</cell></row><row><cell>000okapi</cell><cell>0.056</cell><cell>0.045</cell><cell>0.054</cell><cell>0.034</cell><cell>0.031</cell><cell>0.175</cell><cell>0.176</cell><cell>0.156 0.143</cell><cell>0.088</cell></row><row><cell>LucDeflt</cell><cell>0.059</cell><cell>0.049</cell><cell>0.059</cell><cell>0.040</cell><cell>0.038</cell><cell>0.213</cell><cell>0.185</cell><cell>0.197 0.164</cell><cell>0.123</cell></row><row><cell>sabmq08b1</cell><cell>0.061</cell><cell>0.052</cell><cell>0.061</cell><cell>0.036</cell><cell>0.033</cell><cell>0.173</cell><cell>0.202</cell><cell>0.132 0.121</cell><cell>0.094</cell></row><row><cell>indriLowMu08</cell><cell>0.068</cell><cell>0.053</cell><cell>0.063</cell><cell>0.047</cell><cell>0.043</cell><cell>0.246</cell><cell>0.191</cell><cell>0.201 0.195</cell><cell>0.116</cell></row><row><cell>mpiimq0801</cell><cell>0.070</cell><cell>0.057</cell><cell>0.067</cell><cell>0.041</cell><cell>0.040</cell><cell>0.219</cell><cell>0.193</cell><cell>0.207 0.188</cell><cell>0.132</cell></row><row><cell>neuMSRF</cell><cell>0.074</cell><cell>0.061</cell><cell>0.072</cell><cell>0.047</cell><cell>0.038</cell><cell>0.197</cell><cell>0.215</cell><cell>0.243 0.206</cell><cell>0.111</cell></row><row><cell>neustbl</cell><cell>0.091</cell><cell>0.073</cell><cell>0.083</cell><cell>0.056</cell><cell>0.055</cell><cell>0.298</cell><cell>0.254</cell><cell>0.217 0.226</cell><cell>0.164</cell></row><row><cell>neumsfilt</cell><cell>0.094</cell><cell>0.075</cell><cell>0.088</cell><cell>0.058</cell><cell>0.057</cell><cell>0.298</cell><cell>0.214</cell><cell>0.213 0.244</cell><cell>0.180</cell></row><row><cell>dxrun</cell><cell>0.101</cell><cell>0.081</cell><cell>0.094</cell><cell>0.061</cell><cell>0.061</cell><cell>0.297</cell><cell>0.299</cell><cell>0.277 0.305</cell><cell>0.207</cell></row><row><cell>indriQLST08</cell><cell>0.101</cell><cell>0.079</cell><cell>0.093</cell><cell>0.063</cell><cell>0.064</cell><cell>0.297</cell><cell>0.306</cell><cell>0.269 0.300</cell><cell>0.203</cell></row><row><cell>txrun</cell><cell>0.101</cell><cell>0.080</cell><cell>0.097</cell><cell>0.061</cell><cell>0.063</cell><cell>0.301</cell><cell>0.324</cell><cell>0.286 0.305</cell><cell>0.226</cell></row><row><cell>LucLpTfS</cell><cell>0.102</cell><cell>0.081</cell><cell>0.096</cell><cell>0.064</cell><cell>0.068</cell><cell>0.331</cell><cell>0.249</cell><cell>0.295 0.300</cell><cell>0.239</cell></row><row><cell cols="2">ind25QLnST08 0.103</cell><cell>0.081</cell><cell>0.093</cell><cell>0.064</cell><cell>0.064</cell><cell>0.304</cell><cell>0.330</cell><cell>0.261 0.295</cell><cell>0.233</cell></row><row><cell>indri25DM08</cell><cell>0.108</cell><cell>0.085</cell><cell>0.099</cell><cell>0.065</cell><cell>0.067</cell><cell>0.368</cell><cell>0.358</cell><cell>0.281 0.323</cell><cell>0.254</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,72.00,73.91,468.00,332.63"><head>Table 7 :</head><label>7</label><figDesc>MTC and statAP estimation of MAP (weighted w.r.t. numbers of judgments per query) separately for each category set of queries.</figDesc><table coords="16,76.71,73.91,458.59,295.98"><row><cell></cell><cell></cell><cell>MTC</cell><cell></cell><cell></cell><cell></cell><cell>statAP</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">short,heavy long,heavy short,slant long,slant short,heavy long,heavy short,slant long,slant</cell></row><row><cell>run</cell><cell cols="4">197queries 194queries 196queries 197queries</cell><cell cols="4">155queries 158queries 121queries 130queries</cell></row><row><cell>hedge0</cell><cell>0.003</cell><cell>0.003</cell><cell>0.004</cell><cell>0.003</cell><cell>0.000</cell><cell>0.000</cell><cell>0.003</cell><cell>0.000</cell></row><row><cell>vsmstat07</cell><cell>0.005</cell><cell>0.005</cell><cell>0.006</cell><cell>0.005</cell><cell>0.006</cell><cell>0.004</cell><cell>0.000</cell><cell>0.000</cell></row><row><cell>vsmstat</cell><cell>0.005</cell><cell>0.005</cell><cell>0.006</cell><cell>0.005</cell><cell>0.006</cell><cell>0.004</cell><cell>0.000</cell><cell>0.000</cell></row><row><cell>lsi150stat</cell><cell>0.005</cell><cell>0.005</cell><cell>0.006</cell><cell>0.005</cell><cell>0.007</cell><cell>0.001</cell><cell>0.002</cell><cell>0.000</cell></row><row><cell>sabmq08a1</cell><cell>0.009</cell><cell>0.009</cell><cell>0.009</cell><cell>0.009</cell><cell>0.003</cell><cell>0.003</cell><cell>0.002</cell><cell>0.008</cell></row><row><cell>lsi150dyn</cell><cell>0.022</cell><cell>0.020</cell><cell>0.017</cell><cell>0.020</cell><cell>0.037</cell><cell>0.029</cell><cell>0.014</cell><cell>0.060</cell></row><row><cell>vsmdyn</cell><cell>0.031</cell><cell>0.025</cell><cell>0.020</cell><cell>0.024</cell><cell>0.088</cell><cell>0.037</cell><cell>0.026</cell><cell>0.062</cell></row><row><cell>000cos</cell><cell>0.044</cell><cell>0.019</cell><cell>0.020</cell><cell>0.016</cell><cell>0.140</cell><cell>0.019</cell><cell>0.047</cell><cell>0.026</cell></row><row><cell>000tfidfLOG</cell><cell>0.046</cell><cell>0.032</cell><cell>0.028</cell><cell>0.029</cell><cell>0.117</cell><cell>0.076</cell><cell>0.100</cell><cell>0.108</cell></row><row><cell>000tfidfBM25</cell><cell>0.052</cell><cell>0.036</cell><cell>0.033</cell><cell>0.031</cell><cell>0.146</cell><cell>0.099</cell><cell>0.117</cell><cell>0.106</cell></row><row><cell>LucDeflt</cell><cell>0.058</cell><cell>0.054</cell><cell>0.040</cell><cell>0.055</cell><cell>0.229</cell><cell>0.164</cell><cell>0.181</cell><cell>0.188</cell></row><row><cell>000okapi</cell><cell>0.058</cell><cell>0.045</cell><cell>0.040</cell><cell>0.043</cell><cell>0.227</cell><cell>0.121</cell><cell>0.187</cell><cell>0.137</cell></row><row><cell>000klabs</cell><cell>0.062</cell><cell>0.039</cell><cell>0.039</cell><cell>0.038</cell><cell>0.267</cell><cell>0.082</cell><cell>0.167</cell><cell>0.081</cell></row><row><cell>indriLowMu08</cell><cell>0.062</cell><cell>0.063</cell><cell>0.046</cell><cell>0.059</cell><cell>0.253</cell><cell>0.216</cell><cell>0.260</cell><cell>0.213</cell></row><row><cell>neuMSRF</cell><cell>0.077</cell><cell>0.070</cell><cell>0.051</cell><cell>0.063</cell><cell>0.159</cell><cell>0.335</cell><cell>0.197</cell><cell>0.138</cell></row><row><cell>sabmq08b1</cell><cell>0.077</cell><cell>0.052</cell><cell>0.040</cell><cell>0.047</cell><cell>0.248</cell><cell>0.113</cell><cell>0.096</cell><cell>0.109</cell></row><row><cell>mpiimq0801</cell><cell>0.084</cell><cell>0.062</cell><cell>0.044</cell><cell>0.051</cell><cell>0.310</cell><cell>0.133</cell><cell>0.214</cell><cell>0.144</cell></row><row><cell>neustbl</cell><cell>0.104</cell><cell>0.079</cell><cell>0.059</cell><cell>0.071</cell><cell>0.332</cell><cell>0.317</cell><cell>0.193</cell><cell>0.222</cell></row><row><cell>neumsfilt</cell><cell>0.110</cell><cell>0.083</cell><cell>0.063</cell><cell>0.074</cell><cell>0.334</cell><cell>0.325</cell><cell>0.208</cell><cell>0.198</cell></row><row><cell>indriQLST08</cell><cell>0.111</cell><cell>0.094</cell><cell>0.064</cell><cell>0.078</cell><cell>0.302</cell><cell>0.255</cell><cell>0.240</cell><cell>0.273</cell></row><row><cell>ind25QLnST08</cell><cell>0.113</cell><cell>0.095</cell><cell>0.066</cell><cell>0.082</cell><cell>0.294</cell><cell>0.328</cell><cell>0.260</cell><cell>0.264</cell></row><row><cell>dxrun</cell><cell>0.116</cell><cell>0.092</cell><cell>0.067</cell><cell>0.077</cell><cell>0.299</cell><cell>0.326</cell><cell>0.300</cell><cell>0.281</cell></row><row><cell>LucLpTfS</cell><cell>0.116</cell><cell>0.092</cell><cell>0.070</cell><cell>0.082</cell><cell>0.306</cell><cell>0.289</cell><cell>0.237</cell><cell>0.262</cell></row><row><cell>txrun</cell><cell>0.118</cell><cell>0.090</cell><cell>0.068</cell><cell>0.077</cell><cell>0.315</cell><cell>0.327</cell><cell>0.292</cell><cell>0.288</cell></row><row><cell>indri25DM08</cell><cell>0.120</cell><cell>0.098</cell><cell>0.069</cell><cell>0.085</cell><cell>0.369</cell><cell>0.308</cell><cell>0.288</cell><cell>0.292</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,72.00,100.74,468.00,390.87"><head>Table 8 :</head><label>8</label><figDesc>MTC and statAP estimation of other measures. The numbers are weighted averages over queries. Ordered by MTC estimate of prec@10.</figDesc><table coords="17,77.74,100.74,446.91,321.40"><row><cell></cell><cell>048</cell><cell>0.022</cell><cell>0.030</cell><cell>0.055</cell><cell>0.000</cell><cell>0.001</cell><cell>0.001</cell><cell>0.000</cell></row><row><cell>vsmstat07</cell><cell>0.059</cell><cell>0.046</cell><cell>0.053</cell><cell>0.070</cell><cell>0.016</cell><cell>0.040</cell><cell>0.024</cell><cell>0.013</cell></row><row><cell>vsmstat</cell><cell>0.059</cell><cell>0.048</cell><cell>0.054</cell><cell>0.069</cell><cell>0.017</cell><cell>0.045</cell><cell>0.027</cell><cell>0.014</cell></row><row><cell>lsi150stat</cell><cell>0.060</cell><cell>0.050</cell><cell>0.055</cell><cell>0.071</cell><cell>0.009</cell><cell>0.086</cell><cell>0.041</cell><cell>0.014</cell></row><row><cell>sabmq08a1</cell><cell>0.071</cell><cell>0.079</cell><cell>0.080</cell><cell>0.085</cell><cell>0.012</cell><cell>0.034</cell><cell>0.038</cell><cell>0.044</cell></row><row><cell>000cos</cell><cell>0.104</cell><cell>0.127</cell><cell>0.138</cell><cell>0.128</cell><cell>0.114</cell><cell>0.094</cell><cell>0.155</cell><cell>0.109</cell></row><row><cell>lsi150dyn</cell><cell>0.091</cell><cell>0.138</cell><cell>0.127</cell><cell>0.110</cell><cell>0.067</cell><cell>0.135</cell><cell>0.136</cell><cell>0.123</cell></row><row><cell>vsmdyn</cell><cell>0.103</cell><cell>0.163</cell><cell>0.146</cell><cell>0.125</cell><cell>0.093</cell><cell>0.136</cell><cell>0.148</cell><cell>0.126</cell></row><row><cell>000tfidfLOG</cell><cell>0.115</cell><cell>0.206</cell><cell>0.175</cell><cell>0.141</cell><cell>0.166</cell><cell>0.309</cell><cell>0.236</cell><cell>0.169</cell></row><row><cell>000tfidfBM25</cell><cell>0.122</cell><cell>0.222</cell><cell>0.189</cell><cell>0.150</cell><cell>0.194</cell><cell>0.294</cell><cell>0.245</cell><cell>0.190</cell></row><row><cell>000klabs</cell><cell>0.130</cell><cell>0.227</cell><cell>0.202</cell><cell>0.160</cell><cell>0.201</cell><cell>0.249</cell><cell>0.264</cell><cell>0.178</cell></row><row><cell>000okapi</cell><cell>0.133</cell><cell>0.243</cell><cell>0.212</cell><cell>0.163</cell><cell>0.219</cell><cell>0.281</cell><cell>0.287</cell><cell>0.205</cell></row><row><cell>LucDeflt</cell><cell>0.143</cell><cell>0.255</cell><cell>0.221</cell><cell>0.176</cell><cell>0.244</cell><cell>0.331</cell><cell>0.278</cell><cell>0.223</cell></row><row><cell>indriLowMu08</cell><cell>0.146</cell><cell>0.280</cell><cell>0.241</cell><cell>0.181</cell><cell>0.271</cell><cell>0.300</cell><cell>0.290</cell><cell>0.247</cell></row><row><cell>sabmq08b1</cell><cell>0.133</cell><cell>0.282</cell><cell>0.228</cell><cell>0.165</cell><cell>0.202</cell><cell>0.391</cell><cell>0.314</cell><cell>0.281</cell></row><row><cell>mpiimq0801</cell><cell>0.139</cell><cell>0.316</cell><cell>0.248</cell><cell>0.171</cell><cell>0.245</cell><cell>0.384</cell><cell>0.355</cell><cell>0.235</cell></row><row><cell>neuMSRF</cell><cell>0.146</cell><cell>0.328</cell><cell>0.263</cell><cell>0.181</cell><cell>0.261</cell><cell>0.440</cell><cell>0.398</cell><cell>0.261</cell></row><row><cell>neustbl</cell><cell>0.161</cell><cell>0.359</cell><cell>0.289</cell><cell>0.201</cell><cell>0.302</cell><cell>0.417</cell><cell>0.367</cell><cell>0.282</cell></row><row><cell>dxrun</cell><cell>0.170</cell><cell>0.373</cell><cell>0.309</cell><cell>0.212</cell><cell>0.366</cell><cell>0.399</cell><cell>0.398</cell><cell>0.315</cell></row><row><cell>indriQLST08</cell><cell>0.170</cell><cell>0.379</cell><cell>0.309</cell><cell>0.212</cell><cell>0.349</cell><cell>0.431</cell><cell>0.395</cell><cell>0.306</cell></row><row><cell>txrun</cell><cell>0.169</cell><cell>0.382</cell><cell>0.310</cell><cell>0.211</cell><cell>0.373</cell><cell>0.454</cell><cell>0.406</cell><cell>0.311</cell></row><row><cell>neumsfilt</cell><cell>0.161</cell><cell>0.385</cell><cell>0.303</cell><cell>0.200</cell><cell>0.316</cell><cell>0.444</cell><cell>0.402</cell><cell>0.291</cell></row><row><cell cols="2">ind25QLnST08 0.171</cell><cell>0.389</cell><cell>0.311</cell><cell>0.213</cell><cell>0.355</cell><cell>0.448</cell><cell>0.388</cell><cell>0.296</cell></row><row><cell>indri25DM08</cell><cell>0.174</cell><cell>0.398</cell><cell>0.325</cell><cell>0.218</cell><cell>0.390</cell><cell>0.475</cell><cell>0.403</cell><cell>0.341</cell></row><row><cell>LucLpTfS</cell><cell>0.172</cell><cell>0.407</cell><cell>0.322</cell><cell>0.215</cell><cell>0.359</cell><cell>0.506</cell><cell>0.408</cell><cell>0.317</cell></row></table><note coords="17,72.00,482.03,382.91,9.57"><p>an Analysis of Variance (ANOVA) model into AP scores [BOZ99, BL07, Bre01].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="19,88.94,665.30,451.06,23.12"><head></head><label></label><figDesc>demonstrate the stability levels for both methods. Both methods lead faster to stable results (both wMAP values and induced rankings) for long</figDesc><table coords="20,83.61,73.62,438.11,313.06"><row><cell></cell><cell></cell><cell></cell><cell>wMAP on 'Short' Queries</cell><cell></cell><cell></cell><cell>wMAP on 'Long' Queries</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell></row><row><cell cols="2">Stability Level</cell><cell>0.6 0.7 0.8</cell><cell></cell><cell cols="2">Stability Level</cell><cell>0.6 0.7 0.8</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>StatAP Scores</cell><cell></cell><cell></cell><cell>0.5</cell><cell>StatAP Scores</cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell cols="2">StatAP Rankings MTC Scores</cell><cell></cell><cell>0.4</cell><cell>StatAP Rankings MTC Scores</cell></row><row><cell></cell><cell></cell><cell>0 0.3</cell><cell>50 Number of Queries 100 MTC Rankings</cell><cell>150</cell><cell></cell><cell>0 0.3</cell><cell>50 Number of Queries 100 MTC Rankings</cell><cell>150</cell></row><row><cell></cell><cell></cell><cell></cell><cell>wMAP on 'GovSlant' Queries</cell><cell></cell><cell></cell><cell>wMAP on 'GovHeavy' Queries</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell cols="2">0.9</cell><cell></cell><cell></cell><cell cols="2">0.9</cell></row><row><cell>Stability Level</cell><cell cols="2">0.6 0.7 0.8</cell><cell></cell><cell>Stability Level</cell><cell cols="2">0.6 0.7 0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>StatAP Scores</cell><cell></cell><cell></cell><cell>StatAP Scores</cell></row><row><cell></cell><cell cols="2">0.5</cell><cell cols="2">StatAP Rankings MTC Scores</cell><cell cols="2">0.5</cell><cell>StatAP Rankings MTC Scores</cell></row><row><cell></cell><cell cols="2">0 0.4</cell><cell>50 Number of Queries 100 MTC Rankings</cell><cell>150</cell><cell cols="2">0 0.4</cell><cell>50 Number of Queries 100 MTC Rankings</cell><cell>150</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,88.59,681.36,70.43,7.86"><p>http://drupal.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="17,88.59,661.52,451.41,7.86;17,72.00,672.48,285.57,7.86"><p>Note that statAP does not report scores for queries with no relevant document found. Therefore, studies for statAP were only on the 564 queries for which statAP returned scores.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="18,88.59,665.84,431.70,7.86"><p>We have observed in our experiments that a stability of 0.95 leads to a Kendall's tau of approximately 0.9.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> under contract number <rs type="grantNumber">HR0011-06-C-0023</rs>, in part by the <rs type="funder">National Science Foundation (NSF)</rs> under grant numbers <rs type="grantNumber">IIS-0533625</rs> and <rs type="grantNumber">IIS-0534482</rs>, and in part by <rs type="funder">Microsoft Live Labs</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5EGyNkz">
					<idno type="grant-number">HR0011-06-C-0023</idno>
				</org>
				<org type="funding" xml:id="_cuwacNQ">
					<idno type="grant-number">IIS-0533625</idno>
				</org>
				<org type="funding" xml:id="_rK3DwfP">
					<idno type="grant-number">IIS-0534482</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="21,78.74,226.33,20.23,9.57;21,98.97,224.38,6.59,6.99;21,106.05,226.33,433.95,9.57;21,125.45,239.88,414.55,9.57;21,125.45,253.43,75.31,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="21,227.95,239.88,234.54,9.57">Overview of the TREC 2007 Million Query Track</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blagovest</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Dachev</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kanoulas</surname></persName>
		</author>
		<idno>ACA + 07</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,485.63,239.88,54.37,9.57;21,125.45,253.43,40.02,9.57">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,125.45,275.94,414.55,9.57;21,125.45,289.49,133.70,9.57" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="21,295.71,275.94,244.29,9.57;21,125.45,289.49,47.10,9.57">A practical sampling strategy for efficient retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pavlu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>technical report</note>
</biblStruct>

<biblStruct coords="21,125.45,312.01,190.15,9.57;21,337.62,312.01,202.38,9.57;21,125.45,325.56,138.44,9.57;21,285.72,325.56,254.28,9.57;21,125.45,339.11,355.00,9.57" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="21,337.62,312.01,202.38,9.57;21,125.45,325.56,133.72,9.57">A practical sampling strategy for efficient retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pavlu</surname></persName>
		</author>
		<ptr target="http://www.ccs.neu.edu/home/jaa/papers/drafts/statAP.html" />
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
	<note>Working draft available at the following URL</note>
</biblStruct>

<biblStruct coords="21,125.45,361.62,414.56,9.57;21,125.45,375.17,414.55,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="21,383.79,361.62,156.21,9.57;21,125.45,375.17,182.97,9.57">A statistical method for system evaluation using incomplete judgments</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,332.01,375.17,99.34,9.57">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,125.45,397.69,414.54,9.57;21,125.45,411.24,54.27,9.57" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="21,285.20,397.69,175.11,9.57">Sampling With Unequal Probabilities</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R W</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hanif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,125.45,433.75,414.54,9.57;21,125.45,447.30,137.06,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="21,250.46,433.75,198.11,9.57">Test theory for assessing ir test collection</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bodoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,472.54,433.75,67.46,9.57;21,125.45,447.30,28.31,9.57">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,125.45,469.82,414.55,9.57;21,125.45,483.37,200.70,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="21,346.80,469.82,193.20,9.57;21,125.45,483.37,54.58,9.57">Blind men and elephants: Six approaches to trec data</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nien-Fan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,189.71,483.37,43.45,9.57">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="34" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,125.44,505.88,377.10,9.57" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brennan</surname></persName>
		</author>
		<title level="m" coord="21,224.13,505.88,108.87,9.57">Generalizability Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,125.45,528.40,414.55,9.57;21,125.45,541.95,321.24,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="21,408.24,528.40,131.75,9.57;21,125.45,541.95,89.89,9.57">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><forename type="middle">K</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,238.53,541.95,99.41,9.57">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,72.00,564.46,26.82,9.57;21,98.82,562.51,6.59,6.99;21,105.90,564.46,434.10,9.57;21,125.45,578.01,405.21,9.57;21,72.00,600.53,26.82,9.57;21,98.82,598.58,6.59,6.99;21,105.90,600.53,434.10,9.57;21,125.45,614.08,343.46,9.57" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="21,125.45,578.01,174.06,9.57;21,125.45,614.08,115.87,9.57">Evaluation over thousands of queries</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam ; Ben Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Allan</surname></persName>
		</author>
		<idno>CPK + 09</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,322.50,578.01,99.41,9.57;21,264.32,614.08,95.42,9.57">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2009</date>
			<biblScope unit="page" from="288" to="300" />
		</imprint>
	</monogr>
	<note>Proceedings of ECIR</note>
</biblStruct>

<biblStruct coords="21,125.45,636.59,414.56,9.57;21,125.45,650.14,414.55,9.57;21,125.45,663.69,97.26,9.57" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="21,206.01,636.59,329.99,9.57">Sampling without replacement with probability proportional to size</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">L</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,125.45,650.14,327.09,9.57">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="397" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,125.45,75.51,414.55,9.57;22,125.45,89.06,47.93,9.57" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sampling</surname></persName>
		</author>
		<title level="m" coord="22,285.74,75.51,254.27,9.57;22,125.45,89.06,15.56,9.57">Wiley Series in Probability and Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,125.45,111.57,414.55,9.57;22,125.45,125.12,326.57,9.57" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="22,301.70,111.57,238.30,9.57;22,125.45,125.12,95.48,9.57">Estimating average precision with incomplete and imperfect judgments</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,244.62,125.12,97.67,9.57">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
