<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.40,71.96,328.81,16.59">Overview of the TREC-2008 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.60,116.82,55.31,11.06"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.92,116.82,90.05,11.06"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.32,116.82,64.88,11.06"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
							<affiliation key="aff1">
								<orgName type="institution">NIST Gaithersburg</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.40,71.96,328.81,16.59">Overview of the TREC-2008 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">473F9C6CD17A3833AD6E609B49A03472</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Blog track explores the information seeking behaviour in the blogosphere. The track was introduced in 2006 <ref type="bibr" coords="1,223.74,233.52,9.51,8.07" target="#b1">[1]</ref>, with a main pilot search task, namely the opinion-finding task. In TREC 2007 <ref type="bibr" coords="1,280.20,243.96,9.51,8.07" target="#b2">[2]</ref>, the track investigated two main tasks inspired by the analysis of a commercial blog-search query log: the opinion-finding task (i.e. "What do people think about X?") and the blog distillation task (i.e. "Find me a blog with a principal, recurring interest in X."). In addition, the Blog 2007 track investigated a natural extension to the opinion-finding task, namely the polarity task (i.e. "Find me positive or negative opinionated posts about X."). All tasks thus far investigated in the Blog track have used the so-called Blogs06 collection, which was created by the University of Glasgow <ref type="bibr" coords="1,280.00,338.04,9.66,8.07" target="#b3">[3]</ref>. The Blogs06 collection was crawled over an 11-week period from 6th December 2005 until the 21st February 2006. The collection is 148GB in size, consisting of 38.6GB of feeds, 88.8GB of permalink documents, and 28.8GB of homepages.</p><p>For TREC 2008, the track continued using the Blogs06 collection. It also continued investigating the opinion-finding, polarity, and blog distillation tasks. In addition, the Blog track 2008 introduced a baseline blog post retrieval task (i.e. "Find me blog posts about X."), to encourage participants to study the impact of their opinion-finding techniques across different underlying topicrelevance baselines. As a consequence, following our conclusions from both the TREC 2006 and the Blog 2007 tracks, we structured the Blog track 2008 around four tasks:</p><p>(1) Baseline adhoc (blog post) retrieval task;</p><p>(2) Opinion-finding (blog post) retrieval task;</p><p>(3) Polarity opinion-finding (blog post) retrieval task; and (4) Blog (feed) distillation task.</p><p>The track has seen an increased level of participation over the years from 17 groups in 2006, to 24 groups in 2007 (20 participants in the opinion-finding task, 11 in the polarity task, and 9 in the blog distillation task). In TREC 2008, 20 groups submitted runs to the baseline task, 19 groups submitted runs to the opinion-finding task, 16 groups submitted runs to the polarity task, and 12 groups submitted runs to the blog distillation task.</p><p>The remainder of this paper is structured as follows. Section 2 describes the baseline and opinion-finding tasks, providing an overview of the submitted runs, as well as a summary of the main effective techniques used by the participating groups. Section 3 describes the polarity task, and the main obtained results by the participating groups. Section 4 describes the blog search (blog distillation) task, and summarises the results of the runs and the main effective approaches deployed by the participating groups. We provide concluding remarks in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASELINE AND OPINION-FINDING TASKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tasks and Topics</head><p>The opinion-finding task addresses a search scenario where a user aims to uncover what the bloggers are saying about X. Roughly speaking, the user's intention is to "take the pulse of the blogosphere" on a topic X. The task has been running in TREC since the Blog track inception in 2006 <ref type="bibr" coords="1,442.62,303.23,9.51,8.07" target="#b1">[1]</ref>. One of the lessons learnt from Blog tracks of TREC 2006 &amp; TREC 2007 is that a good performance in opinion-finding is strongly dominated by the underlying document ranking performance (topic-relevance baseline), where the system's aim is to retrieve as many relevant documents as possible regardless of their opinionated nature <ref type="bibr" coords="1,499.04,355.55,9.68,8.07" target="#b1">[1,</ref><ref type="bibr" coords="1,511.88,355.55,6.45,8.07" target="#b2">2]</ref>. In addition, while some participants were able to show a marked increase in performance when using opinion detection features on top of good topic-relevance baselines, other groups did not manage to improve their baselines. In a recent study, we showed that some stronger topic-relevance baselines could not be improved even by applying the most effective opinion-finding approaches proposed in TREC 2007 <ref type="bibr" coords="1,371.44,428.75,9.51,8.07">[6]</ref>.</p><p>As a consequence, to allow the further study of the performance of a specific opinion-finding technique across a range of different topic-relevance baseline systems, we introduced a two-stage submission procedure for the opinion-finding task. In the first stage (baseline adhoc retrieval task), the participating groups were asked to submit their topic-relevance baselines. Five submitted topicrelevance baseline runs were then selected by TREC as the "standard baselines" and made available to the participating groups. These standard baselines use a variety of different retrieval approaches, and have varying retrieval effectiveness. More specifically, they were selected based on their high topic-relevance and opinion-finding performances on the TREC 2006 and TREC 2007 old topics. Table <ref type="table" coords="1,338.76,564.71,4.48,8.07" target="#tab_0">1</ref> summarises the five provided standard baseline runs.</p><p>In the second phase (opinion-finding retrieval task), the participating groups were encouraged to apply their opinion-finding techniques on their own baselines and on as many standard baselines as possible. The idea was to provide the participating groups with an experimental setting where they could assess the impact of their opinion-finding techniques across a range of different topic-relevance baselines or independently of their own baselines. Through this experiment, the Blog track 2008 also aimed to draw a better understanding of the most effective and stable opinion-finding techniques, by observing their performances on common standard topic-relevance baselines.</p><p>This experiment was made possible by the fact that most of the participating groups in both TREC 2006 and 2007 approached the opinion-finding task as a re-ranking problem <ref type="bibr" coords="1,481.74,711.23,9.68,8.07" target="#b1">[1,</ref><ref type="bibr" coords="1,494.34,711.23,6.68,8.07" target="#b2">2,</ref><ref type="bibr" coords="1,503.82,711.23,6.45,8.07" target="#b5">5]</ref> stage, a group's system aims to find as many relevant documents as possible, regardless of their opinionated nature, while in the second stage, the system re-ranks those documents using some opinion detection techniques, and an appropriate combination of scores.</p><p>For those participating groups that could not separate the topicrelevance and opinion-finding components, the submission guidelines were flexible enough to allow these groups to submit runs without the requirement of specifying a baseline run.</p><p>Since the commercial query logs used in TREC 2006 and 2007 have been running out of workable topics, for TREC 2008, the assessors were asked to create 50 new topics using the query logs as a source, but also by following their own ideas when browsing the collection. Groups were asked to submit their runs using the 50 new topics, as well as the 100 queries from the TREC 2006 and 2007 opinion-finding tasks. The idea was to draw conclusions about the difficulty of the query topics across the Blog track years as well as to provide the participating groups with an experimental setting allowing them to evaluate their training methods and re-ranking functions. In fact, our study in <ref type="bibr" coords="2,167.21,342.00,10.43,8.07">[6]</ref> shows that it is often necessary to train the used re-ranking function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Assessments and Pools</head><p>Each submitted run consisted of the top 1000 retrieved documents for each topic. The retrieval units are the documents from the permalinks component of the Blogs06 test collection. The content of a blog post is defined as the content of the post itself and the contents of all comments to the post: if the relevant content is in a comment, then the permalink is declared to be relevant. We used the same assessment procedure as defined in the TREC Blog tracks 2006 and 2007 <ref type="bibr" coords="2,110.40,460.08,9.68,8.07" target="#b1">[1,</ref><ref type="bibr" coords="2,122.88,460.08,6.45,8.07" target="#b2">2]</ref>. In particular, the assessment procedure had two levels. The first level assesses whether or not a given blog post, i.e. a permalink, contains information about the target and is therefore relevant. The second level assesses the opinionated nature of the blog post if it was deemed relevant in the first assessment level. The relevance assessments were conducted by NIST.</p><p>Groups were allowed to submit at most 2 baseline runs, including a compulsory automatic title-only run, and up to 4 opinionfinding runs using their own baselines, again including a compulsory automatic title-only run. In addition, groups could submit up to 4 runs using each of the 5 provided standard baselines. Hence, each group could submit up to 24 opinion-finding runs. TREC received 41 baseline runs from 20 groups, and 191 opinion-finding runs from 19 groups. Of the 191 submitted opinion-finding runs, all but two runs were automatic: run prisbm (baseline run) and run prisom1 (opinion-finding), which were both manual runs by the BUPT pris group. Among the opinion-finding runs, 130 runs used one of the provided standard baselines, 12 runs had N/A for the baseline (i.e. their system does not separate topic-relevance from opinion-finding), and the other 49 runs used a baseline run from the corresponding group. For the 130 runs using one of the standard baselines, Table <ref type="table" coords="2,148.10,679.80,4.48,8.07" target="#tab_2">2</ref> shows the number of runs using each baseline type, including the breakdown per standard baseline. The baseline, opinion-finding, and polarity tasks shared the same pool. NIST pooled the top 100 documents of two opinion-finding and   Table <ref type="table" coords="2,348.48,368.52,4.48,8.07" target="#tab_3">3</ref> shows a breakdown of the average pool size per topic, and the distribution of relevance assessment levels over the three years of the Blog track opinion-finding task. It is of note that the TREC 2006 pool had the largest size. On the other hand, the TREC 2007 topics were the least opinionated. Table <ref type="table" coords="2,481.22,410.40,4.48,8.07" target="#tab_3">3</ref> also shows that, on average, each of the three pools had roughly an equal number of negative and mixed opinionated documents, but slightly more positive opinionated documents, suggesting that, overall, bloggers had more positive opinions about the topics tackled by the three years of the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>The baseline and opinion-finding tasks are adhoc-like retrieval tasks. Therefore, the primary measure for evaluating the retrieval performance of the participating groups is the mean average precision (MAP). Other metrics used for the baseline and opinionfinding tasks are R-Precision (R-Prec), binary Preference (bPref), and Precision at 10 documents (P@10).</p><p>Table <ref type="table" coords="2,348.12,557.16,4.48,8.07" target="#tab_4">4</ref> provides the average best, median, and worst MAP and P@10 measures for each topic, across all submitted 41 baseline runs. Table <ref type="table" coords="2,365.06,578.04,4.48,8.07" target="#tab_5">5</ref> provides the same measures across all submitted 191 opinion-finding runs. Note that the medians are calculated using the "lower medians" and using only the submitted runs for the given task <ref type="foot" coords="2,367.92,607.68,2.99,5.38" target="#foot_0">1</ref> . In particular, it is of interest to note that the retrieval performances of the systems on the TREC 2006 topics were markedly lower than those obtained on the TREC 2007 and TREC 2008 topics both in terms of topic-relevance and opinion-finding, using both the MAP and P@10 evaluation measures. This suggests that the 2006 topics were slightly more difficult than those used in TREC 2007 and 2008. On the other hand, on average, the performances of the participating groups on the TREC 2007 topics dataset were markedly higher than those reported last year for the same dataset <ref type="bibr" coords="3,115.92,109.92,9.51,8.07" target="#b2">[2]</ref>. However, it is unclear whether this is due to the deployed systems having better retrieval approaches or to intensive training. Nevertheless, it is of note that the performances of the participating groups on the unseen TREC 2008 queries were higher than those observed in TREC 2007, while being overall comparable to the performances of the same (trained) systems on the TREC 2007 dataset. This might suggest that the TREC 2008 topics are the easiest. Table <ref type="table" coords="3,104.52,183.12,4.48,8.07" target="#tab_6">6</ref> provides the average best, median, and worst MAP and P@10 measures for each topic, across all 2006-2008 years (150 topics), for all submitted 41 baseline and 191 opinion-finding runs.</p><p>In the following, to limit the influence of the training that some participating groups might have performed on the TREC 2006 and TREC 2007 topics, we only present the results corresponding to the 50 TREC 2008 unseen queries. Table <ref type="table" coords="3,204.42,245.87,4.48,8.07" target="#tab_7">7</ref> shows the best-scoring baseline title-only automatic run for each group in terms of topicrelevance MAP, and sorted in decreasing order. R-Prec, bPref and P@10 measures are also reported. Table <ref type="table" coords="3,201.48,277.32,4.48,8.07" target="#tab_10">8</ref> shows the best baseline run from each group, in terms of topic-relevance MAP, regardless of the topic length used. All top ranked runs are title-only runs but one.</p><p>The top ranked group, KLE, deployed a passage-based retrieval language modelling approach. Other groups, such as UAms and UoGtr, used collection enrichment, by applying query expansion on external news corpora. In addition, UAms's run included the use of document priors based on credibility indicators such as spelling and capitalisation. UoGtr's run applied a Divergence From Randomness (DFR) term dependency model to boost documents where query terms appear in close proximity. The UIC group used a concept-based information retrieval system and phrasal search. The UniNE group merged two title-only runs based on a 2-Word indexing strategy: one run applies query expansion, while the second applies collection enrichment using Wikipedia. Tables <ref type="table" coords="3,250.10,434.27,4.48,8.07" target="#tab_7">7</ref> and<ref type="table" coords="3,271.80,434.27,4.48,8.07" target="#tab_10">8</ref> also report the opinion-finding MAP measures for these baseline runs.</p><p>It is of note that the overall rankings of the 41 baseline systems on either the opinion-finding or topic-relevance measures are very similar, as stressed by the obtained high correlation coefficients, namely Spearman's ρ = 0.9934 and Kendall's τ = 0.9488.</p><p>In TREC 2008, the participating groups were encouraged to apply their opinion-finding techniques on top of their own baselines, as well as on as many of the provided five standard baselines as possible. Table <ref type="table" coords="3,114.00,528.35,4.48,8.07" target="#tab_11">9</ref> shows the best-scoring opinion-finding run for each group in terms of opinion-finding MAP, regardless of the used baseline and the query type. Other metrics reported are R-Precision (R-Prec), binary Preference (bPref), and Precision at 10 (P@10). In the table, we also compare the opinion-finding MAP performance of the run to the opinion-finding MAP performance achieved by its underlying topic-relevance baseline. A relative MAP increase in performance indicates that the used opinion-finding features were useful. A relative MAP decrease in performance indicates that the deployed opinion-finding features did not help in retrieval (see column ∆ MAP). It is interesting to note that the best two runs used a system that does not clearly separate the topic-relevance and the opinion-finding components. Table <ref type="table" coords="3,221.76,653.87,8.92,8.07" target="#tab_12">10</ref> shows the bestscoring opinion-finding run for each group in terms of opinionfinding MAP, when the group used one of its own submitted baseline runs, regardless of the query type.</p><p>Tables <ref type="table" coords="3,87.72,695.75,4.48,8.07" target="#tab_11">9</ref> and<ref type="table" coords="3,108.60,695.75,8.92,8.07" target="#tab_12">10</ref> show that several groups managed to improve the opinion-finding performance of their underlying topic-relevance base- line. However, the improvements are rather slim, especially when the used topic-relevance baseline is strong enough (e.g. run uams08nlolsp using the strongly performing baseline run uamso8n1o1).</p><p>On the other hand, run DUTIR08BRun4, which led to the highest improvement over the used baseline (31.60%), did not use the best baseline submitted by the corresponding group (see Tables <ref type="table" coords="3,526.14,366.00,4.48,8.07" target="#tab_7">7</ref> &amp;<ref type="table" coords="3,541.68,366.00,7.06,8.07" target="#tab_12">10</ref>). Among the five provided standard baselines, baseline4, (run KLE-PsgFeedTD), which used title and description topics, had the highest topic-relevance and opinion-finding MAP on the 50 new TREC 2008 queries. Table <ref type="table" coords="3,394.22,407.88,8.92,8.07" target="#tab_13">11</ref> shows the median of the opinion-finding runs using each of the standard baselines. According to Table <ref type="table" coords="3,549.24,418.32,3.34,8.07" target="#tab_2">2</ref>, it is also the most frequently used one among the provided standard baselines. Table <ref type="table" coords="3,393.86,439.20,8.92,8.07" target="#tab_14">12</ref> shows the best performing opinion-finding run from each group, if and when the corresponding system used baseline4 as the baseline. In fact, putting apart those two runs that used a system that cannot separate the topic-relevance and opinionfinding components, the top 4 best runs in Table <ref type="table" coords="3,486.86,481.07,4.48,8.07" target="#tab_11">9</ref> all used baseline4 as their underlying baseline. This observation is further emphasised in Figure <ref type="figure" coords="3,354.02,501.95,3.34,8.07" target="#fig_0">1</ref>. For each opinion-finding task run using a standard baseline run, the figure shows how the opinion-finding MAP relates to the opinion-finding MAP of the corresponding baseline run. Indeed, most of the top runs used baseline4. However, there were also some approaches which did not perform well using this baseline.</p><p>Furthermore, we investigated the extent to which a given opinionfinding technique improved the opinion-finding MAP of all the 5 provided standard baselines. The more an opinion-finding technique consistently improves the opinion-finding retrieval performance of the 5 provided baselines, the more likely that it is effective. For a fair comparison of the opinion-finding techniques, we only considered the groups who attempted their opinion-finding techniques on all 5 provided standard baselines. Overall, 21 sets of runs using all five standard baselines were submitted by 8 groups. Table <ref type="table" coords="3,340.20,648.47,8.92,8.07" target="#tab_15">13</ref> shows the best opinion-finding approach from each of the 8 groups, ranked by the mean of their relative improvements over the five standard baselines (see column Mean ∆ MAP). The mean of the opinion-finding performance of the corresponding run on the five standard baselines is also reported (see column Mean MAP). Table <ref type="table" coords="3,366.45,700.67,8.92,8.07" target="#tab_15">13</ref> shows that only three groups had opinion-finding approaches that seem to be effective across the five standard base-  lines: UIC IR Group, KLE and UoGtr. Interestingly, from Table 13, we observe that Mean ∆ MAP and Mean MAP are correlated, indicating that those opinion-finding techniques which on average do best are also the most stable across all five standard baselines.</p><p>Finally, for the 191 submitted opinion-finding runs, we computed the correlation between the opinion-finding MAP, and the topic-relevance MAP. The overall rankings of systems on both opinion-finding and topic-relevance measures are very similar, as stressed by the obtained high correlations, namely, Spearman's ρ=0.9862 and Kendall's τ =0.9054. Figure <ref type="figure" coords="4,172.10,553.92,4.48,8.07" target="#fig_1">2</ref> shows a scatter plot of opinionfinding MAP against topic-relevance MAP, which confirms that the correlation is very high. Overall, similar to previous years, a good performance on the opinion-finding task is strongly dominated by a good performance on the underlying document retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Participants Approaches</head><p>All the participating groups only indexed the permalinks component of the Blogs06 collection, with the exception of the THUIR group, which experimented with two indices: one based on the permalinks component of the Blogs06 collection and, for two submitted runs only, with an index based on both the permalinks and feeds components of the collection.</p><p>In terms of opinion-finding approaches, similar to the general trend in TREC 2006 and 2007 <ref type="bibr" coords="4,168.66,700.68,9.68,8.07" target="#b1">[1,</ref><ref type="bibr" coords="4,181.50,700.68,6.45,8.07" target="#b2">2]</ref>, most of the submitted runs used a two-stage approach, where an initial set of relevant but not necessarily opinionated documents are re-ranked by taking into account various document opinion features. Only 12 runs out of the submitted 191 runs did not adopt this strategy, instead deploying a system that does not separate the topic-relevance component from the opinion-finding features.</p><p>We focus on those three opinion-finding approaches that were consistently effective across the five provided baselines as shown in Table <ref type="table" coords="4,347.28,312.96,7.41,8.07" target="#tab_15">13</ref>. The approach uicop1bl1r, deployed by UIC IR Group, achieved the best average opinion-finding improvements over the five standard topic-relevance baselines (an average of 11.76% improvement). The UIC IR Group's opinion identification component uses an SVM classifier to distinguish subjective texts from objective texts, and determines whether each opinion in the subjective text is related to the query. Its effectiveness is enhanced by a concept abbreviation component, which attempts to recognize abbreviated query concepts in the vicinity of an opinion. The approach B1PsgOpinAZN, from the KLE group, used a lexiconbased approach. The opinion score of a given term is estimated using SentiWordNet and the Amazon review data. The opinionated level of a blog post is defined as the sum of opinion scores of terms within the post. The scores are normalised to take into account the length of the blog post. The KLE group used the Okapi's length normalisation component of BM25. Finally, the uogOP1PrintL opinion-finding approach, deployed by the UoGtr group, confirmed its effectiveness in the TREC Blog track 2007, by improving the opinion-finding performance of the five provided standard baselines by an average of 5.21%. Moreover, the UoGtr group enhanced their TREC 2007 dictionary-based approach <ref type="bibr" coords="4,545.17,522.12,10.78,8.07" target="#b8">[8]</ref> by automatically building an internal opinion dictionary from the collection itself. This approach measures the opinionated discriminability of each term in the dictionary using an information theoretic divergence measure based on the relevance assessments of previous opinion-finding tasks. In addition, UoGtr experimented with a novel method to measure the informativeness of the query terms occurring in a close proximity to opinionated sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">POLARITY TASK</head><p>One of the conclusions from the TREC 2007 Blog track is that the polarity detection task should be a more integral part of the opinion-finding task <ref type="bibr" coords="4,392.40,669.35,9.51,8.07" target="#b2">[2]</ref>. In particular, instead of being defined as a classification task where the system merely identifies the opinion direction (positive, negative, or mixed) of a blog post, the task has been redefined to simulate a user search scenario where the system would retrieve both the positive and negative opinionated documents, categorised in the user display 2 . Evaluation can then be carried out in a more straightforward adhoc document-ranking manner (e.g., using MAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topics and Assessment</head><p>The polarity task shared the same topics as the opinion-finding task. The participating groups were also asked to use all 150 topics: the 50 new topics, as well as the 100 queries from the TREC 2006 and TREC 2007 opinion-finding tasks. In particular, for each topic, a participating system should retrieve and rank all the positive opinionated documents. Then, for each topic, the system should retrieve and rank all the negative opinionated documents. To minimise the number of submitted run files, the groups were asked to concatenate the two runs together in one run file, separated by a blank line. We also required that mixed opinionated documents, i.e. documents containing both positive and negative opinions, should not be listed in the positive (resp. negative) rankings of retrieved documents. The polarity runs were assessed using the same pool described in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Main Approaches</head><p>In a similar vein to the opinion-finding task, the groups were permitted to submit up to 2 runs to the polarity task, using their own previously submitted baseline runs. A compulsory automatic titleonly run was required. In addition, they could submit up to 2 runs using each of the five provided standard topic-relevance baselines (see Table <ref type="table" coords="5,93.96,680.76,3.23,8.07" target="#tab_0">1</ref>). As a consequence, each group could submit up to 12 polarity runs. TREC received a total of 87 polarity runs from 2 The Opinmind.com search engine used to do this. Table <ref type="table" coords="5,340.32,477.36,7.91,8.07" target="#tab_4">14</ref>: Breakdown of the submitted polarity runs using one of the provided five standard baselines.</p><p>16 groups. Of these runs, 59 used one of the five standard provided baselines, 5 runs had N/A for the baseline (i.e. again, their system did not necessarily separate topic-relevance from polarity detection), and the remaining 23 runs used a baseline run from the corresponding group. For the 59 runs using one of the standard baselines, Table <ref type="table" coords="5,393.98,575.16,8.92,8.07" target="#tab_4">14</ref> shows the breakdown of runs per standard baseline. Similar to the opinion-finding task, baseline4, which has the highest topic-relevance effectiveness among the provided five standard baselines, was the most frequently used provided baseline. All the submitted runs were automatic runs. For the provided topics, each submitted run consisted of the top 1000 retrieved positive opinionated permalink documents for each topic, followed by the top 1000 retrieved negative opinionated permalink documents for each topic. First, we assessed the effectiveness of the 41 submitted baselines in finding positive (pos) and negative (neg) polarised opinions. Moreover, to have an overall retrieval performance for each run, we compute the mean of the positive and negative measures for each run, denoted Mix (e.g. Mix MAP).   the average best, median, and worst MAP and P@10 measures for each topic, across all submitted 41 baseline runs. In particular, we observed that the retrieval performance of the systems on the TREC 2007 topics was markedly higher than those obtained on the TREC 2006 and TREC 2008 topics when for positive opinionated documents, using both MAP and P@10. In contrast, the retrieval performance of the systems on the TREC 2008 topics was higher than those obtained on the TREC 2006 and TREC 2007 topics when searching for negative opinionated documents using both MAP and P@10 evaluation measures. Overall, there is no clear evidence that the three topic sets have different difficulty levels. Table <ref type="table" coords="8,93.48,172.68,8.92,8.07" target="#tab_18">16</ref> provides the average best, median, and worst MAP and P@10 measures for each topic across all 87 submitted polarity runs. The TREC 2007 topic set appeared to be the easiest for the retrieval of positive opinionated documents, while the three topic sets showed the same level of difficulty when searching for negative opinionated documents. Table <ref type="table" coords="8,186.84,225.00,8.92,8.07" target="#tab_20">17</ref> provides the average best, median, and worst MAP and P@10 measures for each topic, across all 2006-2008 years (150 topics), for all submitted 41 baseline and 87 polarity runs. Similar to the opinion-finding task, to avoid any bias towards old topics, in the following, we focus on the performances of the submitted 87 polarity runs on the 50 new TREC 2008 unseen queries. Using MAP, each run is evaluated in terms of its ability to rank positive (resp. negative) opinionated permalinks higher up in the ranking. In order to have an overall performance for each run, we compute the mean of the positive and negative MAPs of each run (denoted Mix MAP), and rank them accordingly. Regardless of the used baseline and the query type, Table <ref type="table" coords="8,199.22,350.51,8.92,8.07" target="#tab_21">18</ref> shows the best-scoring polarity run for each group in terms of the mean of the positive and negative opinion-finding MAPs of each run (i.e. Mix MAP), sorted in decreasing order. The P@10 measure is also reported. When applicable, the table also compares the Mix MAP of the run to the Mix MAP achieved by its underlying topic-relevance baseline (denoted Mix ∆ MAP in the table). A relative increase in performance indicates that the used polarity detection features were useful. However, in most cases, we observe a relative decrease in performance, suggesting that most of the deployed polarity techniques by the participating groups were not successful. Actually, this is also apparent from Tables <ref type="table" coords="8,173.20,465.59,8.92,8.07" target="#tab_17">15</ref> and<ref type="table" coords="8,199.96,465.59,8.92,8.07" target="#tab_18">16</ref> where, on average, the submitted baseline systems achieved a higher polarity effectiveness than the submitted polarity runs. Table <ref type="table" coords="8,198.52,486.47,8.92,8.07" target="#tab_22">19</ref> shows the best-scoring polarity run for each group in terms of Mix MAP, when the group used one of its own submitted baseline runs, regardless of the query type. We observe the same trends, namely that most of the participating systems did not improve the polarity finding effectiveness of the underlying baselines. Overall, we conclude that similar to TREC 2007 <ref type="bibr" coords="8,99.76,549.23,9.51,8.07" target="#b2">[2]</ref>, the polarity search task appears to be a challenge to most participating systems.</p><p>Similar to the analysis performed in Section 2.3, to see the most effective and stable polarity opinion detection techniques, we investigated the extent to which a given polarity opinion finding technique improved the polarity finding MAP of all the five provided standard baselines. Overall, 10 sets of runs using all five standard baselines were submitted by 8 groups. Table <ref type="table" coords="8,226.24,622.55,8.92,8.07" target="#tab_23">20</ref> shows the median of their improvements over each standard baseline. Table <ref type="table" coords="8,283.92,632.99,8.92,8.07" target="#tab_2">21</ref> shows the best polarity approach from each of the 8 groups, ranked by the mean of their relative improvements over the five standard baselines, taking into account both their positive and negative polarity opinion retrieval (see column Mean Mix ∆ MAP). Only the approach by the KLE group had on average improved the polarity performance of the five provided runs, followed by the approach by the UoGtr group, albeit to some less extent. Both groups used a straightforward extension to their opinion-finding approaches. Indeed, similar to opinion-finding retrieval, the KLE system calculated a positive/negative score of a blog post using the Amazon Review data, while the UoGtr group extended their dictionary-based approach to weight terms according to their positive (resp. negative) opinionated discriminability.</p><p>Finally, for the 87 submitted polarity runs, we computed the correlation between the polarity MAP and the topic-relevance MAP. Since for each polarity run there is a positive or a negative part, we correlated using the appropriate run's part (e.g. we correlated negative AP, calculated on the negative MAP run with topic relevance AP, calculated on the negative part of the run). In terms of finding positive opinionated blog posts, the overall rankings of systems are very similar (Spearman's ρ=0.9144 and Kendall's τ =0.7856). A similar high correlation is also observed for negative opinionfinding (Spearman's ρ=0.9341 and Kendall's τ =0.7909). This suggests that the effectiveness of polarity retrieval is strongly dependent on the topic-relevance effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BLOG DISTILLATION TASK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task and Topics</head><p>The blog distillation task was first introduced in TREC 2007 <ref type="bibr" coords="8,543.08,295.91,9.63,8.07" target="#b2">[2]</ref>. It addresses a search scenario where the user aims to find a blog to follow or read in their RSS reader. This blog should be principally devoted to topic X over the timespan of the collection. For example, Google's RSS reader provides an integrated blog search tool to allow users to easily find new blogs of interest. Unlike the blog post search tasks, the blog distillation task aims to rank blogs (aggregates of blog posts) instead of permalink documents.</p><p>Like in TREC 2007, the topics were contributed and judged by the participating groups. However, the topic creation guidelines given to the participating groups have been tightened up. Indeed, based on experience from TREC 2007, the participating groups were explicitly asked to avoid topics that are too general, with too many relevant documents (e.g. Linux), or topics with temporal aspects, i.e. topics likely to be of interest only in a specific period of time (Christmas). Each participating group was asked to contribute 6 topics, along with some relevant blogs. Similar to TREC 2007, to help the participating groups in creating their topics, the organisers have provided a standard search system for documents on the Blogs06 collection using the Terrier search engine <ref type="bibr" coords="8,501.25,494.63,9.51,8.07" target="#b4">[4]</ref>, which also displays the blogs for each document, as well as all the documents for a given blog. Overall, 11 participating groups sent a total of 66 topics. From each group, TREC selected 4 or 5 topics to form a set of 50 new topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Assessments</head><p>Relevance judgements were conducted by 11 participating groups, using a slight improvement of the TREC Blog track 2007's community judgements system interface <ref type="bibr" coords="8,437.18,589.67,9.68,8.07" target="#b2">[2,</ref><ref type="bibr" coords="8,449.06,589.67,6.45,8.07" target="#b5">5]</ref>. In particular, the assessors were asked to mark splogs (spam blogs), and to differentiate between relevant and highly relevant blogs. This allows the use of measures such as nDCG, and to have a better analysis of the blog distillation task's relevance assessments. As a consequence, the guidelines instructed to the assessors of each participating group were to read the query and its narrative, and to judge each blog in the provided pool. Relevance judgements were made on a fourpoint scale: Spam: This is a spam blog (splog).</p><p>Not relevant: I would definitely not subscribe to this feed. 0133 P@10pos P@10neg P@10 P@10pos P@10neg P@10 mix P@10pos P@10neg P@10 mix median 0.1220 0.0820 0.0014 0.0030 P@10pos P@10neg P@10 mix P@10pos P@10neg P@10 mix P@10pos P@10neg P@10 mix median 0. Relevant: This contains enough on-topic posts such that I would probably subscribe to it in my RSS reader.</p><p>Highly relevant: I would definitely subscribe to this blog for that topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Participants were allowed to submit up to 4 runs, including a compulsory automatic title-only run. Each run had blogs ranked by their likelihood of having a principal (recurring) interest in the topic. Given the number of blogs in the collection (just over 100k blogs), each run consisted of up to 100 returned blogs for each topic. Overall, 43 runs were submitted by 12 participating groups 3 . All of the submitted runs were automatic. A pool was then formed by NIST including the top 50 documents from two runs per participant. Table <ref type="table" coords="9,98.64,582.72,8.92,8.07" target="#tab_19">22</ref> shows the distribution of relevance levels across all topics. On average, each topic had about 16 highly relevant blogs, which are principally devoted to the topic of the query.</p><p>Figure <ref type="figure" coords="9,87.86,614.04,4.48,8.07">3</ref> shows the distribution of the number of blogs in the pool for the different relevance levels per topic. The topics are ordered by the descending sum of their corresponding relevant and highly relevant documents. The general topics appear early in the graph, while those with very few relevant documents appear late on the X axis. Considering the presence of spam in the pool, we note some 3 One group who participated in the topics creation and assessments, UCSC, did not submit runs, while two groups submitted runs but did not participate in the topics creation and relevance assessments phases, namely, KLE and IITKGP. variance in the number of returned splogs across the 50 used topics. For example, some topics had more than 118 spam blogs in the pool (e.g. "subprime lending" (1058), "celebrity babies" (1078), or "3d cities globes" (1086)), while others had very little corresponding spam in the pool (e.g. "road cycling" (1077), "jazz" (1064), or "Hubei" (1095)). We also note that there appears to be some variance in the number of relevant blogs across the 50 used topics. Indeed, some topics had very little relevant blogs in the pool (e.g. "beach volleyball" (1062) had only two relevant blogs and no highly relevant ones), while others had a high number of relevant and highly relevant blogs (e.g. topic "Firefox" (1059), which had 40 relevant blogs and 116 highly relevant ones). Other topics that had over 100 relevant and highly relevant blogs are topics "cooking recipes" (1053) and "SEO" (1060). Such large numbers of relevant blogs were observed even after tightening up the topics creation guidelines provided to the participating groups, so as to precisely avoid such a situation.</p><p>The blog distillation task is a precision-oriented search task where systems that retrieve the highly relevant documents should be favoured. Therefore, in evaluating the runs, we report the nDCG evaluation measure, which takes into account the graded relevance levels. We also report the classical retrieval measures such as MAP and precision at fixed ranks. Table <ref type="table" coords="9,433.84,627.48,8.92,8.07" target="#tab_3">23</ref> provides the average best and median nDCG, MAP, and MRR measures for each topic, across all 43 submitted runs.</p><p>Table <ref type="table" coords="9,348.84,658.92,8.92,8.07" target="#tab_4">24</ref> shows the best-scoring automatic title-only run from each participating group in terms of nDCG, and sorted in decreasing order. MAP(2) denotes the MAP of the run, when only the judged highly relevant blogs are considered to be relevant. Table <ref type="table" coords="9,546.96,690.24,8.92,8.07" target="#tab_5">25</ref> shows the best run from each group, regardless of the topic length used. Note that most of the 43 submitted runs were title-only runs.  Indeed, there were 36 submitted runs using the title-only field, 3 submitted runs using the title, description and narrative fields, and 4 submitted runs using the title and description fields. However, Table <ref type="table" coords="10,75.72,401.88,8.92,8.07" target="#tab_5">25</ref> shows that 3 out of the top 5 runs used more than the title field of the topics. The overall rankings of systems using either the nDCG or the MAP measures were very similar. Indeed, we observed a very high Spearman's correlation of ρ = 0.9807 for the 43 submitted runs (Kendall's τ distance leads to a similar high correlation of τ = 0.8936). If only the highly relevant documents are considered in ranking the systems (i.e. systems are ranked by MAP(2)), then the ranking of systems is very similar to the one obtained using both relevant and highly relevant documents (i.e. using MAP measure): ρ = 0.9461, τ = 0.7984 for the 43 submitted runs. This suggests that the ranking of systems are almost identical whether using nDCG, MAP or MAP(2) <ref type="bibr" coords="10,167.03,527.40,9.51,8.07" target="#b7">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Participants Approaches</head><p>Almost all groups indexed only the permalinks component of the Blogs06 collection. The only exceptions are the CMU and DUTIR groups who indexed both blogs and permalinks components, and the WHU group who experimented with two indexes: one based on feeds only and one based on the permalinks component only.</p><p>In terms of retrieval approaches, we noted an interesting trend, namely the use of expert search techniques to rank blogs. The idea, first proposed by the University of Glasgow in TREC 2007 <ref type="bibr" coords="10,279.94,637.92,9.51,8.07" target="#b8">[8]</ref>, was used by three groups in TREC 2008: UAms, UoGtr and USI. Both UoGtr and USI use the Voting Model to rank blogs <ref type="bibr" coords="10,263.21,658.92,9.51,8.07" target="#b8">[8]</ref>. Using an expert search approach, the UAms group explored the use of various external corpora to improve the effectiveness of query expansion. They also used several blog characteristics such as the number of comments, post length, or the posting time to estimate the strength of association between a post and a blog. Starting from their Voting Model for blog search, the UoGtr group added a component with a focus on a balanced and neutral retrieval that does not favour prolific bloggers. They also investigated the use of a feature which ascertains if the retrieved posts in a given blog for a topic are spread across the timespan of the Blogs06 collection. The idea is to model the notion of recurring interests. Finally, to further enrich the topics, UoGtr employed a collection enrichment technique, using the Wikipedia corpora. They observed that while each of their deployed techniques improved the effectiveness of their baseline run, the latter had an only average retrieval effectiveness. Finally, on top of an expert search approach used as a baseline, the USI group tested the use of structure-based evidence besides content in a Rank Learning approach. However, they observed that the Rank Learning model appears to be very sensitive to the properties of the data set, and did not perform well in their experiments.</p><p>Other retrieval groups, such as WHU, tested whether using folksonomies to expand the queries improves the retrieval effectiveness. They showed that the approach is only beneficial with a Feedsbased index, while it is detrimental to retrieval when a Permalinksbased index is used. The FEUP group investigated two features based on temporal evidence -temporal span and temporal dispersion. The temporal span of a topic in a blog corresponds to the period between the newest relevant post and the oldest relevant post. Both features were combined with a baseline BM25 run based on Terrier. Finally, the UMass group used a query likelihood language modelling approach. Recent posts are boosted higher in the aggregation of the scores of relevant posts.</p><p>Various groups implemented their solutions on top of existing information retrieval platforms such as Lucene (IITKGP), Indri/Lemur (CMU, Umass), and Terrier (UoGtr, USI, FEUP, WHU), using various document ranking models ranging from BM25 to language modelling, through Divergence From Randomness models. In the following, we provide a detailed description of the methods used by the two top performing groups in the blog distillation task.  The KLE group used two scores for a given blog. The first score is the average score of all posts in the blog. The KLE system assumes that the blog that has many relevant posts is more relevant. The second score is the average score of the top N posts that have the highest relevance scores. The KLE system assumes that the top N posts best represent the topic of the blog. The topic-relevance score of each post is calculated using a language modeling approach. To estimate the query model, KLE used the top M blogs in the feedback step. This method increases the diversity of feedback documents, and results in a more effective query model.</p><p>The CMU group explored document representation, retrieval models, query expansion and spam filtering. CMU's retrieval system, based on Indri, used a combined index of the permalink and blog documents, distinctly weighting text from various parts of the HTML and XML. Two retrieval models were applied to blog distillation: the large document model, where each blog is viewed as a single document; and the small document model, where a blog is represented as a collection of individual entry documents. Similarly to last year's results, CMU's best performing run used a query expansion method that leverages the link structure in Wikipedia. A spam filtering component was also integrated, which led to further performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>Back in 2006, when we first proposed the Blog track, our aim was to have a long-term objective for the Blog track, recognising that the richness of the blogosphere and its peculiarities will require several years of investigation before reaching a full understanding of the different blog search tasks, and how they should be effectively addressed. In particular, we proposed to adopt an incremental approach, where we begin with basic blog search tasks and progressively move to more complex search scenarios. We believe that the opinion-finding, its natural polarity extension, as well as the blog distillation tasks are good articulations of real user tasks, albeit basic, in adhoc search behaviour on the blogosphere.</p><p>After three years of the Blog track, we believe that we have a good test collection for the opinion-finding task and its polarity extension. In particular, the setting of the TREC 2008 Blog track's opinion-finding and polarity tasks, which provides the participating groups with various standard topic-relevance baselines, on which they can evaluate their opinion-finding techniques, should allow for a better understanding of these tasks and how the opinion-finding performance varies across different baselines. We believe, therefore, that the opinion-finding task in its current form should be discontinued. Instead, we propose to use the notion of opinion as a feature or a dimension of more refined and complex search tasks, as outlined below.</p><p>The current blog distillation task only focuses on topical relevance. It does not address the quality aspect of the retrieved blogs. In a position paper, Hearst et al. <ref type="bibr" coords="11,433.58,414.96,10.43,8.07" target="#b9">[9]</ref> proposed an interesting refinement of the blog distillation task that takes into account a number of attributes or facets such as the authority of the blog, the trustworthiness of its authors, or the genre of the blog (e.g. opinionated or not) and its style of writing. For example, a user might be interested in blogs to read about a topic X, but where the blogger expresses opinionated viewpoints, backed up by a scientific methodology or evidence. In other words, a user might not be interested in all blogs having a recurring and principal interest in a given topic X, but only those blogs that satisfy a set of criteria or facets.</p><p>For TREC 2009, we propose to move to a second phase of the Blog track, where more refined and complex search scenarios will be investigated. In particular, we propose to use a new and larger collection of blogs, Blogs08, which has a much longer timespan than the 11-weeks period covered in the Blogs06 collection. This allows for investigating another important characteristic of the blogosphere, namely the temporal/chronological aspect of blogging, and various related search tasks such as story identification and tracking. One of our proposed tasks for next year is a refinement of the blog distillation task, which addresses the quality aspect through the use of facets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,316.80,237.72,239.08,8.07;3,316.80,248.16,239.12,8.07;3,316.80,258.72,239.09,8.07;3,316.80,269.16,239.03,8.07;3,316.80,279.60,239.10,8.07;3,316.80,290.04,51.20,8.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: For each of the 130 opinion-finding task runs using a standard baseline, this figures shows the opinion-finding MAP (denoted O-MAP) of the opinion-finding task run compared to the opinion-finding MAP of the corresponding baseline. Ordering is by baseline run performance then opinion-finding run performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,53.76,406.92,239.28,8.07;4,53.76,417.36,239.28,8.07;4,53.76,427.80,126.20,8.07"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scatter plot of opinion-finding MAP (O-MAP) against topic-relevance MAP (T-MAP) for all of the 191 submitted opinion-finding task runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,408.36,310.80,56.76,8.07;9,480.84,310.80,20.00,8.07;9,369.60,321.72,15.98,8.07;9,407.64,321.72,95.44,8.07;9,369.60,332.16,133.48,8.07;9,316.80,354.00,239.20,8.07;9,316.80,364.44,225.08,8.07"><head></head><label></label><figDesc>Best, median and worst nDCG, MAP &amp; MRR measures for the 43 submitted runs to the blog distillation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,510.27,711.23,45.55,8.07"><head>Table 1 : Details of the five provided standard baselines.</head><label>1</label><figDesc>. In the first</figDesc><table coords="2,72.54,55.40,201.64,61.48"><row><cell>Baseline</cell><cell>Run ID</cell><cell>Run type</cell><cell>Topics</cell></row><row><cell cols="2">baseline1 uicirwa</cell><cell cols="2">Automatic Title-only</cell></row><row><cell cols="2">baseline2 DCUCDVPtdbl</cell><cell cols="2">Automatic Title-desc</cell></row><row><cell cols="2">baseline3 UniNEBlog1</cell><cell cols="2">Automatic Title-desc</cell></row><row><cell cols="4">baseline4 KLEPsgFeedTD Automatic Title-desc</cell></row><row><cell cols="2">baseline5 prisbm</cell><cell>Manual</cell><cell>Title-only</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,316.80,148.68,239.26,134.45"><head>Table 2 : Breakdown of the baselines used by the submit- ted opinion-finding runs, including the five standard baselines. Own denotes when a run was based on a participating group's own baseline retrieval system, while N/A denotes when a par- ticipant's system did not submit separate topic-relevance and opinion-finding runs.</head><label>2</label><figDesc></figDesc><table coords="2,337.44,221.36,197.78,61.77"><row><cell>Relevance level</cell><cell>2006</cell><cell>2007</cell><cell>2008</cell></row><row><cell>Not Relevant</cell><cell>949.82</cell><cell>848.68</cell><cell>841.60</cell></row><row><cell>Relevant</cell><cell>167.22</cell><cell>103.74</cell><cell>58.76</cell></row><row><cell>Relevant, negative opinions</cell><cell>74.14</cell><cell>36.88</cell><cell>55.78</cell></row><row><cell>Relevant, mixed opinions</cell><cell>73.28</cell><cell>43.92</cell><cell>53.40</cell></row><row><cell>Relevant, positive opinions</cell><cell>83.18</cell><cell>59.20</cell><cell>66.76</cell></row><row><cell>Total</cell><cell cols="3">1347.64 1092.42 1076.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="2,316.80,296.64,239.10,69.51"><head>Table 3 : Average number of judged documents per topic in each of the considered relevance levels across years 2006-2008.</head><label>3</label><figDesc></figDesc><table /><note coords="2,316.80,337.20,238.90,8.07;2,316.80,347.64,239.08,8.07;2,316.80,358.08,49.18,8.07"><p>one polarity runs per group. If a group didn't have any opinionfinding or polarity runs, it only contributed runs from the task it did participate in.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,53.76,55.04,502.18,131.01"><head>Table 4 : Baseline runs: Best, median, and worst topic-relevance and opinion-finding MAP and P@10 measures of the 2008 partici- pating groups across the three topic sets.</head><label>4</label><figDesc>MAP rel P@10 rel MAPop P@10op MAP rel P@10 rel MAPop P@10op MAP rel P@10 rel MAPop P@10op MAP rel P@10 rel MAPop P@10op MAP rel P@10 rel MAPop P@10op MAP rel P@10 rel</figDesc><table coords="4,63.24,55.04,482.64,131.01"><row><cell></cell><cell></cell><cell cols="2">2006 (851-900)</cell><cell></cell><cell></cell><cell cols="2">2007 (901-950)</cell><cell></cell><cell></cell><cell cols="2">2008 (1001-1050)</cell><cell></cell></row><row><cell>median</cell><cell>0.3152</cell><cell>0.6800</cell><cell>0.2080</cell><cell>0.4220</cell><cell>0.3973</cell><cell>0.7280</cell><cell>0.2940</cell><cell>0.4880</cell><cell>0.3529</cell><cell>0.6960</cell><cell>0.2890</cell><cell>0.5700</cell></row><row><cell>best</cell><cell>0.5049</cell><cell>0.9440</cell><cell>0.3664</cell><cell>0.7580</cell><cell>0.6498</cell><cell>0.9600</cell><cell>0.4991</cell><cell>0.8000</cell><cell>0.5994</cell><cell>0.9140</cell><cell>0.5002</cell><cell>0.8260</cell></row><row><cell>worst</cell><cell>0.0242</cell><cell>0.0480</cell><cell>0.0131</cell><cell>0.0260</cell><cell>0.0532</cell><cell>0.0780</cell><cell>0.0281</cell><cell>0.0220</cell><cell>0.0381</cell><cell>0.0780</cell><cell>0.0284</cell><cell>0.0520</cell></row><row><cell></cell><cell></cell><cell cols="2">2006 (851-900)</cell><cell></cell><cell></cell><cell cols="2">2007 (901-950)</cell><cell></cell><cell></cell><cell cols="2">2008 (1001-1050)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MAPop P@10op</cell></row><row><cell>median</cell><cell>0.3408</cell><cell>0.7620</cell><cell>0.2549</cell><cell>0.5360</cell><cell>0.4407</cell><cell>0.8220</cell><cell>0.3552</cell><cell>0.6060</cell><cell>0.3819</cell><cell>0.7140</cell><cell>0.3291</cell><cell>0.6100</cell></row><row><cell>best</cell><cell>0.5747</cell><cell>0.9860</cell><cell>0.6456</cell><cell>0.9580</cell><cell>0.6965</cell><cell>0.9840</cell><cell>0.7626</cell><cell>0.9480</cell><cell>0.6279</cell><cell>0.9400</cell><cell>0.5610</cell><cell>0.8980</cell></row><row><cell>worst</cell><cell>0.0598</cell><cell>0.0340</cell><cell>0.0459</cell><cell>0.0140</cell><cell>0.0482</cell><cell>0.0100</cell><cell>0.0322</cell><cell>0.0040</cell><cell>0.0405</cell><cell>0.0060</cell><cell>0.0330</cell><cell>0.0020</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,53.76,199.56,502.00,193.10"><head>Table 5 : Opinion-finding runs: Best, median, and worst topic-relevance and opinion-finding MAP and P@10 measures of the 2008 participating groups across the three topic sets.</head><label>5</label><figDesc></figDesc><table coords="4,67.21,243.56,212.50,149.09"><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>O-MAP</cell><cell>0.25 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>0.25</cell><cell>0.3</cell><cell>0.35</cell><cell>0.4</cell><cell>0.45</cell><cell>0.5</cell><cell>0.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T-MAP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,59.76,55.04,490.00,304.87"><head>Table 6 : Baseline and opinion-finding runs over all 150 topics.</head><label>6</label><figDesc></figDesc><table coords="5,59.76,55.04,490.00,304.87"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Baseline runs</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Opinion-finding runs</cell></row><row><cell></cell><cell cols="10">MAP rel P@10 rel MAPop P@10op MAP rel P@10 rel MAPop P@10op</cell></row><row><cell></cell><cell>median</cell><cell>0.3551</cell><cell>0.7013</cell><cell>0.2636</cell><cell>0.4933</cell><cell cols="2">0.3878</cell><cell cols="2">0.7660</cell><cell>0.3131</cell><cell>0.5840</cell></row><row><cell></cell><cell>best</cell><cell>0.5847</cell><cell>0.9393</cell><cell>0.4552</cell><cell>0.7947</cell><cell cols="2">0.6330</cell><cell cols="2">0.9700</cell><cell>0.6564</cell><cell>0.9347</cell></row><row><cell></cell><cell>worst</cell><cell>0.0385</cell><cell>0.0680</cell><cell>0.0232</cell><cell>0.0333</cell><cell cols="2">0.0495</cell><cell cols="2">0.0167</cell><cell>0.0370</cell><cell>0.0067</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topic-Relevance</cell><cell></cell><cell></cell><cell></cell><cell>Opinion-Finding</cell></row><row><cell>Group</cell><cell>Run</cell><cell></cell><cell cols="2">MAP R-prec</cell><cell>bPref</cell><cell>P@10</cell><cell cols="2">MRR</cell><cell cols="2">MAP R-prec</cell><cell>bPref</cell><cell>P@10</cell><cell>MRR</cell></row><row><cell>KLE</cell><cell cols="2">KLEPsgFeedT</cell><cell cols="8">0.4954 0.5150 0.5364 0.7920 0.9058 0.4052 0.4366 0.4314 0.6440 0.8184</cell></row><row><cell cols="2">UAms De Rijke uams08n1o1</cell><cell></cell><cell cols="8">0.4644 0.4867 0.5034 0.7620 0.8892 0.3797 0.4176 0.4117 0.6620 0.8052</cell></row><row><cell>UIC IR Group</cell><cell>uicirnoa</cell><cell></cell><cell cols="8">0.4403 0.4804 0.5062 0.7700 0.8667 0.3438 0.3956 0.3929 0.5880 0.7480</cell></row><row><cell>UniNE</cell><cell cols="2">UniNEBlog2</cell><cell cols="8">0.4283 0.4551 0.4659 0.6580 0.8482 0.3537 0.3781 0.3676 0.5620 0.7963</cell></row><row><cell>UoGtr</cell><cell cols="2">uogBLProxCE</cell><cell cols="8">0.4219 0.4548 0.4481 0.7060 0.8228 0.3531 0.3840 0.3646 0.6100 0.7723</cell></row><row><cell>THUIR</cell><cell cols="2">THUrelTwpmf</cell><cell cols="8">0.4067 0.4565 0.4625 0.6940 0.8263 0.3313 0.3942 0.3749 0.5900 0.7487</cell></row><row><cell>BUPT pris</cell><cell>prisba</cell><cell></cell><cell cols="8">0.4065 0.4506 0.4561 0.6780 0.8290 0.3346 0.3876 0.3684 0.5580 0.7456</cell></row><row><cell>DUTIR</cell><cell cols="2">DUT08BRun1</cell><cell cols="8">0.3617 0.4188 0.4345 0.6540 0.7633 0.2974 0.3586 0.3598 0.5420 0.7204</cell></row><row><cell>iitkgp</cell><cell cols="10">IITKGPNOSPAM 0.3598 0.4090 0.4394 0.7400 0.8817 0.2988 0.3664 0.3642 0.5720 0.7955</cell></row><row><cell>IU-SLIS</cell><cell>wdoqsBase</cell><cell></cell><cell cols="8">0.3431 0.3918 0.4001 0.7280 0.8636 0.2818 0.3367 0.3215 0.5900 0.7551</cell></row><row><cell>UWaterlooEng</cell><cell>UWBase2</cell><cell></cell><cell cols="8">0.3309 0.3824 0.3875 0.6380 0.8127 0.2753 0.3391 0.3249 0.5160 0.7254</cell></row><row><cell>aic-dcu</cell><cell cols="2">DCUCDVPtbl</cell><cell cols="8">0.3303 0.3671 0.3601 0.6520 0.7783 0.2875 0.3280 0.3089 0.5560 0.7066</cell></row><row><cell>UIUC</cell><cell cols="2">UIUCb08uwTtl</cell><cell cols="8">0.3240 0.3766 0.3771 0.6800 0.8223 0.2723 0.3336 0.3133 0.5540 0.7777</cell></row><row><cell>fub</cell><cell cols="2">FIUbasePL2c9</cell><cell cols="8">0.3199 0.3738 0.3601 0.6120 0.7351 0.2659 0.3206 0.2915 0.5020 0.6862</cell></row><row><cell>UTD SLP Lab</cell><cell>SplBaseT</cell><cell></cell><cell cols="8">0.3077 0.3688 0.3706 0.5960 0.7152 0.2473 0.3195 0.3012 0.4760 0.6569</cell></row><row><cell>KobeU-Seki</cell><cell>ku</cell><cell></cell><cell cols="8">0.3035 0.3602 0.3531 0.5820 0.7053 0.2475 0.3051 0.2806 0.4960 0.6585</cell></row><row><cell>KU</cell><cell>kunlpKLtt</cell><cell></cell><cell cols="8">0.2791 0.3568 0.3487 0.5700 0.7784 0.2263 0.3042 0.2815 0.4520 0.6955</cell></row><row><cell>USI</cell><cell>run0</cell><cell></cell><cell cols="8">0.2567 0.3363 0.3289 0.4020 0.5472 0.2048 0.2604 0.2523 0.3060 0.4605</cell></row><row><cell>feup irlab</cell><cell>feupB</cell><cell></cell><cell cols="8">0.2518 0.3190 0.3243 0.5800 0.7133 0.2006 0.2660 0.2573 0.4360 0.5745</cell></row><row><cell>york</cell><cell>york08bb2</cell><cell></cell><cell cols="8">0.2074 0.2923 0.2863 0.5540 0.7954 0.1700 0.2489 0.2343 0.4520 0.7308</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,53.76,373.68,502.03,8.07"><head>Table 7 : Baseline task: Topic-Relevance and Opinion-Finding -Title only -using the TREC 2008 new topics. All runs are automatic.</head><label>7</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="5,489.24,711.24,66.39,8.07"><head>Table 15</head><label>15</label><figDesc></figDesc><table coords="5,525.12,711.24,30.51,8.07"><row><cell>provides</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="6,77.16,321.36,455.44,303.51"><head>Table 8 : Baseline task: Topic-Relevance and Opinion-Finding using the TREC 2008 new topics. All runs are automatic.</head><label>8</label><figDesc></figDesc><table coords="6,77.88,417.30,453.88,207.57"><row><cell>Group</cell><cell>Run</cell><cell cols="2">Fields Baseline</cell><cell>MAP</cell><cell cols="2">∆ MAP R-prec</cell><cell>bPref</cell><cell>P@10</cell><cell>MRR</cell></row><row><cell>KLE</cell><cell>KLEDocOpinT</cell><cell>T</cell><cell>N/A</cell><cell>0.4569</cell><cell>N/A</cell><cell cols="3">0.4797 0.4791 0.7200 0.8503</cell></row><row><cell>IU-SLIS</cell><cell>top3dt1mRd</cell><cell>T</cell><cell>N/A</cell><cell>0.4335</cell><cell>N/A</cell><cell cols="3">0.4618 0.4428 0.6780 0.8483</cell></row><row><cell>aic-dcu</cell><cell>DCUCDVPgoo</cell><cell>TD</cell><cell>baseline4</cell><cell>0.4155</cell><cell>8.71%</cell><cell cols="3">0.4479 0.4411 0.6800 0.8218</cell></row><row><cell>UIC IR Group</cell><cell>uicop2bl4r</cell><cell>T</cell><cell>baseline4</cell><cell>0.4067</cell><cell>6.41%</cell><cell cols="3">0.4527 0.4338 0.6160 0.7528</cell></row><row><cell>fub</cell><cell>FIUBL4DFR</cell><cell>T</cell><cell>baseline4</cell><cell>0.4006</cell><cell>4.81%</cell><cell cols="3">0.4447 0.4281 0.6240 0.8097</cell></row><row><cell>UoGtr</cell><cell>uogOP4intL</cell><cell>T</cell><cell>baseline4</cell><cell>0.3964</cell><cell>3.72%</cell><cell cols="3">0.4370 0.4236 0.6400 0.8137</cell></row><row><cell>DUTIR</cell><cell>DUTIR08Run4</cell><cell>T</cell><cell>DUT08BRun2</cell><cell cols="5">0.3902 31.60% 0.4257 0.4191 0.6620 0.8082</cell></row><row><cell>UTD SLP Lab</cell><cell>NOpMM47</cell><cell>TD</cell><cell>baseline4</cell><cell>0.3844</cell><cell>0.58%</cell><cell cols="3">0.4258 0.4158 0.6300 0.7908</cell></row><row><cell cols="2">UAms De Rijke uams08n1o1sp</cell><cell>T</cell><cell>uams08n1o1</cell><cell>0.3823</cell><cell>0.68%</cell><cell cols="3">0.4204 0.4139 0.6580 0.8052</cell></row><row><cell>THUIR</cell><cell cols="2">THUopnTmfRmf T</cell><cell>THUrelTwpmf</cell><cell>0.3522</cell><cell>6.31%</cell><cell cols="3">0.4104 0.3902 0.6320 0.7347</cell></row><row><cell>UniNE</cell><cell>UniNEopZ1</cell><cell>TD</cell><cell>UniNEBlog1</cell><cell cols="5">0.3418 -4.12% 0.3961 0.3661 0.5840 0.7859</cell></row><row><cell>UWaterlooEng</cell><cell>UWnb4Op</cell><cell>T</cell><cell>baseline4</cell><cell cols="5">0.3381 -11.54% 0.3718 0.3613 0.6060 0.8231</cell></row><row><cell>BUPT pris</cell><cell>prisoa1</cell><cell>T</cell><cell>prisba</cell><cell cols="5">0.3344 -0.06% 0.3868 0.3679 0.5560 0.7539</cell></row><row><cell>USI</cell><cell>opin1kl</cell><cell>T</cell><cell>baseline1</cell><cell cols="5">0.3122 -3.61% 0.3584 0.3390 0.5460 0.7062</cell></row><row><cell>iitkgp</cell><cell>KGPPOS1</cell><cell>TD</cell><cell cols="2">IITKGPTITLE1 0.3005</cell><cell>2.39%</cell><cell cols="3">0.3735 0.3633 0.6260 0.8024</cell></row><row><cell>KobeU-Seki</cell><cell>kuo</cell><cell>T</cell><cell>ku</cell><cell>0.2704</cell><cell>9.25%</cell><cell cols="3">0.3259 0.2978 0.5380 0.7058</cell></row><row><cell>york</cell><cell>york08bo1a</cell><cell>T</cell><cell>baseline1</cell><cell cols="5">0.2600 -19.73% 0.3160 0.3033 0.3960 0.4817</cell></row><row><cell>SUNY Buffalo</cell><cell>UBop1</cell><cell>TD</cell><cell>N/A</cell><cell>0.1872</cell><cell>N/A</cell><cell cols="3">0.2184 0.2259 0.3140 0.4051</cell></row><row><cell>KU</cell><cell>kunlpKLttOc</cell><cell>T</cell><cell>kunlpKLtt</cell><cell cols="5">0.1752 -22.58% 0.2609 0.2386 0.5200 0.7390</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="6,53.76,638.64,502.28,39.39"><head>Table 9 : Opinion-Finding task: Any baseline, any topic fields, using the TREC 2008 new topics. Ranked by (opinion-finding) MAP. N/A denotes a run by a system that cannot separate the topic-relevance and opinion-finding components. All runs are automatic. ∆ MAP denotes the percentage increase in opinion-finding MAP that the opinion-finding run achieved over the opinion-finding MAP of the corresponding baseline run.</head><label>9</label><figDesc></figDesc><table coords="7,77.88,67.98,453.88,155.25"><row><cell>Group</cell><cell>Run</cell><cell cols="2">Fields Baseline</cell><cell>MAP</cell><cell cols="2">∆ MAP R-prec</cell><cell>bPref</cell><cell>P@10</cell><cell>MRR</cell></row><row><cell>DUTIR</cell><cell>DUTIR08Run4</cell><cell>T</cell><cell>DUT08BRun2</cell><cell cols="5">0.3902 31.60% 0.4257 0.4191 0.6620 0.8082</cell></row><row><cell cols="2">UAms De Rijke uams08n1o1sp</cell><cell>T</cell><cell>uams08n1o1</cell><cell>0.3823</cell><cell>0.68%</cell><cell cols="3">0.4204 0.4139 0.6580 0.8052</cell></row><row><cell>UoGtr</cell><cell>uogOPb2ofL</cell><cell>T</cell><cell>uogBLProxCE</cell><cell>0.3709</cell><cell>5.04%</cell><cell cols="3">0.4049 0.3824 0.6380 0.8114</cell></row><row><cell>THUIR</cell><cell cols="2">THUopnTmfRmf T</cell><cell>THUrelTwpmf</cell><cell>0.3522</cell><cell>6.31%</cell><cell cols="3">0.4104 0.3902 0.6320 0.7347</cell></row><row><cell>UniNE</cell><cell>UniNEopZ1</cell><cell>TD</cell><cell>UniNEBlog1</cell><cell cols="5">0.3418 -4.12% 0.3961 0.3661 0.5840 0.7859</cell></row><row><cell>BUPT pris</cell><cell>prisoa1</cell><cell>T</cell><cell>prisba</cell><cell cols="5">0.3344 -0.06% 0.3868 0.3679 0.5560 0.7539</cell></row><row><cell>aic-dcu</cell><cell>DCUCDVPtol</cell><cell>T</cell><cell>DCUCDVPtbl</cell><cell cols="5">0.3299 14.75% 0.3679 0.3553 0.6360 0.7689</cell></row><row><cell>IU-SLIS</cell><cell>wdqfdt1mRd</cell><cell>TDN</cell><cell>wdoqlnvN</cell><cell cols="5">0.3127 13.38% 0.3702 0.3518 0.6200 0.8035</cell></row><row><cell>iitkgp</cell><cell>KGPPOS1</cell><cell>TD</cell><cell cols="2">IITKGPTITLE1 0.3005</cell><cell>2.39%</cell><cell cols="3">0.3735 0.3633 0.6260 0.8024</cell></row><row><cell>fub</cell><cell>FIUPL2c9DFR</cell><cell>T</cell><cell>FIUbasePL2c9</cell><cell cols="5">0.2951 10.98% 0.3507 0.3161 0.5640 0.7288</cell></row><row><cell></cell><cell>UWopinion2</cell><cell>T</cell><cell>UWBase2</cell><cell>0.2892</cell><cell>5.05%</cell><cell cols="3">0.3361 0.3222 0.5840 0.7832</cell></row><row><cell>KobeU-Seki</cell><cell>kuo</cell><cell>T</cell><cell>ku</cell><cell>0.2704</cell><cell>9.25%</cell><cell cols="3">0.3259 0.2978 0.5380 0.7058</cell></row><row><cell>KU</cell><cell>kunlpKLttOc</cell><cell>T</cell><cell>kunlpKLtt</cell><cell cols="5">0.1752 -22.58% 0.2609 0.2386 0.5200 0.7390</cell></row><row><cell>USI</cell><cell>opin0kl</cell><cell>T</cell><cell>run0</cell><cell cols="5">0.1484 -27.54% 0.1868 0.1736 0.2660 0.3757</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="7,53.76,237.00,502.28,77.21"><head>Table 10 : Opinion-Finding task: Own baseline, any topic fields, using the TREC 2008 new topics. Ranked by MAP. All runs are automatic.</head><label>10</label><figDesc></figDesc><table coords="7,146.16,288.68,317.32,25.53"><row><cell></cell><cell cols="5">baseline1 baseline2 baseline3 baseline4 baseline5</cell><cell cols="2">improvement</cell></row><row><cell>Baseline</cell><cell>0.3239</cell><cell>0.2639</cell><cell>0.3564</cell><cell>0.3822</cell><cell>0.2988</cell><cell>mean</cell><cell>stdev</cell></row><row><cell>TREC median</cell><cell>0.3493</cell><cell>0.2705</cell><cell>0.3705</cell><cell>0.3846</cell><cell>0.3010</cell><cell cols="2">+0.76% 0.73%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="7,55.80,326.52,498.07,166.83"><head>Table 11 : Median opinion MAP over each of the 5 standard baselines and median average improvement for the TREC 2008 topics.</head><label>11</label><figDesc></figDesc><table coords="7,185.16,369.54,239.19,123.81"><row><cell>Group</cell><cell>Run</cell><cell cols="2">Fields MAP</cell><cell>∆ MAP</cell></row><row><cell>KLE</cell><cell cols="2">B4PsgOpinAZN T</cell><cell>0.4189</cell><cell>9.60%</cell></row><row><cell>aic-dcu</cell><cell>DCUCDVPgoo</cell><cell>TD</cell><cell>0.4155</cell><cell>8.71%</cell></row><row><cell>UIC IR Group</cell><cell>uicop2bl4r</cell><cell>T</cell><cell>0.4067</cell><cell>6.41%</cell></row><row><cell>IU-SLIS</cell><cell>b4dt1mRd</cell><cell>T</cell><cell>0.4023</cell><cell>5.26%</cell></row><row><cell>fub</cell><cell>FIUBL4DFR</cell><cell>T</cell><cell>0.4006</cell><cell>4.81%</cell></row><row><cell>UoGtr</cell><cell>uogOP4intL</cell><cell>T</cell><cell>0.3964</cell><cell>3.72%</cell></row><row><cell>UTD SLP Lab</cell><cell>NOpMM47</cell><cell>TD</cell><cell>0.3844</cell><cell>0.58%</cell></row><row><cell>UWaterlooEng</cell><cell>UWnb4Op</cell><cell>T</cell><cell cols="2">0.3381 -11.54%</cell></row><row><cell>iitkgp</cell><cell>KGPBASE4</cell><cell>T</cell><cell cols="2">0.2852 -25.38%</cell></row><row><cell cols="2">UAms De Rijke uams08b4pr</cell><cell>T</cell><cell cols="2">0.1369 -64.18%</cell></row><row><cell>UniNE</cell><cell>UniNEopLRb4</cell><cell></cell><cell cols="2">0.2341 -38.75%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="7,53.76,507.12,502.31,165.15"><head>Table 12 : Opinion-Finding task: Results for runs using standard baseline4, which has the highest topic-relevance and opinion- finding MAP. Ranked by ∆ MAP, using the TREC 2008 new topics. No topic fields were specified for run UniNEopLRb4. All runs are automatic.</head><label>12</label><figDesc></figDesc><table coords="7,152.76,569.34,304.10,102.93"><row><cell>Group</cell><cell>Approach of</cell><cell>Fields</cell><cell>MAP Mean</cell><cell>σ</cell><cell cols="2">∆ MAP Mean</cell><cell>σ</cell></row><row><cell>UIC IR Group</cell><cell>uicop1bl1r</cell><cell>T</cell><cell cols="3">0.3614 0.04 11.76%</cell><cell cols="2">6.93%</cell></row><row><cell>KLE</cell><cell cols="2">B1PsgOpinAZN T</cell><cell cols="2">0.3565 0.05</cell><cell>9.67%</cell><cell cols="2">0.77%</cell></row><row><cell>UoGtr</cell><cell>uogOP1PrintL</cell><cell>T</cell><cell cols="2">0.3412 0.04</cell><cell>5.21%</cell><cell cols="2">5.10%</cell></row><row><cell>UTD SLP Lab</cell><cell>NOpMM107</cell><cell>TD</cell><cell cols="2">0.3273 0.04</cell><cell>0.76%</cell><cell cols="2">0.73%</cell></row><row><cell>UWaterlooEng</cell><cell>UWnb1Op</cell><cell>T</cell><cell cols="3">0.3215 0.02 -0.14%</cell><cell cols="2">7.86%</cell></row><row><cell>fub</cell><cell>FIUBL1DFR</cell><cell>T</cell><cell cols="5">0.2938 0.13 -11.16% 35.62%</cell></row><row><cell>UniNE</cell><cell>UniNEopLRb1</cell><cell>T</cell><cell cols="5">0.2118 0.02 -34.60% 2.31%</cell></row><row><cell cols="2">UAms De Rijke uams08b1pr</cell><cell>T</cell><cell cols="5">0.1378 0.03 -57.41% 8.02%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="7,53.76,685.74,502.28,18.81"><head>Table 13 : Opinion-Finding task: Results for runs using all 5 standard baselines, ranked by</head><label>13</label><figDesc></figDesc><table /><note coords="7,399.23,685.74,156.81,8.37;7,53.76,696.48,245.96,8.07"><p>Mean ∆ MAP, using the TREC 2008 new topics. σ denotes the standard deviation. All runs are automatic.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="9,53.76,110.00,502.23,112.65"><head>Table 15 : Baseline runs: Best, median and worst positive, negative, and mixed MAP and P@10 measures of the 2008 participating groups across the three topic sets.</head><label>15</label><figDesc></figDesc><table coords="9,99.12,110.00,409.66,112.65"><row><cell></cell><cell></cell><cell></cell><cell>0.1110</cell><cell>0.2200</cell><cell>0.0580</cell><cell>0.1490</cell><cell>0.1520</cell><cell>0.1380</cell><cell>0.1550</cell></row><row><cell>best</cell><cell>0.3480</cell><cell>0.2860</cell><cell>0.2610</cell><cell>0.4760</cell><cell>0.2520</cell><cell>0.3120</cell><cell>0.3980</cell><cell>0.3140</cell><cell>0.3040</cell></row><row><cell>worst</cell><cell>0.0000</cell><cell>0.0020</cell><cell>0.0030</cell><cell>0.0120</cell><cell>0.0000</cell><cell>0.0080</cell><cell>0.0080</cell><cell>0.0080</cell><cell>0.0140</cell></row><row><cell></cell><cell></cell><cell>2006 (851-900)</cell><cell></cell><cell></cell><cell>2007 (901-950)</cell><cell></cell><cell></cell><cell cols="2">2008 (1001-1050)</cell></row><row><cell></cell><cell>MAPpos</cell><cell>MAPneg</cell><cell>MAP mix</cell><cell>MAPpos</cell><cell>MAPneg</cell><cell>MAP mix</cell><cell>MAPpos</cell><cell>MAPneg</cell><cell>MAP mix</cell></row><row><cell>median</cell><cell>0.0796</cell><cell>0.0638</cell><cell>0.0751</cell><cell>0.1734</cell><cell>0.0628</cell><cell>0.1241</cell><cell>0.0899</cell><cell>0.0678</cell><cell>0.0808</cell></row><row><cell>best</cell><cell>0.3890</cell><cell>0.5178</cell><cell>0.4019</cell><cell>0.5405</cell><cell>0.5332</cell><cell>0.4763</cell><cell>0.2723</cell><cell>0.2365</cell><cell>0.2297</cell></row><row><cell>worst</cell><cell>0.0033</cell><cell>0.0013</cell><cell>0.0033</cell><cell>0.0025</cell><cell>0.0007</cell><cell>0.0030</cell><cell>0.0027</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="9,53.76,234.20,502.23,136.99"><head>Table 16 : Polarity runs: Best, median and worst positive, negative, and mixed MAP and P@10 measures of the 2008 participating groups across the three topic sets.</head><label>16</label><figDesc></figDesc><table coords="9,72.41,234.20,432.33,136.99"><row><cell></cell><cell>1640</cell><cell>0.1260</cell><cell>0.1530</cell><cell>0.2740</cell><cell>0.1120</cell><cell>0.2000</cell><cell>0.1640</cell><cell>0.1300</cell><cell>0.1550</cell></row><row><cell>best</cell><cell>0.7120</cell><cell>0.7460</cell><cell>0.6190</cell><cell>0.7940</cell><cell>0.6200</cell><cell>0.6160</cell><cell>0.4940</cell><cell>0.4060</cell><cell>0.4030</cell></row><row><cell>worst</cell><cell>0.0000</cell><cell>0.0020</cell><cell>0.0010</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0010</cell><cell>0.0020</cell><cell>0.0000</cell><cell>0.0020</cell></row><row><cell cols="4">Relevance Scale Level Nbr. of Documents</cell><cell>Avg.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Highly Relevant</cell><cell>2</cell><cell></cell><cell>792</cell><cell>15.84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relevant</cell><cell>1</cell><cell></cell><cell>1151</cell><cell>23.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Not Relevant</cell><cell>0</cell><cell></cell><cell cols="2">13979 279.58</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spam</cell><cell>-1</cell><cell></cell><cell>2080</cell><cell>41.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell>-</cell><cell></cell><cell cols="2">18002 360.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="9,53.76,384.96,239.04,18.51"><head>Table 22 : Blog distillation task: Distribution of relevance levels in the pool.</head><label>22</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="10,60.38,55.04,488.81,273.39"><head>Table 17 : Best, median, worst of baseline and polarity runs over all 150 topics.</head><label>17</label><figDesc></figDesc><table coords="10,366.12,55.04,40.76,7.17"><row><cell>Polarity runs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="10,94.56,340.80,420.87,8.07"><head>Table 18 : Polarity task: Any baseline, any topic fields, using the TREC 2008 new topics. Ranked by Mix MAP.</head><label>18</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="11,60.39,55.14,488.85,201.59"><head>Table 19 : Polarity task: Own baseline, any topic fields, using the TREC 2008 new topics. Ranked by Mix MAP.</head><label>19</label><figDesc></figDesc><table coords="11,60.39,55.14,488.85,201.59"><row><cell>Group</cell><cell>Run</cell><cell cols="2">Fields Baseline</cell><cell></cell><cell>MAP</cell><cell cols="2">Mix ∆ MAP P@10</cell><cell>MAP</cell><cell cols="2">Positive ∆ MAP P@10</cell><cell>MAP</cell><cell>Negative ∆ MAP P@10</cell></row><row><cell>THUIR</cell><cell cols="2">THUpolTmfPNR T</cell><cell cols="2">THUrelTwpmf</cell><cell>0.1353</cell><cell>7.16%</cell><cell cols="2">0.1870 0.1289</cell><cell>6.27%</cell><cell>0.1880 0.1417</cell><cell>7.92%</cell><cell>0.1860</cell></row><row><cell>UoGtr</cell><cell>uogPLb21</cell><cell>T</cell><cell cols="2">uogBLProxCE</cell><cell cols="6">0.1274 -0.62% 0.1560 0.1372 -1.15% 0.1700 0.1176</cell><cell>0.00%</cell><cell>0.1420</cell></row><row><cell>IU-SLIS</cell><cell>wdqbdt1mP5</cell><cell>T</cell><cell>wdoqsBase</cell><cell></cell><cell>0.1143</cell><cell>9.51%</cell><cell></cell><cell>0.1147</cell><cell>6.80%</cell><cell>0.2280 0.1138 12.34% 0.2040</cell></row><row><cell>iitkgp</cell><cell>KGPPOL1</cell><cell>T</cell><cell cols="8">IITKGPTITLE1 0.1139 -6.15% 0.1990 0.1304 -1.95% 0.2300 0.0975 -11.12% 0.1680</cell></row><row><cell>aic-dcu</cell><cell>DCUCDVPtpl</cell><cell>T</cell><cell cols="2">DCUCDVPtbl</cell><cell>0.1092</cell><cell>9.88%</cell><cell cols="4">0.1550 0.1087 13.35% 0.1380 0.1097</cell><cell>6.61%</cell><cell>0.1720</cell></row><row><cell cols="2">UWaterlooEng UWpolarity2</cell><cell>T</cell><cell>UWBase2</cell><cell></cell><cell cols="4">0.1078 -0.27% 0.1670 0.1215</cell><cell>9.66%</cell><cell>0.2000 0.0942 -10.63% 0.1340</cell></row><row><cell>KobeU-Seki</cell><cell>kup</cell><cell>T</cell><cell>ku</cell><cell></cell><cell>0.0994</cell><cell>9.83%</cell><cell cols="4">0.1650 0.1056 13.79% 0.1740 0.0933</cell><cell>5.78%</cell><cell>0.1560</cell></row><row><cell>UniNE</cell><cell>UniNEpolLR1</cell><cell>TD</cell><cell cols="2">UniNEBlog1</cell><cell cols="6">0.0775 -41.33% 0.1780 0.0882 -35.90% 0.2000 0.0667 -47.31% 0.1560</cell></row><row><cell>fub</cell><cell>FIUpPL2DFR</cell><cell>T</cell><cell cols="2">FIUbasePL2c9</cell><cell cols="6">0.0506 -46.91% 0.1290 0.0529 -48.94% 0.1680 0.0483 -44.55% 0.0900</cell></row><row><cell>KU</cell><cell>kunlpKLttPs</cell><cell>T</cell><cell>kunlpKLtt</cell><cell></cell><cell cols="6">0.0416 -54.39% 0.1560 0.0542 -38.34% 0.1900 0.0291 -69.21% 0.1220</cell></row><row><cell>DUTIR</cell><cell cols="2">DUTIR08Run2P T</cell><cell cols="2">DUT08BRun2</cell><cell cols="6">0.0301 -73.43% 0.1500 0.0352 -72.28% 0.1840 0.0250 -74.87% 0.1160</cell></row><row><cell></cell><cell>negative</cell><cell></cell><cell cols="6">baseline1 baseline2 baseline3 baseline4 baseline5</cell><cell cols="2">improvement</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>0.1175</cell><cell>0.0865</cell><cell cols="2">0.1266</cell><cell>0.1288</cell><cell>0.1085</cell><cell cols="2">mean</cell><cell>stdev</cell></row><row><cell></cell><cell cols="2">TREC median</cell><cell>0.0597</cell><cell>0.0457</cell><cell cols="2">0.0743</cell><cell>0.0677</cell><cell>0.0453</cell><cell cols="2">-48.49%</cell><cell>2.66%</cell></row><row><cell></cell><cell>positive</cell><cell></cell><cell cols="6">baseline1 baseline2 baseline3 baseline4 baseline5</cell><cell cols="2">improvement</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>0.1364</cell><cell>0.0951</cell><cell cols="2">0.1376</cell><cell>0.1532</cell><cell>0.1229</cell><cell cols="2">mean</cell><cell>stdev</cell></row><row><cell></cell><cell cols="2">TREC median</cell><cell>0.0953</cell><cell>0.0547</cell><cell cols="2">0.0955</cell><cell>0.0973</cell><cell>0.0708</cell><cell cols="2">-36.79% 17.48%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" coords="11,53.76,270.24,502.18,18.51"><head>Table 20 : Median negative and positive MAP over each of the 5 standard baselines and median average improvement for the TREC 2008 topics.</head><label>20</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,321.24,657.36,234.46,8.07;2,316.80,666.36,239.12,8.07;2,316.80,675.36,35.18,8.07"><p>The TREC distributed opinion-finding medians for each topic are computed over all runs (baselines + opinion-finding runs, i.e. 191+41 =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="232" xml:id="foot_1" coords="2,370.56,675.36,185.29,8.07;2,316.80,684.24,239.06,8.07;2,316.80,693.24,238.88,8.07;2,316.80,702.24,239.14,8.07;2,316.80,711.24,108.82,8.07"><p>runs). We consider that including the baseline runs in the computation of the medians would be inappropriate, as these baseline runs were not intended to retrieve opinionated documents. In addition, when the number of runs is even (e.g. 232), TREC computes the "upper median".</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The description of system runs are based on paragraphs contributed by the participating groups. Thanks are also due to the 11 groups who created and assessed this year's blog distillation task topics. Finally, we are grateful to <rs type="person">Rodrygo Santos</rs> for handling the blog distillation task relevance assessments system and for various editing help with the overview paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach of Fields</head><note type="other">Mix</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,58.25,279.84,96.95,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,68.76,295.08,223.00,8.07;13,68.76,305.52,203.40,8.07;13,68.76,315.96,42.44,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,201.02,295.08,90.73,8.07;13,68.76,305.52,36.41,8.07">Overview of TREC-2007 Blog track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,120.48,305.52,96.58,8.07">Proceedings of TREC-2007</title>
		<meeting>TREC-2007<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,68.76,327.36,219.20,8.07;13,68.76,337.92,198.52,8.07;13,68.76,348.36,140.52,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,68.76,337.92,129.35,8.07">Overview of TREC-2006 Blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,213.59,337.92,53.68,8.07;13,68.76,348.36,40.59,8.07">Proceedings of TREC-2006</title>
		<meeting>TREC-2006<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,68.76,359.76,213.87,8.07;13,68.76,370.20,223.41,8.07;13,68.76,380.76,205.82,8.07;13,68.76,391.20,104.86,8.07;13,68.76,402.12,178.18,7.05;13,68.76,412.56,194.38,7.05" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<ptr target="http://www.dcs.gla.ac.uk/˜craigm/publications/macdonald06creating.pdf" />
		<title level="m" coord="13,170.49,359.76,112.14,8.07;13,68.76,370.20,186.88,8.07">The TREC Blog06 Collection : Creating and Analysing a Blog Test Collection DCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="13,68.76,423.60,218.44,8.07;13,68.76,434.04,189.94,8.07;13,331.80,281.88,221.41,8.07;13,331.80,292.32,111.32,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,107.04,434.04,151.67,8.07;13,331.80,281.88,109.45,8.07">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,457.08,281.88,96.14,8.07;13,331.80,292.32,33.35,8.07">Proceedings of OSIR&apos;2006 Workshop</title>
		<meeting>OSIR&apos;2006 Workshop<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,303.84,216.51,8.07;13,331.80,314.28,215.72,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,479.18,303.84,69.13,8.07;13,331.80,314.28,18.89,8.07">On the TREC Blog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,366.48,314.28,102.64,8.07">Proceedings of ICWSM-2008</title>
		<meeting>ICWSM-2008<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,325.68,206.03,8.07;13,331.80,336.12,185.92,8.07;13,331.80,346.68,108.46,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,504.73,325.68,33.10,8.07;13,331.80,336.12,116.38,8.07">Limits of opinion-finding baseline systems</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,464.04,336.12,53.68,8.07;13,331.80,346.68,41.17,8.07">Proceedings of SIGIR-2008</title>
		<meeting>SIGIR-2008<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,358.08,206.32,8.07;13,331.80,368.52,174.82,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,379.34,358.08,144.57,8.07">Evaluation by highly relevant documents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,331.80,368.52,97.04,8.07">Proceedings of SIGIR-2001</title>
		<meeting>SIGIR-2001<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,380.04,196.49,8.07;13,331.80,390.48,212.21,8.07;13,331.80,400.92,187.72,8.07;13,331.80,411.36,140.40,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,331.80,390.48,212.21,8.07;13,331.80,400.92,118.42,8.07">University of Glasgow at TREC2007: Experiments in Blog and Enterprise tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,465.84,400.92,53.68,8.07;13,331.80,411.36,40.59,8.07">Proceedings of TREC-2007</title>
		<meeting>TREC-2007<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,331.80,422.88,201.04,8.07;13,331.80,433.32,223.88,8.07;13,331.80,443.76,42.32,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,466.03,422.88,66.81,8.07;13,331.80,433.32,67.96,8.07">What Should Blog Search Look Like?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,412.32,433.32,90.98,8.07">Proceedings of SSM-2008</title>
		<meeting>SSM-2008<address><addrLine>Napa Valley, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
