<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.64,127.93,248.21,16.30;1,253.08,144.99,109.35,16.30">TREC 2013 Microblog Track Experiments at Kobe University</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,171.69,181.80,58.36,10.45"><forename type="first">Taiki</forename><surname>Miyanishi</surname></persName>
							<email>miyanishi@ai.cs.kobe-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of System Informatics</orgName>
								<orgName type="institution">Kobe University</orgName>
								<address>
									<addrLine>1-1 Rokkodai, Nada-ku</addrLine>
									<postCode>657-8501</postCode>
									<settlement>Kobe</settlement>
									<region>Hyogo</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.50,181.80,65.53,10.45"><forename type="first">Sayaka</forename><surname>Kitaguchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of System Informatics</orgName>
								<orgName type="institution">Kobe University</orgName>
								<address>
									<addrLine>1-1 Rokkodai, Nada-ku</addrLine>
									<postCode>657-8501</postCode>
									<settlement>Kobe</settlement>
									<region>Hyogo</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.38,181.80,53.02,10.45"><forename type="first">Kazuhiro</forename><surname>Seki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of System Informatics</orgName>
								<orgName type="institution">Kobe University</orgName>
								<address>
									<addrLine>1-1 Rokkodai, Nada-ku</addrLine>
									<postCode>657-8501</postCode>
									<settlement>Kobe</settlement>
									<region>Hyogo</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.67,181.80,60.14,10.45"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of System Informatics</orgName>
								<orgName type="institution">Kobe University</orgName>
								<address>
									<addrLine>1-1 Rokkodai, Nada-ku</addrLine>
									<postCode>657-8501</postCode>
									<settlement>Kobe</settlement>
									<region>Hyogo</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.64,127.93,248.21,16.30;1,253.08,144.99,109.35,16.30">TREC 2013 Microblog Track Experiments at Kobe University</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0E88050C19B7D758558E56B19D51F2F9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach to real-time ad hoc task processing in the TREC 2013 microblog track. The approach uses a concept-based query expansion method based on a temporal relevance model that uses the temporal variation of concepts (e.g. terms or phrases) on microblogs. Our model extends an effective existing word and concept-based relevance models by tracking the concept frequency over time in microblogging services. The experimentally obtained results demonstrate that our concept-based query expansion method improve search performance, especially when using tweet selection feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many people search microblog documents to find temporally relevant information such as breaking news and real-time content <ref type="bibr" coords="1,297.70,392.60,10.07,10.45" target="#b8">[9]</ref>, so that temporal properties (e.g., recency and temporal variations) are important factors for retrieving relevant and informative microblogs <ref type="bibr" coords="1,188.70,415.35,10.27,10.45" target="#b0">[1,</ref><ref type="bibr" coords="1,200.54,415.35,6.84,10.45" target="#b2">3]</ref>. Particularly, query expansion methods based on relevance feedback incorporating the temporal property of words into their models have been demonstrated as effective for improving microblog search performance <ref type="bibr" coords="1,358.47,438.10,10.27,10.45" target="#b6">[7,</ref><ref type="bibr" coords="1,370.32,438.10,6.84,10.45" target="#b7">8]</ref>. These time-based query expansion methods mainly use word frequency in pseudo-relevant documents as lexical information and temporal variations of word frequency as temporal information.</p><p>However, such word-based pseudo-relevance feedback (PRF) methods result in limited retrieval effectiveness for retrieving. The fundamental reason is that words have semantic ambiguity. Furthermore, word frequency often points out different time-ranges in which crowds of people are interested.</p><p>To overcome the shortcomings of word-based temporal IR, we propose a novel concept weighting scheme based on the temporal relevance model. Our approach uses concept (e.g. terms, or phrases) as with existing concept importance weighting methods <ref type="bibr" coords="1,462.01,540.48,10.27,10.45" target="#b3">[4,</ref><ref type="bibr" coords="1,143.22,551.85,7.90,10.45" target="#b4">5]</ref> because concepts generally have more discriminative power than words. The proposed method provides a unified framework for weighting concepts using both lexical and temporal information.</p><p>To clarify differences between the existing methods and the proposed one, Table <ref type="table" coords="1,467.54,585.98,4.74,10.45" target="#tab_0">1</ref> contrasts words and concepts suggested by a standard word-based PRF method <ref type="bibr" coords="1,458.86,597.35,10.07,10.45" target="#b1">[2]</ref>, wTRM, a standard concept-based lexical PRF method, cTRM (Lexical) that is equal to Latent Concept Expansion (LCE) <ref type="bibr" coords="1,290.26,620.10,10.07,10.45" target="#b4">[5]</ref>, and our proposed concept-based temporal PRF method using only temporal information, cTRM (Temporal), for a topic numbered MB030: "Keith Olbermann new job" used in the TREC microblog track. This topic is related to the news that Keith Theodore Olbermann who is an American sports and political commentator head for the Current TV and become the chief news officer. Table <ref type="table" coords="2,184.28,248.53,4.74,10.45" target="#tab_0">1</ref> clarifies that the word-based PRF method wTRM suggests topic-related words current and tv. However, current and tv often retrieve irrelevant documents at the top because these words appear in many documents. In contrast, concept-based methods cTRM (Lexical) and cTRM (Temporal) suggest exact topic-related concepts: #uw8(olbermann keith), #uw8(current tv), and #8(tv olbermann)<ref type="foot" coords="2,396.66,292.68,3.46,7.63" target="#foot_0">1</ref> . However, cTRM (Lexical) also suggests meaningless concepts: #8(keith keith) and #8(olbermann olbermann). Therefore, we assume that our temporal PRF method cTRM integrating lexical and temporal information for selecting topic-related concepts will be more effective than PRF method using only lexical information (e.g. LCE) as well as the standard word-based PRF method. The remainder of the paper is organized as follows: in Section 2 we describes details of the proposed concept-based relevance model. Experimental settings and results are presented in Section 3. Finally, Section 4 presents conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed method</head><p>Many researchers have reported recently that the concept-based IR method outperformed the word-based one across many tasks <ref type="bibr" coords="2,318.59,437.25,10.27,10.45" target="#b3">[4,</ref><ref type="bibr" coords="2,330.44,437.25,6.84,10.45" target="#b4">5]</ref>. However, these concept weighting approaches do not take account of temporal factors which are important factors for microblog searches. Microblog services often have real-time features by which many microblogs are posted by crowds of people when a notable event occurs. Many reports have described the effectiveness of incorporating such real-time features into PRF methods for microblog search <ref type="bibr" coords="2,241.63,494.13,10.27,10.45" target="#b6">[7,</ref><ref type="bibr" coords="2,253.48,494.13,6.84,10.45" target="#b7">8]</ref>. Therefore, we proposed a concept-based PRF method that combines lexical and temporal information of concepts.</p><p>We assume that the proposed concept-based relevant model P(c|R) derives from both lexical and temporal information sources, and following the notion of bag-ofconcepts, each concept c is sampled identically and independently from a lexical distribution of pseudo-relevant documents, R l (top M retrieved documents), and a time distribution of ones, R t (top N retrieved documents). Therefore, we have</p><formula xml:id="formula_0" coords="2,213.16,582.47,259.11,51.67">P(c|Q) = D l ∈R l D t ∈R t P(c, D l , D t |Q) = D l ∈R l D t ∈R t P(D l |c, D t , Q)P(c, D t |Q),<label>(1)</label></formula><p>where D l denotes a document from pseudo-relevant documents R l and D t denotes each time (a day in our case) in R t . Then, we applied the simple assumption that the temporal information D t is independent of the lexical information D l , so that D t is dropped from the conditional probability in Eq. 1. Therefore, we have</p><formula xml:id="formula_1" coords="3,204.72,188.36,267.55,54.79">P(c|Q) = D l ∈R l P(D l |c, Q) D t ∈R t P(c, D t |Q) ∝ 1 P(c|Q) D l ∈R t P(c, D l |Q) D t ∈R t P(c, D t |Q).<label>(2)</label></formula><p>Then, we assume that P(c|Q) is a non-negative function and the concept c and that all query concepts are also sampled independently and identically once we choose distributions D l and D t . We have the score function that ranks a concept c in response to query Q as</p><formula xml:id="formula_2" coords="3,143.36,301.56,339.93,57.07">S cTR M (c, Q) rank = D l ∈R l P(D l )P(c|D l ) m i P( qi |D l ) Lexical • D t ∈R t P(D t )P(c|D t ) m i P( qi |D t ) Temporal ,<label>(3)</label></formula><p>where P(c|D l ) is the probability of concept occurrence in a document D and P(c|D t ) denotes the probability of concept occurrence at time t (day in our case). P( qi |D l ) is the probability of a i-th query concept q i under the concept distribution for document D. The maximum likelihood estimator of P( q|D) is</p><formula xml:id="formula_3" coords="3,144.03,401.34,328.24,31.40">P ml (c|D) = f (c;D) c ′ ∈V f (c ′ ;D) . Therein, f (c; D) denotes the number of concept counts of c in document D, c ′ ∈V f (c ′ ; D)</formula><p>is the number of concepts in D where V is the set of all concepts in the vocabulary. In most cases, this probability is applied to smoothing to temper over-fitting using a given collection. Among numerous smoothing methods, the following Dirichlet smoothing <ref type="bibr" coords="3,456.49,450.56,15.79,10.45" target="#b9">[10]</ref> is often used.</p><formula xml:id="formula_4" coords="3,212.66,479.25,259.61,29.16">P(c|D l ) = |D l | |D l | + µ P ml (c|D l ) + µ |D l | + µ P(c|C),<label>(4)</label></formula><p>where µ is the Dirichlet prior and P(c|C) is a uni-gram language model of concept in a corpus C. We assume that concepts appear in a document only once because Twitter messages are very short, so that we approximate P(c|C) ≈ df (c) |C | to avoid pre-computing concept count, where df (c) is the document frequency of concept c and |C| is the number of documents in a corpus. Smoothing the maximum likelihood estimator of the uni-gram language model improves the estimated probabilities.</p><p>In addition, P( qi |D t ) is i-th query concept qi under the concept distribution at time t. To improve our estimates for P(( qi |D t ), we also use Dirichlet smoothing as with the standard query likelihood model in Eq. 4 because the value of query likelihood m i P( qi |D t ) becomes 0 when a query concept qi does not appear over time in R t . We have</p><formula xml:id="formula_5" coords="3,208.71,633.20,263.56,29.16">P(c|D t ) = |D t | |D t | + µ t Pml (c|D t ) + µ t |D t | + µ t P(c|C),<label>(5)</label></formula><p>where Pml (c|D t ) =</p><formula xml:id="formula_6" coords="4,231.53,129.21,84.10,20.06">f (c;D t ) c ′ ∈V f (c ′ ;D t ) , f (c; D t )</formula><p>is the frequency of concept c at time t, |D t | is the total number of concepts at time t, and µ t is a parameter for smoothing.</p><p>Here P(D l ) and P(D t ) are uniform over all the distributions in D l and D t . The value of P(c|D t ) | m | j P( q j |D t ) in Eq. 3 increases when the candidate concept c and query concepts q1 , q2 , . . . , qm were simultaneously appeared in a range. Moreover, using the probabilities of concept occurrence P(c|D t ) derived from document time-stamps of pseudo-relevant documents R t , the PRF model derived from Eq. 3 represents real-time feature of a given topic in microblogging services. Finally, we rank candidate concepts in descending order of the association score S cTR M (c, Q) and use the top k concepts for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Evaluation data. We evaluated our proposed method using the test collection for the TREC 2013 microblog track<ref type="foot" coords="4,250.37,316.51,3.46,7.63" target="#foot_1">2</ref> . This collection consists of about 240 million tweets sampled between February 1 and March 31, 2013, for 110 search topics. To evaluate any IR system, relevance judgment is applied to the whole tweet set of each topic. The relevance levels are categorized into irrelevant (labeled 0), minimally relevant (labeled 1), and highly relevant (labeled 2). We separately evaluate our method with respect to allrel and highrel query sets: allrel has both minimally relevant and highly relevant tweets as relevant documents and highrel has only highly relevant tweets. Table <ref type="table" coords="4,418.98,386.10,4.74,10.45" target="#tab_1">2</ref> summarizes topic numbers that we used in our experiments.</p><p>Microblog search settings. We retrieved tweets posted before the specific time associated with each topic by Twitter tools <ref type="foot" coords="4,300.96,432.01,3.46,7.63" target="#foot_2">3</ref> with the following setting. All queries and tweets are stemmed using the Porter stemmer without stop-word removal. They are case-insensitive.</p><p>To retrieve documents, we used a basic query likelihood model with Dirichlet smoothing <ref type="bibr" coords="4,158.55,479.14,15.79,10.45" target="#b9">[10]</ref> (we set smoothing parameter µ = 2500) implemented by the Lucene search engine <ref type="foot" coords="4,168.50,489.17,3.46,7.63" target="#foot_3">4</ref> as the language model for IR (LM) and all PRF methods used this LM as initial search results. For temporal smoothing parameter µ t in Eq. 5, we set µ t = 2500 when retrieving documents. We retrieved 3000 tweets through Twitter API.</p><p>Then, we filtered out all non-English retrieved tweets using a language detector with infinity-gram, called ldig<ref type="foot" coords="4,238.81,534.97,3.46,7.63" target="#foot_4">5</ref> except for a specific run, kobeU. Retweets<ref type="foot" coords="4,411.16,534.97,3.46,7.63" target="#foot_5">6</ref> were regarded as irrelevant for evaluation in the TREC microblog track; however, we used retweets except in a final ranking of tweets because a set of retweets is a good source that might contain topic-related words for improving Twitter search performance. In accordance with the track's guidelines, all retweets including the string "RT" at the beginning of the tweet were removed from the final ranking. Finally, we used the top 1000 results for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IR Models</head><p>We introduce the setting of the proposed PRF method based on a concept-based temporal relevance model. The concept-based method uses the combination of one or two words as a candidate concept. All concepts are extracted from tweets based on full dependence, which assumes that dependence exists between all query terms <ref type="bibr" coords="5,430.17,297.61,10.07,10.45" target="#b3">[4]</ref>. We denote the proposed PRF method combining lexical and temporal information of concepts as cTRM.</p><p>Moreover, to assess the effectiveness of incorporating concept into the retrieval model, we also proposed a word-based temporal relevance model, wTRM, that incorporates lexical and temporal information of words into its relevance model. wTRM uses only a single word as a concept in Eq. 3: wTRM does not consider multi-term concepts that combine more than two words.</p><p>For some runs, we used a two-stage relevance feedback approach which conducts PRF after relevance feedback with manual tweet selection called Tweet Selection Feedback; TSF. Miyanishi et al. <ref type="bibr" coords="5,257.09,411.36,10.27,10.45" target="#b5">[6,</ref><ref type="bibr" coords="5,268.94,411.36,7.90,10.45" target="#b7">8]</ref> report that this two-stage relevance feedback approaches considerably improve search result relevance for microblog search. We manually select relevant tweets from initial search results among top L tweets for TSF by each topic. We set L to 30. If relevant tweets do not exist among initial search results, we use the original user query for tweet selection feedback. All selected tweets were stopped using common stop words list with URL and mention (e.g. @trecmicroblog) removal. In the new query, the selected tweet and the original query were weighted as 1 : 1 for each method using TSF. After tweet selection feedback, we conduct the proposed query expansion method using concept-based temporal relevance model, cTRM.</p><p>For query expansion methods based on PRF, we select candidate words or concepts among the top M tweets retrieved using the original query after removing the uniform resource locators (URLs), and user names starting with '@' or special characters (!, @, #, ', ", etc.). All query terms, candidates of words and concepts, and tweets are decapitalized. The candidates of words and concepts include no common stop-words. Then, we select k words or concepts among candidates in descending order of the concept weighting score such as S cTR M (c, Q). We use the normalized score for concept weighting. For example, the weight of i-th concept is</p><formula xml:id="formula_7" coords="5,320.52,590.96,75.05,18.22">s i = S cTR M (c i ,Q) k j S cTR M (c j ,Q)</formula><p>when using cTRM.</p><p>Finally, we combined the expanded concepts of PRF with their weight and the original query as an expanded query (and selected tweets if any exists). They were weighted with 1 : 1. Fig. <ref type="figure" coords="5,204.48,631.48,4.74,10.45">1</ref> shows the example of query expansion we used. In this figure, "water shortage" is an original query, "water shortage defunct bore wells ..." is a single  relevant tweet, and others are expand concepts. In our study, we let λ 1 , λ1 = 0.5 and λ 2 , λ2 = 0.5 when using TSF; λ 2 = 1 and λ2 = 0, otherwise. For all query expansion methods, we tuned the parameters the number of pseudorelevance feedback documents (i.e. M), the number of pseudo-relevant documents as temporal information (i.e. N ), and the number of expansion words (i.e. k). Values of the these parameters are optimized for best performance of precision at 30 on training data. For example, we tune parameters of the IR model using TREC 2011 and 2012 microblog track dataset. Finally, we presents the description of our methods in Table <ref type="table" coords="6,465.17,420.79,3.55,10.45" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Measure</head><p>To evaluate retrieval effectiveness, we used precision at 30 (P@30) and average precision (AP). P@30 was the official microblog track metric in 2011 and both P@30 and AP were used in TREC 2012. Moreover, in TREC 2012 microblog track, "highly relevant" tweets are the required level of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>We summarized the results of our experiments in Table <ref type="table" coords="6,353.40,540.48,3.55,10.45" target="#tab_3">4</ref>, which shows that kobeRMU that uses our proposed temporal PRF method outperformed kobeU for both allrel and highrel data sets in P@30 that is an objective measure for the parameter estimation. Table <ref type="table" coords="6,166.43,574.60,4.74,10.45" target="#tab_3">4</ref> also shows kobeTSFRM that uses TSF and the concept-based temporal query expansion method cTRM performs better than kobeTSFRMU that applies URL filtering to the results of kobeTSFRM when retrieving allrel documents. In contrast, ko-beTSFRMU outperformed kobeTSFRM when retrieving highrel documents. These results suggest that URL filtering is harmful for retrieving relevant tweets; however it is effective for retrieving highly relevant and informative tweets. Moreover, kobeTS-FRMU markedly outperformed kobeRMU, suggesting that relevance feedback using </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper has presented a concept-based query expansion method based on a temporal pseudo-relevance feedback (PRF) model. Our experimentally obtained results on a datasets used in TREC 20013 microblog track demonstrate that incorporating temporal information of concepts into the query expansion method improves retrieval performance. We demonstrated that using our temporal PRF method combined with URL filtering can be useful for retrieving highly relevant documents. Moreover, our twostage relevance feedback that consists of tweet selection feedback and temporal PRF considerably improved retrieval performance for microblog search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,143.22,220.71,329.04,10.19;6,143.22,231.41,26.77,9.40"><head>#weight( λ 1 #Fig. 1 .</head><label>11</label><figDesc>Fig. 1. Example of query expansion of topic "water shortage" from TREC microblog track queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,143.22,128.63,329.03,77.79"><head>Table 1 .</head><label>1</label><figDesc>Example of word and concept about a topic MB030 suggested by a word-based model (wTRM) and a concept one (cTRM).</figDesc><table coords="2,173.08,154.71,269.32,51.71"><row><cell>wTRM</cell><cell>cTRM (Lexical)</cell><cell>cTRM (Temporal)</cell></row><row><cell>0.0642 tv</cell><cell>0.0132 #8(keith keith)</cell><cell>0.0162 #8(current tv)</cell></row><row><cell>0.0642 current</cell><cell>0.0121 #8(olbermann keith)</cell><cell>0.0162 #8(tv olbermann)</cell></row><row><cell cols="3">0.0561 countdown 0.0121 #8(olbermann olbermann) 0.0162 #8(tv keith)</cell></row><row><cell>0.0513 home</cell><cell>0.0098 #8(current keith)</cell><cell>0.0154 tv</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,186.55,128.63,242.39,67.36"><head>Table 2 .</head><label>2</label><figDesc>Summary of TREC collections and topics used for evaluation.</figDesc><table coords="5,221.93,144.24,169.50,51.76"><row><cell>Name</cell><cell cols="2">Type #Topics Topic Numbers</cell></row><row><cell cols="2">TREC 2013 allrel</cell><cell>60 111-170</cell></row><row><cell></cell><cell cols="2">highrel 56 111,112, 114-124,</cell></row><row><cell></cell><cell></cell><cell>126, 127, 129-148,</cell></row><row><cell></cell><cell></cell><cell>150-170</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,202.20,247.00,208.95,77.83"><head>Table 3 .</head><label>3</label><figDesc>Descriptions of IR models</figDesc><table coords="6,202.20,262.60,208.95,62.22"><row><cell cols="2">Method Description</cell></row><row><cell>LM</cell><cell>Query likelihood model with Dirichlet smoothing</cell></row><row><cell cols="2">wTRM Word-based temporal pseudo-relevance feedback</cell></row><row><cell cols="2">cTRM Concept-based temporal pseudo-relevance feedback</cell></row><row><cell>TSF</cell><cell>Tweet selection feedback</cell></row><row><cell>URL</cell><cell>URL filtering</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,143.22,128.63,329.06,117.71"><head>Table 4 .</head><label>4</label><figDesc>Overall results in our official runs. The best results per column are marked in boldface.</figDesc><table coords="7,356.85,146.33,76.61,6.94"><row><cell>allrel</cell><cell>highrel</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,152.71,633.20,319.55,9.40;2,152.71,643.63,63.85,9.40"><p>#8uw(•) denotes an unordered window in which all words must appear within a window of 8 terms in any order.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,152.71,600.05,254.17,9.40"><p>https://github.com/lintool/twitter-tools/wiki/TREC-2013-Track-Guidelines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,152.71,610.95,130.40,9.40"><p>https://github.com/lintool/twitter-tools</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,152.71,621.84,101.07,9.40"><p>http://lucene.apache.org/core/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,152.71,632.73,100.22,9.40"><p>https://github.com/shuyo/ldig</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,152.71,643.63,240.50,9.40"><p>Tweets re-posted by another user to share information with other users</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,151.52,410.90,320.76,10.45;7,159.82,422.27,187.04,10.45" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,313.19,410.90,159.10,10.45;7,159.82,422.27,76.36,10.45">Improving retrieval of short texts through document expansion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Organisciak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fenlon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,256.55,422.27,22.60,10.45">SIGIR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="911" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,433.65,320.76,10.45;7,159.82,445.02,33.18,10.45" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,265.96,433.65,128.73,10.45">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,417.08,433.65,22.60,10.45">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,456.40,318.73,10.45" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,231.68,456.40,166.69,10.45">Temporal relevance profiles for tweet search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,418.49,456.40,19.31,10.45">TAIA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,467.78,320.76,10.45;7,159.82,479.15,90.30,10.45" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,256.18,467.78,198.62,10.45">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,159.82,479.15,22.60,10.45">SIGIR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,490.53,320.76,10.45;7,159.82,501.90,103.20,10.45" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,260.76,490.53,208.25,10.45">Latent concept expansion using Markov random fields</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.71,501.90,22.60,10.45">SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,513.28,320.76,10.45;7,159.82,524.65,132.74,10.45" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,303.20,513.28,169.09,10.45;7,159.82,524.65,58.62,10.45">TREC 2012 microblog track experiments at Kobe university</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miyanishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,238.32,524.65,21.28,10.45">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,536.03,320.76,10.45;7,159.82,547.40,242.49,10.45" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,298.48,536.03,173.80,10.45;7,159.82,547.40,135.22,10.45">Combining recency and topic-dependent temporal variation for microblog search</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miyanishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
		<editor>ECIR.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="331" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,558.78,320.76,10.45;7,159.82,570.15,165.59,10.45" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,309.48,558.78,162.80,10.45;7,159.82,570.15,55.36,10.45">Improving pseudo-relevance feedback via tweet selection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miyanishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
		<editor>CIKM.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.52,581.53,320.76,10.45;7,159.82,592.90,189.26,10.45" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,304.07,581.53,168.22,10.45;7,159.82,592.90,83.58,10.45">#TwitterSearch: a comparison of microblog search and web search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>WSDM</publisher>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,151.12,604.28,321.15,10.45;7,159.82,615.34,200.75,11.32" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,243.46,604.28,228.82,10.45;7,159.82,615.65,87.01,10.45">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,253.58,615.65,20.90,10.45">TOIS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
