<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,65.97,57.98,480.06,19.66;1,81.41,78.68,449.15,19.66">Pitt at TREC 2013: Different Effects of Click-through and Past Queries on Whole-session Search Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,148.43,101.04,59.72,12.50"><forename type="first">Jiepu</forename><surname>Jiang</surname></persName>
							<email>jpjiang@cs.umass.edu</email>
						</author>
						<author>
							<persName coords="1,404.10,101.04,55.90,12.50"><forename type="first">Daqing</forename><surname>He</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,65.97,57.98,480.06,19.66;1,81.41,78.68,449.15,19.66">Pitt at TREC 2013: Different Effects of Click-through and Past Queries on Whole-session Search Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7CFBF4D92016D7A63197FEB85589649F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Search session</term>
					<term>TREC</term>
					<term>evaluation</term>
					<term>relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Past search queries and click-through information within a search session have been heavily exploited to improve search performance. However, it remains unclear how do these two data source contribute to whole-session search performance due to the lack of reliable evaluation approaches. For example, as pointed out in our last year's report [2], using past search queries as positive relevance feedback information can make search results of the current query similar to previous queries' results. Such issues cannot be disclosed by evaluation metrics such as nDCG@10. Therefore, in this paper, we focus on analyzing the effects of past queries and click-through information on whole-session search performance. We adopted alternative evaluation approaches other than the TREC official ones. We found that past queries may seemingly enhance nDCG@10 by retrieving previously returned results, which is difficult to result in real improvements of wholesession search performance; in comparison, click-through can enhance search performance without sacrificing search novelty, consequently leading to improved search performance across the whole session. However, after appropriate demotion of repeated results, both past queries and click-through can improve search performance while balancing novelty of results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="611.982" lry="791.982"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="611.982" lry="791.982"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="611.982" lry="791.982"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="611.982" lry="791.982"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="791.982" lry="611.982"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="791.982" lry="611.982"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="791.982" lry="611.982"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="791.982" lry="611.982"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Since 2011, the main task of the TREC session track is to improve search performance of a query in a search session using previous user behaviors (such as past search queries and clicked results). However, our studies <ref type="bibr" coords="1,132.53,500.15,9.74,8.72" target="#b1">[1,</ref><ref type="bibr" coords="1,144.38,500.15,7.50,8.72" target="#b2">2]</ref> revoked us to reconsider the validity of this task and its evaluation approach. In both the 2011 and 2012 TREC session track logs, we found that each time reformulating a query, there was no significant change of search performance (as measured by nDCG@10), but substantial difference in the set of results retrieved (with the Jaccard similarity of two queries' top 10 results ranging from 0.1 to 0.2). Due to this fact, we argued that users may expect to find novel search results, instead of simply to improve search performance when they reformulate queries <ref type="bibr" coords="1,281.36,582.93,9.56,8.72" target="#b2">[2]</ref>. Therefore, it may also be problematic to evaluate a system purely by whether or not it can improve search performance of a query in a search session and the magnitude of the improvement.</p><p>Except for the reason stated above, our concern also comes from the observation that a popular approach taken by many groups in the TREC session tracks may sacrifice novelty to improve search performance. The approach is to utilize past queries and clicked results as positive relevance feedback information for the current * The majority of the work was finished when Jiepu Jiang was at School of Information Sciences, University of Pittsburgh.</p><p>query. Our systems in the 2011 and 2012 TREC session tracks <ref type="bibr" coords="1,548.19,186.01,9.74,8.72" target="#b2">[2,</ref><ref type="bibr" coords="1,317.84,196.36,7.50,8.72" target="#b3">3]</ref> as well as many other participants' systems all adopted similar approaches. However, previous search queries and clicked results are probably related to relevant but obsolete information for the users. As found in our last year's TREC report <ref type="bibr" coords="1,493.04,227.41,9.56,8.72" target="#b2">[2]</ref>, search results became more similar to those retrieved by previous queries after applying this approach. Therefore, it is unclear whether or not the improvement of search performance comes from novel relevant information or just some repeated relevant results found also by previous queries. Such issues had not been disclosed in the TREC session track due to its evaluation approaches.</p><p>These two reasons motivate us to explore approaches balancing search performance and novelty of results in a search session, as well as alternative metrics for evaluating systems and analyzing search results at whole-session level. We focus on the following research goals in this year's participation:</p><p>1. Although last year we suspected that the positive relevance feedback approach is problematic, we did not fully investigate whether it is true and how serious it is due to the lack of proper evaluation approaches. Also, it is unclear how such issues affect system performance at whole-session level. Therefore, this year we conducted more detailed analysis of this approach based on alternative metrics indicating whole-session search performance. We introduce this approach and our findings in section 2 and 3. Results indicate that using previous queries as positive relevance feedback information may largely reduce the novelty of search results to enhance metrics such as nDCG@10. In comparison, using click-through as positive relevance feedback consistently improves search performance without loss of novelty of results.</p><p>2. Last year we proposed an approach to demote the rankings of the repeatedly occurred results. However, this approach only considered the results repetition issue but cannot help with the content novelty in a search session. Therefore, we propose another approach trying to rank results based on whole-session relevance (run "KM1" and "KM1N"). Unfortunately, it did not perform well in the TREC 2013 session track evaluation. Due to the lack of qrels data at the time of finalizing our report, we do not analyze this approach in details.</p><p>The rest of the paper introduces our approaches, experiments, and findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">POSITIVE RELEVANCE FEEDBACK USING PAST SEARCH BEHAVIORS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Model</head><p>The approaches adopted by the Pitt group in the 2011 and 2012 TREC session track <ref type="bibr" coords="2,131.96,80.59,9.76,8.72" target="#b2">[2,</ref><ref type="bibr" coords="2,145.37,80.59,7.50,8.72" target="#b3">3]</ref> are variants of the "context-sensitive relevance feedback" approach <ref type="bibr" coords="2,167.27,90.94,9.57,8.72" target="#b8">[8]</ref>. This approach adopts the KL-Divergence language modeling framework <ref type="bibr" coords="2,212.40,101.27,9.76,8.72">[5,</ref><ref type="bibr" coords="2,224.89,101.27,7.50,8.72" target="#b9">9]</ref> for retrieval and highlights a query language model combining the current search query, past search queries, and click-through documents. In our study, we adopt this approach as an example of using past search behaviors for positive relevance feedback due to its popularity 2 . Shen et al. <ref type="bibr" coords="2,95.54,153.02,10.50,8.72" target="#b8">[8]</ref> proposed four query model estimation methods. In our experiments, we adopt the "FixInt" method because both Shen et al. and we found that it outperforms the other methods.</p><p>Let qk be the kth query in a search session, FixInt estimates query language model θk as Eq(1): P(w|qk) is the current query's MLE model; P(w|Hc) and P(w|Hq) are, respectively, relevance feedback models estimated based on click-through documents and previous queries. Eq <ref type="bibr" coords="2,94.53,229.46,12.30,8.72" target="#b2">(2)</ref> and Eq(3) show details of Hc and Hq: Ci is the concatenation of all clicked documents' summaries returned by qi; P(w|qi) is the ith query's MLE model. Parameter α is the weight of the current query in the FixInt query language model.</p><formula xml:id="formula_0" coords="2,59.29,276.11,234.31,68.35">( ) ( ) ( ) ( ) ( ) ( ) | | 1 | 1 | k k c q P w P w q P w H P w H θ α α β β   = + - + -   (1) ( ) ( ) 1 1 1 | | 1 k c i i P w H P w C k - = = -∑ (2) ( ) ( ) 1 1 1 | | 1 k q i i P w H P w q k - = = -∑ (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Demoting Repeated Results</head><p>We assume that the usefulness of a result for the user degrades each time viewing the result or its snippet. Therefore, repeatedly occurred results within a search session should be demoted in a rank list. We demote the rankings of the repeatedly results by P(d|s), which can be explained as: the probability that the user will still be interested in examining the document d provided the session search history s, which typically includes past search queries and results. We explained this approach in details in our previous studies <ref type="bibr" coords="2,272.61,436.95,9.74,8.72" target="#b1">[1,</ref><ref type="bibr" coords="2,284.31,436.95,6.52,8.72" target="#b2">2]</ref>.</p><p>We assume the following user behaviors:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The user examines results by sequence from top to bottom. The user will always examine the first result in a result page. After examined each result, the user has probability p to continue examining the next one, and probability 1 -p to stop (either to reformulate a new query for search or to terminate the current session). This browsing model is similar to the one adopted in rank-biased precision <ref type="bibr" coords="2,191.43,528.37,9.57,8.72" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Each time the user examined a result, it has probability β that the result will lose its attractiveness to the user in the rest of the search session.</p><p>According to these assumptions, as in Eq(4), a document d can keep its attractiveness if and only if it did not lose attractiveness in any of the previous searches. In Eq(4): R (i) refers to the results for the ith query in the session (assuming q is the nth query); Pexamine(d|R (i) ) is the probability that d will be examined when the user browses results R (i) , as calculated in Eq(5); rank(d, i) is the rank of d in R (i) .</p><p>According to this model, a document will be demoted to a larger magnitude if it was retrieved by many of the previous queries and/or it was ranked at top positions.</p><p>2 By the time of finalizing our report, Shen et al.'s article <ref type="bibr" coords="2,237.62,699.79,9.34,7.74" target="#b8">[8]</ref> has been cited 329 times according to Google Scholar.</p><p>( )</p><formula xml:id="formula_1" coords="2,357.54,56.13,200.42,55.90">1 ( ) examine 1 ( | ) 1 1 ( | ) n i i P d s P d R β - = = - -⋅ ∏ (4) ( , ) 1 ( ) ( ) examine ( ) ( | ) 0 rank d i i i i p d R P d R d R -  ∈ =  ∉  (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Alternative Evaluation Approach</head><p>We use the TREC session track 2012 datasets for evaluation. The dataset includes static search session logs and whole-session level relevance judgments. A static search session is the search history of a real user in an interactive search system, including the users' search queries, click-through, and other information. For each static search session, whole-session level relevance judgments are provided in the datasets: annotators judged documents regarding whether or not they are relevant to the topic or task underlying the search session (instead of an individual query).</p><p>The past TREC session tracks evaluated participant systems based on nDCG@10 of the last queries in each session. In comparison, we adopted the following experiment procedure to study the wholesession search performance. Let {q1, q2, … , qn} be a static search session in the dataset. We iteratively produce results for q1, q2, … , qn using FixInt. For each qi, we use the past queries and clickthrough (if any) in the static search session log as positive relevance feedback information to produce search results. For q1, FixInt downgrades to query likelihood model.</p><p>After generated results for each query in a static session, we calculate the following measures:</p><p>(1) nDCG@10 (macro-average). Let {S1, S2, … , Sm} be m static search sessions in a dataset, and {Ri1, Ri2, … , Rin} be the results of the n queries in session Si. We calculate the macro-average nDCG@10 of the dataset (referred to as nDCG@10) as follows:</p><p>( )</p><formula xml:id="formula_2" coords="2,365.77,407.09,129.69,22.74">1 2 1 1 @10 1 m n ij i j nDCG R m n = =   ⋅ ⋅   -   ∑ ∑</formula><p>Note that we do not count the first query of each session because for the first query there is no search history information available (and here we mainly hope to examine the effect of the past search history to search performance).</p><p>(2) nsDCG@10 (normalized session DCG). For a static search session, nsDCG@10 concatenates the top 10 results of each query and evaluates the performance of the concatenated list of results. Please refer to <ref type="bibr" coords="2,376.97,517.03,10.49,8.72" target="#b4">[4]</ref> for details. The same parameters have been adopted in this study.</p><p>(3) Instance recall (instRec). This is a variant of a major metric adopted in the early TREC interactive tracks <ref type="bibr" coords="2,478.03,554.09,9.56,8.72" target="#b7">[7]</ref>. In previous TREC interactive tracks, annotators identified relevant instances of each topic and marked up the occurrences of relevant instances in documents. The metric "instance recall" was originally calculated as the proportion of relevant instances covered by the search results over all the identified instances. Here we calculate a similar measure by considering each single relevant document as a unique instance.</p><p>Let {Di} be the top 10 results of qi, and DR be the set of judged relevant documents. We concatenate the top 10 results of each query in the session as a whole set of retrieved documents (DF).</p><p>Then, we calculate instance recall (instRec) of the session as the proportion of DR covered by DF.</p><formula xml:id="formula_3" coords="3,109.29,82.80,126.37,23.13">i 1 { } n F i D D = =  instRec F R R D D D = </formula><p>(4) Jaccard similarity. For a static session, we calculate for each unique pair of queries the Jaccard similarity of the pair of queries' top 10 results. Then, we calculate the macro-average value for each unique pair of queries across all search sessions. Although jaccard similarity is not a metric of search performance, it can help us analyze the novelty of search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>We separately examine the effects of past queries and clicked results to whole-session search performance. Figure <ref type="figure" coords="3,242.30,207.58,4.50,8.72">1</ref> and Figure <ref type="figure" coords="3,289.58,207.58,4.50,8.72">2</ref> show the whole-session search performance of FixInt model using sorely past queries or clicked results with different weights α. Figure <ref type="figure" coords="3,81.86,238.63,4.50,8.72">3</ref> and Figure <ref type="figure" coords="3,135.90,238.63,4.50,8.72" target="#fig_1">4</ref> further shows the whole-session search performance of FixInt using past queries or click-through results after demotion of repeated results.</p><p>Our results indicate that:</p><p>(1) As we suspected in <ref type="bibr" coords="3,138.60,288.02,9.57,8.72" target="#b2">[2]</ref>, past queries can lead to serious decline of search novelty by making the results of the current query similar to previous queries' results. As shown in Figure <ref type="figure" coords="3,233.48,308.72,3.38,8.72">1</ref>, in all types of tasks, the average Jaccard similarity of top 10 results can be increased from around 0.3 (α = 1.0, using sorely the user query) to around 0.8 (α = 0, using sorely the past queries). Whenever we increase the weight of past queries, there will be increase of the jaccard similarity.</p><p>When nDCG@10 reaches the peak value, although we achieve 10% -20% increase in nDCG@10, there is also 0.1 -0.2 increase of jaccard similarity. In addition, instance recall dropped from 0.088 (α = 1.0) to 0.082 (α = 0.5) in the 2012 dataset (counting all types of tasks). This makes it difficult to assess whether there is a true improvement of search performance, because the increase of nDCG@10 may come from previously retrieved relevant results.</p><p>Overall we found that past queries have no apparent effect of improving whole-session search performance such as instance recall. Average nDCG@10 of queries does not consider the novelty of search results and therefore cannot disclose the drop of novelty in FixInt. As shown in Figure <ref type="figure" coords="3,165.33,492.66,3.38,8.72">1</ref>, instRec can at most be increased from 0.0881 (α = 1.0) to 0.0896 (α = 0.8). We also did not observe that FixInt performed differently in any type of tasks.</p><p>(2) In comparison, our results suggest that click-through is a valuable relevance feedback information for improving search performance without sacrificing novelty of results. As shown in Figure <ref type="figure" coords="3,80.21,558.76,3.38,8.72">2</ref>, using click-through documents, the jaccard similarity of results will at most increase by about 0.1 (comparing to the increase from about 0.3 to 0.8 when using past queries).</p><p>In all types of tasks, click-through can increase nDCG@10 by 10% -20%. In addition, instRec can also be increased by 10%, from 0.088 (α = 1.0) to 0.101 (α = 0.5) (counting all types of tasks). When instRec reaches the peak value, there are also about 10% increase of nDCG@10, indicating that click-through may improve the ranking of relevant documents without sacrificing novelty of results, which result in performance improvement all over the search session (as indicated from instRec).</p><p>( (4) Our approach of demoting repeated results leads to slight decrease of nDCG@10 but apparent increase of instRec. After selecting appropriate parameters, it can achieve improved search performance balancing nDCG@10 and novelty of results. For example, as shown in Figure <ref type="figure" coords="3,429.64,197.57,4.50,8.72">3</ref> (FixInt with past queries), when setting both p and β to 0.5, we can increase nDCG@10 from 0.252 (α = 0 and do not demote repeated results) to 0.272 (α = 0.8, p = 0.5, β = 0.5) and at the same time increase instRec from 0.088 to 0.106. This indicates that past queries should be combined with proper approaches demoting repeated results in order to balance search performance and novelty of results.</p><formula xml:id="formula_4" coords="3,57.50,680.60,7.00,8.72">)<label>3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SUBMITTED RUNS</head><p>Based on our observations, we submitted three groups of runs:</p><p>(1) FixInt28: a FixInt run optimizing macro-average nDCG@10 (α = 0.2, β = 0.8). FixInt28N further applied the ranking discount method we proposed last year to FixInt28.</p><p>(2) FixInt58: a FixInt run optimizing instance recall (α = 0.5, β = 0.8). FixInt58N further applied the ranking discount method we proposed last year to FixInt58.</p><p>(3) KM1 and KM1N: new retrieval models aiming at whole-session relevance. Unfortunately, this new approach did not work well. Due to the lack of qrels at the time of finalizing our report, we do not examine details of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>We evaluated the effects of using past queries and click-through as positive relevance feedback information on whole-session search performance. Our results indicate that it is risky to utilize past queries because we may easily sacrifice novelty of results to enhance search performance. In comparison, click-through seems to be a more valuable resource to balance search performance and novelty of results. However, after demoted repeated results in a search session, we can balance search performance and novelty of results using either past queries or click-through effectively. This also indicates that it is very necessary to demote repeated results in a search session.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,116.01,474.79,561.44,8.72;5,76.21,485.15,641.05,8.72"><head>Figure 1 .Figure 2 .Figure 3 .</head><label>123</label><figDesc>Figure 1. The weight of past queries as positive relevance feedback information in FixInt and the corresponding whole-session search performance (the greater the value of α, the smaller the weight of past queries in FixInt; α = 1.0 means only using user query for ranking, and α = 0 means sorely using past queries).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,48.87,492.14,695.63,8.72;8,70.40,502.51,652.57,8.72;8,37.85,466.80,7.59,5.63"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The weight of click-through as positive relevance feedback information in FixInt (after demoting repeated results) and the corresponding whole-session search performance (the greater the value of α, the smaller the weight of click-through in FixInt; α = 1.0 means only using user query for ranking, and α = 0 means sorely using click-through). 0.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,54.00,680.60,240.12,39.77"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note coords="3,97.63,680.60,196.44,8.72;3,54.00,690.95,240.10,8.72;3,54.00,701.30,240.12,8.72;3,54.00,711.65,29.23,8.72"><p>shows the correlation (Pearson's r) of various metrics on different parameter values of α and β. Results indicate that nDCG@10 &amp; nsDCG@10 have slight negative correlation with instRec.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,336.17,56.43,203.43,83.18"><head>Table 1 . Pearson's correlation of metrics on different parameter values of α and β.</head><label>1</label><figDesc></figDesc><table coords="3,342.85,83.38,190.08,56.24"><row><cell></cell><cell cols="2">TREC 2011</cell><cell cols="2">TREC 2012</cell></row><row><cell></cell><cell cols="4">nDCG@10 instRec nDCG@10 instRec</cell></row><row><cell>nDCG@10</cell><cell>1.000</cell><cell>-0.235</cell><cell>1.000</cell><cell>0.245</cell></row><row><cell>nsDCG@10</cell><cell>0.985</cell><cell>-0.244</cell><cell>0.994</cell><cell>0.204</cell></row><row><cell>instRec</cell><cell>-0.235</cell><cell>1.000</cell><cell>0.245</cell><cell>1.000</cell></row><row><cell>avgJaccard</cell><cell>0.413</cell><cell>-0.957</cell><cell>0.180</cell><cell>-0.890</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,322.35,562.11,92.40,11.63" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.84,575.08,222.08,8.72;3,335.84,585.43,222.08,8.72;3,335.84,595.78,222.08,8.72;3,335.84,606.13,222.08,8.72;3,335.84,616.50,150.98,8.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,432.45,575.08,125.48,8.72;3,335.84,585.43,218.42,8.72">Contextual evaluation of query reformulations in a search session by user simulation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,335.84,595.78,222.08,8.72;3,335.84,606.13,199.04,8.72">Proceedings of the 21st ACM international conference on Information and knowledge management (CIKM &apos;12)</title>
		<meeting>the 21st ACM international conference on Information and knowledge management (CIKM &apos;12)<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10">2012. Oct. 2012</date>
			<biblScope unit="page">2635</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.84,630.83,222.12,8.72;3,335.84,641.18,222.08,8.72;3,335.84,651.55,49.50,8.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,408.29,630.83,145.95,8.72">On Duplicate Results in a Search Session</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,335.84,641.18,189.47,8.72">Proceedings of the 21st Text REtrieval Conference</title>
		<meeting>the 21st Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2012">2012. 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.84,665.89,222.08,8.72;3,335.84,676.22,222.08,8.72;3,335.84,686.59,49.50,8.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,425.00,665.89,129.55,8.72">Pitt at TREC 2011 session track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,335.84,676.22,222.08,8.72;3,335.84,686.59,16.80,8.72">Proceedings of the 20th Text REtrieval Conference, (TREC 2011</title>
		<meeting>the 20th Text REtrieval Conference, (TREC 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.00,56.43,222.10,8.72;4,72.00,66.78,222.09,8.72;4,72.00,77.15,49.50,8.72" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<title level="m" coord="4,169.20,56.43,124.90,8.72;4,72.00,66.78,222.09,8.72;4,72.00,77.15,21.00,8.72">Session track overview. The 19th Text REtrieval Conference Notebook Proceedings (TREC 2010)</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.00,91.48,222.10,8.72;4,72.00,101.83,222.08,8.72;4,72.00,112.18,222.09,8.72;4,72.00,122.53,222.08,8.72;4,72.00,132.89,177.71,8.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,189.32,91.48,104.78,8.72;4,72.00,101.83,218.86,8.72">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,72.00,112.18,222.09,8.72;4,72.00,122.53,222.08,8.72;4,72.00,132.89,31.00,8.72">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.00,147.22,222.08,8.72;4,72.00,157.57,222.12,8.72;4,72.00,167.94,101.97,8.72" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,196.78,147.22,97.30,8.72;4,72.00,157.57,138.12,8.72">Rank-biased precision for measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,215.93,157.57,78.19,8.72">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2008-12">2008. Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.00,182.27,222.08,8.72;4,72.00,192.62,222.10,8.72;4,72.00,202.99,62.23,8.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,133.02,182.27,161.06,8.72;4,72.00,192.62,44.53,8.72">The TREC interactive track: an annotated bibliography</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,123.39,192.62,144.19,8.72">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="369" to="381" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.00,217.31,222.10,8.72;4,72.00,227.66,222.12,8.72;4,72.00,238.01,222.08,8.72;4,72.00,248.36,222.12,8.72;4,72.00,258.73,121.48,8.72" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,152.32,217.31,141.79,8.72;4,72.00,227.66,88.63,8.72">Context-sensitive information retrieval using implicit feedback</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,169.61,227.66,124.51,8.72;4,72.00,238.01,222.08,8.72;4,72.00,248.36,177.46,8.72">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval -SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval -SIGIR &apos;05<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">2005. Aug. 2005</date>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.00,273.06,222.08,8.72;4,72.00,283.41,222.12,8.72;4,335.84,56.43,222.10,8.72;4,335.84,66.80,212.20,8.72" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,187.65,273.06,106.43,8.72;4,72.00,283.41,218.90,8.72">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,335.84,56.43,222.10,8.72;4,335.84,66.80,147.73,8.72">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
