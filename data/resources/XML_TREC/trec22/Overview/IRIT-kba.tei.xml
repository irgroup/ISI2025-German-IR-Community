<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,122.09,88.06,362.12,12.62;1,147.95,105.99,310.40,12.62">IRIT at TREC Knowledge Base Acceleration 2013: Cumulative Citation Recommendation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,118.85,143.69,51.38,8.74"><forename type="first">Rafik</forename><surname>Abbes</surname></persName>
							<email>abbes@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.54,143.69,98.80,8.74"><forename type="first">Karen</forename><surname>Pinel-Sauvagnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.25,143.69,84.24,8.74"><forename type="first">Nathalie</forename><surname>Hernandez</surname></persName>
							<email>hernandez@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.09,143.69,90.35,8.74"><forename type="first">Mohand</forename><surname>Boughanem</surname></persName>
							<email>boughanem@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT</orgName>
								<address>
									<addrLine>118 route de Narbonne</addrLine>
									<postCode>F-31062</postCode>
									<settlement>Toulouse cedex 9</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,122.09,88.06,362.12,12.62;1,147.95,105.99,310.40,12.62">IRIT at TREC Knowledge Base Acceleration 2013: Cumulative Citation Recommendation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32E20E6D0D2859F72FBB720D8ED05C11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the IRIT lab participation to the Cumulative Citation Recommendation task of the TREC 2013 Knowledge Base Acceleration Track. In this task, we are asked to implement a system which aims to detect "Vital" documents that a human would want to cite when updating the Wikipedia article for the target entity. Our approach is built on two steps. First, for each topic (entity), we retrieve a set of potential relevant documents containing at least one entity mention. These documents are then classified using a supervised learning algorithm to identify which ones are vital. We submitted three runs using different combinations of features. Obtained results are presented and discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the Knowledge Base Acceleration (KBA) track is to help people enrich and update information about entities <ref type="bibr" coords="1,217.89,407.01,9.96,8.74" target="#b0">[1]</ref>. This year, we participated for the first time, in the first task of the KBA track which is called Cumulative Citation Recommendation (CCR). In this task, we are given a list of target entities from Wikipedia and Twitter, and we aim at identifying from the given streamcorpus<ref type="foot" coords="1,204.19,441.30,3.97,6.12" target="#foot_0">1</ref> , which stream items (documents) are worth citing when updating the target entity profile (e.g., Wikipedia article).</p><p>The streamcorpus is about 4.5TB and just over 500 million documents appearing in the period from October 2011 through February 2013. The corpus is divided into 11948 folders, each one represents one distinct hour. As CCR task requires systems to iterate over the hourly directories of data in chronological order, hour by hour, we indexed each single folder separately using the Lucene Software Library<ref type="foot" coords="1,298.13,513.05,3.97,6.12" target="#foot_1">2</ref> . We indexed only the clean visible form (plain text) of the documents without dealing with HTML tags.</p><p>In this task, we are provided with annotations from an early portion of the corpus (from October 2011 through February 2012) as training data. For a given topic (entity), a document can be judged as:</p><p>-Garbage: If it gives no information about target entity.</p><p>-Neutral: It is informative but not citable.</p><p>-Useful: It is useful when building a Knowledge Base entry from scratch, but it does not help to update an already existing Knowledge Base entry. -Vital: It contains a timely information about the entity's current state, actions, or situation. This would be citable when updating a knowledge Base entry.</p><p>Our approach is inspired from previous works at TREC KBA 2012 <ref type="bibr" coords="2,429.63,91.10,20.40,8.74">[2][3]</ref>. It aims at identifying vital documents. We proceed on two steps: we first retrieve a set of relevant documents for the entity query. Then, these documents go through a supervised algorithm to be classified as garbage, neutral, useful or vital.</p><p>In Section 2, we detail our approach. Then, in section 3, we present and discuss results and some perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IRIT Cumulative Citation Recommendation approach</head><p>2.1 First Step: Retrieving candidate documents Queries formulation In the CCR task, we distinguish two types of target entities (topics): Wikipedia topics and Twitter topics that were given in the form of URLs. The first step of our approach consists of retrieving a set of candidate documents for the specified target entity. For that, we need to construct a query text for each topic:</p><p>-For Wikipedia topics, we used DBpedia to get entities variants like the work described in <ref type="bibr" coords="2,118.82,299.25,9.96,8.74" target="#b2">[3]</ref>. We retrieved all entity predicates containing the pattern "/name" or "#label". Then, the corresponding attributes' values are merged to get the final query text. -For Twitter topics, we used the Twitter API to get the full name of the entity, and used it as query text.</p><p>We wanted to get a high recall in this step, so we did not use quotes to search expressions. For example, for the topic http://en.wikipedia.org/wiki/Jeff Tamarkin we submit Jeff Tamarkin as query and not "Jeff Tamarkin" with quotes.</p><p>Retrieval Model To retrieve and score documents, we used Lucene Formula which is based on the Vector Space Model (VSM). The similarity between document d belonging to a folder f, and a query q composed of terms t is evaluated as follow:</p><formula xml:id="formula_0" coords="2,189.29,457.70,227.21,21.98">Score(q, d) = coord(q, d) . t in q tf (t in d) . idf (t) 2</formula><p>Where:</p><p>coord(q, d) is a score factor based on how many of the query terms are found in the specified document tf (t in d) = f requency(t, d) idf (t) = 1 + log #documents in the f older f #documents in the f older f containing t + 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Second step: Supervised classifier</head><p>In this step, we classify the obtained documents from the previous step by applying a supervised learning algorithm using three groups of features described in Table <ref type="table" coords="2,433.63,613.18,3.87,8.74" target="#tab_0">1</ref>. We evaluated some learning algorithms (ZeroR, SVM, Bayesian Network, Random Tree, Random Forest) using the Weka plateform <ref type="bibr" coords="2,213.18,637.09,10.52,8.74" target="#b3">[4]</ref> and we kept the Random Forest algorithm which outperforms the others on the training data. The output of the classifier is the corresponding class (garbage, neutral, useful, vital) of each document with regard to the entity with a confidence score between 0 and 1. For the submission runs we kept only "vital" documents with confidence score above 0.7. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Runs</head><p>We submitted 3 runs for the CCR task. Documents retrieved in the first step are the same for all runs. The difference between runs concerns the type of features used in the classifier:</p><p>-sig irit 1: We use some representative features from each group G1, G2 and G3 (bold features in table <ref type="table" coords="4,192.80,154.86,3.87,8.74" target="#tab_0">1</ref>). -sig irit 2: We use all features of groups G1 and G2.</p><p>-sig irit 3: We use all features of groups G1 and G3. Results show that including features characterizing documents appearing in the same period (G3) degrades the system performance. We are currently performing some experiments regarding the precision obtained after the first step of our approach. It appears that searching the exact match of entities names using expressions improves results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,103.28,122.36,401.29,525.35"><head>Table 1 :</head><label>1</label><figDesc>Number of documents where score &gt; 0.3 and t -timestamp &gt; 24h -nbDoc last72h scr gt03* Number of documents where score &gt; 0.3 and t -timestamp &gt; 72h -nbDoc last168h scr gt03* Number of documents where score &gt; 0.3 and t-timestamp &gt; 168h -nb mention body last24h* Number of entity mentions in the document body where ttimestamp &gt; 24h -nb mention title last24h* Number of entity mentions in the document title where t -Features list divided in 3 groups: Basic features, Document-Centric related features and Timestamp related features. Features with star symbol are inspired from the work described in [2].</figDesc><table coords="3,103.28,122.36,401.29,492.07"><row><cell>G1: Basic features</cell><cell></cell></row><row><cell>-Score*</cell><cell>Similarity (VSM) score between document and query</cell></row><row><cell>-Rank</cell><cell>Rank of the document</cell></row><row><cell cols="2">G2: Document-Centric related features</cell></row><row><cell cols="2">-cos similarity entity doc* Cosine similarity between document and the last version of entity</cell></row><row><cell></cell><cell>profile (before the timestamp)</cell></row><row><cell>-nb mention body</cell><cell>Number of entity mentions in the document body</cell></row><row><cell>-nb mention body 0 20*</cell><cell>Number of entity mentions in the first 20% of document body</cell></row><row><cell>-nb mention body 20 40*</cell><cell>Number of entity mentions in the second 20% of document body</cell></row><row><cell>-nb mention body 40 60*</cell><cell>Number of entity mentions in the third 20% of document body</cell></row><row><cell>-nb mention body 60 80*</cell><cell>Number of entity mentions in the fourth 20% of document body</cell></row><row><cell>-nb mention body 80 100*</cell><cell>Number of entity mentions in the last 20% of document body</cell></row><row><cell>-nb novelty words</cell><cell>Number of words expressing novelty in the document body. We</cell></row><row><cell></cell><cell>defined a list of 44 novelty words for example, create, invent, new,</cell></row><row><cell></cell><cell>buzz, novelty, etc. We supposed that vital documents are likely to</cell></row><row><cell></cell><cell>contain these words.</cell></row><row><cell>-title found</cell><cell>1 if the document has a title, 0 otherwise</cell></row><row><cell>-title mention</cell><cell>1 if the document title mentions the entity, 0 otherwise</cell></row><row><cell cols="2">G3: Timestamp related features</cell></row><row><cell></cell><cell>let t is the timestamp of the current document</cell></row><row><cell>-nbDoc last24h scr gt03*</cell><cell></cell></row><row><cell></cell><cell>timestamp &gt; 24h</cell></row><row><cell>-std dev 0 mention 24h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0 and t -timestamp &gt; 24h</cell></row><row><cell>-std dev 0 mention 72h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0 and t -timestamp &gt; 72h</cell></row><row><cell>-std dev 0 mention 168h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0 and t -timestamp &gt; 168h</cell></row><row><cell>-std dev 1 mention 24h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0.1 and t -timestamp &gt; 24h</cell></row><row><cell>-std dev 1 mention 72h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0.1 and t -timestamp &gt; 72h</cell></row><row><cell>-std dev 1 mention 168h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0.1 and t -timestamp &gt; 168h</cell></row><row><cell>-std dev 3 mention 24h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0.3 and t -timestamp &gt; 24h</cell></row><row><cell>-std dev 3 mention 72h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0.3 and t -timestamp &gt; 72h</cell></row><row><cell>-std dev 3 mention 168h*</cell><cell>Standard Deviation of entity mentions in documents where score &gt;</cell></row><row><cell></cell><cell>0.3 and t -timestamp &gt; 168h</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,101.89,209.25,377.75,140.77"><head>Table 2 :</head><label>2</label><figDesc>Comparison of submitted runs using macro average measures, (cutoff step=10)</figDesc><table coords="4,101.89,209.25,367.64,121.62"><row><cell>3 Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>avg(P)</cell><cell>avg(R)</cell><cell>max(F(avg(P),avg(R)))</cell><cell>Scaled Utility</cell></row><row><cell>sig irit 1</cell><cell>0.121</cell><cell>0.038</cell><cell>0.057</cell><cell>0.252</cell></row><row><cell>sig irit 2</cell><cell>0.147</cell><cell>0.048</cell><cell>0.072</cell><cell>0.255</cell></row><row><cell>sig irit 3</cell><cell>0.078</cell><cell>0.031</cell><cell>0,044</cell><cell>0.254</cell></row><row><cell>TREC Median</cell><cell></cell><cell></cell><cell>0.174</cell><cell></cell></row><row><cell>TREC Max</cell><cell></cell><cell></cell><cell>0.311</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,111.85,663.68,332.69,7.86"><p>http://s3.amazonaws.com/aws-publicdatasets/trec/kba/kba-streamcorpus-2013-v0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,455.79,663.68,38.45,7.86;1,111.85,674.64,140.28,7.86;1,104.63,683.83,3.65,5.24;1,111.85,685.60,103.95,7.86"><p>0-englishand-unknown-language/index.html 2 http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,105.47,480.70,336.33,8.12" xml:id="b0">
	<monogr>
		<ptr target="http://trec-kba.org/trec-kba-2013.shtml" />
		<title level="m" coord="4,114.04,480.70,132.18,7.86">Trec knowledge base acceleration</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.47,491.66,398.94,7.86;4,114.04,502.62,390.37,7.86;4,114.04,513.57,94.48,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,344.04,491.66,160.37,7.86;4,114.04,502.62,37.73,7.86">Lsis-lia at trec 2012 knowledge base acceleration</title>
		<author>
			<persName coords=""><forename type="first">Ludovic</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Bouvier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,169.95,502.62,274.69,7.86">The Twenty-First Text REtrieval Conference (TREC 2012) Notebook</title>
		<meeting><address><addrLine>Gaithersburg (USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-11">november 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.47,524.53,398.94,7.86;4,114.04,535.49,390.37,7.86;4,114.04,546.45,52.76,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,285.03,524.53,219.38,7.86;4,114.04,535.49,167.81,7.86">Jiyin He, Corrado Bosscarino, and Arjen de Vries. Cwi at trec 2012, kba track and session track</title>
		<author>
			<persName coords=""><forename type="first">Samur</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gebrekirstos</forename><surname>Gebremeskel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,292.03,535.49,208.09,7.86">Proceedings of the 21 st Text REtrieval Conference</title>
		<meeting>the 21 st Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,105.47,557.41,398.94,7.86;4,114.04,568.37,390.37,7.86;4,114.04,579.33,64.53,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,153.68,568.37,180.06,7.86">The weka data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,346.77,568.37,99.78,7.86">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
