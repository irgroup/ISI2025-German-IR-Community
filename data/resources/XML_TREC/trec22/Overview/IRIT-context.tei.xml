<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,199.93,96.79,212.73,15.12;1,139.66,118.71,333.09,15.12">IRIT, GeoComp, and LIUPPA at the TREC 2013 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,146.86,152.43,69.08,10.68"><forename type="first">Gilles</forename><surname>Hubert</surname></persName>
							<email>gilles.hubert@univ-tlse3.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Toulouse</orgName>
								<orgName type="institution" key="instit2">IRIT</orgName>
								<address>
									<country>France *</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.82,152.43,100.89,10.68"><forename type="first">Guillaume</forename><surname>Cabanac</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Toulouse</orgName>
								<orgName type="institution" key="instit2">IRIT</orgName>
								<address>
									<country>France *</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.59,152.43,118.20,10.68"><forename type="first">Karen</forename><surname>Pinel-Sauvagnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Toulouse</orgName>
								<orgName type="institution" key="instit2">IRIT</orgName>
								<address>
									<country>France *</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.52,166.37,80.31,10.68"><forename type="first">Damien</forename><surname>Palacio</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Geography</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.49,166.37,103.46,10.68"><forename type="first">Christian</forename><surname>Sallaberry</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">LIUPPA</orgName>
								<orgName type="institution" key="instit2">University of Pau et des Pays de l&apos;Adour</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,199.93,96.79,212.73,15.12;1,139.66,118.71,333.09,15.12">IRIT, GeoComp, and LIUPPA at the TREC 2013 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">066FA9E43F47AB0F14C854A323FAE0AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we give an overview of the participation of the IRIT, GeoComp, and LIUPPA labs in the TREC 2013 Contextual Suggestion Track. Our framework combines existing geo-tools or services (e.g., Google Places, Yahoo! BOSS Geo Services, PostGIS, Gisgraphy, GeoNames) and ranks results according to features such as context-place distance, place popularity, and user preferences. We participated in the Open Web and ClueWeb12 sub-tracks with runs IRIT.OpenWeb and IRIT.ClueWeb.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our participation to TREC 2013 Contextual Suggestion Track allowed us to revise our framework experimented in the previous edition of the track <ref type="bibr" coords="1,271.38,376.19,31.05,9.63" target="#b0">[HC12]</ref>. The 2012 framework has some limitations that we intended to overcome, such as checking result contents (i.e., webpages) to exclude inadequate pages (e.g., general information not related to a given location) and providing more accurate descriptions about results according to the user's context (orientation, distance in meter/time, other interesting POIs around, and so on). The new framework we propose triggers additional geo-tools and services, as well as description enriching, filtering and ranking processes based on all collected data (e.g., preferences, popularity, and proximity). In addition, the new framework, first designed to search the web, was then adapted and experimented on the ClueWeb12 sub-collection provided this year. We submitted two runs, one run per sub-track: IRIT.OpenWeb and IRIT.ClueWeb. The process flows for both runs is composed of four main stages, described in Figure <ref type="figure" coords="1,184.56,498.14,4.24,9.63" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Run IRIT.OpenWeb on the Open Web</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preference Processing</head><p>We identified two ways for processing user preferences (Figure <ref type="figure" coords="1,364.97,577.70,9.48,9.63" target="#fig_0">1a</ref>, process 1). In parallel, we (i) build positive and negative term preferences for the user, and (ii) we define his/her categories of interest.</p><p>Positive and negative term preferences were extracted from description ratings only. We considered ratings with values 3 and 4 to build positive preferences and ratings with values 0 and 1 to build negative preferences. For each preference, we indexed the corresponding descriptions of the selected examples by removing stopwords and keeping representative terms without stemming to build a term vector <ref type="bibr" coords="1,512.88,645.44,30.23,9.63" target="#b1">[Sal79]</ref>.</p><p>Categories of user interests either correspond to Google Places categories (GP) 1 or to our own defined categories (OD), based on Swarbrooke's categorization <ref type="bibr" coords="1,308.92,672.54,34.04,9.63" target="#b2">[Swa95]</ref>. To associate users with categories of interest, we first automatically labeled each place (of the example suggestions) with one or more categories. An example is labeled with a category and an associated confidence score if this example is retrieved by Terrier using the category as a query (for our own categories, we also used the hyponyms of the category returned The confidence score corresponds to the retrieval score of Terrier. Then, we defined the categories of interest of a user as the categories corresponding to the examples for which he/she gave a positive vote regarding the description. A confidence score w(c, u) is also associated with the user's categories of interest. It is evaluated as w(c, u) = eâˆˆ{examples} ((ratingDesc(u, e) + ratingU rl(u, e)) * w(c, e), where ratingDesc(u, e) is the rating of user u on the description of example e, ratingU rl(u, e) is the rating of user u on the URL of example e, and w(c, e) is the weight of the category c for example e (i.e., the confidence score indicating the level of representativeness of category c in example e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context Processing</head><p>As showed in Figure <ref type="figure" coords="2,160.38,626.76,11.13,9.63" target="#fig_0">1a</ref> (process 2), for each context we first queried Google Places with 1) its GPS coordinates and 2) all aforementioned categories as keyword. The result list ranks places according to their proximity from the context. Each place is described by its name, address, location (GPS coordinates), website URL, rating extracted from social media, and the initial categories that matched the place. Note that this process does not consider any characteristics of the users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Place Filtering and Enrichment</head><p>This process (Figure <ref type="figure" coords="2,155.08,730.63,9.29,9.63" target="#fig_0">1a</ref>, process 3) consists of two phases.</p><p>First, it filters inadequate pages (URLs) with the Yahoo! BOSS Geo Services geo-parsing tools. A page is discarded when no relevant placenames are detected within its text. A page may have no placenames or it may be too far away from the user's context. We used Google MatrixDistance API to compute such a car distance in minutes. Considering all user contexts, overly frequent URLs were also blacklisted, since it is likely that they refer to global corporations (e.g., www.mcdonalds.com) vs. local places.</p><p>Second, the descriptions of remaining places were enriched. We used distance computing results (Google MatrixDistance) to get distance by feet and car distance between the user location and that of the retrieved places. Other geo-tools (e.g., PostGIS) were used to calculate Euclidian distance and orientation information between the user location and the retrieved places. Gisgraphy, based on GeoNames, processes URLs and generates a set of nearby POIs for each place. Gisgraphy suggests categories to stamp on the POIs (e.g. restaurant, park. . . ). Finally, we used Microsoft Bing search engine to get short descriptions of URLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Suggestion Ranking and Refinement</head><p>The final step of our approach is to return for each pair (user, context) a ranked list of documents. This list stems from two sets of candidate documents, computed based on evidences from GP on the one hand, and evidences from OD on the other hand. Each document is associated with 6 scores:</p><p>1. s 1 (p, u) reflects the matching between the categories of place p and user u categories inferred as explained in section 2.1. Places that did not match any of the user categories were first dismissed.</p><p>Finally s 1 (p, u) = iâˆˆC w(c i , u) for all remaining categories c i âˆˆ C.</p><p>2. s 2 (p, u) = |C| is the number of categories in common between user categories of interest and the categories of the considered place.</p><p>3. s 3 (p) is the number of POIs located around the place p, evaluated with GeoNames.</p><p>4. s 4 (p, u) is the Euclidian distance between place p and the location of user u.</p><p>5. s 5 (p) is the rating value provided by Google Places about place p. Places without rating where ranked at the bottom of the list.</p><p>6. s 6 (p, u) = ps(p, u) -ns(p, u) is the similarity between u and p based on full-text of the document describing p. The positive score ps is the cosine value between the vector of user positive preferences and the document vector. ns is computed likewise with negative preferences.</p><p>These 6 scores are then transformed to be uniformly distributed and defined on the same domain of values (range). The final score of a place given a user and a context is then computed. Places present in the two sets (GP and OD) are ranked higher in the final list. This is done by summing up the scores of the document in the two sets. These 12 scores are summed altogether with the following weighting scheme: 1 /3 for s 1 , 1 /3 for s 6 , and 1 /12 for the remaining factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Run IRIT.ClueWeb on the ClueWeb12 Sub-collection</head><p>For this sub-track, we indexed the ClueWeb12 sub-collection with Terrier, without considering the different contexts. We also identified categories of interest for each user, as done for the Open Web sub-track (see section 2.1). These categories of interest either correspond to Google Places categories (GP) or to our own categories (OD). We then submitted two queries to the Terrier search engine for each user: they were respectively composed of his/her weighted GP/OD categories of interest.</p><p>We finally applied the ranking method based on ranks used on the open web with three factors: i) the RSV of Terrier with the GP query, ii) the RSV of Terrier with the OD query, and iii) the s 6 score, computed with the Lucene search engine. The resulting ranked list is user-dependent but context independent. For a given context, we thus tailored the list by removing the documents irrelevant to the context at stake. Each place suggestion was then returned associated with a description featuring the snippet returned by Bing using the URL of the document.</p><p>Figure <ref type="figure" coords="4,89.23,81.65,5.56,9.63">2</ref> shows the evaluation results for the run labeled IRIT.OpenWeb according to the P@5 measure. The x-axis shows groups of topics, provided that a topic is a pair (profile, context). Five groups of topics are constituted according to the best P@5 reached by participants. The difficulty of topics ranges from "easy" to "impossible" since no participants suggested any relevant place among the top 5 documents. The y-axis shows the number of topics having P@5 = x. For instance, our system retrieved 24 easy topics with a P@5 = 0.60. This corresponds to 29 % of the easy topics, as showed over the third bar. Figure <ref type="figure" coords="4,155.50,328.96,4.24,9.63">2</ref>: Analysis of our results for the run IRIT.OpenWeb according to P@5 Evaluation results according to the P@5 measure for the run labeled IRIT.ClueWeb are more contrasted since 27 % of the topics comprised relevant documents among the top 5 documents.</p><p>Table <ref type="table" coords="4,100.80,381.77,5.45,9.63" target="#tab_1">1</ref> shows the official results of our runs compared with the baselines of the two sub-tracks. The IRIT.OpenWeb run outperforms significantly the BaselineA run that relies on Google Places. This means that the additional processes used in our approach leads to effectiveness improvement. Other experiments are however needed to evaluate each part of our framework. Regarding the IRIT.ClueWeb run, we considered that all documents in the collection were still accessible by assessors, which was unfortunately not always the case. This can explain the poor results of our run compared to the BaselineB run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This article introduced our personalized contextual framework, with a focus on the evolutions integrated since the previous TREC Contextual Suggestion Track in 2012. Evolutions intended to overcome previous limitations regarding inadequate suggestions (e.g., general information not related to a given location) provided to users and poorly informative descriptions of suggestions. We integrated additional geo-tools and services (such as Yahoo! BOSS Geo Services, PostGIS, Gisgraphy, and GeoNames), as well as filtering and ranking processes based on all the available information (e.g., preferences, popularity, proximity). The framework was also adapted to process the ClueWeb12 Contextual Suggestion sub-collection provided this year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,138.29,463.36,335.44,9.63"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of our personalized contextual retrieval process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,177.97,413.24,239.24,89.19"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,177.97,413.24,239.24,89.19"><row><cell></cell><cell cols="2">: Official results</cell><cell></cell></row><row><cell>Run</cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell cols="2">IRIT.OpenWeb 0.3112</cell><cell>0.4915</cell><cell>1.4638</cell></row><row><cell>BaselineA</cell><cell>0.1372</cell><cell>0.2316</cell><cell>0.5234</cell></row><row><cell cols="2">IRIT.ClueWeb 0.0798</cell><cell>0.1346</cell><cell>0.3279</cell></row><row><cell>BaselineB</cell><cell>0.1417</cell><cell>0.2452</cell><cell>0.4797</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Future work will tackle ranking algorithms giving a weight distributed on the different arguments involved. In addition, we plan to test our framework with the whole ClueWeb12 collection.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,92.24,126.27,465.13,8.80;5,91.48,138.22,467.71,9.08;5,92.24,151.08,154.45,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,263.61,126.27,212.47,8.80">IRIT at TREC 2012 Contextual Suggestion Track</title>
		<author>
			<persName coords=""><forename type="first">Gilles</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Cabanac</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec21/papers/IRIT.context.final.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="5,497.23,126.27,60.15,8.80;5,91.48,138.22,149.73,8.80">Proceedings of the 21th Text REtrieval Conference</title>
		<meeting>the 21th Text REtrieval Conference<address><addrLine>Gaithersburg, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,92.24,166.12,439.07,8.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,160.98,166.12,167.59,8.80">Mathematics and information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,336.85,166.12,112.08,8.80">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,92.24,182.06,467.28,8.80;5,91.75,194.01,22.69,8.80" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,170.34,182.06,239.78,8.80">The Development and Management of Visitor Attractions</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Swarbrooke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Butterworth Heinemann</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
