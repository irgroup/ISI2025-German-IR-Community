<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,58.77,83.76,492.17,15.48;1,76.55,105.68,456.62,15.48">University of Amsterdam at the TREC 2013 Contextual Suggestion Track: Learning User Preferences from Wikitravel Categories</title>
				<funder ref="#_qpUqVtn">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
				<funder ref="#_5wgvNUq">
					<orgName type="full">European Communitys Seventh Framework Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.35,138.20,75.75,10.75"><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Logic, Language and Computation</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.44,138.20,93.33,10.75"><forename type="first">Hugo</forename><surname>Huurdeman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.12,138.20,63.92,10.75"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,58.77,83.76,492.17,15.48;1,76.55,105.68,456.62,15.48">University of Amsterdam at the TREC 2013 Contextual Suggestion Track: Learning User Preferences from Wikitravel Categories</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2F6139C1F99F374227B8273E6D4E1408</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the TREC 2013 Contextual Suggestion Track. The goal of the track is to evaluate systems that provide suggestions for activities to users in a specific location, taking into account their personal preferences. As a source for travel suggestions we use Wikitravel, which is a community-based travel guide for destinations all over the world. From pages dedicated to cities in the US we extract suggestions for sightseeing, shopping, eating and drinking. Descriptions from positive examples in the user profiles are used as queries to rank all suggestions in the US. Our user-dependent approach merges the per-query rankings of the positive examples of a single user. We automatically classified the rated examples according to the Wikitravel categories-Buy, Do, Drink, Eat and Seeand derived a user-specific prior probability per category. With these we re-rank Wikitravel suggestions. The ranked suggestions are then filtered based on the location of the user.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wikitravel 1 is a collaboratively created site for travel and tourist information, with lists of things to see and do in places all over the world. Locations are neatly structured in countries, states, regions, districts, cities and suburbs and have a dedicated page, and the places to visit within each location are presented in lists and tables in each page. This information provides travellers with easy access to a list of options for sightseeing, shopping, eating, drinking and sleeping. If you find yourself in a particular city, it is easy to browse this list. For larger cities, the number of options can be very large and is often spread over multiple pages, making it hard to find options that you like. For smaller places the list can be very short and not contain anything of interest in the immediate area, but pages on nearby places may have better options.</p><p>1 URL: http://wikitravel.org/ Our aim for the TREC 2013 Contextual Suggestion Track is to use Wikitravel as a source for suggestions based on the user's current location, which are ranked by distance and how well they match the user's known preferences.</p><p>We use the descriptions of the suggestions as document representations and the descriptions of preferred items in the profiles as queries to retrieve and rank suggestions.</p><p>The rest of this paper is organised as follows. We first describe our experimental setup in Section 2. We discuss our results in Section 4 and provide a more detailed analysis in Section 5. We summarize our findings in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data collection</head><p>Wikitravel is an open platform where anyone can add, edit and delete travel information about places in the world. There are many pages, each dedicated to a specific city or town, with sections describing how to get there and things to see and do. Most pages are structured according to some general rules, to get a consistent travel guide, with clearly separated sections for transportation, sightseeing, shopping and accommodation. Activities, attractions, restaurants and bars are usually presented in lists or tables, with the name of the shop, museum, park, restaurant or hotel, a short description and often a hyperlink to the homepage of a dedicated site. These are provided by a community of travellers and locals and can be used as a source for contextual suggestions.</p><p>We crawled all Wikitravel pages of locations within the US, starting with the page on the United States of America as the seed list. We extracted site-internal links from all the States, Regions, Cities, Districts and Burroughs sections. The pages within the Districts and Burroughs categories describe neighbourhoods in large cities. While extracting links from each of these sections, a mapping is stored that identifies how the source is connected to the target page. For instance, in the Regions section of the page for the U.S. state Oregon we extract links for the regions Cascade Mountains, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Category Priors</head><p>Some users may prefer Do suggestions over Drink suggestions, or Eat over Buy suggestions. From the ratings of the examples, the system could derive a predicted category, but for the 50 examples provided by the Track organisers, the Wikitravel categories are unknown (although some of them may be on the Wikitravel page for Philadelphia). Therefore, we use the descriptions from the 50 examples and the 21,872 Wikitravel suggestions to assign the 50 examples to the 5 Wikitravel categories. To assign the examples to the categories, we crawled all 50 example websites, downloading the homepage from each example, and following site-internal links up to one level deep. Subsequently, we extracted and concatenated the plain text from all crawled pages for each separate site, and tokenized it, followed by basic stop word filtering. Using a tf-idf measure, we extracted the top 30 keywords for each example website, that  <ref type="table" coords="2,360.34,374.66,3.60,8.64" target="#tab_0">1</ref>), 12 to Do, 7 to Drink, 9 to Eat and 12 to See.</p><p>The distribution is somewhat different over the 50 examples than over the Wikitravel suggestions. The number of ratings per category is small and may be too low to predict a rating useful for re-ranking. We investigate this by comparing a ranking based on document retrieval scores alone with a ranking based on both document scores and category-based predicted ratings.</p><p>We provide statistics on the user ratings (based on the ratings after seeing the full document) in Table <ref type="table" coords="2,490.99,482.97,3.74,8.64" target="#tab_1">2</ref>. In the top part of the table, the we see the rating distribution over profiles, where the ratings of the 50 examples are averaged per profile. For the statistics on the examples we first average over the 562 profiles. The preferences of users are highly varied. Some users are at best neutral towards examples. Some users are positive about a few things but negative about most other things, with a median score of 0. Others are mostly positive, with median ratings of 4. One user gave all examples a rating of 4. The ratings over the examples are distributed more evenly, with the lowest rated example having an average rating of 1.41 and the highest 3.49. In the bottom half of Table <ref type="table" coords="2,550.93,614.47,4.98,8.64" target="#tab_1">2</ref> we show rating statistics per Wikitravel category, based on the estimated category per example. The Do and Drink categories are the least liked while the Eat category is the highest rated. Per profile the category ratings vary strongly. Some strongly prefer the See category while others prefer the Buy or Drink categories. These preferences can be captured by the user's personal rating distribution over categories.</p><p>The average rating ru of examples D E by a user u is given as:</p><formula xml:id="formula_0" coords="3,126.97,65.48,165.93,27.50">ru = 1 |D E | d∈D E r u (d)<label>(1)</label></formula><p>The average rating of example websites D C in Wikitravel category C by user u is:</p><formula xml:id="formula_1" coords="3,119.18,132.09,173.72,27.50">ru (C) = 1 |D C | d∈D C r u (d)<label>(2)</label></formula><p>We use these average ratings as category-based predicted ratings to rerank retrieved suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing and Retrieval</head><p>Each suggestion is a document with the description as representation, which we indexed with Indri. We used Krovetz stemming and removed common stopwords. The topic set consists of 50 examples, 562 user profiles and 50 contexts.</p><p>The examples are suggestions in Philadelphia and consist of a short description and a URL to a dedicated website. Each user profile contains judgements from a single user on all 50 examples, with an initial judgement based on the description of the example suggestion and a final judgement after visiting the website. The contexts contain a location (city and state in the US). In the user profiles, the description of each positive example (where the user rated the example positive (score 3 or 4) based on seeing the actual website) was used as a query, resulting in the set Q + u . <ref type="bibr" coords="3,193.09,383.76,3.49,6.05">2</ref> We ranked suggestions per query (default language model with Dirichlet smoothing, µ = 2500) and scores are merged over all queries per profile using CombSUM. The score of each retrieved suggestion is the sum of all its scores for all queries q for user u. Formally, score S(d) for suggestion d is computed as:</p><formula xml:id="formula_2" coords="3,132.13,466.20,160.77,33.06">S(d) = |Q + u | i=1 P (d|q i )<label>(3)</label></formula><p>The language model score P (d|q i ) is computed as:</p><formula xml:id="formula_3" coords="3,121.51,531.71,171.39,9.65">P (d|q i ) = P (d) • P (q i |d)<label>(4)</label></formula><p>where P (d) is a document prior probability, which is P (d) = 1 in the baseline system and P (d) = ru (C)/r max , with r max = 4 when we use the category-based predicated rating. This produces a location-independent ranking of suggestions, which can be updated each time the user adds new information to her profile. When the user wants suggestions based on where she is, the ranking is filtered on distance to her location. All suggestions within the city where the user is located are ranked first, then suggestions within the same region, then within the same state, then the rest of the suggestions. The top 50 suggestions are returned to the user. For large cities this often means all suggestions are within the same city. For smaller locations, with only a small number of suggestions, this often means the suggestions further down the list require some travelling. In Section 5 we analyse the difference between suggestions for small and large cities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Official Runs</head><p>For this year's Contextual Suggestions Track, systems have to provide 50 suggestions for each pair of user and context. There are 562 • 50 = 28, 100 user/context pairs. We submitted one run:</p><p>UAmsTF30WU : this is a baseline run without category priors. Suggestions are ranked per profile/context pair based on the positive examples in the profile and filtered on the context location, with additional suggestions from other cities in the same region or state if there are fewer than 50 suggestions in the context location itself.</p><p>In addition, we prepared another run, which we unfortunately could not submit in time:</p><p>UAmsTF30WUC : This run is the same as the one above, but with the category prior probability assigned to each suggestion.</p><p>These runs allow us to investigate the value of the category priors. Is a category-based document prior effective for ranking? Or are category preferences already captured by using only the descriptions of positively rated examples?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We submitted only the baseline run so there are no official results for the run with category priors. However, to find out if the category prior has potential value for improving the ranking, we take the top 5 results of the baseline run and rerank them based on the ranking of the UAmsTF30WUC run and designate this run UAmsTF30WU cat . Note that this results in a different ranking from the UAmsTF30WUC run, as that run may have different suggestions in the top 5 than the baseline. However, if the category prior is an effective relevance indicator, we expect it to improve performance scores. Suggestions are judged on 3 aspects: the description (Desc) of the suggestion, the document (Doc) and the geographical location (G). Suggestions are considered relevant if the G score is at least 1 (marginally geographically appropriate) and Desc and Doc scores are at least 3 (interesting) for P@5 and MRR. For TBG, the Desc has to be at least 2 (neutral) and Doc at least 3. All dimensions are judged on the same 5 level scale as the examples.</p><p>The evaluation results are shown in Table <ref type="table" coords="3,500.41,686.91,3.74,8.64" target="#tab_2">3</ref>. The Track Median is the mean of the per topic Median scores. Our baseline UAmsTF30WU scores well above the Track Median  <ref type="formula" coords="4,255.00,301.00,9.01,8.64">15</ref>) on all three performance measures. The reranking of the top 5 results based on the category prior, UAmsTF30WU cat , further improves performance. The category prior seems to be a useful signal for ranking. We analyse our runs in more detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section we take a closer look at differences between users, the per topic performance of our two methods and the impact of user-dependent result merging on the final ranking.</p><p>There are a few notable differences between the description and document judgements (see Table <ref type="table" coords="4,230.99,506.79,3.60,8.64">4</ref>). First, 17% of the documents fail to load, which does not happen with the descriptions. Because of this there are fewer neutral and interested document-level judgements compared to description-level judgements. Second, the number of negatively rated suggestions remains relatively stable. However, the number of strongly interested suggestions increases at the document judgement level. In general, going from the description-based ratings to the document-based ratings there are some small shifts from the moderate ratings to the more extreme ratings.</p><p>In Table <ref type="table" coords="4,100.74,639.09,4.98,8.64" target="#tab_3">5</ref> we see the relation between the descriptionbased judgements and the document-based judgements. Row 1 shows how the suggestions initially given a negative rating (229 in total) and how these are rated after seeing the full document. Most suggestions are still rated negatively (143 or 62%), but 18% (28 and 14 out of 229) are rated higher. Suggestions initially rated neutral (row 2) tend to get a non-neutral after seeing the full document: 43 are rated negatively (17%) and 72 positively (29%). Of the suggestions rated positively upon seeing the descriptions, the majority are also rated positively based on the document (440 or 75%). Of the rest, most change due to pages failing to load (90 or 15%), while for 57 (10%) the pages turn out to be less than interesting. In total, 86 judgements shift from negative or neutral to positive while only 57 shift form positive to neutral or negative. Again, we see that ratings become less neutral upon seeing the full document, but when the page loads, the majority of the top 5 results are rated positively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Categories</head><p>Finally, we look at the categories of the top 5 retrieved results. Recall that the Wikitravel suggestions all have explicit categories, whereas for the examples we had to estimate a category.</p><p>In Table <ref type="table" coords="4,360.93,603.23,4.98,8.64">6</ref> we see the distribution of Wikitravel categories over the top 5 retrieved suggestions and over all suggestions in the index. The See category is overrepresented in the top 5, whereas the Eat and Drink categories are underrepresented. The Buy and Do categories are similarly distributed in the top 5 and the index. The high number of suggestions from the See category may be due to the relatively high ratings of the examples in the See category. However, the examples from the Eat category were rated even higher but fail to push Eat suggestions to the top of the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we detailed our official runs for the TREC 2013 Contextual Suggestion Track. We extracted a larger number of suggestions from Wikitravel pages on cities and towns in the US and created two systems that generate geographically independent rankings. Per geographic context the ranked suggestions are filtered on location. This year we experimented with the Wikitravel suggestion categories for buying, doing, drinking, eating and seeing. By estimating the Wikitravel category for the provided examples, we created personalised category prior probabilities. For the baseline system, suggestions are ranked per user profile based on their positively rated examples and filtered on the geographic context. We compare this against a system that incorporates the personalised category prior. Unfortunately, due to time constraints, only the baseline run was submitted so we cannot properly measure the impact of the category prior on performance. However, by reranking the top 5 results of the baseline according to how the system with category prior would rank them and using the same relevance judgements we found that the category prior improves both early precision and Time-Based Gain. Part of making good suggestions is knowing what type of activities a user likes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,64.07,239.11,415.39"><head>Table 1 :</head><label>1</label><figDesc>Number of suggestions and examples in each Wik-</figDesc><table coords="2,53.80,76.02,239.11,403.43"><row><cell>itravel category</cell><cell></cell><cell></cell></row><row><cell cols="3">Category # suggestions % # examples %</cell></row><row><cell>Buy</cell><cell>2496 11</cell><cell>10 20</cell></row><row><cell>Do</cell><cell>5841 27</cell><cell>12 24</cell></row><row><cell>Drink</cell><cell>2476 11</cell><cell>7 14</cell></row><row><cell>Eat</cell><cell>6333 29</cell><cell>9 18</cell></row><row><cell>See</cell><cell>4726 22</cell><cell>12 24</cell></row><row><cell>Total</cell><cell>21,872</cell><cell></cell></row><row><cell cols="3">Central Oregon, Columbia Gorge and four other regions.</cell></row><row><cell cols="3">With each link we store a mapping indicating that that re-</cell></row><row><cell cols="3">gion is a region in the state Oregon. This hierarchical map-</cell></row><row><cell cols="3">ping can be used as an indication of distance between the</cell></row><row><cell cols="3">location of the user and other locations. When there are not</cell></row><row><cell cols="3">enough suggestions in the city where the user is located, we</cell></row><row><cell cols="3">can add suggestions from cities in the same region. From the</cell></row><row><cell cols="3">City, District and Burrough pages we extracted suggestions</cell></row><row><cell cols="3">from the sections Do, See, Buy, Eat, and Drink. Each sug-</cell></row><row><cell cols="3">gestion is identified by either a paragraph, list item or table</cell></row><row><cell cols="3">row in html markup. We only considered items that have an</cell></row><row><cell cols="3">hyperlink to an external web page as suggestions and used</cell></row><row><cell cols="3">the surrounding text in the list item or table row element as</cell></row><row><cell cols="3">description. We extracted a total of 21,872 suggestions from</cell></row><row><cell cols="3">1735 cities and towns. For some locations there is only a sin-</cell></row><row><cell cols="3">gle suggestion, the median (mean) number of suggestions is</cell></row><row><cell cols="3">4 (13). The place with the highest number of suggestions is</cell></row><row><cell cols="2">Chicago (816 suggestions).</cell><cell></cell></row><row><cell cols="3">The number of suggestions from each category is shown</cell></row><row><cell cols="3">in column 2 of Table 1. The Buy and Drink categories are the</cell></row><row><cell cols="3">smallest, with 2496 and 2476 suggestions respectively. The</cell></row><row><cell cols="3">Eat category is the biggest, with 6333 suggestions (29%).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,316.81,64.07,239.10,319.23"><head>Table 2 :</head><label>2</label><figDesc>Statistics on the user ratings across profiles, examples and Wikitravel categories</figDesc><table coords="2,316.81,102.09,239.10,161.65"><row><cell cols="2">Aggregate #</cell><cell cols="5">min max medn mean stdev</cell></row><row><cell>Profiles</cell><cell>562</cell><cell cols="3">0.38 4.00 2.38</cell><cell>2.38</cell><cell>0.51</cell></row><row><cell cols="2">Examples 50</cell><cell cols="3">1.41 3.49 2.37</cell><cell>2.38</cell><cell>0.44</cell></row><row><cell>Category</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Buy</cell><cell cols="2">4496 0</cell><cell>4</cell><cell>3</cell><cell>2.37</cell><cell>1.28</cell></row><row><cell>Do</cell><cell cols="2">7306 0</cell><cell>4</cell><cell>2</cell><cell>2.14</cell><cell>1.31</cell></row><row><cell>Drink</cell><cell cols="2">3372 0</cell><cell>4</cell><cell>2</cell><cell>2.11</cell><cell>1.35</cell></row><row><cell>Eat</cell><cell cols="2">3934 0</cell><cell>4</cell><cell>3</cell><cell>2.68</cell><cell>1.19</cell></row><row><cell>See</cell><cell cols="2">8992 0</cell><cell>4</cell><cell>3</cell><cell>2.54</cell><cell>1.20</cell></row><row><cell cols="7">could serve as queries. The crawled and concatenated text</cell></row><row><cell cols="7">of each of the 5 Wikitravel categories served as document</cell></row></table><note coords="2,316.81,267.06,239.10,8.64;2,316.81,279.02,239.10,8.64;2,316.81,290.97,239.10,8.64;2,316.81,302.93,239.10,8.64;2,316.81,314.88,239.10,8.64;2,316.81,326.84,239.10,8.64;2,316.81,338.79,239.10,8.64;2,316.81,350.75,239.10,8.64;2,316.81,362.52,239.10,8.82;2,316.81,374.66,40.56,8.64"><p>representations, which we indexed using Indri. Issuing the generated queries based on the top 30 keywords per site resulted in a ranked list of the 5 candidate categories for each given example website. With the ratings per user, we can then compute predicted ratings per category. Each profile contains the ratings of the 50 examples by a single user, on a 5 level scale: strongly uninterested (0), uninterested (1), neutral (2) interested (3) and strongly interested (4). Of the 50 examples, 10 are assigned to the Buy category (column 4 in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,53.80,64.07,239.10,245.56"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results for the TREC 2013 Contextual Suggestion Track. The run marked * is not an official submission</figDesc><table coords="4,53.80,112.00,239.10,197.63"><row><cell>Run</cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>Track Median</cell><cell cols="3">0.2368 0.3415 0.8593</cell></row><row><cell>UAmsTF30WU</cell><cell cols="3">0.3121 0.4803 1.1905</cell></row><row><cell cols="4">UAmsTF30WU cat 0.3237 0.5036 1.2413</cell></row><row><cell cols="4">Table 4: Distribution of judgements based on descriptions</cell></row><row><cell>and documents</cell><cell></cell><cell></cell><cell></cell></row><row><cell># results</cell><cell cols="3">description document (%)</cell></row><row><cell>does not load</cell><cell></cell><cell>0 (0)</cell><cell>184 (17)</cell></row><row><cell>strongly uninterested</cell><cell></cell><cell>68 (6)</cell><cell>74 (7)</cell></row><row><cell>uninterested</cell><cell cols="2">161 (15)</cell><cell>134 (13)</cell></row><row><cell>neutral</cell><cell cols="2">251 (24)</cell><cell>149 (14)</cell></row><row><cell>interested</cell><cell cols="2">474 (44)</cell><cell>365 (34)</cell></row><row><cell>strongly interested</cell><cell cols="2">113 (11)</cell><cell>161(</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,316.81,64.07,257.31,246.28"><head>Table 5 :</head><label>5</label><figDesc>Change in judgement from description to document</figDesc><table coords="4,473.68,90.14,40.68,8.64"><row><cell>Change to</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,68.14,702.66,224.76,6.91;3,53.80,712.12,175.63,6.91"><p>Profile IDs 146 and 420 have no positively rated examples. For these profiles, we use all neutrally rated examples as queries.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs> projects # <rs type="grantNumber">640.005.001</rs>) and by the <rs type="funder">European Communitys Seventh Framework Program</rs> (<rs type="grantNumber">FP7 2007/2013</rs>, Grant Agreement 270404 ).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qpUqVtn">
					<idno type="grant-number">640.005.001</idno>
				</org>
				<org type="funding" xml:id="_5wgvNUq">
					<idno type="grant-number">FP7 2007/2013</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
