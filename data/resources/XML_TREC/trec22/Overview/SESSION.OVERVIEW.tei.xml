<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.25,114.94,295.50,15.12">Overview of the TREC 2013 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,77.98,153.54,76.57,10.48;1,154.55,151.92,1.41,6.99"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.76,153.54,102.07,10.48;1,278.82,151.92,1.88,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Google, Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.02,153.54,52.34,10.48;1,353.37,151.92,1.88,6.99"><forename type="first">Mark</forename><surname>Hall</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Edge Hill University</orgName>
								<address>
									<settlement>Ormskirk</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.67,153.54,58.42,10.48;1,430.09,151.92,1.41,6.99"><forename type="first">Ashraf</forename><surname>Bah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,457.03,153.54,63.57,10.48;1,520.60,151.92,1.88,6.99"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.25,114.94,295.50,15.12">Overview of the TREC 2013 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">783E78BA8BFCBC5BC30403C0450DDECE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Session track ran for the fourth time in 2013. The track has the primary goal of providing test collections and evaluation measures for studying information retrieval over user sessions rather than one-time queries. These test collections are meant to be portable, reusable, statistically powerful, and open to anyone that wishes to work on the problem of retrieval over sessions.</p><p>The experimental design of the track was similar to that of the previous two years <ref type="bibr" coords="1,469.12,323.13,11.52,9.57" target="#b3">[4,</ref><ref type="bibr" coords="1,484.27,323.13,7.68,9.57" target="#b4">5]</ref>:</p><p>• sessions were real user sessions with a search engine that include queries, retrieved results, clicks, and dwell times;</p><p>• retrieval tasks were designed to study the effect of using session data in retrieval for only the mth query in a session.</p><p>Changes from last year's track include: This overview is organized as follows: in Section 2 we describe the tasks participants were to perform. In Section 3 we describe the corpus, topics, and sessions that comprise the test collection. Section 4 gives some information about submitted runs. In Section 5 we describe relevance judging and evaluation measures, and Sections 6 present evaluation results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Tasks</head><p>We use the word "session" to mean a sequence of reformulations along with any user interaction with the retrieved results in service of satisfying an information need. The primary goal for participants of the 2013 track was to provide the best possible results for the mth query in a session given data from the session leading up to it.</p><p>NIST provided participants with a set of 133 sessions of varying length (described in more detail in Section 3). 87 of these were targeted for evaluation; the remaining 46 could be used for training. Each of the 87 evaluated sessions consists of:</p><p>• the current query q m ;</p><p>• the query session prior to the current query:</p><p>1. the set of past queries in the session, q 1 , q 2 , ..., q m-1 ;</p><p>2. the ranked list of URLs for each past query;</p><p>3. the set of clicked URLs/snippets.</p><p>In addition, each user action is accompanied by a time relative to the start of the session.</p><p>Participants were to run their retrieval systems over only the current query under each of the following three conditions separately:</p><p>RL1 ignoring the session prior to this query RL2 considering all the items (1), ( <ref type="formula" coords="2,254.75,397.11,4.65,9.57">2</ref>) and (3) above for the current session, i.e the queries prior to the current, the ranked lists of URLs and the corresponding web pages, the clicked URLs and the time spent on any interaction RL3 considering all data in the entire session log (in particular, other user sessions on the same topic)</p><p>Comparing the retrieval effectiveness in (RL1) with the retrieval effectiveness in (RL2)-(RL3), one can evaluate the effectiveness of different algorithms for incorporating session information into retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Collection</head><p>Like most IR test collections, ours consists of a corpus, a set of topics, and relevance judgments (described in the next section). Unlike most test collections, ours also includes a set of sessions of user interactions (including query reformulations). A single topic can have more than one session associated with it, since two different sessions could go about satisfying the same information need in very different ways and with different degrees of success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus</head><p>The track used the ClueWeb12 collection. The full collection consists of roughly 730 million Englishlanguage web pages, comprising approximately 5TB of compressed data. The dataset was crawled from the Web during February and March 2012.</p><p>Participants were encouraged to use the entire collection, however submissions over the smaller "Category B" collection of about 35 million documents were accepted. Note that Category B submissions was evaluated as if they were Category A submissions. Two of six participating groups used the Category B collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>To define a set of topics, we followed the 2012 track in attempting to control two facets of search tasks as defined by Li and Belkin <ref type="bibr" coords="3,242.81,259.54,10.91,9.57" target="#b5">[6]</ref>: "product" and "goal quality". The "product" facet can reflect intellectual or factual tasks; intellectual tasks produce new ideas or findings (e.g. learn about a topic or make decisions based on information collected), while factual tasks only involve locating facts, data, and other informational items. An example of such variation (from the 2012 Session track topics) can be viewed below.</p><p>Query: dehumidifiers The "goal quality" facet reflects specific goal(s) and amorphous goal(s). This is very similar to the dimension that <ref type="bibr" coords="3,151.76,536.69,11.52,9.57" target="#b2">[3]</ref> proposed as well-defined and ill-defined information need. Tasks with specific goals have a well-defined information need, while in tasks with amorphous goals, the information need is ill-defined. Tasks with amorphous goals might require users to redefine the topic or identify specific aspects of the subject themselves. An example (again from the 2012 track) can be viewed below:</p><p>Query: Swahili dishes</p><p>• Session topic: 38 "Goal" facet: specific goals Description: What are some traditional Swahili dishes? What ingredients do they use to cook them? Are swahili people using any particular herb in their dishes? Could you find these ingredients in your country? Are there any recipes you can find online?</p><p>• Session topic: 40 "Goal" facet: amorphous goals Description: One of your friends from Kenya invited you to attend a party in his house and have a taste of traditional swahili dishes. You would like to search and find some information about Swahili dishes.</p><p>Combining "product" and "goal quality" facets generates four classes of topics, characterized as follows: a factual task with specific goals is known-item search, a factual task with amorphous goals is known-subject search, an intellectual task with specific goals is interpretive search, and an intellectual task with amorphous goals is exploratory search. A complete example of all four combinations can be viewed below:</p><p>Query: depression symptoms</p><p>• Task: Known-item search Description: What is depression? What are the major symptoms of depression? What medications, therapies and other treatments can be used to treat depression symptoms? Who performs therapy and what are the costs? Does health insurance pay for any of the treatments?</p><p>• Task: Known-subject search Description: You think that one of your friends may have depression, and you want to search information about the depression symptoms and possible treatments.</p><p>• Task: Interpretive search Description: Depression is a loaded word in our culture. What are the symptoms that could differentiate depression from having just a bad month of excessive emotions? When should one seek help and what kind?</p><p>• Task: Exploratory search Description: A friend has been complaining for months that she is unhappy with her life. She has also mentioned that she can't easily sleep at nights. You think that she may be suffering from depression. You want to understand if this is the case and how you could assist her in getting some help from medical professionals.</p><p>Most of the "known-item search" topics were taken from the 2012 Web track and modified and expanded to cover more questions/aspects than the the original Web track queries did. Topics for the other three types of searches were partly inspired by topics that were still in the news while topics were being developed (e.g. gun violence, gun control) and partly inspired by topics that, at the time, were of interest to or being explored by the person who was generating the session topics (e.g. road-trip, internet phone service, naturalism vs existence of God). Some questions were formulated differently in order to fit under different task types (e.g. internet phone service appeared in both known-item and interpretive search tasks).</p><p>Constructing topics of different task types allows to study both how user interactions different across varying task types and whether/how systems can improve search quality under different session characteristics. We developed a total of 69 different topics across these four categories, though we used only 61 of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sessions</head><p>As describe above, a session is a series of actions, including queries and clicks on ranked results, that a user performs in the process of trying to satisfy the information need represented by the topic. The topics were presented to actual users, who would see three randomly-selected topics and asked to choose one to try to satisfy. Topics were selected for displaying to users randomly according to a distribution inverse to how many times the topic had been selected previously, ensuring that topics selected more frequently would be shown less. After selecting a topic, users were able to use a fully-functional custom search engine for ClueWeb12 in order to satisfy the information need described by the topic.</p><p>The search system used an indri index of ClueWeb12 as a backend. Each of the 20 ClueWeb12 segments (ClueWeb12 00 through ClueWeb12 19) was indexed using the Krovetz stemmer and no stopword list. The indexes searched contained only text from title fields, anchor text from incoming links ("inline" text), and page URLs. We chose to index these fields only in an effort to get faster response times.</p><p>Each query was plugged into an indri query language template as exemplified below for the query "quitting smoking": The first line indicates that we only want documents with a spam score of -130 or less, effectively filtering out 65% of the spammiest documents in the collection at query time (based on Waterloo spam scores). The retrieval score is then computed from four component models: a basic querylikelihood model for the full document representation and three weighted combinations of basic query-likelihood field models with unordered-window within-field models. The "inlink" model was weighted 50 times higher than the title model, and 100 times higher than the URL model. This query template is the product of manual search and investigation of retrieved results. The search interface connected to our indri backend to retrieve the top 50 results (which were filtered further so that at most two documents from any domain would be shown). Users were shown these results in pages of 10 along with a snippet produced by indri's built-in snippet generator. They could click results to see the current version of the page, which of course could be different from the version in the index.</p><p>The system recorded the user's interactions with the retrieval system, including the queries issued, query reformulations, and items clicked in the results page. When data collection was complete, we had acquired a set of candidate sessions to go with the candidate topics we defined above. Each session consists of a topic, a set of queries actual users posed about the topic, the retrieved results, and the user interactions with the retrieved results.</p><p>Session data is provided in an XML file. Part of an example session is shown on page 7.</p><p>The released data comprised 133 full sessions. 87 of these have a minimum of one reformulation (two queries total, of which the second is the currentquery); these were the ones targeted for evaluation.</p><p>Of those, 67 have at least three reformulations, 49 have at least four, 29 have at least five, 7 have at least 10, and there is one session with 20 reformulations. Figure <ref type="figure" coords="6,378.18,482.80,5.45,9.57" target="#fig_2">1</ref> shows the distribution of session lengths. The median and mean of the distribution are both 11.5 queries in a session, which reflects much longer sessions than previous years (the mean for 2012 was 2.03). Figure <ref type="figure" coords="6,460.80,509.90,5.45,9.57" target="#fig_2">1</ref> also shows the amount of time spent in the session; the median length was 6.87 minutes. There are also a total of 586 recorded clicks across all 133 sessions, or 4.4 clicks per session on average. This too is higher than previous years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions</head><p>Participating sites were permitted to submit up to three runs. Each submitted run includes three separate ranked result lists for all 87 sessions. Files were named "runTag.RLn", where "runTag" is a unique identifier for the site and the particular submission, and "RLn" is RL1, RL2, or RL3, depending on the experimental condition.</p><p>&lt;session num="10" starttime="0"&gt; &lt;topic num="12"&gt; &lt;desc&gt;Your friend would like to quit smoking. You would like to provIde him with relevant information about: the different ways to quit smoking, programs available to help quit smoking, benefits of quitting smoking, second effects of quitting smoking, using hypnosis to quit smoking, using the cold turkey method to quit smoking&lt;/desc&gt; &lt;/topic&gt; &lt;interaction num="1" starttime="8. <ref type="bibr" coords="7,281.21,168.38,26.15,8.30">30123</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Session Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relevance Judgments</head><p>Judging was done by assessors at NIST. As described above, each topic was the subject of one or more sessions. Thus pools for judging were formed by topic, not by session.</p><p>For each of the 61 topics, a pool was formed from the ranked results produced by our indri system for the past queries q 1 ...q m-1 and the current query q m , along with the top 10 ranked documents from the submitted runs on the current query q m .The NIST assessors then judged each document in the pool with respect to the topic description. URLs were sorted by domain prior to judging so that assessors would see all pages from the same domain before moving to another one.</p><p>The qrels produced have the following format: &lt;topic-id&gt; 0 &lt;doc-id&gt; &lt;judgment&gt; Judgment values are: -2 for spam document (i.e. the page does not appear to be useful for any reasonable purpose; it may be spam or junk.); 0 for not relevant (i.e. the content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query); 1 for relevant (i.e. the content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page); 2 for highly relevant (i.e. the content of this page provides substantial information on the topic); 3 for key, (i.e. the page or site is dedicated to the topic; authoritative and comprehensive, worthy of being a top result in a web search engine; typically, key pages are more comprehensive, have higher quality, and are from more trustworthy sources than the merely highly relevant page); and 4 for navigational (i.e. this page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site; there is often at most one page that deserves a Navigational judgment for an aspect).</p><p>Relevance judgments were eventually transformed to relevance grades with spam and non-relevant documents assigned a grade of 0, relevant assigned a grade of 1, highly relevant assigned a grade of 2, key assigned a grade of 3, and navigational assigned a grade of 4.</p><p>A total of 13,132 pages were judged. Out of these 13,132 pages, 44 were judged as navigational, 72 as key, 665 as highly relevant, 2,641 as relevant, 9,591 as nonrelevant, and 113 as spam. On average there were 159 nonrelevant (or spam) and 56 relevant (across all types of relevance) documents per query. Only 14 of the topics had at least one "key" document, and only six had at least one "nav" document. <ref type="foot" coords="9,121.70,127.75,4.23,6.99" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Measures</head><p>Based on the qrels provided by NIST and the decisions described above, we evaluated submitted runs by eight measures:</p><p>• Expected Reciprocal Rank (ERR) <ref type="bibr" coords="9,265.67,228.14,11.52,9.57" target="#b1">[2]</ref> • ERR@10 • ERR normalized by the maximum ERR per query (nERR)</p><formula xml:id="formula_0" coords="9,88.36,294.65,124.06,76.07">• nERR@10 • nDCG • nDCG@10 • Average Precision (AP)</formula><p>• Precision@10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>Table <ref type="table" coords="9,101.32,450.95,5.45,9.57">2</ref> shows all results (by nDCG@10) for all submitted runs in all three experimental conditions. If RL1 (no information about the session) is the baseline, nearly all (17 of 20) of the submitted runs were able to improve on that using information about the session prior to the last query (RL2 results). Most of those were statistically significant improvements. A smaller set of systems (7 of 20) were further able to improve by making use of the full log (RL3 results), though only three of these were statistically significant.</p><p>Figure <ref type="figure" coords="9,106.72,536.78,5.45,9.57" target="#fig_0">2</ref> shows changes in nDCG@10 from the RL1 baseline (left) or with increasing information (right). The plots going down the left column show changes in nDCG@10 from using no previous data (RL1) to using greater and greater amounts of previous data. The dashed line is a difference in nDCG of zero; points above that line represent systems that saw an improvement from using the additional data while points below it represent systems that were hurt with the additional data. The 95% confidence intervals give a rough idea of whether the results are significant.</p><p>On the right-hand side, Figure <ref type="figure" coords="9,221.48,622.61,5.45,9.57" target="#fig_0">2</ref> shows changes in nDCG@10 with increasing amounts of previous data: going from RL1 to RL2, then RL2 to RL3. Only a few systems see improvement at every step, and the improvements are not significant. There are many possible reasons for this, one of which is simply that the log was simply too small to provide much use beyond the single session.  <ref type="table" coords="10,101.79,382.97,4.24,9.57">2</ref>: All results by nDCG@10 for the current query in the session for each condition (sorted in decreasing order of RL1 nDCG@10). Boldface indicates the highest nDCG@10 in the condition. ↑, ↓ indicate positive or negative differences from the prior condition. ⇑, ⇓ indicate statistically significant (p &lt; 0.05 by a paired two-sided t-test) positive or negative differences from the prior condition. ↔ indicates no difference from the prior condition. The indri baseline system is our custom search system described above.</p><p>Figure <ref type="figure" coords="10,107.52,483.72,5.45,9.57" target="#fig_4">3</ref> shows the same information for unnormalized ERR. The two are well-correlated, with linear correlations of 0.85 and higher for all three sets of differences.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,85.33,454.38,105.87,9.57;1,85.33,476.90,145.55,9.57;1,85.33,499.42,240.12,9.57;1,85.33,521.93,189.03,9.57"><head>1. a new set of topics; 2 .</head><label>2</label><figDesc>a new corpus (ClueWeb12); 3. a new search system for collecting session data; 4. a small change in the retrieval tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.00,470.68,125.53,8.30;5,113.84,482.63,47.07,8.30;5,176.61,482.63,135.99,8.30;5,155.69,494.59,5.23,8.30;5,176.61,494.59,251.06,8.30;5,218.45,506.54,156.91,8.30;5,155.69,518.50,282.44,8.30;5,218.45,530.45,162.14,8.30;5,155.69,542.41,251.06,8.30;5,218.45,554.36,156.91,8.30"><head>#</head><label></label><figDesc>filreq(#less(spam -130) #weight(1 #combine(quitting smoking) 1 #weight(1 #combine(quitting.title smoking.title) 1 #uw(quitting smoking).title) 50 #weight(1 #combine(quitting.inlink smoking.inlink) 1 #uw(quitting smoking).inlink) 0.5 #weight(1 #combine(quitting.url smoking.url) 1 #uw(quitting smoking).url)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,72.00,232.47,468.00,9.57;6,72.00,246.02,213.12,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: number of sessions of a given length (in terms of total number of queries recorded). Right: amount of time spent in each session.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,72.00,441.00,468.00,9.57;11,72.00,454.55,468.00,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Changes in nDCG@10 from RL1 to (from top to bottom) RL2 and RL3. Right: Changes in nDCG@10 from RL1 to RL2 and RL2 to RL3. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,72.00,554.75,468.00,9.57;12,72.00,568.30,404.27,9.57"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: Changes in ERR from RL1 to (from top to bottom) RL2 and RL3. Right: Changes in ERR from RL1 to RL2 and RL2 to RL3. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,87.69,168.38,528.27,510.42"><head>Table 1 :</head><label>1</label><figDesc>The track received 20 runs from six groups, for a total of 60 ranked lists for each session. They are listed in Table1. Groups participating in the 2013 Sessions Track.</figDesc><table coords="7,307.37,168.38,10.46,8.30"><row><cell>"&gt;</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="9,88.59,683.99,451.41,7.86"><p>Somewhat strangely, two topics had more than 15 "key" documents, and one had more than 35 "nav" documents.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,88.97,565.17,451.03,9.57;10,88.97,578.72,451.03,9.87;10,88.97,593.06,134.76,9.09" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<ptr target="http://ir.cis.udel.edu/ECIR11Sessions" />
		<title level="m" coord="10,423.98,565.17,116.02,9.57;10,88.97,578.72,300.19,9.57">Proceedings of the ECIR 2011 Workshop on Information Retrieval Over Query Sessions</title>
		<meeting>the ECIR 2011 Workshop on Information Retrieval Over Query Sessions</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.97,614.79,451.04,9.57;10,88.97,628.34,451.03,9.57;10,88.97,641.89,113.83,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,362.65,614.79,177.35,9.57;10,88.97,628.34,41.75,9.57">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.65,628.34,373.35,9.57;10,88.97,641.89,79.33,9.57">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.97,487.55,451.03,9.57;11,88.97,501.10,451.03,9.57;11,88.97,514.65,53.94,9.57" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,237.54,487.55,302.46,9.57;11,88.97,501.10,210.22,9.57">The Turn: Integration of Information Seeking and Retrieval in Context (The Information Retrieval Series)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.97,537.17,451.03,9.57;11,88.97,550.71,451.03,9.57;11,88.97,564.26,451.03,9.57;11,88.97,578.60,104.64,9.09" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,406.99,537.17,128.10,9.57">Session track 2011 overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec20/papers/SESSION.OVERVIEW.2011.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,103.04,550.71,324.14,9.57">The Twentieth Text REtrieval Conference Proceedings (TREC 2011</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.97,600.33,451.03,9.57;11,88.97,613.88,240.75,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,440.37,600.33,99.63,9.57;11,88.97,613.88,84.15,9.57">Overview of the trec 2012 session track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,196.14,613.88,98.30,9.57">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.97,636.39,451.03,9.57;11,88.97,649.94,244.40,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,210.33,636.39,325.00,9.57">A faceted approach to conceptualizing tasks in information seeking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,88.97,649.94,101.09,9.57">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1822" to="1837" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
