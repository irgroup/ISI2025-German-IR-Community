<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.59,115.96,302.18,12.62;1,151.96,133.89,311.43,12.62">NovaSearch at TREC 2013 Federated Web Search Track: Experiments with rank fusion</title>
				<funder ref="#_CkmhgWc">
					<orgName type="full">Portuguese National Foundation</orgName>
				</funder>
				<funder ref="#_H3TgZ3R">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.53,171.56,61.29,8.74"><forename type="first">André</forename><surname>Mourão</surname></persName>
							<email>a.mourao@campus.fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia Universidade Nova de Lisboa Caparica</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.32,171.56,64.71,8.74"><forename type="first">Flávio</forename><surname>Martins</surname></persName>
							<email>flaviomartins@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia Universidade Nova de Lisboa Caparica</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.72,171.56,69.11,8.74"><forename type="first">João</forename><surname>Magalhães</surname></persName>
							<email>jm.magalhaes@fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia Universidade Nova de Lisboa Caparica</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.59,115.96,302.18,12.62;1,151.96,133.89,311.43,12.62">NovaSearch at TREC 2013 Federated Web Search Track: Experiments with rank fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6AE5A4167AE94F8D660293D35157F9EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an unsupervised late-fusion approach for the results merging task, based on combining the ranks from all the search engines. Our idea is based on the known pressure for Web search engines to put the most relevant documents at the very top of their ranks and the intuition that relevance of a document should increase as it appears on more search engines <ref type="bibr" coords="1,259.27,323.43,9.21,7.86" target="#b8">[9]</ref>. We performed experiments with state-of-the-art rank fusion algorithms: RRF and Condorcet Fuse and our proposed method: Inverse Square Rank (ISR) fusion algorithm. Rank fusion algorithms have low computational complexity and do not need engines to return document scores nor training data. Inverse Square Rank is a novel fully unsupervised rank fusion algorithm based on quadratic decay and on logarithmic document frequency normalization. The results achieved in the competition were very positive and we were able to improve them further post-TREC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Federated search techniques search on multiple search engines simultaneously. The Federated Web Search (FedWeb) track as designed "to evaluate approaches to federated search at very large scale in a realistic setting, by combining the search results of existing web search engines." <ref type="foot" coords="1,338.03,494.17,3.97,6.12" target="#foot_0">1</ref>Web search engines may target different categories (e.g. news, blogs, articles) and retrieve multiple data types (e.g. web pages, video, images). Some federated retrieval systems try to guess the query category and tailor the final rank to emphasize results from the engines of the category detected. However, regardless of data type or category, all search engines share the basic idea of analysing and indexing documents to produce relevant ranks for user queries. Our main motivation is to leverage the knowledge of a number of web search engines through the combination of their ranks.</p><p>The 2013 track was focused on both resource selection (Task 1) and results merging (Task 2). Since our approach was based on unsupervised fusion algorithms using all search engines, we did not require resource selection. Thus, we only participated on the results merging task.</p><p>The design of the tasks allows a multitude of approaches and techniques: (Use full documents vs. only snippets; external data vs. no external data; supervised vs. unsupervised). Our participation is an unsupervised technique for combining the ranks of all engines in a late fusion approach.</p><p>Rank fusion aims at combining ranked document lists (ranks) from multiple sources into a single (combined) ranked list. Unsupervised methods can be divided into score-based fusion (CombSUM and variants <ref type="bibr" coords="2,383.71,190.72,10.30,8.74" target="#b7">[8]</ref>), rank-based fusion (RR <ref type="bibr" coords="2,156.62,202.68,15.50,8.74" target="#b9">[10]</ref> and RRF <ref type="bibr" coords="2,219.30,202.68,10.79,8.74" target="#b1">[2]</ref>) and voting algorithms <ref type="bibr" coords="2,336.15,202.68,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,348.33,202.68,7.01,8.74" target="#b0">1]</ref>. For the specific case of federated Web search, score-based fusion is not feasible, as search engines do not provide a score for the documents, therefore we focused on rank and voting algorithms.</p><p>This paper is organized as follows: section 2 details our merging approach, section 3 contains the evaluation and section 4 contains the conclusion.</p><p>2 Rank fusion: Standing on the shoulders of giants Vogt et Cottrell <ref type="bibr" coords="2,207.30,315.52,10.52,8.74" target="#b8">[9]</ref> observe the two effects related to search engine fusion:</p><p>-The Skimming effect: search engines are designed to put relevant documents at the very top; -The Chorus effect: documents that appear on multiple ranks are more relevant.</p><p>The Skimming effect is present in the inherent motivation of web search engines to rank relevant documents at top positions. Thus, the higher the rank of a document, the most important it is for the query.</p><p>The Chorus effect is present in the idea that, if a document is deemed relevant by multiple engines, the probability of it being relevant increases. For our experiments, we simply assume that different documents are identified by URL, meaning that documents with the same URL are duplicates. appear only in one engine. In line with the Chorus effect, we believe that the the remaining 3% should be ranked higher on the final rank.</p><p>To support our hypothesis, we performed a post-TREC analysis of the dataset of document relevance versus frequency. For each query, we analysed the relevance of the documents for all engines and measured the document frequency across engines. Figure <ref type="figure" coords="3,236.08,179.70,4.98,8.74" target="#fig_1">1</ref> contains the average results for all queries. The document frequency axis represents the number of search engines where a certain document appears and the Percentage of documents axis represents the percentage of documents divided by relevance level. The increase in relevance with frequency is clear: 12% of the documents that appear only one engine are relevant vs. over 75% of relevant results for documents with a frequency of 4 or more. The plot also shows that the increase in relevance occurs in all relevance levels, meaning that documents that appear on more engines are also more likely to have an higher relevance level.  Inspired by the described effects, our idea was to apply rank fusion techniques for merging the results from all the engines. Unsupervised rank fusion techniques do not need to give weighs to search engines; they combine the results from all provided search engines without the need for resource selection. The initial step of the process is to transform the provided result snippets into ranks. Consider the following sorted rank for the search engine E for the query q:</p><formula xml:id="formula_0" coords="3,228.56,656.12,252.03,9.65">E(q) = {(d 1 , 1), ..., (d k , k), ..., (d i , i)}<label>(1)</label></formula><p>Having formalized rank representation and duplicate detection, the final reranking score must be computed. The new ranks are sorted descending by the re-ranking score.</p><p>The re-ranking is computed using rank fusion methods. Rank fusion methods take the ranked results and calculate a new score using the document ranks and frequency across multiple results lists, Figure <ref type="figure" coords="5,358.04,179.68,20.89,8.74">3 (b)</ref>. Existing methods are Reciprocal Rank <ref type="bibr" coords="5,210.24,191.64,15.49,8.74" target="#b9">[10]</ref> (RR) and Reciprocal rank fusion <ref type="bibr" coords="5,375.09,191.64,10.79,8.74" target="#b1">[2]</ref>) (RRF). RRF can be defined as:</p><formula xml:id="formula_1" coords="5,247.65,215.59,232.95,31.06">RRF(d) = N (d) e=1 1 k + R(e, d))<label>(2)</label></formula><p>A typical value for k (and the one we used on our experiments) is 60. The motivation behind our method ISR, is that existing techniques underestimate the weight of document frequency and overestimate the importance of low-rank documents. ISR can be defined as:</p><formula xml:id="formula_2" coords="5,240.05,319.23,135.25,31.06">ISR(d) = N (d) × N (d) e=1 1 R(e, d) 2 .</formula><p>(</p><p>Where N(d) is the number of times a document appears on a results list (document frequency), and R(e,d) is the rank of document d on engine e.</p><p>ISR uses document frequency as an explicit multiplier (N(i)) and proposes a faster document score decay as rank increases (R(e, d) 2 vs. R(e, d))</p><p>The simplest form of the ISR technique weights document frequency using the absolute document frequency. We observed that linear weighting over-emphasises documents present on multiple ranks and fails to penalize documents that appear in a single rank. In our initial experiments, penalizing documents that appear on a single rank, combined with logarithmic document frequency weighting, leads to a significant performance improvement. Inspired by BM25L <ref type="bibr" coords="5,418.26,470.65,10.52,8.74" target="#b3">[4]</ref> (where the logarithm was introduced to counteract increased score on long documents), we identified logarithmic normalization to provide a good model for document frequency weighting. The other functions in the ISR family are log ISR and normalized log ISR (logN ISR).</p><formula xml:id="formula_4" coords="5,221.40,543.33,254.95,31.06">log ISR(d) = log(N (d)) × N (d) e=1 1 R(e, d) 2 . (<label>4</label></formula><formula xml:id="formula_5" coords="5,208.62,554.60,271.97,66.31">) logN ISR(d) = log(N (d) + σ) × N (d) e=1 1 R(e, d) 2 . (<label>5</label></formula><formula xml:id="formula_6" coords="5,476.35,601.12,4.24,8.74">)</formula><p>We set σ = 0.01 on this paper, based on previous experiments. For TrecFedWeb 2013, we tested ISR <ref type="bibr" coords="5,324.23,644.16,9.96,8.74" target="#b5">[6]</ref>, along with existing rank-based fusion (RRF <ref type="bibr" coords="5,192.25,656.12,10.79,8.74" target="#b1">[2]</ref>) and a voting approach, Condorcet Fuse <ref type="bibr" coords="5,384.83,656.12,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The 2013 TREC FedWeb dataset is an extension to the 2012 TREC FedWeb dataset <ref type="bibr" coords="6,170.61,174.34,9.96,8.74" target="#b6">[7]</ref>. It contains a collection of search results sampled from 157 search engines over 200 queries. Each search engine is related to one or more search categories, such as web, news, travel, and video. Relevance judgments span 5 categories: Navigational (rel = 1), Key (rel = 1), Highly relevant (rel = 0.5), relevant (rel = 0.25) and Not relevant (rel = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>We submitted runs using the described approach for the result merging task. The differences between the runs was the fusion method: ISR (nsISR), RRF <ref type="bibr" coords="6,470.08,283.48,10.52,8.74" target="#b1">[2]</ref> (nsRRF) and CondorFuse <ref type="bibr" coords="6,249.41,295.44,10.52,8.74" target="#b4">[5]</ref> (nsCondor). All runs use all search engines equally for fusion, no resource selection or weighting was performed. The results are in Table <ref type="table" coords="6,162.16,319.35,3.87,8.74" target="#tab_2">2</ref>: All the runs obtained good global results, and our RRF run achieved the best global nDCG@20 <ref type="bibr" coords="6,237.00,505.13,9.96,8.74" target="#b2">[3]</ref>, which as the primary evaluation metric. The other runs were also above the the median for some metrics.</p><p>In addition to the submitted runs, we performed further experiments with the provided relevance judgements. We tested the logarithmic variants of ISR: Log ISR and LogN ISR. These variants focus on logarithmic weighting for document frequency (contrasting with the linear weighting for ISR).</p><p>The results are very positive, beating our previous results by a significant margin and even beating maximum nDCG@20 and nDCG@50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and discussion</head><p>In this paper, we describe our participation at TREC FedWeb 2013. We propose a unsupervised approach for the results merging task, based on combining the ranks from all the search engines (late fusion). Our approach is completely unsupervised and does not require any external data; we only used the provided result snippets. We tested existing rank fusion methods and our novel proposal, ISR.</p><p>According to the provided global results for TREC FedWeb 2013, our run with the RRF fusion algorithm (nsRRF), was the best performing approach in terms of nDCG@20 and got results above the median for other tested metrics <ref type="bibr" coords="7,467.30,190.72,9.96,8.74" target="#b2">[3]</ref>. In our post-TREC experiments, ISR variants were able to improve results even further on all metrics, keeping the good performance and lack of need for supervision inherent to these methods.</p><p>The detection of duplicated documents greatly influenced the results. Our technique (URL matching) was designed to be fast and analyse only the snippets (vs. analysing document content), and missed some similar or near-similar pages. The organizers <ref type="bibr" coords="7,202.96,274.41,10.52,8.74" target="#b2">[3]</ref> detected duplicates using a more thorough set of techniques that account for document similarity (MD5 hash and Simhash) and an exclusion list of pages that contain false positives URL duplicates. A similar false positive exclusion list can be included in future iterations of the algorithm.</p><p>We believe rank fusion is an important area in information retrieval and federated search can greatly benefit to novel developments in these area. Rank fusion approaches are unsupervised, do not require external data and are computationally inexpensive, enabling real time use. ISR approaches allow improved retrieval performance with low complexity, by harnessing the power of multiple individual search engines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,447.78,345.83,7.89;3,134.77,458.76,345.83,7.86;3,134.77,469.72,39.43,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Document frequency across engines versus relevance for all queries. Not Rel.: not relevant; Rel.: relevant; HRel.: Highly relevant; Key or Nav.: Key or navigational document</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,200.96,490.37,213.42,124.52"><head>Table 1 .</head><label>1</label><figDesc>Document frequency across search engines.</figDesc><table coords="2,242.67,514.36,126.94,100.54"><row><cell cols="3">Doc. freq. Doc. count % of doc.</cell></row><row><cell>1</cell><cell cols="2">130058 97.26%</cell></row><row><cell>2</cell><cell>2258</cell><cell>1.69%</cell></row><row><cell>3</cell><cell>755</cell><cell>0.56%</cell></row><row><cell>4</cell><cell>330</cell><cell>0.25%</cell></row><row><cell>5</cell><cell>213</cell><cell>0.16%</cell></row><row><cell>6</cell><cell>79</cell><cell>0.06%</cell></row><row><cell>7</cell><cell>24</cell><cell>0.02%</cell></row><row><cell>8</cell><cell>7</cell><cell>0.01%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,134.77,644.16,345.83,20.69"><head>Table 1</head><label>1</label><figDesc>contains document frequency values across search engines on the 2013 Federated Web Search dataset. The table shows that about 97% of documents</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,157.91,348.79,299.53,112.61"><head>Table 2 .</head><label>2</label><figDesc>2013 TREC FedWeb 2013 results merging results for all queries.</figDesc><table coords="6,162.50,372.78,287.28,88.62"><row><cell>Runs</cell><cell cols="5">Algorithm nDCG nDCG@20 nDCG@50 nDCG@100 P@10</cell></row><row><cell cols="2">nsRRF RRF</cell><cell>0.5082 0.2569</cell><cell>0.2347</cell><cell>0.2553</cell><cell>0.3700</cell></row><row><cell>nsISR</cell><cell>ISR</cell><cell>0.4793 0.1654</cell><cell>0.1635</cell><cell>0.1990</cell><cell>0.3100</cell></row><row><cell cols="2">nsCondor Condor</cell><cell>0.4690 0.1353</cell><cell>0.1550</cell><cell>0.1993</cell><cell>0.2780</cell></row><row><cell></cell><cell></cell><cell cols="2">Post-mortem results</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell>log ISR</cell><cell>0.5229 0.2620</cell><cell>0.2763</cell><cell>0.2904</cell><cell>0.3660</cell></row><row><cell>-</cell><cell cols="2">logN ISR 0.5122 0.2613</cell><cell>0.2393</cell><cell>0.2594</cell><cell>0.3680</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,193.00,7.47"><p>https://sites.google.com/site/trecfedweb/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially funded by the <rs type="funder">Portuguese National Foundation</rs> under the projects <rs type="grantNumber">PTDC/EIA-EIA/111518/2009</rs> and <rs type="grantNumber">UTA-Est/MAI/0010/2009</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CkmhgWc">
					<idno type="grant-number">PTDC/EIA-EIA/111518/2009</idno>
				</org>
				<org type="funding" xml:id="_H3TgZ3R">
					<idno type="grant-number">UTA-Est/MAI/0010/2009</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>d 1 represents the most relevant document (ranked 1), d k represents document ranked k and i represents the size of a rank (number of documents retrieved by the engine). For the given dataset, i was limited to a maximum value of 10.</p><p>In the provided snippets (Figure <ref type="figure" coords="4,299.18,156.72,3.87,8.74">2</ref>), documents are identified with a local unique id (LID) tied to the engine and query (e.g. FW13-e176-7027-01 is the rank 1 document for query 7027 in engine 176). Fig. <ref type="figure" coords="4,154.40,487.30,4.13,7.89">3</ref>. Rank-based fusion example. Initially, the documents in the ranked lists are grouped by GID (a). Individual document score is computed from the Rank,LID lists and the new combined rank is sorted by new scores (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head><p>To find similar documents across ranks, Figure <ref type="figure" coords="4,351.82,546.66,4.98,8.74">3</ref> (a), it is necessary to use the documents URL as a global unique document identifier (GID). GID are converted into one of the corresponding LID after fusion, Figure <ref type="figure" coords="4,379.96,570.57,21.07,8.74">3 (b)</ref>, to conform with the submission format.</p><p>This approach to find duplicated documents is fast and only requires a global identifier; it is completely agnostic to document content or category. This means that is can be deployed instantly on existing systems from a variety of domains with minimal performance requirements. It performs well in most cases, only failing on edge cases (e.g. engines retrieving the same 404 page for multiple queries, engines that retrieve different documents with the same URL).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,503.86,337.64,7.86;7,151.52,514.82,100.72,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,269.88,503.86,90.39,7.86">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,383.10,503.86,97.49,7.86;7,151.52,514.82,72.05,7.86">Proceedings of the 24th SIGIR. SIGIR &apos;01</title>
		<meeting>the 24th SIGIR. SIGIR &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,525.69,337.63,7.86;7,151.52,536.65,282.92,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,339.08,525.69,141.51,7.86;7,151.52,536.65,191.88,7.86">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
		<idno>SIGIR &apos;09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,547.53,337.64,7.86;7,151.52,558.49,203.63,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,395.71,547.53,84.88,7.86;7,151.52,558.49,127.60,7.86">Overview of the trec 2013 federated web search track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,300.12,558.49,26.36,7.86">TREC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,569.37,337.63,7.86;7,151.52,580.33,329.07,7.86;7,151.52,591.29,205.98,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,223.92,569.37,175.85,7.86">When documents are very long, bm25 fails</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,421.36,569.37,59.23,7.86;7,151.52,580.33,329.07,7.86;7,151.52,591.29,85.63,7.86">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1103" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,602.16,337.64,7.86;7,151.52,613.12,191.52,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,267.00,602.16,157.72,7.86">Condorcet fusion for improved retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,445.37,602.16,35.23,7.86;7,151.52,613.12,113.70,7.86">Proceedings of the 11th ACM CIKM</title>
		<meeting>the 11th ACM CIKM</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,624.00,337.64,7.86;7,151.52,634.96,202.43,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,312.94,624.00,144.02,7.86">NovaSearch on medical ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,165.60,634.96,118.10,7.86">Working Notes of CLEF 2013</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,645.84,337.64,7.86;7,151.52,656.80,233.96,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,387.20,645.84,93.39,7.86;7,151.52,656.80,15.56,7.86">Federated search in the wild</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,188.12,656.80,168.69,7.86">Proceedings of the 21st CIKM. CIKM &apos;12</title>
		<meeting>the 21st CIKM. CIKM &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,119.67,337.64,7.86;8,151.52,130.63,208.93,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,249.61,119.67,136.52,7.86">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,409.58,119.67,71.01,7.86;8,151.52,130.63,85.81,7.86">The Second Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
	<note type="report_type">TREC-2</note>
</biblStruct>

<biblStruct coords="8,142.96,141.59,337.63,7.86;8,151.52,152.55,111.37,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,244.13,141.59,164.54,7.86">Fusion via a linear combination of scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,415.71,141.59,64.88,7.86;8,151.52,152.55,25.37,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,163.51,337.98,7.86;8,151.52,174.47,329.07,7.86;8,151.52,185.43,60.92,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,269.82,163.51,210.78,7.86;8,151.52,174.47,236.98,7.86">Expansion-based technologies in finding relevant and new information: Thu trec2002 novelty track experiments</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<editor>TREC</editor>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="586" to="590" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
