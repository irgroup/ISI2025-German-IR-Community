<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.12,122.52,372.15,22.58;1,153.96,146.40,292.61,22.58">A Related Entity based Approach for Knowledge Base Acceleration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,210.72,190.78,53.99,10.99"><forename type="first">Xitong</forename><surname>Liu</surname></persName>
							<email>xtliu@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware Newark</orgName>
								<address>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.74,190.78,58.99,10.99"><forename type="first">Jerry</forename><surname>Darko</surname></persName>
							<email>jdarko@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware Newark</orgName>
								<address>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.28,190.78,47.38,10.99"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<email>hfang@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware Newark</orgName>
								<address>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.12,122.52,372.15,22.58;1,153.96,146.40,292.61,22.58">A Related Entity based Approach for Knowledge Base Acceleration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0D67F5486937B3E2ECC45B2FC6288984</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present the overview of our work in the TREC 2013 KBA Track. The goal is to find documents which may contribute to the update of knowledge base entries (e.g., Wikipedia or Freebase articles). Two tasks are introduced in this year's track: <ref type="bibr" coords="1,376.08,333.45,12.76,9.96" target="#b0">(1)</ref> Cumulative Citation Recommendation (CCR), <ref type="bibr" coords="1,165.97,345.45,12.76,9.96" target="#b1">(2)</ref> Streaming Slot Filling (SSF). Particularly, we focus on the CCR task, follow our previous work on TREC 2012 KBA Track <ref type="bibr" coords="1,280.82,357.33,10.45,9.96" target="#b2">[3]</ref> and propose an improved approach by incorporating weighting to related entities.</p><p>Our approach is based on the framework of leveraging related entities for document filtering. In particular, we pool related entities from the profile page (i.e., Wikipedia) of target entity, estimate the weight of each related entities based on the training data, and apply the weighted related entities to estimate the confidence scores of streaming documents. We explore three methods based on different weighting strategies: <ref type="bibr" coords="1,176.04,429.09,12.76,9.96" target="#b0">(1)</ref> all related entities get zero weight, which is equivalent to exact matching against the target entity only. <ref type="bibr" coords="1,212.30,441.09,12.76,9.96" target="#b1">(2)</ref> all related entities get same weight. (3) related entities are weighted based on training data. Experiments on the KBA Stream Corpus 2013 reveal the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preprocessing</head><p>The CCR task aims to find relevant documents worth citing for the profile of a given entity (e.g., Wikipedia or Freebase entry). In this year 170 target entities are provided, among which 150 are from Wikipedia (specified by URL of Wikipedia entry page) and 20 are from Twitter (specified by URL of Twitter user's profile page). All the entities are manually selected by the KBA organizers. However, 29 entities are dropped from the official query list because they have excessive volume of matched documents in the streaming corpus and thus insufficient resource for manual assessment. The remaining 141 entities have moderate matched documents in the streaming corpus.</p><p>A new KBA Stream Corpus 2013 is released to support both the CCR and SSF tasks. The corpus ranges from October 2011 through January 2013 with 11,948 hours in total, including the content of KBA Stream Corpus 2012 as a subset. Documents in the first 3,551 hours are for training, and the remaining 8,397 hours for testing, respectively. Due to the large volume of data set (4.5 TB compressed text), it is empirically impractical to apply our related entity based methods on it directly given the limited computation resource. To minimize the computational cost, we first construct a much smaller working set by extracting candidate documents from the original data set. The goal is to identify documents which possibly bear some information about a certain target entity. A simple strategy is to check whether the document mentions the target entity. However, this method would miss many relevant documents as entities may have several surface name variations on the Web. For example, one target entity "Benjamin Bronfman" has at least three different variations: "Benjamin Zachary Bronfman", "Ben Brewer" and "BZB". To ensure the high recall and maintain relative low false positive rate, we follow the work by Balog et al. <ref type="bibr" coords="2,291.62,112.53,10.45,9.96" target="#b0">[1]</ref> to extract the surface names of each target entity from its profile page. For Wikipedia entities, we refer to the corresponding DBpedia entry page and extract the attributes which are possible name variations from certain fields (e.g., birthName, sameAs, etc.) by leveraging the structure of DBpedia. For Twitter entities, we use the real names of twitter users as name variations. With the collected name variations, the initial working set construction has been transferred to filtering the documents which match the target entity or any of its name variations. Manual inspection is involved thereafter and name variations which have excessive matched documents are removed as it implies they are common phrases (e.g., short acronyms) in the corpus and documents matching them may not to relevant to the target entity. Documents matching these popular name variations are removed from the working set to make sure the false positive rate is reasonably low. At last, we get a working set with 3.9GB compressed text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Entity based Framework</head><p>Following our previous work <ref type="bibr" coords="2,206.67,286.89,10.00,9.96" target="#b3">[4]</ref>, we estimate the relevance score between a given target entity E and document d based on the matching of target entity and its related entities:</p><formula xml:id="formula_0" coords="2,165.00,322.55,354.87,21.47">score(d, E) = α • mention(d, E) + β • e∈R(E) occ(d, e) • w(E, e),<label>(1)</label></formula><p>Notations include:</p><p>• mention(d, E): a binary function indicating whether the document d mentions E or any of its name variations (1 means mention exists and 0 otherwise).</p><p>• R(E): the set of related entities of E.</p><p>• occ(d, e): the number of occurrences of related entity e in d.</p><p>• w(E, e): the prior weight of e to control the influence. Important entities will be favored and trivial ones will be penalized by controlling the prior weight.</p><p>• α and β: coefficients which balance the impacts of two score components.</p><p>The underlining intuition is that more occurrences of e in d means higher confidence we can infer that d is relevant to E. By setting a cutoff threshold T , we only return the documents with score above T as relevant.</p><p>We now discuss how each of the score components is estimated. Since we construct the working set based on the criterion that each document d contains mention to the target entity E or any of name variations, <ref type="figure" coords="2,155.64,555.42,49.08,9.07">mention(d,</ref><ref type="figure" coords="2,206.39,555.42,5.90,9.07">E</ref>) is 1 for all documents in the working set.</p><p>The related entity set R(E) can be generated from the profile page of E. For Wikipedia entities, we follow the URL to retrieve target entity's profile page and collect related entities by parsing the profile page. Related entities are identified and extracted from the outgoing links to other entity profile pages in Wikipedia. For Twitter entities, we do not collect related entities from the profile page and leave R(E) as an empty set, as we found that the connected entries to the target entity (i.e., the followers and people E is following) are not informative to serve as related entities as the following relationship on Twitter is a weak and non-reciprocal social relation. We plan to explore other ways of collecting related entities for Twitter entities in the future work.</p><p>By leveraging the labelled relevant documents D rel in the training data, we estimate the prior weight of related entity e with regard to E (i.e., w(E, e)) in an iterative approach, as described in Algorithm 1. Basically it aims to maximize the performance gain in a greedy way. More details about Algorithm 1 can be found in our previous work <ref type="bibr" coords="2,286.37,698.49,9.91,9.96" target="#b3">[4]</ref>. For w(e * , R sec (E)) in line 15, we choose the linear decaying weight (shown in Equation <ref type="formula" coords="2,241.83,710.37,4.46,9.96">2</ref>) as it shows superior effectiveness over uniform weight. </p><formula xml:id="formula_1" coords="3,81.60,235.50,166.95,31.79">G ′ = Gain(D rel , R sel (E) ∪ {e * }) 12: ∆G = G ′ -G 13:</formula><p>if ∆G &gt; 0 then 4 Experiment Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submitted Runs</head><p>We submitted five official runs to the KBA track. The differences between them are α, β and w(E, e) in Equation ( <ref type="formula" coords="3,139.40,566.25,4.25,9.96" target="#formula_0">1</ref>) and filtering threshold T . Details are summarized as follows.</p><p>1. UDInfoK EX: α = 1000, β = 0 and T = 0. It essentially returns all the documents in the working set, in which each document has exact match of the topic entity or any of its name variations. It serves as a baseline.</p><p>2. UDInfoK Wiki1: α = 0, β = 50, T = 1 and w(E, e) = 1 for all related entities, i.e., uniform weight prior. We drop mention(d, E) as it does not affect the results on the working set. Related entities are utilized as the pivot to estimate the relevance between topic entity and documents.</p><p>T is set empirically based on the results of training data. Note that Twitter entities do not have related entities, and we directly return all associated documents in the working set, as what we do in UDInfoK Ex.</p><p>3. UDInfoK Wiki2: α = 0, β = 50, T = 51. w(E, e) = 1 for all related entities. 4. UDInfoK Weight1: α = 0, β = 50, T = 1 and w(E, e) is estimated by Algorithm 1 and the performance gain function is estimated by harmonic mean of macro-average precision and recall (which is the major evaluation metric in TREC KBA 2012 <ref type="bibr" coords="4,393.36,395.85,10.83,9.96" target="#b1">[2]</ref>) over the Vital evaluation set of training data. Note that Algorithm 1 would yield zero weighting for all related entities as none of the related entities would bring performance improvement on the training data. In that case, we use the results of UDInfoK Wiki1 instead.</p><p>5. UDInfoK Weight1: α = 0, β = 50, T = 1 and w(E, e) is estimated by Algorithm 1 and the performance gain function is estimated by harmonic mean of macro-average precision and recall over the Vital+Useful evaluation set of training data. Topics do not work with Algorithm 1 fall back to UDInfoK Wiki1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results Analysis</head><p>The results of all the runs are summarized in Table <ref type="table" coords="4,310.81,533.73,3.90,9.96" target="#tab_0">1</ref>. The mP, mR, mF1, and mSu are the macroaverage of precision, recall, F1, and scaled utility over all the queries with a global cutoff, respectively. The hF1 is the harmonic mean of mP and mR, which is used as one of the major evaluation metric in the evaluation. We observe that all of our five runs can reach good results among all the submitted runs. It is interesting to see that our baseline UDInfoK Ex can already deliver satisfying results, implying the name variation based approach is empirically effective on selecting candidate documents. However, runs with related entities involved do not show advantage over the exact matching baseline UDInfoK Ex, actually they even perform worse. Results analysis shows that there are 116 Wikipedia entities with related entities, and the number of related entities for each topic fall in a wide range, with minimum 5, maximum 130, mean 32.647 and standard deviation 22.168, which implies the optimal cutoff for each topic may also fall in a wide range. To verify our hypothesis, we conduct the evaluation using per-topic cutoff, and summarize the results in Table <ref type="table" coords="4,389.28,665.25,3.90,9.96" target="#tab_1">2</ref>. Clearly, methods involving related entities demonstrate advantage over the baseline UDInfoK Ex in terms of both F1 and SU under the per-topic cutoff evaluation setting.</p><p>To better understand the performance, we conduct topic-level analysis and plot the per-topic difference between UDInfoK Ex and UDInfoK Wiki1 on both Vital and Vital+Useful evaluation  settings, as shown in Figure <ref type="figure" coords="5,209.41,284.73,3.90,9.96" target="#fig_0">1</ref>. Topics are sorted in the decreasing order of mF1 score differences, positive values mean UDInfoK Wiki1 performs better while negative values mean it performs worse. We find that incorporating related entities can improve more topics while hurting less.</p><p>We notice that incorporating related entity weighting could not improve the performance over the uniform weight prior, which is different from the observation in our previous work <ref type="bibr" coords="5,465.03,332.49,9.91,9.96" target="#b3">[4]</ref>. We conduct result analysis and find that only a few topics can be applied with Algorithm 1. For UDIn-foK Weight1, only 5 topics qualify and for UDInfoK Weight2, only 12 topics qualify. We think the main reason is that the topics in 2013 is less popular than those in 2012 and their Wikipedia profile entry is less populated thus have less related entities. Actually the mean of number of related entities in 2012 is 135.897, comparing to 32.647 in 2013. The lack of related entities limits the potential of our approach, and we plan to address it in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This is the second year for us to participate the TREC KBA Track. We extend our approach from last year and evaluate it on the new data set. Experiment results demonstrate that our approach can still deliver good performance under the per-topic cutoff evaluation setting. However, the new data set imposes a new challenge from less populated entity profile, and our approach could not work well. We plan to investigate how to deal with the data sparsity of topic entity profile in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,80.52,88.41,176.01,9.96;3,80.52,101.99,350.52,10.18;3,431.04,106.36,10.19,6.19;3,85.92,116.13,87.32,9.96;3,85.92,127.91,84.33,10.18;3,85.92,140.13,73.02,9.96;3,85.92,152.53,46.84,8.96;3,85.92,164.46,29.04,9.89;3,115.56,163.79,41.22,10.18;3,85.92,176.34,33.01,9.89;3,119.52,175.67,54.51,10.18;3,85.92,187.89,37.62,9.96;3,85.92,200.34,62.53,9.89;3,149.04,199.67,50.01,10.18;3,85.92,211.77,349.92,10.33;3,436.44,211.77,28.98,9.96;3,81.60,225.85,10.75,5.56;3,107.40,224.22,4.64,9.07;3,112.08,223.62,4.08,4.55;3,119.40,223.77,44.74,9.96;3,164.16,224.22,109.56,10.92;3,274.32,223.55,45.39,10.18;3,81.60,237.85,10.75,5.56"><head>Algorithm 1</head><label>1</label><figDesc>Related Entity Weighting Input: topic entity E, related entity set R(E), labeled relevant document set D rel 1: /*Initialization*/ 2: for e ∈ R(E) do 3: w(E, e) = 1 4: end for 5: R sel (E) ← {} 6: R lef t (E) ← R(E) 7: G = 0 8: while R lef t (E) = ∅ do 9: /* Select the entity which maximizes performance gain when added to R sel (E) */ 10: e * = arg max e∈R lef t (E) Gain(D rel , R sel (E) ∪ {e}) 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,81.60,273.61,10.75,5.56;3,117.36,271.53,127.00,9.96;3,81.60,285.61,10.75,5.56;3,117.36,283.53,83.96,9.96;3,201.36,283.38,4.08,4.55;3,205.80,283.98,21.96,9.89;3,228.36,283.53,19.48,9.96;3,81.60,297.61,10.75,5.56;3,117.36,295.53,134.28,10.33;3,252.36,295.53,28.98,9.96;3,81.60,309.49,10.75,5.56;3,117.36,307.86,17.52,9.89;3,135.48,307.41,46.56,10.33;3,182.64,307.19,36.44,10.18;3,219.12,307.26,4.08,4.55;3,223.56,307.19,4.98,10.18;3,81.60,321.49,10.75,5.56;3,117.36,319.86,21.49,9.89;3,139.44,319.41,50.41,10.33;3,190.44,319.19,34.76,10.18;3,225.24,319.26,4.08,4.55;3,229.80,319.19,4.98,10.18;3,81.60,333.49,10.75,5.56;3,117.36,331.41,28.95,9.96;3,146.28,331.14,2.29,4.55;3,81.60,345.37,10.75,5.56;3,107.40,343.81,18.33,8.96;3,81.60,357.37,10.75,5.56;3,117.36,355.29,189.56,9.96;3,81.60,369.25,10.75,5.56;3,117.36,366.95,55.57,10.55;3,173.52,367.17,31.05,9.96;3,81.60,381.25,10.75,5.56;3,127.32,379.17,51.54,9.96;3,81.60,393.25,10.75,5.56;3,117.36,391.69,35.32,8.96;3,81.60,405.13,10.75,5.56;3,117.36,403.50,21.49,9.89;3,139.44,402.83,149.36,10.18;3,81.60,417.13,10.75,5.56;3,107.40,415.57,28.09,8.96;3,81.60,427.57,63.57,8.96;3,210.72,473.70,38.48,9.07;3,249.24,472.62,4.08,4.55;3,253.80,473.70,21.96,9.89;3,276.36,473.03,84.24,10.18;3,360.60,477.40,9.96,6.19;3,371.16,473.25,18.37,9.96;3,507.12,473.25,12.76,9.96"><head>R</head><label></label><figDesc>-estimate the weight */ 15: w(E, e) = weight(e * , R sel (E)) 16: /* Incrementally augment R sel (E) */ 17: R sel (E) = R sel (E) ∪ {e * } 18: R lef t (E) = R lef t (E) \ {e * } lef t (E) ← ∅ /* Force quit the loop */ 26: end if 27: end while weight(e * , R sel (E)) = |R(E)| -|R sel (E)| (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,80.52,247.05,439.44,9.96;5,80.52,259.05,27.37,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance comparison between UDInfoK EX and UDInfoK Wiki1 using per-topic cutoff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,80.52,88.53,443.96,238.20"><head>Table 1 :</head><label>1</label><figDesc>Results of official runs. all-mean and all-median are the mean and median of results aggregated from all the submitted runs in this year's KBA track respectively.</figDesc><table coords="4,85.44,88.53,439.04,238.20"><row><cell>Evaluation Set</cell><cell></cell><cell></cell><cell>Vital</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Vital+Useful</cell><cell></cell></row><row><cell>Run</cell><cell>mP</cell><cell>mR</cell><cell>mF1</cell><cell>hF1</cell><cell>mSU</cell><cell>mP</cell><cell>mR</cell><cell>mF1</cell><cell>hF1</cell><cell>mSU</cell></row><row><cell>all-mean</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.166 0.137</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.376 0.425</cell></row><row><cell>all-median</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.174 0.255</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.406 0.423</cell></row><row><cell>UDInfoK Ex</cell><cell cols="10">0.175 0.675 0.229 0.277 0.149 0.512 0.795 0.552 0.623 0.509</cell></row><row><cell>UDInfoK Wiki1</cell><cell cols="10">0.174 0.601 0.216 0.269 0.158 0.556 0.708 0.534 0.623 0.515</cell></row><row><cell>UDInfoK Wiki2</cell><cell cols="10">0.174 0.564 0.212 0.265 0.158 0.566 0.662 0.519 0.604 0.510</cell></row><row><cell cols="11">UDInfoK Weight1 0.173 0.579 0.207 0.266 0.160 0.556 0.686 0.518 0.614 0.504</cell></row><row><cell cols="11">UDInfoK Weight2 0.172 0.563 0.210 0.264 0.157 0.557 0.681 0.524 0.613 0.511</cell></row><row><cell>Evaluation Set</cell><cell></cell><cell></cell><cell>Vital</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Vital+Useful</cell><cell></cell></row><row><cell>Run</cell><cell>mP</cell><cell>mR</cell><cell>mF1</cell><cell>hF1</cell><cell>mSU</cell><cell>mP</cell><cell>mR</cell><cell>mF1</cell><cell>hF1</cell><cell>mSU</cell></row><row><cell>UDInfoK EX</cell><cell cols="10">0.175 0.675 0.229 0.277 0.149 0.521 0.795 0.552 0.623 0.509</cell></row><row><cell>UDInfoK Wiki1</cell><cell cols="10">0.243 0.602 0.259 0.346 0.215 0.663 0.708 0.571 0.684 0.584</cell></row><row><cell>UDInfoK Wiki2</cell><cell cols="10">0.239 0.564 0.250 0.335 0.213 0.659 0.662 0.548 0.660 0.567</cell></row><row><cell cols="11">UDInfoK Weight1 0.241 0.579 0.249 0.341 0.215 0.661 0.686 0.554 0.673 0.574</cell></row><row><cell cols="11">UDInfoK Weight2 0.233 0.563 0.249 0.330 0.213 0.659 0.681 0.559 0.670 0.576</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,182.40,340.05,235.58,9.96"><head>Table 2 :</head><label>2</label><figDesc>Results of official runs using per-topic cutoff.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,96.00,561.57,423.78,9.96;5,96.00,573.57,424.05,9.96;5,96.00,585.57,121.57,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,347.58,561.57,172.21,9.96;5,96.00,573.57,166.40,9.96">Multi-step Classification Approaches to Cumulative Citation Recommendation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Takhirov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ramampiaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,285.84,573.91,234.21,9.62;5,96.00,585.91,22.51,9.62">Open research Areas in Information Retrieval (OAIR 2013)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,96.00,605.49,423.82,9.96;5,96.00,617.37,424.09,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,482.91,605.49,36.92,9.96;5,96.00,617.37,284.26,9.96">Building an Entity-Centric Stream Filtering Test Collection for TREC 2012</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,399.72,617.71,88.50,9.62">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,96.00,637.29,423.80,9.96;5,96.00,649.29,89.89,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,186.75,637.29,276.73,9.96">Entity Profile based Approach in Automatic Knowledge Finding</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,483.96,637.63,35.84,9.62;5,96.00,649.63,57.42,9.62">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,96.00,669.21,423.71,9.96;5,96.00,681.55,423.85,9.62;5,96.00,693.09,74.77,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,185.31,669.21,265.46,9.96">Leveraging Related Entities for Knowledge Base Accleleration</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,470.16,669.55,49.55,9.62;5,96.00,681.55,423.85,9.62">Proceedings of 4th International Workshop on Web-scale Knowledge Representation, Retrieval and Reasoning</title>
		<meeting>4th International Workshop on Web-scale Knowledge Representation, Retrieval and Reasoning</meeting>
		<imprint>
			<publisher>Web-KR</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
