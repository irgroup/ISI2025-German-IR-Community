<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,188.98,172.72,232.30,15.11">TREC 2013 Web Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-01-30">January 30, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.57,206.62,130.19,10.48"><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
						</author>
						<author>
							<persName coords="1,308.79,206.62,65.70,10.48"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
						</author>
						<author>
							<persName coords="1,383.89,206.62,74.79,10.48"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
						</author>
						<author>
							<persName coords="1,188.98,240.37,105.91,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
						</author>
						<author>
							<persName coords="1,332.52,240.37,93.86,10.48"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,188.98,172.72,232.30,15.11">TREC 2013 Web Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-01-30">January 30, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">09FD51CF28A0F8E175CDF098D4EEB0C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the TREC Web track is to explore and evaluate retrieval approaches over large-scale subsets of the Web -currently on the order of one billion pages. For TREC 2013, the fifth year of the Web track, we implemented the following significant updates compared to 2012.</p><p>First, the Diversity task was replaced with a new Risk-sensitive retrieval task that explores the tradeoffs systems can achieve between effectiveness (overall gains across queries) and robustness (minimizing the probability of significant failure, relative to a provided baseline). Second, we based the 2013 Web track experiments on the new ClueWeb12 collection created by the Language Technologies Institute at Carnegie Mellon University. ClueWeb12 is a successor to the ClueWeb09 dataset, comprising about one billion Web pages crawled between Feb-May 2012. <ref type="foot" coords="1,306.33,497.07,4.23,6.99" target="#foot_0">1</ref> The crawling and collection process for ClueWeb12 included a rich set of seed URLs based on commercial search traffic, Twitter and other sources, and multiple measures for flagging undesirable content such as spam, pornography, and malware. The Adhoc task continued as in previous years.</p><p>Both the Adhoc and Risk-sensitive tasks used a common topic set of 50 new topics, and differed only in their evaluation methodology. With the goal of reflecting aspects of authentic Web usage, the Web track topics were again developed from the logs and data resources of commercial search engines. However, a different extraction methodology was used compared to last year. This year, two types of topics were developed: faceted topics, and unfaceted (single-facet) topics. Faceted topics were more like "head" queries, and structured as having a representative set of subtopics, with each subtopic corresponding to a popular subintent of the main topic. The faceted topic queries were less directly ambiguous than last year: the ambiguity lay more in which subintents were likely to be most relevant to users and not in the direct interpretation of the query. Unfaceted (single-facet) topics were intended to be more like "tail" queries with a clear question or intent. For faceted topics, query clusters were developed and used by NIST for topic development. Only the base query was released to participants initially: the topic structures containing subtopics and single-vs multi-faceted vs. topic type were only released after runs were submitted. This was done to avoid biases that might be caused by revealing extra information about the information need that may not be available to Web search systems as part of the actual retrieval process.</p><p>The Adhoc task judged documents with respect to the topic as a whole. Relevance levels are similar to the levels used in commercial Web search, including a spam/junk level. The top two levels of the assessment structure are related to the older Web track tasks of homepage finding and topic distillation. Subtopic assessment was also performed for the faceted topics, as described further in Section 3.</p><p>Table <ref type="table" coords="2,171.62,500.71,5.45,9.57" target="#tab_0">1</ref> summarizes participation in the TREC 2013 Web Track. Overall, we received 61 runs from 15 groups: 34 ad hoc runs and 27 risk-sensitive runs. The number of participants in the Web track increased slightly over 2012 (when 12 groups participated, submitting 48 runs), including some institutions that had not previously participated. Eight runs were manual runs, submitted across four groups: all other runs were automatic with no human intervention. Nine of the runs used the Category B subset of ClueWeb12: all other runs used the main Category A corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ClueWeb1Category A and B collections</head><p>As with ClueWeb09, the ClueWeb12 collection comes with two datasets: Category A, and Category B. The Category A dataset is the main corpus and contains about 733 million documents (27.3 TB uncompressed, 5.54 TB compressed). The Category B dataset is a sample from Category A, containing about 52 million documents, or about 7% of the Category A total. Details on how the Category A and B collections were created may be found on the Lemur project website<ref type="foot" coords="3,297.03,236.99,4.23,6.99" target="#foot_1">2</ref> . We strongly encouraged participants to use the full Category A data set if possible. Results in the paper are labeled by their collection category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>NIST created and assessed 50 new topics for the Web track. Unlike TREC 2012, the TREC 2013 Web track included a significant proportion of more focused topics designed to represent more specific, less frequent, possibly more difficult queries. To retain the Web flavor of queries in this track, we retain the notion from last year that some topics may be multi-faceted, i.e. broader in intent and thus structured as a representative set of subtopics, each related to a different potential aspect of user need. Examples are provided below. For topics with multiple subtopics, documents were judged with respect to the subtopics. For each subtopic, NIST assessors made a scaled six-point judgment as to whether or not the document satisfied the information need associated with the subtopic. For those topics with multiple subtopics, the set of subtopics was intended to be representative, not exhaustive.</p><p>Subtopics were based on information extracted from the logs of a commercial search engine. Topics having multiple subtopics had subtopics selected roughly by overall popularity, which was achieved using combined query suggestion and completion data from two commercial search engines. In this way, the focus was retained on a balanced set of popular subtopics, while limiting the occurrence of strange and unusual interpretations of subtopic aspects. Single-facet topic candidates were developed based on queries extracted from search log data that were low-frequency ('tail-like') but issued by multiple users; less than 10 terms in length; and relatively low effectiveness scores across multiple commercial search engines (as of January 2013).</p><p>The topic structure was similar to that used for the TREC 2009 topics.</p><p>Examples of single-facet topics include:</p><p>&lt;topic number="227" type="single"&gt; &lt;query&gt;i will survive lyrics&lt;/query&gt; &lt;description&gt;Find the lyrics to the song "I Will Survive".&lt;/description&gt; &lt;/topic&gt; &lt;topic number="229" type="single"&gt; &lt;query&gt;beef stroganoff recipe&lt;/query&gt; &lt;description&gt; Find complete (not partial) recipes for beef stroganoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/description&gt; &lt;/topic&gt;</head><p>Examples of faceted topics include:</p><p>&lt;topic number="235" type="faceted"&gt; &lt;query&gt;ham radio&lt;/query&gt; &lt;description&gt;How do you get a ham radio license?&lt;/description&gt; &lt;subtopic number="1" type="inf"&gt;How do you get a ham radio license?&lt;/subtopic&gt; &lt;subtopic number="2" type="nav"&gt;What are the ham radio license classes?&lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt;How do you build a ham radio station?&lt;/subtopic&gt; &lt;subtopic number="4" type="inf"&gt;Find information on ham radio antennas.&lt;/subtopic&gt; &lt;subtopic number="5" type="nav"&gt;What are the ham radio call signs?&lt;/subtopic&gt; &lt;subtopic number="6" type="nav"&gt;Find the web site of Ham Radio Outlet.&lt;/subtopic&gt; &lt;/topic&gt; &lt;topic number="245" type="faceted"&gt; &lt;query&gt;roosevelt island&lt;/query&gt; &lt;description&gt;What restaurants are on Roosevelt Island (NY)?&lt;/description&gt; &lt;subtopic number="1" type="inf"&gt;What restaurants are on Roosevelt Island (NY)?&lt;/subtopic&gt; &lt;subtopic number="2" type="nav"&gt;Find the Roosevelt Island tram schedule.&lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt;What is the history of the Roosevelt Island tram?&lt;/subtopic&gt; &lt;subtopic number="4" type="nav"&gt;Find a map of Roosevelt Island (NY).&lt;/subtopic&gt; &lt;subtopic number="5" type="inf"&gt; Find real estate listings for Roosevelt Island (NY).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/subtopic&gt;</head><p>Initial topic release to participants included only the query field, as shown in the excerpt here: As shown in the above examples, those topics with a clear focused intent have a single subtopic. Topics with multiple subtopics reflect underspecified queries, with different aspects covered by the subtopics. We assume that a user interested in one aspect may still be interested in others. Each subtopic was categorized as being either navigational ("nav") or informational ("inf"). A navigational subtopic usually has only a small number of relevant pages (often one). For these subtopics, we assume the user is seeking a page with a specific URL, such as an organization's homepage. On the other hand, an informational query may have a large number of relevant pages. For these subtopics, we assume the user is seeking information without regard to its source, provided that the source is reliable.</p><p>For the adhoc task, relevance is judged on the basis of the description field. Thus, the first subtopic is always identical to the description sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology and Measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pooling and Judging</head><p>For each topic, participants in the adhoc and risk-sensitive tasks submitted a ranking of the top 10,000 results for that topic. All submitted runs were included in the pool for judging. A common pool was used.</p><p>For the risk-sensitive task, new versions of ndeval and gdeval that supported the risk-sensitive versions of the evaluation measures (described below) were provided to NIST.</p><p>All data and tools required for evaluation, including the scoring programs ndeval and gdeval as well as the baseline run used in computation of the risksensitive scores (run results-cata-filtered.txt) were available in the track's github distribution<ref type="foot" coords="5,215.89,522.69,4.23,6.99" target="#foot_2">3</ref> .</p><p>The relevance judgment for a page was one of a range of values as described in Section 4.2. The topic-aspect combinations with zero known relevant documents were eliminated from all the evaluations. These are: topic 202, aspect 2 topic 202, aspect 3 topic 216, aspect 2 topic 225, aspect 1 topic 225, aspect 5 topic 244, aspect 2 topic 244, aspect 3</p><p>For topics that had a single aspect in the original topics file, that one aspect is used. For all other topics except topic 225, aspect number 1 is the single aspect. For topic 225, aspect number 2 is the single aspect (aspect 2 is used instead of aspect 1 because aspect 1 has no known relevant documents).</p><p>Different topics were pooled to different depths because the original depth (20) resulted in too many documents to be judged in the allotted amount of assessing time. Smaller pools were built using a depth of 10. In all cases, the pools were built over all submitted runs. Pools were sorted so that pages from the same site (as determined by URL syntax) were contiguous in the pool. For multi-aspect topics, assessors judged a given page against all aspects before moving to the next page in the pool. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ad-hoc Retrieval Task</head><p>An ad-hoc task in TREC provides the basis for evaluating systems that search a static set of documents using previously-unseen topics. The goal of an ad-hoc task is to return a ranking of the documents in the collection in order of decreasing probability of relevance. The probability of relevance for a document is considered independently of other documents that appear before it in the result list. For the ad-hoc task, documents are judged on the basis of the description field using a six-point scale, defined as follows:</p><p>1. Nav: This page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site.</p><p>(relevance grade 4)</p><p>2. Key: This page or site is dedicated to the topic; authoritative and comprehensive, it is worthy of being a top result in a web search engine.</p><p>(relevance grade 3)</p><p>3. HRel: The content of this page provides substantial information on the topic. (relevance grade 3) 4. Rel: The content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page. (relevance grade 1)</p><p>5. Non: The content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query. (relevance grade 0) 6. Junk: This page does not appear to be useful for any reasonable purpose; it may be spam or junk (relevance grade -2).</p><p>After each description we list the relevance grade assigned to that level as they appear in the judgment (qrels) file. These relevance grades are also used for calculating graded effectiveness measures, except that a value of -2 is treated as 0 for this purpose. For binary effectiveness measures, we treat grades 1/2/3/4 as relevant and grades 0/-2 as non-relevant.</p><p>The primary effectiveness measure for the ad-hoc task is expected reciprocal rank (ERR) as defined by Chapelle et al. <ref type="bibr" coords="7,338.83,339.32,10.91,9.57" target="#b0">[1]</ref>. We also report a variant of NDCG [3] as well as standard binary effectiveness measures, including mean average precision (MAP) and precision at rank k (P@k). To account for the faceted topics, we also report diversity-based versions of these measures: intent-aware expected reciprocal rank (ERR-IA) <ref type="bibr" coords="7,360.30,393.52,11.52,9.57" target="#b0">[1]</ref> and α-nDCDG <ref type="bibr" coords="7,450.14,393.52,10.91,9.57" target="#b1">[2]</ref>.</p><p>Figure <ref type="figure" coords="7,176.70,407.07,5.45,9.57">1</ref> summarizes the per-topic variability in ERR@10 across all submitted runs. Figure <ref type="figure" coords="7,231.10,420.61,5.45,9.57" target="#fig_1">2</ref> shows the variability in ERR@10 for two specific top-ranked runs, from the Technion and University of Glasgow, with baseline included for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Risk-sensitive Retrieval Task</head><p>The new risk-sensitive retrieval task for Web evaluation rewards algorithms that not only achieve improvements in average effectiveness across topics (as in the ad-hoc task), but also maintain good robustness, which we define as minimizing the risk of significant failure relative to a given baseline.</p><p>Search engines use increasingly sophisticated stages of retrieval in their quest to improve result quality: from personalized and contextual re-ranking to automatic query reformulation. These algorithms aim to increase retrieval effectiveness on average across queries, compared to a baseline ranking that does not use such operations. However, these operations are also risky since they carry the possibility of failure -that is, making the results worse than if they had not been used at all. The goal of the risk-sensitive task is two-fold: 1) To encourage research on algorithms that go beyond just optimizing average effectiveness in order to effectively optimize both effectiveness and ro-bustness, and achieve effective tradeoffs between these two competing goals; and 2) to explore effective risk-aware evaluation criteria for such systems.</p><p>The risk-sensitive retrieval track is related to the goals of the earlier TREC Robust Track <ref type="bibr" coords="8,229.46,173.94,59.11,9.57">(TREC 2004</ref><ref type="bibr" coords="8,297.33,173.94,19.39,9.57">(TREC , 2005</ref>),<ref type="foot" coords="8,326.43,171.99,4.23,6.99" target="#foot_3">4</ref> which focused on increasing retrieval effectiveness for poorly-performing topics using evaluation measures such as geometric MAP that focused on maximizing the average improvement on the most difficult topics. The risk-sensitive retrieval track can be thought of as a next step in exploring more general retrieval objectives and evaluation measures that (a) explicitly account for, and can differentiate systems based on, differences in variance or other risk-related statistics of the win/loss distribution across topics for a single run, (b) the quality of the curve derived from a set of tradeoffs between effectiveness and robustness achievable by systems, measured across multiple runs at different average effectiveness levels, and (c) computing (a) and (b) by accounting for the effectiveness of a competing baseline (both standard, and participant-supplied) as a factor in optimizing retrieval performance.</p><p>As a standard baseline, we used a pseudo-relevance feedback run as implemented by the Indri retrieval engine.<ref type="foot" coords="8,321.03,361.68,4.23,6.99" target="#foot_4">5</ref> Specifically, for each query, we used 10 feedback documents, 20 feedback terms, and a linear interpolation weight of 0.60 with the original query. Additionally, we used the Waterloo spam classifier to filter out all documents with a percentile-score less than 70. <ref type="foot" coords="8,139.74,415.88,4.23,6.99" target="#foot_5">6</ref> .</p><p>As with the adhoc task, we use Intent-Aware Expected Reciprocal Rank (ERR-IA) as the basic measure of retrieval effectiveness, and per-query retrieval delta is defined as the absolute difference in effectiveness between a contributed run and the above standard baseline run, for a given query. A positive delta means a win for the system on that query, and negative delta means a loss. We also report other flavors of the risk-related measure based on NDCG. For single runs, the following will be the main risk-sensitive evaluation measure. Let ∆(q) = R A (q) -R BASE (q) be the absolute win or loss for query q with system retrieval effectiveness R A (q) relative to the baseline's effectiveness R BASE (q) for the same query. We categorize the outcome for each query q in the set Q of all N queries according to the sign of ∆(q), giving three categories: Hurt Queries (Q -) have ∆(q) &lt; 0; Unchanged Queries (Q 0 ) have ∆(q) = 0; Improved Queries (Q + ) have ∆(q) &gt; 0.</p><p>The risk-sensitive utility measure U RISK (Q) of a system over the set of queries Q is defined as:</p><formula xml:id="formula_0" coords="9,178.65,171.35,301.15,11.71">U RISK (Q) = 1/N • [Σ q∈Q + ∆(q) -(α + 1)Σ q∈Q -∆(q)] (<label>1</label></formula><formula xml:id="formula_1" coords="9,479.80,171.35,4.65,9.57">)</formula><p>where α is the key risk-aversion parameter. In words, this rewards systems that maximize average effectiveness, but also penalizes losses relative to the baseline results for the same query, weighting losses α + 1 times as heavily as successes. When the risk aversion parameter α is large, a system will become more conservative and put more emphasis on avoiding large losses relative to the baseline. When α is small, a system will tend to ignore the baseline. The adhoc task objective, maximizing only average effectiveness across queries, corresponds to the special case α = 0. Details are given in Appendix A of the TREC Web 2013 Guidelines<ref type="foot" coords="9,353.81,302.30,4.23,6.99" target="#foot_6">7</ref> .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Plans</head><p>The Web track will continue for a sixth year in TREC 2014, using substantially the same tasks and methodology as this year, but with potential adjustments in some aspects. The following are known areas for refinement, based on participant feedback and our experience organizing this year's Web track.</p><p>• Improved methodology for developing and assessing the more focused, unfaceted (also referred to as single-facet) topics. Many of the unfaceted topic candidates were indeed unambiguous, more tail-like queries, but a number had potentially multiple answers (e.g. [dark chocolate health benefits]). This led to many pages being partially relevant, with no clear way for assessors to know when it was complete enough. We believe retaining a blend of more or less focused query types is important to reflect the nature of authentic Web queries, but will look at revised query clusters and clearer topic development and assessment guidelines for unfaceted topics. • Having a two-stage submission process to determine baselines, or usersupplied baselines. This would involve ad-hoc runs being submitted first, followed by selection of some of those runs to be redistributed to participants to use in the 2nd re-ranking task.</p><p>• Using judgments more effectively/judging more queries -one idea that might be feasible if we do a two-stage process is to specifically target queries where the submitted ad-hoc runs have high variability. It would require careful thought since it is possible the risk-sensitive approaches could still degrade performance here.</p><p>• More holistic types of analysis that compare tradeoff curves within and across systems.</p><p>We will continue the use of ClueWeb12 as the test collection for TREC 2014.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,125.80,605.73,358.66,9.57;15,125.80,619.28,358.66,9.57;15,125.80,632.83,358.65,9.57;15,125.80,646.38,279.18,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Chart showing the significant variation in ERR@10 per topic between top-ranked Technion and Glasgow runs. Topics are sorted by decreasing baseline ERR@10 (pink dashed bar). Faceted topics are prefixed with 'F', single-facet topics by 'S', ambigous topics by 'A'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,193.43,131.82,223.39,60.58"><head>Table 1 :</head><label>1</label><figDesc>TREC 2013 Web Track participation.</figDesc><table coords="2,231.25,131.82,147.77,37.06"><row><cell cols="4">Task Adhoc Risk Total</cell></row><row><cell>Groups</cell><cell>4</cell><cell>11</cell><cell>15</cell></row><row><cell>Runs</cell><cell>34</cell><cell>27</cell><cell>61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,125.80,309.43,358.65,63.77"><head></head><label></label><figDesc>Topics judged to depth 20 were: 201, 202, 203, 204, 205, 206, 208, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226, 232, 233, 234, 239, 240, 243, 247. Topics judged to depth 10 were: 207, 209, 213, 222, 227, 228, 229, 230, 231, 235, 236, 237, 238, 241, 242, 244, 245, 246, 248, 249, 250.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,125.80,327.81,376.35,262.55"><head>Table 2 :</head><label>2</label><figDesc>Top ad-hoc task results ordered by ERR@10. Only the best run according to ERR@10 from each group is included in the ranking.</figDesc><table coords="9,131.78,364.45,370.37,225.92"><row><cell>Group</cell><cell>Run</cell><cell>Cat</cell><cell>Type</cell><cell cols="2">ERR@10 nDCG@10</cell></row><row><cell>Technion</cell><cell>clustmrfaf</cell><cell>A</cell><cell>auto</cell><cell>0.175</cell><cell>0.298</cell></row><row><cell>udel fang</cell><cell>UDInfolabWEB2</cell><cell>A</cell><cell>auto</cell><cell>0.167</cell><cell>0.284</cell></row><row><cell>uogTr</cell><cell>uogTrAIwLmb</cell><cell>A</cell><cell>auto</cell><cell>0.151</cell><cell>0.247</cell></row><row><cell>udel</cell><cell>udelManExp</cell><cell>A</cell><cell>manual</cell><cell>0.150</cell><cell>0.241</cell></row><row><cell>ICTNET</cell><cell>ICTNET13RSR2</cell><cell>A</cell><cell>auto</cell><cell>0.149</cell><cell>0.224</cell></row><row><cell>ut</cell><cell>ut22xact</cell><cell>A</cell><cell>auto</cell><cell>0.144</cell><cell>0.230</cell></row><row><cell>diro web 13</cell><cell>udemQlm1lFbWiki</cell><cell>A</cell><cell>auto</cell><cell>0.143</cell><cell>0.255</cell></row><row><cell>wistud</cell><cell>wistud.runD</cell><cell>A</cell><cell>manual</cell><cell>0.125</cell><cell>0.215</cell></row><row><cell>CWI</cell><cell>cwiwt13cps</cell><cell>A</cell><cell>auto</cell><cell>0.121</cell><cell>0.211</cell></row><row><cell>UJS</cell><cell>UJS13LCRAd2</cell><cell>B</cell><cell>auto</cell><cell>0.100</cell><cell>0.155</cell></row><row><cell>webis</cell><cell>webisrandom</cell><cell>A</cell><cell>auto</cell><cell>0.093</cell><cell>0.171</cell></row><row><cell>RMIT</cell><cell>RMITSC75</cell><cell>A</cell><cell>auto</cell><cell>0.093</cell><cell>0.172</cell></row><row><cell>Organizers</cell><cell>baseline</cell><cell>A</cell><cell>auto</cell><cell>0.088</cell><cell>0.162</cell></row><row><cell>MSR Redmond</cell><cell>msr alpha0 95 4</cell><cell>A</cell><cell>manual</cell><cell>0.087</cell><cell>0.157</cell></row><row><cell>UWaterlooCLAC</cell><cell>UWCWEB13RISK02</cell><cell>A</cell><cell>auto</cell><cell>0.080</cell><cell>0.134</cell></row><row><cell>DLDE</cell><cell>dlde</cell><cell>B</cell><cell>manual</cell><cell>0.008</cell><cell>0.009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,125.80,131.82,376.35,262.55"><head>Table 3 :</head><label>3</label><figDesc>Top ad-hoc task results ordered by ERR@20. Only the best run according to ERR@20 from each group is included in the ranking.</figDesc><table coords="10,131.78,168.45,370.37,225.93"><row><cell>Group</cell><cell>Run</cell><cell>Cat</cell><cell>Type</cell><cell cols="2">ERR@20 nDCG@20</cell></row><row><cell>Technion</cell><cell>clustmrfaf</cell><cell>A</cell><cell>auto</cell><cell>0.184</cell><cell>0.310</cell></row><row><cell>udel fang</cell><cell>UDInfolabWEB2</cell><cell>A</cell><cell>auto</cell><cell>0.176</cell><cell>0.282</cell></row><row><cell>uogTr</cell><cell>uogTrAIwLmb</cell><cell>A</cell><cell>auto</cell><cell>0.160</cell><cell>0.259</cell></row><row><cell>ICTNET</cell><cell>ICTNET13RSR2</cell><cell>A</cell><cell>auto</cell><cell>0.158</cell><cell>0.236</cell></row><row><cell>udel</cell><cell>udelManExp</cell><cell>A</cell><cell>manual</cell><cell>0.157</cell><cell>0.246</cell></row><row><cell>ut</cell><cell>ut22xact</cell><cell>A</cell><cell>auto</cell><cell>0.152</cell><cell>0.228</cell></row><row><cell>diro web 13</cell><cell>udemQlm1lFbWiki</cell><cell>A</cell><cell>auto</cell><cell>0.152</cell><cell>0.254</cell></row><row><cell>wistud</cell><cell>wistud.runD</cell><cell>A</cell><cell>manual</cell><cell>0.134</cell><cell>0.225</cell></row><row><cell>CWI</cell><cell>cwiwt13cps</cell><cell>A</cell><cell>auto</cell><cell>0.128</cell><cell>0.218</cell></row><row><cell>UJS</cell><cell>UJS13LCRAd2</cell><cell>B</cell><cell>auto</cell><cell>0.107</cell><cell>0.148</cell></row><row><cell>RMIT</cell><cell>RMITSCTh</cell><cell>A</cell><cell>auto</cell><cell>0.102</cell><cell>0.179</cell></row><row><cell>webis</cell><cell>webisrandom</cell><cell>A</cell><cell>auto</cell><cell>0.101</cell><cell>0.181</cell></row><row><cell>MSR Redmond</cell><cell>msr alpha0 95 4</cell><cell>A</cell><cell>manual</cell><cell>0.097</cell><cell>0.175</cell></row><row><cell>Organizers</cell><cell>baseline</cell><cell>A</cell><cell>auto</cell><cell>0.096</cell><cell>0.168</cell></row><row><cell>UWaterlooCLAC</cell><cell>UWCWEB13RISK02</cell><cell>A</cell><cell>auto</cell><cell>0.085</cell><cell>0.132</cell></row><row><cell>DLDE</cell><cell>dlde</cell><cell>B</cell><cell>manual</cell><cell>0.008</cell><cell>0.007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,125.80,131.82,441.07,262.55"><head>Table 4 :</head><label>4</label><figDesc>Top diversity measures on results ordered by ERR-IA@10. Only the best run according to ERR-IA@10 from each group is included in the ranking.</figDesc><table coords="11,131.78,168.45,435.09,225.93"><row><cell>Group</cell><cell>Run</cell><cell>Cat</cell><cell>Type</cell><cell cols="3">ERR-IA@10 α-nDCG@10 NRBP</cell></row><row><cell>udel fang</cell><cell>UDInfolabWEB2</cell><cell>A</cell><cell>auto</cell><cell>0.574</cell><cell>0.628</cell><cell>0.547</cell></row><row><cell>Technion</cell><cell>clustmrfaf</cell><cell>A</cell><cell>auto</cell><cell>0.554</cell><cell>0.620</cell><cell>0.521</cell></row><row><cell>ICTNET</cell><cell>ICTNET13RSR3</cell><cell>A</cell><cell>auto</cell><cell>0.542</cell><cell>0.598</cell><cell>0.512</cell></row><row><cell>uogTr</cell><cell>uogTrAIwLmb</cell><cell>A</cell><cell>auto</cell><cell>0.539</cell><cell>0.606</cell><cell>0.498</cell></row><row><cell>udel</cell><cell>udelPseudo2</cell><cell>A</cell><cell>auto</cell><cell>0.531</cell><cell>0.612</cell><cell>0.486</cell></row><row><cell>ut</cell><cell>ut22base</cell><cell>A</cell><cell>auto</cell><cell>0.506</cell><cell>0.574</cell><cell>0.470</cell></row><row><cell>wistud</cell><cell>wistud.runD</cell><cell>A</cell><cell>manual</cell><cell>0.503</cell><cell>0.558</cell><cell>0.466</cell></row><row><cell>diro web 13</cell><cell>udemQlm1lFbWiki</cell><cell>A</cell><cell>auto</cell><cell>0.475</cell><cell>0.557</cell><cell>0.433</cell></row><row><cell>CWI</cell><cell>cwiwt13cps</cell><cell>A</cell><cell>auto</cell><cell>0.473</cell><cell>0.531</cell><cell>0.439</cell></row><row><cell>UJS</cell><cell>UJS13Risk2</cell><cell>B</cell><cell>auto</cell><cell>0.461</cell><cell>0.516</cell><cell>0.434</cell></row><row><cell>webis</cell><cell>webismixed</cell><cell>A</cell><cell>auto</cell><cell>0.409</cell><cell>0.468</cell><cell>0.374</cell></row><row><cell>RMIT</cell><cell>RMITSC75</cell><cell>A</cell><cell>auto</cell><cell>0.376</cell><cell>0.448</cell><cell>0.330</cell></row><row><cell>MSR Redmond</cell><cell>msr alpha0 95 4</cell><cell>A</cell><cell>manual</cell><cell>0.358</cell><cell>0.444</cell><cell>0.306</cell></row><row><cell>Organizers</cell><cell>baseline</cell><cell>A</cell><cell>auto</cell><cell>0.342</cell><cell>0.416</cell><cell>0.294</cell></row><row><cell>UWaterlooCLAC</cell><cell>UWCWEB13RISK02</cell><cell>A</cell><cell>auto</cell><cell>0.315</cell><cell>0.373</cell><cell>0.283</cell></row><row><cell>DLDE</cell><cell>dlde</cell><cell>B</cell><cell>manual</cell><cell>0.045</cell><cell>0.058</cell><cell>0.038</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,125.80,131.82,441.07,262.55"><head>Table 5 :</head><label>5</label><figDesc>Top diversity measures ordered by ERR-IA@20. Only the best run according to ERR-IA@20 from each group is included in the ranking.Figure1: Boxplots for TREC 2013 Web topics, showing variation in ERR@10 effectiveness across all submitted runs. Topics are sorted by decreasing baseline ERR@10 (pink bar). Faceted topics are prefixed with 'F', single-facet topics by 'S', ambigous topics by 'A' .</figDesc><table coords="12,131.78,168.45,435.09,225.93"><row><cell>Group</cell><cell>Run</cell><cell>Cat</cell><cell>Type</cell><cell cols="3">ERR-IA@20 α-nDCG@20 NRBP</cell></row><row><cell>udel fang</cell><cell>UDInfolabWEB2</cell><cell>A</cell><cell>auto</cell><cell>0.582</cell><cell>0.654</cell><cell>0.547</cell></row><row><cell>Technion</cell><cell>clustmrfaf</cell><cell>A</cell><cell>auto</cell><cell>0.567</cell><cell>0.668</cell><cell>0.521</cell></row><row><cell>ICTNET</cell><cell>ICTNET13RSR3</cell><cell>A</cell><cell>auto</cell><cell>0.551</cell><cell>0.627</cell><cell>0.512</cell></row><row><cell>uogTr</cell><cell>uogTrAIwLmb</cell><cell>A</cell><cell>auto</cell><cell>0.548</cell><cell>0.637</cell><cell>0.498</cell></row><row><cell>udel</cell><cell>udelPseudo2</cell><cell>A</cell><cell>auto</cell><cell>0.539</cell><cell>0.637</cell><cell>0.486</cell></row><row><cell>ut</cell><cell>ut22base</cell><cell>A</cell><cell>auto</cell><cell>0.513</cell><cell>0.596</cell><cell>0.470</cell></row><row><cell>wistud</cell><cell>wistud.runD</cell><cell>A</cell><cell>manual</cell><cell>0.512</cell><cell>0.589</cell><cell>0.466</cell></row><row><cell>CWI</cell><cell>cwiwt13cps</cell><cell>A</cell><cell>auto</cell><cell>0.480</cell><cell>0.557</cell><cell>0.439</cell></row><row><cell>diro web 13</cell><cell>udemQlm1lFbWiki</cell><cell>A</cell><cell>auto</cell><cell>0.480</cell><cell>0.576</cell><cell>0.433</cell></row><row><cell>UJS</cell><cell>UJS13Risk2</cell><cell>B</cell><cell>auto</cell><cell>0.468</cell><cell>0.539</cell><cell>0.434</cell></row><row><cell>webis</cell><cell>webismixed</cell><cell>A</cell><cell>auto</cell><cell>0.423</cell><cell>0.516</cell><cell>0.374</cell></row><row><cell>RMIT</cell><cell>RMITSCTh</cell><cell>A</cell><cell>auto</cell><cell>0.388</cell><cell>0.489</cell><cell>0.330</cell></row><row><cell>MSR Redmond</cell><cell>msr alpha1</cell><cell>A</cell><cell>manual</cell><cell>0.368</cell><cell>0.476</cell><cell>0.308</cell></row><row><cell>Organizers</cell><cell>baseline</cell><cell>A</cell><cell>auto</cell><cell>0.352</cell><cell>0.451</cell><cell>0.294</cell></row><row><cell>UWaterlooCLAC</cell><cell>UWCWEB13RISK02</cell><cell>A</cell><cell>auto</cell><cell>0.323</cell><cell>0.399</cell><cell>0.283</cell></row><row><cell>DLDE</cell><cell>dlde</cell><cell>B</cell><cell>manual</cell><cell>0.045</cell><cell>0.058</cell><cell>0.038</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,142.39,655.59,336.00,8.11"><p>Details on ClueWeb12 are available at http://boston.lti.cs.cmu.edu/clueweb12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,142.39,658.12,202.41,7.47"><p>http://lemurproject.org/clueweb12/specs.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,142.39,654.76,189.77,7.47"><p>http://github.com/trec-web/trec-web-2013</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,142.39,629.85,240.06,7.47"><p>http://trec.nist.gov/data/robust/04.guidelines.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,142.39,640.80,160.04,7.47"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,142.39,651.76,235.36,9.21"><p>http://www.mansci.uwaterloo.ca/ ~msmucker/cw12spam/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="9,142.39,627.92,247.35,7.86"><p>http://research.microsoft.com/en-us/projects/trec-web-2013/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We thank <rs type="person">Jamie Callan</rs>, <rs type="person">David Pane</rs> and the <rs type="institution">Language Technologies Institute at Carnegie Mellon University</rs> for creating and distributing the ClueWeb12 collection. This track could not operate without this valuable resource. Also, thanks to <rs type="person">Nick Craswell</rs> for his many valuable suggestions and feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="13,155.40,131.82,4.24,9.57">6</ref>: Overall ERR@10 and risk measures for each team according to difference from the baseline's ERR@10 ("Organizers" below). Ordered by α = 1 performance (i.e., slight risk sensitivity). The best performance in each column was selected for a team and therefore this may be overly optimistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>ERR@10 ∆, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.77,555.62,341.68,9.57;12,142.77,569.17,341.69,9.57;12,142.77,582.71,341.68,9.57;12,142.77,596.26,163.37,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,391.11,555.62,93.34,9.57;12,142.77,569.17,121.28,9.57">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,289.61,569.17,194.85,9.57;12,142.77,582.71,262.69,9.57">Proceedings of the 18th ACM conference on Information and knowledge management, CIKM &apos;09</title>
		<meeting>the 18th ACM conference on Information and knowledge management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.77,618.78,341.69,9.57;12,142.77,632.33,341.68,9.57;12,142.77,645.88,341.68,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,306.64,632.33,177.81,9.57;12,142.77,645.88,88.75,9.57">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,251.68,645.88,232.77,9.57">Proceedings of the 31st annual international ACM</title>
		<meeting>the 31st annual international ACM</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
