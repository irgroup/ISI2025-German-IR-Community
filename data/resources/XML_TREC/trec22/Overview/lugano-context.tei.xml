<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,166.38,164.85,278.48,15.12;1,205.96,186.77,199.32,15.12">University of Lugano at the TREC 2013 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,172.90,219.25,95.93,10.48"><forename type="first">Andrei</forename><surname>Rikitianskii</surname></persName>
							<email>andrei.rikitianskii@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Lugano (USI)</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.50,219.25,76.40,10.48"><forename type="first">Morgan</forename><surname>Harvey</surname></persName>
							<email>morgan.harvey@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Lugano (USI)</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.33,219.25,75.02,10.48"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<email>fabio.crestani@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Lugano (USI)</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,166.38,164.85,278.48,15.12;1,205.96,186.77,199.32,15.12">University of Lugano at the TREC 2013 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B05534634DBC1B7E8FE760855A238D60</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on the University of Lugano's participation in the Contextual Suggestion track of TREC 2013 for which we submitted two runs. In particular we present our approach for contextual suggestion which very carefully constructs user profiles in order to provide more accurate and relevant recommendations. The evaluations of our two runs are reported and compared to each other. Based on the track evaluations we demonstrate that our system performs very well in comparison to other runs submitted to the track, managing to achieve the best results in nearly half of all runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes University of Lugano's participation in the 2013 TREC Contextual Suggestion track. This track continued on from the successful 2012 Contextual Suggestion track <ref type="bibr" coords="1,262.29,462.70,9.96,8.74" target="#b1">[2]</ref>, including more profiles and the possibility of using ClueWeb12 as a source for suggestions. However, this year the organizers decided not to include a temporal (day of week, time of day, season) component for contexts, consisting now of only a location. The task of the track is to recommend places to an individual, given a specific context.</p><p>In this work we present a new approach to recommending places to users incorporating geographical information as context and exploiting data from multiple sources. Our approach mostly concentrates on building user profiles very carefully to catch user preferences more accurately. To construct user profiles we exploit simple Natural Language Processing (NLP) technique and Machine Learning (ML) approach. We submitted two runs with slightly different parameters for the learning algorithm. Via analysis of results from TREC evaluations performed by a large group of users we demonstrate the high level of performance delivered by our method, showing that it is able to outperform all other track runs in nearly half of all cases. Over all metrics our system performs considerably better than the median result.</p><p>The rest of this paper is organized as follows. We describe our approach for context suggestion in Section 3. Section 4 presents evaluations for the two submitted runs. Analysis of our results is detailed in Section 5. Finally, Section 6 summarizes the conclusions and provides guidelines for future work.</p><p>2 Dataset and Tasks</p><p>The TREC Contextual Suggestion Track investigates search techniques for complex information needs that are highly dependent on context and user interests. In this track the goal is to suggest personalized attractions to an individual, given a specific geographic context. The track imagines a traveler in a new city. Given a set of the traveler's preferences for places and activities in their home city, the system should suggest places and activities in a new city that the person may enjoy. In this paper we use the terms "attraction," "place" and "venue" interchangeably.</p><p>As input to the task, participants were provided with a set of 635 profiles, a set of 50 example suggestions, and a set of 50 geo contexts in CSV/JSON format. Example suggestions can represent attractions of different types, for example: bars, restaurants, museums, etc. All the attractions are from the Philadelphia area. Each profile corresponds to a single user, and indicates the user's preference with respect to each example suggestion. Each training suggestion includes a title, description, and an associated URL. Each context corresponds to the GPS coordinate of the centre of a number of cities in the United States. The set of cities is quite diverse in terms of population: from small cities such as Beckley, WV (with a population of 17,606) up to much larger cities such as Atlanta, GA and Wichita, KS (with populations in the hundreds of thousands or even millions). Profiles consist of two ratings for a series of attractions, one rating for the attraction's title and description and another for the attraction's website. The ratings are given on a five-point scale, ranging from "strongly disinterested" to "strongly interested", based on how interested the user would be in going to the venue if they were visiting the particular city it is located in.</p><p>As output to the task, for each profile/context pairing, the participant should return a ranked list of up to 50 ranked suggestions. Each suggestion should be appropriate to the profile (based on the user's preferences) and the context (according to the location), contains a title, description and attraction's URL. The description of the suggestion may be tailored to reflect the preferences of that user. Profiles correspond to the stated preferences of real individuals, who will return to judge the proposed suggestions. Users were recruited through crowdsourcing sites or are university undergraduate and graduate students. For the purposes of this experiment, it was assumed that users are of legal drinking age for the location specified by the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A New Approach for Context Suggestion</head><p>To generate ranked lists of appropriate recommendations for each user profile and geographical context we developed a geo context-aware system. The system can be broken down into the following 4 steps:</p><p>1. processing geo contexts; 2. inferring user term preferences; 3. building a personal ranking model;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ranking suggestions</head><p>In the following section we describe these individual steps in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Processing Geographical Contexts</head><p>Before we can apply any user profile-based personalisation we first need a set of appropriate candidate attractions located within a small radius of the geo context specified. We used the Google Places API<ref type="foot" coords="3,359.79,300.19,3.97,6.12" target="#foot_0">1</ref> to obtain a list of potential suggestions, retrieved based on a query consisting of GPS coordinates and attraction types. We considered only types of venues, as defined by the Google Places API, which were present within the training set. In doing so we retrieved 27 different types, such as: night clubs, amusement parks, libraries, movie theaters, shopping malls, etc. On average, for each geo context, we collected about 350 suggestions.</p><p>Google Places only provides a short title and a web site URL for each suggestion. In order users can evaluate the quality of each suggestion a description of each venue should be provided. To generate these brief descriptions we first queried the Yandex Rich Content API<ref type="foot" coords="3,301.71,419.74,3.97,6.12" target="#foot_1">2</ref> which, given a URL as a query, returns a short static textual description of the page's content. While the Yandex API has generally quite good coverage, there were instances where it was unable to return any information and in these cases we instead queried the Google Custom Search API and used the web site snippet it returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inferring User Term Preferences</head><p>In order to make personalized suggestions for each user we need to be able to compare a new venue with that user's profile to determine how likely it is that the user will like it. We therefore need to have some representation of the user's likes and dislikes based on the training data made available to us, i.e. the venues each user has already rated. In line with previous work on recommender systems <ref type="bibr" coords="3,235.13,575.19,9.96,8.74" target="#b3">[4]</ref>, we chose to maintain separation between positive and negative preferences using descriptive terms from already rated venues. We used the Natural Language Toolkit (NLTK) software 3 to extract only nouns, adjectives, adverbs and verbs from each description and represented these as binary vectors, indicating the presence or absence of each term. For each user, we extracted positive and negative terms, using them to build separate positive and negative profiles. A positive term is one derived from a positively rated venue, a negative term is one from a venue that was given a negative rating. Venues with a title and description rating of more than 2 were considered to be positive, while venues were considered to be negatively rated when it was allocated a rating of less than 2. Terms from neutral suggestions were ignored.</p><p>Due to the relative brevity of the descriptions, this approach of using only the terms present is unlikely to result in many exact term matches and will therefore deliver quite unreliable similarity scores. Consider, for example, if the negative profile for a user contains the word "sushi" and this is matched against a description containing the terms "raw" and "fish". Without performing any kind of term expansion these concepts, despite their obvious similarity, would not make any contribution to the similarity score. However, by expanding existing raw terms using similar words we can (at least partially) overcome this vocabulary mismatch. Thus, comparing profiles with venue descriptions, instead of simply using the raw terms, we checked for matches between the synonym lists returned for each term in WordNet<ref type="foot" coords="4,310.84,317.67,3.97,6.12" target="#foot_3">4</ref> . Given a list of synonyms for a term a and a term b, we consider the terms to be matching if the two lists share at least one component (i.e. if there is some overlap).</p><p>Using both the positive and negative models, we can estimate what the user's opinion might be about a potential suggestion based on its description. To estimate how positive the description is for user u, we can calculate the cosine distance between vector -→ D i , representing the description of venue i, and a positive user profile --→ M + u as follows:</p><formula xml:id="formula_0" coords="4,239.00,424.01,127.08,35.78">cos + ( -→ D i , --→ M + u ) = -→ D i • --→ M + u -→ D i • --→ M + u</formula><p>The same formula was applied to estimate how negative the description is, by using the negative user profile. cos + and cos -scores were used in the final ranking model as described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Building a Personal Ranking Model</head><p>After obtaining a set of potential suggestions for a given geographical context (described in Section 3.1), we need to rank these potential candidates according to the user's preferences. Because of the variability of individual preferences among users it is nearly impossible to build an accurate global ranking model and given that the task is to provide personalised suggestions this would not be suitable anyway. To investigate how varied user preferences were, we measured the level of agreement between all judgments from user profiles by using a standard statistical overlap metric <ref type="bibr" coords="4,269.68,623.42,10.52,8.74" target="#b4">[5]</ref> where the overlap between two sets of items A and B is defined as:</p><formula xml:id="formula_1" coords="5,237.51,136.43,239.97,22.31">Overlap(A, B) = |A ∩ B| min(|A|, |B|)<label>(1)</label></formula><p>The mean pairwise overlap between title and description ratings is 0.38 and between website ratings is 0.39. Both of these overlaps are quite small, suggesting that users have different preferences. Therefore, we decided to build a personal ranking model for each user in order to more precisely adapt suggestion to their own preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Model and Training data.</head><p>We consider the choice of suitable candidates as a binary classification problem. We separate relevant and non-relevant suggestions for each individual user, and then rank those classed as relevant based on a confidence score estimated by the classifier. To generate a training data set for each profile we used example suggestion weight calculated as a linear combination of title and description and website ratings:</p><formula xml:id="formula_2" coords="5,133.77,330.22,253.63,26.67">W eight(S) = λR desc,s + (1 -λ)R url,s with λ ∈ [0, 1] and W eight ∈ [0, 4].</formula><p>In the formula, S indicates the example suggestion from a particular profile; R desc,s ∈ [0, 4] is the suggestion title and description rating; R url,s ∈ [0, 4] is the suggestion website rating. We then assigned a positive label to suggestions with a combined weight of more than a threshold T + and negative label to the suggestion with weight less than threshold T -. These thresholds T + and T -were tuned to try to balance the number of positive and negative samples in the training set. The degree of imbalance is represented by the ratio of sample size of the small class to that of the large class. We considered a training set to be imbalanced if the ratio was less than 1:5, i.e. if the largest class was more than 4 times greater than the smallest. T + and T -were turned for each profile by using a simple iterative algorithm. If the algorithm was unable to converge (i.e. sufficiently balance the 2 classes) after 3 iterations we consider this particular profile to be unsuitable for classification and use an alternate approach for making recommendations which we outline later.</p><p>By default we set uniform weights to the 2 different ratings for each example (λ = 0.5), meaning that the importance of title and description is the same as website. It is possible that the true influence of these two factors may not be equal and this may depend on the kind of venue under consideration. For example, for many cafes the description may provide sufficient information upon which to base a decision, whereas for a restaurant the user may wish to browse the website first, perhaps to look at the menu before making a decision. In this track, we organized a simple user study with a small number of participants from our university to estimate these type-dependent values for λ. For each venue type the participants were asked to rate the importance of title and description in comparison with importance of website on a 9-point scale. A rating value greater than 5 means that the title and description is more important than the website, and vise-versa. A rating value of 5 means that both factors have similar importance. In total, each participant evaluated 27 different types of venue. To calculate λ for each type, we rescaled the rating values for this type into the range [0, 1], then these values were averaged across all the participants. In Table <ref type="table" coords="6,173.62,187.74,4.98,8.74" target="#tab_0">1</ref> we show λ for some types of venue.</p><p>In Section 4 we describe in detail how we used default and type-dependent values for λ separately in two submitted runs. We chose a Naïve Bayes classifier as our learning algorithm. This is a simple probabilistic classifier based on applying Bayes' theorem and making the assumption that each feature's weight (or in the binary case, presence or absence) is independent of the weights of other features, given the class variable. Although this assumption is unlikely to be entirely true in many cases, it greatly simplifies the model -making it tractable -and does not significantly degrade performance in practice. The motivation for choosing such a simple classifier was that it generally performs better on small data sets than more sophisticated machine learning techniques <ref type="bibr" coords="6,262.35,480.18,10.52,8.74" target="#b0">[1]</ref> and does not require any complex parameter tuning or additional learning. In our case, the size of the training data set is never greater than 50 examples; the number of examples for each profile lies in the range 30-49. We use the Weka implementation of the classifier <ref type="bibr" coords="6,426.53,516.05,10.52,8.74" target="#b5">[6]</ref> for all of our experiments. Each suggestion in the training set is represented by a feature vector, consisting of two different types of features: boolean and real-valued. The boolean features were derived based on attraction types, representing the user's preferences with regard to the kind of venue suggested. As described in Section 4.1, the Google Places API returns a simple type for each place, indicating what kind of venue it is. Each place can be assigned to multiple types and as such our binary feature vector encodes the types each suggestion has been assigned to: 1 if it is assigned to that type, 0 otherwise. The 3 real-valued features were based on the cosine distance between the suggestion description and both user profiles (positive and negative), reflecting user term preferences, and the description length. For a given user u and sug-gestion i, we calculated two features cos + ( -→ D i , --→ M + u ) and cos -(</p><formula xml:id="formula_3" coords="7,407.92,121.65,36.79,18.55">-→ D i , --→ M - u )</formula><p>, where D i is a description of suggestion S and M + u and M - u are the positive and negative profiles for user u. The description length feature is simply the length of the description in characters. We believe that the length of description may be an important factor when the user explores the suggestions for an attraction as a longer description may provide more detailed information, allowing the user to be more sure of their rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ranking Suggestions</head><p>To rank the potential suggestions for each user, we use their individual personal ranking model as described in Section 4.3. The personal ranking model was first used to determine the 50 most relevant suggestions for each geographical context, in descending order of confidence score as estimated by the classifier. The confidence score, in the case of a Naïve Bayes classifier, is simply the posterior probability that the suggestion belongs to class "relevant" and therefore encodes, in some sense, how likely it is that the user will like the candidate venue. This approach has been demonstrated to work well for ranking <ref type="bibr" coords="7,443.46,331.28,9.96,8.74" target="#b6">[7]</ref>.</p><p>As mentioned in the previous section, there were a few profiles for which it wasn't sensible to build a classifier due to the level of imbalance between the 2 classes in the training data. In this case, potential suggestions were ranked by using only the user term preferences. We ordered the suggestions in descending order of their scores, which were calculated as the difference between cos + and cos -. This is a reasonable, if slightly simplified approach, since it will return a positive value if the similarity between the candidate venue and the positive profile is greater than its similarity compared with the negative profile and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section we present an overview of the performance of our system. Output suggestions were judged both by the original group of users who supplied the training data and NIST assessors. The user corresponding to each profile judged suggestions in the same manner as for the training examples, assigning a value of 0-4 for each title and description and url. NIST assessors judged suggestions in terms of geographical appropriateness. In total, 223 of the potential 31750 profile/context pairs were judged, i.e. not all pairs were used for evaluation. The top 5 suggestions for each profile/context pair were taken into account for evaluation.</p><p>To evaluate the performance of the model for the problem of Contextual Suggestion, three different measures were used: Precision at Rank 5 (P@5), Mean Reciprocal Rank (MRR) and Time-Biased Gain (TBG). P@5 and MRR are traditional evaluation metrics to measure the overall effectiveness of an IR system in terms of its ability to return a good ranked list. The TBG metric, on the other hand, was developed specially for the contextual suggestion task <ref type="bibr" coords="7,464.20,661.02,9.96,8.74" target="#b2">[3]</ref>.</p><p>As the basis for evaluation, a suggestion was counted as "relevant" if the user liked both the description and the geographically appropriate document. All other suggestions were counted as "non-relevant". P@5 and MRR are calculated by using these definitions for relevant and non-relevant. The TBG metric is more complex and takes into account the impact of descriptions and disliked suggestions, which are ignored by P@5 and MRR. All the metrics were computed for each profile/context pair, and then averaged across all pairs.</p><p>We submitted two runs to the TREC 2013 Contextual Suggestion Track: simpleScore and complexScore. For the simpleScore run we used λ = 0.5 for all venue types, while for complexScore we used type-dependent values for λ as described in Section 3.3.1. The overlap of the top 5 suggestions between the two runs is about 41%, i.e. on average both of two runs contain 41% of the same suggestions among top 5 ranked suggestions for each profile/context pair. At the top rank the overlap is 40%, meaning that the top suggestions from two runs are generally different.</p><p>Table <ref type="table" coords="8,175.74,307.29,4.98,8.74" target="#tab_1">2</ref> shows evaluation results for our runs and the median score, which is calculated based on the results from all 34 runs submitted to the TREC track. The results show that the simpleScore run slightly outperforms the com-plexScore one: P@5 +4.2%, MRR +1.7%, TBG +0.8%. The difference can be explained by two main factors. First of all, the type-dependent values for λ may not reflect user behavior when making a decision, i.e. both the title and description and the website influence the user equally. Another reason is perhaps that the user study, which we organized with 7 participants, isn't a reliable methodology for estimating these values. They could perhaps be learned from the training data, however we leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>The results of our runs demonstrate that our two runs greatly outperform the median score with P@5 +45%, MRR +41%, TBG +53% for our best run (simpleScore). Table <ref type="table" coords="8,227.39,540.14,4.98,8.74" target="#tab_2">3</ref>  According to the MRR metric, our system was able to return the best result over all entrants for 48.43% of user/context pairs. When considering P@5, our system returned the best result in 22% of cases and was better than the median Run P@5 MRR TBG simpleScore 61.88% 53.36% 66.37% complexScore 55.15% 52.47% 66.82% Table <ref type="table" coords="9,160.68,173.49,3.87,8.74">4</ref>: Share of user/context pairs where particular run returned better result better than the median over all entrants score 61% of the time for simpleScore run. However according to TBG metric complexScore run performs slightly better than simpleScore in terms of share of user/context pairs with the best results and results which were higher than the median. We found that about 1.5% of all suggestions had a website which could not be loaded during the assessment procedure. Removing these suggestions from the top 5 leads to performance improvements of: P@5: +0.5%, MRR: +1% and TBG: +1.5% for both runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Besides final/max/min/median results for each profile/context pair, the organizers also provided all judgments (description, website, geographical relevance) for each suggestion from the 223 profile/context pairs which were judged. Using an evaluation script provided for the track and these judgments we performed a more detailed analysis of our results.</p><p>Table <ref type="table" coords="9,177.02,411.57,4.98,8.74" target="#tab_3">5</ref> shows how the geographical relevance(G), description(D) and the website(W) ratings contributed to the P@5 and MRR scores. According to this statistic, almost all documents retrieved by our system were geographically appropriate to the context, suggesting that the approach of pre-filtering candidates is effective. The website of each suggestion and its description appear to contribute equally to final result quality. We found that there was a small correlation (0.271) between the number of candidates returned by the Google Places API for a city and the performance of the system, suggesting that it is easier to make good recommendations when there is a wide variety of possible candidates. The 4 context cities with the smallest population also have the worst performance in terms of P@5 and, unsurprisingly, there is a strong correlation between the population of a city and the number of candidates returned for it by the API (0.693).</p><p>In general, assessors judged 1115 suggestions from the Top 5 suggestions for 136 different profiles. These suggestions represent 772 unique venues, i.e. some venues were recommended for different profiles at the same time. We explored types of venues which were represented by these suggestions, amounting to a total of 24. Figure <ref type="figure" coords="9,220.82,614.81,4.98,8.74" target="#fig_0">1</ref> presents a distribution over different types of venues for two runs: restaurants (23%), museums (12%) and parks (11.7%) are the most common types of venues. For each venue type we calculated a popularity score, which is the fraction of "relevant" suggestions over all of the suggestions made for that type. Figure <ref type="figure" coords="9,227.95,662.63,4.98,8.74" target="#fig_1">2</ref>    were zoos, bars, museums and restaurants. This can perhaps be explained by the fact that there were few zoos recommended to users, and all of them were counted as "relevant". Restaurants, bars and museums are often suggested and are highly popular because they are very common tourist attractions and their overall popularity is perhaps not strongly affected by a visitor's interests. The popularity of venues such as travel agencies, shopping malls and electronics stores is 0, likely because these types of places are not particular attractive to tourists and are more likely to be frequented regularly by people who live in the area. In terms of distribution over different types of venues and popularity of venue's types, both runs are very similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper we have described a new system designed to take part in the TREC Contextual Suggestion track, for making context-sensitive recommendations to tourists visiting a new city. Based on analysis of results obtained from the same users who contributed the training data we have shown that the method is very effective for this problem and, when compared to the 34 other runs in the track, delivered results which were well above the median. In nearly half of all contexts our approach was able to deliver the best set of results, confirming that the choices made during the development of the system were sensible and beneficial. Our method is based on quite a simple strategy of using the descriptions of previously rated places to build user profiles, however we introduce a number of novel additions which have clearly lead to improved performance. Evaluation results of two submitted runs has shown that the runs are similar in terms of performance, although they have considerable different suggestions at the top 5 ranked ones.</p><p>There are several directions for future work. Our ranking model could be easily extended by adding new features to the classifier. For example, in the current ranking model we did not use information about the distance between the geolocation specified for each context and the venue, the venue rating (provided by content system) or the cuisine type of restaurants, cafes and bars. We believe that new features based on this information could allow the ranking model to reflect user preferences more precisely and that weighting suggestions by their distance from the user's location could lead to better acceptance of the recommendations made. It would also be interesting to use other content systems (such as Foursquare and TripAdvisor) to expand the list of potential candidates and the brief descriptions could perhaps be improved or tailed to the user's interests by also considering user reviews or comments from social networks. Finally, instead of estimating type-dependent values for λ via a user study, we could learn specifics weights for the λ parameter in our model from the training data. We leave all these directions to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,133.77,453.16,343.71,8.74;10,133.77,465.12,60.44,8.74;10,164.61,287.09,282.03,150.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of numbers of suggestion for 24 different types of venues for both runs.</figDesc><graphic coords="10,164.61,287.09,282.03,150.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,162.26,661.25,286.73,8.74;10,165.23,484.87,280.79,161.26"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Popularity of 24 different types of venues for both runs.</figDesc><graphic coords="10,165.23,484.87,280.79,161.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,133.77,234.28,247.48,140.61"><head>Table 1 :</head><label>1</label><figDesc>λ for some types of venue 3.3.2 Learning Algorithm and Features.</figDesc><table coords="6,257.71,234.28,95.83,80.90"><row><cell>Type of venue</cell><cell>λ</cell></row><row><cell>Night club</cell><cell>0.6</cell></row><row><cell>Library</cell><cell>0.7</cell></row><row><cell>Spa</cell><cell>0.2</cell></row><row><cell>Cafe</cell><cell>0.9</cell></row><row><cell>Zoo</cell><cell>0.4</cell></row><row><cell>Museum</cell><cell>0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,189.96,339.23,231.33,66.89"><head>Table 2 :</head><label>2</label><figDesc>Results for our two runs and median scores.</figDesc><table coords="8,206.65,339.23,194.13,45.03"><row><cell></cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>simpleScore</cell><cell>0.4332</cell><cell cols="2">0.5871 1.8374</cell></row><row><cell>complexScore</cell><cell>0.4152</cell><cell>0.5777</cell><cell>1.8226</cell></row><row><cell>median</cell><cell>0.2368</cell><cell>0.3415</cell><cell>0.8593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,133.77,540.14,343.71,89.92"><head>Table 3 :</head><label>3</label><figDesc>and 4 present more detailed analys of our results. Share of user/context pairs where particular run returned the best result over all entrants.</figDesc><table coords="8,211.71,562.61,187.82,33.08"><row><cell>Run</cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>simpleScore</cell><cell cols="3">21.97% 48.43% 13.90%</cell></row><row><cell cols="4">complexScore 21.97% 48.43% 15.70%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,236.14,662.63,241.34,8.74"><head>Table 5 :</head><label>5</label><figDesc>demonstrates that the most popular venues for assessor Geographical relevance(G), description(D) and website(W) contributions into P@5 and MRR metrics for both runs.</figDesc><table coords="10,174.27,129.49,259.39,111.67"><row><cell></cell><cell></cell><cell>simpleScore</cell><cell></cell><cell></cell></row><row><cell></cell><cell>G</cell><cell>W</cell><cell>D</cell><cell>Final(WDG)</cell></row><row><cell>P@5</cell><cell>0.9363</cell><cell>0.5776</cell><cell>0.5381</cell><cell>0.4332</cell></row><row><cell cols="2">MRR 0.9675</cell><cell>0.7149</cell><cell>0.6700</cell><cell>0.5871</cell></row><row><cell></cell><cell></cell><cell>complexScore</cell><cell></cell><cell></cell></row><row><cell></cell><cell>G</cell><cell>W</cell><cell>D</cell><cell>Final(WDG)</cell></row><row><cell>P@5</cell><cell>0.9381</cell><cell>0.5668</cell><cell>0.5283</cell><cell>0.4152</cell></row><row><cell cols="2">MRR 0.9720</cell><cell>0.7050</cell><cell>0.6773</cell><cell>0.5777</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,149.01,641.94,294.12,7.21"><p>Google Places APIhttps://developers.google.com/places/documentation/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,149.01,651.44,216.15,7.21"><p>Yandex Rich Content APIhttp://api.yandex.com/rca/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,149.01,660.94,216.05,7.21"><p>Toolkit version 1.0 used, available from http://nltk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,149.01,652.37,160.66,7.21"><p>WordNethttp://wordnet.princeton.edu</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,149.27,149.78,328.22,8.74;12,149.27,161.74,328.22,8.74;12,149.27,173.69,328.21,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,257.37,149.78,220.11,8.74;12,149.27,161.74,102.27,8.74">On the effect of data set size on bias and variance in classification learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Brain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,271.20,161.74,206.28,8.74;12,149.27,173.69,150.93,8.74">Proceedings of the Fourth Australian Knowledge Acquisition Workshop (AKAW &apos;99)</title>
		<meeting>the Fourth Australian Knowledge Acquisition Workshop (AKAW &apos;99)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,149.27,193.62,328.22,8.74;12,149.27,205.57,328.22,8.74;12,149.27,217.53,116.76,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,149.27,205.57,239.29,8.74">Overview of the trec 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,412.88,205.57,64.60,8.74;12,149.27,217.53,85.15,8.74">Text REtrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,149.27,237.45,328.22,8.74;12,149.27,249.41,328.22,8.74;12,149.27,261.36,185.40,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,346.65,237.45,130.84,8.74;12,149.27,249.41,15.94,8.74">Evaluating contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cla</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.75,249.41,293.73,8.74;12,149.27,261.36,31.14,8.74">The Fifth International Workshop on Evaluating Information Access (EVIA)</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,149.27,281.29,328.22,8.74;12,149.27,293.24,328.21,8.74;12,149.27,305.20,328.21,8.74;12,149.27,317.15,293.27,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,335.24,281.29,142.25,8.74;12,149.27,293.24,135.83,8.74">You are what you eat: Learning user tastes for rating prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Elsweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,206.25,305.20,194.76,8.74">String Processing and Information Retrieval</title>
		<title level="s" coord="12,149.27,317.15,152.09,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Kurland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lewenstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Porat</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8214</biblScope>
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,149.27,337.08,328.21,8.74;12,149.27,349.03,328.21,8.74;12,149.27,360.99,328.21,8.74;12,149.27,372.94,188.94,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,223.01,337.08,254.47,8.74;12,149.27,349.03,90.63,8.74">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,260.22,349.03,217.26,8.74;12,149.27,360.99,328.21,8.74;12,149.27,372.94,34.03,8.74">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval (SI-GIR&apos;98)</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval (SI-GIR&apos;98)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,149.27,392.87,328.21,8.74;12,149.27,404.82,328.21,8.74;12,149.27,416.78,45.94,8.74" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">E</forename></persName>
		</author>
		<title level="m" coord="12,243.98,392.87,233.50,8.74;12,149.27,404.82,45.57,8.74">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan and Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct coords="12,149.27,436.71,328.21,8.74;12,149.27,448.66,328.21,8.74;12,149.27,460.62,156.61,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,244.82,436.71,160.12,8.74">Naive bayesian classifiers for ranking</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,427.82,436.71,49.65,8.74;12,149.27,448.66,295.68,8.74">Proceedings of the 15th European Conference on Machine Learning (ECML2004)</title>
		<meeting>the 15th European Conference on Machine Learning (ECML2004)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="501" to="512" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
