<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,99.56,59.28,413.43,17.10">PITT at TREC 2013 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,246.77,83.56,55.09,11.36"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.24,83.56,55.71,11.36"><forename type="first">Daqing</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,99.56,59.28,413.43,17.10">PITT at TREC 2013 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6ED1DEE348CC3237F5B7133E0CDA7F9D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC</term>
					<term>contextual suggestion</term>
					<term>vector space model</term>
					<term>linear regression model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports the IRIS Lab@Pitt's participation to 2013 TREC Contextual Suggestion track, which focuses on technology and issues related to location-based recommender systems (LBRSs). Besides the data provided by the track, our recommendation algorithms also retrieve information from Yelp for creating candidate, example and user profiles. Our algorithms uses linear regression model to combine multiple attributes of candidate profiles into the calculation, and performed 5-fold cross validation for training and testing on 2012 track data. The two runs we submitted this year both obtained reasonable good performance comparing with the median results of all runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recommender systems (RSs) provide personalized suggestions by analyzing users' history record and thus they expand the users' capabilities of interacting with web content <ref type="bibr" coords="1,213.21,370.92,9.72,7.80" target="#b1">[1,</ref><ref type="bibr" coords="1,225.37,370.92,6.48,7.80" target="#b2">2]</ref>. Traditional RSs only focus on user-item ratings, whereas location-based recommender systems combing such ratings with real-world location information <ref type="bibr" coords="1,133.93,402.12,9.72,7.80" target="#b1">[1,</ref><ref type="bibr" coords="1,147.78,402.12,6.48,7.80" target="#b7">8]</ref>. With the advancements in wireless communication and mobile location techniques, accessing users' location information in real time becomes easier and faster <ref type="bibr" coords="1,272.19,422.76,9.72,7.80" target="#b2">[2,</ref><ref type="bibr" coords="1,284.66,422.76,6.48,7.80" target="#b3">3]</ref>. Consequently, many personalized recommendation services integrate such location information. Examples include travel recommendation <ref type="bibr" coords="1,119.21,453.72,9.55,7.80" target="#b5">[5]</ref>, point of interests (POIs, e.g., restaurants, shopping malls, etc.) recommendation <ref type="bibr" coords="1,204.71,464.04,9.71,7.80" target="#b2">[2,</ref><ref type="bibr" coords="1,219.33,464.04,6.48,7.80" target="#b4">4]</ref>, and commercial recommendation <ref type="bibr" coords="1,119.78,474.36,9.55,7.80" target="#b6">[6]</ref>. However, there are still some important open questions. For example, what could be the relationship between users' locations and other factors in recommendation (e.g., user's preferences, the general popularity of items)? How can the location information be effectively utilized in LBRSs? TREC contextual suggestion track provides an open platform with standard testing data for researchers to study LBRSs <ref type="bibr" coords="1,263.17,542.52,9.54,7.80">[9]</ref>. The recommendation task involves the design of a recommender system that can suggest interesting venues to users based on their locations and preference history. The evaluation of each suggestion is based on the relevance of the suggestion to users' preferences and the accessibility of the site for the users. More detail about this track is available in the track's official website <ref type="bibr" coords="1,54.30,615.00,10.45,7.80" target="#b0">[7]</ref> and the track overview paper.</p><p>Our participation to the 2013 TREC Contextual Suggestion track generates two runs of results. Both of them used Yelp API to generate a list of candidates and then used Google to obtain the description of each candidate. We extracted important features (e.g., title, description, category, etc.) for each candidate, and used linear regression model with 5-fold cross validation to compute a ranking model.</p><p>The rest of the paper is organized as follows. Related works are discussed in Section 2. The architecture of whole system and the methods used for data collection, profile creation, ranking as well as description generation are discussed in Section 3. In Section 4, the evaluation of experiment results is described. Finally, conclusions are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Compared with the demands and given data of last year [9], 2013 TREC Contextual Suggestion track has three main differences at candidate resources, the content of given data, and the format of results. The corpora for this year's track include ClueWeb12 (i.e., a dataset containing 870,043,929 English web pages) [10] as a choice of data source in addition to open web. While context file eliminates the temporal attribute (i.e., time, day and season), which reduces some factors that should be considered when collecting candidates, the number of testing users (increasing from 34 to 550) and the ratings (which are on a 5-point scale rather than 3-point scale) still bring new challenges to participants.</p><p>Top five runs of last year, including iritSplit3CPv1 <ref type="bibr" coords="1,516.31,362.04,13.77,7.80" target="#b8">[11]</ref>, guinit <ref type="bibr" coords="1,318.30,372.36,13.77,7.80" target="#b9">[12]</ref>, gufinal <ref type="bibr" coords="1,371.71,372.36,13.77,7.80" target="#b9">[12]</ref>, UDInfoCSTc <ref type="bibr" coords="1,446.61,372.36,13.77,7.80" target="#b10">[13]</ref>, and PRISabc [14], are analyzed in this section. Based on the concerns of this track, we mainly focus on the candidate collection and ranking algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Candidate Collection</head><p>Table <ref type="table" coords="1,344.15,421.32,4.44,7.80" target="#tab_0">1</ref> analyzes different approaches of these five runs on candidate collection. Compared the data source, three mainstream open datasets (i.e., Google Places, Yelp, and Foursquare), storing the information of locations, are all used. Among them, two runs use Google Places <ref type="bibr" coords="1,387.27,462.60,14.20,7.80" target="#b8">[11,</ref><ref type="bibr" coords="1,404.00,462.60,11.96,7.80" target="#b9">12]</ref> and the other two select Yelp <ref type="bibr" coords="1,527.38,462.60,14.25,7.80" target="#b9">[12,</ref><ref type="bibr" coords="1,544.15,462.60,10.66,7.80" target="#b10">13]</ref>. Consequently, these two datasets are also used in our method. As to the collecting approaches, three of them formulate the query by the categories of each example as well as each context <ref type="bibr" coords="1,526.56,493.80,14.27,7.80" target="#b8">[11,</ref><ref type="bibr" coords="1,544.15,493.80,10.66,7.80" target="#b9">12]</ref>. The other two only consider about contexts <ref type="bibr" coords="1,477.66,504.12,14.23,7.80" target="#b10">[13,</ref><ref type="bibr" coords="1,494.34,504.12,10.66,7.80">14]</ref>. According to the result of these five runs [9], using a pair of context and categories as a query obtained more related candidates. Since the contexts of this year do not have temporal attributes, in this paper, the combination of category for each example and geo coordinates belong to each context is used as a query to search candidates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking</head><p>Among five aforementioned runs, analyzing the sentiment of users to each example based on users' ratings and computing the similarity between candidates and examples to predict the sentiment score of each candidate for each user is the general idea of ranking candidates. For similarity computation, three of five runs <ref type="bibr" coords="2,72.11,386.04,14.22,7.80" target="#b8">[11,</ref><ref type="bibr" coords="2,88.67,386.04,11.22,7.80" target="#b10">13,</ref><ref type="bibr" coords="2,102.23,386.04,11.96,7.80">14]</ref> select Vector Space Model (VSM) (See Table <ref type="table" coords="2,284.64,386.04,3.24,7.80" target="#tab_1">2</ref>), showing that this model is quite useful in the computation of similarity. Thus, in this paper, we also choose VSM to compute the similarity and our basic idea is similar to <ref type="bibr" coords="2,216.56,416.76,13.73,7.80" target="#b10">[13]</ref>. Through the analysis of existing works, candidates' descriptions <ref type="bibr" coords="2,318.30,231.96,14.22,7.80" target="#b8">[11,</ref><ref type="bibr" coords="2,338.22,231.96,11.96,7.80">14]</ref> and categories <ref type="bibr" coords="2,416.67,231.96,14.23,7.80" target="#b9">[12,</ref><ref type="bibr" coords="2,436.60,231.96,11.96,7.80" target="#b10">13]</ref> are two key factors when considering the ranking problem. However, these five runs only consider the similarity based on one aspect rather than combining these two factors. Except the constraints of contexts (i.e., geo location or temporal attributes), current works mainly focus on the matching of personal preferences, whereas the general popularity of sites and the accessibility from users' location to the site are other important factors when considering the problem of ranking.</p><p>In this paper, five features, containing users' preferences, general popularity as well as accessibility, are extracted from each candidate and considered for ranking. Hence, a more comprehensive way used for ranking candidates is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM STATEMENT</head><p>Given the information about users, we aim to provide recommendations by considering the users' contexts, personal preferences as well as the general popularity of candidates to be recommended. The given information includes a set of contexts (only containing locations) L, a set of example suggestions E located in Philadelphia including title, description and url, and a set of ratings R provided by the users. Two types of ratings, of which one describe the example e's title and description r t+d (u, e) and one judge the example e's website r w (u, e), are included in R graded by each user u. With this information in mind, the problems of providing recommendations can be formalized as follows:</p><p>• Candidate Collection. Given E and L, search for a set of candidate sites C = c e ∈ E, 𝑙 ∈ L}, where c is similar with at least one e and c is around 𝑙.</p><p>• Feature Extraction. Given C and E, extract features from c ∈ C and e ∈ E that represent the user u's personal preferences. For c, features representing its general popularity and accessibility also should be extracted. The set of candidates and examples should be reorganized as: </p><formula xml:id="formula_0" coords="2,354.00,597.60,204.34,21.60">C ! = c, 𝑙 f ! , f ! , … , f !" } and E ! = {e|f ! , f ! , … , f !" } ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OUR APPROACH</head><p>The Pitt recommendation system designed for 2013 TREC Contextual Suggestion track uses data collected from Yelp. Our approach also uses VSM to compute the similarity of descriptions between candidates and examples, and a linear regression model <ref type="bibr" coords="3,54.30,161.16,14.95,7.80">[15]</ref> to compute the ranking score for each candidate. The system was trained and tested using 5-fold cross validation on 2012 track data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The framework of our approach is shown in Figure <ref type="figure" coords="3,249.23,210.12,3.36,7.80">1</ref>. There are four main parts: 1) data collection module gets the relevant information of examples and candidates from Yelp1; 2) profile construction module creates and maintains users' profiles; 3) candidate ranking module generates a list of candidate sites in order based on the context l for each user u; and 4) description generation module produces a description for each ranked candidate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Candidate Collection</head><p>As the problem in candidate collection mentioned above, all candidates collected from open web should be similar with at least one example suggestion, so that we can detect users' preferences on these candidates based on users' ratings to relevant examples. Thus, how to determine the similarity between candidates and examples is a problem that we need to consider during the candidate collection. Based on the analysis of related works, each item returned by open geo-dataset (i.e., Google Places, Yelp and Foursquare) is accompanied by its categories, and hence we used the category classified by these geo-datasets to judge the similarity. We believe that any two items with a common category are similar to each other. In this case, each item in E also needs to be searched on the open dataset to obtain its categories. The way for us to obtain such information is by searching the titles of examples and their located city "Philadelphia" on the dataset.</p><p>Initially, we choose Google Places API for searching data. Due to the context constraint of candidates, we tried to use the "nearby search" function to collect nearby places around the users' contexts as the candidates. We therefore performed Google Place search with the radius of 20km to the users' context to look for candidates. However, by analyzing the categories of returned results for both candidates and examples, we find that the candidates returned by Google Places are for both commercial and 1 http://www.yelp.com/ non-commercial use, whereas the examples of E were mainly focus on commercial places. Therefore Google Places gives us lots of non-relevant candidates. Also, Google Places' categories contain many sub-categories and most of returned results always contain a sub-category "establishment", which is hard for us to determine whether the candidate is similar with any item of E. For example, Smokey Joe's (an example suggestion in 2012 TREC Contextual Suggestion) is labeled as "cafe", "restaurant", "food", and "establishment". One candidate we collected, Seven Dolors Catholic Church, is labeled as "church", "place of worship", "establishment". These two items are completely irrelevant and therefore it was a mistake for our way to classify these two items as similar because they share a common sub-category "establishment.</p><p>We therefore explored Yelp as the data source for identifying candidates. The advantages of using Yelp include: 1) places in Yelp are mainly commercial sites, which is more align with the relevant candidates in this track; 2) the category structure in Yelp is simple and more direct to identify whether the candidate is similar with any example suggestion. For example, Smokey Joe's is only categorized as bars, which is specific enough to be used directly in our task; and 3) the data for each candidate is accompanied with more detail reviews than those of Google Places --the more information that we can harvest to describe the candidate. However, Yelp only returns at most 20 sites for each context, which are less than 50 candidates we want to achieve. Also, some results of nearby search are not similar with examples. We, therefore, complement the Yelp location search with the categories of examples. For each category, at most 20 related results are returned. As there are 50 examples in E, for each context 𝑙, at most 1000 results can be obtained. However, examples with the same category brought duplicated results. In this case, those results that have the same title with the former one are eliminated. Finally, the candidate set is composed by processed results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Profile Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Candidate Profile</head><p>For each candidate suggestion, nine attributes, presented in detail in Table <ref type="table" coords="4,89.22,145.08,3.36,7.80" target="#tab_4">3</ref>, are extracted. These attributes are used to represent candidates and compute the ranking score by considering personal preferences, general popularity as well as context constraints. Of the nine attributes, three are focused on the matching of users' preferences, two on the representation of candidates in the final result, two on the accessibility by considering users' current contexts, one on the general popularity and one on the identification of each candidate (See "Usage" in Table <ref type="table" coords="4,261.87,217.32,3.24,7.80" target="#tab_4">3</ref>). This profile was created by indexing those nine attributes for each candidate as a record with Lucene, a java-based platform that provides indexing and searching technologies, as well as text preprocessing.</p><p>For the consideration of each candidate's category, we assume that users who like a specific object might like a broader area where that object belongs. For example, if a person likes sushi and disserts, we infer that he/she likes food. Thus, Lv1-category is defined by classifying Lv2-category (i.e., the specific categories of all candidates from Yelp) manually (See Table <ref type="table" coords="4,264.65,327.00,3.24,7.80" target="#tab_5">4</ref>). The classification rule is constructed by considering the category hierarchy of Foursquare <ref type="bibr" coords="4,152.82,347.64,14.95,7.80">[16]</ref> and Yelp [17] as well as our experience.</p><p>Generally, people are easier affected by community and hence we assume that a candidate with high general popularity, belong to the category that a user prefers, is more likely than other candidates in the same category to attract a user's attention. In this way, we extracted rating from raw data collected on Yelp to reflect the general popularity of a candidate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Example Profile</head><p>The example profile is used to represent the users' preferences that can be matched with candidates. Similar to the candidate profile, six attributes except rating, distance, and coordinate, are stored using Lucene to represent each example. Of these attributes, the information of title, descriptions and url comes from the original data set E while others are accessed like candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">User Profile</head><p>To solve the problem of preferences detection, we created user profile. Based on the 5-point scale of rating, we defined that point 4 and point 3 as positive rating, point 2 as neutral rating and point 1 as well as point 0 as negative rating. In this way, with the consideration of both r t+d (u, e) and r w (u, e)，examples without negative ratings are classified into positive set 𝐸 ! ! (𝑢) while those without positive ratings are classified into negative set 𝐸 ! ! (𝑢). In our system, examples with both neutral ratings are ignored. Both 𝐸 ! ! (𝑢) and 𝐸 ! ! (𝑢) only store the examples' id. The user profile was created to store these two kinds of example set for each user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ranking Model</head><p>In this section, how to fuse the information of three profiles and compute the ranking score for each candidate is discussed. Among constructed profiles, five features including similarity, level-2 category, level-1 category, rating and distance are computed or extracted based on the pair of user and context. The ranking model is designed by combining the value of five features using linear regression model with 5-fold cross validation. With the computation of ranking model, each candidate obtains a ranking score and at most top 50 are selected as suggestions recommended to users at different context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Similarity</head><p>Based on the general idea of literature <ref type="bibr" coords="4,466.86,496.92,13.77,7.80" target="#b10">[13]</ref>, the computation of similarity for each pair of candidate c i and user u j whose context is 𝑙 ! is focused on comparing c i 's (a candidate suggestion in 𝐶 ! around 𝑙 ! ) review and the description of examples in both 𝐸 ! ! (𝑢 ! ) and 𝐸 ! ! (𝑢 ! ), computing the similarity between c i and u j 's positive set - 𝑆𝑖𝑚 (𝑐 ! , 𝑢 ! ) ! as well as negative set - 𝑆𝑖𝑚 𝑐 ! , 𝑢 ! ! using vector space model, and combine the results. To normalize 𝑆𝑖𝑚 (𝑐 ! , 𝑢 ! ) ! and 𝑆𝑖𝑚 (𝑐 ! , 𝑢 ! ) ! , we divide the value by N P , the number of examples in positive example set, and N N , the number of examples in negative example set, separately. The formula is shown as the follower:</p><formula xml:id="formula_1" coords="4,318.30,623.28,239.15,43.31">𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 𝑐 ! , 𝑢 ! = !"# ! ! ,! ! ! ! ! - !"# ! ! ,! ! ! ! ! = !"# ! ! ,! ! ! ! ∈! ! ! ! ! - !"# ! ! ,! ! ! ! ∈! ! ! ! ! (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Lv2-category &amp; Lv1-category</head><p>In addition to the similarity of description, category is also a key factor to judge whether a candidate meets user's interests. Usually, users who like one particular place also prefer other places that belong to the same category. In this paper, we judge whether a candidate c i 's category c i _cat is the one that u j prefers or not by determining whether the category set of 𝐸 ! ! 𝑢 ! or 𝐸 ! ! 𝑢 ! contains c i _cat. If c i _cat belongs to the category set of 𝐸 ! ! 𝑢 ! or 𝐸 ! ! 𝑢 ! , we will set the value of corresponding category feature to 1 or -1. Otherwise, the value will be set to 0. Figure <ref type="figure" coords="5,80.05,125.88,4.44,7.80" target="#fig_0">2</ref> shows the flowchart of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Rating &amp; Distance</head><p>These two features are directly accessed from candidate profile. Rating represents the general popularity of candidates and distance determines whether the candidate meets the contextual constraint that the site should be accessed within a 5h drive from the context of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Integration</head><p>In this paper, a linear regression model is used to combine the values of aforementioned five features into computing the final ranking score. Using the data of last year as training data, we compute the weight of each feature by SPSS, a software package that contains statistic models used for analysis. To evaluate the performance of the linear regression model, we used 5-fold cross validation on last year's results. Based on our data, the weight of each feature is set as the following: w s = -8.660, w c1 = 0.351, w c2 = 0.013, w r = -0.086, w d = 0. One problem of our approach is that we didn't consider the degree of value variation between features, which is the main reason why the weight of distance is zero. After normalizing the features of last year's judgment data and rerunning the model, of five features reported by SPSS, Lv1category is the only significant feature, with the p-value as 0.000. Although Yelp's reviews are comparatively more detail than Google Places', most of them expressed the feeling of users rather than the description of candidates. We think this leads to the similarity without significance. In the future, we will solve such problem by accessing the description of candidates from other data sources like candidates' official website or social networks. The final ranking score formula is shown in the following: </p><formula xml:id="formula_2" coords="5,54.95,449.12,238.75,179.16">𝑆𝑐𝑜𝑟𝑒 𝑐 ! , 𝑢 ! = 𝑤 ! ×𝑆𝑖𝑚 𝑐 ! , 𝑢 ! + 𝑤 !! ×𝐿𝑣1 !"! + 𝑤 !! ×𝐿𝑣2 !"# + 𝑤 ! ×𝑅𝑎𝑡𝑖𝑛𝑔 + 𝑤 ! ×𝐷𝑖𝑠 (2) ci_cat =el_cat el∈EP Category of ci (ci_cat) Set ci_cat = 1 Y ci_cat =ek_cat ek∈EN N Y Set ci_cat = -1 N Set ci_cat = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Description Generation</head><p>Originally, we planed to use reviews on Yelp as descriptions for candidates. However, those reviews are a bit broad. For example, one review for Abraham Lincoln Presidential Library and Museum is "A-Maz-Ing! I have been told by several different members of my family and several of my friends that I would love this Museum, and I was NOT...", which mainly describes reviewer's feeling instead of the site itself. Thus, we switched to searching each candidate's title on Google through the URL connection and intercepted the snipped text of the first result from HTML metadata as a description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND EVALUATION</head><p>In this section, two runs, submitted to TREC 2013 Contextual Suggestion Track, are introduced. Also, the official results of these two runs are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submitted Runs</head><p>For this year's participation, two runs, labeled as ming_1 and ming_2, are both constructed based on the same data sets and ranking methods. The differences are mainly on the data collection:</p><p>• The first run ming_1 only accessed the first returned result from Yelp when using the category of each example with each context as a query. In this situation, the best result is 50 candidates for each context when the categories of each example are different. Also, the description of each candidate is based on the reviews on Yelp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The second run ming_2 obtained at most top 20 returned results from Yelp when using the same query as the first run. The reason to expand the number of results is because many categories overlap between different examples so that many instances of searching results are duplicated. Also, the description of each candidate is based on both reviews on Yelp and snipped text searched on Google.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>In this paper, all candidates are judged based on the context and the attractiveness of description. All judgments are collected by NIST from user themselves. After accessing users' judgments, NIST computes the value of P@5, MRR as well as TBG for each candidate in each run. Generally, for each user, NIST randomly selects two contexts' candidates to evaluate the performance of recommendations. Also, NIST provides the best, median and the worst results for each selected pair of user and context based on the results of all participated runs.</p><p>According to the results of P@5 displayed on Figure <ref type="figure" coords="5,512.44,515.40,4.44,7.80">3</ref> and Figure <ref type="figure" coords="5,318.30,525.72,3.36,7.80">4</ref>, where the red line shows the comparison between the best results and the median results, the green line shows the comparison of worst results and median ones, and the bar chart shows our results compared with median performance, many cases of our approach are better than the median value according to the users' preferences and contexts, even some of them achieved the best results.</p><p>Figure <ref type="figure" coords="5,344.61,604.20,4.44,7.80">3</ref> shows the judgment of each user to recommendations at different contexts. By comparing our results with median value, both of our runs have over 30% cases better than the average performance of all runs and over 35% cases are similar with the average. Based on the overall performance, Ming_1 is better than Ming_2, which shows that candidates constructed by the first returned result from Yelp are more consistent with example set and thus the computation of similarity between each pair of user and candidate is more precise. However, there are 10% of cases in Ming_2 that achieved the best value while in Ming_1 is 8%. We consider this is because the number of candidates in Ming_2 is</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,65.49,645.00,217.66,7.80;5,121.73,655.08,105.19,7.80"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The flow chart of setting candidates' categories based on user's preferences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,58.35,407.51,215.97,265.58"><head></head><label></label><figDesc></figDesc><graphic coords="7,58.35,407.51,215.97,265.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,319.03,569.88,235.86,141.75"><head>Table 1 . Summary of candidate collection for top 5 runs</head><label>1</label><figDesc></figDesc><table coords="1,319.03,584.68,235.86,126.95"><row><cell></cell><cell></cell><cell></cell><cell>Problems (P)</cell></row><row><cell>Runs</cell><cell>Data Source</cell><cell>Approach</cell><cell>&amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Solutions (S)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>P:</cell></row><row><cell></cell><cell></cell><cell>• Get the categories of</cell><cell>The limitation of</cell></row><row><cell></cell><cell></cell><cell>each example on</cell><cell>GP for each</cell></row><row><cell></cell><cell></cell><cell>GP;</cell><cell>query;</cell></row><row><cell>iritSplit3 CPv1</cell><cell>Google Places (GP)</cell><cell>• Based on the temporal component,</cell><cell>S: Split each query into 3 sub</cell></row><row><cell></cell><cell></cell><cell>construct place sets</cell><cell>queries</cell></row><row><cell></cell><cell></cell><cell>with different</cell><cell>according to the</cell></row><row><cell></cell><cell></cell><cell>categories;</cell><cell>categories of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>each example</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,56.89,57.64,497.61,658.79"><head>Table 2 . Summary of ranking candidates for top 5 runs Runs Used Model Approach</head><label>2</label><figDesc></figDesc><table coords="2,56.89,460.12,234.33,256.31"><row><cell></cell><cell></cell><cell>• For each user, constructing a positive</cell></row><row><cell></cell><cell></cell><cell>vector VP to represent all terms in</cell></row><row><cell></cell><cell></cell><cell>examples that user prefers and a negative</cell></row><row><cell>iritSplit3 CPv1</cell><cell>VSM</cell><cell>vector VN to represent the terms of example that user dislikes;</cell></row><row><cell></cell><cell></cell><cell>• Remove stopwords for each candidate;</cell></row><row><cell></cell><cell></cell><cell>• Compute the ranking score for each</cell></row><row><cell></cell><cell></cell><cell>candidate based on the similarity with VP</cell></row><row><cell></cell><cell></cell><cell>and VN;</cell></row><row><cell></cell><cell></cell><cell>• Build a matrix, counting the number of</cell></row><row><cell></cell><cell></cell><cell>positive suggestions (judged by initial</cell></row><row><cell></cell><cell></cell><cell>ratings) for each profile and each</cell></row><row><cell></cell><cell></cell><cell>category, to determine the category</cell></row><row><cell></cell><cell></cell><cell>score;</cell></row><row><cell>guinit</cell><cell>SVMRank</cell><cell>• Process each raw category score;</cell></row><row><cell></cell><cell></cell><cell>• Rank a list of resources for each</cell></row><row><cell></cell><cell></cell><cell>category using SVMRank and Google</cell></row><row><cell></cell><cell></cell><cell>ranking;</cell></row><row><cell></cell><cell></cell><cell>• Select top 10 results of each category</cell></row><row><cell></cell><cell></cell><cell>and merge them by their category</cell></row><row><cell></cell><cell></cell><cell>scores;</cell></row><row><cell></cell><cell></cell><cell>• Do the same way as guinit, the only</cell></row><row><cell>gufinal</cell><cell>SVMRank</cell><cell>difference is positive suggestions are</cell></row><row><cell></cell><cell></cell><cell>judged by final ratings;</cell></row><row><cell>UDInfo CSTc</cell><cell>VSM</cell><cell>• The basic idea is to compute the similarity between each candidate and</cell></row><row><cell></cell><cell></cell><cell>each example. Then, separate examples</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,336.15,609.92,222.22,81.52"><head></head><label></label><figDesc>where 𝑙 ∈ L , f represents a feature, NC is the number of candidate features and NE is the number of example features.Candidate Ranking. Given C ! , E ! , E ! ! (u) and E ! ! (u), integrating the value of different features and compute the ranking score of c ∈ C ! for each user u at the context 𝑙.</figDesc><table /><note coords="2,336.15,650.40,222.18,10.34;2,354.00,660.72,138.67,9.65;2,493.05,662.48,65.31,9.28;2,354.00,671.28,136.83,9.65;2,491.22,673.04,67.15,9.28;2,354.00,683.64,6.72,7.80;3,72.15,58.55,4.08,8.19"><p><p><p>•</p>Preferences Detection. Given R and E ! , identify a set of examples with positive sentiments E ! ! (u) and a set of examples with negative sentiments E ! ! (u) for each user u.</p>•</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,323.34,430.68,227.79,284.07"><head>Table 3 . The description of attributes in candidate profile</head><label>3</label><figDesc></figDesc><table coords="3,323.34,445.24,227.79,269.51"><row><cell>Attributes</cell><cell>Description</cell><cell>Usage</cell></row><row><cell></cell><cell>The combination of each context's</cell><cell></cell></row><row><cell>Id</cell><cell>geometry and the sequence number of results for each</cell><cell>Identification</cell></row><row><cell></cell><cell>context;</cell><cell></cell></row><row><cell>Title</cell><cell>The name of candidate;</cell><cell>Candidate Representation</cell></row><row><cell></cell><cell>A bag of terms, which are created</cell><cell></cell></row><row><cell></cell><cell>by removing common stopwords</cell><cell></cell></row><row><cell></cell><cell>and stemming by Lucene from</cell><cell>Users'</cell></row><row><cell>Review</cell><cell>snipped text on Yelp and Google</cell><cell>Preferences</cell></row><row><cell></cell><cell>search engine (the first is only</cell><cell>Matching</cell></row><row><cell></cell><cell>based on Yelp; the second run is</cell><cell></cell></row><row><cell></cell><cell>based on two resources);</cell><cell></cell></row><row><cell>Url</cell><cell>The link address of Yelp webpage describing each candidate;</cell><cell>Candidate Representation</cell></row><row><cell>Lv2-category</cell><cell>Specific category collected from Yelp;</cell><cell>Users' Preferences Matching</cell></row><row><cell>Lv1-category</cell><cell>Broad category collected manually;</cell><cell>Users' Preferences Matching</cell></row><row><cell>Rating</cell><cell>The general popularity of each candidate graded by Yelp users;</cell><cell>General Popularity</cell></row><row><cell>Distance</cell><cell>The distance between each</cell><cell>Accessibility</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,60.55,57.72,491.66,657.48"><head>Table 4 . The description of Lv1-category &amp; Lv2-category</head><label>4</label><figDesc></figDesc><table coords="4,60.55,457.08,227.65,258.12"><row><cell>Lv1-category</cell><cell></cell><cell cols="2">Lv2-category</cell></row><row><cell></cell><cell cols="4">cafes, mideastern, desserts, mediterranean,</cell></row><row><cell></cell><cell cols="4">food, sandwiches, turkish, coffee, icecream,</cell></row><row><cell></cell><cell>gourmet,</cell><cell>chinese,</cell><cell>burmese,</cell><cell>italian,</cell></row><row><cell>Food</cell><cell cols="4">localflavor, irish, tea, bbq, mexican, japanese,</cell></row><row><cell></cell><cell cols="4">dimsum, sushi, mediterranean, vegetarian,</cell></row><row><cell></cell><cell cols="4">greek, korean, chicken_wings, breweries,</cell></row><row><cell></cell><cell>chocolate;</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">newamerican, ticketsales, theater, arts,</cell></row><row><cell>Art &amp; Entertainment</cell><cell cols="4">galleries, arcades, venues, hindu_temples, theater, spas, massage, laser_hair_removal, museums, jazzandblues, musicvenues, mini_golf, bowling, yoga, comedyclubs,</cell></row><row><cell></cell><cell cols="2">pilates, sportsteams</cell><cell></cell></row><row><cell></cell><cell cols="4">bars, wine_bars, juicebars, eventplanning,</cell></row><row><cell>Nightlife</cell><cell cols="4">danceclubs, gastropubs, sportsbars, lounges,</cell></row><row><cell></cell><cell cols="4">divebars, jazzandblues, pubs, comedyclubs</cell></row><row><cell>Outdoor</cell><cell cols="4">zoos, parks, amusementparks, gardens, lakes, travelservices, ticketsales</cell></row><row><cell></cell><cell>homedecor,</cell><cell cols="2">deptstores,</cell><cell>stationery,</cell></row><row><cell>Shopping</cell><cell cols="4">tobaccoshops, icecream, gourmet, grocery, localflavor, shoppingcenters, bookstores,</cell></row><row><cell></cell><cell cols="4">farmersmarket, fleamarkets, hobbyshops,</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>much larger than Ming_1 and thus people are more likely to access their preferred recommendations.</p><p>Figure <ref type="figure" coords="6,81.29,82.20,4.44,7.80">4</ref> shows the performance of two runs based on contexts. By recommending suggestions to different people at the same context, the results of two runs show that over 45% cases are better than the median value and over 30% cases are similar with the median results. Unfortunately, evaluation in this way illustrates none of our results got to the best value.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a). Ming_1 compared with Median by Context</head><p>To further analyzing the performance of our two runs, we referred to the idea of Georgetown's presentation <ref type="bibr" coords="7,212.54,278.76,14.95,7.80">[18]</ref> and explored the relationship between our results and the population size of contexts. Based on the data of population for each context in 2012 on Google, contexts are divided into four classes shown in Figure <ref type="figure" coords="7,54.30,320.28,3.36,7.80">5</ref>, where records in yellow are first class cities (i.e., cities with more than 1,000,000 inhabitants), in green are second class (i.e., cities with a population between 100,000 and 1,000,000), in red are third class (i.e., cities with a population between 50,000 and 100,000) and the rest are fourth class (i.e., cities with not more than 50,000 inhabitants). The contexts used for judgments have one first class city, 12 second-class cities, 19 third-class cities and 14 belong to the fourth class.</p><p>Based on the average precision of both runs for each kind of city type, the performance of the approach proposed in this paper is determined by the population size of contexts. The results show that the average precision of two runs decreases with the size of population increases (See Table <ref type="table" coords="7,437.95,309.96,3.24,7.80">5</ref>). We hypothesize that contexts with a large population might have more candidates than those with small size, so that how to effectively rank them according to uses' preferences become quite challenging. Also, the number of noise candidates for large-scale contexts might be higher than that for small-scale contexts, which might be another reason that leads to the lowest performance of our approaches on metropolises. All these assumptions will be verified in the future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5. Results based on the scale of context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>City Type Avg_P@5 compared with median results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND FUTURE WORKS</head><p>This paper discusses UPitt's participation in the 2013 TREC Contextual Suggestion track, which aims to design and implement a location-based recommender system based on given testing data about their locations and preferences. Two runs, using VSM model and linear regression model on data collected from Yelp and Google, were submitted. A multi-criteria ranking approach considering personal preferences, general popularity and accessibility was proposed in this paper. For both runs, all suggestions meet the location constraint that the distance between each pair of candidate and user should be less than five-hour drive away. Based on the evaluation of NIST, the performance of both runs is reasonable good compared with the median performance.</p><p>One limitation of our approach lies in that the level-1 categories of candidate locations were labeled manually. Therefore, it remains unclear whether the locations can be categorized automatically and how well human wisdom can be approximated.</p><p>As shown in our analysis, such categorical information is beneficial for the TREC contextual suggestion task. Thus, this also suggests that one of our future works is to solve this problem.</p><p>--0.  Besides, due to the reviews on Yelp focus more on users' feelings than candidates' descriptions, the accuracy of similarity computation is hard to guarantee. So in the future, we plan to collect such information from other data sources like social networks or candidates' official websites to improve the accuracy of similarity computation. Also, providing a list of options to users in contexts with a large population size, which can highly satisfied their personal demands is another challenging work need to be considered in the future.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,58.80,157.18,92.38,10.54" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,170.52,197.16,7.80;8,72.30,180.84,204.44,7.80;8,72.30,191.16,219.71,7.80;8,72.30,201.24,56.21,7.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,72.30,180.84,204.44,7.80;8,72.30,191.16,78.59,7.80">Context relevance assessment and exploitation in mobile recommender systems</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,157.03,191.16,130.76,7.80">Personal and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="507" to="526" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,215.88,216.02,7.80;8,72.30,226.20,206.97,7.80;8,72.30,236.52,205.69,7.80;8,72.30,246.60,26.21,7.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,72.30,226.20,206.97,7.80;8,72.30,236.52,55.05,7.80">Recommending friends and locations based on individual location history</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,133.04,236.52,140.42,7.80">ACM Transactions on the Web (TWEB)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,261.24,204.52,7.80;8,72.30,271.56,221.64,7.80;8,72.30,281.88,216.40,7.80;8,72.30,292.20,207.44,7.80;8,72.30,302.28,22.72,7.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,232.87,261.24,43.96,7.80;8,72.30,271.56,221.64,7.80;8,72.30,281.88,29.52,7.80">Anonymous usage of location-based services through spatial and temporal cloaking</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,117.54,281.88,171.16,7.80;8,72.30,292.20,162.49,7.80">Proceedings of the 1st international conference on Mobile systems, applications and services</title>
		<meeting>the 1st international conference on Mobile systems, applications and services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003-05">2003. May</date>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,316.92,202.40,7.80;8,72.30,327.24,219.85,7.80;8,72.30,337.56,215.10,7.80;8,287.49,335.39,4.67,5.27;8,72.30,348.12,193.70,7.80;8,72.30,358.20,152.72,7.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,72.30,327.24,219.85,7.80;8,72.30,337.56,120.93,7.80">Location-based and preference-aware recommendation using sparse geo-social networking data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Mokbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,208.75,337.56,78.65,7.80;8,287.49,335.39,4.67,5.27;8,72.30,348.12,193.70,7.80;8,72.30,358.20,73.67,7.80">Proceedings of the 20 th International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 20 th International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-11">2012, November</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,372.60,211.14,7.80;8,72.30,383.16,211.16,7.80;8,72.30,393.48,208.32,7.80;8,72.30,403.80,186.39,7.80;8,72.30,413.88,133.44,7.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,153.28,383.16,130.17,7.80;8,72.30,393.48,208.32,7.80;8,72.30,403.80,41.01,7.80">A context-aware personalized travel recommendation system based on geotagged social media data mining</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Majid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Woodward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,119.54,403.80,139.15,7.80;8,72.30,413.88,71.21,7.80">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="662" to="684" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,428.52,198.75,7.80;8,72.30,438.84,206.41,7.80;8,72.30,448.92,154.71,7.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,201.87,428.52,69.18,7.80;8,72.30,438.84,176.66,7.80">A recommendation mechanism for contextualized mobile advertising</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,254.75,438.84,23.96,7.80;8,72.30,448.92,92.57,7.80">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="414" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,477.96,202.40,7.80;8,72.30,488.28,213.34,7.80;8,72.30,498.36,100.96,7.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,221.87,477.96,52.84,7.80;8,72.30,488.28,78.59,7.80">Context-aware recommender systems</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,166.77,488.28,118.87,7.80">Recommender systems handbook</title>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="217" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,551.88,212.02,7.80;8,72.30,561.96,65.46,7.80" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,170.83,551.88,113.49,7.80;8,72.30,561.96,61.59,7.80">IRIT at TREC 2012 Contextual Suggestion Track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cabanac</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.30,576.60,220.56,7.80;8,72.30,586.92,206.69,7.80;8,72.30,597.00,118.00,7.80" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Deboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kunath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Not Too. Personalized Learning to Rank for Contextual Suggestion</note>
</biblStruct>

<biblStruct coords="8,72.30,611.64,208.19,7.80;8,72.30,621.72,151.74,7.80" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,179.37,611.64,101.12,7.80;8,72.30,621.72,147.90,7.80">An Exploration of Rankingbased Strategy for Contextual Suggestion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
