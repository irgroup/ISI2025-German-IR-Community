<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,66.26,71.87,477.19,16.84;1,112.10,91.79,385.52,16.84">CWI and TU Delft at TREC 2013: Contextual Suggestion, Federated Web Search, KBA, and Web Tracks</title>
				<funder ref="#_qZHr5yY">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_6KYesKQ">
					<orgName type="full">Netherlands Organization for Scientific Research</orgName>
				</funder>
				<funder ref="#_YsM6WYv">
					<orgName type="full">European Comission FP7</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,78.79,136.82,97.02,11.06;1,175.80,137.44,1.72,5.24"><forename type="first">Alejandro</forename><surname>Bellogín</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde en Informatica</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,186.38,136.82,157.88,11.06;1,344.26,137.44,1.72,5.24"><forename type="first">Gebrekirstos</forename><forename type="middle">G</forename><surname>Gebremeskel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde en Informatica</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.83,136.82,42.51,11.06;1,397.35,137.44,1.72,5.24"><forename type="first">Jiyin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde en Informatica</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,407.92,136.82,53.62,11.06;1,461.54,137.44,1.27,5.24"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park, Maryland</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,472.51,136.82,51.17,11.06;1,523.67,137.44,1.72,5.24"><forename type="first">Alan</forename><surname>Said</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde en Informatica</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,156.66,149.27,69.76,11.06;1,226.41,149.89,1.72,5.24"><forename type="first">Thaer</forename><surname>Samar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde en Informatica</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.99,149.27,87.73,11.06;1,324.72,149.89,1.72,5.24"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Centrum Wiskunde en Informatica</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.72,149.27,110.41,11.06;1,449.13,149.89,1.72,5.24"><forename type="first">Jeroen</forename><forename type="middle">B P</forename><surname>Vuurens</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Ratio Springfield</orgName>
								<address>
									<addrLine>414 106 25.60 La Crosse, 696 329 47.27 Orlando, 467 128 27.41 Corpus Christi, 185 26.70 Palm Bay</addrLine>
									<postCode>138996 775, 206 26.58, 105864 599 190 31.72, 160525 665 193 29.02, 51675, 74479 539 122 22.63, 61196 423 125 29.55, 40146 246 62 25.20, 81836, 956 82 8.58, 67752 796 107 13.44, 586429 880, 306 34.77, 633701, 992 339 34.17, 313093 817 395 48.35, 45116 390 114 29.23, 201153, 912922, 1125 377 33.51, 4165769, 1456 571 39.22, 42111 377 90 23.87, 13728 499 78 15.63, 84688 436 107 24.54, 140537 565 174 30.80, 72972 633 88 13.90, 637727, 1097 355 32.36, 349842, 1122 307 27.36, 32670 548 72 13.14, 57243 419, 106 25.30, 106887 472 161 34.11, 102026 525 185 35.24, 30373 499 96 19.24, 50061 585 107 18.29, 429068, 1044 323 30.94, 55055 400 150 37.50, 64378 826 129 15.62, 101923 472 126 26.69, 1587486 1161 550 47.37, 74581, 172425 821 191 23.26, 67954 372, 106 28.49, 46826 531 135 25.42, 264770 687, 242 35.23, 230483 793 200 25.22, 85636 530 159 30.00, 138705 581 193 33.22, 26491 667 83 12.44, 243803 743 298 40.11, 106302 494 158 31.98, 15686 671 45 6.71, 185157 693, 33922, 1064 69 6.48, 160996 435 197 45.29, 95818 698, 152 21.78, 13548982 33696 9369</postCode>
									<settlement>Cheyenne, Fargo, Kennewick, Valdosta, Houma, Greenville, Hickory, Cincinnati, St. Louis, Asheville, Beckley, Myrtle Beach, Washington, Anniston, Crestview, Youngstown, Macon, Monroe, Tampa, Albany, Sumter, Wenatchee, Lakeland, Appleton, Lewiston, Lima, Rochester, Gulfport, Johnson City, Lynchburg, Atlanta, Williamsport, Dothan, Parkersburg, Wichita, Greenville, Yakima, Cedar Rapids, Kahului, Harrisburg, Bismarck, Saint George, Montgomery, Rockford, Manhattan</settlement>
									<region>IL, WY, ND, WA, WI, GA, LA, NC, NC, OH, MO, NC, WV, SC, FL, D. C., DC, AL, FL, OH, GA, LA, FL, NY, SC, WA, FL, WI, ID, OH, NY, MS, TN, VA, GA, PA, TX, AL, WV, KS, SC, WA, IA, HI, PA, ND, UT, AL, FL, IL, KS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,66.26,71.87,477.19,16.84;1,112.10,91.79,385.52,16.84">CWI and TU Delft at TREC 2013: Contextual Suggestion, Federated Web Search, KBA, and Web Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1848A780EF1CDA565AEA1B29197C0C2C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an overview of the work done at the Centrum Wiskunde &amp; Informatica (CWI) and Delft University of Technology (TU Delft) for different tracks of TREC 2013. We participated in the Contextual Suggestion Track, the Federated Web Search Track, the Knowledge Base Acceleration (KBA) Track, and the Web Ad-hoc Track. In the Contextual Suggestion track, we focused on filtering the entire ClueWeb12 collection to generate recommendations according to the provided user profiles and contexts. For the Federated Web Search track, we exploited both categories from ODP and document relevance to merge result lists. In the KBA track, we focused on the Cumulative Citation Recommendation task where we exploited different features to two classification algorithms. For the Web track, we extended an ad-hoc baseline with a proximity model that promotes documents in which the query terms are positioned closer together. Find Context Generate Dictionary Transform Documents Sim(Document,user) Generate Profiles Generate Dictionary Transform Profiles WARC Files &lt;(contextId,docId), doc content&gt; &lt;term, id&gt; &lt;(contextId,docId) , {&lt;termId, tf&gt;}&gt; Profiles &amp; Attractions &lt;userID , descriptions&gt; &lt;userId , {termId, tf}&gt; &lt;userId, contextId, docId, score&gt; Generate Desc &amp; Titles Generate Ranked list &lt;(contextId, docId), desc, title&gt; &lt;userId, contextId, docId, rank, desc, title&gt; cluster local</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We have participated in four tracks this year. The Contextual Suggestion track is introduced in Section 2, where our personalised approach on top of the ClueWeb12 collection is presented. Section 3 shows our results on the Federated Web Search track, where we participated in the two available tasks: resource ranking and results merging. Our participation in the Knowledge Base Acceleration is described in Section 4, extending our participation of last year with further experiments and new approaches. Finally, a proximity model applied to the Ad-hoc Web track is presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CONTEXTUAL SUGGESTION</head><p>In this section we describe our work in the contextual suggestion track, which represents our first participation in this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Track Description</head><p>The Contextual Suggestion Track investigates search techniques for complex information needs that depend on context and user interests. Input to the task are a set of profiles (users), a set of example suggestions (attractions), and a set of contexts (locations). Each attraction includes a title, a description, and an associated URL. Each profile corresponds to a single user, and indicates the user's preference with respect to each attraction using two ratings: one for the attraction's title and description and another for the attraction's website. Finally, each context corresponds to a particu-lar geographical location (a city and its corresponding state in the United States).</p><p>For each pair of context and profile, a ranked list of up to 50 ranked suggestions (attractions) should be generated. Each suggestion should be appropriate to both the user profile and the context. The description and title of the suggestion may be tailored to reflect the preferences of that user.</p><p>The source from where the suggestions are selected may be either the ClueWeb12 collection or the open web. Our submission was based on the former, in order to ensure reproducibility and further comparison of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Initial Ideas</head><p>At the beginning we aimed to use approaches based on Collaborative Filtering <ref type="bibr" coords="1,377.42,405.92,13.74,7.77" target="#b27">[27]</ref>. However, after taking a closer look into the problem, we realised that the items (documents) for which we have some information -i.e., the rated attractions -correspond to a context different to any of the target contexts. That is, whereas the ratings were given in the context of Philadelphia, PA, the contexts for which suggestions have to be generated are other metropolitan areas different to that one. Therefore, they cannot be used directly -since the potential candidates naturally depend on the contextwhich limits the scope of these techniques since they are based on the word of mouth effect. For this reason, we decided to explore content-based techniques <ref type="bibr" coords="1,410.24,510.52,13.74,7.77" target="#b21">[21]</ref>, which represent users and items in the same space, and generate recommendations based on some distance measure. We discuss our approach in Section 2. <ref type="bibr" coords="1,509.36,531.45,3.36,7.77" target="#b3">3</ref>.</p><p>We considered additional methods to exploit the ratings, but we were not able to generate a valid submission based on them. For instance, we also applied neighbour-based methods <ref type="bibr" coords="1,512.56,562.83,14.94,7.77" target="#b11">[11]</ref> to find suggestions based on the documents similar (not the documents liked or rated, as in standard collaborative filtering) to the user's neighbours; these neighbours may be found by exploiting textual or rating-based similarity between user profiles. Besides, a separate learning for the positive and negative aspects of each user profile was also considered, and will be explained in Section 2.3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Methodology</head><p>Our approach is to compare text of user's attractions with documents that mention a context. The first step we did in this task was to find all documents from the ClueWeb12 collection that mention each context. In parallel, we generated user profiles based on the descriptions of the attractions rated by them. Finally, we used the cosine similarity between the context documents and the user profiles, as represented in the |V |-dimensional vector space, where each element in the vector is a pair of term id and the frequency of that term in the document, |V | is the size of the vocabulary <ref type="bibr" coords="2,275.72,301.46,13.74,7.77" target="#b28">[28]</ref>. Based on these similarity values, we ranked the documents tailored to each user. Figure <ref type="figure" coords="2,130.13,322.38,4.48,7.77" target="#fig_0">1</ref> shows the complete procedure that we followed to generate a ranked list of documents for the (context, user) pairs. In the following sections we discuss our approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Document Filtering (Finding Contexts)</head><p>The first step in the pipeline presented in Figure <ref type="figure" coords="2,246.03,376.73,4.48,7.77" target="#fig_0">1</ref> consists of finding the most appropriate documents related to a context. For this task, we are interested in maximising the precision of this filtering, instead of the recall -that is, we want to be confident enough that the filtered documents mention the specific target context, although in this process we may lose some (potentially relevant) documents due to typographical or parsing errors. With this goal in mind, we focused on extracting the relevant documents for each context from the ClueWeb12 collection and created smaller subcollections. Thanks to this filtering, the next steps -i.e., the ranking generation -can be executed using a smaller set of documents (see Table <ref type="table" coords="2,92.42,491.80,3.73,7.77" target="#tab_1">1</ref>) which in fact allows for more efficient processing.</p><p>Sub-collections were made as follows. A MapReduce job read the document content from the entire collection and kept only those documents that mention exactly the context as provided by the organisers, ignoring those documents where the target city appeared more than once, but with different states. We decided not to keep such documents as they could (potentially) consist of lists of city names, which we believe would provide zero interest to any user. To do this, we used a regular expression to check the mention of contexts in the document -that is, the pair (city, state) mentioned above -, along with another regular expression checking if the city was mentioned near another state different from the target state. For example, for the context Springfield, IL, we would include in its corresponding sub-collection all the documents where Springfield and IL are mentioned and only spaces or commas are in between, however, a document would not be valid if, besides Springfield, IL, it also contains Springfield, FL. Algorithm 1 shows the pseudocode of this step.</p><p>After filtering, we found 13, 548, 982 documents that mention at least one of the contexts among the total number of 733, 019, 372 documents from the ClueWeb12 collection. Table <ref type="table" coords="2,247.52,701.01,4.48,7.77" target="#tab_1">1</ref> shows the number of documents found for each context, along with the inter- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Modelling Documents</head><p>In this section, we describe how we transformed the candidate documents in each sub-collection into its representation in the Vector Space Model (VSM). First, we generated a dictionary that has a mapping between terms and their integer ids. To decrease the size of the dictionary and remove useless terms, we filtered out the HTML tags from the content of the documents, then we removed stop-words and non-alphanumeric terms. Algorithm 2 shows the pseudo-code for generating the collection's dictionary. After that, we used this dictionary to transform documents into vectors of weighted terms, where the weight of each dimension (term) is the standard term frequency tf. We implemented this process as a Map-Reduce job that reads the ClueWeb12 WARC files and transforms them into vectors of pairs, where each pair is the term id and its corresponding frequency. Algorithm 3 gives specific details on this job. Since now we deal with a representation based on integer instead of string vectors for each sub-collection of documents, the size of the sub-collections will decrease and a faster processing will be possible. Table <ref type="table" coords="2,387.96,508.35,4.48,7.77" target="#tab_2">2</ref> shows the effect on the collection size after cleaning, transforming, and optimising the vector-based document representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Modelling Users</head><p>We generate each user's profile according to the user's preference for the given attractions and the descriptions of those attractions. The initial idea was to use the content of the attractions instead, by extracting the HTML content of the attractions websites. However, we found a coverage problem between the ClueWeb12 collection and this set of attractions: first, only 7 pages were found with a one-one URL mapping, which ended up to 35 by matching hostname and considering URL variations such as adding or removing www and http(s); second, we found that the user ratings for attraction's descriptions and websites were very similar in most of the cases. Third, to participate as a ClueWeb12 submission we could not crawl the attractions from the Open Web or use any other external information. Figure <ref type="figure" coords="2,421.08,686.31,4.48,7.77">2</ref> shows a histogram of the difference between the ratings for descriptions and websites -i.e., a negative value denotes that the rating for the website is higher than for the description. We can observe that most of the values are concentrated around 0, which means that no difference is observed between the two ratings. We have to note, however, that this pattern may change depending on the actual attraction analysed, as in Figure <ref type="figure" coords="3,67.67,701.01,4.48,7.77">3</ref> where a larger shift is observed for the attractions with id 52 (Eastern State Penitentiary) and 57 (Chinatown). Inspired by the best approaches of last year <ref type="bibr" coords="4,227.79,464.40,14.19,7.77" target="#b18">[18,</ref><ref type="bibr" coords="4,245.18,464.40,10.65,7.77" target="#b35">35]</ref>, we used three different methods to represent the users profiles:</p><p>• generate user profiles based only on the attraction descriptions, without taking into account the ratings</p><p>• generate positive user profiles based on the attraction descriptions that have positive ratings from each user</p><p>• generate negative user profiles based on the attraction descriptions that have negative ratings from each user Since the ratings are on 5-point scale, each rating represents a user's level of interest in visiting the corresponding attraction, the levels ranging from "0" for strongly uninterested to "4" for strongly interested. In this context, we consider the "2.5" as threshold between negative and positive ratings.</p><p>Moreover, we need the same representation between the user profiles and the candidate documents. Because of this, we transformed these profiles into weighted vectors following the same methodology we used to transform the documents (see Section 2.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Personalising Ranking</head><p>To generate the final ranking (given a pair of context and user information), we computed the similarity in the vector space representation between the document and user profile representations, as presented before in Sections 2.3.2 and 2.3.3. With this goal in mind, we tested the Jaccard and cosine functions as similarities, which are defined like:</p><formula xml:id="formula_0" coords="4,340.88,268.37,215.03,22.41">sim(u, d) = |u ∩ d| |u ∪ d| = i δ(ui == di! = 0) i δ(ui! = 0||di! = 0)<label>(1)</label></formula><formula xml:id="formula_1" coords="4,340.88,294.21,211.55,22.10">sim(u, d) = cos(u, d) = i ui • di u 2 d 2 (<label>2</label></formula><formula xml:id="formula_2" coords="4,552.43,300.80,3.48,7.77">)</formula><p>where δ is the function that outputs 1 if its argument is true. Note that since Jaccard does not take into account the actual value encoded in the vector (in our case, frequencies), we binarise the vectors prior computing the Jaccard similarity.</p><p>We implemented a MapReduce job to compute these similarities, in part because they involve a large dimensional vocabulary space, mainly produced by the document representation. This MapReduce job only involves map tasks, where the user vectors are added to the distributed cache, and then, when the similarity is computed, it outputs the tuple (context, document, user) as key and the actual similarity as value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Generating Description and Titles</head><p>As mentioned before, besides producing a document ranking for each user and context, we also have to generate a description and title for each suggestion, and if possible, tailored to the users, that is, considering their preferences as means to explain why such document is presented.</p><p>We decided to only provide personalised descriptions, since we consider the title as a global property of the document, inherent to its content and, thus, should not be different for each user. In this situation, we generated the titles by extracting the title or heading tags from the HTML content of the document. On the other hand, we observed the task of generating descriptions similar to snippet generation where the query is the combination of context and user preferences <ref type="bibr" coords="4,360.66,589.09,13.74,7.77" target="#b22">[22]</ref>. Because of that, we aimed at obtaining the most relevant sentences for the user within the document in a particular context. To do this, we first split the document into sentences by using the Java BreakIterator class 2 which can detect sentence boundaries in a text. We then followed similar steps to those of the document ranking but at a sentence level, i.e., filter out those sentences not mentioning the context and ranking the remaining sentences according to their similarity with the user profile. Finally, we assumed that larger descriptions were preferred, and hence, we concatenated sentences -in decreasing order of similarity -until the maximum number of bytes 512 was reached, controlling to not combine two very similar sentences to decrease the redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Sub-Collection Post-Filtering</head><p>In this section, we present a post-filtering method we ran on each sub-collection. Due to temporal and technical issues, we were not able to submit the results after applying this filtering; we analyse in more detail its potential effect in Section 2.4.</p><p>The main motivation to perform this post-filtering was the fact that a webpage mentioning a context does not provide enough evidence that such webpage is about an attraction worth to visit, and thus, whether it should be recommendable at all. To address this problem we built a classifier based on the content of selected websites from ClueWeb12. These selected websites aimed to capture well-known travel and tourism related pages, in particular we considered: yelp, tripadvisor, wikitravel, zagat, xpedia, orbitz, and yahoo-travel. We used a decision tree classifier as implemented by Weka <ref type="foot" coords="5,85.66,234.38,2.99,5.18" target="#foot_1">3</ref> , where the positive labels correspond to every page under the domains aforementioned (we found 171, 188 documents), whereas the negative labels were assigned based on a random number of pages not contained in the previous positive sub-collection. This is a typical unbalanced classification problem (or imbalanced dataset) <ref type="bibr" coords="5,83.78,288.54,13.74,7.77" target="#b34">[34]</ref>, where one class (the negative one, in our case, not attraction) has a much larger number of examples than the other one, which is, in fact, the class of interest. To allow a fair learning by the classifier, we decided to not account for this bias and selected the same number of negative documents as positive, something known as subsampling in Machine Learning; we have to note, however, that other strategies exist in the literature <ref type="bibr" coords="5,202.20,351.31,9.52,7.77" target="#b3">[3]</ref>.</p><p>Once the classifier was built, we labelled each document in our context-dependent sub-collections as either positive (similar to webpages of the selected travel and tourism related sites) or negative, and perform the same ranking approach presented in Section 2.3.4 but only with the positive documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Analysis</head><p>Now we analyse the approach we submitted and how it compares to other approaches that we wanted to try but were unable to submit on time (see Section 2.2). We first discuss the effect of the subcollection creation may have in our results, and then we experiment with other similarity and formulations for the ranking step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Effect of the Sub-Collection</head><p>As mentioned in Section 2.3.1, the organisers provided a subcollection from ClueWeb12 specifically tailored for this task <ref type="foot" coords="5,270.45,517.78,2.99,5.18" target="#foot_2">4</ref> . The first analysis we present is a comparison between the submitted ranking (based on filtering the entire ClueWeb12 collection) and the same algorithm (Section 2.3.4) using the given sub-collection. In this way, we can discriminate which part has a larger effect in the final result, the filtering method to create sub-collections or the actual ranking function.</p><p>The results are presented in Table <ref type="table" coords="5,187.36,592.87,3.36,7.77" target="#tab_3">3</ref>. We present the MRR metric as computed by the evaluation script provided by the organisers. However, since we are also evaluating unsubmitted runs, we should not use the judgements for the descriptions, since eventually the descriptions could be different for every method, and thus their relevance may differ (in contrast with the document relevance, which we assume to be more consistent and stable). Hence, MRR d and P@5 d represent the performance of these methods when the description judgements are ignored (d here stands for document). Additionally, P@5 dḡ shows the value obtained when the geographical assessments are not considered. We observe in this table that the documents originally retrieved were relevant for the user but not geographically appropriate, since the value of P@5 dḡ improves significantly. This is in contrast with what happens when using the given sub-collection for this method, mostly because the documents were tailored to be appropriate for each context. Additionally, the performance in terms of MRR is higher when using the given sub-collection than our filtered subcollections from the entire ClueWeb12 collection. This is a clear evidence that the filtering step should be refined.</p><p>As already presented in Table <ref type="table" coords="5,437.20,325.42,3.36,7.77" target="#tab_1">1</ref>, and now in more detail in Table 4, the intersection between our sub-collections and the given one is very low. This is very clear when we use the classifier presented before as a post-processing step, since we were not able to submit this run, and thus, most of the documents recommended by this approach (since its intersection with our other method is very low) remain unjudged. This, in turn, decreases the final performance of the method based on the classifier and the complete collection. In fact, the performance improves again when the given sub-collection is used, although now the values are lower than before. This may also be due to a low coverage of the relevance assessments and further analysis is needed in this regard. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Effect of the Similarity Function</head><p>As we mentioned at the beginning in Section 2.2, one of our first ideas was to exploit collaborative filtering techniques for this task. However, as already discussed, this is not possible with the data available. Nonetheless, inspired by the nearest-neighbour techniques from the recommender systems literature <ref type="bibr" coords="5,499.13,659.17,13.74,7.77" target="#b11">[11]</ref>, we developed some approaches able to capture the concept of collaborative recommendation. Basically, we start with a generated set of rankings {Ru i }i respectively assigned to user ui. Then, for every user u we find a set of users (neighbours) Vu based on which we will generate the final recommendations. To aggregate the rankings Rv for v ∈ Vu we used Borda aggregation, a standard technique from rank aggregation literature <ref type="bibr" coords="6,152.99,68.38,13.74,7.77" target="#b12">[12]</ref>. In this setting, there are two parameters we have tested: the amount of neighbours (size of |Vu|) and how the neighbours are selected (user similarity metric).</p><p>In Table <ref type="table" coords="6,94.99,99.77,4.48,7.77" target="#tab_5">5</ref> we show the best results we found using these methods. We observe that we never outperform the results of the submitted run, this may be due, like before, to a low coverage of the relevance assessments, since these approaches were not evaluated. Regarding the best parameters, 5 neighbours obtains the best results (we also tested with 10 and 50 neighbours), and the best similarity metric depends on the actual method used: when IBCosTop1 is used, the Pearson's correlation similarity based on the ratings given by the users to the example attractions achieves the best results; on the hand, when the input uses the post-filtering based on the classifier, best results are obtained with the Jaccard similarity between the textual representation of the users. Additionally, we also tested profiles based only on the attractions with positive ratings <ref type="bibr" coords="6,151.55,436.22,14.19,7.77" target="#b18">[18,</ref><ref type="bibr" coords="6,168.73,436.22,10.65,7.77" target="#b35">35]</ref>. As a sanity check, we found that the recommendations based on the negative profile obtain very low performance. Interestingly, recommendations based only on the positive profile are competitive, although not as good as the submitted approach. In any case, we confirm that the collaborative nearest-neighbour approach also reduces the performance for this method in most of the situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discussion</head><p>In this first attempt of the track we have faced several challenges: dealing with a very large dataset like the ClueWeb12 collection, filtering it to make it more manageable, personalising the ranking in different ways, and post-filtering the results to produce more touristic results. Based on our performance, there is enough room to improve in most of these steps. The filtering step, however, seems to be a determining factor in the subsequent chain of events.</p><p>While doing the aforementioned analysis, we identified a subset of documents that were submitted as part of the ClueWeb12 collection whose corresponding URLs were also submitted (by other participants) as Open Web documents. We noticed that the subjective assessments (document judgements) are not consistent, especially in the number of 3's, 4's, and -2's received by the same document in each dataset. This fact may indicate a bias towards higher performance for the methods using documents from the Open Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FEDERATED WEB SEARCH</head><p>In this section we describe our work in the federated web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Track Description</head><p>The Federated Web Search track investigates techniques for the selection and combination of search results from a large number of real on-line web search services. The goal of this track is to evaluate approaches to federated search at very large scale in a realistic setting, by combining the search results of existing web search engines. This year, two tasks were available: resource selection (selecting the search engines that should be queried) and results merging (combining the results into a single ranked list). We participated in both tasks and present our contribution in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Resource Selection Task</head><p>The input for this task is a collection provided by the organisers (FedWeb 2013 collection) consisting of sampled search results from 157 search engines. Then, for every query the system should return a ranking such that the most appropriate search engines are ranked highest without using the actual results for the given query (which were, in fact, provided after the submission deadline of this task).</p><p>We used two different strategies to produce the resource ranking, then a third submission was generated based on their combination.</p><p>ODP based run. The Open Directory Project (ODP) <ref type="foot" coords="6,508.04,273.61,2.99,5.18" target="#foot_3">5</ref> is a humanedited directory of the Web, constructed and maintained by volunteers. In this directory, each URL has a category assigned, which can contain sub-categories. Besides, it has a service where it returns a list of categories in response to a query. We used this service <ref type="foot" coords="6,543.05,315.45,2.99,5.18" target="#foot_4">6</ref> to get the categories associated to each resource and to every query. We then computed similarities between the two lists of categories using cosine and Jaccard similarities (see Section 2.3.4), considering and ignoring the ranking information from each category. For the queries, we also experimented with using the actual query text in the computation of the similarity. We did not observe any significant difference between these variations, so we will only report the results for the simplest alternatives (i.e., no order, no query text).</p><p>Retrieval model based run. This strategy concatenates all the snippets from each resource and indexes them as a single document, so that when a query is issued, the aggregated documents (resources) are ranked according to their relevance with respect to the query. We built a different Lucene<ref type="foot" coords="6,441.43,455.38,2.99,5.18" target="#foot_5">7</ref> index for each retrieval model we tested: a simple TF-IDF method, BM25 (with varying parameters k1 and b), two language model approaches (Jelinek-Mercer with parameter λ and Dirichlet with µ). Besides, since each snippet has both a title and a description, we tested considering only the title field to match the query, only the description field (desc), or both.</p><p>Hybrid run. This run takes two rankings as input for each query. Then it aggregates the information using a Borda voting mechanism <ref type="bibr" coords="6,336.48,555.33,13.74,7.77" target="#b12">[12]</ref>, where each document gives a number of votes inversely proportional to its ranking (the higher the position, the lower the number and the larger the number of votes). Finally, documents are sorted according to the number of votes (the higher, the better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results Merging Task</head><p>For the result merging task, the input consists of the ranked list of documents retrieved using the 157 different search engines. Then, for each query an aggregated ranked list of these documents should be returned. Documents are ranked in descending order of their relevance to the query.</p><p>We employed four different strategies to aggregate the ranked lists, with different motivations. Relevance based run (CWI13IndriQL). This run only considers the relevance of a document to a query. We rank the documents with the simple query likelihood model. For a document d and query q, the query likelihood of p(q|d) is computed as</p><formula xml:id="formula_3" coords="7,133.33,131.36,159.57,17.64">p(d|q) ∝ w∈q p(w|d),<label>(3)</label></formula><p>where p(w|d) is computed based on the title and snippets of the document. Dirichlet smoothing with µ = 2500, i.e., default setting implemented with Indri is used.</p><p>Cluster based run (CWI13clTODPJ). We take a two-step process to rank the documents. First, the resources are ranked based on one of the strategies described above (specifically, the submitted run is based on a hybrid method between ODP with Jaccard and the TF-IDF retrieval model). Then, documents within a resource are ranked by the query likelihood model. With this run, we are interested in whether some resources are better than others in satisfying a particular information need. That is, whether the majority of documents provided by a good resource would be relevant to the query. This run was not submitted. Diversity based run (CWI13iaTODPJ). This run is based on a different assumption. That is, by diversifying documents from different resources, it is more likely that at least one type of documents (resource) will satisfy the information need.</p><p>We take an approach similar to the cluster-based diversification <ref type="bibr" coords="7,286.92,341.37,14.19,7.77" target="#b16">[16,</ref><ref type="bibr" coords="7,53.80,351.83,11.95,7.77" target="#b17">17]</ref> methods, where each resource can be seen as a "cluster", as described in our cluster based run. An initial ranked list is retrieved with the query likelihood model. It is then re-ranked with an IAselect <ref type="bibr" coords="7,76.46,383.21,10.45,7.77" target="#b1">[1]</ref> based diversification algorithm.</p><p>The key elements used in the algorithm can be reduced to the following quantities: i) V (d|q; z), the probability that d is relevant to q when the associated resource (cluster) is z; and ii) p(z|q), the probability resource z is related to q. We compute p(z|q) by normalizing the resource ranking scores discussed above (i.e., ODP + Jaccard) over all resources. To compute V (•), intuitively, we can set V (d|q; z) = 1 if d ∈ z; otherwise 0. However, this results in a product of 0 in many cases given the IA-select framework. To address this issue we applied a minor smoothing over the V (•) scores which assigns a probability mass of 0.01 to cases where d / ∈ z. Boost documents based on resource (CWI13bstTODPJ). For this run, we intend to boost the documents coming from a reliable resource. Specifically, we rank documents in descending order of p(d|q, z) ∝ p(d|q)p(q|z),</p><p>assuming d is independent of z given q. We then compute p(d|q) as in the query likelihood run, and p(q|z) as in the resource ranking, based on ODP and Jaccard similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>In addition to the officially submitted runs, we also evaluated all the variants of the methods presented above. The 2012 data set is described in <ref type="bibr" coords="7,101.34,627.89,14.94,7.77" target="#b25">[25]</ref> and although the content is similar, we have to note that the search results correspond to the year 2012 (FedWeb 2012) and the search engines included are not exactly the same as in the FedWeb 2013 collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Resource Selection</head><p>For the resource selection task we tested different variations of the strategies presented above. Table <ref type="table" coords="7,190.44,701.01,4.48,7.77" target="#tab_6">6</ref> shows the results obtained for some of these methods with the FedWeb 2012 collection. These values were computed generating relevance judgements for each resource according to the relevance judgements (based on documents) included in the collection for a subset of 50 TREC queries. We aimed to emulate the procedure described in the track page, where the organisers note that the relevance of a search engine for a given query is determined by calculating the graded precision on the top 10 results, using weights according to the relevance levels of the documents: Nav and Key levels are assigned a weight of 1, Hrel a value of 0.5, and Rel, 0.25. We found that the best methods (in FedWeb 2012) were the TF-IDF retrieval method (where the query is issued on the description and title fields), the Jaccard similarity over ODP categories, and a combination of these approaches. Moreover, methods where the query is applied only to one the fields had a much lower performance. We present in the table only the best values for each of them (Jelinek LM for the description field and TF-IDF for the title) and an additional method (BM25 desc) which will serve us as reference later. The results obtained, however, with the FedWeb 2013 collection are completely different (see Table <ref type="table" coords="8,179.36,68.38,3.24,7.77" target="#tab_7">7</ref>). The three runs we submitted performed not consistently with respect to what we observed in the training collection, where, for instance, the hybrid approach was the best one. Furthermore, some of the other methods evaluated had a much higher performance values, in particular the use of the description as the only field to issue the queries turned out to be the most effective approach for the FedWeb 2013 collection, and one of the worst methods in the FedWeb 2012 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Results Merging</head><p>As presented before, we experimented with one run based on document relevance and with three other runs depending on the output of the previous task, that is, a ranking of resources. We used the hybrid method (submitted run CWI13ODPTI) because it was the best performing method in the FedWeb 2012 collection, as we analysed before, and thus we expected it to perform equally well in the FedWeb 2013 collection, in particular now, to aggregate search results. We observe in Table <ref type="table" coords="8,136.58,459.68,4.48,7.77" target="#tab_8">8</ref> that, again, the order of the approaches for result merging in FedWeb 2012 do not agree with the one in Fed-Web 2013. In terms of P@10 the best methods are different (boost vs relevance based, among the submitted ones). In terms of nDCG, the relevance based run is in both cases the best method, but the performance of the diversity approach is much lower than that of the boost one, and similar to the cluster run in FedWeb 2012; however, in FedWeb 2013 the diversity run outperforms the boost and cluster runs. This suggests that, when the resource ranking is not good (the performance of the hybrid method in resource selection is far from optimal), the diversification approach seems to help a little bit. On the other hand, the boosting method is highly dependent on the ranking of the resources, as we observe when a better resource selection method is used (BM25 desc in FedWeb 2013 or the hybrid run in FedWeb 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>After participating in the Federated Search track, we have discovered that training techniques in an older dataset, even if its characteristics are very similar to the current one, does not guarantee a reproduction of the results obtained, making it very difficult to have reliable judgements about which techniques will perform better. We argue that this may be due to the content of the search engines changing from one collection to the other, but also because the queries, and in particular, its specificity (e.g., can it be answered by a general search engine or is it tailored to more focused, specialised engines?) may drastically change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">KNOWLEDGE BASE ACCELERATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Track Description</head><p>In this section we describe our work in TREC Knowledge Base Acceleration (KBA) Cumulative Citation Recommendation (CCR), a task that aims at filtering a stream for documents that are citationworthy for Wikipedia or Twitter entities of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Motivation</head><p>Our participation in KBA 2013 was inspired by a desire to combine the best performing aspects of several approaches. In TREC-KBA 2012, we experimented with several approaches including string-matching and string-learning <ref type="bibr" coords="8,447.09,219.98,9.52,7.77" target="#b2">[2]</ref>. With string-matching, we represented the entities with rich features from a resource called Google Cross Lingual Dictionary (GCLD) which is a mapping (with probability distributions) from strings to concepts and vice versa. The string-learning approach learns the context (characters) around the mention of an entity in the provided relevance judgements and builds a profile of the entity from the characters.</p><p>The string-matching approach achieved good performance in general, but it was very good at recall in particular. The string-learning approach was very good at precision indicating that context around entity mentions is important for determining relevance of a document to an entity. We noted also high-performing approaches from TREC 2012 included an approach that used entity and related entity mentions <ref type="bibr" coords="8,352.61,355.97,9.71,7.77" target="#b2">[2,</ref><ref type="bibr" coords="8,365.24,355.97,10.65,7.77" target="#b20">20]</ref>. Finally, we came across studies that use multistep approaches and a huge feature set for CCR <ref type="bibr" coords="8,500.43,366.43,9.71,7.77" target="#b4">[4,</ref><ref type="bibr" coords="8,513.62,366.43,7.47,7.77" target="#b5">5]</ref> that also achieve good performance.</p><p>One of the studies is called multi-step classification approach which compares two approaches: 2-step and 3-step. Both of them start with an initial step which filters the stream for potentially relevant documents. The 3-step approach next trains a classifier to separate garbage and neutral from relevant and central, and finally trains another classifier to separate relevant from central. The 2step approach achieves a better performance than that of the 3-step approach.</p><p>The second of the studies is related to <ref type="bibr" coords="8,465.50,471.04,9.52,7.77" target="#b5">[5]</ref>, but trains a Learning to Rank algorithm (LTR) instead of classification <ref type="bibr" coords="8,499.05,481.50,9.52,7.77" target="#b4">[4]</ref>. The classification and LTR approaches of <ref type="bibr" coords="8,436.71,491.96,9.71,7.77" target="#b4">[4,</ref><ref type="bibr" coords="8,449.65,491.96,7.47,7.77" target="#b5">5]</ref> shared the same set of 68 distinct features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Method</head><p>We combine all the strengths in all the above approaches in an attempt to benefit from the strengths of each. Thus, we gathered features from the different approaches and added some new ones making a huge initial feature set. We reduced the feature set using different methods until we have few powerful subset which we ranked according to information gain. We applied the approach to the 2012 task and our performance was encouraging (both Fmeasure and SU being above 4.0). Encouraged by our performance on 2012 task, we applied the approach to the 2013 CCR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Features</head><p>The multi-step classification and LTR approaches used a set of 68 (5 document, 1 entity, 24 document-entity and 38 temporal) features (all numeric) <ref type="bibr" coords="8,400.66,669.63,9.71,7.77" target="#b4">[4,</ref><ref type="bibr" coords="8,414.09,669.63,6.47,7.77" target="#b5">5]</ref>. Document and entity features are computed from processing the documents and entities respectively. Document-entity features are computed by aggregating scores of strings for which a match has been found in a document. For example, if we consider the Personalised Page Rank (PPR) feature, for each entity, there are 100 related entities each with its PPR score. When processing a document entity pair, if a document matches strings from the entity's pre-constructed PPR, we add up the scores and the sum becomes the PPR score for that document-entity pair. We take the 68 features as provided by the authors<ref type="foot" coords="9,234.82,97.90,2.99,5.18" target="#foot_6">8</ref> and add others from <ref type="bibr" coords="9,74.18,110.23,9.71,7.77" target="#b2">[2,</ref><ref type="bibr" coords="9,86.84,110.23,10.65,7.77" target="#b20">20]</ref>, described below and some of them modified to suit our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Google's Cross Lingual Dictionary (GCLD)</head><p>This is a mapping of strings to Wikipedia concepts and vice versa <ref type="bibr" coords="9,75.08,162.64,13.74,7.77" target="#b32">[32]</ref>. The GCLD corpus computes two probabilities: (1) the probability with which a string is used as anchor text to a Wikipedia entity and (2) the probability that indicates the strength of co-reference of an anchor with respect to other anchors to a given Wikipedia entity. We use the product of both for each string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">PPR</head><p>For each entity, we computed a PPR score from a Wikipedia snapshot and we kept the top 100 entities along with the corresponding scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Surface Form (sForm)</head><p>For each Wikipedia entity, we gathered DBpedia name variants. These are redirects, labels and names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Context (contxL, contxR)</head><p>From the WikiLink corpus <ref type="bibr" coords="9,163.70,330.35,13.74,7.77" target="#b30">[30]</ref>, we collected all left and right contexts (2 sentences left and 2 sentences right) and generated ngrams between uni-grams and quadro-grams for each left and right context. Finally, we select the 5 most frequent n-grams for each context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Classification</head><p>The 2-step approach that we use, following <ref type="bibr" coords="9,221.14,403.69,10.45,7.77" target="#b5">[5]</ref> and <ref type="bibr" coords="9,249.36,403.69,9.52,7.77" target="#b4">[4]</ref>, consists of filtering followed by classification or learning. The first step filters the stream for documents that are potentially relevant using DBpedia name variants of the Wikipedia entities. The second step trains classification or learning to rank (LTR) algorithm. In both cases, we treat central as positive, and garbage and neutral as negative examples. However, relevant is excluded from the training stage, as these may introduce confusing examples for the classifiers.</p><p>We train J48 (CL-J48) and Random forest Model (CL-RF) decision tree classifiers, as implemented in Weka. Thus, we take the same settings as described in <ref type="bibr" coords="9,159.64,518.76,9.52,7.77" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Result and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Feature analysis</head><p>Our Feature Analysis and selection was done using the 2012 datasets and relevance judgements. We compared our results with the state of the art studies on TREC-KBA CCR of 2012.</p><p>Figure <ref type="figure" coords="9,90.02,602.67,4.48,7.77" target="#fig_1">4</ref> shows the performances (F) of the three algorithms against feature addition on the 2012 CCR task. The features are sorted from left to right, in descending order, in terms of information gain. The plot of Classification CL-RF is based on the average performance for 10 different random initialisations. The plus sign on a feature indicates that we incrementally add the feature into the feature set to the left of it.</p><p>From Figure <ref type="figure" coords="9,111.16,675.89,3.36,7.77" target="#fig_1">4</ref>, we see that the performances of the three algorithms increase with the addition of features to the initial feature Table <ref type="table" coords="9,341.78,63.75,3.73,8.06">9</ref>: KBA Track. Performances comparison of our approach (lower half) with baselines (upper half). Best scores are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method F SU</head><p>MC-RF 0.360 0.263 LTR-RF 0.390 0.369 CL-RF 0.402 0.396 LTR-RF 0.394 0.411 CL-J48 0.388 0.306 set, reach maxima and then decrease. The increase and decrease are not uniform. However, we can see that the three algorithms reach their respective maxima within the first 13 features. This indicates that adding more features does not always improve performance. In fact, performance deteriorates with more features. We have taken the best F scores from the three plots and they are shown along with three baselines in Table <ref type="table" coords="9,420.37,277.87,3.36,7.77">9</ref>. We have included two of the highly performing methods on 2012 CCR task as baselines. From classification, the 2-step approach's Random Forest is used as a baseline (MC-RF). The second is LTR's Random Forest (LTR-RF).</p><p>The scores in Table <ref type="table" coords="9,397.14,319.71,4.48,7.77">9</ref> show that our reduced feature set performs better than the baselines on both performance measures. The most informative features, as measured by information gain and contribution to performance, are: name variants (GCLD), similarity features (cos, jac, kl), related entities (PPR), context, position of entity mention in the document, and length of body text. These features can serve as baseline features for CCR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Results on TREC-KBA 2013's CCR</head><p>Encouraged by the results on TREC-KBA 2012's CCR task, we applied our reduced feature set and two classification algorithms (J48 and Random Forest) to the 2013 CCR task. We used three sets of features: all 26 features, features up-to FirstPosNorm (FPN) and Features up-to MentionsBody (MB). We also used three training datasets: 2012, 2013, and 2012-2013 combined relevance judgements. We generated 3 J48 runs using all features and the training sets and 9 RF runs using 3 feature sets and 3 training sets.</p><p>Results for the groups for which performance was above the median are shown in Table <ref type="table" coords="9,401.58,511.74,7.47,7.77" target="#tab_9">10</ref>. System name format is algorithm_feature set_training dataset_13. For example RF_all_13_13 stands for Random Forest using all features, trained on 2013 and applied on 2013 <ref type="foot" coords="9,550.94,530.80,2.99,5.18" target="#foot_7">9</ref> .</p><p>Table <ref type="table" coords="9,348.41,543.12,8.97,7.77" target="#tab_9">10</ref> shows our best performance according to micro average F and SU. The scores are obtained from the classification confidence scores and the classification label. We map the scores of irrelevant document-entity pair to (0, 500] and the scores of relevant to (500, 1000]. For vital classification, the highest score is on the Turing group. On all entities, micro-average-F is 0.247, and on Wikipedia entities, it is 0.290. On vital+useful, we have done well, achieving performance of 0.603 on all entities and 0.649 on Wikipedia only.</p><p>Our approach was very weak in Twitter entities achieving Fmeasure of 0.0. The low performance on Twitter entities is, of course, expected since almost all of the strong features we used did not apply to Twitter entities. For example, all the similarity (cos, kl, jac), GCLd, PPR, sform and context features were assigned 0  score. We also performed very poorly on the groups of startups, french, mining and comedians.</p><p>From algorithms, RF performs better in almost all cases. Regarding training dataset, we see that 2013 relevance judgements help train a better model. In many cases, training on 2012 data achieved 0.0 or very low performance. This is probably due to the fact that the CCR task has been changed from its 2012 definition.</p><p>Our performance on 2012 CCR task did not translate well to the 2013 CCR task. We suspect that this has to do with the change of the CCR task. We will need to do further analysis. However, we have achieved good results for some groups. We want to do further analysis of why we achieve better results for some groups and bad results for others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AD-HOC WEB</head><p>The goal of our Web Track participation is to evaluate the improvement of our proximity model over its baseline. Our model has no specific mechanism for diversity or risk sensitivity, and therefore we do not consider our participation to be competitive to that of other participants, on the metrics that are used for the evaluation. We therefore only participated in the Web Track ad-hoc task. This section describes our participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Track description</head><p>The TREC Web Track ad-hoc task investigates the performance of systems that search a static set of documents using previouslyunseen topics. The goal of the task is to return a ranking of the documents in the collection in order of decreasing probability of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Term Proximity</head><p>Ranking functions typically consider query terms as being independent, which simplifies the ranking problem. However, the user is often interested in the joint co-occurrence of query terms, forming a noun phrase or being otherwise related in the text. One feature that can be used to express the relatedness of co-occurring terms is their proximity in text. Intuitively, researchers have suspected that query terms that appear closer together in documents represent stronger evidence for relevance <ref type="bibr" coords="10,434.69,701.01,9.71,7.77" target="#b8">[8,</ref><ref type="bibr" coords="10,447.52,701.01,11.21,7.77" target="#b23">23,</ref><ref type="bibr" coords="10,461.85,701.01,11.21,7.77" target="#b24">24,</ref><ref type="bibr" coords="10,476.19,701.01,11.21,7.77" target="#b33">33,</ref><ref type="bibr" coords="10,490.52,701.01,10.65,7.77" target="#b37">37]</ref>, however past research has shown this is not an easy feature to exploit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Related Work</head><p>In the past, there has been much interest into automating query reformulation with respect to term dependency <ref type="bibr" coords="11,228.04,81.34,64.86,7.77;11,53.80,91.80,96.10,7.77">[6-10, 13-15, 19, 23, 24, 26, 29, 31, 33, 37]</ref>. Various ways have been proposed to promote documents in which query terms appear at closer proximity. Some approaches assign a score to each span of text that appears in a document containing multiple query terms <ref type="bibr" coords="11,259.33,123.18,9.71,7.77">[6,</ref><ref type="bibr" coords="11,272.00,123.18,6.72,7.77" target="#b8">8,</ref><ref type="bibr" coords="11,281.70,123.18,11.21,7.77" target="#b14">14,</ref><ref type="bibr" coords="11,53.80,133.64,11.21,7.77" target="#b24">24,</ref><ref type="bibr" coords="11,68.03,133.64,10.65,7.77" target="#b37">37]</ref>, while others restrict their proximity model to using only term-pairs <ref type="bibr" coords="11,93.68,144.10,14.19,7.77" target="#b26">[26,</ref><ref type="bibr" coords="11,110.91,144.10,10.65,7.77" target="#b33">33]</ref>. The scope of proximity that is considered is often limited to a fixed word distance, for instance 5 terms by Rasolofo and Savoy <ref type="bibr" coords="11,118.39,165.02,14.94,7.77" target="#b26">[26]</ref> and 50 terms by Song et al. <ref type="bibr" coords="11,237.13,165.02,13.74,7.77" target="#b31">[31]</ref>. The score of the term-combinations found in a document is often added to a baseline that considers the terms to be independent <ref type="bibr" coords="11,239.51,185.94,9.71,7.77">[6,</ref><ref type="bibr" coords="11,251.58,185.94,11.21,7.77" target="#b24">24,</ref><ref type="bibr" coords="11,265.15,185.94,11.21,7.77" target="#b33">33,</ref><ref type="bibr" coords="11,278.71,185.94,10.65,7.77" target="#b37">37]</ref>, while other models modify the relevance contribution of appearing terms <ref type="bibr" coords="11,75.96,206.86,14.19,7.77" target="#b14">[14,</ref><ref type="bibr" coords="11,92.40,206.86,10.65,7.77" target="#b31">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baseline</head><p>Zhai and Lafferty <ref type="bibr" coords="11,130.19,237.29,14.94,7.77" target="#b36">[36]</ref> presented a function to rank documents according to the (negative) Kullback-Leibler (KL) divergence between a language model of query Q and a Dirichlet-smoothed language model of document D. Each document D obtains a score for query Q using Equation <ref type="formula" coords="11,144.32,279.14,3.36,7.77" target="#formula_5">5</ref>, where t is a term in Q, P (t|C) is the likelihood of term t appearing in the corpus, c (t, D) is the term frequency of term t in document D, P (D) is a term independent document prior (Equation <ref type="formula" coords="11,148.74,310.52,3.24,7.77" target="#formula_6">6</ref>), |Q| is the number of terms in Q, and µ is the Dirichlet smoothing parameter. We refer to this function as KLD in this paper.</p><formula xml:id="formula_5" coords="11,74.95,346.31,217.95,23.60">KLD (Q, D) = P (D) + t∈Q log 1 + c (t, D) µ • P (t|C)<label>(5)</label></formula><formula xml:id="formula_6" coords="11,100.05,372.05,192.85,19.74">P (D) = |Q| • log µ µ + |D|<label>(6)</label></formula><p>In this study, we used the KLD function as the base function for ranking, expanding it with a proximity model to promote documents based on the distance between co-occurring query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Proximity Model</head><p>Existing work indicates that documents that contain query terms at closer proximity are more likely to be relevant than documents that contain the same terms over longer distance. Of many attempts, only a few appear successful in exploiting this feature in a ranking function <ref type="bibr" coords="11,126.33,491.80,14.19,7.77" target="#b24">[24,</ref><ref type="bibr" coords="11,144.29,491.80,10.65,7.77" target="#b33">33]</ref>. Both methods emphasise near cooccurrences; Metzler and Croft <ref type="bibr" coords="11,165.86,502.26,14.94,7.77" target="#b24">[24]</ref> used only co-occurrences within a very small window and Tao and Zhai <ref type="bibr" coords="11,203.60,512.72,14.94,7.77" target="#b33">[33]</ref> used only the smallest distance between any two different query terms in a document. The use of these near occurrences alone is shown to give significant improvement over a non-proximity baseline. However, the need to exclude co-occurrences over longer distance reveals the need to properly balance the relevance estimation of term co-occurrences with respect to their distance in text.</p><p>We hypothesise that proximity between co-occurring query terms can still be useful over longer distance, for instance to decide which document is most likely to be relevant between two documents that are the same in other respects. A model that uses all available proximity information can outperform a model that uses less information, if the estimation of relevance for co-occurring terms over distance is sufficiently accurate. For this, we analysed the likelihood that term co-occurrences appear in a relevant document, by counting all term co-occurrences in the TREC 1-8 ad-hoc queries that according the qrels either appear in relevant or irrelevant documents. Using these statistics we estimate the likelihood that a term co-occurrence given a certain distance appears in a relevant document. This likelihood is inversely proportional to the distance of the terms in text. We also observed that adjacently appearing terms are close to twice as likely to appear in relevant documents than terms that most likely appear independently based on their span. Given that in the baseline function (Equation <ref type="formula" coords="11,480.70,89.31,3.73,7.77" target="#formula_5">5</ref>) each occurring independent term is scored as a count of 1, we designed a function that estimates the relevance of a term co-occurrence to be 1 for adjacent terms, decaying with 1 over distance and approaching 0 at infinity. In Equation <ref type="formula" coords="11,393.33,131.15,3.36,7.77" target="#formula_7">7</ref>, S is a set consisting of two or more query terms, O is an occurrence of S in a document D, and span (O) is the number of word positions of a text in D that starts with a term from S, ends with a term from S, and contains all terms from S.</p><formula xml:id="formula_7" coords="11,386.83,178.32,169.09,19.75">R (O, S) = |S| -1 span (O) -1<label>(7)</label></formula><p>Metzler and Croft <ref type="bibr" coords="11,382.47,206.87,14.94,7.77" target="#b24">[24]</ref> argued that terms that appear adjacent in the query are more important candidates than terms that are separated. However, in our pre-study we found that distinguishing between the weights of adjacent and non-adjacent query terms hardly improved results over simply using all possible term combinations using the same weight. Our ranking function in Equation 8 combines the KLD baseline with the additional score for the term proximities. For balancing the respective weight of the baseline and the proximity models, we have to compensate for the fact that the number of term combinations grow at a rate of 2 |Q| -|Q| -1 possible term combinations. In practice, the growth ratio of the weight of proximity model with respect to the size of the query does not grow exponentially, but increases with a ratio of |Q|, which we determined empirically. This less than exponential growth we observed can partly be explained by combinations of many terms being very rare. Term cooccurrences are scored as separate evidence over the unigrams they contain.</p><p>In Equation <ref type="formula" coords="11,371.41,395.17,3.36,7.77" target="#formula_9">9</ref>, the proximity model PM uses all combinations of two or more query terms, defined as an element of the powerset over Q with a cardinality greater than 1. Each term combination S is scored as the sum of the KLD function over all terms contained, replacing the original count of the unigram in the document with the estimated relevance of the term co-occurrence (Equation <ref type="formula" coords="11,531.61,447.47,7.10,7.77" target="#formula_10">10</ref>). In Equation 10, we estimate the relevance of each term combination S by adding up the relevance estimation of each occurrence O of that combination in document D using Equation <ref type="formula" coords="11,492.30,478.85,3.36,7.77" target="#formula_7">7</ref>.</p><formula xml:id="formula_8" coords="11,331.40,494.34,224.52,19.74">score (Q, D) = 1 |Q| • PM (Q, D) + KLD (Q, D)<label>(8)</label></formula><formula xml:id="formula_9" coords="11,336.03,519.64,219.89,25.33">PM (Q, D) = S∈P ≥1 (Q) t∈S log 1 + R (S, D) µ • P (t|C)<label>(9)</label></formula><formula xml:id="formula_10" coords="11,346.19,553.37,209.72,17.80">R (S, D) = O R (O, S)<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Stop words</head><p>In general, retrieval performance improves when stop words are removed from the query. The rationale for this is that stop words rarely occur more often in relevant documents, not even if they convey part of the information need by being part of a noun phrase, phrase of speech, describing a relation or complementing another term. Meaningful stop words may however occur closer to other query terms in relevant documents, making them potentially useful in proximity models.</p><p>The proximity model (Equation <ref type="formula" coords="11,442.75,680.09,3.73,7.77" target="#formula_9">9</ref>) uses the power set P ≥1 (Q), where Q is the query from which stop words are removed. For this variant, we can use the same proximity model by replacing this power set with a set that contains combinations with stop words.</p><p>To avoid irrelevant combinations with stop words from being used, we use a simple heuristic: a combination of query terms is only valid when all stop words used are used in combination with all words that connect them in the query to non stop words or the query boundary. For example, "The Beatles on a zebra crossing" would generate besides "Beatles zebra", "zebra crossing", "Beatles crossing" and "Beatles zebra crossing", the following combinations containing stop words: "The Beatles", "The Beatles on a zebra", "The Beatles zebra", "The Beatles zebra crossing", "The Beatles crossing", "Beatles on a zebra", "Beatles on a zebra crossing", and "The Beatles on a zebra crossing".</p><p>To test this hypothesis, we will run a model that contains all stop words as a variant of the proximity model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Results</head><p>For this study, we indexed ClueWeb12 using a Porter-2 stemmer, lowercasing terms and storing term-document positions in the posting lists. Stop words were included in the index, but removed from the queries for the KLD baseline and the proximity model CPE , but used for the proximity model with stop words CPS as described.  <ref type="table" coords="12,84.37,596.40,8.97,7.77" target="#tab_10">11</ref> and 12 compare the results over the metrics that emphasise diversification. These results show both proximity models to consistently improve over the KLD baseline. The proximity model that uses stop words performs significantly better than the proximity model without stop words. CPS scores close to the median of all TREC participants, but should not be considered a real competitor having no mechanism to handle diversity.</p><p>Table <ref type="table" coords="12,85.79,669.63,8.97,7.77" target="#tab_12">13</ref> compares the results over ERR@20 and nDCG@20, and Table <ref type="table" coords="12,92.07,680.09,8.97,7.77" target="#tab_13">14</ref> and Table <ref type="table" coords="12,142.11,680.09,8.97,7.77" target="#tab_14">15</ref> show the relative improvement in percentage of the proximity models over KLD and the track median. On these non-diversified metrics the proximity models outperform both baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Discussion</head><p>We participated in the Web Track ad-hoc task to evaluate a proximity model that uses the maximum available information. Our first hypothesis is that using the distance between query terms in a document is useful over longer distance if a sufficiently accurate relevance estimation is used. Although the results by the CPE model are encouraging, we will need additional experiments to show that using occurrences over longer distance is indeed beneficial. Our second hypothesis is that retrieval performance can be improved by including term combinations with stop words in the proximity model. The results show that the proximity model that uses stop words significantly improved results over the KLD baseline and also consistently outperformed the model that uses no stop words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,237.97,239.10,8.06;2,53.80,248.43,122.78,8.06"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Contextual Suggestion Track. Workflow followed to generate contextual suggestions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,111.67,362.65,386.37,8.06;10,53.80,53.80,502.11,294.61"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: KBA Track. Performance (F) of classification and LTR algorithms against feature addition.</figDesc><graphic coords="10,53.80,53.80,502.11,294.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,53.80,63.75,239.11,28.98"><head>Table 1 :</head><label>1</label><figDesc>Contextual Suggestion Track. Number of documents per context in our sub-collections, in the provided one by the organisers (given), and the intersection (and ratio) between them.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,316.81,63.75,239.11,631.41"><head>Table 2 :</head><label>2</label><figDesc>Contextual Suggestion Track. Effect of optimisation of the vector representation on the size of the collection.</figDesc><table coords="3,316.81,97.11,239.11,598.05"><row><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell cols="3">Size (GB)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Subcollection</cell><cell></cell><cell></cell><cell cols="2">918</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Vector representation</cell><cell></cell><cell>40</cell><cell></cell><cell></cell></row><row><cell cols="10">Data: ClueWeb12 WARC files, stop_words (from distributed</cell></row><row><cell cols="2">cache)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Input: &lt;(contextId,docId), WARC record&gt;</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Output: &lt;term, termId&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">begin mapper</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">content ← HTML content of the map input value;</cell><cell></cell></row><row><cell cols="7">cleaned_content← jsoup.clean(content);</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">tokens← tokenize(cleaned_content);</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">for each tok ∈ tokens do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">docTerms ← Map&lt;String,Long&gt; ;</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">if tok / ∈ stop_words then</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">if tok / ∈ docTerms then</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">docTerms.add(term,1);</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">for each term ∈ docTerms do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">emit(term,1);</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">begin reducer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">termIdMap ← Map&lt;String,Integer&gt;;</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">if term / ∈ termIdMap then</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">termIdMap.add(term,id);</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">for each term ∈ termIdMap do</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">emit (term, id);</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Algorithm 2: Contextual Suggestion Track. Pseudo-code for</cell></row><row><cell cols="5">generating collection dictionary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-4</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="10">Figure 2: Contextual Suggestion Track. Histogram for rating</cell></row><row><cell cols="10">differences considering all the contexts (description rating -</cell></row><row><cell cols="2">website rating).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,316.81,106.29,239.11,96.23"><head>Table 3 :</head><label>3</label><figDesc>Contextual Suggestion Track. Performance of our method when the entire ClueWeb12 collection or the given subcollection are used.</figDesc><table coords="5,326.07,148.40,220.60,54.13"><row><cell>Method</cell><cell>MRR MRR d</cell><cell>P@5 d P@5 dḡ</cell></row><row><cell>IBCosTop1</cell><cell cols="2">0.0559 0.0745 0.0587 0.2202</cell></row><row><cell>IBCosTop1 (given)</cell><cell cols="2">0.0528 0.0955 0.0484 0.0780</cell></row><row><cell>With classifier</cell><cell cols="2">0.0090 0.0179 0.0045 0.0260</cell></row><row><cell cols="3">With classifier (given) 0.0256 0.0540 0.0136 0.0159</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,316.81,469.79,246.76,99.35"><head>Table 4 :</head><label>4</label><figDesc>Contextual Suggestion Track. Intersection measures between submitted and unsubmitted approaches. x \ y represents the set difference or relative complement, i.e., those elements in x that are not y. Jacc stands for Jaccard similarity.</figDesc><table coords="5,322.19,530.56,241.38,38.57"><row><cell>Method a</cell><cell>Method b</cell><cell cols="3">|a \ b|/|a| |b \ a|/|b| Jacc(a, b)</cell></row><row><cell cols="2">IBCosTop1 IBCosTop1 (given)</cell><cell>0.99</cell><cell>0.98</cell><cell>0.01</cell></row><row><cell cols="2">IBCosTop1 Classifier</cell><cell>0.95</cell><cell>0.86</cell><cell>0.04</cell></row><row><cell>Classifier</cell><cell>Classifier (given)</cell><cell>0.99</cell><cell>0.80</cell><cell>0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,53.80,242.06,243.80,159.12"><head>Table 5 :</head><label>5</label><figDesc>Contextual Suggestion Track. Performance of variations of our method. NN stands for Nearest Neighbour.</figDesc><table coords="6,59.18,282.16,238.42,119.02"><row><cell>Method</cell><cell>MRR MRR d</cell><cell cols="2">P@5 d P@5 dḡ</cell></row><row><cell>IBCosTop1</cell><cell cols="2">0.0559 0.0745 0.0587</cell><cell>0.2202</cell></row><row><cell>IBCosTop1 + 5NN text cos</cell><cell cols="2">0.0455 0.0562 0.0330</cell><cell>0.1486</cell></row><row><cell>IBCosTop1 + 5NN text Jacc</cell><cell cols="2">0.0433 0.0521 0.0330</cell><cell>0.1294</cell></row><row><cell>IBCosTop1 + 5NN rating cos</cell><cell cols="2">0.0429 0.0553 0.0349</cell><cell>0.1477</cell></row><row><cell cols="3">IBCosTop1 + 5NN rating Pearson 0.0450 0.0580 0.0358</cell><cell>0.1560</cell></row><row><cell>Classifier + 5NN text cos</cell><cell cols="2">0.0045 0.0112 0.0036</cell><cell>0.0251</cell></row><row><cell>Classifier + 5NN text Jacc</cell><cell cols="2">0.0045 0.0121 0.0045</cell><cell>0.0260</cell></row><row><cell>Classifier + 5NN rating cos</cell><cell cols="2">0.0045 0.0090 0.0027</cell><cell>0.0242</cell></row><row><cell>Classifier + 5NN rating Pearson</cell><cell cols="2">0.0045 0.0067 0.0018</cell><cell>0.0233</cell></row><row><cell>Positive profile</cell><cell cols="2">0.0396 0.0588 0.0359</cell><cell>0.1498</cell></row><row><cell>Negative profile</cell><cell cols="2">0.0045 0.0045 0.0009</cell><cell>0.0152</cell></row><row><cell>Positive + 5NN text cos</cell><cell cols="2">0.0426 0.0572 0.0341</cell><cell>0.1399</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,316.81,174.70,239.11,202.55"><head>Table 6 :</head><label>6</label><figDesc>Federated Web Track. Performance of some variations of our approaches for the resource selection task using the FedWeb 2012 collection (ordered by MAP).</figDesc><table coords="7,350.57,218.51,171.59,158.73"><row><cell>Method</cell><cell cols="2">MAP nDCG MRR</cell></row><row><cell>TF-IDF+ODP Jacc</cell><cell>0.338</cell><cell>0.516 0.564</cell></row><row><cell>TF-IDF</cell><cell>0.285</cell><cell>0.412 0.610</cell></row><row><cell>ODP Jaccard</cell><cell>0.283</cell><cell>0.471 0.439</cell></row><row><cell>BM25 (1.2, 0.2)</cell><cell>0.283</cell><cell>0.400 0.545</cell></row><row><cell>LM (λ = 0.1)</cell><cell>0.280</cell><cell>0.407 0.590</cell></row><row><cell>ODP Cosine</cell><cell>0.278</cell><cell>0.462 0.400</cell></row><row><cell>BM25 (1.2, 0.8)</cell><cell>0.272</cell><cell>0.397 0.557</cell></row><row><cell>LM (λ = 0.5)</cell><cell>0.263</cell><cell>0.394 0.571</cell></row><row><cell>LM (λ = 0.9)</cell><cell>0.252</cell><cell>0.387 0.566</cell></row><row><cell>LM (λ = 0.1) desc</cell><cell>0.241</cell><cell>0.386 0.602</cell></row><row><cell>LM (µ = 200)</cell><cell>0.240</cell><cell>0.378 0.551</cell></row><row><cell>LM (µ = 2000)</cell><cell>0.240</cell><cell>0.378 0.551</cell></row><row><cell cols="2">BM25 (1.2, 0.8) desc 0.239</cell><cell>0.383 0.608</cell></row><row><cell>TF-IDF title</cell><cell>0.215</cell><cell>0.321 0.495</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,316.81,524.79,239.11,180.11"><head>Table 7 :</head><label>7</label><figDesc>Federated Web Track. Performance of the variations of our approaches in the resource selection task (ordered by nDCG@20).</figDesc><table coords="7,328.64,567.95,215.46,136.95"><row><cell>Method</cell><cell>Run</cell><cell cols="2">nDCG@20 ERR@20</cell></row><row><cell cols="2">BM25 (1.2, 0.8) desc -</cell><cell>0.1588</cell><cell>0.0204</cell></row><row><cell>LM (λ = 0.1) desc</cell><cell>-</cell><cell>0.1476</cell><cell>0.0204</cell></row><row><cell>BM25 (1.2, 0.2)</cell><cell>-</cell><cell>0.1346</cell><cell>0.0068</cell></row><row><cell>LM (λ = 0.1)</cell><cell>-</cell><cell>0.1322</cell><cell>0.0068</cell></row><row><cell>TF-IDF</cell><cell>CWI13SniTI</cell><cell>0.1235</cell><cell>0.0067</cell></row><row><cell>BM25 (1.2, 0.8)</cell><cell>-</cell><cell>0.1223</cell><cell>0.0102</cell></row><row><cell>LM (λ = 0.5)</cell><cell>-</cell><cell>0.1218</cell><cell>0.0051</cell></row><row><cell>LM (λ = 0.9)</cell><cell>-</cell><cell>0.1153</cell><cell>0.0041</cell></row><row><cell>LM (µ = 2000)</cell><cell>-</cell><cell>0.1033</cell><cell>0.0051</cell></row><row><cell>LM (µ = 200)</cell><cell>-</cell><cell>0.1017</cell><cell>0.0051</cell></row><row><cell>TF-IDF title</cell><cell>-</cell><cell>0.1016</cell><cell>0.0017</cell></row><row><cell>TF-IDF+ODP Jacc</cell><cell>CWI13ODPTI</cell><cell>0.0961</cell><cell>0.0034</cell></row><row><cell>LM (λ = 0.9)</cell><cell>-</cell><cell>0.0934</cell><cell>0.0017</cell></row><row><cell>ODP Jaccard</cell><cell>CWI13ODPJac</cell><cell>0.0497</cell><cell>0.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,53.80,274.52,239.11,156.54"><head>Table 8 :</head><label>8</label><figDesc>Federated Web Track. Performance of our approaches ordered by P@10. * indicates an unsubmitted run.</figDesc><table coords="8,59.38,307.48,227.94,123.58"><row><cell>Method</cell><cell cols="3">P@10 nDCG@20 nDCG@50 nDCG</cell></row><row><cell>2013 data</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CWI13bstBM25desc  *  0.3408</cell><cell>0.1224</cell><cell>0.2024 0.5366</cell></row><row><cell>CWI13IndriQL</cell><cell>0.3220</cell><cell>0.1622</cell><cell>0.2371 0.5438</cell></row><row><cell>CWI13iaTODPJ</cell><cell>0.2840</cell><cell>0.1509</cell><cell>0.1915 0.5253</cell></row><row><cell>CWI13bstTODPJ</cell><cell>0.2500</cell><cell>0.1466</cell><cell>0.1839 0.4973</cell></row><row><cell>CWI13clTODPJ  *</cell><cell>0.1940</cell><cell>0.0551</cell><cell>0.0892 0.4610</cell></row><row><cell>2012 data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CWI12bstTODPJ  *</cell><cell>0.4960</cell><cell>0.1246</cell><cell>0.1989 0.6081</cell></row><row><cell>CWI12IndriQL  *</cell><cell>0.4900</cell><cell>0.1464</cell><cell>0.2627 0.6525</cell></row><row><cell>CWI12clTODPJ  *</cell><cell>0.2200</cell><cell>0.0666</cell><cell>0.1106 0.5462</cell></row><row><cell>CWI12iaTODPJ  *</cell><cell>0.1940</cell><cell>0.0532</cell><cell>0.1015 0.5407</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,53.80,398.72,239.10,175.42"><head>Table 10 :</head><label>10</label><figDesc>KBA Track. System Performances on vital (upper half) and vital+useful (lower half).</figDesc><table coords="10,79.03,431.82,188.63,142.32"><row><cell>system_id</cell><cell>F</cell><cell>SU</cell><cell>Group</cell></row><row><cell>RF _all_13_13</cell><cell cols="3">0.575 0.571 turing</cell></row><row><cell>RF _MB_13_13</cell><cell cols="3">0.388 0.404 fargo</cell></row><row><cell>RF _all_1213_13</cell><cell cols="3">0.353 0.370 hep</cell></row><row><cell cols="4">RF _FPN_1213_13 0.338 0.333 ocala</cell></row><row><cell>RF _FPN_12_13</cell><cell cols="3">0.298 0.395 bronfman</cell></row><row><cell>RF _MB_13_13</cell><cell cols="3">0.290 0.333 wikipedia</cell></row><row><cell>RF _FPN_13_13</cell><cell cols="3">0.279 0.422 danville</cell></row><row><cell>RF _MB_13_13</cell><cell cols="3">0.247 0.333 all-entities</cell></row><row><cell>RF _MB_13_13</cell><cell cols="3">0.241 0.341 hoboken</cell></row><row><cell>J48_13_13</cell><cell cols="3">0.232 0.333 screenwriters</cell></row><row><cell>RF _all_13_13</cell><cell cols="3">0.649 0.647 wikipedia</cell></row><row><cell cols="4">RF _MB_1213_13 0.603 0.602 all-entities</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="12,53.80,291.50,239.11,118.60"><head>Table 11 :</head><label>11</label><figDesc>Ad-hoc Web Track. Comparison of three runs and the median of all Web Track runs using ERR. Results marked with † are significant over KLD and results marked with ‡ are significant over CPE , tested using 1-tailed paired Student's Ttest, α = 0.05.</figDesc><table coords="12,73.80,355.98,199.10,54.13"><row><cell>runid</cell><cell cols="3">ERR-IA@5 ERR-IA@10 ERR-IA@20</cell></row><row><cell cols="2">cwiwt13kld 0.3032</cell><cell>0.3272</cell><cell>0.3363</cell></row><row><cell cols="2">cwiwt13cpe 0.3912  †</cell><cell>0.4081  †</cell><cell>0.4177  †</cell></row><row><cell cols="3">cwiwt13cps 0.4555  † ‡ 0.4726  † ‡</cell><cell>0.4803  † ‡</cell></row><row><cell>median</cell><cell></cell><cell></cell><cell>0.4739</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="12,53.80,451.31,239.11,152.86"><head>Table 12 :</head><label>12</label><figDesc>Ad-hoc Web Track. Comparison of three runs and the median of all Web Track runs using α-nDCG. Results marked with † are significant over KLD and results marked with ‡ are significant over CPE , tested using 1-tailed paired Student's T-test, α = 0.05.</figDesc><table coords="12,62.77,515.51,213.77,88.67"><row><cell>runid</cell><cell cols="3">α-nDCG@5 α-nDCG@10 α-nDCG@20</cell></row><row><cell cols="2">cwiwt13kld 0.3456</cell><cell>0.3970</cell><cell>0.4284</cell></row><row><cell cols="2">cwiwt13cpe 0.4342  †</cell><cell>0.4713  †</cell><cell>0.5033  †</cell></row><row><cell cols="2">cwiwt13cps 0.4920  †</cell><cell>0.5309  † ‡</cell><cell>0.5572  † ‡</cell></row><row><cell>median</cell><cell></cell><cell></cell><cell>0.5745</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="12,316.81,63.75,239.11,118.86"><head>Table 13 :</head><label>13</label><figDesc>Ad-hoc Web Track. Comparison of three runs and the median of all Web Track runs using nDCG@20 and ERR@20. Results marked with † are significant over KLD and results marked with ‡ are significant over CPE , tested using 1tailed paired Student's T-test, α = 0.05.</figDesc><table coords="12,369.49,128.49,133.75,54.13"><row><cell>runid</cell><cell cols="2">nDCG@20 ERR@20</cell></row><row><cell cols="2">cwiwt13kld 0.1648</cell><cell>0.0850</cell></row><row><cell cols="2">cwiwt13cpe 0.1910</cell><cell>0.1090</cell></row><row><cell cols="3">cwiwt13cps 0.2181  † 0.1285  † ‡</cell></row><row><cell>median</cell><cell>0.1739</cell><cell>0.0980</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="12,316.81,217.55,239.11,75.31"><head>Table 14 :</head><label>14</label><figDesc>Ad-hoc Web Track. Relative improvement of CPE and CPS over KLD and the track median, based on the nDCG@20results.</figDesc><table coords="12,370.58,259.66,131.57,33.20"><row><cell>runid</cell><cell cols="2">nDCG@20 ERR@20</cell></row><row><cell>cwiwt13cpe</cell><cell>+15.9%</cell><cell>+9.8%</cell></row><row><cell>cwiwt13cps</cell><cell cols="2">+32.4% +25.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="12,316.81,327.80,239.11,75.31"><head>Table 15 :</head><label>15</label><figDesc>Ad-hoc Web Track. Relative improvement of CPE and CPS over KLD and the track median, based on the ERR@20results.</figDesc><table coords="12,372.95,369.90,126.83,33.20"><row><cell>runid</cell><cell>cwiwt13kld median</cell></row><row><cell>cwiwt13cpe</cell><cell>+28.2% +11.1%</cell></row><row><cell>cwiwt13cps</cell><cell>+51.1% +31.0%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,321.30,703.35,220.57,6.31;4,316.81,712.32,150.64,6.31"><p>http://docs.oracle.com/javase/6/docs/api/ java/text/BreakIterator.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="5,58.28,683.83,193.67,6.31"><p>http://www.cs.waikato.ac.nz/ml/weka/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,58.28,693.54,234.62,7.77;5,53.80,702.51,239.11,7.77;5,53.80,711.47,153.00,7.77"><p>As described by the organisers: "this sub-collection was created by issuing a variety of queries targeted to the Contextual Suggestion track on a commercial search engine."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,321.30,682.41,102.22,6.31"><p>http://www.dmoz.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="6,321.30,692.96,215.19,6.31;6,316.81,701.93,21.52,6.31"><p>http://www.dmoz.org/search?q=QUERY&amp;type= more</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="6,321.30,712.32,129.12,6.31"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="9,58.28,703.35,225.95,6.31;9,53.80,712.32,113.48,6.31"><p>http://krisztianbalog.com/files/resources/ oair2013-kba/runs.zip</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="9,321.30,702.51,234.62,7.77;9,316.81,711.47,67.14,7.77"><p>On our run submissions we used RM to mean RF. Please replace every RM with RF</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research</rs> (<rs type="projectName">NWO</rs> project #<rs type="grantNumber">640.005.001</rs>) and under <rs type="projectName">COM-MIT</rs> project <rs type="projectName">Infiniti</rs>. Part of this work was carried out during the tenure of an <rs type="grantName">ERCIM "Alain Bensoussan" Fellowship Programme</rs>, funded by <rs type="funder">European Comission FP7</rs> grant agreement no.<rs type="grantNumber">246016</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_6KYesKQ">
					<idno type="grant-number">640.005.001</idno>
					<orgName type="project" subtype="full">NWO</orgName>
				</org>
				<org type="funded-project" xml:id="_qZHr5yY">
					<orgName type="project" subtype="full">COM-MIT</orgName>
				</org>
				<org type="funded-project" xml:id="_YsM6WYv">
					<idno type="grant-number">246016</idno>
					<orgName type="grant-name">ERCIM &quot;Alain Bensoussan&quot; Fellowship Programme</orgName>
					<orgName type="project" subtype="full">Infiniti</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,321.30,674.42,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.23,701.01,219.69,7.77;12,336.23,711.32,186.51,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,533.05,701.01,22.86,7.77;12,336.23,711.47,74.74,7.77">Diversifying search results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,427.12,711.32,22.51,7.72">WSDM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,57.92,219.69,7.77;13,73.22,68.38,219.68,7.77;13,73.22,78.69,79.68,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,126.06,68.38,166.85,7.77;13,73.22,78.84,16.80,7.77">CWI at TREC 2012, KBA track and session track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gebremeskel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosscarino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,106.32,78.69,19.33,7.72">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,96.16,219.69,7.77;13,73.22,106.62,219.69,7.77;13,73.22,117.08,219.69,7.77;13,73.22,127.39,154.14,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,197.18,96.16,95.73,7.77;13,73.22,106.62,219.69,7.77;13,73.22,117.08,219.69,7.77;13,73.22,127.54,35.18,7.77">Why label when you can search? Alternatives to active learning for applying human resources to build classification models under extreme class imbalance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Attenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,125.25,127.39,15.88,7.72">KDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,144.85,219.68,7.77;13,73.22,155.16,219.68,7.93;13,73.22,165.77,38.11,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,192.71,144.85,100.19,7.77;13,73.22,155.31,133.74,7.77">Cumulative citation recommendation: Classification vs. ranking</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ramampiaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,224.84,155.16,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="941" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,183.08,219.69,7.77;13,73.22,193.54,219.69,7.77;13,73.22,203.85,178.55,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,73.22,193.54,219.69,7.77;13,73.22,204.00,58.34,7.77">Multi-step classification approaches to cumulative citation recommendation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ramampiaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Takhirov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,148.66,203.85,17.73,7.72">OAIR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,221.31,219.69,7.77;13,73.22,231.77,219.69,7.77;13,73.22,242.08,155.96,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,208.67,221.31,84.23,7.77;13,73.22,231.77,219.69,7.77;13,73.22,242.23,33.69,7.77">Modeling higher-order term dependencies in information retrieval using query hypergraphs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,123.59,242.08,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="941" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,259.54,219.69,7.77;13,73.22,270.00,219.69,7.77;13,73.22,280.31,105.59,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,236.02,259.54,56.88,7.77;13,73.22,270.00,204.58,7.77">Term proximity scoring for ad-hoc retrieval on very large text collections</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,73.22,280.31,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="621" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,297.77,219.69,7.77;13,73.22,308.08,219.68,7.93;13,73.22,318.54,138.29,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,256.01,297.77,36.90,7.77;13,73.22,308.23,129.03,7.77">Relevance ranking for one to three term queries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Tudhope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,208.57,308.08,84.33,7.72;13,73.22,318.54,53.65,7.72">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="311" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,336.01,219.69,7.77;13,73.22,346.31,219.68,7.93;13,73.22,356.93,69.73,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,227.01,336.01,65.89,7.77;13,73.22,346.47,171.98,7.77">The use of phrases and structured queries in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,268.25,346.31,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="32" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,374.24,219.69,7.77;13,73.22,384.70,219.69,7.77;13,73.22,395.01,105.59,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,205.95,374.24,86.96,7.77;13,73.22,384.70,204.62,7.77">Learning in a pairwise term-term proximity framework for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>O'riordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,73.22,395.01,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,412.47,219.69,7.77;13,73.22,422.93,219.69,7.77;13,73.22,433.39,219.69,7.77;13,73.22,443.70,219.68,7.93;13,73.22,454.16,204.36,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,191.47,412.47,101.44,7.77;13,73.22,422.93,168.24,7.77">A comprehensive survey of neighborhood-based recommendation methods</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,210.60,443.70,82.30,7.72;13,73.22,454.16,35.20,7.72">Recommender Systems Handbook</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="107" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,471.62,219.68,7.77;13,73.22,481.93,219.68,7.93;13,73.22,492.54,20.17,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,259.98,471.62,32.92,7.77;13,73.22,482.08,111.89,7.77">Rank aggregation methods for the web</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,207.32,481.93,18.49,7.72">WWW</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,509.85,219.69,7.77;13,73.22,520.16,110.82,7.93" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,109.90,509.85,179.84,7.77">Automatic phrase indexing for document retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,82.93,520.16,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,537.62,219.69,7.77;13,73.22,547.93,219.68,7.93;13,73.22,558.55,20.17,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,213.71,537.62,79.19,7.77;13,73.22,548.09,113.68,7.77">Dependence language model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,207.56,547.93,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,575.86,219.69,7.77;13,73.22,586.17,219.68,7.93;13,73.22,596.63,131.24,7.93" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,200.91,575.86,92.00,7.77;13,73.22,586.32,165.39,7.77">Modeling term proximity for probabilistic information retrieval models</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,250.06,586.17,42.84,7.72;13,73.22,596.63,29.43,7.72">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3017" to="3031" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,614.09,219.69,7.77;13,73.22,624.40,219.68,7.93;13,73.22,634.86,219.68,7.93;13,73.22,645.47,38.11,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,193.59,614.09,99.31,7.77;13,73.22,624.55,119.33,7.77">Result diversification based on query-specific cluster ranking</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,202.70,624.40,90.20,7.72;13,73.22,634.86,173.02,7.72">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="571" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,662.78,219.69,7.77;13,73.22,673.09,219.68,7.93;13,73.22,683.70,78.70,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,195.16,662.78,97.74,7.77;13,73.22,673.24,180.07,7.77">Combining implicit and explicit topic representations for result diversification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,268.25,673.09,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,73.22,701.01,219.69,7.77;13,73.22,711.32,120.28,7.93" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,179.05,701.01,113.86,7.77;13,73.22,711.47,57.40,7.77">IRIT at TREC 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cabanac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,146.92,711.32,19.33,7.72">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,57.92,219.69,7.77;13,336.23,68.23,212.56,7.93" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,381.31,57.92,174.61,7.77;13,336.23,68.38,42.11,7.77">The use of term position devices in ranked output experiments</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Keen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,385.40,68.23,92.55,7.72">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,86.02,219.69,7.77;13,336.23,96.33,130.88,7.93" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="13,408.45,86.02,147.47,7.77;13,336.23,96.48,60.23,7.77">Leveraging related entities for knowledge base acceleration</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<editor>Web-KR</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,114.11,219.69,7.77;13,336.23,124.57,219.69,7.77;13,336.23,135.03,219.69,7.77;13,336.23,145.34,219.68,7.93;13,336.23,155.80,199.88,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,488.27,114.11,67.65,7.77;13,336.23,124.57,170.05,7.77">Content-based recommender systems: State of the art and trends</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lops</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,473.61,145.34,82.30,7.72;13,336.23,155.80,35.20,7.72">Recommender Systems Handbook</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="73" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,173.44,219.68,7.93;13,336.23,183.90,219.68,7.93" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,372.38,173.59,160.42,7.77">The automatic creation of literature abstracts</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,539.98,173.44,15.93,7.72;13,336.23,183.90,134.87,7.72">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,201.68,219.69,7.77;13,336.23,211.99,166.39,7.93" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,409.12,201.68,146.80,7.77;13,336.23,212.15,44.70,7.77">Positional language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,397.03,211.99,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,229.78,219.69,7.77;13,336.23,240.09,200.44,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,444.92,229.78,111.00,7.77;13,336.23,240.24,78.03,7.77">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,431.09,240.09,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,257.87,219.69,7.77;13,336.23,268.33,219.69,7.77;13,336.23,278.64,213.90,7.93" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="13,336.23,268.33,219.69,7.77;13,336.23,278.80,82.70,7.77">Federated search in the wild: the combined power of over a hundred search engines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,435.57,278.64,19.73,7.72">CIKM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1874" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,296.43,219.69,7.77;13,336.23,306.74,219.68,7.93;13,336.23,317.20,115.92,7.93" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="13,450.08,296.43,105.84,7.77;13,336.23,306.89,114.92,7.77">Term proximity scoring for keyword-based retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rasolofo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,467.51,306.74,88.40,7.72;13,336.23,317.20,31.48,7.72">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,334.98,219.69,7.77;13,336.23,345.45,219.68,7.77;13,336.23,355.75,191.78,7.93" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="13,371.00,345.45,184.91,7.77;13,336.23,355.91,67.82,7.77">GroupLens: An open architecture for collaborative filtering of netnews</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Iacovou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,420.92,355.75,20.92,7.72">CSCW</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,373.54,219.69,7.77;13,336.23,383.85,219.68,7.93;13,336.23,394.46,56.04,7.77" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="13,476.41,373.54,79.51,7.77;13,336.23,384.00,79.50,7.77">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,422.08,383.85,100.26,7.72">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,412.10,219.69,7.77;13,336.23,422.40,214.18,7.93" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="13,419.69,412.10,136.23,7.77;13,336.23,422.56,83.76,7.77">Using various term dependencies according to their utilities</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,435.85,422.40,19.73,7.72">CIKM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1493" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,440.19,219.69,7.77;13,336.23,450.65,219.69,7.77;13,336.23,461.11,219.69,7.77;13,336.23,471.57,179.20,7.77" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="13,537.85,440.19,18.07,7.77;13,336.23,450.65,219.69,7.77;13,336.23,461.11,97.87,7.77">Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>UM-CS-2012- 015</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="13,336.23,489.21,219.69,7.77;13,336.23,499.52,219.68,7.93;13,336.23,509.98,196.85,7.93" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="13,336.23,499.67,188.73,7.77">Viewing term proximity from a different perspective</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,542.97,499.52,12.95,7.72;13,336.23,509.98,112.42,7.72">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="346" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,527.76,219.69,7.77;13,336.23,538.07,219.68,7.93;13,336.23,548.68,20.17,7.77" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="13,462.18,527.76,93.73,7.77;13,336.23,538.22,111.42,7.77">A cross-lingual dictionary for English Wikipedia concepts</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,464.29,538.07,19.33,7.72">LREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,566.32,219.69,7.77;13,336.23,576.63,194.77,7.93" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="13,409.33,566.32,146.59,7.77;13,336.23,576.78,73.08,7.77">An exploration of proximity measures in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,425.42,576.63,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,594.41,219.69,7.77;13,336.23,604.72,104.60,7.93" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="13,385.57,594.41,156.63,7.77">Learning with rare cases and small disjuncts</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,336.23,604.72,18.93,7.72">ICML</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,622.51,219.69,7.77;13,336.23,632.82,176.18,7.93" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="13,412.82,622.51,143.10,7.77;13,336.23,632.97,79.87,7.77">Opinion-based user profile modeling for contextual suggestions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,432.72,632.82,20.55,7.72">ICTIR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,650.60,219.69,7.77;13,336.23,661.06,219.69,7.77;13,336.23,671.37,105.59,7.93" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="13,428.51,650.60,127.41,7.77;13,336.23,661.06,203.87,7.77">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,336.23,671.37,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.23,689.16,219.69,7.77;13,336.23,699.47,177.34,7.93" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="13,414.27,689.16,141.65,7.77;13,336.23,699.62,55.65,7.77">A proximity language model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,407.99,699.47,20.54,7.72">SIGIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
