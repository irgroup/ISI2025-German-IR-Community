<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.13,72.23,339.47,16.98">PKUICST at TREC 2013 Microblog Track</title>
				<funder ref="#_2kdxQKc">
					<orgName type="full">National Natural science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.91,125.01,74.96,11.32"><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
							<email>qiangrw@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.15,125.01,39.53,11.32"><forename type="first">Yue</forename><surname>Fei</surname></persName>
							<email>feiyue@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.95,125.01,68.87,11.32"><forename type="first">Yihong</forename><surname>Hong</surname></persName>
							<email>hongyihong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.10,125.01,66.78,11.32;1,448.87,118.64,1.54,8.37"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
							<email>yangjw@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.13,72.23,339.47,16.98">PKUICST at TREC 2013 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4FF78BF4DB345C46667B9BD8CD7A5A9F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes PKUICST's entry into the TREC 2013 Microblog track. In this year of microblog track, we designed and conducted a series of experiments based on both our local crawled collection and the official track API. For runs with local crawled dataset, we exploited different retrieval models, such as TFIDF, Okapi BM25 and statistic language model and tuned optimal parameters for all these models with the dataset in TREC 2012. Furthermore, we attempted to combine these models to gain a better performance with the help of learning to rank framework. For runs with the official track API, we employed language model with two-stage pseudo feedback query expansion. In addition, a filtering component was adopted to refine the results retrieved by the expanded query. Experimental results demonstrate that our approach obtains convincing performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Information retrieval in microblogging environment has attracted a lot of attention with the growing popularity of microblog. To explore the search behavior and boost the retrieval performance in the real-time environment, TREC first introduced Real-Time Search task in 2011 <ref type="bibr" coords="1,252.68,462.54,9.21,7.86" target="#b8">[8]</ref>, where a user's information need is represented by a query at a specific point in time. Systems should favor relevant and highly informative tweets about the query topic, which makes this task skin to ad-hoc search on Twitter. The primary difference this year from the 2011-2012 lies in the tweet collection and the way the participants interact with it. For TREC 2011-2012, participants are requested to acquire local copy of a canonical corpus and do all the experiments on the local corpus, while for TREC 2013 participants can also interact with a tweet collection stored remotely via a search API. The motivation for this trackas-a-service design is to increase the size of the collection while adhering to Twitter's terms of services. However, this restricts a highly-customized preprocessing of the corpus but only allows a small number of operations such as query modification and re-ranking of results obtained from the baseline retrieval. Thus we also crawled a local copy of the tweets2013 collection with the help of provided official streaming tool 1 and attempted both interactive ways in our experiments.</p><p>For runs with our local corpus, we first retrieved top * Corresponding author. 1 https://github.com/lintool/twitter-tools 20,000 relevant tweets as candidates with Real-time Tweet Ranking (RTR) model, and then re-rank the candidate tweets to gain a better performance with the help of learning to rank framework. For runs with the official track API, we employed language model with a two-stage pseudo feedback query expansion. In addition, a filtering component was adopted to refine the results retrieved by the expanded query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM OVERVIEW</head><p>Our system mainly contains two steps : (1) initial search using RTR model, which is based on three classical IR models(e.g. language model), and (2) re-ranking the initial result sets with Ranking SVM <ref type="bibr" coords="1,451.94,354.20,9.21,7.86">[6]</ref>. The architecture of our system is shown in Figure <ref type="figure" coords="1,447.09,364.66,3.58,7.86" target="#fig_0">1</ref>. Note that for our runs with local copy of the canonical corpus, a preprocessing component is utilized to gain a better retrieval performance.</p><p>We first apply a few preprocessings such as stemming, stopwords elimination and non-English detection on both initial corpus and query. The shortened URLs within tweets are very informative as they are always aimed at tracking breaking news stories, recommending interesting video clips and brand marketing <ref type="bibr" coords="1,402.29,448.34,9.21,7.86" target="#b5">[5]</ref>. Thus, we extract topic information from the web pages we crawl through these shortened URLs, and form a new corpus called TopicInfo Corpus. Another usage of the topic information is directly replacing the URLs in the tweet and generating a new DE (i.e. Document Expansion) Corpus.</p><p>In the initial search stage, RTR (i.e. Real-Time Ranking) models are utilized to return the top 20,000 candidate tweets for each query. In the re-ranking stage, effective features (i.e. semantic features and quality features) of these candidate tweets are generated so that we could employ the learning to rank framework to re-rank the candidate tweets. Finally, the re-ranked top 1,000 relevant tweets are selected as the final results of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>The official collection of TREC 2013 consists of approximately 240 million tweets, while our local copy of the canonical corpus contains about 259 million tweets in total, which is really a huge size compared to TREC 2012 collection (about 10 million tweets over two weeks). Both of the corpora are collected via the Twitter streaming API over a two-month period: 1 February, 2013 -31 March, 2013 (inclusive).</p><p>The preprocessings we adopted on the corpus and topic set (i.e. queries) are described as follows:  • Non-English Filtering: We discarded the non-English tweets by using a language detector with infinity-gram, named ldig 2 .</p><p>• Simple Retweet Elimination: We eliminate tweets that begin with 'RT' with the consideration that these tweets are simple retweets without any additional information.</p><p>• Hashtag Segmentation: Many tweets are marked with so-called hashtags. A hashtag is a character string preceded by a '#' sign. Hashtags often signal aspects of a tweet's meaning such as its topic or its intended audience <ref type="bibr" coords="2,154.66,520.49,9.21,7.86" target="#b3">[3]</ref>. As no space is allowed in a hashtag, we segment them into tokens using an English dictionary generated with the tweets2013 corpus and the MaxMatch algorithm. The segmented words are then added to the original tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• TopicInfo based Document Expansion:</head><p>As mentioned above, we collected all the external URLs (i.e. TopicInfo corpus) contained in TREC 2013 corpus and extracted their title information for our document expansion process. Note that web pages might be deleted as time elapsed, we have only crawled a portion of the external URL set. When adding the topic information to the original corpus, we name the newly generated corpus DE corpus. This expansion is optional as we can get more independent features when computing similarity scores using Origin Corpus or TopicInfo Corpus respectively.</p><p>2 https://github.com/shuyo/ldig</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Real-Time Tweet Ranking Model</head><p>Given a real-time search problem, the ideal system should consider: (1) build a dynamic dataset for each query to avoid using the future resources; (2) use expansion techniques to enrich the representation of both queries and documents; (3) make a tradeoff between relevance and recentness. To solve these challenges, Liang et al. <ref type="bibr" coords="2,546.20,143.71,9.73,7.86" target="#b7">[7]</ref> proposed a RTR model, which highlights the following aspects: (1) describe a two-stage pseudo-relevance feedback query expansion to estimate a query language model. <ref type="bibr" coords="2,544.15,175.10,11.77,7.86" target="#b2">(2)</ref> propose two ways to expand documents with the shortened URL's information to enrich the representation of the documents and (3) suggest several temporal re-ranking functions and two representations of temporal profile to evaluate the temporal aspect of documents.</p><p>In our systems, we implemented different RTR models with three classical IR algorithms for comparison:</p><p>• Vector Space Model Vector space model is an algebraic model for representing text documents as vectors of identifiers. We express the tweet and query as vectors.</p><p>-→ Ti = (w1i, w2i, w3i,</p><formula xml:id="formula_0" coords="2,391.11,313.26,112.92,32.61">• • • wni) -→ Qi = (w1q, w2q, w3q, • • • wnq)</formula><p>The TFIDF weighting scheme is adopted as the term weight and the Cosine Similarity Metric is used to evaluate the relevance between tweets and query. The Cosine Similarity Metric is defined as Eq.1.</p><formula xml:id="formula_1" coords="2,409.77,396.95,146.16,28.85">Sim = -→ Ti • -→ Q -→ Ti • -→ Q (1)</formula><p>• Okapi BM25 Model Okapi BM25 model is a bag-ofwords retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). The similarity of a document D to query Q is defined as Eq.2.</p><formula xml:id="formula_2" coords="2,339.23,511.03,216.70,37.39">Sim = ∑ q i ∈Q IDF (qi)• f (qi, D) • (k1 + 1) f (qi, D) + k1 • (1 -b + b • |D| avgdl ) (2)</formula><p>Where f (qi, D) is qi's term frequency in the document D, |D| is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. k1 and b are free parameters.</p><p>• Language Model A statistical language model assigns a probability to a sequence of m words P (w1, . . . , wm) by means of a probability distribution. With query Q as input, retrieved documents are ranked based on the probability that the document's language model would generate the terms of the query.</p><p>Next we describe the language model based RTR model in detail. For the other two approaches, we use the same query expansion and document expansion techniques. To rank tweets for a given topic, RTR model estimates the probability of generating a query Q given the content D and timestamp t of the tweet as follows:</p><formula xml:id="formula_3" coords="3,107.24,84.36,185.68,19.75">P (Q|D, t) = P (t|Q, D) • P (Q|D) P (t|D)<label>(3)</label></formula><p>Assume that P (Q|D) ∝ Score(Q, D) which can be calculated using Kullback-Leibler retrieval model <ref type="bibr" coords="3,257.41,123.48,13.50,7.86" target="#b10">[10]</ref>, and that P (t|D) can be assumed as a constant because it is query-independent, the ranking formula can be rewritten as follows:</p><formula xml:id="formula_4" coords="3,64.50,172.12,228.41,46.88">P (Q|D, t) ∝ P (t|Q, D) • P (Q|D) ∝ P (t|Q, D) • Score(Q, D) = P (t|Q, D) • ∑ w∈V P (w| θQ) • log P (w| θD)(4)</formula><p>With the ranking formula, the retrieval task is reduced to three subtasks, i.e. the estimation of query model θQ, the estimation of document model θD and the temporal re-ranking component P (t|Q, D), respectively. Considering that this year's task doesn't require participants to rank returned tweets by timestamp, we just implement the estimation of query model and the estimation of document model.</p><p>For the estimation of query model, RTR model adopts a two-stage pseudo-relevance feedback query expansion as follows: <ref type="bibr" coords="3,89.60,333.16,11.77,7.86" target="#b1">(1)</ref> In the first stage, a single tweet is picked up to generate topical words using the maximum likelihood estimator. (2) In the second stage, a group pseudorelevant tweets are used to distill the relevant content by implementing the model-based feedback approach <ref type="bibr" coords="3,257.39,375.00,13.50,7.86" target="#b11">[11]</ref>.</p><p>It is important to point out that the single tweet (i.e. issue tweet), which is generated in the first stage query expansion can be used to calculate another score with both original tweets and topic information for further semantic representation. Overall, the estimation of query model can be represented as:</p><formula xml:id="formula_5" coords="3,77.70,452.66,215.21,11.61">P (w| θQ ) = (1 -α) • P (w| θQ) + α • P (w| θP RF 1 ) (5)</formula><p>For the estimation of the document model, RTR model presents two ways to utilize the external resource, i.e. TopicInfo corpus and we use it as local context <ref type="bibr" coords="3,237.68,493.21,9.20,7.86" target="#b7">[7]</ref>. We merge the original tweet T and topic information I if exists to form a new document and estimate the document language model using Dirichlet Smoothing <ref type="bibr" coords="3,162.12,524.59,14.33,7.86" target="#b10">[10]</ref> as follows:</p><formula xml:id="formula_6" coords="3,109.80,541.36,183.12,20.26">P (w| θD) = c(w, D) + µP (w|C) |D| + µ (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Optimizing classical IR models</head><p>Microblogs are extremely short compared to traditional IR documents due to a 140 characters limit, thus it seems intuitive that standard term weighting approaches may not be effective for short documents like microblogs.</p><p>Paul et al. <ref type="bibr" coords="3,124.56,627.50,9.72,7.86" target="#b4">[4]</ref> examined the applicability of term frequency statistics and document length normalization to microblog retrieval. They found that document length normalization always harms performance, and benefit from incorporating term frequency statistics is minor, where term frequency statistics denotes parameter k1 and document length normalization denotes parameter b in Eq.2. Our experiments also show that the optimal parameters for BM25 model are quite different from typical parameters(k1 = 1.2, b = 0.75). We get the optimal parameters in TREC 2012 corpus where k1 = 0.3 and b = 0.05, indicating that document length normalization and term frequency statistics have little contributions in short text retrieval. For the language model approaches, we use the best empirical parameters reported in <ref type="bibr" coords="3,412.32,109.94,9.22,7.86" target="#b7">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Track as a Service</head><p>Since the corpus of TREC 2013 is more than an order of magnitude larger than the previously used TREC 2012 collection, we are provided an official search API to interact with the tweet collection. The search API returns us a ranked list of tweets retrieved by a state-of-the-art IR model (e.g., language modeling). Besides, participants can also access the text, API-supplied metadata from retrieved data and corpus-level statistics.</p><p>When applying RTR models with the search API, we only employ the two-stage pseudo relevance query expansion component while ignoring highly-customized preprocessing and document expansion as we are not allowed to access the official corpus directly. On the other hand, a filtering component was adopted to refine the tweet list to compensate for the different approach in preprocessing. Our filtering strategies are described as follows:</p><p>• Discard non-English tweets with the help of language detector ldig.</p><p>• Remove the simple retweeted tweets beginning with 'RT' based on the assumption that such tweets has no extra information beyond the original ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning to Rank Framework</head><p>Learning to rank is a data-driven approach which integrates a bag of features in the model effectively <ref type="bibr" coords="3,506.15,410.55,9.21,7.86" target="#b2">[2]</ref>. Our system adopts the same framework that Duan et al <ref type="bibr" coords="3,507.89,421.02,9.72,7.86" target="#b2">[2]</ref> proposed except that we employ different features for the learning algorithm. The basic learning to rank framework is shown in Figure <ref type="figure" coords="3,345.85,452.40,3.58,7.86">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labelers</head><p>Learning Algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vectors</head><p>Ranking System</p><formula xml:id="formula_7" coords="3,328.08,527.20,173.10,69.78">Instants with label 1 1 ( , ) M M N q t   11 ( , ) MQ q t  1 2 ( , ) MQ q t  1 1 ( , ) MQ M Q N q t   ... ... ... 11 ( , ) M q t <label>1 2 ( , ) M q t</label></formula><formula xml:id="formula_8" coords="3,328.86,494.33,230.60,146.57"> Feature Selection Vectors 11 ( , ) M S q t  1 2 ( , ) M S q t  1 1 ( , ) M M N S q t   ... 11 ( , ) MQ S q t  1 2 ( , ) MQ S q t  1 1 ( , ) MQ M Q N S q t   ... ... Scores 11 ( , )<label>qt 1 2 ( , ) qt 1 1 ( , ) N qt ... 1 ( , ) M qt 2 ( , ) M qt ( , ) M M N qt ... ...</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Learning to rank framework</head><p>In order to train an effective model, adequate training data and useful feature set are required. The candidate tweets are produced by the language model based RTR  <ref type="table" coords="4,79.02,181.51,3.58,7.86" target="#tab_1">1</ref>. Our system trains Ranking SVM <ref type="bibr" coords="4,225.61,181.51,9.73,7.86">[6]</ref> as learning to rank model. Several features have been proved effective in the prior work <ref type="bibr" coords="4,77.55,212.89,9.22,7.86" target="#b2">[2]</ref>. In our experiments, two groups of features are utilized in our learning to rank approach: semantic features and quality features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic features</head><p>Semantic features refer to the features that describe the relevance between the query and tweets, such as the Kullback-Leilbler divergence between query model and document model. RTR model with different similarity algorithms can generate different semantic features, such as Kullback-Leilbler divergence and BM25 similarity. Using different query or document model can generate different features that may reflect different aspects of query-document similarity. For example, using TopicInfo Corpus, we may get the relevance between the tweet link and user's query while using Origin Corpus, we can get the content relevance between the query and the tweet text.</p><p>In our approaches, we propose four semantic features.</p><p>• OriginTweetScore score generated by the RTR Model with the original query and Origin Corpus</p><p>• OriginTitleScore score generated by the RTR Model with the original query and TopicInfo Corpus</p><p>• IssueTweetScore issue tweet is the highest-scored tweet retrieved in the first stage of RTR Model that is described in section 2.2.1, IssueTweetScore is generated by the RTR Model with this issue tweet, as a new query and Origin Corpus.</p><p>• IssueTitleScore like IssueTweetScore, generated by the RTR Model with the issue tweet and TopicInfo Corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Features</head><p>Many of the microblogs are not informative or have very little content due to their personal and ephemeral nature. Providing effective retrieval in a microblog service will require addressing the challenge of distinguishing the highquality, informative documents from the others <ref type="bibr" coords="4,246.75,637.96,9.21,7.86" target="#b1">[1]</ref>.</p><p>Recent work has focused on finding features that indicate the quality of microblog documents. For example, tweets containing a URL indicate that more information can be referred on the linked page. Besides, a tweet usually includes a group of well-defined symbols which indicate the social interactive behaviors between different users <ref type="bibr" coords="4,246.04,700.73,9.21,7.86" target="#b9">[9]</ref>. Those symbols are described as follow:</p><p>• 'RT' symbol denotes re-tweet. A tweet with symbol RT reflects that more users are interested in the topic talked about in this tweet.</p><p>• '@' symbol followed with a user's screen name stands for mentions and replies. A tweet with more '@' means this tweet may attract more persons' attention.</p><p>• '#' symbol (i.e. hashtag) is used for organizing tweets into a particular topic. A symbol '#' marks the tweet as belonging to a particular topic.</p><p>Thus, in our system we use retweet count, mention count and hashtag count as our quality features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULT ANALYSIS</head><p>Table <ref type="table" coords="4,350.94,223.92,4.61,7.86" target="#tab_2">2</ref> shows the retrieval performance of our submitted four runs. The primary evaluation measures for this year's task are still P@30 (Precision at 30), MAP (Mean Average Precision) and R-Prec(R-Precision). Our training metric in optimizing the RTR model and learning to rank framework is P@30. PKUICST1 uses learning to rank framework, adopting RTR scores as semantic features and all the quality features. PKUICST2 and PKUICST3 are both from the top 1,000 candidate tweets from the RTR model on local DE corpus, the former employs BM25 model while the latter is based on language model. PKUICST4 is the only run that relies strictly on data obtained from the official track API.</p><p>From the experimental results, we can first see that PKUICST4 is slightly worse than local corpus runs, which points out the significance of preprocessing and document expansion. Compared with PKUICST2, PKUICST3 achieves 4.43% and 6.67% further increase in P@30 and MAP, respectively. RTR model based on language model still performs better than the one based on BM25 model, perhaps BM25 model becomes average on short text for the weakening of parameters.</p><p>However, we didn't gain any improvements with the learning-to-rank approach, which turned out to be the best one in training. This might be caused by the different corpus we used for training and testing, and further investigation is still needed for this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we present our system for TREC 2013 Microblog Track. For our local corpus, we adopt Real-time Tweet Ranking (RTR) model to rank the tweets to the given topic, and meanwhile the RTR model provides candidate tweets to learning to rank framework for the further ranking process. For data obtained from the official search API, we employed a two-stage pseudo feedback for to query expansion. Then a filtering component was adopted to refine the results retrieved by the expanded query. Many studies remain for the future work. One of the most interesting directions is to improve learning to rank framework for better results. Moreover, we are also interested in how to adapt classical IR models to short text retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,102.30,364.01,142.11,7.89"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,63.80,239.11,125.57"><head>Table 1 : Training Data Relevance Distribution</head><label>1</label><figDesc></figDesc><table coords="4,53.80,74.38,239.11,114.99"><row><cell cols="3">Category # of Tweets Crawled</cell></row><row><cell>Minimally-Relevant</cell><cell>6,286</cell><cell>6,223</cell></row><row><cell>Highly-Relevant</cell><cell>2,572</cell><cell>2,549</cell></row><row><cell>Non-Relevant</cell><cell>66,787</cell><cell>56,838</cell></row><row><cell>Total</cell><cell>73,073</cell><cell>63,061</cell></row><row><cell cols="3">model described in section 2.2.1. Our training set is</cell></row><row><cell cols="3">generated from official result set of TREC 2012 Microblog</cell></row><row><cell cols="3">Track, the relevance distribution in the result set is listed in</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,333.52,303.77,205.69,59.05"><head>Table 2 : Performance of our submitted runs</head><label>2</label><figDesc></figDesc><table coords="4,352.73,312.62,167.28,50.20"><row><cell>Run ID</cell><cell>P@30</cell><cell cols="2">MAP R-Prec</cell></row><row><cell>PKUICST1</cell><cell>0.5478</cell><cell>0.3351</cell><cell>0.3721</cell></row><row><cell>PKUICST2</cell><cell>0.5311</cell><cell>0.3268</cell><cell>0.3637</cell></row><row><cell cols="4">PKUICST3 0.5567 0.3486 0.3827</cell></row><row><cell>PKUICST4</cell><cell>0.5017</cell><cell>0.2768</cell><cell>0.3260</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Feifan Fan</rs> and <rs type="person">Chao Lv</rs> for technical assistance in this year's microblog track. The work reported in this paper was supported by the <rs type="funder">National Natural science Foundation of China</rs> Grant <rs type="grantNumber">61370116</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2kdxQKc">
					<idno type="grant-number">61370116</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,58.28,179.22,96.80,10.53" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,194.33,198.58,7.86;5,72.59,204.79,216.78,7.86;5,72.59,215.25,180.07,7.86;5,72.59,225.71,190.69,7.86;5,72.59,236.17,95.35,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,72.59,204.79,152.80,7.86">Quality models for microblog retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jaeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,243.51,204.79,45.86,7.86;5,72.59,215.25,180.07,7.86;5,72.59,225.71,160.75,7.86">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1834" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,247.63,213.08,7.86;5,72.59,258.09,210.76,7.86;5,72.59,268.55,188.00,7.86;5,72.59,279.01,214.45,7.86;5,72.59,289.47,92.13,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,157.98,258.09,125.36,7.86;5,72.59,268.55,66.70,7.86">An empirical study on learning to rank of tweets</title>
		<author>
			<persName coords=""><forename type="first">Yajuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,144.46,279.01,33.82,7.86">COLING</title>
		<editor>
			<persName><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<imprint>
			<publisher>Tsinghua University Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="295" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,300.93,198.97,7.86;5,72.59,311.39,168.47,7.86;5,72.59,321.85,188.21,7.86;5,72.59,332.31,218.84,7.86;5,72.59,342.77,86.15,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,124.95,300.93,146.61,7.86;5,72.59,311.39,48.09,7.86">Hashtag retrieval in a microblogging environment</title>
		<author>
			<persName coords=""><forename type="first">Miles</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,237.68,332.31,23.87,7.86">SIGIR</title>
		<editor>
			<persName><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stéphane</forename><surname>Marchand-Maillet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Efthimis</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="787" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,354.23,204.73,7.86;5,72.59,364.69,200.13,7.86;5,72.59,375.15,214.45,7.86;5,72.59,385.61,204.07,7.86;5,72.59,396.07,60.38,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,196.22,364.69,76.50,7.86;5,72.59,375.15,199.40,7.86">An investigation of term weighting approaches for microblog retrieval</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Ohare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Lanagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Owen</forename><surname>Phelan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,72.59,385.61,137.31,7.86">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="552" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,407.53,198.63,7.86;5,72.59,417.99,207.21,7.86;5,72.59,428.45,200.16,7.86;5,72.59,438.91,189.77,7.86;5,72.59,449.37,207.27,7.86;5,72.59,459.83,202.27,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,148.06,417.99,131.75,7.86;5,72.59,428.45,63.24,7.86">Micro-blogging as online word of mouth branding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mimi</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kate</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdur</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chowdury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,262.89,449.37,16.97,7.86;5,72.59,459.83,74.82,7.86">CHI Extended Abstracts</title>
		<editor>
			<persName><forename type="first">Dan</forename><forename type="middle">R</forename><surname>Olsen</surname><genName>Jr</genName></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Richard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ken</forename><surname>Arthur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Hinckley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Morris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Saul</forename><surname>Hudson</surname></persName>
		</editor>
		<editor>
			<persName><surname>Greenberg</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3859" to="3864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,471.29,211.12,7.86;5,72.59,481.75,198.16,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,154.40,471.29,129.30,7.86;5,72.59,481.75,69.04,7.86">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,160.27,481.75,17.65,7.86">KDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,493.21,185.38,7.86;5,72.59,503.67,193.25,7.86;5,72.59,514.13,200.39,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,72.59,503.67,193.25,7.86;5,72.59,514.13,67.96,7.86">Exploiting real-time information retrieval in the microblogosphere</title>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,159.35,514.13,21.34,7.86">JCDL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,525.59,220.33,7.86;5,72.59,536.05,196.95,7.86;5,72.59,546.51,174.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,111.45,536.05,158.08,7.86;5,72.59,546.51,21.13,7.86">Overview of the TREC-2011 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craigand</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,112.50,546.51,106.43,7.86">Proceedings of TREC 2011</title>
		<meeting>TREC 2011</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.58,557.96,185.38,7.86;5,72.59,568.43,182.51,7.86;5,72.59,578.89,196.20,7.86;5,72.59,589.35,48.33,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,72.59,568.43,182.51,7.86;5,72.59,578.89,74.50,7.86">Exploiting ranking factorization machines for microblog retrieval</title>
		<author>
			<persName coords=""><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,153.70,578.89,39.38,7.86">CIKM &apos;13</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1783" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.59,600.80,191.05,7.86;5,72.59,611.26,205.70,7.86;5,72.59,621.73,181.74,7.86;5,72.59,632.19,82.29,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,220.89,600.80,42.75,7.86;5,72.59,611.26,205.70,7.86;5,72.59,621.73,81.91,7.86">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,161.12,621.73,89.57,7.86">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.59,643.64,213.69,7.86;5,72.59,654.10,189.96,7.86;5,72.59,664.57,218.32,7.86;5,72.59,675.03,20.96,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,235.60,643.64,50.68,7.86;5,72.59,654.10,189.96,7.86;5,72.59,664.57,81.91,7.86">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,172.62,664.57,22.40,7.86">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
