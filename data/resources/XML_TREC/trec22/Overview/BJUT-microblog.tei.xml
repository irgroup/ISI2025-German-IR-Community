<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8B7F080D42F1647841DF8F19903FBAAE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Microblog retrieval</term>
					<term>Information expansion</term>
					<term>Frequency method</term>
					<term>C measure</term>
					<term>Entropy differences</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the first participation of BJUT in the TREC Micro-blog Track 2013. We perform the experiments on the 2013 TREC Microblog data using the standard retrieval model with several different query expansion methods including frequency method, C measure and Entropy differences. Also we introduce the details of our system, which consists of data preprocessing, retrieval structure, and query expansion &amp; results analysis module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>his paper describes the first participation of BJUT in the TREC Micro-blog Track 2013 <ref type="bibr" coords="1,184.36,375.02,10.37,10.79">[1]</ref>. This year's track focus on one single task: Real-time Adhoc Task. All participants should answer a query by providing a list of relevant tweets ranked in decreasing order by predicted relevance score.</p><p>The primary difference of the 2013 TREC from the 2011-2012 Micro-blog tracks lies in the data size, and the data size is more than 100 G. Unfortunately we cannot obtain the complete data collection, we can only receive the 10 thousand tweets at most for each topic through the official API. Therefore in this track, we focus on the query expansion and re-ranking methods for the received tweets through the API. We perform the experiments on the 2013 TREC Micro-blog data using the standard retrieval model with different query expansion methods including frequency method, C measure and Entropy differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CORPUS AND SYSTEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corpus</head><p>The corpus of 2013 Micro-blog track is more than an order of magnitude larger than the previously use tweets 2011 collection. Approximately, the corpus consists of 259,057,269 tweets over a two-month period: 1 February, 2013-31 March, 2013 (inclusive). We cannot obtain the whole collection through the official API and only receive 10 thousand relevant tweets for each topic through the official search API <ref type="bibr" coords="1,247.81,673.28,10.38,10.79" target="#b1">[2]</ref>. The 10 thousand tweets are encoded by json and composed of tweet id, user id, text and so on. Then we deal with these tweets of one topic with our system to get the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-Processing</head><p>There are three tasks to be done when we deal with each topic file in our system: 1) extracting the tweet id and text, 2) removing the repetitive tweets, and removing the no-English characters, and 3) the http links, removing stop-word.</p><p>Firstly, we extract the tweet id and its text of each tweet from the corpus file in one topic. If a tweet without the label "RT" in the tweets text is found, it is converted to hash format. Otherwise it is removed from data for already existing. Finally we write the content with hash format in a new file. At the same time, we delete the no-English characters, web links and stop-words of each tweet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Retrieval Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BJUT at TREC 2013 Microblog Track</head><p>Zhen YANG, Guangyuan ZHANG, Shuyong SI, Yingxu LAI, Kefeng FAN {yangzhen, laiyingxu}@bjut.edu.cn, {zhangguangyuan, sishuyong}@emails.bjut.edu.cn, kefengfan@163.com College of Computer Science, Beijing University of Technology, Beijing 100124, China T As we cannot get the whole collection, we only deal with each topic one by one through our system. When we get the final results, the tweets of different topics are independent of each other. And our system mainly contains three parts as the Fig. <ref type="figure" coords="2,61.77,133.04,4.06,10.79" target="#fig_0">1</ref> shows.  The first part is corpus and preprocessing. We already detailed introduce this part in previous chapter.  The second part is query expansion. It is the most important part. First of all, we make use of the top 100 tweets in the new corpus file of one topic to get the expansion words through our methods that will be introduced detailed in next chapter. Then we combine the original query words with the expansion words to refine search with the following formula:</p><p>The new query words = the original query words + α (the expansion words).  The final part is getting the results. The number of result tweets is 1000 by one topic, and one result contains the topic number, an unused column, a tweet id, the rank, the score and the run tag such as 'MB111 Q0 311248486941200386 1 -4.13049 BJUTFreq'. And we set the score with tf-idf model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. QUERY EXPANSION</head><p>In our system, we use three keywords extraction measures for query expansion, which are respectively based on the frequency, words' spatial distributions along the text and the theory of Shannon's entropy difference between the intrinsic and extrinsic modes which refer to the fact that relevant words significantly reflect the author's writing intentions.</p><p>For every topic we import the top 100 twitters which are pre-processed through our system into one file which we consider as an initial whole text. All of our three algorithms are based on above hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Frequency Measure</head><p>This frequency based method is set as baseline. In our system, we count every word which appears in the corpus, and build a map structure to save statistical data. We filtered the stop words in our corpus by using an English stop words list. After above all works have done, we arrange all the words in reverse chronological order and select the top-5 words as the expanded key query words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. C Measure</head><p>P. Carpenas et al. <ref type="bibr" coords="2,125.79,618.92,11.40,10.79" target="#b2">[4]</ref> suggest using the statistical analysis of spatial distributions, i.e. spectra, to detect relevant words with the same occurrence frequency. They claim that long-range correlation or clustering (self-attraction to each other) in the spatial distribution of relevant keywords is an important feature of human-written texts, in spite of random occurrences of irrelevant words. Normalized standard deviation (C) of the nearest neighbor spacing is used to characterize the spatial distribution of a particular word.     . The C score is our purpose value. C= 0 indicates that the word appears at randomly, C &gt; 0 that the word is clustered, and C&lt;0 that the word repels itself. In addition, two words with the same C value can have different clustering, but the same statistical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Entropy Differences Measure</head><p>Yang et al. <ref type="bibr" coords="2,359.43,255.74,11.40,10.78" target="#b3">[5]</ref> propose a new metric 'Entropy difference' to evaluate and rank the relevance of words in a text. The method uses the Shannon's entropy difference between the intrinsic and extrinsic mode, which refers to the fact that relevant words significantly reflect the author's writing intentions, i.e., their occurrences are modulated by the author's purposes, while the irrelevant words are distributed randomly in the text.</p><p>The idea of intrinsic-extrinsic mode is based on the general idea that relevant words are clustered, and therefore the set of distances between consecutive appearances of a word should consist of small intra-cluster distances and large inter-cluster distances.</p><p>A simple way is suggested to distinguish the intrinsic and extrinsic mode, the positions of the word occurrences in a text with frequency m are denoted by t 1 , t 2 , t 3 ,….t m . The distance between two successive occurrences of a word, can be written as d i =t i+1 -t i . The arrival time differences belongs to the intrinsic mode d A if d i ≦μ. Thus the intrinsic modes entropy of a word is defined as: </p><p>Thus the entropy differences between the intrinsic and extrinsic mode can be define as follows:</p><p>( ) ( ( )) ( ( ))</p><formula xml:id="formula_1" coords="2,363.84,581.66,181.96,11.89">q Aq Bq ED d H d H d  <label>(4</label></formula><p>) Obviously, a word with different p randomly placed in a text would have a different entropy difference between the intrinsic and extrinsic mode, the ED geo , the normalized entropy difference measure ED nor is defined as (</p><formula xml:id="formula_2" coords="2,362.88,645.56,186.73,13.49">) ( )/ | ( ) | q q q nor geo ED d ED d ED d <label>(4)</label></formula><p>When we have calculated the ED nor for every word, we can sort words by using it. A word with larger ED nor explains that it plays more important role in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>In this year's TREC Microblog Track, we submit 3 versions of runs which are shown in the Tab. 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we describe the details of our methods and system structure. In our system the first method have received the best performance, the other two methods have no better performances than the first one. We think the reasons to have those results may be because we don't have the whole corpus. If we have the integrated corpus, we believe that the other two algorithms will have better performances than the first one. Furthermore, we will research the short texts to find more effective modes to describe it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,334.38,652.04,185.38,10.79;1,314.40,675.98,235.30,10.79;1,304.56,687.98,245.14,10.79;1,304.56,699.92,245.07,10.79;1,304.56,711.86,107.15,10.79;1,333.60,381.12,187.18,270.48"><head>FIG. 1</head><label>1</label><figDesc>FIG. 1 The Framework of Microblog RetrievalAs shown in FIG.1, we performed the experiments on the 2013 TREC Micro-blog data using the standard retrieval model which consists of data preprocessing, query expansion and results analysis module [3].</figDesc><graphic coords="1,333.60,381.12,187.18,270.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,445.32,492.12,2.80,6.19;2,372.48,485.96,17.92,10.71;2,432.42,485.96,12.28,10.71;2,417.30,498.35,2.48,4.48;2,382.56,485.28,3.42,6.19;2,426.66,492.12,2.80,6.19;2,455.34,492.12,2.80,6.19;2,406.38,499.50,9.88,6.19;2,364.14,485.96,16.72,10.71;2,422.22,485.96,5.91,10.71;2,450.90,485.96,5.91,10.71;2,409.56,498.86,3.99,6.85;2,392.88,482.21,25.71,17.76;2,538.20,485.78,11.41,10.78;2,314.40,508.28,20.95,10.78;2,335.40,506.97,3.85,6.98;2,339.24,508.28,18.19,10.78;2,357.48,512.85,1.75,6.98;2,362.34,508.28,6.79,10.78;2,369.18,512.85,1.75,6.98;2,372.66,508.28,122.17,10.78;2,494.88,512.85,1.75,6.98;2,498.42,508.28,51.20,10.78;2,304.56,521.48,181.72,10.78;2,445.26,541.14,2.80,6.19;2,372.48,534.98,17.92,10.71;2,432.42,534.98,12.28,10.71;2,417.06,547.37,2.48,4.48;2,382.32,534.30,3.42,6.19;2,426.60,541.14,2.80,6.19;2,455.28,541.14,2.80,6.19;2,406.32,548.52,9.88,6.19;2,364.14,534.98,16.72,10.71;2,422.16,534.98,5.91,10.71;2,450.84,534.98,5.91,10.71;2,409.50,547.88,3.99,6.85;2,392.82,531.23,25.71,17.76"><head></head><label></label><figDesc>Let d B = {d i |d i &gt;μ} be the union set for all d i &gt;μ. We can define the extrinsic mode entropy of a word as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,45.54,85.07,243.49,64.57"><head>TABLE 1 .</head><label>1</label><figDesc>RESULTS OF OUR TEAM</figDesc><table coords="3,45.54,94.84,243.49,54.79"><row><cell>Run id</cell><cell>MAP</cell><cell>R-Prec</cell><cell>bpref</cell><cell>P@30</cell><cell>Methods</cell></row><row><cell>BJUTFreq</cell><cell>0.1088</cell><cell>0.1610</cell><cell>0.1891</cell><cell>0.2328</cell><cell>Frequency</cell></row><row><cell>BJUTCnor</cell><cell>0.0729</cell><cell>0.1137</cell><cell>0.1431</cell><cell>0.1822</cell><cell>C measure</cell></row><row><cell>BJUTEntr</cell><cell>0.0731</cell><cell>0.1174</cell><cell>0.1493</cell><cell>0.1639</cell><cell>Entropy Differences</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,84.90,312.38,205.73,10.78;3,62.88,323.72,227.69,10.78;3,62.88,335.12,4.87,10.78" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Micro-Blog Track</forename><surname>Guidelines</surname></persName>
		</author>
		<ptr target="https://github.com/lintool/twitter-tools/wiki/TREC-2013-Track-Guidelines" />
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,63.00,346.46,227.63,10.78;3,62.88,357.86,225.14,10.78" xml:id="b1">
	<monogr>
		<ptr target="https://github.com/lintool/twitter-tools/wiki/TREC-2013-API-Specifications" />
		<title level="m" coord="3,63.00,346.46,121.86,10.78">TREC 2013 API Specifications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,63.00,380.60,227.56,10.78;3,63.00,392.00,227.55,10.78;3,63.00,403.40,209.17,10.78" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,268.39,380.60,22.17,10.78;3,63.00,392.00,227.55,10.78;3,63.00,403.40,75.86,10.78">Level statistics of words: Finding keywords in literary texts and symbolic sequences</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Carpena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bernaola-Galván</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hackenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,145.50,403.40,71.87,10.78">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,63.00,414.74,227.57,10.78;3,63.00,426.14,227.56,10.78;3,63.00,437.54,227.55,10.78;3,63.00,448.88,69.42,10.78" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,63.00,426.14,227.56,10.78;3,63.00,437.54,124.47,10.78">Keyword extraction by entropy difference between the intrinsic and extrinsic mode</title>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian-Jun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ke-Feng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying-Xu</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,200.76,437.54,43.03,10.78">Physics A</title>
		<imprint>
			<biblScope unit="volume">392</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="4523" to="4531" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
