<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.28,110.91,401.45,15.12">Overview of the TREC 2013 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,112.58,149.38,86.97,10.48"><forename type="first">Adriel</forename><surname>Dean-Hall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.51,149.38,105.90,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.42,149.38,63.95,10.48"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,141.24,183.12,68.18,10.48;1,157.04,197.07,36.58,10.48"><forename type="first">Paul</forename><forename type="middle">Thomas</forename><surname>Csiro</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nicole Simone University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.28,110.91,401.45,15.12">Overview of the TREC 2013 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B62C5810181461ED6D9333794A58014</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.Summary for Previous Participants</head><p>For participants familiar with the 2012 Contextual Suggestion Track we have provided a list of the main changes to this year's track:</p><p>• Contexts no longer include a temporal component (day of week, time of day, and season), contexts consist of only a location. • Users were recruited from a crowdsourcing service (Mechanical Turk) as well as from the University of Waterloo student body. • Suggestion attractiveness judgements are given on a 5-point, rather than a 3-point, scale.</p><p>• Submissions based off of the ClueWeb12 corpus were allowed in addition to submissions based off of the open web. • A modified Time-Biased Gain (TBG) metric was used in addition to P@5 and MRR. This metric is described in section 4.3. • The option to submit solely based on context or solely based or user profiles was removed.</p><p>• The file format used for profiles, contexts, and suggestions was switched from XML to CSV and JSON.</p><p>If you are already familiar with this track you can skip to section 5 which provides an overview of the approaches used by participants and section 6 which contains the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Task Description</head><p>The contextual suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests. For example, imagine an information retrieval researcher with a November evening to spend in Gaithersburg, Maryland. A contextual suggestion system might recommend a beer at the Dogfish Head Alehouse, dinner at the Flaming Pit, or even a trip into Washington on the metro to see the National Mall. The primary goal of this track is to develop evaluation methodologies for such systems.</p><p>This track ran for the second time as part of TREC 2013 after a positive response at TREC 2012. This year participants were again given, as input, a set of user profiles and set of geographical contexts. The task was to take these profiles and contexts and to produce a list of up to 50 ranked suggestions for each profile-context pair. Participants could choose to gather suggestions from either the open web or the ClueWeb12 dataset.</p><p>Each profile corresponds to a single user and indicates that user's preference with respect to each sample suggestion. For example, if one sample suggestion is a beer at the Dogfish Head Alehouse, the profile might indicate a negative preference to that suggestion. Each suggestion includes a title, short description, and an associated URL. Each context corresponds to a particular location at the granularity of a city. For example, a context might be Gaithersburg, Maryland.</p><p>As with last year each groups was allowed to submit up to two runs. A total of 19 groups submitting 34 runs participated in the track this year, an increase by 7 runs from last year. This included two baseline runs submitted by the track organizers which are described later in this report in section 3.3. 7 of these runs comprised suggestions from the ClueWeb12 dataset, the other 27 runs comprised suggestions from the open web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Detailed Task Description</head><p>Profiles and contexts were distributed to participants as CSV and JSON files. For this track we generated 562 profiles and 50 contexts, below we describe how these were generated. An experimental run consists of a single suggestion (CSV) file generated automatically from the profile and context files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Profiles</head><p>Profiles indicate a user's preferences to a list of 50 example suggestions within Philadelphia, PA. These profiles are built by conducted a survey advertised to both University of Waterloo students and Mechanical Turk users.</p><p>Profiles are split into two files: examples2013.csv and profiles2013.csv 1 . examples2013.csv contains a list of 50 suggestions which each consist of an id, a title, a description, and a url. Below are two suggestions from the file: The second file contains a list of ratings for each suggestion in examples2013.csv given by each user, below are a few example lines from profiles2013.csv: The first line means that user id 534 gave example suggestion number 51 a description rating of 1 and a website rating of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Generating Example Suggestions</head><p>First we need to generate example suggestion which will rated by users in a survey. A suggestion consists of a title, short description, and a website URL. 50 example suggestions were taken from attractions submitted by participants in the TREC 2012 version of this track. These example suggestions were all from the Philadelphia, PA area (one of the contexts used in 2012). Example suggestions were chosen if they had a high quality description and a website that was available; example suggestions were also chosen so that there was diversity in the types of attractions in our set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Gathering Attraction Preferences</head><p>Profiles distributed to participants indicated users' preference towards the example suggestions. In order to form the profile users, recruited from Mechanical Turk and the University of Waterloo, where asked to complete an online survey. In the survey sample suggestions were presented to users in a random order. Users were asked to give two 5-point ratings for each attraction, one for how interesting the attraction seemed to the user based on its description and one for how interesting the attraction seemed to the user based on its website. The survey interface, as presented to University of Waterloo users, can be seen in figure <ref type="figure" coords="3,533.39,553.14,3.87,8.74" target="#fig_0">1</ref>. Mechanical Turk users saw a similar interface but it also included some Mechanical Turk interface elements that were not controlled by us.</p><p>In total 62 Waterloo students and 500 Mechanical Turk users responded to the survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contexts</head><p>Contexts describe which city a user is currently located in. There were 50 cities chosen randomly from the list of primary cities in metropolitan areas in the United States (which are not part of a larger metropolitan area) excluding Philadelphia, PA (the seed city). The list of metropolitan areas was taken from Wikipedia<ref type="foot" coords="3,533.90,675.56,3.97,6.12" target="#foot_0">2</ref> .</p><p>Contexts are distributed to participants in the file contexts2013.csv (as with the profile files, a JSON file with the contexts is also distributed).</p><p>. . . 7 1 , Monroe , LA, 3 2 . 8 1 5 1 3 , -9 2 . 2 0 5 6 9 7 2 ,Tampa , FL, 2 7 . 9 4 7 5 2 , -8 2 . 4 5 8 4 3 . . . 7 8 , Lewiston , ID , 4 6 . 4 1 6 5 5 , -1 1 7 . 0 1 7 6 6 7 9 , Lima ,OH, 4 0 . 7 4 2 5 5 , -8 4 . 1 0 5 2 3 . . . Listing 2: An excerpt from contexts2013.csv.</p><p>Here the first line means that context number 71 represents Monroe (city), LA (state) with a latitude of 32.81513 and a longitude of -92.20569. For contexts the latitude and longitude are provided as a convenience and are synonymous with the city and are not meant to represent the exact position of the user. Contexts represent locations at the granularity of a city-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Collections</head><p>Participants were able to gather suggestions from either the open web, ClueWeb12<ref type="foot" coords="4,442.61,307.60,3.97,6.12" target="#foot_1">3</ref> , ClueWeb12 B13, or ClueWeb12 CS. ClueWeb12 and ClueWeb12 B13 are datasets prepared by Jamie Callan's research group at CMU. ClueWeb12 CS was prepared for the track by the track organizers.</p><p>The ClueWeb12 CS subcollection was created by issuing a variety of queries for each context location against a commercial search entire. Returned results that had URLs which matched documents in ClueWeb12 were grouped by context and included in the subcollection. URLs were normalized before they were matched, for example forward-slashes were removed from the end of URLs. In total the subcollection contains 30 144 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Submitted Suggestions</head><p>Each submitted run consists of up to 50 ranked suggestions for each profile-context pair. Similarly to the example suggestions, profiles consist of a title, description, and URL that correspond to an attraction. The URL can be substituted with a ClueWeb12 DocID. Suggestions also contain a group id, run id, profile id, context id, and rank.</p><p>In order to generate suggestions participants were allowed to use whatever resources they wished to use, for example review websites such as Yelp. The goal was that each suggestion should be tailored to the profile and located within the context that was being targeted. Ideally, the description of the suggestion would be tailored to reflect the preferences of the user.</p><p>Here are two of the suggestions we received: </p><formula xml:id="formula_0" coords="4,85.81,589.62,74.04,8.77">• Group ID</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Judging</head><p>Judging was split up into two tasks. Suggestions were judged with respect to their profile relevance by users and with respect to the contextual relevance by accessors at NIST as well as users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Profile Relevance</head><p>In order to judge the relevance of suggestions with respect to profiles a second survey was conducted, which was similar to the first one. Some users were invited back to give ratings for the attraction descriptions and websites of the top 5 ranked suggestions for each run for their profile and one or two randomly chosen contexts.</p><p>The judgements given were one of:</p><p>-2 Could not load 0 Strongly uninterested 1 Uninterested 2 Neutral 3 Interested 4 Strongly interested Some users were not invited back to give judgements on suggestions. While completing both surveys users were asked whether suggestions were geographically appropriate and the amount of time user took to make judgements was also recorded. In the initial survey used to generate profiles 5 suggestions not in Philadelphia, PA were included with the 50 suggestions in Philadelphia, PA. A score was generated for each user after the first survey that was based on how long users took to make judgements and how many geographical judgements users got correct. If users took too little time in making judgements or got too many geographical judgements incorrect they were not invited back.</p><p>Approximately the top 80% of users were invited back. Some users did not respond to our invitation to the second survey. In total 223 context-profile pairs were judged by users.</p><p>Judgements of relevance of suggestions with respect to profiles are distributed in desc-doc.qrels.</p><p>. Here the first line means that the user was neither interested nor uninterested (neutral, 2) in the attraction based on the description provided by run UDInfoCS1 for profile 534, context 71, and the website http: //www.yelp.com/biz/cotton-monroe but the user was interested (3) in the attraction based on the content of the website. The last two numbers mean that the user took 31 sec. to rate the description and 13 sec. to rate the website. A -1 means that no timing data is available. This timing data is not used as part of the scoring calculations for runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geographical Relevance</head><p>In order to judge the geographical relevance of suggestions users were asked, during the survey, whether the attraction was in the city it was submitted for or not. Additionally accessors at NIST where also asked to make the same judgement for attractions. The list of context-profile pairs judged by users and those judged by NIST were not the same list however there was an overlap of approximately nine thousand judgements. Of the documents judged for context by both NIST accessors and users there was an agreement on judgements of 77% if judgements of "marginally appropriate" and "appropriate" are considered the same.</p><p>-2 Could not load 0 Not geographically appropriate 1 Marginally geographically appropriate 2 Geographically appropriate Note that only NIST accessors explicitly made judgements of 1, users made judgements of either -2, 0, or 2, however some of the user judgements are reported as 1 when users didn't agree with each other on whether an attraction was geographically appropriate. For purposes of calculating final metric scores if both NIST accessors and users disagree on whether a suggestion is contextually appropriate the value the NIST accessors gave is taken.</p><p>Judgements of geographical appropriateness are distributed in geo-nist.qrels and geo-user.qrels for NIST assessments and user assessments respectively.</p><p>. . . 71 h t t p : / /www. y e l p . com/ b i z / w a t e r f r o n t -g r i l l -monroe 2 71 h t t p : / / y e l p . com/ b i z / l a -p e r l a -3-mexican-r e s t a u r a n t -and-g r o c e r y -s t o r e -johnson-c i t y 0 . . . Here the first line means that for context 71 the website http://www.yelp.com/biz/waterfront-grill-monroe is geographically appropriate (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Runs</head><p>Two baseline runs were submitted. BaselineA takes the top 50 attractions returned by the Google Places API when provided with the city in the context. For the description, a Google Places provided description, review, or a blurb from the meta-description tag on the website is used. BaselineB uses the same strategy except that suggestions not in ClueWeb12 were filtered out and the remaining suggestion URLs were mapped to ClueWeb12 document ids. Exact URL matches were not needed, for filtering suggestion URLs had forwardslashes removed from the end of them before being matched to ClueWeb12 documents. Personalization was not attempted for either baseline run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Measures</head><p>Three measures are used to rank runs. Our main measure, Precision at Rank 5 (P@5), is supplemented by Mean Reciprocal Rank (MRR) and a modified version of Time-Biased Gain (TBG) <ref type="bibr" coords="7,428.65,173.83,14.37,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">P@5</head><p>An attraction is considered relevant for P@5 if it has a geographical relevance of 1 or 2 and if the user reported that both the description and document were found to be interesting (3) or strongly interesting (4).</p><p>A P@5 score for a particular topic (a profile-context pair) is determined by how many of the top 5 ranked attractions are relevant, divided by 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MRR</head><p>For MRR, an attraction is considered relevant using the same criteria used for P@5. A MRR score is calculated as 1 k , where k is the rank of the first relevant attraction found. If there are no relevant attractions in the first 5 attractions in the ranked list a score of 0 is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TBG</head><p>In an effort to develop a metric better suited to evaluating this task the organizers of this track developed a metric based on TBG metric introduced by Smucker and Clarke <ref type="bibr" coords="7,340.02,411.92,12.46,8.74" target="#b1">[2]</ref>. The modified version of TBG is calculated by the equation described by Dean-Hall, et al. <ref type="bibr" coords="7,272.23,423.88,10.20,8.74" target="#b0">[1]</ref>:</p><formula xml:id="formula_1" coords="7,232.19,449.37,147.12,30.55">5 k=1 D(T (k))A(k)(1 -Θ) k-1 j=1 Z(j)</formula><p>• D is a decay function.</p><p>• T(k) is how long it took the user to reach rank k, calculated using the following two rules:</p><p>-The user reads every description which takes time T desc .</p><p>-If the description judgement is 2 or above then the user reads the document which takes time T doc .</p><p>• A(k) is 1 if the user gives a judgement of 2 or above to the description and 3 or above to the document, otherwise it is 0. • Z(k) is 1 if the user gives a judgement of 1 or below to either the description or the document, otherwise it is 0.</p><p>Note that, for this metric, the user always gives a rating of 0 to the document if the document has a geographical rating of 0. The four parameters for this metric are taken from Dean-Hall et al. <ref type="bibr" coords="7,485.64,614.52,9.96,8.74" target="#b0">[1]</ref>: Θ = 0.5, T desc = 7.45s, and T doc = 8.49s, and the half-life for the decay function H = 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participant Approaches</head><p>There were 34 runs submitted by 19 groups, 7 of these were ClueWeb12 runs and 27 were open web runs. 14 groups provided descriptions of their runs which are included below. One of the groups consisted of the two baseline runs which are described above in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PITT at TREC 2013 Contextual Suggestion Track</head><p>Rundids: ming 1, ming 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors: Ming Jiang and Daqing He</head><p>This system made used of data from Yelp for creating candidate suggestion and supplementing user profiles.</p><p>The system used vector space models to compute the similarity between candidates and examples and linear regression models to combine multiple attributes of candidate profiles into the calculations. The system was trained and tested using 5-fold cross validation on 2012 track data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">An Opinion-aware Approach to Contextual Suggestion</head><p>Runids: UDInfoCS1, UDInfoCS2</p><p>Authors: Peilin Yang and Hui Fang</p><p>This system set out to evaluate the effectiveness of (1) an opinion-based method to model user profiles and rank candidate suggestions; and (2) a template-based summarization method that leverages the information from multiple resources to generate the description of candidate suggestion.</p><p>Given a user and context pair, this system gathered candidate suggestions from Yelp and then ranked the candidate suggestions based on their similarity with the user profile. This system estimated the user profile based on the reviews of the candidate suggestions in contrast to using the description or category information of the suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Boosting Venue Page Positions for Contextual Retrieval InfoSense at TREC 2013 Contextual Suggestion</head><p>Runids: BOW V17, BOW V18</p><p>Authors: Jiyun Luo and Grace Hui Yang</p><p>This system makes ClueWeb12 suggestions and uses two main approaches. The first approach extracts venue names from WikiTravel and then formulates queries to search the collection. The similarity between venue name and anchor text for pages is used to locate the most relevant URL for the venue, as opposed to nonrelevant documents, for example "yellow-page"-like list pages. The second approach divides venues into a two-level venue categorization, for example "landmark" or "amusement park", then the system creates a language model for each category. The category-specific language models are used to perform the retrieval for each individual category mentioned in a user's profile.</p><p>Suggestions are personalized by making distinctions between "major", "minor", and "negative" personal interests. The system creates ranked suggestions lists by merging venues from multiple categories while favouring venues that the user has a "major" personal interest in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Exploiting Location-based Social Networks for Contextual Recommendations</head><p>Runids: uogTrCFX, uogTrCFP Authors: M-Dyaa Albakour, Nut Limsopatham, Craig Macdonald, and Iadh Ounis</p><p>This system uses Location-based Social Networks (LSNs), such as FourSquare and Facebook Places, to gather data on both venues and users. First, the similarity between the venue descriptions and a textual representation of the user's interests is calculated. Venue descriptions are extracted from their web pages or their profiles on LSNs. In the uogTrCFP run, on top of this similarity, the system focuses on using the social aspect in the ranking by incorporating an estimation of the popularity of the venue based on the number of previous interactions of the users on LSNs. The second run, uogTrCFX, also uses this popularity but focuses on using a personalisation model based on the XQuAD diversity framework, which allows the system to cover multiple categories of interest to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">DUTH Team</head><p>Runids: DuTH A and DuTH B Authors: George Drosatos, Giorgos Stamatelatos, Avi Arampatzis and Pavlos S. Efraimidis</p><p>This system collects attractions from three commercial search engines, Google Places, Foursquare, and Yelp. The attractions returned by these services are enhanced by adding snippets from the Google and Bing search engines using crowdsourcing techniques. The first run submits each candidate place as a query in an index of examples and scores it based on the top-k users' preferences. The second run is based on Rocchio's algorithm and uses the examples per profile to generate a personal query which is then submitted to the index of attractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">CWI team</head><p>Runid: IBCosTop1</p><p>Authors: Thaer Samar, Arjen de Vries, Alejandro Bellogin, Jimmy Lin, and Alan Said</p><p>This system uses the full ClueWeb12 dataset as a source. For each context a sub-collection of the most relevant documents is formed. Once this is done the system then ranks these subcollections based on the profiles. User profiles are formed by taking user preferences for the sample attractions and descriptions for those attractions. Personalized rankings are generated by computing the cosine similarity between the document and these user profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">University of Lugano at the TREC 2013 Contextual Suggestion Track</head><p>Runids: complexScore, simpleScore Authors: Andrey Rikitianskiy, Morgan Harvey, and Fabio Crestani</p><p>This system uses the Google Places API to obtain an initial list of suggestions. These were grouped into 27 different types. Description snippets were generated using the Yandex Rich Content API and Google Custom Search APIs. For each user a positive and negative model is generated, these models were based on the example descriptions which the system then expanded. These models were then used to rank suggestions lists for each profile-context pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">A Simple Context Dependent Suggestion System</head><p>Runid: isirun Authors: Dwaipayan Roy, Ayan Bandyopadhyay, and Mandar Mitra</p><p>This system groups attractions into categories and determines which attraction categories users prefer by using the attraction ratings given in the user profiles. Suggestions are fetched using the Google Places API, passing the context location as a parameter. These suggestions are then ranked based on the distance between the suggestion and the context location and the level of preference the user has for the suggestion category. Suggestion descriptions are result snippets returned from Google when the suggestion named is passed as a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">IRIT Team</head><p>Runids: IRIT.ClueWeb, IRIT.OpenWeb Authors: Guillaume Cabanac, et. al.</p><p>The "IRIT.OpenWeb" run ranks suggestions based on how close it is to the context and by considering the categories matched between the user and the suggestions. The system assigns users one or more categories from WordNet and Google Places based on which suggestions users liked. Attractions are retrieved for the 50 contexts from Google Places and also tagged with the same categories. Descriptions for suggestions were fetched using Yahoo! BOSS Placefinder and Bing.</p><p>The "IRIT.ClueWeb" run is made up of documents retrieved using Terrier with queries composed the users' categories. Documents are then ranked according to their retrieval score and similarity to the profile. Users are assigned to categories as in the "IRIR.OpenWeb" run and descriptions are also generated in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Diversifying Contextual Suggestion Search Based on User Profiles</head><p>Runids: udel run D, udel run SD Authors: Karankumar Sabhnani and Ben Carterette</p><p>This system starts by analysing user profiles and generating bags of keywords depicting the user interests.</p><p>Then, given a context, they query the Google Places API with each bag, retrieving lists containing places in that context which fit in the specified genre. Their first run creates suggestions for each profile-context combination by iterating through the lists in a round-robin fashion, selecting one place at a time to create a list of 50. Their second run sorts the retrieved lists by average user ratings before iterating round-robin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">A Nearest-Neighbor Approach to Contextual Suggestion</head><p>Runids: uncsils base, uncsils param Authors: Sandeep Avula, John O'Connor, and Jaime Arguello</p><p>This system gathers a candidate set of venues from the Yelp API. In the first run, uncsils base, for each context-profile pair, the candidate venues are scored using the weighted average rating associated with the venues in the profile. For this calculation, each profile venue was given a weight based on the cosine similarity between the candidate venue and profile venue. The goal with this approach is to score each candidate venue based on the rating associated with the most similar venues in the profile. The second run, uncsils param, boosted the contribution from the profile venue with the greatest similarity with the candidate venue and rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.12">University of Amsterdam Team</head><p>Runids: UAmsTF30WU</p><p>This systems extracts suggestions for sightseeing, shopping, eating, and drinking from Wikitravel pages dedicated to US cities. Descriptions from positive examples in the user profiles are used as queries to rank suggestions. The system then merges the per-query rankings of positive examples into a single result list. These ranked suggestions are then filtered based on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.13">University of Indonesia Team</head><p>Runids: csui01, csui02</p><p>This system gathers a list of candidate venues by issuing queries against Yelp and Foursquare that combined context and preference information. Venues were then ranked giving a high amount of weight to the rating and number of people who rated. Reviews/comments which both were liked by users and gave the venue a high rating were used as suggestion descriptions. The first run considered only the venue rating whereas the second run also employed diversity based on venue categories.</p><p>5.14 National University of Ireland, Galway Team </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" coords="11,97.59,378.76,4.98,8.74" target="#tab_4">1</ref> lists the scores for all open web runs for all three metrics and table 2 lists the scores for all ClueWeb12 runs. Both of these tables are sorted by their P@5 rankings (our main metric). We do not compare open web and ClueWeb12 runs against each other as part of this track. Figure <ref type="figure" coords="11,378.51,402.67,4.98,8.74" target="#fig_3">2</ref> compares the three metrics against each other for all runs, note that there is a high amount of agreement between the three metrics. Also note that the best two performing runs are ranked the same regardless of the metric used.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,205.95,275.68,200.11,8.74;3,73.91,70.87,230.42,193.29"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Screenshots of survey seen by users.</figDesc><graphic coords="3,73.91,70.87,230.42,193.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,71.53,540.73,393.36,8.74;6,71.53,552.68,333.59,8.74;6,73.27,564.64,13.11,8.74;6,215.87,579.58,180.27,8.74"><head></head><label></label><figDesc>78 h t t p : / /www. r e d l i o n l e w i s t o n . com/ m e r i w e t h e r s a m e r i c a n g r i l l . htm 2 78 h t t p : / /www. r e d l i o n l e w i s t o n . com/ m j b a r l e y h o p p e r s . htm 2 . . . Listing 4: An excerpt from geo-nist.qrels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,96.29,347.78,8.65,8.54;13,126.73,347.78,8.65,8.54;13,157.12,347.78,8.65,8.54;13,187.44,347.78,8.65,8.54;13,217.75,347.78,8.65,8.54;13,248.21,347.78,8.65,8.54;13,278.51,347.78,8.65,8.54;13,185.79,354.55,12.19,8.54;13,90.14,343.54,8.65,8.54;13,90.26,323.29,8.65,8.54;13,90.38,303.05,8.65,8.54;13,90.49,282.76,8.65,8.54;13,90.18,262.55,8.65,8.54;13,90.30,242.30,8.65,8.54;13,90.20,222.05,8.65,8.54;13,80.89,281.38,8.54,11.28;13,131.44,375.13,113.99,7.86;13,333.63,347.78,8.65,8.54;13,364.08,347.78,8.65,8.54;13,394.47,347.78,8.65,8.54;13,424.79,347.78,8.65,8.54;13,455.10,347.78,8.65,8.54;13,485.55,347.78,8.65,8.54;13,515.86,347.78,8.65,8.54;13,423.13,354.55,12.19,8.54;13,327.49,343.54,8.65,8.54;13,327.63,326.19,8.65,8.54;13,327.67,308.83,8.65,8.54;13,327.56,291.48,8.65,8.54;13,327.43,274.12,8.65,8.54;13,327.60,256.77,8.65,8.54;13,327.47,239.41,8.65,8.54;13,327.59,222.05,8.65,8.54;13,318.26,280.79,8.54,12.26;13,367.73,375.13,116.10,7.86;13,96.29,522.47,8.65,8.54;13,122.39,522.47,8.65,8.54;13,148.45,522.47,8.65,8.54;13,174.42,522.47,8.65,8.54;13,200.39,522.47,8.65,8.54;13,226.51,522.47,8.65,8.54;13,252.48,522.47,8.65,8.54;13,278.57,522.47,8.65,8.54;13,185.59,529.24,12.26,8.54;13,90.14,518.23,8.65,8.54;13,90.26,497.98,8.65,8.54;13,90.38,477.74,8.65,8.54;13,90.49,457.45,8.65,8.54;13,90.18,437.24,8.65,8.54;13,90.30,416.99,8.65,8.54;13,90.20,396.74,8.65,8.54;13,80.89,456.07,8.54,11.28;13,129.71,549.82,117.44,7.86"><head></head><label></label><figDesc>MRR vs TBG τ = 0.8632</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,185.43,570.76,241.14,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparisons between P@5, MRR, and TBG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,110.72,589.65,430.42,116.33"><head></head><label></label><figDesc>Barleyhoppers is an american (traditional) restaurant. HERE ARE THE DE-SCRIPTIONS FROM ITS WEB SITE:Lewiston's red lion hotel is the home of m.j. barleyhopper's, the area's largest microbrewery. HERE ARE REVIEWS FROM OTHER PEOPLE:I've only been here a few times but i'll be back many more. This little brewpub has the best beers i know of that you can find in the valley. THIS PLACE IS SIMILAR TO OTHER PLACE(S) YOU LIKED, i.e. Fergie's Pub. URL/Doc ID http://www.redlionlewiston.com/mj_barleyhoppers.htm</figDesc><table coords="4,110.72,589.65,100.02,68.51"><row><cell>•</cell><cell>Group ID udel fang</cell></row><row><cell></cell><cell>Run ID UDInfoCS1</cell></row><row><cell></cell><cell>Profile ID 534</cell></row><row><cell></cell><cell>Context ID 78</cell></row><row><cell></cell><cell>Rank 1</cell></row><row><cell></cell><cell>Title M J Barleyhoppers</cell></row><row><cell></cell><cell>Description M J</cell></row><row><cell></cell><cell>udel fang</cell></row><row><cell></cell><cell>Run ID UDInfoCS1</cell></row><row><cell></cell><cell>Profile ID 534</cell></row><row><cell></cell><cell>Context ID 71</cell></row><row><cell></cell><cell>Rank 1</cell></row><row><cell></cell><cell>Title Waterfront Grill</cell></row></table><note coords="4,110.72,661.36,430.42,8.77;4,110.72,673.34,430.41,8.74;4,110.72,685.30,430.42,8.74;4,110.72,697.25,242.00,8.74;5,110.72,73.99,318.71,9.05"><p><p>Description Waterfront Grill is a seafood restaurant. HERE ARE REVIEWS FROM OTHER PEOPLE:Great views of the water while dining on great real food. There is a perfect view of the bayou, and on a nice day, you can sit on the patio. As with any place, there are good days and bad days but overall, they possess a wonderful track record.</p>URL/Doc ID http://www.yelp.com/biz/waterfront-grill-monroe</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,70.87,222.35,470.27,98.43"><head></head><label></label><figDesc>Runids: CIRG IRDISCOA, CIRG IRDISCOBThis systems finds candidate places from Google Places and WikiTravel. For each candidate place a corresponding description is extracted from the Google Places API or the Bing API. Related Wikipedia articles are also found for both example suggestions and candidate places based on the place's description. The system calculates an intersection of these Wikipedia articles between example suggestions and candidate places, these Wikipedia articles are then used to extract Wikipedia categories. A score based on the similarity of categories between candidate places and examples suggestions is calculated by the system, places with the highest score are then returned.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,79.47,114.25,453.05,541.49"><head>Table 1 :</head><label>1</label><figDesc>P@5, TBG, and MRR rankings for all open web runs.</figDesc><table coords="12,79.47,114.25,453.05,541.49"><row><cell></cell><cell cols="2">P@5 Rank P@5 Score</cell><cell>TBG Rank</cell><cell cols="3">TBG Score MRR Rank MRR Score</cell></row><row><cell>UDInfoCS1</cell><cell>1</cell><cell>0.5094</cell><cell>1 (-)</cell><cell>2.4474</cell><cell>1 (-)</cell><cell>0.6320</cell></row><row><cell>UDInfoCS2</cell><cell>2</cell><cell>0.4969</cell><cell>2 (-)</cell><cell>2.4310</cell><cell>2 (-)</cell><cell>0.6300</cell></row><row><cell>simpleScore</cell><cell>3</cell><cell>0.4332</cell><cell>4 (Down 1)</cell><cell>1.8374</cell><cell>4 (Down 1)</cell><cell>0.5871</cell></row><row><cell>complexScore</cell><cell>4</cell><cell>0.4152</cell><cell>5 (Down 1)</cell><cell>1.8226</cell><cell>6 (Down 2)</cell><cell>0.5777</cell></row><row><cell>DuTH B</cell><cell>5</cell><cell>0.4090</cell><cell>3 (Up 2)</cell><cell>1.8508</cell><cell>3 (Up 2)</cell><cell>0.5955</cell></row><row><cell>1</cell><cell>6</cell><cell>0.3857</cell><cell>8 (Down 2)</cell><cell>1.5329</cell><cell>7 (Down 1)</cell><cell>0.5588</cell></row><row><cell>2</cell><cell>7</cell><cell>0.3731</cell><cell>7 (-)</cell><cell>1.5843</cell><cell>5 (Up 2)</cell><cell>0.5785</cell></row><row><cell>udel run D</cell><cell>8</cell><cell>0.3659</cell><cell>9 (Down 1)</cell><cell>1.5243</cell><cell>8 (-)</cell><cell>0.5544</cell></row><row><cell>isirun</cell><cell>9</cell><cell>0.3650</cell><cell>6 (Up 3)</cell><cell>1.6278</cell><cell>9 (-)</cell><cell>0.5165</cell></row><row><cell>udel run SD</cell><cell>10</cell><cell>0.3354</cell><cell>16 (Down 6)</cell><cell>1.2882</cell><cell>10 (-)</cell><cell>0.5061</cell></row><row><cell>york13cr2</cell><cell>11</cell><cell>0.3309</cell><cell>12 (Down 1)</cell><cell>1.3483</cell><cell>15 (Down 4)</cell><cell>0.4637</cell></row><row><cell>DuTH A</cell><cell>12</cell><cell>0.3283</cell><cell>14 (Down 2)</cell><cell>1.3109</cell><cell>12 (-)</cell><cell>0.4836</cell></row><row><cell>york13cr1</cell><cell>13</cell><cell>0.3274</cell><cell>15 (Down 2)</cell><cell>1.2970</cell><cell>14 (Down 1)</cell><cell>0.4743</cell></row><row><cell>UAmsTF30WU</cell><cell>14</cell><cell>0.3121</cell><cell>17 (Down 3)</cell><cell>1.1905</cell><cell>13 (Up 1)</cell><cell>0.4803</cell></row><row><cell>IRIT.OpenWeb</cell><cell>15</cell><cell>0.3112</cell><cell>10 (Up 5)</cell><cell>1.4638</cell><cell>11 (Up 4)</cell><cell>0.4915</cell></row><row><cell>CIRG IRDISCOA</cell><cell>16</cell><cell>0.3013</cell><cell>18 (Down 2)</cell><cell>1.1681</cell><cell>16 (-)</cell><cell>0.4567</cell></row><row><cell>CIRG IRDISCOB</cell><cell>17</cell><cell>0.2906</cell><cell>20 (Down 3)</cell><cell>1.1183</cell><cell>19 (Down 2)</cell><cell>0.4212</cell></row><row><cell>uncsils param</cell><cell>18</cell><cell>0.2780</cell><cell>13 (Up 5)</cell><cell>1.3115</cell><cell>18 (-)</cell><cell>0.4271</cell></row><row><cell>uogTrCFP</cell><cell>19</cell><cell>0.2753</cell><cell>11 (Up 8)</cell><cell>1.3568</cell><cell>17 (Up 2)</cell><cell>0.4327</cell></row><row><cell>ming 1</cell><cell>20</cell><cell>0.2601</cell><cell>22 (Down 2)</cell><cell>1.0495</cell><cell>22 (Down 2)</cell><cell>0.3816</cell></row><row><cell>uncsils base</cell><cell>21</cell><cell>0.2565</cell><cell>19 (Up 2)</cell><cell>1.1374</cell><cell>20 (Up 1)</cell><cell>0.4136</cell></row><row><cell>ming 2</cell><cell>22</cell><cell>0.2493</cell><cell>23 (Down 1)</cell><cell>0.9673</cell><cell>23 (Down 1)</cell><cell>0.3473</cell></row><row><cell>uogTrCFX</cell><cell>23</cell><cell>0.2332</cell><cell>21 (Up 2)</cell><cell>1.0894</cell><cell>21 (Up 2)</cell><cell>0.4022</cell></row><row><cell>run01</cell><cell>24</cell><cell>0.1650</cell><cell>24 (-)</cell><cell>0.7359</cell><cell>24 (-)</cell><cell>0.2994</cell></row><row><cell>baselineA</cell><cell>25</cell><cell>0.1372</cell><cell>25 (-)</cell><cell>0.5234</cell><cell>25 (-)</cell><cell>0.2316</cell></row><row><cell>csui02</cell><cell>26</cell><cell>0.0565</cell><cell>26 (-)</cell><cell>0.1785</cell><cell>26 (-)</cell><cell>0.1200</cell></row><row><cell>csui01</cell><cell>27</cell><cell>0.0565</cell><cell>27 (-)</cell><cell>0.1765</cell><cell>27 (-)</cell><cell>0.1016</cell></row><row><cell>Run</cell><cell cols="6">P@5 Rank P@5 Score TBG Rank TBG Score MRR Rank MRR Score</cell></row><row><cell>baselineB</cell><cell>1</cell><cell>0.1417</cell><cell>1 (-)</cell><cell>0.4797</cell><cell>1 (-)</cell><cell>0.2452</cell></row><row><cell>BOW V17</cell><cell>2</cell><cell>0.1022</cell><cell>3 (Down 1)</cell><cell>0.3389</cell><cell>3 (Down 1)</cell><cell>0.1877</cell></row><row><cell>BOW V18</cell><cell>3</cell><cell>0.1004</cell><cell>2 (Up 1)</cell><cell>0.3514</cell><cell>2 (Up 1)</cell><cell>0.1971</cell></row><row><cell>IRIT.ClueWeb</cell><cell>4</cell><cell>0.0798</cell><cell>4 (-)</cell><cell>0.3279</cell><cell>4 (-)</cell><cell>0.1346</cell></row><row><cell>RUN1</cell><cell>5</cell><cell>0.0628</cell><cell>5 (-)</cell><cell>0.2069</cell><cell>5 (-)</cell><cell>0.1265</cell></row><row><cell>RUN2</cell><cell>6</cell><cell>0.0565</cell><cell>6 (-)</cell><cell>0.2020</cell><cell>6 (-)</cell><cell>0.1223</cell></row><row><cell>IBCosTop1</cell><cell>7</cell><cell>0.0448</cell><cell>7 (-)</cell><cell>0.1029</cell><cell>7 (-)</cell><cell>0.0569</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,164.32,669.32,283.36,8.74"><head>Table 2 :</head><label>2</label><figDesc>P@5, TBG, and MRR rankings for all ClueWeb12 runs.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,86.11,697.19,321.80,6.64"><p>http://en.wikipedia.org/wiki/List_of_metropolitan_areas_of_the_United_States</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,86.11,716.26,143.97,6.64"><p>http://lemurproject.org/clueweb12/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Conclusions</head><p>We plan to continue this track for TREC 2014. Task details should remain essentially the same. Removing the temporal aspect of contexts lead to a more focused task and we have no plans of bringing it back. The ClueWeb12 collection allows easier reuse of data, however we plan to keep the open web option available. We are happy with the success of using crowdsourcing and plan to move to a fully crowdsourced approach to generating user profiles.</p><p>For next 2014 we plan to continue to use the same metrics however we will consider using contexts that are based on locations outside of the US.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,86.36,635.74,454.77,8.74;11,86.36,647.70,22.69,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,395.25,635.74,141.55,8.74">Evaluating contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">Adriel</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,86.36,667.62,454.77,8.74;11,86.36,679.58,454.77,8.74;11,86.36,691.53,115.45,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,266.05,667.62,205.80,8.74">Time-based calibration of effectiveness measures</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,491.48,667.62,49.65,8.74;11,86.36,679.58,450.93,8.74">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
