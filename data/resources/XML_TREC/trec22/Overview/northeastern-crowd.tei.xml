<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,148.91,468.01,15.12">Northeastern University Runs at the TREC13 Crowdsourcing Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-02-05">February 5, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,144.93,181.39,77.62,10.48"><forename type="first">Maryam</forename><surname>Bashir</surname></persName>
							<email>maryam@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.64,181.39,75.52,10.48"><forename type="first">Jesse</forename><surname>Anderton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.80,181.39,59.80,10.48"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.89,181.39,81.19,10.48"><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,148.91,468.01,15.12">Northeastern University Runs at the TREC13 Crowdsourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-02-05">February 5, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">CC80AF20A22B1A2327EDDEA887E46997</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of the TREC 2013 Crowdsourcing Track was to evaluate approaches to crowdsourcing high quality relevance judgments for web pages and search topics. This paper describes our submission to Crowdsourcing track. Participants of this track were required to assess documents judged on a six-point scale. Our approach is based on collecting a linear number of preference judgements, and combining these into nominal grades using a modified version of QuickSort algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We have participated in Crowdsourcing Track of TREC 2013 which required collecting relevance judgments for web pages and search topics. This year's Crowdsourcing Track offered two entry levels for participation, basic and standard. Basic task required relevance assessment of approximately 3,500 documents (a subset of NIST pool, 10 topics), whereas standard task required relevance assessment of approximately 20,000 documents (entire NIST pool, 50 topics). We have participated in basic task of the track. Instead of the usual nominal judgments ''how relevant is this web page?'', our assessing system used preference judgments ''which one of these two pages is more relevant?''. Evaluation of adhoc documents works better with preferences for following reasons:</p><p>• reliability. Traditionally, assessors are asked to give absolute relevance grades to each document with respect to some topic. However, studies have shown that assessors can give more reliable judgments if they are asked which of a pair of documents they prefer, i.e. "is document A better than document B?" <ref type="bibr" coords="1,116.97,534.55,9.96,9.96" target="#b1">[2]</ref>.</p><p>• consistency. A much larger agreement among assessors is observed from preference judgments, most likely because assessors do not have to guess or interpret the given grade scale as they have to do on nominal judgments.</p><p>• training. Another advantage of using preferences is that many popular learning-to-rank algorithms such as RankBoost <ref type="bibr" coords="1,182.00,609.17,10.52,9.96" target="#b2">[3]</ref> and RankNet <ref type="bibr" coords="1,256.28,609.17,10.52,9.96" target="#b0">[1]</ref> are trained on preferences (we are not using here such learning algorithm). When preferences are not available, which is often the case, these algorithms need to infer preferences from the absolute relevance judgments collected from assessors; some information is lost during this process, leading to many ties between documents. This suggests that preferences can be used to improve the training of learning-to-rank algorithms that use such a pairwise approach.</p><p>The use of preference judgments on document pairs, as opposed to absolute judgments on documents, for IR evaluations creates new challenges. There are n 2 unique pairs of documents for a list of n documents, which means that the number of judgments we need to collect increases to O(n 2 ). Since collecting judgments is costly (even with Mechanical Turks), we need a mechanism for collecting these preferences efficiently.</p><p>In the first stage we are approaching the assessing problem as a sorting-the-documents task. Our goal is to minimize the number of judgments needed to sort all documents and find true differences in the performance of retrieval systems. We have implement a QuickSort-like algorithm, using preference judgments (comparisons), so that web documents can be organized into grades following the preferences between each document and preselected special "pivot" documents. Such an algorithm reduces in general the number of judgments needed to fully order a list, as the rate of growth in the number of comparisons is O(n lg n), considerably slower than the O(n 2 ) growth rate for all comparisons. Our algorithm uses constant number of pivots (given by the grade scale, typically 0-4 integers), so the total number of comparison is reduced to O(n) in number of documents assessed.</p><p>The rest of paper is organized as follows: Section 2 describes our sorting algorithm and collection of preference judgments on document pairs. Section 3 provides details on our interface design and experimental setup. Section 4 describes the results of experiments. The final section, Section 5, concludes the paper with a description of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this Crowdsourcing track, participants are actually free to use or not use crowdsourcing techniques however they wish. As discussed in Section 1, preference judgments are easier for assessors as compared to nominal grades. We have used a modified version of QuickSort algorithm for sorting documents. This algorithm can be divided into following steps:</p><p>1. Select pivot documents such that each pivot belongs to a different relevance class and one pivot is selected from each relevance class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sort pivot documents</head><p>3. For each document, search for the correct position between the sorted pivots.</p><p>Each of the above steps is explained in the following sections. Only one assessor (graduate student) was used in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pivot Selection</head><p>For pivot selection, we want to sample a subset of documents such that it has at least one document from all possible classes of relevance for a particular topic. Our goal is to minimize the number of documents we need to examine in order to select documents from all possible relevance classes. We sampled documents for pivot selection in the following manner. First, we calculated a prior relevance score for each document using BM25. This produced an initial ranking of the documents for each topic. We sampled every fifth document from this rank list for pivot selection. In addition to every fifth document, we also sampled top 10 documents from the rank list. The motivation behind this strategy is that BM25 rank list has higher ratio of relevant documents at top of list as compared to bottom of list. For pivot selection, our goal is not to select the most relevant documents, but to select at least one document from all relevance classes for a particular topic (If we only sample documents from top of the list, then there is a high chance that our sample has no document from some relevance class whose documents had very few topic keywords and were ranked very low in the BM25 rank list). Selecting every fifth document from BM25 rank list decreases chances of missing an entire relevance class. These sampled documents were shown to an assessor who examined all these sampled documents. The assessor selected all the documents form this sample that were even slightly relevant to the topic. These selected documents were then re-examined for selection of small number of pivots. These pivots are selected in a manner such that each pivot is from a different relevance class, for a given topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linear Search for Each Document</head><p>Once pivot documents are selected for a topic, they are sorted based on their relevance to the topic. Each of the remaining documents is compared with pivot documents to find its "correct spot". We start by comparing each document to the least relevant pivot document. If the document is assessed as more relevant than the pivot document then it is compared to the next least relevant pivot document. If the document is assessed as less relevant than a pivot document, we stop comparisons for that document. One would think binary search is the most effective search among sorted pivots, but in our case a weighted binary search essentially reduces a linear search: our prior assumption about relevance of Information Retrieval text collections is that the ratio of non-relevant documents, then little-relevance, and so on, is high enough compared to the next grade, such that the distribution over grades dictates a linear search as the most efficient. All the non-relevant documents will be compared only once to the least relevant pivot. Since the number of pivots is fixed, the overall assessment takes O(n) comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Grades from Preference</head><p>After comparing all documents with pivot documents, we sorted the documents using preference judgements. The documents are partitioned into n + 1 relevance classes for n pivots. Each topic-document pair needs to be judged on a six-point scale for this Crowdsourcing Track. Each of the pivots were manually examined by an assessor for assigning a relevance grade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we describe our experimental design for the collection of preference judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interface Design</head><p>The interface we created to collect preference judgements had the following design. After accepting the assignment, assessor was shown the instructions in Figure <ref type="figure" coords="3,327.59,701.44,3.87,9.96" target="#fig_0">1</ref>. In these instructions, we explained that documents should be preferred strictly based on whether they provide information about the query, description, 2: Preference pair selection interface with topic keywords and description and narrative for a particular topic: that a well-written discussion of a related topic should not be preferred to a poorly-written document which is exactly on topic.</p><p>After dismissing the instructions, the assessor is shown the interface presented in Figure <ref type="figure" coords="4,476.05,401.44,3.87,9.96">2</ref>. The "Title" field of a TREC topic is displayed on top, along with its description and narrative. This information describes in detail what constitutes a relevant document for this query. Below the query information is a series of buttons, which allow assessor to record their preferences. Two documents are displayed side-by-side below the buttons. The leftmost and rightmost buttons are labeled "This One," with an arrow pointing to the left or right document, respectively. These buttons allow assessors to choose a winning document. Between these buttons are two buttons for recording ties, labeled "They're Equally Good" and "They're Equally Bad".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data</head><p>In the basic task of Crowdsourcing track, there were a total of 3,470 documents from ClueWeb12 dataset. ClueWeb12 dataset consists of 733,019,372 English web pages, collected between February 10, 2012 and May 10, 2012 using web crawlers. 10 topics were provided by NIST assessors for this task. Participants of the Crowdsourcing Track were required to simulate the role of NIST assessors for the 10 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Four groups submitted a total of 11 runs to the Crowdsourcing track this year. Graded judgments of all runs were evaluated against the NIST qrels, using the GAP (Robertson et al <ref type="bibr" coords="4,417.54,629.70,10.79,9.96" target="#b3">[4]</ref>) measure. NIST did not judge as much of the pool as anticipated, and thus there are fewer documents judged than expected. A ranking of TREC 2013 Web track adhoc runs (34 adhoc runs) was induced using each of the submitted runs (crowd.qrels). Each crowd.qrel.ranking was compared to nist.qrel.ranking using AP-Correlation (Yilmaz et al <ref type="bibr" coords="4,82.55,677.52,10.79,9.96" target="#b4">[5]</ref>) and RMSE (root mean squared error). All groups were evaluated on the basic subset of 10 topics only. The evaluation results of our run submitted to TREC are given in Table <ref type="table" coords="4,388.14,689.48,3.87,9.96" target="#tab_0">1</ref>. Table <ref type="table" coords="4,427.30,689.48,4.98,9.96" target="#tab_0">1</ref> compares our submitted run (NEUPivot1) to the median scores of the systems that participated in the TREC 2013 crowdsourcing task. Our methodology exhibits higher than the median GAP score for all but one topic (202) on submitted The metrics GAP, ERR@20, AP-correlation and RMSE are listed for each topic. Note that for row all, (i) GAP is the mean gap over all 10 topics, (ii) APCorr and RMSE depend on the ranking of runs induced by the mean ERR@20 for all the 10 topics. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0  runs. The median GAP score for topic 202 is 0.036. In our opinion, the reason for this poor median GAP for topic 202 is missing images from ClueWeb12 documents. The topic 202 ("USS Carl Vinson") is navigational topic and participants were asked to find homepage for this topic. The homepage had large missing image in the ClueWeb12 documents so no assessor could identify that web page as home page for this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper we have described our work based on preference judgments for obtaining high quality multiple graded relevance judgements. We have used modified version of QuickSort for sorting documents using linear number of preference judgments. The choice of good pivots is essential, to good performance of our sorting algorithm so pivots should be selected by expert assessors. Once we select good pivots, the task of placing documents in right position among pivots is not complex and can be assigned to crowd workers. In future, we plan to use this methodology with crowd workers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,172.36,334.70,210.77,9.96;3,401.69,334.70,37.95,9.96;3,189.00,108.86,234.01,211.39"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Instructions for preference judgements assessors</figDesc><graphic coords="3,189.00,108.86,234.01,211.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,104.27,617.46,414.09,8.09;5,104.27,627.16,150.08,8.09;5,274.11,627.16,47.17,8.09"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NEUPivot1-basic-ERR@20 vs qrels.basic-ERR@20. qrels.basic is the TREC 2013 web track qrels reduced to topics 202, 214, 216, 221, 227, 230, 234, 246, and 250.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,72.00,614.53,468.00,9.96;5,72.00,626.49,301.73,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NEUPivot1-basic-ERR@20 vs qrels.basic-ERR@20. qrels.basic is the TREC 2013 web track qrels reduced to topics 202, 214, 216, 221, 227, 230, 234, 243, 246, and 250</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,72.28,88.15,470.97,264.77"><head></head><label></label><figDesc></figDesc><graphic coords="4,72.28,88.15,470.97,264.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,59.79,468.00,280.25"><head>Table 1 :</head><label>1</label><figDesc>This table shows per-topic statistics and overall averages for the run NEUPivot1 and median score for 11 runs submitted to crowdsourcing track. The metrics GAP, ERR@20, AP-correlation and RMSE are listed for each topic. Note that for row all, (i) GAP is the mean gap over all 10 topics, (ii) APCorr and RMSE depend on the ranking of runs induced by the mean ERR20 for all the 10 topics. NEUPivot1 for the Basic task of the TREC 2013 Crowdsourcing Track. The 10 topics for the basic task were randomly selected from the TREC 2013 web track ad-hoc task.</figDesc><table coords="5,79.16,59.79,453.67,243.97"><row><cell></cell><cell cols="4">TREC 2013 Crowdsourcing Track Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Group: (NEUIR) Northeastern University</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Run ID: NEUPivot1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Task: Basic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Run type: Primary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Topic # Docs Description of run:</cell><cell>GAP</cell><cell></cell><cell></cell><cell>τ AP</cell><cell></cell><cell></cell><cell>RMSE</cell></row><row><cell></cell><cell cols="9">NEUPivot1 Median This run has been judged by one judge (a graduate student) using preference judgments. of NEUPivot1 Median of NEUPivot1 Median</cell><cell>of</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TREC Runs</cell><cell></cell><cell cols="2">TREC Runs</cell><cell></cell><cell>TREC Runs</cell></row><row><cell>202</cell><cell>231 Results</cell><cell>0.007</cell><cell>0.035714</cell><cell></cell><cell>-0.045</cell><cell cols="2">-0.01868796</cell><cell>0.157</cell><cell>0.3667390</cell></row><row><cell>214 216 221 227</cell><cell cols="7">305 387 368 246 Evaluation of crowd qrel Topic #Docs GAP τ AP (APCorr) RMSE 0.797 0.629760 0.325 0.16270939 0.730 0.588345 0.449 0.14264343 0.708 0.535430 0.430 0.08049942 0.606 0.113211 0.642 -0.23982220 202 231 0.007 -0.045 0.157</cell><cell>0.210 0.090 0.183 0.107</cell><cell>0.2100571 0.2284822 0.1972541 0.4655352</cell></row><row><cell>230 234 243</cell><cell>172 298 342</cell><cell>0.678 0.814 0.598</cell><cell>0.272572 214 0.602812 216 0.254524 221 227</cell><cell>305 387 368 246</cell><cell>0.569 0.797 0.231 0.730 0.732 0.708 0.606</cell><cell cols="2">-0.21109738 0.325 0.210 0.12199660 0.449 0.090 0.38503298 0.430 0.183 0.642 0.107</cell><cell>0.169 0.320 0.158</cell><cell>0.3311034 0.2912807 0.3668166</cell></row><row><cell>246</cell><cell>202</cell><cell>0.428</cell><cell>0.373818 230</cell><cell>172</cell><cell>0.048 0.678</cell><cell cols="2">0.22663554 0.569 0.169</cell><cell>0.210</cell><cell>0.3438483</cell></row><row><cell>250 All</cell><cell>207 2758</cell><cell>0.474 0.584</cell><cell>0.093525 234 NA 243 246</cell><cell>298 342 202</cell><cell>0.366 0.814 0.461 0.598 0.428</cell><cell cols="2">-0.18280615 0.231 0.320 NA 0.732 0.158 0.048 0.210</cell><cell>0.107 0.085</cell><cell>0.2342819 NA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>250</cell><cell>207</cell><cell>0.474</cell><cell>0.366</cell><cell>0.107</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>all</cell><cell>2758</cell><cell>0.584</cell><cell>0.461</cell><cell>0.085</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,104.27,313.42,311.57,8.09"><head>Table 1 :</head><label>1</label><figDesc>This table shows per-topic statistics and overall averages for the run NEUPivot1.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,87.50,247.72,452.50,9.96;6,87.50,259.67,452.50,9.96;6,87.50,271.63,255.65,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,489.06,247.72,50.94,9.96;6,87.50,259.67,119.78,9.96">Learning to rank using gradient descent</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,230.27,259.67,309.73,9.96;6,87.50,271.63,41.09,9.96">Proceedings of the 22nd international conference on Machine learning, ICML &apos;05</title>
		<meeting>the 22nd international conference on Machine learning, ICML &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.50,291.55,452.50,9.96;6,87.50,303.51,22.69,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,379.13,291.55,55.74,9.96">Here or there</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,454.86,291.55,22.21,9.96">ECIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.50,323.43,452.50,9.96;6,87.50,335.39,221.94,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,309.81,323.43,230.18,9.96;6,87.50,335.39,21.26,9.96">An efficient boosting algorithm for combining preferences</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,117.43,335.39,90.49,9.96">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="933" to="969" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.50,355.31,452.51,9.96;6,87.50,367.27,28.56,9.96;6,185.03,367.27,354.97,9.96;6,87.50,379.22,363.94,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,300.07,355.31,239.93,9.96;6,87.50,367.27,23.80,9.96">Extending average precision to graded relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,185.03,367.27,354.97,9.96;6,87.50,379.22,139.43,9.96">of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;10</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.50,399.15,452.50,9.96;6,87.50,411.10,452.50,9.96;6,87.50,423.06,363.94,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,280.34,399.15,255.81,9.96">A new rank correlation coefficient for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,99.74,411.10,440.26,9.96;6,87.50,423.06,139.43,9.96">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;08</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="587" to="594" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
