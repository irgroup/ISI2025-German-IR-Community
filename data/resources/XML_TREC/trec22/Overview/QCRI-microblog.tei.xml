<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.06,71.74,305.61,16.98">QCRI at TREC 2013 Microblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,112.91,123.86,91.40,11.32"><forename type="first">Tarek</forename><surname>El-Ganainy</surname></persName>
							<email>telganainy@qf.org.qa</email>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.86,123.86,70.07,11.32;1,335.93,117.49,1.54,8.37"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
							<email>zywei@se.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.80,123.86,68.61,11.32"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
							<email>wmagdy@qf.org.qa</email>
							<affiliation key="aff2">
								<orgName type="institution">Qatar Computing Research Institute Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.78,185.15,46.15,11.32"><forename type="first">Wei</forename><surname>Gao</surname></persName>
							<email>wgao@qf.org.qa</email>
							<affiliation key="aff3">
								<orgName type="institution">Qatar Computing Research Institute Doha</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.06,71.74,305.61,16.98">QCRI at TREC 2013 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F01231B5B09AA1887FF9465358488746</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report our work in the real-time ad hoc search task of TREC-2013 Microblog track. Our system focuses on improving retrieval effectiveness of Microblog search through query expansion and reranking of search results. We apply web-based query expansion algorithm for enriching the microblog queries with additional terms from concurrent webpages related to the search topic. Later we apply results reranking through utilizing state-of-the-art learning to rank algorithms to train 12 different ranking models using relevance judgment of Tweets2011-12 queries, for which we conduct feature engineering, validation dataset selection, and the ensemble of these models. Our approach differs from salient approaches in the previous Microblog tracks that are based on document expansion utilizing embedded URLs and that leverage some single ranking model for tweets re-ranking. Our pipeline constructed using the hybrid of these two components showed promising retrieval results on Tweets2013 benchmark dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This year comes the third edition of TREC Microblog track 1 consisting of a single task -real-time ad hoc search, while the real-time filtering task introduced last year is eliminated this time. Although the basic concept of the search task is the same as previous years, there are two new genres:</p><p>• Firstly, a new version of tweets collection is provided for evaluation with much larger size than before which includes approximately 240 million tweets spread over a two-month period: from February 1, 2013 to March 31, 2013 (inclusive).</p><p>• Secondly, instead of maintaining a local copy of the data independently per group, the new tweets collection is stored on a remote server shared by all participants and a set of search APIs 2 are provided to users for interaction with the data. The server will return up to 10,000 results for a specific query each time utilizing a state-of-the-art baseline retrieval model. Since in previous years the local copy can be different among groups, it is now fairer to share an identical corpus * The work was performed during the author's internship at Qatar Computing Research Institute 1 https://github.com/lintool/twitter-tools/ wiki/TREC-2013-Track-Guidelines 2 https://github.com/lintool/twitter-tools/ wiki/TREC-2013-API-Specifications remotely by all users so that participants can focus on core techniques and compare with the same baseline run.</p><p>We, the group of participants from Qatar Computing Research Institute (QCRI), submitted four runs for the ad hoc search task, which were configured differently by using query expansion for retrieval <ref type="bibr" coords="1,348.50,316.38,14.94,7.88" target="#b13">[13]</ref> and learning-to-rank <ref type="bibr" coords="1,441.46,316.38,10.45,7.88" target="#b9">[9]</ref> models to re-rank the search results. The four runs are named as QCRI1, QCRI2, QCRI3 and QCRI4 whose configurations are described as follows:</p><p>• QCRI1 uses a single ranking model for re-ranking based on the results obtained from the expanded queries using standard pseudo relevance feedback (PRF) <ref type="bibr" coords="1,479.94,376.43,13.74,7.88" target="#b17">[17]</ref>.</p><p>• QCRI2 uses a single ranking model for re-ranking with a selected validation set for model selection based on the results obtained from the expanded queries using PRF.</p><p>• QCRI3 combines multiple ranking models for re-ranking based on the results obtained from the expanded queries using PRF.</p><p>• QCRI4 combines multiple ranking models for re-ranking based on the results obtained from the expanded queries that combine the expansion terms from the standard PRF and those extracted from the corresponding Google search results of the same query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM OVERVIEW</head><p>The architecture of our real-time Microblog search and re-ranking system is described in Figure <ref type="figure" coords="1,423.65,554.46,3.36,7.88" target="#fig_0">1</ref>.</p><p>We fetched 10,000 retrieval results for each topic with the expanded queries via search API. Two query expansion schemes were adopted in our system, one utilizing the standard PRF technique and the other mining query expansion terms from Google search results based on the same set of Microblog search queries and timebounded by the query timestamp. The run QCRI4 was obtained by retrieving the tweets using the combination of two sets of expansion terms which resulted from the corresponding query expansion schemes, while the other three runs were conducted using the expanded queries which resulted from PRF only and did not use any external information. Then we processed the initially retrieved tweets by removing non-English tweets and filtering retweets. After that, we extracted four categories of ranking features including content-based features, Twitter-specific features, account authority features and temporal features from this tweets set, for re-ranking  the search results by different learning to rank models. For the run QCRI1 and QCRI2, a single ranking model MART <ref type="bibr" coords="2,238.02,391.78,10.45,7.88" target="#b6">[6]</ref> was applied for re-ranking, and an ensemble of 12 trained rankers was used for both QCRI3 and QCRI4. The difference between QCRI1 and QCRI2 was that the latter utilized an automatically selected validation set. Overall, we designed our pipeline to combine query expansion and result re-ranking. For query expansion, besides the commonly used PRF, we also made use of the search result from Google for query expansion. The details will be presented in Section 4. For result re-ranking, our system resorted to learning to rank, the application of which although successful in previous Microblog tracks, was still shallow for the task. Therefore, we performed extensive engineering work for re-ranking including feature engineering, validation set selection and the ensemble of various ranking models. In addition to previously used content-based features, Twitter-specific features and account authority features, we also incorporated temporal features into our system for the sake of real-time fashion of Microblog search. Also, we tried to select validation set using the query similarity measure described in <ref type="bibr" coords="2,205.72,580.07,10.45,7.88" target="#b3">[3]</ref> for selecting the single best performing ranking model. Finally, we explored different methods to combine multiple ranking models. The details about ranking model learning will be given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CORPUS STATISTICS &amp; SEARCH API</head><p>A new tweets collection was provided this year for evaluation that contains 243,271,538 tweets and is an order of magnitude larger than the Tweets2011 collection. A brief comparison between the new collection and Tweet2011 corpus is displayed in Table <ref type="table" coords="2,263.13,679.99,3.36,7.88" target="#tab_1">1</ref>. Other than the total size, the average length of tweets in Tweet2011 is 2.4 tokens shorter than that of Tweet2013.</p><p>The collection was stored and indexed on a remote server pre- processed using some basic operations such as tokenization, normalization and stemming. And a set of search APIs were released for accessing to the corpus where three services were provided including baseline retrieval, text and API-supplied metadata from retrieved tweets, and access to corpus-level statistics. The baseline retrieval was based on negative KL-divergence language modeling approach with Dirichlet prior smoothing parameter µ set to 2,500. For each tweet returned, there were 13 fields of information provided, including tweet id, tweet content, score given by language model, time stamp, language identity, and so on. For corpus-level statistics, information about term frequency and document frequency for terms appearing more than 10 times were provided. The open-source search engine Lucene<ref type="foot" coords="2,484.79,261.12,2.99,5.25" target="#foot_0">3</ref> was employed for indexing and providing the baseline retrieval result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QUERY EXPANSION BASED ON WEB SEARCH RESULTS</head><p>The main challenge in finding relevant tweets to a given topic is word mismatch between search query and tweets text. Some TREC reports in Microblog track <ref type="bibr" coords="2,416.95,345.49,9.71,7.88">[8,</ref><ref type="bibr" coords="2,430.01,345.49,11.21,7.88" target="#b11">11,</ref><ref type="bibr" coords="2,444.56,345.49,11.21,7.88" target="#b13">13,</ref><ref type="bibr" coords="2,459.11,345.49,11.95,7.88" target="#b16">16]</ref> showed that query expansion helped in improving the Microblog retrieval effectiveness since it could enrich the query with additional terms that led to better matching with more relevant tweets. Our work last year <ref type="bibr" coords="2,540.97,376.87,14.94,7.88" target="#b13">[13]</ref> tested expanding the microblog query with title of the top concurrent web result from searching Google with the microblog query. Results showed significant improvement in retrieval effecitiveness compared to standard pseudo relevance feedback (PRF) from the tweets collection <ref type="bibr" coords="2,379.71,429.17,13.74,7.88" target="#b13">[13]</ref>. In this year, we further investigate the webbased query expansion approach through testing advanced settings and configurations rather than just simply appending the title of the top web result to the query.</p><p>Teevan et al. <ref type="bibr" coords="2,376.28,471.02,14.94,7.88" target="#b14">[14]</ref> showed that Microblog search queries were typically time-related since users usually searched social platform for updates about events happening at the time of search. Thus, our query expansion approach is to leverage the web search results obtained by using the Microblog search query aiming to find the concurrent relevant webpages, such as news articles or websites discussing the topic of search. Then we extract expansion terms from the Web search result pages to enrich the initial expansion terms produced by the standard PRF. The process is described as follows:</p><p>• The original query Q0 is used to search in the tweets collection in an initial step. The most frequent nt terms (excluding stop words) appearing in the top retrieved nD tweets are extracted with standard PRF <ref type="bibr" coords="2,437.57,614.35,13.74,7.88" target="#b17">[17]</ref>. Extracted expansion terms are denoted as QP RF .</p><p>• Q0 is used to search the Web via search engine in the same time frame of the query for the concurrent results, in which we extract two types of information: <ref type="bibr" coords="2,473.07,663.71,10.45,7.88" target="#b1">(1)</ref> The title of the topmost search result is extracted and pruned by removing stop words and website name (similar to <ref type="bibr" coords="2,480.27,684.63,13.44,7.88" target="#b13">[13]</ref>). The title part usually contains delimiters like '-' and '|' that separate the real title content and the domain name of the webpage, e.g., "... | CNN.com", "... -Wikipedia, the free encyclopedia".</p><p>Only the real title is used for expansion, referred to as Q title .</p><p>(2) Both titles and snippets of the top-10 ranked results are collected. Then all terms appearing more than nw times are extracted and used for expansion, referred to as Q web .</p><p>• All expansion terms are combined and appended with a given weight to the original query as follows:</p><formula xml:id="formula_0" coords="3,86.19,152.22,196.73,8.37">Qexp = (1 -α) • Q0 + α • (QP RF ∪ Q title ∪ Q web )</formula><p>where Qexp is the final expanded query used for searching tweets at the second time and α is the weight assigned to the expansion terms.</p><p>The final formulated query Qexp is expected to be richer in information about the topic than the original query, and potentially leads to better search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ENSEMBLE OF RANKING MODELS FOR TWEETS RE-RANKING</head><p>In our participation, we employed multiple ranking models and their ensemble for re-ranking the retrieved tweets. Our models were learned using Tweets2011-12 qrels and tested with Tweets2013 queries.</p><p>Our approach aims to improving re-ranking effectiveness by using validation set selection and the combination of different ranking models. We employed six state-of-the-art ranking algorithms in our system: RankNet <ref type="bibr" coords="3,133.22,347.09,9.52,7.88" target="#b2">[2]</ref>, RankBoost <ref type="bibr" coords="3,190.78,347.09,9.52,7.88" target="#b5">[5]</ref>, Coordinate Ascent <ref type="bibr" coords="3,275.71,347.09,13.74,7.88" target="#b12">[12]</ref>, MART <ref type="bibr" coords="3,81.25,357.55,9.52,7.88" target="#b6">[6]</ref>, LambdaMART <ref type="bibr" coords="3,152.90,357.55,13.74,7.88" target="#b16">[16]</ref>, and RandomForests <ref type="bibr" coords="3,245.49,357.55,9.52,7.88" target="#b1">[1]</ref>. All these algorithms have been implemented in the RankLib package <ref type="foot" coords="3,266.90,366.19,2.99,5.25" target="#foot_1">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature Description</head><p>We defined a set of 21 features belonging to 4 different categories. The brief description of all the features can be found in Table <ref type="table" coords="3,75.73,419.57,3.36,7.88" target="#tab_2">2</ref>. Some of these features are detailed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Content-based Features</head><p>Content-based features aim to capture textual similarity between query and the target tweet. This kind of feature is widely used in Web search and full-text retrieval, and has been proved indicative.</p><p>• BM25 &amp; BM25_Exp: BM25 measures the content relevancy between original query Q0 and tweet T by BM25 weighting function. The standard BM25 is formulated as:</p><formula xml:id="formula_1" coords="3,93.72,519.31,180.48,28.52">∑ q i ∈Q 0 IDF (qi) * T F (qi, T ) * (k1 + 1) T F (qi, T ) + k1 * (1 -b + b * Length(T ) Avg length )</formula><p>where Length(T ) denotes the length of T , T F (qi, T ) is the frequency of term qi in tweet T , Avg length stands for average length of tweets in the tweet collection, and IDF (qi) is inverse document frequency. Both average length and IDF are provided by the collection server as corpus-level statistics. For BM25_Exp, the similarity is computed between the expanded query and the target tweet.</p><p>• LM &amp; LM_Exp: LM measures the content relevancy between Q and T by KL-divergence. For LM_Exp, the content relevance is computed between the expanded query and the target tweet. Since the scoring function used by the search API is KL-divergence, we utilize the relevance score of the target tweet as feature value here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Twitter-specific Features</head><p>Twitter provides many special characteristics and we identify some of them as features for the ranking models.</p><p>• Has_URL &amp; URL#: An informative tweet always contains URL for information extension. However, a tweet embedding too many URLs might be a spam. We use two features to capture the URL related information of the target tweet.</p><p>Has_URL is a binary feature which is assigned 1 if the tweet contains at least one URL, and 0 otherwise. URL# indicates the number of URLs included in the tweet.</p><p>• Has_Hashtag &amp; Hashtag#: Users always use hashtag within a tweet to highlight a topic. These two features are used to capture the hashtag usage in the target tweet. Has_Hashtag is a binary feature which is assigned 1 if the tweet contains at least one hashtag, and 0 otherwise. Hashtag# indicates the number of hashtags included in the tweet.</p><p>• RT# &amp; RT#_Level (RTL): Generally, if a tweet is more informative, it is more likely to be reposted by other users. We use two features to indicate the popularity of the target tweet.</p><p>RT# is the number of times the tweet is reposted, and RTL is an integer between 0 and 4 (inclusive) indicating the level of the retweet count. The value of RTL can be computed as follows corresponding to the border points of RT# such as 0, 1, 10, 100 and 1000:</p><formula xml:id="formula_2" coords="3,369.78,329.23,149.91,56.20">RT L =          0, if RT # = 0 1, if RT # ∈ [1, 9] 2, if RT # ∈ [10, 99] 3, if RT # ∈ [100, 999] 4, if RT # ∈ [1000, 9999]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Account-related Features</head><p>A tweet becomes more informative if it is posted by authoritative users. Therefore, we also utilize some straightforward count related to this intuition as features.</p><p>• Follower# &amp; Follower#_Level(FL): Follower# is the number of followers the author who publishes the target tweet owns, and FL is an integer between 0 and 5 (inclusive) indicating the level of follower count. The value of FL is computed based on Follower# with separating points at 10, 100, 1000, 10000 and 100000.</p><p>• Status# &amp; Status#_Level(SL): Status# is the number of tweets the author publishes, and SL is an integer between 0 and 5 (inclusive) indicating the level of Status#. The value of SL is computed based on Status# with separating points at 10, 100, 1000, 10000 and 100000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Temporal Features</head><p>According to our previous work on Microblog search <ref type="bibr" coords="3,528.33,588.88,14.19,7.88" target="#b15">[15,</ref><ref type="bibr" coords="3,546.19,588.88,6.47,7.88" target="#b7">7]</ref>, temporal features appeared effective for both result re-ranking and query expansion. Therefore, we propose two temporal features.</p><p>• Recency_Degree (RD): RD indicates whether the tweet is published recently according to the query time. Time difference between tweet and query is used here to measure the recency degree:</p><formula xml:id="formula_3" coords="3,386.91,674.29,120.82,8.37">RD = T imequery -T imetweet</formula><p>where T imequery stands for the time stamp (in millisecond) the query is issued and T imetweet denotes the time stamp (in millisecond) the target is posted. • Is_Peak (IP): IP is a binary feature indicating whether the target tweet is posted in the peak time of queried topic. Peakfinding algorithm <ref type="bibr" coords="4,142.47,358.26,14.94,7.88" target="#b10">[10]</ref> is used to identify the peak time for the query. Following the strategy used in our real-time tweet search system last year <ref type="bibr" coords="4,165.63,379.18,9.52,7.88" target="#b7">[7]</ref>, we apply peak-finding for the top-n search results and treat the first k largest peaks as the real peak of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Validation Set Selection</head><p>In machine learning, validation dataset is commonly used to select the model with better performance. Since a final ranker is supposed to achieve the best performance on validation set, the more similar the validation set is to the test set, the higher performance the ranker may obtain on test set. In our system, we tried to select validation set by choosing the training queries that bear high similarity to test queries. We adopted two different query selection methods by following the strategy described in <ref type="bibr" coords="4,248.59,504.35,9.52,7.88" target="#b3">[3]</ref>, namely, the document feature aggregation and the query comparison methods. According to our experiments, the document feature aggregation method demonstrated higher performance which therefore was used in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ensemble of Rankers</head><p>To improve the ranking effectiveness, some retrieval systems tried to combine the ranked lists from different ranking models. The result of Yahoo Learning to Rank Challenge <ref type="bibr" coords="4,232.94,598.19,10.45,7.88" target="#b4">[4]</ref> also revealed that the ensemble of ranking models are powerful in Web search. In our system, we learned a number of ranking models separately and used an ensemble approach to incorporate them. We trained the following 12 models for the ensemble:</p><p>1. A Rankboost model is trained without validation set; 2. A RandomForest model is trained without validation set; 3. Two MART models are learned by selecting 20% training queries into validation set and optimizing on P@30, where the validation set of one model is selected using the query selection method described in <ref type="bibr" coords="4,450.73,337.34,10.45,7.88" target="#b3">[3]</ref> and the validations set of the other is selected just randomly;</p><p>4. Two RankNet models are learned with validation set in the same way as above;</p><p>5. Two Coordinate Ascent models are learned with the validation set in the same way as above, but one of them optimizes on MAP instead of P@30;</p><p>6. Four LambdaMART models are learned with the validation sets as above, where two of them are validated using the selected validation set and the other two are validated using the random validation set, and in each group there is one model optimizing on P@30 and the other optimizing on MAP.</p><p>The detail of configuration regarding the training of these separate rankers can be found in Table <ref type="table" coords="4,442.51,505.60,3.36,7.88" target="#tab_3">3</ref>. For the ensemble of rankers, we compared two different approaches that combined these 12 resulted rankers: (1) linear combination by LambdaRank; (2) simply averaging the ranking scores, and the result showed that the lattersimply adding up the normalized model scores -produced higher performance. Therefore, it was eventually adopted for the ensemble in our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION 6.1 Setup</head><p>In TREC ad-hoc Microblog track, two different tweets collections and three sets of queries have been released so far. The tasks of the first two years shared the same collection Tweets2011 which contains around 16 million tweets. The much larger collection Tweets2013 containing 243,271,538 tweets was newly constructed. There are 3 different query sets, one for each year, which are denoted as QS2011, QS2012 and QS2013 containing 50, 60 and 60 queries, respectively. The basic statistics about the relevance judgment of all query sets of the three years are given in Table <ref type="table" coords="4,526.63,711.37,3.36,7.88" target="#tab_4">4</ref>.  All the parameters of query expansion and result re-ranking models we used this year were optimized using the qrels of QS2011 and QS2012 as the training sets. Following the track benchmark, we report P@30 as the major evaluation metric, and we will also report mean average precision (MAP) for reference. The assessment was based on two levels of strictness regarding the relevance judgement: (1) All: All relevant and highly relevant tweets were considered as relevant; (2) High: Only highly relevant ones were treated as relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Parameter Tuning for Query Expansion</head><p>We examined the effectiveness of our different query expansion strategies and tried to find reasonable configuration for each. Then we tested the combined expansion method with the appropriate configuration. The level of All, the less strict assessment, was used here. For the ease of comparison with the best systems available in the track, we evaluated our system on the query sets of QS2011 and QS2012. Later, we applied the best configuration of expansion parameters to QS2013.</p><p>We initially investigated the best configurations to PRF using the retrieved tweets only (referred to as PRF thereafter) and to Websearch-based query expansion, each separately. Later, we combined expansion terms coming from both methods and added to the original query Q0 to formulate the final search microblog query.</p><p>Regarding the PRF, the best achieved improvement over the baseline, measured by P@30, when tested on the TREC2011 collection using QS2011 and QS2012 was:</p><p>• The optimal number of tweets nD to be used in the feedback process was 50.</p><p>• The optimal number of feedback terms nt was 12.</p><p>• The optimal weight for the expansion queries α was 0.2.</p><p>For our Web-search-based query expansion, the timestamp provided with the topics was utilized to simulate the live query expansion from the web described in Section 4. The query of each topic was used to search Google for relevant webpages appeared at the time of the topic. In our experiments, the time was specified between the earliest tweet time in the collection and the time of the topic issued. This assured that the returned results were temporally aligned with the time of the query issued on Twitter <ref type="bibr" coords="5,538.73,68.28,13.74,7.88" target="#b13">[13]</ref>. We examined the effectiveness of expansion with different feedback information (i.e., Q title and Q web ). It was found that query expansion using Q title lead to slight improvement in the retrieval effectiveness over the baseline. Also, we found that the optimal nw for Q web was equal to 3, which led to some improvement over the baseline as well. However, t-test indicated that these improvements were statistically insignificant. Therefore, we simply combined all the expansion terms from QP RF , Q title and Q web (nw = 3) to add them into the final expanded query (referred to as PRF+Web thereafter). By combining the expansion terms it led to higher retrieval effectiveness. We found that PRF+Web got the peak scores when α=0.2 and α=0.3.</p><p>Therefore, our final expansion configuration were set as:</p><formula xml:id="formula_4" coords="5,343.65,221.69,185.42,8.37">Qexp = 0.8 • Q0 + 0.2 • (QP RF ∪ Q title ∪ Q web )</formula><p>where QP RF is configured with nD=50, nt=12, and Q web is configured with nw=3. Table <ref type="table" coords="5,348.02,260.39,4.48,7.88" target="#tab_5">5</ref> demonstrates some example queries with the expansion terms produced from the three expansion strategies, where we can see that expansion terms can alleviate word mismatches between queries and tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results with Re-ranking</head><p>For tweets re-ranking, there are three parts of parameters which need to be tuned. Firstly, the parameters n (top-n search results) and k (the k largest peaks) of peak-finding for the Is_Peak feature were validated based on Tweet2011 corpus using QS2011 and QS2012, which were fixed as n = 100 and k = 2.</p><p>Secondly, for the validation set selection method, we trained our rankers on QS2011 and validated them on QS2012, and our result showed that the document feature aggregation method <ref type="bibr" coords="5,528.91,397.55,10.45,7.88" target="#b3">[3]</ref> performed better. Thus, we choose it as the query similarity measure for validation set selection.</p><p>Thirdly, for the choice of ranker ensemble approach, we compared the linear combination of the rank scores by LambdaRank and simply averaging the rank scores from the 12 ranking models, for which we trained the rankers on QS2011 and validated them on QS2012. The result showed that the simple average is better. Thus, we just summed over the normalized model scores as the final rank score of each tweet.</p><p>For the training of the individual ranking models, we used the default parameters given by RankLib.</p><p>Table <ref type="table" coords="5,349.15,523.08,4.48,7.88" target="#tab_6">6</ref> shows the overall performance of our four submitted runs. We also listed the performance of two baseline runs for comparison: LM(PRF) -language-model-based retrieval with PRF; LM(PRF+Web) -language-model-based retrieval plus PRF with Web-search-based query expansion. Besides, the official median performance of each query among the 65 automatic runs were added up and then averaged to provide an official reference.</p><p>As shown, there is no much difference between QCRI1 run and the baseline LM(PRF), which indicates that the shallow application of learning to rank for tweets re-ranking may not be helpful especially when the baseline itself can already perform fairly well. Therefore, some deeper techniques are needed.</p><p>The effectiveness of QCRI2 is higher than QCRI1 which indicates that the validation dataset selection method is helpful, but their difference is not statistically significant. The performance is improved further when we combined multiple ranking models as it could be shown that QCRI3 outperformed QCRI2. This suggests that different models can perform differently on each individual query and the simple summation over the normalized ranking Basically, leveraging external information could result in performance gain in large margin, evidenced as the obvious improvement made by LM(PRF+Web) (even without re-ranking) over QCRI3, where the differences in terms of P@30 and MAP were statistically significant according to the ALL level assessment. Further plus reranking based on LM(PRF+Web) resulted in QCRI4 that leads to higher performance, and all the improvements over QCRI3 became statistically significant.</p><p>Figure <ref type="figure" coords="6,87.87,421.19,4.48,7.88" target="#fig_1">2</ref> shows the performance of P@30 on individual test queries in QS2013. The P@30 values varied widely across all the 60 queries. Our system obtained very high P@30 values on 6 queries, namely MB126 ("Pitbull rapper"), MB127 ("Hagel nomination filibustered"), MB129 ("Angry Birds cartoon"), MB141 ("Mila Kunis in Oz movie"), MB157 ("Kardashian maternity style") and MB169 ("Honey Boo Boo Girl Scout cookies"). These topics are easy queries due to their relatively large number of relevant tweets in qrels (all of them have more than 50 relevant tweets and 4 of which even have over 300 relevant tweets). The sufficiency of the relevant tweets could also boost the performance of query expansion. On the other hand, the worst 4 queries on which our system achieved P@30 lower than 0.1 include MB134 ("The Middle TV show"), MB150 ("UK wine industry"), MB160 ("social media as educational tool") and MB165 ("ACPT Crossword Tournament"). Further examination revealed that MB150 and MB165 had only 5 and 7 relevant tweets in qrels, respectively, and each relevant tweet contained just 2 or even less original query terms which made them difficult search queries. Furthermore, fewer relevant information also harmed the effectiveness of query expansion. Although the rest of two queries MB134 and MB160 were not lack of relevant tweets, our system still did not generate good performance. For the topic MB134, we found that most of the retrieved tweets were about another TV show "Malcolm in the Middle" which although containing the term 'Middle', was actually retrieved due to the term 'Malcolm' introduced as an expansion term by our query expansion that led to a query topic shift. For the topic MB160, our expanded query gave a larger weight to 'tool' than 'education', which lost the real focus of the original query, and as a consequence, we found most of the retrieved tweets were related to "using social media as tool".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this work, we reported an efficient and effective method for real-time ad hoc Microblog search task based on query expansion and ensemble of rankers for tweets re-ranking. Our techniques are innovative in a sense that (1) query expansion based on PRF was enriched by using expansion terms extracted from the concurrent Web search results; (2) instead of using a single ranking model, we exploited an ensemble of multiple ranking models trained by using different state-of-the-art learning to rank algorithms for better tweets re-ranking. Our method is very efficient since it does not require the time-consuming processing of external information such as the webpages given by the URLs embedded in tweets. Our pipeline constructed by integrating the two components demonstrate promising retrieval effectiveness on TREC 2013 Microblog track datasets.</p><p>In the future, we plan to combine query expansion and document expansion within an efficient framework to further improve retrieval effectiveness. For result re-ranking, we will study the time-related features more deeply and broadly for integrating more of them into our framework, and we will also try to improve the re-ranking of individual queries by using some query sensitive approaches.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,57.23,351.34,232.23,7.90"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Real-time Microblog search and re-ranking system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,138.97,358.19,331.76,7.90"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The performance of P@30 on all the queries in QS2013 by our run of QCRI4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,318.61,63.88,235.49,40.56"><head>Table 1 :</head><label>1</label><figDesc>The general comparison to previous tweets collection</figDesc><table coords="2,329.05,74.64,214.63,29.79"><row><cell>Collection # of tweets</cell><cell># of terms</cell><cell>Tweet length</cell></row><row><cell cols="3">Tweet2013 243,271,538 2,928,041,436 12.04</cell></row><row><cell>Tweet2011 16,141,809</cell><cell>155,562,660</cell><cell>9.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,123.38,63.82,362.94,240.71"><head>Table 2 :</head><label>2</label><figDesc>Feature description (t: a tweet; Qo: original query; Qexp: expanded query) Qo and t LM Language model similarity between Qo and t Length The number of tokens in t Unique_TF The number of unique terms in t that match terms in Qo TF The frequency of terms in t that match terms in Qo BM25_Exp BM25 similarity between Qexp and t LM_Exp Language model similarity between Qexp and t Unique_TF_Exp The number of unique terms in t that match terms in Qexp TF_Exp The frequency of terms in t that match terms in Qexp</figDesc><table coords="4,123.38,74.97,362.94,229.55"><row><cell cols="2">Feature category Feature name</cell><cell>Feature description</cell></row><row><cell cols="3">Content-based BM25 similarity between Twitter-specific BM25 Has_URL Whether t contains at least one URL Has_HashTag Whether t contains at least one hashtag URL# The number of URLs in t HashTag# The number of hashtags in t</cell></row><row><cell></cell><cell>RT#</cell><cell>The counts that t has been reposted</cell></row><row><cell></cell><cell>RT#_Level</cell><cell>The level of RT#</cell></row><row><cell></cell><cell>Status#</cell><cell>The number of tweets the user publishes</cell></row><row><cell>Account authority</cell><cell>Follower# Status#_Level</cell><cell>The number of followers the user owns The level of status count the user publishes</cell></row><row><cell></cell><cell cols="2">Follower#_Level The level of follower count the user owns</cell></row><row><cell>Temporal</cell><cell cols="2">Recency_Degree The gap between query time and tweet time Is_Peak Whether the tweet is published in the peak date of the query</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,63.66,63.88,219.38,115.09"><head>Table 3 :</head><label>3</label><figDesc>The 12 ranking models learned for the ensemble</figDesc><table coords="5,63.66,74.21,219.38,104.76"><row><cell>Algorithm</cell><cell>Validation%</cell><cell>Validation set</cell><cell>Optimized metric</cell></row><row><cell>RankBoost [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RandomForest [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RankNet [2]</cell><cell>20% 20%</cell><cell>selected random</cell><cell>P@30 P@30</cell></row><row><cell>MART [6]</cell><cell>20% 20%</cell><cell>selected random</cell><cell>P@30 P@30</cell></row><row><cell>Coordinate Ascent [12]</cell><cell>20% 20%</cell><cell>random random</cell><cell>P@30 MAP</cell></row><row><cell></cell><cell>20%</cell><cell>selected</cell><cell>P@30</cell></row><row><cell>LambdaMART [16]</cell><cell>20% 20%</cell><cell>selected random</cell><cell>MAP P@30</cell></row><row><cell></cell><cell>20%</cell><cell>random</cell><cell>MAP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,74.90,210.58,196.90,62.49"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="5,74.90,210.58,196.90,62.49"><row><cell cols="4">: The statistics of relevance judgement</cell></row><row><cell></cell><cell cols="3">QS2011 QS2012 QS2013</cell></row><row><cell># of queries</cell><cell>50</cell><cell>60</cell><cell>60</cell></row><row><cell cols="2"># of annotated tweets 40,855</cell><cell>73,073</cell><cell>71,279</cell></row><row><cell># of highly relevant</cell><cell>558</cell><cell>2,572</cell><cell>3,155</cell></row><row><cell># of all relevant</cell><cell>2,864</cell><cell>6,286</cell><cell>9,011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,53.80,63.88,416.02,271.05"><head>Table 5 :</head><label>5</label><figDesc>Examples of expansion terms extracted by each of the expansion method</figDesc><table coords="6,53.80,74.46,416.02,260.47"><row><cell>Q0</cell><cell>QP RF</cell><cell></cell><cell>Q title</cell><cell cols="2">Q web (nw = 3)</cell></row><row><cell>2022 FIFA</cell><cell cols="2">11 playing world cup</cell><cell>qatar unveils new green</cell><cell cols="2">stadiums qatar cup</cell></row><row><cell>soccer</cell><cell cols="2">qatar winter president</cell><cell>stadium designs for</cell><cell>2018 world</cell><cell></cell></row><row><cell></cell><cell cols="2">sports blatter sepp 2010</cell><cell>2022 fifa</cell><cell></cell><cell></cell></row><row><cell>Moscow</cell><cell cols="2">bomb suicide killed 35</cell><cell>moscow bombing car-</cell><cell cols="2">blast dead busiest ter-</cell></row><row><cell>airport</cell><cell cols="2">domodedovo blast peo-</cell><cell>nage at russias domode-</cell><cell cols="2">rorist moscows killed</cell></row><row><cell>bombing</cell><cell cols="2">ple 31 dead kills afp</cell><cell>dovo airport</cell><cell cols="2">100 suicide 35 russias</cell></row><row><cell></cell><cell>russias</cell><cell></cell><cell></cell><cell cols="2">attack russian people</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>domodedovo</cell><cell></cell></row><row><cell>Bieber and</cell><cell cols="2">jon justin kristen video</cell><cell>justin bieber trades</cell><cell cols="2">jon daily justin night</cell></row><row><cell>Stewart</cell><cell cols="2">daily new trade sex</cell><cell>places with jon stewart</cell><cell>bodies</cell><cell></cell></row><row><cell>trading</cell><cell cols="2">stewarts bodies good</cell><cell>raps for sean kingston</cell><cell></cell><cell></cell></row><row><cell>places</cell><cell>teens</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>US behind</cell><cell>survival</cell><cell>cancer-</cell><cell>us rejects venezuelas</cell><cell>venezuelan</cell><cell>embassy</cell></row><row><cell>Chaevez</cell><cell cols="2">treatment firms special</cell><cell>conspiracy claims over</cell><cell>death</cell><cell>venezuela</cell></row><row><cell>cancer</cell><cell cols="2">rosy claims report</cell><cell>hugo chavezs cancer</cell><cell cols="2">maduro chavez united</cell></row><row><cell></cell><cell cols="2">breast reuters reason</cell><cell></cell><cell cols="2">hugo chavezs officials</cell></row><row><cell></cell><cell cols="2">@cancerfollowers rally</cell><cell></cell><cell>states</cell><cell></cell></row><row><cell>Tony</cell><cell cols="2">argo cia real #argo spy</cell><cell>argo what really hap-</cell><cell cols="2">oscars film talks cia real</cell></row><row><cell>Mendez</cell><cell cols="2">lives ben hero affleck</cell><cell>pened in tehran cia</cell><cell cols="2">oscar argo agent</cell></row><row><cell></cell><cell cols="2">hostage latino aka</cell><cell>agent remembers</cell><cell></cell><cell></cell></row><row><cell cols="2">scores can generally improve the overall re-ranking.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,53.80,63.88,502.10,242.25"><head>Table 6 :</head><label>6</label><figDesc>The overall performance of all our runs based on ALL relevance and HIGH relevance. ** and * indicate significance at 99% and 95% confidence level, respectively, compared to our second best run QCRI3</figDesc><table coords="7,117.36,94.26,303.31,211.86"><row><cell></cell><cell></cell><cell>ALL</cell><cell></cell><cell cols="2">HIGH</cell></row><row><cell></cell><cell></cell><cell>P@30</cell><cell>MAP</cell><cell>P@30</cell><cell>MAP</cell></row><row><cell></cell><cell>Median (65 auto)</cell><cell>0.4217</cell><cell>0.2126</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>LM(PRF)</cell><cell>0.4849</cell><cell>0.3030</cell><cell>0.2578</cell><cell>0.2156</cell></row><row><cell></cell><cell>QCRI1</cell><cell>0.4678</cell><cell>0.2993</cell><cell>0.2706</cell><cell>0.2212</cell></row><row><cell></cell><cell>QCRI2</cell><cell>0.4733</cell><cell>0.3001</cell><cell>0.2706</cell><cell>0.2246</cell></row><row><cell></cell><cell>QCRI3</cell><cell>0.4817</cell><cell>0.3068</cell><cell>0.2728</cell><cell>0.2268</cell></row><row><cell></cell><cell>LM(PRF+Web)</cell><cell cols="3">0.5356  *  *  0.3444  *  *  0.2828</cell><cell>0.2464</cell></row><row><cell></cell><cell>QCRI4</cell><cell cols="4">0.5372  *  *  0.3494  *  *  0.2983  *  0.2656  *  *</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P@30</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,263.15,329.46,104.74,4.83"><head>The set of queries in Microblog track 2013</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,321.30,712.26,134.49,6.72"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,58.28,703.29,193.67,6.72;3,53.80,712.26,43.04,6.72"><p>http://sourceforge.net/p/lemur/wiki/ RankLib/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.29,602.94,96.81,10.53" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,336.23,615.23,216.78,7.88;6,336.23,625.69,20.17,7.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,381.55,615.23,55.50,7.88">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,442.57,615.32,62.49,7.69">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,336.23,637.15,201.95,7.88;6,336.23,647.61,196.73,7.88;6,336.23,658.07,209.28,7.88;6,336.23,668.53,195.02,7.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,451.53,647.61,81.43,7.88;6,336.23,658.07,57.11,7.88">Learning to rank using gradient descent</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,408.94,658.16,136.57,7.69;6,336.23,668.62,119.16,7.69">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,336.23,679.99,216.35,7.88;6,336.23,690.45,219.00,7.88;6,336.23,701.00,208.49,7.69;6,336.23,711.37,78.69,7.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,479.37,679.99,73.21,7.88;6,336.23,690.45,90.28,7.88">Query weighting for ranking model adaptation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,442.07,690.54,113.16,7.69;6,336.23,701.00,205.02,7.69">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="112" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,388.02,219.67,7.88;7,73.22,398.48,142.25,7.88;7,73.22,408.94,162.68,7.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,170.91,388.02,121.97,7.88;7,73.22,398.48,30.69,7.88">Yahoo! learning to rank challenge overview</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,109.99,398.57,105.48,7.69;7,73.22,409.03,102.66,7.69">Journal of Machine Learning Research-Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,420.40,219.68,7.88;7,73.22,430.86,206.22,7.88;7,73.22,441.32,168.99,7.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,250.54,420.40,42.36,7.88;7,73.22,430.86,163.45,7.88">An efficient boosting algorithm for combining preferences</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,242.56,430.95,36.88,7.69;7,73.22,441.41,99.83,7.69">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="933" to="969" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,452.78,211.70,7.88;7,73.22,463.24,206.47,7.88;7,73.22,473.70,20.17,7.88" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,130.99,452.78,153.93,7.88;7,73.22,463.24,61.66,7.88">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,141.21,463.33,67.84,7.69">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,485.16,199.70,7.88;7,73.22,495.62,217.53,7.88;7,73.22,506.08,92.40,7.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,194.74,485.16,78.18,7.88;7,73.22,495.62,217.53,7.88;7,73.22,506.08,29.33,7.88">Microblog search and filtering with time sensitive feedback and thresholding based on bm25</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,119.05,506.17,19.33,7.69">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,517.54,217.82,7.88;7,73.22,528.00,137.97,7.88" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,255.19,517.54,35.86,7.88;7,73.22,528.00,76.07,7.88">Hit at trec 2012 microblog track</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,164.61,528.09,19.34,7.69">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,539.45,187.72,7.88;7,73.22,550.01,177.93,7.69;7,73.22,560.38,73.46,7.88" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,109.99,539.45,147.78,7.88">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,73.22,550.01,174.43,7.69">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,571.83,187.62,7.88;7,73.22,582.29,199.12,7.88;7,73.22,592.75,217.51,7.88;7,73.22,603.31,194.36,7.69;7,73.22,613.68,153.66,7.88" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,179.06,582.29,93.27,7.88;7,73.22,592.75,157.55,7.88">Twitinfo: aggregating and visualizing microblogs for event exploration</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Badar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,246.30,592.84,44.43,7.69;7,73.22,603.31,194.36,7.69;7,73.22,613.77,68.89,7.69">Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,625.13,218.67,7.88;7,73.22,635.59,56.29,7.88" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,156.88,625.13,131.65,7.88">Usc/isi at trec 2011: Microblog track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,82.93,635.68,19.33,7.69">TREC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,647.05,217.03,7.88;7,73.22,657.51,216.32,7.88;7,73.22,667.97,20.17,7.88" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,175.48,647.05,114.77,7.88;7,73.22,657.51,73.08,7.88">Linear feature-based models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,151.71,657.60,76.56,7.69">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="274" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,679.43,216.54,7.88;7,73.22,689.89,176.60,7.88" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,186.35,679.43,103.40,7.88;7,73.22,689.89,114.90,7.88">Web-based pseudo relevance feedback for microblog retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Saad</forename><surname>El Din</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,203.24,689.98,19.33,7.69">TREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,73.22,701.35,211.86,7.88;7,336.23,388.02,185.25,7.88;7,336.23,398.57,217.95,7.69;7,336.23,408.94,180.00,7.88" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,222.79,701.35,62.29,7.88;7,336.23,388.02,171.94,7.88"># twittersearch: a comparison of microblog search and web search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,336.23,398.57,217.95,7.69;7,336.23,409.03,104.02,7.69">Proceedings of the Fourth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fourth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,336.23,420.40,213.68,7.88;7,336.23,430.86,208.22,7.88;7,336.23,441.32,83.67,7.88" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,514.04,420.40,35.86,7.88;7,336.23,430.86,208.22,7.88;7,336.23,441.32,21.55,7.88">Exploring tweets normalization and query time sensitivity for twitter search</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,373.33,441.41,19.33,7.69">TREC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,336.23,452.78,200.68,7.88;7,336.23,463.24,202.77,7.88;7,336.23,473.70,115.16,7.88" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,503.54,452.78,33.37,7.88;7,336.23,463.24,153.72,7.88">Adapting boosting for information retrieval measures</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,496.16,463.33,42.84,7.69;7,336.23,473.79,31.48,7.69">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="270" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,336.23,485.16,189.17,7.88;7,336.23,495.62,201.00,7.88;7,336.23,506.17,192.69,7.69;7,336.23,516.54,210.67,7.88;7,336.23,527.00,20.17,7.88" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,421.58,485.16,103.83,7.88;7,336.23,495.62,188.13,7.88">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,336.23,506.17,192.69,7.69;7,336.23,516.63,147.71,7.69">Proceedings of the Tenth International Conference on Information and Knowledge Management</title>
		<meeting>the Tenth International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
