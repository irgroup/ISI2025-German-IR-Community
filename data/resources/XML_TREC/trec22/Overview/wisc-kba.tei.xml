<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.24,154.89,306.78,15.12">Bootstrapping Knowledge Base Acceleration</title>
				<funder ref="#_jTMEzsc">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,243.00,187.37,62.82,10.48"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
						</author>
						<author>
							<persName coords="1,315.57,187.37,46.82,10.48"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName coords="1,162.79,201.32,96.25,10.48"><forename type="first">Sriraam</forename><surname>Natarajan</surname></persName>
						</author>
						<author>
							<persName coords="1,273.27,201.32,78.27,10.48;1,351.54,199.70,1.41,6.99"><forename type="first">Christopher</forename><surname>Re</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,382.93,201.32,65.52,10.48"><forename type="first">Jude</forename><surname>Shavlik</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison, + Indiana University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.24,154.89,306.78,15.12">Bootstrapping Knowledge Base Acceleration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AE0C756926CDB864DB43526EB044FF25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Streaming Slot Filler (SSF) task in TREC Knowledge Base Acceleration track involves detecting changes to slot values (relations) over time. To handle this task, the system needs to extract relations to identify slot-filler values and detect novel values. Being the first attempt at KBA, the biggest challenge that we faced was the scale of the data. We present the approach used by University of Wisconsin for the SSF task and the large scale challenge. We used Elementary, a scalable statistical inference and learning system, developed in University of Wisconsin as our core system. We used Stanford NLP Toolkit to generate parse trees, dependency graphs and named-entity recognition information. These were then converted to features for the logistic regression learner of Elementary. To handle the lack of early SSF training data, we used our existing Knowledge Base Population system to bootstrap a logistic regression model and added rules to handle the new relations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Knowledge Base Acceleration (KBA) track seeks to help humans expand knowledge bases like Wikipedia by automatically recommending edits based on incoming content streams. To this end, KBA systems must filter a large stream of text to find changes to a knowledge base. To recognize changes in knowledge base profiles for particular entities of interest, a KBA system will have to extract relations (slot fillers) for these entities from the corpus stream and find novel relations from these extractions.</p><p>Given the scale of the streaming corpus, running a relation extraction system on all the documents was infeasible. Hence, our system had to filter down the documents for relation extraction. Moreover, the filtering step needs to be much faster than the relation extraction step. Even after filtering the documents, we need a scalable learning and inference system to perform relation extraction. Since many of the relations in this task were new, we also needed a flexible system which allows us to easily specify new features / rules for these relations.</p><p>We used basic string search to quickly filter out irrelevant documents (that do not mention the entities of interest). From the filtered list of documents, we extracted relations using Elementary <ref type="bibr" coords="1,475.65,451.28,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="1,489.77,451.28,7.01,8.74" target="#b6">7]</ref>, a statistical inference and learning system. A key advantage of the Elementary system is that it is a prototype system that scales to very large corpuses. A secondary advantage of this system is that it uses the richer representation of a probabilistic logic formation called Markov Logic <ref type="bibr" coords="1,425.43,523.01,10.52,8.74" target="#b1">[2]</ref> allowing to model and capture rules that are likely but not certain to be correct. We used the Stanford NLP toolkit<ref type="foot" coords="1,487.79,545.35,3.97,6.12" target="#foot_0">1</ref> to extract parse trees, dependency graphs and named entities to generate the features necessary for Elementary. We used the model learned for TAC-KBP 2010 <ref type="bibr" coords="1,528.73,582.79,10.52,8.74" target="#b3">[4]</ref> by mapping the relations from KBP domain to the KBA task. We designed features to handle the new relations and entity types.</p><p>We also learned various lessons in our first attempt at KBA. Although we filtered down the doc-uments for relation extraction, we still had to download all the data and decrypt it locally which was time-consuming. In hindsight, performing the filtering on Amazon Cloud would have been more efficient. We also assumed given the size of the corpus, most of the relations would be mentioned multiple times. Hence we did not rely on having all the possible rules for each relation, but on capturing the common cases. But since the set of entities were not popular, we were not able to extract many slot values for these entities.</p><p>The rest of the paper is organized as follows. In Section 2, we present few details about the Streaming Slot-Filling task (SSF) that we worked on. Following that, we present the details of our approach in Section 3. Finally, we briefly discuss the results of our approach and future steps to further improve them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The KBA track provides a large streaming corpus by breaking the documents up into hourly chunks which can then be processed sequentially. The track also provides a list of target entities represented as links to Wikipedia or Twitter pages. Given the corpus and entity list as input, the track consists of two tasks:</p><formula xml:id="formula_0" coords="2,81.96,440.95,218.68,8.74">• Cumulative Citation Recommendation (CCR)</formula><p>task. The CCR task involves filtering documents worth citing in a profile of a target entity. The system needs to recognize whether a document is useful (time-invariant e.g., place of birth) or vital (timely e.g. title).</p><p>• Streaming Slot Filling (SSF) task. The SSF task involves detecting changes to slot filler values (relations) for target entities. The system needs to extract slot filler values for target entities and then recognize changes to slot values. We concentrated only on this task in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>The three key challenges that we faced with Streaming Slot Filling task were:</p><p>1. Handling the large scale of data 2. Extracting slot values for target entities</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detecting novel slot values</head><p>To solve each of these problems, we developed heuristic approaches that we outline in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large scale</head><p>Due to the scale of the proposed task, it is not feasible to perform relation extraction on the entire corpus.</p><p>Similarly, employing many of the standard, publicly available natural language processing tools would not be feasible as well. In order to make the corpus size manageable, we searched for target entities (and variants of their names) in the articles. For e.g., if "William Smith" is a person of interest, we will accept any document that mentions "William", "Bill" or "Smith". If an article contains any useful information about a target entity, we assume that some variant of the target entity name will be mentioned in the article. This heuristic is very fast as it does not require any processing to be done on the article and we only need to find the first match for a target entity in the article. We use a simplified version of the Aho-Corasick algorithm <ref type="bibr" coords="2,433.99,416.53,10.52,8.74" target="#b0">[1]</ref> since we only need to find the first match. This reduces the size of the corpus from ∼60TB to a manageable ∼1TB. We implemented parallel version of the filtering algorithm on these articles to further speed up our system. Since we ran this step on our local cluster with limited storage space, this step still took a week to run. Going forward, we will move this step to the Amazon cloud service and reduce the amount of data downloaded to our local cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extracting slot values</head><p>To perform relation extraction, we first extracted parse trees and dependency graphs for sentences in the filtered articles. Although TREC does provide some basic NLP annotations as part of the corpus, we employed Stanford toolkit since we needed dependency graphs as features for relation extraction. We then used the Elementary system developed at University of Wisconsin-Madison to perform relation extraction. At a high level, Elementary first creates the potential mentions based on the named entities. It then creates a list of potential relations between pairs of mentions in the same sentence. For each potential relation, the path in the dependency graph and parse tree is calculated and used as features for a logistic regression model. The weights are learned for each feature and each relation type using gold-standard training data as well as distant supervision examples <ref type="bibr" coords="3,72.00,223.60,9.96,8.74" target="#b4">[5]</ref>. We also manually can specify rules which can be as simple as just specifying feature weights or even constraints on the relations. For further details about Elementary, please refer to Niu et al. <ref type="bibr" coords="3,237.49,259.47,9.96,8.74" target="#b5">[6]</ref>. Once these features were obtained, we needed to learn the weighs for features in Elementary. It should be mentioned that for the initial runs of the SSF task, there was no available training data. Hence, we used a model that had been learned using distant supervision on a subset of the relations for the Knowledge Base Population (TAC KBP 2010) task <ref type="bibr" coords="3,242.36,343.34,9.96,8.74" target="#b3">[4]</ref>. We found mappings between the relation names from KBP task to the KBA task. The mapping between the KBA and KBP relations is shown in Table <ref type="table" coords="3,239.38,379.21,3.87,8.74" target="#tab_0">1</ref>. For the relations in KBA that couldn't be mapped, we manually created features based on few sample sentences. To do so, we searched for sample sentences containing these relations and added the corresponding dependency path or parse tree features to the model. We assumed that capturing few features for these new relations would be sufficient. Given the scale of the data, we assumed that at least one mention of a valid relation will be extracted using a simple relation extractor. Once we extracted the relations, we employed Elementary's entity-linking model to link the slot filler entities with the target entities. We filtered out the relations that did not include any target entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Novelty detection</head><p>Given the extracted relations, we processed them chronologically based on the document time to check for any change in slot value. Unlike the previous steps, this step can not be performed in parallel. For every relation, we check if we have already extracted any relations of the same type for the same entity.  Figure <ref type="figure" coords="3,354.57,558.35,4.98,8.74" target="#fig_0">1</ref> shows the overall system design as a flowchart. As mentioned before, we filter the documents using a basic entity filter. We perform Entity Linking and Relation Extraction using Elementary. We only accept relations over mentions linked to the target entities. We then perform our basic novelty detection over the stream of extracted relations where the previous extraction are cached. Only the novel relations are then used to create the output for the SSF task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We now discuss the results of our system on the SSF task. There were two evaluation measures used by TREC for this task: (1) average F-1 score and (2) average Scaled Utility (SU) <ref type="bibr" coords="4,195.85,185.86,9.96,8.74" target="#b2">[3]</ref>. Each score is calculated over four stages in a pipeline:</p><p>• SSF-DOCS: In this stage, the system's capability to identify the slot type in a document is evaluated. The scores are calculated by comparing the slot types of the output run against the annotations (the slot values are ignored).</p><p>• SSF-OVERLAP: This stage evaluates the slot values of the output run by checking for overlap with the annotations, but only considers the true positives from the previous stage.</p><p>• SSF-FILL: This stage too takes as input the true positives from the previous stage but checks if the output run recognizes the equivalence between the same slot values.</p><p>• SSF-DATE HOUR: This stage checks if the system is able to recognize the first occurrence of the slot value and ignores the duplicates.</p><p>Since our approach concentrated on a subset of the relations, the F1 score on any of these measures averaged over all the relations was very low. In general for all the stages, our F1 score was close to zero but the scaled utility was close to the median value. Also, our basic assumption that most of the relations would be mentioned multiple times in multiple ways was flawed. Since we only relied on capturing the common cases, we missed many extractions for the uncommon entities resulting in a low recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present the design of our system for the streaming slot-filling task. We use a high-throughput and high-recall filter to get a smaller corpus for the computationally intensive relation extraction step. We use a scalable and highly flexible system, Elementary to perform relation extraction. We bootstrapped this system by transferring the learned model from TAC KBP 2010 thereby circumventing the need for training examples.</p><p>We have learned various lessons in our first attempt at this task. Going forward, we will move the filtering task to Amazon Cloud to make it much more computationally efficient. We will work on training a model for all the relations of this domain via distant supervision. We also will work on manually evaluating our results to check for the sources of errors in our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,352.34,476.65,145.18,8.74;3,310.61,318.54,233.62,143.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall System Design.</figDesc><graphic coords="3,310.61,318.54,233.62,143.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,126.76,467.25,547.92"><head>Table 1 :</head><label>1</label><figDesc>If not, we accept this relation as a novel slot value. If Mapping between KBP and KBA relations.All extractions of the relation type on the left were marked as KBA relations of the type on right.</figDesc><table coords="3,335.06,126.76,179.72,132.67"><row><cell>KBP Relations</cell><cell>KBA Relation</cell></row><row><cell>per : date of death</cell><cell>DateOfDeath</cell></row><row><cell>per : title</cell><cell>Titles</cell></row><row><cell>per : spouse</cell><cell>SignificantOther</cell></row><row><cell>per : employee of,</cell><cell></cell></row><row><cell>per : member of, org : member of,</cell><cell>EmployeeOf</cell></row><row><cell>org : top employees</cell><cell></cell></row><row><cell>org : top members</cell><cell>TopMembers</cell></row><row><cell>org : subsidiaries, per : schools attended</cell><cell>Affiliate</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,325.85,667.30,183.16,6.99"><p>http://nlp.stanford.edu/downloads/corenlp.shtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors gratefully acknowledge support of the <rs type="programName">DARPA DEFT Program</rs> under the <rs type="institution">Air Force Research Laboratory (AFRL)</rs> prime contract no. <rs type="grantNumber">FA8750-13-2-0039</rs>. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, <rs type="institution">ARO</rs>, or the US government.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jTMEzsc">
					<idno type="grant-number">FA8750-13-2-0039</idno>
					<orgName type="program" subtype="full">DARPA DEFT Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,324.52,390.80,214.73,6.99;4,324.53,400.26,214.72,6.99;4,324.53,409.73,68.84,6.99" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,444.87,390.80,94.38,6.99;4,324.53,400.26,114.05,6.99">Efficient string matching: an aid to bibliographic search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Corasick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,449.95,400.26,89.30,6.99;4,324.53,409.73,16.70,6.99">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,324.52,427.16,214.72,6.99;4,324.53,436.62,214.23,6.99" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,433.06,427.16,106.19,6.99;4,324.53,436.62,45.71,6.99">Markov Logic: An Interface Layer for AI</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Morgan &amp; Claypool</publisher>
			<pubPlace>San Rafael, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,324.52,454.06,214.72,6.99;4,324.53,463.52,107.07,6.99" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,432.75,454.06,106.50,6.99;4,324.53,463.52,40.41,6.99">The TREC-8 filtering track final report</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,382.86,463.52,21.30,6.99">TREC</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,324.52,480.96,214.73,6.99;4,324.53,490.42,214.72,6.99;4,324.53,499.89,19.29,6.99" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,493.45,480.96,45.80,6.99;4,324.53,490.42,178.89,6.99">An overview of the TAC 2010 knowledge base population track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Griffit</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,324.52,517.32,214.72,6.99;4,324.53,526.79,214.72,6.99;4,324.53,536.25,52.65,6.99" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,511.80,517.32,27.44,6.99;4,324.53,526.79,210.96,6.99">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,335.10,536.25,14.97,6.99">ACL</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,324.52,553.69,214.72,6.99;4,324.53,563.15,214.72,6.99;4,324.53,572.61,214.72,6.99;4,324.53,582.08,181.47,6.99" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,324.53,563.15,214.72,6.99;4,324.53,572.61,150.80,6.99">Elementary: Large-scale knowledge-base construction via machine learning and statistical inference</title>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jude</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,481.97,572.61,57.28,6.99;4,324.53,582.08,155.63,6.99">IJSWIS Special Issue on Web-Scale Knowledge Extraction</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,324.52,599.51,214.72,6.99;4,324.53,608.98,214.72,6.99;4,324.53,618.44,19.29,6.99" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="4,444.07,599.51,95.18,6.99;4,324.53,608.98,210.58,6.99">Towards high-throughput gibbs sampling at scale: A study across storage managers</title>
		<author>
			<persName coords=""><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
