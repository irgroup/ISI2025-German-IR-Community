<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.58,186.15,385.62,13.98;1,236.97,208.07,137.62,13.98">The Technion at TREC 2013 Web Track: Cluster-based Document Retrieval</title>
				<funder ref="#_YZzMafn">
					<orgName type="full">Microsoft Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.27,239.90,67.09,9.72"><forename type="first">Fiana</forename><surname>Raiber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Industrial Engineering and Management Technion</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>32000</postCode>
									<settlement>Haifa</settlement>
									<country>Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,316.94,239.90,71.31,9.72"><forename type="first">Oren</forename><surname>Kurland</surname></persName>
							<email>kurland@ie.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Industrial Engineering and Management Technion</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>32000</postCode>
									<settlement>Haifa</settlement>
									<country>Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.58,186.15,385.62,13.98;1,236.97,208.07,137.62,13.98">The Technion at TREC 2013 Web Track: Cluster-based Document Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A275F0EB03ED341B180BF7484857A7CC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many cluster-based document retrieval methods have been proposed over the years. In our submissions to the ad hoc task of the TREC 2013 Web Track we experimented with one such highly effective method. Empirical results demonstrate the effectiveness of using our approach; specifically, with respect to other submitted runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our submissions to the ad hoc task of TREC's 2013 Web Track were based on a clusterbased document retrieval approach. We used the recently proposed ClustMRF method <ref type="bibr" coords="1,492.54,454.22,10.48,8.09" target="#b6">[7]</ref> to rank the document clusters created from the documents most highly ranked by an initial search. The cluster ranking was then transformed to a ranking over documents. ClustMRF utilizes Markov Random Fields, which enable the integration of different types of clusterrelevance evidence. For the initial search, we used several query-independent document quality measures <ref type="bibr" coords="1,184.04,514.00,9.92,8.09" target="#b1">[2]</ref>. These measures are also naturally integrated in ClustMRF.</p><p>The empirical evaluation shows that using our cluster-based approach yields performance that is substantially better than that of the initial search. Furthermore, we show that the performance of our runs was above the median in the ad hoc Web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Approach</head><p>We applied the following steps to produce three submitted runs. First, we ranked all the documents in the corpus to create an initial list of 10, 000 documents<ref type="foot" coords="1,409.57,614.65,3.96,5.66" target="#foot_0">1</ref> . Next, we re-ranked the top 1, 000 documents in the initial list using a learning-to-rank approach; the remaining 9, 000 documents maintained their original positions. We then re-ranked the top 50 documents in the list produced in the previous step using ClustMRF <ref type="bibr" coords="1,396.56,652.46,9.92,8.09" target="#b6">[7]</ref>. Finally, the ranking created using ClustMRF was used to diversify search results. The details of the methods used in each step are provided below.</p><p>Creating the initial list and removing spam documents. The Markov Random Field (MRF) method with the sequential dependence model <ref type="bibr" coords="2,340.99,198.18,10.48,8.09" target="#b5">[6]</ref> was used to rank all the documents in the corpus. Documents that were assigned with a score below 50 by Waterloo's spam classifier <ref type="bibr" coords="2,147.64,222.09,10.48,8.09" target="#b3">[4]</ref> were then removed from the ranking, and the top 10, 000 remaining documents were used to create the initial list D init .</p><p>Learning-to-rank. The top 1, 000 documents in D init were re-ranked using SVM rank <ref type="bibr" coords="2,490.74,259.95,9.92,8.09" target="#b4">[5]</ref>, applied with default parameter values. Most of the 130 features that we used are based on those used in Microsoft's learning-to-rank datasets 2,3 . The rest of the features are queryindependent document quality measures which were shown to be highly effective for Web retrieval <ref type="bibr" coords="2,147.55,307.77,9.92,8.09" target="#b1">[2]</ref>. These features include the ratio between the number of stopwords and nonstopwords in a document, the percentage of stopwords in a stopwords list that appear in the document, the entropy of the term distribution in a document, and the score assigned to a document by Waterloo's spam classifier <ref type="bibr" coords="2,302.64,343.64,9.92,8.09" target="#b3">[4]</ref>. Except for the latter, all the features were computed separately for the entire document, its body, title, URL and anchor text. The resultant model is henceforth referred to as LTR.</p><p>Using document clusters. As the next step, the 50 most highly ranked documents by LTR were clustered using the nearest-neighbor clustering approach. The recently proposed ClustMRF method <ref type="bibr" coords="2,201.05,417.36,10.48,8.09" target="#b6">[7]</ref> was used to rank these clusters based on their presumed relevance to the query. The score assigned to a document by LTR served for the document-query similarity measure that is used in ClustMRF. Finally, the ranking of clusters was transformed to a ranking of documents by replacing each cluster with its constituent documents while omitting repeats; the order of documents within a cluster was determined by the positions of the documents in the ranking produced by LTR. The remaining 950 documents in the LTR result list retained their original positions. Additional technical details are provided in Section 3.1.</p><p>Diversifying search results. To diversify search results, the top 50 documents in the ranking created by LTR were first re-ranked using ClustMRF as described above. The same 50 documents were then re-ranked using MMR+ClustMRF, a variant of the MMR method <ref type="bibr" coords="2,144.83,562.81,10.48,8.09" target="#b2">[3,</ref><ref type="bibr" coords="2,159.27,562.81,6.99,8.09" target="#b6">7]</ref>. The similarity between the query and the document in MMR+ClustMRF was estimated using the rank of a document in the result list produced by ClustMRF <ref type="bibr" coords="2,483.54,574.77,9.92,8.09" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>We submitted runs both for the full ClueWeb12 dataset and for its Category B subset, hereafter denoted CatA and CatB, respectively <ref type="foot" coords="3,320.69,200.59,3.96,5.66" target="#foot_3">4</ref> .</p><p>The Indri toolkit<ref type="foot" coords="3,199.26,212.55,3.96,5.66" target="#foot_4">5</ref> was used for experiments. Queries and documents were stemmed using the Krovetz stemmer. The INQUERY stopword list <ref type="bibr" coords="3,359.91,226.44,10.48,8.09" target="#b0">[1]</ref> was used to remove stopwords from queries, but not from documents; and, to compute the two stopwords-based features used by LTR.</p><p>The IDs of the runs which were submitted are clustmrfbf and clustmrfaf, where the ClustMRF method was implemented for CatB and CatA, respectively, and mmrbf where the MMR+ClustMRF method was implemented for CatB.</p><p>Free-parameter values. The free-parameter values of LTR, ClustMRF and MMR+ClustMRF were learned using the Category B of the ClueWeb09 dataset with queries 1-200 from TREC 2009-2012.</p><p>The values of λ T , λ O , and λ U , the three free parameters of the MRF model that was used to create D init , were set to 0.85, 0.1, and 0.05, respectively, following previous recommendations <ref type="bibr" coords="3,161.77,371.90,9.92,8.09" target="#b5">[6]</ref>. The Dirichlet smoothing parameter that was used to create D init and to compute the LMIR.DIR feature for LTR was set to 1000. The BM25 feature used by LTR was computed with k1=1 and b=0.5; LMIR.JM was computed with λ=0.1. SVM rank was used to learn feature weights in ClustMRF; NDCG@k of the k documents in a cluster was used as the score of a cluster. For each cluster size k (∈ {5, 10, 20}) a ranking of documents was created using ClustMRF; the value of k was selected to optimize MAP@50 of the resultant document ranking. The interpolation parameter of MMR+ClustMRF, which controls the tradeoff between the relevance and diversity estimates, was set to a value in {0.1, 0.2, . . . , 0.9} to optimize ERR-IA@20. All other implementation details are the same as in Raiber and Kurland <ref type="bibr" coords="3,222.82,479.50,9.92,8.09" target="#b6">[7]</ref>.</p><p>The two-tailed paired t-test with p ≤ 0.05 was used for testing statistical significance of performance differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Main result</head><p>The results are presented in Table <ref type="table" coords="3,260.04,568.09,3.87,8.09" target="#tab_0">1</ref>. We can see that LTR outperforms the initial ranking (Init), which was induced using standard MRF, in all relevant comparisons. Yet, LTR is always outperformed by ClustMRF for both CatA and CatB; the differences are statistically significant in the majority of the cases. This finding is consistent with previous work which showed that ClustMRF can be used to improve the performance of various result lists that are produced by effective retrieval methods <ref type="bibr" coords="3,299.15,627.86,9.92,8.09" target="#b6">[7]</ref>.</p><p>MAP@1000 NDCG@20 ERR@20 P-IA@20 α-NDCG@20 ERR-IA@20 NDCG@20 ERR@20 P-IA@20 α-NDCG@20 ERR-IA@20 90 88 92 82 80</p><p>Table <ref type="table" coords="4,136.34,347.09,3.87,8.09">2</ref>: The percentage of queries for which the performance of the ClustMRF run for CatA (clustmrfaf ) is better than or equal to the median performance attained by other TREC runs.</p><p>We can also see that, in general, ClustMRF is the most effective method for MAP@1000 and NDCG@20, while MMR+ClustMRF is the most effective in terms of α-NDCG@20 and ERR-IA@20. This finding can be attributed to the fact that the value of the interpolation parameter in MMR+ClustMRF was set by optimizing ERR-IA@20, whereas in ClustMRF the size of clusters was selected by optimizing MAP@50 and the weights of feature were learned by maximizing relevance and not diversity. All in all, the findings presented above attest to the high effectiveness of using document clusters to re-rank the documents in an (effective) initial result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Comparison with other TREC runs</head><p>We next compare the performance of ClustMRF with that attained by other runs submitted to TREC. To that end, we computed the percentage of queries for which the performance attained by ClustMRF was higher or equal to the median performance attained by other runs. The results for CatA are presented in Table <ref type="table" coords="4,334.90,564.02,3.87,8.09">2</ref>. We can see that ClustMRF was at least as effective as the median run for about 90% of the queries for NDCG@20, ERR@20, and P-IA@20, and for about 80% of the queries for α-NDCG@20 and ERR-IA@20. Overall, we can conclude that the performance of ClustMRF was above the median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We described our submissions to the ad hoc task of TREC's 2013 Web Track. We used a learning-to-rank approach to re-rank an initially retrieved list. Clusters of similar documents created from the top ranked documents in that re-ranked list were ranked using ClustMRF <ref type="bibr" coords="5,108.00,148.37,9.92,8.09" target="#b6">[7]</ref>. The ranking of clusters was then transformed to a ranking of documents. In addition, to diversify the results, the document ranking produced by ClustMRF was used to estimate the similarity between the query and a document in MMR <ref type="bibr" coords="5,345.12,172.28,10.48,8.09" target="#b2">[3,</ref><ref type="bibr" coords="5,358.46,172.28,6.99,8.09" target="#b6">7]</ref>. Empirical results demonstrate the effectiveness of using ClustMRF to re-rank the top ranked documents in a (highly effective) result list.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,108.00,163.22,395.42,135.39"><head>Table 1 :</head><label>1</label><figDesc>Main result. The best result for an experimental setting (CatA/CatB) and evaluation metric is boldfaced. 'i', 'l', and 'c' mark statistically significant differences with the initial ranking (Init), LTR and ClustMRF, respectively. The three submitted runs are marked with ' '.</figDesc><table coords="4,108.00,163.22,388.07,72.51"><row><cell></cell><cell>Init</cell><cell>14.0</cell><cell>20.3</cell><cell>13.1</cell><cell>29.1</cell><cell>57.7</cell><cell>50.7</cell></row><row><cell>CatA</cell><cell>LTR ClustMRF MMR+ClustMRF</cell><cell>18.1 i 19.3 i l 18.3 i c</cell><cell>26.8 i 31.0 i l 29.0 i c</cell><cell>16.7 i 18.4 i l 18.2 i l</cell><cell>35.1 i 42.5 i l 38.0 i c</cell><cell>63.6 66.8 i 67.1 i l</cell><cell>54.5 56.7 57.0</cell></row><row><cell></cell><cell>Init</cell><cell>3.5</cell><cell>11.9</cell><cell>8.3</cell><cell>18.6</cell><cell>50.3</cell><cell>42.3</cell></row><row><cell>CatB</cell><cell>LTR ClustMRF MMR+ClustMRF</cell><cell>4.8 i 6.0 i l 5.9 i lc</cell><cell>15.3 i 19.8 i l 19.4 i l</cell><cell>10.1 12.3 i l 12.4 i l</cell><cell>22.5 i 29.2 i l 27.9 i l</cell><cell>55.5 58.6 i 59.0 i</cell><cell>46.3 50.1 50.5 i l</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,125.94,671.70,254.14,6.47"><p>The requirement of TREC was to submit runs of 10, 000 documents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,125.94,594.01,181.66,6.47"><p>www.research.microsoft.com/en-us/projects/mslr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,125.94,603.51,377.19,6.47;2,108.00,612.98,395.06,6.47;2,108.00,622.44,122.51,6.47"><p>A few features were not considered in our experiments. These include the Boolean Model, Vector Space Model, LMIR.ABS, Outlink number, SiteRank, QualityScore, QualityScore2, Query-URL click count, URL click count, and URL dwell time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,125.94,647.10,377.29,6.47;3,108.00,656.57,209.73,6.47"><p>For consistency with the results published by TREC, the evaluation for both CatA and CatB was performed using the qrels of the full ClueWeb12 dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,125.94,666.07,103.28,6.47"><p>www.lemurproject.org/indri</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgments</head><p>This work has been supported by and carried out at the <rs type="institution">Technion-Microsoft Electronic Commerce Research Center</rs>. This work has also been supported in part by <rs type="funder">Microsoft Research</rs> through its <rs type="programName">Ph.D. Scholarship Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YZzMafn">
					<orgName type="program" subtype="full">Ph.D. Scholarship Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,123.46,329.65,379.65,8.09;5,123.50,341.60,380.45,8.09;5,123.50,353.56,218.12,8.09" xml:id="b0">
	<analytic>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,200.84,341.60,270.22,8.09">Proceedings of the Ninth Text Retrieval Conference (TREC-9)</title>
		<meeting>the Ninth Text Retrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="500" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,123.46,373.49,379.52,8.09;5,123.50,385.44,188.78,8.09" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,306.92,373.49,177.80,8.09">Quality-biased ranking of web documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,123.50,385.44,92.71,8.09">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,123.46,405.37,379.51,8.09;5,123.50,417.32,380.44,8.09;5,123.50,429.28,22.68,8.09" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,278.49,405.37,224.48,8.09;5,123.50,417.32,197.06,8.09">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,341.64,417.32,89.92,8.09">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,123.46,449.21,379.53,8.09;5,123.50,461.16,380.44,8.09;5,123.51,473.12,22.68,8.09" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,350.30,449.21,152.70,8.09;5,123.50,461.16,157.66,8.09">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,289.22,461.16,144.70,8.09">Journal of Informaltiom Retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,123.47,493.04,380.47,8.09;5,123.51,505.00,22.68,8.09" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,181.85,493.04,145.92,8.09">Training linear svms in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,347.99,493.04,82.78,8.09">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,123.48,524.93,379.57,8.09;5,123.51,536.88,190.01,8.09" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,254.13,524.93,230.08,8.09">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,123.51,536.88,90.75,8.09">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,123.47,556.81,379.58,8.09;5,123.51,568.76,190.01,8.09" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,244.96,556.81,240.90,8.09">Ranking document clusters using markov random fields</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Raiber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,123.51,568.76,90.75,8.09">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
