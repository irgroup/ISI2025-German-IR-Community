<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.22,72.35,381.29,16.84;1,495.50,77.41,3.82,5.24">BIT and MSRA at TREC KBA CCR Track 2013 *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,120.64,118.05,75.94,11.06"><forename type="first">Jingang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName coords="1,268.30,118.05,73.11,11.06;1,341.41,118.67,1.72,5.24"><forename type="first">Dandan</forename><surname>Song</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,275.95,179.07,57.82,11.06"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
							<email>liaolj@bit.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.22,72.35,381.29,16.84;1,495.50,77.41,3.82,5.24">BIT and MSRA at TREC KBA CCR Track 2013 *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F341271F9447831D6390748F7397106</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our strategy for TREC KBA CCR track is to first retrieve as many vital or documents as possible and then apply more sophisticated classification and ranking methods to differentiate vital from useful documents. We submitted 10 runs generated by 3 approaches: question expansion, classification and learning to rank. Query expansion is an unsupervised baseline, in which we combine entities' names and their related entities' names as phrase queries to retrieve relevant documents. This baseline outperforms the overall median and mean submissions. The system performance is further improved by supervised classification and learning to rank methods. We mainly exploit three kinds of external resources to construct the features in supervised learning: (i) entry pages of Wikipedia entities or profile pages of Twitter entities, (ii) existing citations in the Wikipedia page of an entity, and (iii) burst of Wikipedia page views of an entity. In vital + useful task, one of our ranking-based methods achieves the best result among all participants. In vital only task, one of our classification-based methods achieve the overall best result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Knowledge Bases (KBs), such as Wikipedia, have shown great power in many applications including query answering, entity retrieval and entity linking. With the explosion of information on the web, it becomes critical to detect relevant documents and assimilate new information to entities in KBs in a timely manner. However, most KBs are maintained manually by volunteer editors, which are hard to keep up-to-date because of the limit number of editors and the huge volume of entities in KBs. <ref type="bibr" coords="1,443.09,251.76,9.71,7.86" target="#b7">[6]</ref> indicates that the median time lag between the publication date of the cited articles and the date of the citations created in Wikipedia is nearly one year. Moreover, some esoteric entities in KBs do not attract enough attentions from the editors. It makes the maintenance more challenging. This gap could be reduced if relevant documents could be automatically found as soon as they are published and then recommended to the editors.</p><p>Cumulative Citation Recommendation(CCR) was introduced by Text REtrieval Conference(TREC) Knowledge Base Acceleration (KBA) track in 2012 to address this problem. A CCR system should filter candidate documents for a given set of entities from a time-ordered stream corpus. CCR track continues this year, including diversified entities and larger stream corpus. The target entity set is composed of 121 entities from Wikipedia and 20 entities from Twitter. KBA 2013 has augmented the stream corpus of KBA 2012, covering the time period from Oct. 2011 to Feb. 2013. Each document in the stream corpus contains several fields. Table <ref type="table" coords="1,342.13,471.44,4.61,7.86">1</ref> lists fields used in our work.</p><p>Table <ref type="table" coords="1,347.85,497.37,4.13,7.89">1</ref>: Document fields used in our CCR system Field Description stream id an unique identifier of the document clean visible plain text content of the document source source of the document timestamp a timestamp measured in seconds since the 1970 epoch</p><p>A CCR system is fed with the stream corpus in chronological order and outputs a confidence score in the range of (0, 1000] for each document-entity pair. The confidence score represents the relevance level between the document and the target entity. A cutoff value is varied from 0 to 1000 (stepsize = 10 in this paper) and the documents with scores above the cutoff are treated as positive instances by the system. Correspondingly, the documents with scores below the cutoff are negative instances. There are two measures defined by TREC KBA 2013 to evaluate the system performance: (i) max(F (avg(P ), avg(R))) and (ii) max(SU ). SU (Scaled utility) is a metric introduce in <ref type="bibr" coords="1,444.53,711.19,9.72,7.86" target="#b10">[9]</ref> to evaluate the ability of a information filtering system to separate relevant and irrelevant document. Given a cutoff, we could calculate P, R, F and SU respectively for each entity and obtain the macroaverage values of all entities.</p><p>There are two sub tasks of CCR in KBA 2013: (i) vital only: treating only vital documents as positive instances and nonvital as negative instances, and (ii) vital + useful: accepting both vital and useful documents as positive instances.</p><p>We submitted 10 runs to KBA CCR Track 2013, including 2 query expansion runs, 2 classification-based runs and 6 ranking-based runs. Query expansion runs, as our baselines, outperform the median and mean of all 140 submissions. In vital + useful task, our ranking-based run with burst feature achieves the best result. While in vital only task, classification runs are better than ranking-based runs in general, and one of them is the overall best run.</p><p>The rest of this paper is organized as follows: Section 2 introduces a pre-processing step to reduce the size of stream corpus. Next, we present our approaches in Section 3. Section 4 lists the features used in our supervised approaches in detail. Finally, we summarize the results of our submissions and conclude some insights in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRE-PROCESSING</head><p>Before relevance analysis for document-entity pairs, our CCR system contains a pre-processing step, including indexing and filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>In order to process the huge stream corpus efficiently, we utilize ElasticSearch to index the whole stream corpus. Elastic-Search is an open-source, Lucene-based text search engine<ref type="foot" coords="2,286.20,418.44,3.65,5.24" target="#foot_0">1</ref> .</p><p>We only care about 4 fields of each document: stream id, clean visible, timestamp and source. Table <ref type="table" coords="2,231.93,441.13,4.61,7.86">1</ref> describes the meanings of these fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filtering</head><p>It's too time-consuming and laborious to process all the documents in the stream corpus for each entity. According to the annotation analysis of KBA 2012, none of the document with zero mention of the target entity is annotated as central, and there are only 0.4% of the documents with zero mention of the target entity have been labeled as relevant <ref type="bibr" coords="2,53.80,549.19,9.20,7.86" target="#b7">[6]</ref>. So we filter the index through retaining as many relevant documents as possible. We construct a high-recall phrase query assuring that the retrieved documents should mention the target entity at least once, either exactly entity name or surface forms. Therefore, The prerequisite of filtering is expanding enough surface forms for each target entity.</p><p>For each target entity from Wikipedia, we extract the redirect<ref type="foot" coords="2,69.17,641.57,3.65,5.24" target="#foot_1">2</ref> names as its surface forms. For example, Geoffrey E. Hinton, who is a computer scientist in machine learning, owns the following redirect names in Wikipedia: Geoffrey Hinton, Geoff Hinton, Geoffrey E. Hinton, Geoffrey Everest Hinton.</p><p>For each target entity from Twitter, we add its display name into its surface form set. For the entity @AlexJoHamilton, we could acquire its display name Alexandra Hamilton via Twitter's APIs.</p><p>We construct a matchPhraseQuery<ref type="foot" coords="2,459.15,108.17,3.65,5.24" target="#foot_2">3</ref> with the target entity name and all its surface forms together and search against the index. Only the hit documents are processed in the following steps.</p><p>After the pre-processing step, the number of documents retained in the stream corpus decreases from 442,325,966 to 77,589. This indeed makes our CCR system more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPROACHES</head><p>We have tried 3 families of methods in our submissions to KBA 2013, including query expansion, classification and ranking-based methods. The query expansion method is an unsupervised baseline method, and the other two are supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query Expansion</head><p>Query expansion is an unsupervised baseline approach. For each entity, we construct a basic phrase query with its name and surface forms (see subsection 2.2). Listing 1 (Line. <ref type="bibr" coords="2,540.07,319.81,3.96,7.86" target="#b2">[1]</ref><ref type="bibr" coords="2,544.03,319.81,3.96,7.86" target="#b3">[2]</ref><ref type="bibr" coords="2,544.03,319.81,3.96,7.86" target="#b4">[3]</ref><ref type="bibr" coords="2,544.03,319.81,3.96,7.86" target="#b5">[4]</ref><ref type="bibr" coords="2,544.03,319.81,3.96,7.86" target="#b6">[5]</ref><ref type="bibr" coords="2,544.03,319.81,3.96,7.86" target="#b7">[6]</ref><ref type="bibr" coords="2,544.03,319.81,3.96,7.86">[7]</ref><ref type="bibr" coords="2,548.00,319.81,7.93,7.86" target="#b9">[8]</ref> shows the basic phrase query construction using java API to ElasticSearch. Although the basic query can hit the documents mentioning the entity names from the index, it neither can disambiguate ambiguous entities with a same name, such as basic element (company) and basic element (music group), nor can differentiate the relevance levels of the hit documents.</p><p>The most pervasive and effective approach to resolve name entity disambiguation is leveraging contextual information <ref type="bibr" coords="2,316.81,434.88,9.20,7.86" target="#b4">[3]</ref>. In this work, we expand the basic phrase query with contextual related entities extracted from three sources: target entities' Wikipedia or profile pages, annotation documents and existing citations in Wikipedia. For Wikipedia entities, we use JWPL APIs <ref type="bibr" coords="2,395.71,476.72,14.32,7.86" target="#b11">[10]</ref> to extract the anchor texts of inlinks in their Wikipedia pages as related entities. For Twitter entities, Stanford Named Entity Recognizer <ref type="bibr" coords="2,485.67,497.64,9.72,7.86" target="#b5">[4]</ref> is employed to recognize entities from their profile pages. Besides, the relevant (vital or useful) documents in the ground truth data is of great help to differentiate documents with different relevance levels. The related entities appear in a vital (useful) document can help us find more vital (useful) documents. So we also extracted related entities for target entities from annotation data. It is worth noting that we only extract related entities from documents annotated as vital or useful.</p><p>In addition, existing citations in Wikipedia entities' entry pages also contribute some related entities. For Twitter entities with few citations in their profile pages, we use the entities' displaying name to query in Google and crawl the top 5 hit documents as their pseudo citations.</p><p>After extracting related entities, we incorporate them into basic query and then search against the built index. The hit documents are treated as relevant documents and the rank- . s h o u l d ( matchPhraseQuery ( " clean_visible " , e n t i t y n a m e ) ) ; 4 // s u r f a c e f o r m s e t i s r e d i r e c t names s e t 5 f o r ( S t r i n g s u r f a c e f o r m : s u r f a c e f o r m s e t ) 6 b a s i c Q u e r y . s h o u l d ( matchPhraseQuery ( " clean_visible " , s u r f a c e f o r m ) ) ; 7 //make s u r e t h e h i t document match a s u r f a c e form a t l e a s t 8 b a s i c Q u e r y . minimumNumberShouldMatch ( 1 ) ; 9 // r e l s e t i s r e l a t e d e n t i t y s e t o f t a r g e t e n t i t y 10 f o r ( S t r i n g r e l e n t i t y : r e l s e t ) 11 b a s i c Q u e r y . s h o u l d ( matchPhraseQuery ( " clean_visible " , r e l e n t i t y ) )</p><p>Listing 1: Query Expansion in ElasticSearch ing scores returned by ElasticSearch are scaled to (0,1000] as the final confidence scores.</p><p>We submitted 2 query expansion runs: ECQ (EntityCen-tricQuery) and ECQUpdate. The difference between them is that we incorporate related entities extracted from pseudo citations into the query in ECQUpdate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification</head><p>CCR could be formulated as a binary classification task to differentiate relevant/irrelevant documents or vital/useful documents.</p><p>We submitted two classification runs: RFClassStrict and RFClassLoose. The former classifies the candidate documents into vital or useful, while the latter classifies the candidate documents into relevant (vital + useful) or irrelevant (neutral + garbage). We employ Random Forest classifier implementation in Weka toolkit <ref type="bibr" coords="3,186.17,404.71,9.71,7.86">[7]</ref> with default parameter settings.</p><p>In KBA CCR track 2012, most of the teams train a unique classifier for each target entity to exploit training data adequately. However, training data of KBA 2013 is not so sufficient. For some entities, there is no vital instance in training data. Therefore, it's unfeasible to train a unique classifier for each entity. Instead, we train a general classifier for the whole entity set with all the training instances. The features used are introduced in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning to Rank</head><p>CCR could also be deemed as a ranking problem because of the ordering of the relevance levels, i.e., vital &gt; usef ul &gt; neutral &gt; garbage. As demonstrated in <ref type="bibr" coords="3,221.03,566.78,9.20,7.86" target="#b2">[1]</ref>, ranking-based approaches have more potential than classification approaches on all evaluation measures. Therefore, we concentrated more on ranking-based approaches.</p><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FEATURES</head><p>In this section, we introduce the features used in our supervised approaches. <ref type="bibr" coords="3,406.37,530.36,9.71,7.86" target="#b3">[2]</ref> has summarized 4 types of useful features for CCR, including document features, entity features, document-entity features and temporal features. We adopt and enrich these features. Furthermore, we explore the citation features to improve the performance further.</p><p>All the features are listed in Table <ref type="table" coords="3,458.36,582.67,3.58,7.86">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Features.</head><p>For each document, we use some features to represent its basic characteristics, such as its length, publishing date and source.</p><p>Entity Features. There is only one entity feature, i.e. the number of related entities of the target entity. For each Wikipedia entity, its entry page is useful in profiling the target entity and filter relevant documents from stream corpus. Similarly, each entity from Twitter owns a profile page. All found in the entry or profile page are considered as related entities of a target entity, as introduced in subsection 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-Entity Features.</head><p>All the document-entity features are listed in the third block of Table <ref type="table" coords="4,221.65,121.90,3.58,7.86">2</ref>. Except the last four similarity features, all the other features are normalized by the document's length. Given a document-entity pair, if the entity owns an entry page in Wikipedia, we calculate cosine and jaccard similarities between its Wikipedia article and the document. If the entity is from Twitter, we calculate the two similarities between its profile page and the document instead.</p><p>Temporal Features. Because CCR is a time-dependent task, some kinds of temporal features have been investigated. <ref type="bibr" coords="4,283.20,238.46,9.71,7.86" target="#b9">[8]</ref> has tried statistics gathered on a sliding window over the past week as temporal features, such as the number of the entity is mentioned in previous documents. In our approaches, we zoomed the sliding window into past 10 hours instead. Besides, <ref type="bibr" coords="4,88.71,290.76,9.72,7.86" target="#b3">[2]</ref> presents that Wikipedia page view statistics is a useful signal to capture if something are happening around the target entity at a given time point. Based on our observation, when an entity's page views present a sudden ascending, which is named as burst, the number of vital and useful documents in stream corpus show a similar trend. one reason of this phenomenon may be that most of the vital edits of entity's page would trigger lots of views from the web. The magnitudes of Wikipedia page views of different entities varies depending on their popularity. To normalize the gap between different entities, we define a burst value for each document-entity pair as follows.</p><p>burst value = N * wpv(dn)</p><formula xml:id="formula_0" coords="4,185.22,430.87,107.68,17.45">N i=1 wpv(di)<label>(1)</label></formula><p>N is the total days the stream corpus covers. dn means the document is published on the n th day of the stream corpus. wpv(di) is the views of the target entity's Wikipedia page during i th day of the stream corpus.</p><p>Citation Features. For Wikipedia entities, there usually exist some citations in their entry pages. In our opinion, these existing citations are extremely valuable in identifying relevant documents. For each document, we calculate similarities (cosine and jaccard) between it and each cited article if the cited date is prior to the document's publishing date. For Twitter entities, we create pseudo citations as described in subsection 3.1 for each target entity. Not all entities have the same number of citations, but we need to set a fixed number of features for different entities to train a general model. Therefore, we use 6 measures to represent all the similarity features: max, mean, min, top1, top2, and top3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND DISCUSSIONS</head><p>As reported in <ref type="bibr" coords="4,119.76,690.26,9.20,7.86" target="#b6">[5]</ref>, ranking-based approach RFBurst 1 is the overall best run on vital+useful, and classification-based method RFClassStrict is the overall best run on vital only.</p><p>All the results of our runs are listed in Table <ref type="table" coords="4,496.29,57.64,3.58,7.86" target="#tab_1">3</ref>. We not only demonstrate the overall measures for all entities, the most primary measure of the performance, but also calculate these measures for Wikipedia and Twitter entities separately to evaluate these runs in a fine-grained level. Please note that the cutoffs to reach maximum of F (SU) for overall entity set and for separate entity set may be different, so the value of an overall measure is not always between the values of two separate measures.</p><p>Our two baselines (ECQ and ECQUpdate) outperform the overall mean and median of all submissions. It is not so hard to filter relevant documents from stream corpus when we expand basic phrase query with related entities. All the measures of ECQUpdate for Twitter entities are better than those of ECQ, which illustrates that pseudo citations for Twitter entities do work, although the overall performance is not improved explicitly. This may result from the few amounts of twitter entities in the whole entity set.</p><p>Almost all the classification and ranking-based approaches perform better than the two baselines in both tasks. The performance of RFDiffModel is better than that of RFUni-Model, revealing that Wikipedia and Twitter entities vary from each other in the CCR task. We should tackle them respectively to improve the performance further. we find that RFMultiModel do not improve the performance very much. This may be caused by the uncertainty of "enough" training data. We manually set a threshold, while different entities require different sizes of training data to train robust models. We could utilize data-dependent mixture techniques to select a more reasonable threshold for each entity in future. All the ranking-based approaches perform very similarly if the temporal features are not included in the feature set. Temporal features (burst value) could improve the ranking results as we speculate in Section 4.</p><p>To differentiate vital from useful, classification methods perform better than ranking methods. This reverses the conclusions on KBA 2012 data in <ref type="bibr" coords="4,428.98,465.61,9.20,7.86" target="#b2">[1]</ref>, in which the authors proves that ranking-based methods are better than classification methods. We do not prepare specialized features for vital/useful classification, which shares the same feature set with relevant/irrelevant classification.</p><p>[5] has pointed out that all submissions perform approximately on max(SU ) and none of them can achieve a max(SU ) over 0.333, which is corresponding to a run with no output. This finding suggests that separating vital and useful documents is the hardest part in the CCR task. Future work needs to be done to investigate better algorithms to recognize vital evidences in stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>The </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,39.24,58.73,251.15,7.86;3,39.24,68.06,4.61,7.86;3,143.68,68.06,63.52,7.86;3,39.24,77.39,4.61,7.86"><head></head><label></label><figDesc>1 B o o l Q e r y B u i l d e r b a s i c Q u e r y = Q u e r y B u i l d e r s 2 . boolQuery ( ) 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,53.80,211.25,503.26,508.05"><head></head><label></label><figDesc>have submitted 6 ranking-based runs. All the random forest ranking runs are implemented with RankLib 4 . The features used in ranking-based methods are mostly consistent with those in classification methods.</figDesc><table coords="3,53.80,211.25,502.13,490.41"><row><cell>temporal features.</cell></row><row><cell>RFMultiModel. Train a general RF ranking model for all</cell></row><row><cell>entities with all the features except temporal features. If</cell></row><row><cell>there exist enough training instances for an entity (more</cell></row><row><cell>than a pre-defined threshold), we also train a specific rank-</cell></row><row><cell>ing model for it. Therefore, for the entities with few train-</cell></row><row><cell>ing data, the general model is selected to make predictions.</cell></row><row><cell>While for entities with enough training data, two prediction</cell></row><row><cell>results by the general model and the specific model are com-</cell></row><row><cell>bined as the final result. RFMultiModel 1 is a parameter-</cell></row><row><cell>tuned version of RFMultiModel.</cell></row><row><cell>RFUniModel. Train a general Random Forest (RF) rank-</cell></row><row><cell>ing model for all the entities with all the features except</cell></row></table><note coords="3,54.25,709.42,3.65,5.24;3,58.40,711.83,206.74,7.47;3,316.81,369.96,240.24,9.44;3,316.81,381.61,239.10,7.86;3,316.81,392.07,239.11,7.86;3,316.81,402.53,162.38,7.86;3,316.81,434.21,239.11,9.44;3,316.81,445.87,239.11,7.86;3,316.81,456.33,239.11,7.86;3,316.81,466.79,239.11,7.86;3,316.81,477.25,89.88,7.86"><p><p><p><p>4 </p>http://sourceforge.net/p/lemur/wiki/RankLib/ RFDiffModel. Train two separate general RF ranking models for Wikipedia and Twitter entities respectively with all the features except temporal features, i.e. a Wikipedia ranking model and a Twitter ranking model.</p>RFBurst.</p>Train a general RF model for all the entities, including burst features. We also submitted a run, named as RFBurst 1, which incorporates the annotation data of KBA 2012 into our training data through treating central as vital and relevant as useful.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,316.81,627.50,239.11,91.55"><head>Table 3 :</head><label>3</label><figDesc>Ep) # of occurrences of the partial name of target entity E in document D N (D, E rel ) # of occurrence of the related entities in document D F P OS(D, E) position of first occurrence of entity E in document D F P OSn(D, E) F P OS(D, E) normalized by document length F P OS(D, Ep) position of first occurrence of partial name of entity E in document D F P OSn(D, Ep) F P OS(D, Ep)normalized by document length LP OS(D, E) position of last occurrence of entity E in document D LP OSn(D, E) LP OS(D, E) normalized by document length LP OS(D, Ep) position of last occurrence of partial name of entity E in document D LP OSn(D, Ep) LP OS(D, Ep) normalized by document length Spread(D, E) LP OS(D, E) -F P OS(D, E) Spreadn(D, E) Spread(D, E) normalized by document length Spread(D, Ep) LP OS(D, E [ p]) -F P OS(D, Ep) Spreadn(D, Ep) Spread(D, Ep) normalized by document length Simcos(D, WE) cosine similarity between document and entity's Wikipedia/Profile page Simjac(D, WE) jaccard similarity between document and entity's Wikipedia/Profile page Temporal Feature P reM ention(E, h) # of the entity E is mentioned in previous h hours before the timestamp of document Burst Value see Equation 1, only used in RFBurst and RFBurst_1 Citation Features Simcos(D, Ci) cosine similarity between document and existing citation Ci Simjac(D, Ci) jaccard similarity between document and existing citation Ci Results of official runs. All the measures are reported by official scorer with cutoff-step-size=10. Median and Mean are the median and mean of results aggregated from all the submissions in this year's KBA CCR track</figDesc><table coords="4,316.81,627.50,239.11,91.55"><row><cell>authors would express their great appreciations to the</cell></row><row><cell>members of Knowledge Mining Group at MSRA, especially</cell></row><row><cell>Jing Liu(Harbin Institute of Technology), Jyunyu Jiang (Na-</cell></row><row><cell>tional Taiwan University) and Chungyi Li (National Tai-</cell></row><row><cell>wan University) for their valuable suggestions and great</cell></row><row><cell>help. This work is funded by MSRA Visiting Young Fac-</cell></row><row><cell>ulty Program, the National Program on Key Basic Research</cell></row><row><cell>Project (973 Program, Grant No. 2013CB329605), National</cell></row><row><cell>Science Foundation of China (NSFC, Grant Nos.60873237</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,58.40,701.50,136.26,7.47"><p>http://www.elasticsearch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,58.40,711.83,220.83,7.47"><p>http://en.wikipedia.org/wiki/Wikipedia:Redirect</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,321.42,693.90,164.45,7.47;2,316.81,702.87,164.45,7.47;2,316.81,711.83,123.65,7.47"><p>http://www.elasticsearch.org/guide/ en/elasticsearch/reference/current/ query-dsl-match-query.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,82.76,665.01,17.90,7.86;5,141.42,665.01,16.37,7.86;5,176.49,665.01,16.37,7.86;5,212.47,665.01,16.37,7.86;5,253.33,665.01,16.37,7.86;5,288.40,665.01,16.37,7.86;5,324.24,665.01,16.37,7.86;5,364.95,665.01,16.37,7.86;5,400.02,665.01,16.37,7.86;5,436.00,665.01,16.37,7.86;5,476.85,665.01,16.37,7.86;5,511.92,665.01,16.37,7.86;5,547.76,665.01,16.37,7.86;5,76.76,675.97,29.92,7.86;5,141.42,675.97,16.37,7.86;5,176.49,675.97,16.37,7.86;5,212.47,675.97,16.37,7.86;5,253.33,675.97,16.37,7.86;5,288.40,675.97,16.37,7.86;5,324.24,675.97,16.37,7.86;5,364.95,675.97,16.37,7.86;5,400.02,675.97,16.37,7.86;5,436.00,675.97,16.37,7.86;5,476.85,675.97,16.37,7.86;5,511.92,675.97,16.37,7.86;5,547.76,675.97,16.37,7.86;5,80.97,686.93,21.49,7.86;5,141.42,686.93,16.37,7.86;5,176.49,686.93,16.37,7.86;5,212.47,686.93,16.37,7.86;5,253.33,686.93,16.37,7.86;5,288.40,686.93,16.37,7.86;5,324.24,686.93,16.37,7.86;5,364.95,686.93,16.37,7.86;5,400.02,686.93,16.37,7.86;5,436.00,686.93,16.37,7.86;5,476.85,686.93,16.37,7.86;5,511.92,686.93,16.37,7.86;5,547.76,686.93,16.37,7.86;6,72.28,57.64,220.62,7.86;6,53.80,68.10,166.22,7.86" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration" coords="6,137.50,57.64,155.40,7.86;6,53.80,68.10,63.87,7.86">Beijing Higher Education Young Elite Teacher Project</orgName>
		</author>
		<idno>Max .303 .300 .330 .277 .280 .292 .659 .660 .665 .570 .566 .604 Median .174 .179 .164 .255 .259 .233 .406 .382 .333 .423 .433 .389 mean .166 .172 .136 .137 .240 .224 .376 .433 .360 .425 .438 .364 61003168</idno>
		<imprint/>
	</monogr>
	<note>Grant No. YETP1198</note>
</biblStruct>

<biblStruct coords="6,58.28,89.88,96.81,10.75" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,102.47,206.59,7.86;6,72.59,112.93,185.53,7.86;6,72.59,123.39,201.58,7.86;6,72.59,133.85,174.54,7.86;6,72.59,144.31,215.92,7.86;6,72.59,154.77,117.12,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,199.15,102.47,80.03,7.86;6,72.59,112.93,169.91,7.86">Cumulative citation recommendation: classification vs. ranking</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ramampiaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,72.59,123.39,201.58,7.86;6,72.59,133.85,174.54,7.86;6,72.59,144.31,128.70,7.86">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;13</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="941" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,166.23,180.03,7.86;6,72.59,176.69,201.68,7.86;6,72.59,187.15,220.32,7.86;6,72.59,197.61,192.31,7.86;6,72.59,208.07,196.93,7.86;6,72.59,218.53,111.72,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,120.16,176.69,154.11,7.86;6,72.59,187.15,144.79,7.86">Multi-step classification approaches to cumulative citation recommendation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ramampiaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Takhirov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,236.34,187.15,56.58,7.86;6,72.59,197.61,192.31,7.86;6,72.59,208.07,130.42,7.86">Proceedings of the 10th Conference on Open Research Areas in Information Retrieval, OAIR &apos;13</title>
		<meeting>the 10th Conference on Open Research Areas in Information Retrieval, OAIR &apos;13<address><addrLine>Paris, France, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,229.99,218.62,7.86;6,72.59,240.45,211.65,7.86;6,72.59,250.91,204.17,7.86;6,72.59,261.37,195.80,7.86;6,72.59,271.83,218.37,7.86;6,72.59,282.29,208.11,7.86;6,72.59,292.76,109.12,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,126.30,229.99,164.92,7.86;6,72.59,240.45,98.98,7.86">Large-scale named entity disambiguation based on Wikipedia data</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,190.22,240.45,94.03,7.86;6,72.59,250.91,204.17,7.86;6,72.59,261.37,195.80,7.86;6,72.59,271.83,149.99,7.86">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,304.21,176.40,7.86;6,72.59,314.67,212.74,7.86;6,72.59,325.13,213.59,7.86;6,72.59,335.59,187.83,7.86;6,72.59,346.06,210.61,7.86;6,72.59,356.52,181.57,7.86;6,72.59,366.98,109.12,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,72.59,314.67,212.74,7.86;6,72.59,325.13,148.90,7.86">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,240.31,325.13,45.86,7.86;6,72.59,335.59,187.83,7.86;6,72.59,346.06,144.11,7.86">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,378.43,113.44,7.86;6,256.10,378.43,22.13,7.86;6,72.59,388.89,197.38,7.86;6,72.59,399.36,215.32,7.86;6,72.59,409.82,197.40,7.86;6,72.59,420.28,149.71,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,180.04,399.36,107.86,7.86;6,72.59,409.82,152.41,7.86">Evaluating stream filtering for entity profile updates for trec 2013</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tripuraneni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,243.76,409.82,26.24,7.86;6,72.59,420.28,119.51,7.86">TEXT RETRIEVAL CONFERENCE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,431.73,195.87,7.86;6,72.59,442.19,216.14,7.86;6,72.59,452.66,205.22,7.86;6,72.59,463.12,199.07,7.86;6,72.59,473.58,107.81,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,241.83,442.19,46.90,7.86;6,72.59,452.66,205.22,7.86;6,72.59,463.12,46.19,7.86">Building an Entity-Centric Stream Filtering Test Collection for TREC 2012</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,137.53,463.12,134.13,7.86;6,72.59,473.58,78.62,7.86">Proceedings of the Text REtrieval Conference (TREC)</title>
		<meeting>the Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,485.03,182.22,7.86;6,72.59,495.49,195.61,7.86;6,72.59,505.96,217.61,7.86;6,72.59,516.42,94.82,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,208.86,495.49,59.34,7.86;6,72.59,505.96,109.25,7.86">The weka data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,189.24,505.96,96.87,7.86">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,527.87,218.20,7.86;6,72.59,538.33,207.83,7.86;6,72.59,548.80,185.28,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,231.22,527.87,59.57,7.86;6,72.59,538.33,132.75,7.86">Lsis/lia at trec 2012 knowledge base acceleration</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">B</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,223.75,538.33,56.66,7.86;6,72.59,548.80,156.09,7.86">Proceedings of the Text REtrieval Conference (TREC)</title>
		<meeting>the Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,560.25,210.91,7.86;6,72.59,570.71,220.31,7.86;6,72.59,581.17,20.96,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,193.44,560.25,90.06,7.86;6,72.59,570.71,46.95,7.86">The trec 2002 filtering track report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,137.85,570.71,148.89,7.86">TEXT RETRIEVAL CONFERENCE</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,592.63,198.46,7.86;6,72.59,603.09,189.04,7.86;6,72.59,613.55,202.85,7.86;6,72.59,624.01,207.18,7.86;6,72.59,634.47,171.13,7.86;6,72.59,644.93,49.42,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,228.21,592.63,42.84,7.86;6,72.59,603.09,189.04,7.86;6,72.59,613.55,40.70,7.86">Extracting lexical semantic knowledge from wikipedia and wiktionary</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,131.92,613.55,143.52,7.86;6,72.59,624.01,203.05,7.86">Proceedings of the 6th International Conference on Language Resources and Evaluation</title>
		<meeting>the 6th International Conference on Language Resources and Evaluation<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
	<note>electronic proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
