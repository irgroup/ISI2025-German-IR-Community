<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.47,102.08,361.06,15.12;1,231.67,124.00,148.65,15.12">University of Waterloo at the TREC 2013 Temporal Summarization Track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute/Calcul Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">Google Founders Grant</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,103.17,156.49,79.26,10.48"><forename type="first">Gaurav</forename><surname>Baruah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.31,156.49,99.16,10.48"><forename type="first">Rakesh</forename><surname>Guttikonda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.35,156.49,78.27,10.48"><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.00,156.49,89.10,10.48"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.47,102.08,361.06,15.12;1,231.67,124.00,148.65,15.12">University of Waterloo at the TREC 2013 Temporal Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">601FC1686C39424000F6E77E404A8E02</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Waterloo participated in the Temporal Summarization Track at TREC 2013 and submitted 8 runs for the Sequential Update Summarization Task. Methods like query likelihood ranking, pseudo relevance feedback, BM25 and cosine similarity, as well as, algorithms for passage retrieval and term expansion using distributional similarity to a set of seed words, were used for returning relevant sentences from a stream of time-ordered documents. Higher scores relative to the average score for all submitted runs were achieved on the Latency Comprehensiveness Metric (returning as many nuggets as possible), however, submitted runs performed poorly on the Expected Latency Gain Metric (speediness of updates).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Temporal Summarization Track allows researchers to investigate the problem of retrieving relevant material from a collection that is dynamic in terms of size and is ordered by time. This closely reflects real world applications in the way documents are generated (on the web) and then collected and processed by information retrieval systems. In addition, by enforcing the retrieval unit to be a sentence, the track encourages researchers to develop algorithms that not only are capable of handling highvolume document streams but are also adept at finding specific highly relevant content within the stream.</p><p>Although iterations of the Microblog Track <ref type="bibr" coords="1,273.38,629.51,10.52,8.74" target="#b4">[5]</ref> address the problem of a time ordered document stream, the documents are short blurbs of at most 140 characters. The collection used for the Temporal Summarization Track is much more diverse and contains web documents of various lengths and content types like news articles, blogs and forums. The text in the documents was also tagged with Named Entities, which could possibly be used for improving retrieval performance.</p><p>Our motivations for participating in the track were manifold. We wanted to understand: how to work with a time-ordered document stream; if the methods used for past Microblog tracks would work for Temporal Summarization; if fundamental algorithms would return relevant sentences with adequate performance on the metrics. We were also driven by the fact that the track's task of providing speedy news updates has direct real world applications.</p><p>Techniques like cosine similarity, query likelihood ranking, pseudo-relevant feedback, passage retrieval, Okapi BM25 and term expansion using distributional similarity to a set of seed words, were employed to find and rank relevant sentences from the corpus. The collection was processed and converted to a form that was flexible enough to support the application of the techniques. It was found that our approaches find relevant sentences (within a given time-frame), however, getting speedy/early relevant updates may require a different approach than the one described in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Track Description</head><p>The Temporal Summarization Track at TREC 2013 <ref type="foot" coords="1,310.98,562.37,3.97,6.12" target="#foot_0">1</ref> , aims to return short relevant updates about an event from a time-ordered stream of documents. TREC 2013 is the first iteration for this track and consisted of two tasks. Task 1, Sequential Update Summarization (SUS), required finding relevant and novel updates (sentences) for an event. Task 2, Value Tracking, required estimating values for a particular attribute for an event.</p><p>A participant was required to simulate a system as follows: Input1: a time ordered stream of documents Input2: topic description with query (event), start &lt;event&gt; &lt;id&gt;1&lt;/id&gt; &lt;start&gt;1329910380&lt;/start&gt; &lt;end&gt;1330774380&lt;/end&gt; &lt;query&gt;buenos aires train crash&lt;/query&gt; &lt;type&gt;accident&lt;/type&gt; &lt;locations/&gt; &lt;deaths/&gt; &lt;injuries/&gt; &lt;/event&gt; Figure <ref type="figure" coords="2,102.35,210.21,3.87,8.74">1</ref>: Example Query: "buenos aires train crash". time, end time, type of event ∈ {accident, bombing, earthquake, shooting, storm}, attributes ∈ {deaths, displaced, financial impact, injuries, locations}. Output: Task 1 : Sentences relevant to the query between start and end times Task 2 : Estimate of the values for the attributes in the query as time progresses. Constraints: we cannot use corpus statistics (or external corpora) that are in the future with respect to the query start time.</p><p>Figure <ref type="figure" coords="2,115.49,374.88,4.98,8.74">1</ref> shows a query of type accident. The "user" wants to be updated about attributes like locations, deaths, injuries for the given accident. The query spans 10 days (240 hours) starting 2012-02-22-11. A training topic "iran earthquake" (referencing the 2012 East Azerbaijan earthquake), along with relevant nuggets was provided by the track organizers.</p><p>Two metrics, Expected Latency Gain (ELG) and Latency Comprehensiveness (LC), were developed by the track organizers to measure the quality of runs. ELG (when only considering binary relevance) can be thought of as similar to traditional precision except that runs are penalized for delaying the emission of relevant updates. LC is analogous to traditional recall and it measures the coverage of relevant nuggets in a run. The University of Waterloo submitted 8 runs for the SUS task. We achieved above average performance with respect to LC for most of the runs, but performed poorly with respect to ELG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Preliminaries</head><p>Described in this section are the steps that were taken to process the KBA stream-corpus<ref type="foot" coords="2,219.39,658.89,3.97,6.12" target="#foot_1">2</ref> , the time-ordered document collection for the track. The corpus was converted to a TREC-style document formatting and &lt;doc&gt; &lt;docno&gt;...&lt;/docno&gt; &lt;lang&gt;..&lt;/lang&gt; &lt;abs_url&gt;...&lt;/abs_url&gt; &lt;original_url&gt;...&lt;/original_url&gt; &lt;time&gt;...&lt;/time&gt; &lt;tagger&gt;...&lt;/tagger&gt; &lt;sentences&gt; ... &lt;/sentences&gt; &lt;/doc&gt; all documents within the query time were scored using query likelihood model. Sentences were extracted for output only from those documents that had query likelihood scores above a specified threshold. The following subsections elaborate on our corpus preprocessing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus Collection and Preprocessing</head><p>The KBA Corpus was downloaded locally in late May 2013. Once the corpus was secured, the C++ thrift sample from https://github.com/trec-kba/ streamcorpus was modified to allow an examination of the content provided by the Thrift format. On examination of the Named Entity Tags in the document text, it was seen that some tagged elements did not appear to correlate well with their intended meaning. Therefore, these tags were ignored during preprocessing as they could also be generated at a later stage. Further, it was decided that the document chunks would be converted from their Thrift format to a TREC-style format to facilitate easier processing and avoid the Thrift processing overhead. Figure <ref type="figure" coords="2,523.62,545.92,4.98,8.74" target="#fig_0">2</ref> illustrates the TREC-style format that each document was converted to. Note that: &lt;time&gt; refers to the Epoch timestamp, &lt;tagger&gt; refers to the sentence tagger, and &lt;sentences&gt; is a list of the sentences in the document, such that each sentence in the Thrift format corresponds to a line in the sentences field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Initial Wumpus Attempt</head><p>Initially, the search engine of choice was the Wumpus Search Engine 3 , which implements the ranking algorithm described in Section 2.4 (as well as others) and various forms of pseudo-relevance feedback (PRF). However, there were three main issues in getting the KBA corpus to work with Wumpus:</p><p>(1) Wumpus would not index some documents of the type MAINSTREAM NEWS and it was uncertain how such removal would affect run performance, (2) duplicate document identifiers are present in the corpus and it would have required significant preprocessing to remove them and allow Wumpus to index the collection, (3) Wumpus indexes documents in the order they were received and accordingly would either require extensive preprocessing of the hourly blocks or delaying return to the end of the hour. Due to these issues, it was decided that Wumpus should be replaced with our own solution to facilitate formulation of runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hour-Wise Query Likelihood with Dirichlet Smoothing</head><p>The time-ordering of documents was a major hurdle to overcome when using the corpus. Even if an index could have been built with Wumpus, it could have been difficult to limit the collection statistics to times before the start times of each query. A further complication was that the documents in the collection, though grouped together in hours, were not ordered within the hour.</p><p>The following procedure outlines our process for generating runs:</p><p>1. Rank documents using query likelihood.</p><p>2. Extract ranked documents (above specified threshold) from the collection (Document Set S).</p><p>3. Process ranked documents in order of increasing time stamps and extract relevant sentences using various algorithms.</p><p>4. Construct runs using the extracted sentences.</p><p>To facilitate techniques like PRF or re-ranking, as well as steps 1 and 2, we converted the collection to a "reduced" form with:</p><p>i Each hour's documents reduced to their termfrequency feature vectors in one file, with each line containing features of one document.</p><p>ii Each hour's documents' metadata in a file with each line containing doc-id, source .gz file of the document, document timestamp and document length.</p><p>iii Each hour's vocabulary and term-frequency (hour-term-counts) in a file.</p><p>iv Other statistics of interest like the number of terms in the collection (l C ) up until that hour and the number of terms in each hour (l H ), were also noted.</p><p>This reduced dataset (approximately 600 GB compressed) helped us in computing (using cumulative sum of hour-term-counts) the collection statistics up to a query's start time and continue on to the query's end time easily. Of course, this resulted in the collection statistics being updated every hour and not for every time-ordered document in the stream as it becomes available to rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Computing hour-wise Document Scores</head><p>Language Modeling with Dirichlet smoothing (LMD) was used to score documents with respect to the query (as described in section 9.3 of Büttcher et al. <ref type="bibr" coords="3,310.98,314.97,10.30,8.74" target="#b0">[1]</ref>), as shown below:</p><formula xml:id="formula_0" coords="3,310.98,336.96,229.02,37.75">score LM D = t∈q q t •log 1 + f t,d µ • l C l t -n• 1 + l d µ<label>(1)</label></formula><p>where, t is the term, q is the query term vector, q t is the term frequency of t in the query, f t,d is the term frequency of t in document d, l C is the length of the collection (the total number of times all terms appear in the collection), l t is the number of times the term t appears in the collection, and µ is the Dirichlet smoothing factor. For our experiments we set µ = 1000.</p><p>Given the query, its start time and end time, and the reduced corpus, a query.score file was generated for each hour between the start and end times containing for each document in that hour: its document id, document timestamp, and score LM D . The following algorithm was used to compute LMD and collection statistics:</p><p>for each hour from collectionStart_time \ until query start_time: l_t += hour-term-counts(t) l_C += l_H end for for each hour from query start_time \ until query end_time: for each document in hour: compute LMD score for document write out LMD score to query.score file end for update l_t and l_C for the hour end for It was found during testing that the corpus had duplicate document identifiers. Further, the content within each duplicate was different, resulting in different scores. It was decided that all documents that shared document identifiers would be ignored, even if they were relevant. Hour-wise LMD worked out favourably here because it allowed duplicate documents to be easily identified and ignored using the query.score files.</p><p>Once the documents were ranked for a query, all documents with score LM D &gt; 0 were extracted from the corpus for further processing (document set S). A score greater than zero would mean that the document contains query terms with a reasonable frequency while being of reasonable length. Note that the threshold was not set after observing the scores of all documents, or to ensure a definite number of documents in the result set. Rather, the aim was to find all documents that were likely to be relevant and hence the cut-off threshold of 0. An arbitrary score cut-off does not break the system simulation as specified by the track guidelines, whereas a fixed number of top ranked documents for every hour would.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs CosineEgrep and NormEgrep</head><p>The runs in this section were intended to be baseline approaches and aimed to explore the differences in the two similarity measures. Accordingly, both runs for each hour ran egrep over documents in S and extracted all lines that contained any word in a list of synonyms with the event type. The list of synonyms was hand-crafted by one of the authors. Table <ref type="table" coords="4,72.00,497.63,4.98,8.74" target="#tab_0">1</ref> lists the queries provided to egrep. Note that for the submitted runs the ignore case option of egrep was not used. The results of using the ignore case option are reported in Table <ref type="table" coords="4,199.68,533.49,3.87,8.74" target="#tab_6">8</ref>. Inclusion of lines returned through the ignore case option, increased the number of sentences for each topic across the board (except Topic 7) with substantial increases for Topics 3, 5, and 6, which saw 1M+ increases.</p><p>Once the sentences were collected from S, they were processed first in order of their document id and then their order within a document. A sentence was selected to be emitted for a topic if it was sufficiently dissimilar to the previously emitted sentences. Two similarity metrics were used on a feature vector comprising of the following: the frequency of each lower case character, the frequency of each upper case character, and the frequency of each digit (e.g. 0-9). This results in a feature vector of 62 elements which is the basis for the 2-norm based similarity metric. Sperling et al. specify the 2-norm based metric in their work on identifying duplicates in legal ediscovery <ref type="bibr" coords="4,354.67,393.62,9.96,8.74" target="#b5">[6]</ref>. The formula used in this work was as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event</head><p>x -q &gt;= γ q (2)</p><p>where x is the feature vector corresponding to a candidate sentence, q the feature vector for a previously emitted sentence, and γ affects how dissimilar a sentence must be before a document is emitted. Sperling et al. prescribe a γ of 0.25 or 0.5, however, in some very crude experiments with the training topic a value of 0.75 was selected as other values appeared to either be too restrictive or too permissive. Although, a more rigorous study would need to be conducted to make any conclusions.</p><p>The second similarity metric was the standard cosine similarity metric such that a cosine of greater than 0.5, which corresponds to a 60 • between the two vectors, was used to indicate that a sentence was too similar. However, little empirical testing was done to select this value aside from some very rudimentary experiments with the training topic. It is worth acknowledging that this threshold is also very conservative with respect to the norm-based metric and that both similarity metrics are conservative in general.</p><p>For both metrics, the score of a sentence was relative to its emission, i.e., the first sentence emitted from the first document processed had the highest score and subsequent scores would be monotonically decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs UWMDSqlec2t25 and UWMDSqlec4t50</head><p>The problem of identifying a "highly" relevant sentence from a document lends itself to the passage scoring method described in section 9.6.1. of Büttcher et al. <ref type="bibr" coords="5,138.51,183.62,9.96,8.74" target="#b0">[1]</ref>. Given the length of the collection (counting all terms) l C , the number of times the term t appears in the collection l t , the score for the passage can be computed as</p><formula xml:id="formula_1" coords="5,99.04,242.26,197.73,20.06">score cover = t∈q (log(l C /l t )) -m • log(l) (<label>3</label></formula><formula xml:id="formula_2" coords="5,296.77,242.26,4.24,8.74">)</formula><p>where q is the subset (cover) of the query terms in the passage, m is the number of query terms in the passage (|q |), l is the length of the passage. For the track's problem description, we consider each sentence as a passage. As was done for score LM D , sentences with score cover ≤ 0 were ignored and considered not relevant (as described in section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of Process</head><p>This subsection describes the procedure that was followed to generate runs UWMDSqlec2t25 and UMD-Sqlec4t50.</p><p>For each query (event/topic),</p><p>1. Generate a background language model B for the query. This language model would be used to generate expansion terms through pseudorelevance feedback (PRF).</p><p>2. Get top 20 documents D h in (current) hour h, for the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Get top expansion terms E h</head><p>for hour h, using the document set D h .</p><p>4. Use top k terms from E h to identify relevant sentences from documents in hour h + 1 of the document stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Remove duplicate sentences.</head><p>The parameter k sets the number of terms chosen to expand the query. In addition, only those documents that have at least c (query + expansion) terms in them where chosen. This helps to prune documents with low number of query and/or expansion terms. It was the case that c = 2 and k = 25 for run UWMDSqlec2t25 and c = 4 and k = 50 for run UMDSqlec4t50.</p><p>We chose to generate expansion terms every hour in order to capture the possible changes in the content of the documents for that hour. For instance, for the training event "iran earthquake", the first few hours could be about the fact that the earthquake occurred, at a particular location, with a particular magnitude. The next few hours may focus on the updates on number of killed, injured or displaced people, updates on magnitudes of aftershocks and regions affected, along with the response from the government and rescue units. Later news may be about foreign aid, damages to property. Later still, we may find articles about the economic impact of the earthquake and government plans for rebuilding and resettlement. As important updates change every hour, the top expansion terms for each hour may be expected to change as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Term Expansion for Ranking Sentences</head><p>Pseudo-relevance feedback was performed with the top 20 documents (as scored by score LM D ) considered relevant (prescribed in Büttcher et al. <ref type="bibr" coords="5,494.22,338.54,9.96,8.74" target="#b0">[1]</ref>, section 8.6.2). We tried various approaches for query expansion and term selection based on (i) choosing terms from the (pseudo-) Relevance Model (section 7.3.2 of Croft et al. <ref type="bibr" coords="5,364.99,386.36,10.30,8.74" target="#b3">[4]</ref>), (ii) choosing terms using KL divergence (Carpineto et al. <ref type="bibr" coords="5,414.12,398.31,9.96,8.74" target="#b1">[2]</ref>, section 9.4 of Büttcher et al. <ref type="bibr" coords="5,328.59,410.27,10.30,8.74" target="#b0">[1]</ref>), and (iii) choosing terms based on IDF-like weights (a modification of the method described in section 8.6.1 of <ref type="bibr" coords="5,379.39,434.18,10.30,8.74" target="#b0">[1]</ref>). We select a term t for expansion, if it has a high value for n t,r • w t</p><p>where, n t,r is the number of times the term appears in relevant documents and w t = log(l C /l t ). l C is the length of the collection up until the current hour in the time-ordered document stream and l t is the number of times the term occurs in the collection up until the current hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Background Language Model</head><p>Ideally, the collection model should be the background model against which we compute w t . However, the document collection is time-ordered and as we go further along the collection, we should get better estimates of a term's importance. In the interest of computation time, we adopted an approximate background model B rather than the whole collection model. Given that the queries spanned 10 days, we started building our background model 20% time (2 days) in advance. For hour h, we scored all documents using query likelihood (score LM D ), and chose the top 20 documents. The terms (and term counts) from these documents were added to B. l B , the total number of terms in B was updated with the total number of terms in hour h. For hour h + 1, the terms and terms counts were cumulatively added to B. This process resulted in a background model that grows with time which may produce better expansion terms as time progresses.</p><p>As we encounter new terms for every hour processed, to calculate the true l t for each new term, we need to go back to start of collection and cumulatively add their hour-term-counts, because of our hour-wise index layout (section 2.3). In the interest of time, we approximated w t = log(l B /l t B ), where l t B is the number of times the term t occurs in B. In hindsight, this may have been a poor approach and we should have used the complete collection model. It may be useful to compare our formulation w t with the Robertson/Spärk Jones weighting formula or BM25 weights for terms, now that we have the test collection for the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Choosing a Query for Term Expansion</head><p>The problem definition for the track specifies upto five attributes of an event for which we need to find relevant sentences. Therefore, we are now in a position to choose a "seed" query that would help us to get good expansion terms through PRF. Table <ref type="table" coords="6,275.86,425.06,4.98,8.74" target="#tab_1">2</ref> lists the various possible seed queries for the training topic (query: "iran earthquake", type="earthquake"). For each of these queries, top 20 documents were identified, relevance models were created and expansion terms were generated. We observed that top expansion terms changed every hour (See Tables <ref type="table" coords="6,179.27,509.08,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="6,209.32,509.08,4.98,8.74" target="#tab_3">4</ref> for an example on the changes in expansion terms for hours 2012-08-11-18 and 2012-08-11-21 respectively). Queries of type "specific all attributes" and "generic all attributes" were found to give better expansion terms across hours. Recall that we use the expansion terms obtained in hour h to score sentences in hour h + 1 (section 4.1). The idea was to capture relevant sentences even as the content about the topic/event changes with time, i.e., each hour (or time period) may report about different aspects of the event and we wanted to capture terms relevant to the time period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Variability in Expansion Terms</head><p>It was observed that the (top 20) documents obtained by PRF do not necessarily contribute good expansion terms when the seed query is of type "specific all at- tributes" (SAA). However, the query of type "generic all attributes" (GAA) does provide some terms related to the event across hours. Tables <ref type="table" coords="6,482.90,369.37,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="6,510.41,369.37,4.98,8.74" target="#tab_3">4</ref> show the top 10 terms for query term expansions with SAA and GAA queries for the training topic "iran earthquake".</p><p>In order to maintain a balance between the generic and the specific queries, we turned to Reciprocal Rank Fusion (RRF) (Cormack et al. <ref type="bibr" coords="6,475.45,448.55,10.79,8.74" target="#b2">[3]</ref>) to fuse the list of expansion terms generated by seed queries of type SAA and GAA. The third column in Tables <ref type="table" coords="6,535.02,472.46,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="6,330.78,484.42,4.98,8.74" target="#tab_3">4</ref> shows the top 10 terms of the RRF fused list of expansion terms. Table <ref type="table" coords="6,347.54,515.79,4.98,8.74" target="#tab_2">3</ref> shows an example of the SAA query expansion terms showing specific locations (Haris, Varzaqan, Ahar) which are not top terms for expanded GAA query. Whereas, Table <ref type="table" coords="6,448.74,551.65,4.98,8.74" target="#tab_3">4</ref> shows an example where GAA query expansion terms may be useful. Terms like ANSS<ref type="foot" coords="6,389.59,573.99,3.97,6.12" target="#foot_2">4</ref> , RMSS<ref type="foot" coords="6,429.73,573.99,3.97,6.12" target="#foot_3">5</ref> , shakemap <ref type="foot" coords="6,485.00,573.99,3.97,6.12" target="#foot_4">6</ref> , may help provide further detailed information related to earthquakes in general. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Weighting Term Importance with RRF Scores</head><p>To account for the revised ranking of terms we modified equation 3 as</p><formula xml:id="formula_4" coords="7,76.32,374.91,220.39,26.80">score cover = t∈q log l C l t × rrf (t) -m • log(l)</formula><p>(5) where, rrf (t) is the RRF value for the term t computed as</p><formula xml:id="formula_5" coords="7,138.81,438.49,162.21,26.65">rrf (t) = i 1 k + r i (t)<label>(6)</label></formula><p>where, 1 ≤ i ≤ number of lists to fuse, r i (t) is the rank of the term t in list i, and prescribed parameter value of k is 60. The revised formula 5 for score cover drives down the IDF-like weights for all terms, but does so based on the RRF score for the term. This ensures that terms having higher relevance/importance to the query contribute more to the sentence score. The original query terms were treated as having the highest possible RRF value (1/61), so that their importance is not devalued as compared to the expansion terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Building the Run</head><p>Many duplicate documents were found (and hence duplicate sentences) in the stream corpus and as such we need a deduplication module to eliminate duplicate sentences. We simulate deduplication (while adhering to the track's guidelines) by using the Unix commands sort and uniq. All updates were sorted first by sentence and then by time. Then exact duplicate sentences were eliminated using uniq. In a real system, one may keep all updates in a map or lookup table in order to avoid emitting exact duplicates of previous updates. While working with the training topic, it was seen that there are many irrelevant sentences that score high on score cover , due to the presence of high ranking expansion terms in them. To mitigate this problem, it was reasoned that a "relevant" document may have relevant material spread throughout its content. Therefore, score cover for a sentence could be weighted with the score LM D for the document from which the sentence was selected. In effect, the sentences from low ranking documents would receive a lower confidence rating and sentences from high ranking documents would receive a higher confidence rating, with score cover × score LM D .</p><p>Finally, as described in section 4.1, the two parameters k and c need to be tuned. Table <ref type="table" coords="7,505.56,545.26,4.98,8.74">5</ref> shows the number of unique sentences obtained for various setting of the parameters. We opted to emit more number of sentences in order not to miss too many updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runs rg1, rg2, rg3 and rg4</head><p>The rg runs (rg1, rg2, rg3, rg4), were generated from the set of documents scored by query likelihood model described in section 2.4. Sentences from document set S were used to generate the runs. In this section, the methods used to generate the runs are reviewed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scoring sentences</head><p>Okapi BM25 (eq.7) was used as the ranking function to score the sentences in the document with respect to the query.</p><formula xml:id="formula_6" coords="8,81.96,350.34,219.06,41.40">score sentence = t∈q q t × f t,d (k 1 + 1) k 1 ((1 -b) + b(l d /l avg )) + f t,d × w t (7)</formula><p>where, f t,d is the frequency of term t in sentence d, w t is the IDF of term t, l d is length of sentence, l avg is the average sentence length (found to be 34) and parameters k 1 = 1.2, b = 0.75 (as prescribed by Büttcher et al. <ref type="bibr" coords="8,139.14,452.46,10.30,8.74" target="#b0">[1]</ref>).</p><p>Since the query terms are fewer and the sentences are relatively short (l avg = 34) compared to document lengths, query expansion techniques were used to improve the recall of the results. Hence, the technique used for finding expanded words forms one of the key components of the approach and is outlined in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Distributional Similarity Based Term Expansion (DSTE)</head><p>As seen earlier in section 1.1, a query can belong to one of the following event types: accident, bombing, earthquake, shooting, storm. For each of the event types, seed words (around 30 words per event type) were found manually from prior Wikipedia 7 articles of each event type. A list of training topics was also created, one for each event type. Top K (=10000) sentences were then retrieved from S for each training topic, using the BM25 score and the seed words list. These top K sentences, along with the seed words, are given as input to an algorithm that uses distributional similarity to find expansion terms <ref type="bibr" coords="8,340.14,361.36,9.96,8.74" target="#b6">[7]</ref>. These newly identified expansion words along with the initial seed words and query terms, constitute the expanded words list (q in equation 7) for calculating the score sentence . Table <ref type="table" coords="8,488.63,397.22,4.98,8.74" target="#tab_4">6</ref> shows examples for seed words and their expansions for the training topic ("iran earthquake").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Modified score function</head><p>Similar to the previously discussed runs (section 4.7), it was found that score sentence alone isn't sufficient to judge the relevance of a sentence with respect to the query. It was observed that there were quite a few sentences which contain the expanded words, but are from very low scoring documents which are not relevant to the query. For example, there were earthquakes in some parts of China during the same time as the Iran earthquake, which is the training topic. With only the sentence scores, the sentences from documents related to the earthquake in China would be as relevant as the sentences from Iran earthquake, which is unlikely to be the case. Also, the document score of a document related to the Chinese earthquake is much smaller than that of a document related to the Iranian earthquake. In an attempt to combat this, we combine the score sentence and score LM D to ensure that we return relevant sentences from relevant documents. It was seen that the simple product of these two scores would in general lead to better updates, i.e., score combined = score sentence × score document (8) This function needs to be evaluated with a series of experiments which we will pursue as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sentence Selection Criteria</head><p>To avoid redundancy in updates, and to improve their quality, we need to shortlist sentence updates as well as avoid duplicate sentences.</p><p>A rigid cutoff for score sentence to shortlist sentences was not used due to the diversity in the documents returned every hour. Hence, an incremental cutoff score (score cutof f ) for score sentence was used every hour to decide whether a sentence should be included in the list of updates.</p><p>The following algorithm was used to generate the list of updates: The deduplication step kicks in while adding a sentence to the update list. If more than 90% of the words in the current sentence are covered in any of the previous update sentences, then it was discarded as a duplicate.</p><formula xml:id="formula_7" coords="9,72.00,322.49,88.92,8.30">S_h = 5000; //max</formula><p>Table <ref type="table" coords="9,348.27,440.28,4.98,8.74" target="#tab_5">7</ref> lists the the number of sentences and documents shortlisted for each rg run. The incremental minimum cut-off scores change every hour based on the score of the S th h sentence of the previous hour. Run rg2 differs from rg1 in that, the length of the sentences was normalized. While testing on the training topic, it was observed that the sentence scores alone do not account for the verbosity of the sentence, even with the BM25 score (after changing b parameter). Due to this, the sentence score was multiplied by log(1+ lavg l d ); as explained in document length normalization in Büttcher et al. <ref type="bibr" coords="9,437.76,574.29,9.96,8.74" target="#b0">[1]</ref>. Similar to rg2, rg4 performs sentence length normalization, whereas rg3 does not. Runs rg2 and rg4 try to increase the ELG of the results by reducing the penalty for verbosity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion of Results</head><p>While neither of the baseline runs (CosineEgrep and NormEgrep) performed particularly well, this is not overly surprising given that both are quite conservative in the number of sentences emitted per topic; approximately 12 on average for CosineEgrep and 151 for NormEgrep. This disparity makes it apparent as to why NormEgrep likely has a higher LC than CosineEgrep. However, even though both emit relatively few sentences per topic the ELG is quite low and likely stems from the simplistic sentence selection measure. It is interesting that introducing more sentences with egrep's ignore case option decreases the performance of the norm-based similarity measure and likely has to do with the introduction of non-relevant sentences.</p><p>The performance of runs UWMDSqlec2t25 (0.5375) and UMDSqlec4t50 (0.5304) with respect to the LC metric, is above the average reported amongst all submitted runs to the track (0.2996). In addition, the maximum scores for these runs (0.979 and 0.9872 respectively) are close to the reported maximum of 0.999. Both these runs performed poorly with respect to the ELG metric. This could be in part because of the differences in the content of the relevant documents as time progresses. Comprehensive (news) articles (or documents) would score higher with score LM D , however they would likely occur later in the time-ordered document stream. The confidence rating of score cover × score LM D would give more importance to sentences from such articles.</p><p>Table <ref type="table" coords="10,109.53,393.27,4.98,8.74" target="#tab_6">8</ref> shows that high recall (LC) is achieved in all rg runs, which can be attributed to the expanded words for the query which were obtained using the DSTE algorithm (section 5.2). On the other hand, the rg runs have performed poorly with respect to ELG, which could be because of the weak deduplication algorithm, and due to the penalty of missing good sentences in low scoring documents which are early. Improvements in these two areas may help in better performance as measured by ELG for the rg runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Participation in the Temporal Summarization Track has provided an opportunity to explore various approaches to the task of identifying relevant sentences in a timely manner. Many of the runs submitted performed quite well with respect to Latency Comprehensiveness (i.e. recall), which may be attributed to the use of standard retrieval techniques. However, all our runs performed poorly with respect to Expected Latency Gain and methods of improving this while not decreasing Latency Comprehensiveness remain to be investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,310.98,222.72,229.02,8.74;2,310.98,234.67,34.09,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Outline of the TREC-style format of Documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,141.16,73.93,5.09,8.77;8,165.09,73.93,6.05,8.77;8,194.13,73.93,33.58,8.77;8,187.54,85.89,46.75,8.77;8,140.84,98.24,5.73,8.77;8,162.39,98.24,11.46,8.77;8,199.46,98.24,22.91,8.77;8,141.22,110.23,4.98,8.74;8,163.13,110.23,9.96,8.74;8,200.96,110.23,19.93,8.74;8,141.22,122.18,34.37,8.74;8,200.96,122.18,19.93,8.74;8,141.22,134.14,4.98,8.74;8,163.13,134.14,9.96,8.74;8,200.96,134.14,19.93,8.74;8,140.84,146.06,5.73,8.77;8,162.39,146.06,11.46,8.77;8,199.46,146.06,22.91,8.77;8,141.22,158.05,34.37,8.74;8,200.96,158.05,19.93,8.74;8,138.73,170.00,34.37,8.74;8,200.96,170.00,19.93,8.74;8,138.73,181.96,34.37,8.74;8,200.96,181.96,19.93,8.74;8,138.73,193.91,36.86,8.74;8,200.96,193.91,19.93,8.74;8,72.00,215.78,229.02,8.74;8,72.00,227.73,229.01,8.74;8,72.00,239.69,229.02,8.74;8,72.00,251.64,68.77,8.74"><head>Table 5 :</head><label>5</label><figDesc>Number of unique updates for the topic "iran earthquake" obtained for values of parameters c and k. Rows in bold represent parameter values for submitted runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,317.99,73.93,217.98,272.00"><head>Table 1 :</head><label>1</label><figDesc>Patterns issued to egrep for each query.</figDesc><table coords="4,319.98,73.93,215.99,249.86"><row><cell></cell><cell>Pattern</cell></row><row><cell>Accident</cell><cell>"accident|calamity|casuality</cell></row><row><cell></cell><cell>|disaster|hazard|mishap|pileup</cell></row><row><cell></cell><cell>|setback|collision|fender-</cell></row><row><cell></cell><cell>bender|smash"</cell></row><row><cell>Bombing</cell><cell>"bombing|bomb|explosion|device</cell></row><row><cell></cell><cell>|explosive|charge|shell|projectile</cell></row><row><cell></cell><cell>|rocket|missile|mine|homemade</cell></row><row><cell></cell><cell>|terrorist|detonate|timer"</cell></row><row><cell cols="2">Earthquake "earthquake|shock|fault|quake</cell></row><row><cell></cell><cell>|shake|shaking|tremor|fault|temblor</cell></row><row><cell></cell><cell>|quake|quaking"</cell></row><row><cell>Shooting</cell><cell>"shooting|shots|shot|fired|firing</cell></row><row><cell></cell><cell>|gunfire|gun|gunning|trigger|bullet</cell></row><row><cell></cell><cell>|machine|fire|terrorist|firefight</cell></row><row><cell></cell><cell>|automatic|rifle|shotgun"</cell></row><row><cell>Storm</cell><cell>"storm|blast|blizzard|cyclone</cell></row><row><cell></cell><cell>|disturbance|downpour|gale|gust</cell></row><row><cell></cell><cell>|hurricane|monsoon|snowstorm</cell></row><row><cell></cell><cell>|squall|tempest|tornado|twister</cell></row><row><cell></cell><cell>|windstorm"</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,310.98,73.93,229.01,235.46"><head>Table 2 :</head><label>2</label><figDesc>Choice of queries to generate expansion terms.</figDesc><table coords="6,320.30,73.93,210.38,201.64"><row><cell cols="3">query type seed query</cell><cell></cell></row><row><cell>Generic All</cell><cell cols="3">earthquake injuries locations</cell></row><row><cell>Attributes</cell><cell>deaths</cell><cell>displaced</cell><cell>financial</cell></row><row><cell>(GAA)</cell><cell>impact</cell><cell></cell><cell></cell></row><row><cell>Generic</cell><cell cols="2">earthquake injuries</cell><cell></cell></row><row><cell>Single</cell><cell cols="2">earthquake locations</cell><cell></cell></row><row><cell>Attributes</cell><cell cols="2">earthquake deaths</cell><cell></cell></row><row><cell></cell><cell cols="2">earthquake displaced</cell><cell></cell></row><row><cell></cell><cell cols="3">earthquake financial impact</cell></row><row><cell>Specific All</cell><cell cols="3">iran earthquake injuries loca-</cell></row><row><cell>Attributes</cell><cell cols="3">tions deaths displaced financial</cell></row><row><cell>(SAA)</cell><cell>impact</cell><cell></cell><cell></cell></row><row><cell>Specific</cell><cell cols="2">iran earthquake injuries</cell><cell></cell></row><row><cell>Single</cell><cell cols="3">iran earthquake locations</cell></row><row><cell>Attributes</cell><cell cols="2">iran earthquake deaths</cell><cell></cell></row><row><cell></cell><cell cols="3">iran earthquake displaced</cell></row><row><cell></cell><cell cols="3">iran earthquake financial impact</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,73.93,229.01,210.75"><head>Table 3 :</head><label>3</label><figDesc>Hour 2012-08-11-18: Expansion Terms for the training topic "iran earthquake", generated using seed queries of type Generic All Attributes (GAA) and Specific All Attributes (SAA).</figDesc><table coords="7,92.98,73.93,184.59,153.03"><row><cell cols="3">Top 10 expansion terms for</cell></row><row><cell cols="3">training query type</cell></row><row><cell>GAA</cell><cell>SAA</cell><cell>RRF-fused</cell></row><row><cell>earthquake</cell><cell>injured</cell><cell>earthquake</cell></row><row><cell>quake</cell><cell cols="2">earthquake injured</cell></row><row><cell>magnitude</cell><cell>magnitude</cell><cell>magnitude</cell></row><row><cell>injured</cell><cell>haris</cell><cell>quake</cell></row><row><cell>hundreds</cell><cell>varzaqan</cell><cell>killed</cell></row><row><cell>killed</cell><cell>ahar</cell><cell>hundreds</cell></row><row><cell>seismic</cell><cell>quake</cell><cell>haris</cell></row><row><cell>iran</cell><cell>killed</cell><cell>ahar</cell></row><row><cell>earthquakes</cell><cell>hundreds</cell><cell>varzaqan</cell></row><row><cell cols="2">northwestern iran</cell><cell>iran</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,310.98,73.93,229.02,210.75"><head>Table 4 :</head><label>4</label><figDesc>Hour 2012-08-11-21: Expansion Terms for the training topic "iran earthquake", generated using seed queries of type Generic All Attributes (GAA) and Specific All Attributes (SAA).</figDesc><table coords="7,328.75,73.93,191.11,153.03"><row><cell cols="3">Top 10 expansion terms for</cell></row><row><cell cols="3">training query type</cell></row><row><cell>GAA</cell><cell>SAA</cell><cell>RRF-fused</cell></row><row><cell>earthquake</cell><cell>earthquake</cell><cell>earthquake</cell></row><row><cell>magnitude</cell><cell>iran</cell><cell>seismogram</cell></row><row><cell>seismogram</cell><cell>villages</cell><cell>anss</cell></row><row><cell>anss</cell><cell>magnitude</cell><cell>nsmp</cell></row><row><cell>nsmp</cell><cell>least</cell><cell>rmss</cell></row><row><cell>rmss</cell><cell cols="2">northwestern magnitude</cell></row><row><cell cols="2">recenteqsww earthquakes</cell><cell>recenteqsww</cell></row><row><cell>crustal</cell><cell>tv</cell><cell>crustal</cell></row><row><cell>shakemap</cell><cell>injured</cell><cell>shakemap</cell></row><row><cell cols="2">seismologist news</cell><cell>seismologist</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,83.09,73.93,456.90,647.84"><head>Table 6 :</head><label>6</label><figDesc>Examples of seed and expansion terms for rg runs</figDesc><table coords="8,83.09,713.10,76.13,8.67"><row><cell>7 www.wikipedia.com</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,72.00,73.93,459.38,648.00"><head>Table 7 :</head><label>7</label><figDesc>For the first hour, all sentences were included in the update set. For subsequent hours, only sentences with score sentence &gt; score cutof f , i.e. minimum score cutoff from previous hour, were added to the update set. Similarly, an incremental minimum cutoff score was computed for documents, which is dependent upon D h number of documents every hour. For a sentence to be included in updates, both the sentence score cutoff and document score cutoff should be passed. S h and D h for rg runs</figDesc><table coords="9,72.00,322.49,224.91,271.31"><row><cell cols="2">num of sentences per hour</cell></row><row><cell cols="2">score_cutoff = 0; //cutoff=0 for first hour</cell></row><row><cell>updatelist[] //list of updates</cell><cell></cell></row><row><cell>for every hour between the start</cell><cell>\</cell></row><row><cell cols="2">and end timestamps</cell></row><row><cell>count = 0;</cell><cell></cell></row><row><cell cols="2">score[]; //sentence scores current hour</cell></row><row><cell>for all the sentences in the hour:</cell><cell></cell></row><row><cell>if score_sentence &gt; score_cutoff:</cell><cell></cell></row><row><cell>add sentence to updatelist;</cell><cell></cell></row><row><cell>score[count] = score_sentence;</cell><cell></cell></row><row><cell>count++;</cell><cell></cell></row><row><cell>end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell cols="2">if count &gt; S_h: //update score_cutoff</cell></row><row><cell>sort score[] desc</cell><cell></cell></row><row><cell>score_cutoff = score[S_h];</cell><cell></cell></row><row><cell>end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>return updatelist</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,326.20,348.49,198.57,8.74"><head>Table 8 :</head><label>8</label><figDesc>Average ELG and LC across Topics.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,326.22,715.13,97.89,6.64"><p>http://www.trec-ts.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,87.24,705.09,25.80,6.99;2,134.13,705.09,32.00,6.99;2,187.22,705.09,7.53,6.99;2,215.84,705.67,85.18,6.64;2,72.00,715.13,120.05,6.64"><p>Details available at http://trec-kba.org/ kba-stream-corpus-2013.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,326.22,648.23,213.78,7.21;6,310.98,658.26,152.43,6.64"><p>ANSS: Advanced National Seismic System http:// earthquake.usgs.gov/monitoring/anss/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,326.22,667.19,213.77,6.99;6,310.98,676.66,229.02,7.21;6,310.98,686.70,50.81,6.64"><p>"RMSS: root-mean-square travel time residual in seconds" -source http://earthquake.usgs.gov/earthquakes/ glossary.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="6,322.07,694.18,217.93,8.44;6,310.98,705.09,229.02,7.21;6,310.98,715.13,169.36,6.64"><p><ref type="bibr" coords="6,322.07,694.18,3.65,5.24" target="#b5">6</ref> Shakemap: A map that presents information on the shaking of ground rather than epicenter and magnitude. http: //earthquake.usgs.gov/research/shakemap/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>We thank <rs type="person">Mark Smucker</rs> for his helpful feedback and advice. This work was made possible by the facilities of the <rs type="institution">Shared Hierarchical Academic Research Computing Network</rs> (SHARCNET:www.sharcnet.ca) and <rs type="funder">Compute/Calcul Canada</rs>, and was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, and in part by the <rs type="funder">Google Founders Grant</rs>, and in part by the <rs type="funder">University of Waterloo</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,326.48,282.42,213.52,8.74;10,326.48,294.38,213.52,8.74;10,326.48,306.33,213.53,8.74;10,326.48,318.29,53.52,8.74" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<title level="m" coord="10,326.48,294.38,213.52,8.74;10,326.48,306.33,96.57,8.74">Information retrieval : Implementing and Evaluating Search Engines</title>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.48,336.29,213.52,8.74;10,326.48,348.24,213.52,8.74;10,326.48,360.20,213.52,8.74;10,326.48,372.15,74.16,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,362.33,348.24,177.66,8.74;10,326.48,360.20,104.80,8.74">An information-theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,439.81,360.20,96.25,8.74">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.48,390.15,213.52,8.74;10,326.48,402.10,213.52,8.74;10,326.48,414.06,213.52,8.74;10,326.48,426.01,213.52,8.74;10,326.48,437.97,213.52,8.74;10,326.48,449.92,213.52,8.74;10,326.48,461.88,78.87,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,326.48,402.10,213.52,8.74;10,326.48,414.06,142.33,8.74">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,490.34,414.06,49.65,8.74;10,326.48,426.01,213.52,8.74;10,326.48,437.97,213.52,8.74;10,326.48,449.92,74.80,8.74">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;09</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.48,479.88,213.52,8.74;10,326.48,491.83,213.52,8.74;10,326.48,503.79,213.53,8.74;10,326.48,515.74,58.67,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,511.87,479.88,28.14,8.74;10,326.48,491.83,209.31,8.74">Search Engines: Information Retrieval in Practice</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Addison-Wesley Publishing Company</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct coords="10,326.48,533.74,213.52,8.74;10,326.48,545.69,17.46,8.74;10,360.78,545.69,179.22,8.74;10,326.48,557.65,213.52,8.74;10,326.48,569.60,213.52,8.74;10,326.48,581.56,89.14,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,360.78,545.69,179.22,8.74;10,326.48,557.65,20.78,8.74">Overview of the trec 2011 microblog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,368.49,557.65,171.51,8.74;10,326.48,569.60,213.52,8.74;10,326.48,581.56,58.60,8.74">NIST Special Publication 500-296: The Twentieth Text REtrieval Conference Proceedings (TREC 2011)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.48,599.56,213.53,8.74;10,326.48,611.51,213.52,8.74;10,326.48,623.47,213.52,8.74;10,326.48,635.42,174.80,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,326.48,611.51,213.52,8.74;10,326.48,623.47,168.37,8.74">Similar document detection and electronic discovery: So many documents, so little time</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sperling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rayvych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,516.28,623.47,23.72,8.74;10,326.48,635.42,120.22,8.74">DESI V Workshop at ICAIL 2013</title>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.48,653.42,213.52,8.74;10,326.48,665.38,213.52,8.74;10,326.48,677.33,213.52,8.74;10,326.48,689.29,213.52,8.74;10,326.48,701.24,213.52,8.74;10,326.48,713.20,99.90,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,403.13,653.42,136.87,8.74;10,326.48,665.38,213.52,8.74;10,326.48,677.33,21.95,8.74">A semi-supervised approach to extracting multiword entity names from user reviews</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,374.86,677.33,165.14,8.74;10,326.48,689.29,213.52,8.74;10,326.48,701.24,95.66,8.74">Proceedings of the 1st Joint International Workshop on Entity-Oriented and Semantic Search, JIWES &apos;12</title>
		<meeting>the 1st Joint International Workshop on Entity-Oriented and Semantic Search, JIWES &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
