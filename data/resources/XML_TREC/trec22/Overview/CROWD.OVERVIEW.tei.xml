<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.69,102.08,348.63,15.12">Overview of the TREC 2013 Crowdsourcing Track</title>
				<funder ref="#_gmcTxaC">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-02-11">February 11, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.25,134.57,90.23,10.48"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<address>
									<country>University of Waterloo</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.36,134.57,80.08,10.48"><forename type="first">Gabriella</forename><surname>Kazai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.08,134.57,77.93,10.48"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.69,102.08,348.63,15.12">Overview of the TREC 2013 Crowdsourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-02-11">February 11, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">ADF0B9A210F13C12468713DC558314FC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 2013, the Crowdsourcing track partnered with the TREC Web Track and had a single task to crowdsource relevance judgments for a set of Web pages and search topics shared by the Web Track. This track overview describes the track and provides analysis of the track's results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Now in its third year, the overall goals of the TREC Crowdsourcing track remained to build awareness and expertise with crowdsourcing in the Information Retrieval (IR) community, to develop and evaluate new methodologies for crowdsourced search evaluation on a shared task and data set, and to create reusable resources to benefit future IR community experimentation.</p><p>While the first year of the track was explicitly focused on crowdsourcing, last year we decided to loosen the crowdsourcing requirements and instead focus on a goal of obtaining document annotations by any means. The advantage of this change was that it gave groups freedom in the creation of their solutions towards methods that combined contributions from humans and computer algorithms. This year, we followed in the same vein and further emphasized the combined human-computer aspect.</p><p>As in previous years, the track set as its challenge the task of 'crowdsourcing' quality relevance judgments. Unlike last year, when we had a textual document and an image relevance judging task, this year the track consisted of a single task that required collecting relevance judgments for Web pages and search topics taken from the TREC Web Track. Participants thus had almost the same task as NIST assessors: judging the relevance of Web pages retrieved by teams who partook in the Web Track challenge. Whereas NIST judges were required to assess all sub-topics of each topic, track participants were only required to judge the first sub-topic. This kept the scale of the task similar to last year, with around 20k documents needing to be judged. To ease participation, we also offered a reduced scale task, with only around 3.5k documents to be judged.</p><p>Four groups participated in the track, submitting 11 runs in total. We next describe details of the task, the data set used, the evaluation methods, and finally the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Overview</head><p>The task required collecting relevance judgments for Web pages and search topics, taken from the TREC Web Track. The Web pages to be judged for relevance were drawn from the recently released ClueWeb12<ref type="foot" coords="1,369.21,502.21,3.97,6.12" target="#foot_0">1</ref> collection. The search topics were created by NIST judges. While the Web Track participants were only provided with the topic titles, the Crowdsourcing Track participants were given the full topic descriptions, matching the setup for the NIST judges. After the Web Track participants submitted their retrieval runs, NIST identified a subset of the submitted documents to be judged for each topic. In parallel with NIST assessment, the Crowdsourcing Track participants were given the same topic and document pairs to label as the NIST judges.</p><p>In this 'crowdsourcing' track, participants were free to use or not use crowdsourcing techniques however they wished. For example, judgments could be obtained via a fully-automated system, or using traditional relevance assessment practices, or a mix of these. Participants could use a purely crowdsourcing-based approach, or employ a hybrid approach combining automated systems, crowds, trained judges or other resources and techniques. Crowdsourcing could be paid or non-paid. It was left entirely up to each team to innovate the best way to obtain accurate judgments, in a reliable and scalable manner, while minimizing the time, cost, and effort required.</p><p>The track offered two entry levels for participation. Participants could choose to enter at either or both levels:</p><p>• Basic: approx. 3.5k documents (subset of NIST pool, 10 topics)</p><p>• Standard: approx. 20k documents (entire NIST pool, 50 topics)</p><p>The task in both cases was to obtain relevance labels for the documents and search topics included in the entry level set.</p><p>Judgments needed to be collected on a six-point scale:</p><p>• 4=Nav This page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site.</p><p>• 3 = Key This page or site is dedicated to the topic; authoritative and comprehensive, it is worthy of being a top result in a web search engine.</p><p>• 2 = HRel The content of this page provides substantial information on the topic.</p><p>• 1 = Rel The content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page.</p><p>• 0 = Non The content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query.</p><p>• -2 = Junk This page does not appear to be useful for any reasonable purpose; it may be spam or junk.</p><p>We informed participants that non-English documents would be judged non-relevant by NIST assessors, even if the assessor understands the language of the document and the document would be relevant in that language. If the location of the user matters, the assessor will assume that the user is located in Gaithersburg, Maryland. In addition, the NIST assessor guidelines were also made available to participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Set</head><p>Participants were provided with details of the search topics and a list of (topic-ID, document-ID) pairs to be judged in both entry level sets. The document-IDs identified the documents to be judged from the ClueWeb12 collection.</p><p>Thanks to Jamie Callan at CMU, and Gaurav Baruah at Waterloo, it was possible to participate in the track without purchasing ClueWeb12 and to download a self-contained corpus of the Web pages included in the entry level sets.</p><p>All topics were expressed in English. The standard entry level set contained all 50 queries from the Web Track ad hoc task; the same set that was sent to NIST judges for assessment. Of the 50 search topics, 25 were multi-faceted, i.e., they contained several sub-topics, representing different possible user intents. For example, the topic below is one of the multi-faceted topics: &lt;topic type="faceted" number="206"&gt; &lt;query&gt;wind power&lt;/query&gt; &lt;description&gt; What are the pros and cons of using wind power. &lt;/description&gt; &lt;subtopic type="inf" number="1"&gt; What are the pros and cons of using wind power. &lt;/subtopic&gt; &lt;subtopic type="inf" number="2"&gt; Find information on wind power in the USA. &lt;/subtopic&gt; &lt;subtopic type="inf" number="3"&gt; Find information on wind power companies. &lt;/subtopic&gt; &lt;subtopic type="inf" number="4"&gt; Find information on residential (home) wind power. &lt;/subtopic&gt; &lt;subtopic type="inf" number="5"&gt; Find information on how wind turbines work. &lt;/subtopic&gt;</p><formula xml:id="formula_0" coords="3,122.22,73.56,119.61,56.96">Team Basic Standard Hrbust 1 - NEUIR 1 - PRIS 1 - udel - 8</formula><p>Table <ref type="table" coords="3,99.08,143.25,3.87,8.74">1</ref>: Submitted runs to the basic and standard entry levels.</p><p>&lt;subtopic type="inf" number="6"&gt; Find pictures of wind turbines used for wind power. &lt;/subtopic&gt; &lt;subtopic type="inf" number="7"&gt; Find pictures of a wind farm. &lt;/subtopic&gt; &lt;/topic&gt;</p><p>In the case of such topics, track participants were instructed to only consider the description field and ignore the various subtopics. Note that the description is always repeated as subtopic number 1. This was done in the interest of keeping the scale of the task to a defined limit, e.g., 20k topic-document pairs. NIST judges were required to assess the relevance of all documents retrieved for a given topic to each of the sub-topics.</p><p>The basic set contained 10 topics, which were randomly selected from the 50 TREC Web Track ad-hoc task topics: 202, 214, 216, 221, 227, 230, 234, 243, 246, 250. These were mostly single faceted topics with the exception of 202, 216 and 243.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>Four groups submitted a total of 11 runs, see Table 1. Only one group (udel) submitted runs for the standard entry level set, the other three groups crowdsourced labels for the basic set of 3.5k topicdocument pairs only.</p><p>The four groups followed very different approaches:</p><p>• The Hrbust team proposed a solution that leveraged social networking sites and multiple different crowds. They differentiated between three groups of crowds: Expert Group, Trustee Group and Volunteer Group. The expert group judged all 3.5k topic-document pairs, and asked their friends to contribute further judgments (trustee group). Additional judgments were collected for topic-document pairs by posting them on various social networking platforms (volunteer group). A consensus method was then used to obtain the final labels.</p><p>• The run by PRIS collected explicit rankings of documents from workers on Amazon's Mechanical Turk<ref type="foot" coords="3,401.80,154.11,3.97,6.12" target="#foot_1">2</ref> . The result implemented a quality control mechanism at the task level based on a simple reading comprehension test.</p><p>• NEUIR's approach was based on using preference judgments made by a single judge (a graduate student), where the number of comparisons required for preference judgments was reduced following a Quick-sort like method, which partitioned the documents into n pivot documents and n+1 groups of documents that were compared to each other.</p><p>• The udel team submitted 8 fully automated runs that made use of three search engines and no human judges. The rating of a document was derived based on its position in each of the systems' rankings, where the different runs had slightly different rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Measures</head><p>Using the judgments obtained from the trusted and highly trained NIST judges as gold standard, we measured the quality of the teams' submitted judgments. Participants may report in their own writeups on the time, cost, and effort required to crowdsource the judgments. We report results for the following three metrics:</p><p>• Rank Correlation: The Web Track participants' ad-hoc IR systems are scored based on NIST judgments according to the primary Web Track metric, ERR@20, inducing a ranking of IR systems. A similar ranking of IR systems is then induced from each Crowdsourcing Track participant's submitted judgments. Rank correlation is then calculated, indicating how accurately crowd judgments can be used to predict the NIST ranking of IR systems. The measure we use for rank correlation is Yilmaz et al.'s AP Correlation (APCorr) <ref type="bibr" coords="3,526.71,646.98,9.96,8.74" target="#b3">[4]</ref>, which improves upon Kendall's Tau as a measure of rank correlation by emphasizing the order of the top ranked systems. To the best of our knowledge, the original version of APCorr does not handle ties; we handle ties by sampling over possible orders.</p><p>• Score Accuracy: In addition to correctly ranking systems, it is important that the evaluation scores be as accurate as possible. We use root mean square error (RMSE) for this measure.</p><p>• Label Quality: Direct comparison of each participant's submitted judgments against the NIST judgments (no evaluation of Web track IR systems). Label quality provides the simplest evaluation metric and can be correlated with the other measures predicting performance of IR systems. In previous years we reported logistic average misclassification rate (LAM), developed for the Spam Track <ref type="bibr" coords="4,278.77,259.77,9.96,8.74" target="#b0">[1]</ref>, and Area Under Curve (AUC). However, LAM does not give any preference to ordering, and only works with binary classification. We introduced AUC to address ordering, but again, AUC deals with binary classification. Average precision (AP) is an alternative to AUC, but AP is also based on binary classification. Hence this year, we use graded average precision (GAP) <ref type="bibr" coords="4,145.77,367.36,9.96,8.74" target="#b1">[2]</ref>. The GAP is computed by ordering the documents as per the score assigned to the document and then using the qrels provided by NIST.</p><p>We provide our implementations of all 3 measures online in the active participants section of the TREC website 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We report two sets of performance measurements. The first set of measurements is based on computing the mean ERR@20 for the 34 ad-hoc web track runs. Given a set of qrels submitted by a Crowdsourcing track participant, we compute the ERR@20 for each of the 10 randomly selected topics and then a mean ERR@20 across these 10 topics. Using the mean ERR@20 scores, we rank the web track runs and compare using APCorr the NIST qrels induced ranking to the Crowdsourcing track participant qrels induced ranking. Likewise, we compute RMSE based on the mean ERR@20. We take the mean ERR@20 produced using the NIST qrels as truth and measure the RMSE for the mean ERR@20 values produced by a participant's submitted qrels. These results are shown in Table <ref type="table" coords="4,100.10,696.92,3.87,8.74" target="#tab_0">2</ref>. We note the number of documents used for the evaluation to highlight that NIST did not judge as much of the pool as anticipated, and thus there are fewer documents used for the evaluation than were actually judged in the runs submitted by Crowdsourcing Track participants.</p><p>The second set of measurements is based on averaging the per-topic APCorr, RMSE, and GAP scores. In this case, for each participant we have 10 scores and report the average. As can be seen in Table <ref type="table" coords="4,347.43,317.64,3.87,8.74" target="#tab_1">3</ref>, the average per-topic APCorr and RMSE are worse than the APCorr and RMSE scores reported in Table <ref type="table" coords="4,392.29,341.55,3.87,8.74" target="#tab_0">2</ref>. Ranking systems based on the mean ERR@20 should and does perform better than ranking systems based on ERR@20 for single topics for all groups except udel; the same result holds for RMSE.</p><p>When we look at the overall results in Table <ref type="table" coords="4,532.26,401.67,3.87,8.74" target="#tab_0">2</ref>, we see that for APCorr, the highest score was obtained by the Hrbust team. APCorr reflects the agreement with the system rankings obtained using the ERR@20 measure and NIST judgments and a team's submitted judgments. When we look at the per-topic average APCorr in Table <ref type="table" coords="4,498.83,473.40,3.87,8.74" target="#tab_1">3</ref>, we see that NEUIR has the best APCorr. While Hrbust obtained the highest APCorr score overall, and NEUIR has the highest average APCorr, the difference in average APCorr between Hrbust and NEUIR is not statistically significant by a paired, two-sided Student's t test (p-value = 0.16), nor are most differences between the groups with their pertopic APCorr values. It appears that 10 topics is not enough to distinguish performance differences in APCorr.</p><p>The best performance in terms of smallest obtained error between the obtained system scores, when using ERR@20 and the NIST qrels vs. the submitted judgments, is achieved by the NEUIR team. The average per-topic RMSE of NEUIR is also better than the next best system, Hrbust, but the difference is not statistically significant (p=0.06).</p><p>The highest quality of when looking at label quality directly is obtained by the NEUIR team both Both the NEUIR and the Hrbust teams relied on human judges, where the former collected judgments from a single (likely trusted) judge, while the latter relied on different groups of crowds as well as high redundancy (minimum 15 labels per topic-document pair) and a consensus method. In contrast the run by the udel team was fully automatic with no human input and did not match the performance achieved by other teams. This result seems to be consistent with earlier findings of Soboroff et al. in which blind evaluation by inducing qrels from rank fusion techniques is unable to distinguish whether outlier rankings are weak or strong <ref type="bibr" coords="5,103.88,378.82,9.96,8.74" target="#b2">[3]</ref>. Consequently, some degree of human labeling appears to remain necessary.</p><p>The difference between the NEUIR, Hrbust and the PRIS runs suggests the need for additional quality control methods on anonymous crowdsourcing platforms such as Amazon's Mechanical Turk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The track this year tackled the issue of crowdsourcing relevance labels for web pages that were retrieved by the participants of the Web Track. This is the same task that faces NIST judges. However, in the interest of keeping the workload stable, we ended up not exactly reproducing NIST judging, in that participants did not have to judge sub-topics. Thus, it is still left for future work to try to reproduce NIST's full workload. Going even further, we planned, but did not run an additional "total recall" task with the goal to gather relevance labels for the 50 search topics over the complete ClueWeb12 corpus. This remains an exciting challenge for the future.</p><p>The overall performance scores obtained this year highlight the need for further research into methods to improve the quality of crowdsourced relevance judgments for IR evaluation. However, this challenge may be better addressed by collab-orative efforts that combine expertise from multiple fields, beyond IR, such as incentive engineering, HCI aspects or game design. We take the low level of participation this year as further evidence to this. Thus, the crowdsourcing track will not run again next year at TREC. Research will, however, continue to be facilitated by similar initiatives, such as the CrowdScale 2013 Shared Task<ref type="foot" coords="5,535.53,269.14,3.97,6.12" target="#foot_2">4</ref> that was launched recently with a broader set of annotation types and larger-scale real-world data sets. The MediaEval benchmarking initiative has also launched a crowdsourcing track<ref type="foot" coords="5,477.33,316.96,3.97,6.12" target="#foot_3">5</ref> this year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,83.09,73.56,456.91,648.20"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for the four primary runs based on the basic set of topics. The APCorr and RMSE values are computed based on the mean ERR@20 for the 34 ad-hoc web track runs.</figDesc><table coords="4,83.09,713.10,186.22,8.67"><row><cell>3 http://trec.nist.gov/act_part/tracks13.html</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.00,73.56,468.00,146.11"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results for the four primary runs based on the basic set of topics. Here the results are the average of the per-topic scores.</figDesc><table coords="5,72.00,73.56,376.03,146.11"><row><cell>Team</cell><cell cols="4">#Topics Mean APCorr Mean RMSE Mean GAP</cell></row><row><cell>Hrbust</cell><cell>10</cell><cell>0.251</cell><cell>0.241</cell><cell>0.392</cell></row><row><cell>NEUIR</cell><cell>10</cell><cell>0.375</cell><cell>0.171</cell><cell>0.584</cell></row><row><cell>PRIS</cell><cell>10</cell><cell>0.203</cell><cell>0.311</cell><cell>0.481</cell></row><row><cell>udel</cell><cell>10</cell><cell>0.051</cell><cell>0.250</cell><cell>0.356</cell></row><row><cell cols="3">overall and for the average per-topic GAP, and</cell><cell></cell><cell></cell></row><row><cell cols="3">NEUIR's GAP performance is statistically signifi-</cell><cell></cell><cell></cell></row><row><cell cols="3">cant compared to the next best by PRIS (p=0.006).</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,335.19,714.56,154.33,6.99"><p>http://www.lemurproject.org/clueweb12/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,335.19,715.13,55.04,6.64"><p>www.mturk.com   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,335.19,705.05,120.60,6.99"><p>www.crowdscale.org/shared-task</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,335.19,714.56,191.50,6.99"><p>www.multimediaeval.org/mediaeval2013/crowd2013</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>Special thanks to <rs type="person">Gaurav Baruah</rs> for his on-going and extensive assistance with the preparation of the corpus and aspects of the evaluation. We are grateful to <rs type="person">Ellen Voorhees</rs> and <rs type="person">Ian Soboroff</rs> for their help and support with running the track. Thanks to the organizers of the Web Track, <rs type="person">Kevyn Collins-Thompson</rs>, <rs type="person">Paul N. Bennett</rs>, <rs type="person">Fernando Diaz</rs> and <rs type="person">Charles Clarke</rs>, for their help and collaboration. We thank <rs type="institution">Amazon</rs> and <rs type="person">Crowd Computing Systems</rs> for sponsoring the track and providing credits or discounted prices to track participants.</p><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, the facilities of <rs type="institution">SHARCNET</rs>, the <rs type="funder">University of Waterloo</rs>, and by <rs type="funder">National Science Foundation</rs> Grant No. <rs type="grantNumber">1253413</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gmcTxaC">
					<idno type="grant-number">1253413</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,335.45,658.29,204.56,8.74;5,335.45,670.25,204.56,8.74;5,335.45,682.20,52.58,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,469.34,658.29,70.66,8.74;5,335.45,670.25,61.67,8.74">Trec 2005 spam track overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,419.43,670.25,115.95,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.50,75.16,204.56,8.74;6,87.50,87.11,204.55,8.74;6,87.50,99.07,204.56,8.74;6,87.50,111.02,204.56,8.74;6,87.50,122.98,204.55,8.74;6,87.50,134.93,204.56,8.74;6,87.50,146.89,52.31,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,87.50,87.11,204.55,8.74;6,87.50,99.07,42.89,8.74">Extending average precision to graded relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,158.10,99.07,133.95,8.74;6,87.50,111.02,204.56,8.74;6,87.50,122.98,204.55,8.74;6,87.50,134.93,33.95,8.74">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SI-GIR &apos;10</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval, SI-GIR &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.50,166.81,204.56,8.74;6,87.50,178.77,204.56,8.74;6,87.50,190.72,204.56,8.74;6,87.50,202.68,204.55,8.74;6,87.50,214.64,204.56,8.74;6,87.50,226.59,52.31,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,255.66,166.81,36.39,8.74;6,87.50,178.77,199.79,8.74">Ranking retrieval systems without relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,99.86,190.72,192.19,8.74;6,87.50,202.68,204.55,8.74;6,87.50,214.64,140.07,8.74">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.50,246.52,204.56,8.74;6,87.50,258.47,204.56,8.74;6,87.50,270.43,204.55,8.74;6,87.50,282.38,204.55,8.74;6,87.50,294.34,204.56,8.74;6,87.50,306.29,204.56,8.74;6,87.50,318.25,99.90,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,87.50,258.47,204.56,8.74;6,87.50,270.43,56.52,8.74">A new rank correlation coefficient for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,170.16,270.43,121.89,8.74;6,87.50,282.38,204.55,8.74;6,87.50,294.34,204.56,8.74;6,87.50,306.29,78.61,8.74">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;08</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="587" to="594" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
