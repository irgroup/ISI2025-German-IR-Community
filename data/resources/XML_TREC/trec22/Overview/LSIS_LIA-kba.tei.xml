<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.28,57.81,253.44,12.90;1,121.43,73.75,333.15,12.90">Filtering Entity Centric Documents using Numerics and Temporals features within RF Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.55,96.28,82.91,10.75"><forename type="first">Vincent</forename><surname>Bouvier</surname></persName>
							<email>vincent.bouvier@kware.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Kware / Aix-Marseille</orgName>
								<orgName type="laboratory">LSIS UMR</orgName>
								<orgName type="institution">Université CNRS</orgName>
								<address>
									<addrLine>7296 Domaine universitaire de Saint Jérôme Avenue Escadrille Normandie</addrLine>
									<postCode>13397</postCode>
									<settlement>Niemen, MARSEILLE Cedex 20</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.37,96.28,69.27,10.75"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@lsis.org</email>
							<affiliation key="aff1">
								<orgName type="department">Aix-Marseille</orgName>
								<orgName type="laboratory">LSIS UMR</orgName>
								<orgName type="institution">Université CNRS</orgName>
								<address>
									<addrLine>7296 Domaine universitaire de Saint Jérôme Avenue Escadrille Normandie</addrLine>
									<postCode>13397</postCode>
									<settlement>Niemen, MARSEILLE Cedex 20</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.28,57.81,253.44,12.90;1,121.43,73.75,333.15,12.90">Filtering Entity Centric Documents using Numerics and Temporals features within RF Classifier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C7235FACFD42DA92868377425E4D5A89</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article introduces the system designed to work for the Knowledge Base Acceleration (KBA) track at Text REtrieval Conference (TREC). This is the second time this track is run with yet this year there is even more challenge.</p><p>KBA is focused on keeping up to date knowledge bases (KB) such as Wikipedia 1 (WP) where each KB node is considered as an entity (WP page for wikipedia example). It has been shown in <ref type="bibr" coords="1,242.97,320.29,36.53,8.64;1,72.00,332.24,38.65,8.64" target="#b2">(Frank et al., 2012)</ref> that the time lag between the publication date of cited news articles and the date of an edit to wikipedia article creating the citation can be really big (median 356 days) for non-popular entities.</p><p>KBA is to give a chance to non-popular entities information to be updated as soon as a useful information is published on the internet. The KBA organizers have built up a stream-corpus which is a huge corpus of timestamped web documents that can be processed chronologically. Hence it is possible to simulate a real time system. The documents come from newswires, blogs, forums, review, memetracker... . In addition, a set of target entities, coming from wikipedia or from twitter, has been selected for their ambiguity or unpopularity. And last but not least, more than 60,000 documents have been annotated so that systems can train on it. The train period starts on documents published from october 2011 until february 2012, and the test period starts from february 2012 to february 2013.</p><p>The KBA track is divided in two tasks: CCR (Cumulative Citation Recommendation) and SSF <ref type="bibr" coords="1,72.00,595.66,96.56,8.64">(Streaming Slot Filling)</ref>. CCR task is to filter out documents worth citing in a profile of an entity (e.g., wikipedia or freebase article). SSF task is to detect changes on given slots for each of the target entities. This article is focused only on CCR task.</p><p>In CCR task, the system is to filter out documents relative to target entities out from the stream-corpus. The system must also be able to give the usefulness 1 wikipedia: http://wikipedia.org of a document ranked using one of those 4 relevance classes:</p><p>garbage : no information about target entity; neutral : informative but not citable; useful : bio, primary or secondary source useful when creating a profile from scratch;</p><p>vital : timely info about the entity's current state, actions, or situation.</p><p>The remaining of this article is organized as follows. The next section give details on how we designed our system. Then the different runs we sent are analyzed to eventually conclude and give a brief outline on perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KBA System</head><p>Before starting designing our system, we first had to think about how to deal with the corpus. For the KBA track, a 6.5 terabytes (compressed) corpus of documents is given to participants where each documents are organized so it can be read chronologically. It is really important to read documents after documents in chronological order to make sure that the system is not considering future information when processing a document. Therefore it is not possible to build a big index on the whole corpus since the retrieval process would be influenced by all the documents. It is yet possible to build an incremental index however, this can be really a time consuming task.</p><p>Our last year system we indexed each hour of stream to make sure we were not messing up with the "look ahead" behavior. This year we have modified our system since the corpus is way much bigger, it would have been too much time consuming to index every hour of stream. In order not to have to process all the corpus each time we want to test our system we first rebuild a new corpus with only documents containing a mention of an entity in a preprocessing phase. Then we can perform the ranking phase which consists in giving a class to each document having a mention of its target entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preprocessing Data</head><p>To extract documents, we first pre-process the target entities. For the CCR task, the entities come either from wikipedia; so the url page is used as a "target id"; or from twitter; so only the user starting with a '@' is given. For twitter entities, we also were allowed to get real name from the twitter profile.</p><p>The remaining of this section explains first how we build entities profiles we use for both extracting documents from the corpus and for document's classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Building Entity Profile</head><p>From now on, we are going to use the word "Profile" to define the structure that holds different data that must define as much as possible the entity. We identified three types of data :</p><p>-variant collection : the variant collection contains different ways to refer to an entity;</p><p>-relation collection: relation collection contains the different entities that have a relation with the target entity;</p><p>-language model : is to contain a textual representation of the entity as a bag of n-grams. This model is used to evaluate how similar/divergent a document is from it.</p><p>We describe in the remaining of this sub-section how we gathered those data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Variant and Relation Collections</head><p>An entity may be cited in a document using different manners. For instance, the homonymic entities Boris Berezovsky (Pianist and Business man), has different middle names (cf., table 1, 2) that can be helpful to identify which one is referred within a document. In a recall oriented system, the variant collection is mandatory.</p><p>target id Boris Berezovsky the Pianist variants Boris Vadimovitch Berezovsky Table <ref type="table" coords="2,100.63,650.63,3.88,8.64">1</ref>: Variants for Boris Berezovsky the pianist Variant collections can be filled either supervised or unsupervised manner. We use in our system only the unsupervised way. We build a system that goes target id Boris Berezovsky Businessman variants Boris Abramovich Berezovsky Table <ref type="table" coords="2,320.70,99.38,3.88,8.64">2</ref>: Variants for Boris Berezovsky businessman through a wikipedia dump 2 and fills variant collections using <ref type="bibr" coords="2,342.45,143.93,69.16,8.64" target="#b1">(Cucerzan, 2007)</ref>  <ref type="table" coords="2,414.10,143.93,36.53,8.64">methods:</ref> -bold words in the first paragraph of the target entity's WP page;</p><p>-legend of anchors in any pages that points to the target entity's WP page.</p><p>While searching for variants we also look for the different kind of relations the target entity may have with other entities and remember each one as : outgoing relation (an anchor in the target entity WP page that points to another WP page), incoming relation (an anchor in any WP page that points to the target entity WP page) or mutual relation (anchors are found in both a WP page and the target entity WP page).</p><p>This year we also have Twitter entities. For those entity we simply use the name displayed on the profile of the entity using the Twitter API (cf. figure <ref type="figure" coords="2,296.50,374.08,3.60,8.64">1</ref>).</p><p>Figure <ref type="figure" coords="2,330.49,526.77,3.88,8.64">1</ref>: Variant's name for Twitter entity @urben00: Brent Faulkner</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">The language model</head><p>In the profile, we also save as a Bag Of Word (BOW with n-gram) we call the reference BOW. It contains a representation of the WP Page of the target entity when it is a WP entity. For Twitter entities it is empty otherwise. The last point is quite important since some profiles have a lack of information. In addition, the entities may evolve as the time goes by. So we added to the profil another BOW that we call the dynamic BOW. This dynamic BOW is filled with documents discovered on the stream when they fulfill the prerequisites: mention the entity, document is ranked as vital (V UPDT), document is scored as useful (VU UPDT). The two last conditions are tested seperately in two different runs.</p><p>In addition, we add another parameter to fill the dynamic BOW. When having a document ranked even vital for an entity, it may not be relevant to consider the whole document since it may not only contain information about the target entity. So we tested both approaches : add whole document ( DOC); add snippet ( SNPT). To build the snippet we simply use every paragraph that contains a mention of the target entity.</p><p>The process of updating the dynamic BOW is performed during the ranking phase of our system since it does require the rank our system assigns to the document. To summarize, our system is giving 5 different outputs: NO UPDT, V UPDT DOC, V UPDT SNPT, VU UPDT DOC, VU UPDT SNPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extracting documents from the Corpus</head><p>Using variant collection from target entities profile, we go through every single document in the corpus and search for a mention of any variant. A single document can be extracted more than once if multiple target entities appear in it. We use a hash of the "target id" as well as the document unique identifier (stream id) to name the xml file that represents the document so it is easier to find which document contains which entity while keeping track on what time the document appears on the stream. It is therefore possible to read all documents still chronologically.</p><p>In order to find a match in a document we implemented an algorithm inspired of the "Backward Nondeterministic Dawg Matching" (BNDM) to which we add the possibility to ignore space and some punctuation such as dash "-" (BNDM IR). An entity name can indeed be composed of several names not always segmented the same way. For instance KBA12 entity "lovebug starsky" was also found as "love bug starski".</p><p>To have an idea of the gain of this method, we compute the recall using the formula in equation 1 on the training and test collection:</p><formula xml:id="formula_0" coords="3,72.08,626.74,207.42,22.31">recall = #documentsf ound ∈ corpus #documentsf ound ∈ train ∪ test (1)</formula><p>4 The Ranking System</p><p>The ranking system is to identify according to a profile how useful a document is. This system can only Method Recall BNDM .7396 BNDM IR .8226 Table <ref type="table" coords="3,321.24,111.34,3.88,8.64">3</ref>: Recall computed using training and test set consider prior and current documents to decide the rank of a document in : garbage, neutral, useful and vital. In addition, the system must issue a confidence score ∈]0, 1000] ∈ Z where 1000 is very confident.</p><p>Our system uses Random Forest (RF) classifiers with a set of features to determine the rank. The RF is composed of a multitude of decision trees, where each one uses a subset of features. The final decision is made by averaging scores of the trees. We use three different kind of features : documents related features, entity related features and time related features. We design our features so adaptive learning can be used. Therefore our system could be used to rank documents of an entity that has not been part of the training. The remaining of this section discuss the features we use for classification.</p><p>Document related features are used to depict a layout from documents independently of the entity. We use three features here apply to a document D as shown in table <ref type="table" coords="3,356.86,372.41,3.74,8.64">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4: Document related Features</head><p>Entity related features are used to determine how a document concerns the target entity it has been extract for. Here, the previously build profile is used to compute the different features(cf., table <ref type="table" coords="3,296.50,520.36,3.60,8.64" target="#tab_0">5</ref>).</p><p>Some entity features require term frequency (TF) to be computed. To compute a TF of an entity e, we sum up the frequency of all mentions of variant names v i from the collection V e in a document D. We eventually normalize by the number of words|D| in D (cf., equation 2).</p><formula xml:id="formula_1" coords="3,344.78,613.88,159.22,25.41">tf (e, D) = Ve i=1 f (v i , D) |D| (2)</formula><p>The features that compute the coverage is computed as in equation 3 When building profile, we said that we extract relation an entity may have with another from WP using three different kind of relations: incoming, outgoing and mutual. For each kind of relation and for each entity in this relation group, we compute the average tf on the whole document.</p><formula xml:id="formula_2" coords="3,333.56,682.96,162.70,22.31">cov.(D snippet , D)) = |D snippet | |D|<label>(</label></formula><p>Time related Features: the corpus offers the pros to be able to work with time information. We designed the time related features so the classifiers are able to work with information concerning previous documents. Such information may help detecting that may be something is going on about an entity using different clues such as burst effect. As shown on the figure 2, the burst does not always depicts vital documents, although it still might be a relevant information for classification. To depict the burst effect we used an implementation of the Kleinberg Algorithm <ref type="bibr" coords="4,204.96,543.92,70.24,8.64" target="#b3">(Kleinberg, 2003)</ref>. Given a time series, it captures burst and measure the strength of it as well as the direction (up or down). We decided to scale the time series on an hour basis. In order not to mess the classifiers with too many information we decided not to use the direction as a feature but to merge the direction with the strength by applying a coefficient of -1 when direction is down and 1 otherwise.</p><p>In addition to burst detection, we also consider the number of documents having a mention the last 24hours.</p><p>We noticed from our last year experiments on KBA12 that time features were actually degrading final results since when ignoring them our scores was better. So we decided to focus only on features (cf table 6) that can really bring useful time information. kleinberg 1h burst strength and direction match 24h</p><p># documents found last 24h To classify documents based on computed features, we designed several ways to handle it. The first method "TwoStep" we use, considers the problem as a binary classification problem where we use two classifiers in cascade. The first one C GN/U V is to classify between two classes: "Garbage/Neutral" and "Useful/Vital". For documents being classified as "Useful/Vital" the second classifier C U/V is used to determine the final output class between "Useful" and "Vital".</p><p>The second method "Single" performs directly a classification between the four classes.</p><p>The third method "VitalVSOthers" trains a classifier on recognizing vital documents amongst all others classes. When this classifier gives a non-vital class, the "Single" method is used to determine another class from "Garbage" to "Useful".</p><p>The last but not least method "CombineScores" uses scores emitted by all previous classifier and try to learn the best output class considering all classifiers scores for every classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System Outputs</head><p>To summarize, we have 5 different outputs possible with 4 different methods which makes 20 different runs. For the official run submission, we had issues with our system making our runs not consistent enough. In addition, we also had issues for extracting documents from stream-corpus which makes our system miss a lot of documents. The result of those experiments have been performed after the TREC conference held in november 2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result Analysis</head><p>All results have been computed using the official kba-scorer with the following command line provided in documentation. Moreover, the official metrics is the f 1 score which is an harmonic mean of precision and recall. Precision and Recall are both micro and macro average before computing f micro and f macro .</p><p>Since there are many results to discuss, I'll summary the more interesting ones. First of all the system which, uses no updates, performs better than any other system on micro-average point of view. The table <ref type="table" coords="5,95.35,245.03,4.98,8.64" target="#tab_2">7</ref> shows that most ranking methods perform equally besides VitalVsOther which has a score a bit below. When observing the different curves given by the official scorer, we noticed that our system has not a good recall which penalize a lot the scores. In addition, the official scorer is considering that documents appearing in test collection and not in the evaluate run as false negative. Let's consider the document not being found by our system and therefore not classified, it has been considered as if we classified it wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking</head><p>We implemented a similar scorer that consider only what has been classified using different cutoff.</p><p>When the confidence score is below the cutoff, the document is considered as false negative. We run the scorer on the both better results above and obtained the following results (we select the best trade off between precision and recall for each methods): Those two tables (9, 10) show that using a dynamic model really helps for both precision and recall. Therefore, the f-measure is also better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Perspectives</head><p>We present an approach to filter out entity centric documents from a stream. This approach has the pros to be adaptive and can therefore be used on entities which no training data have been provided for. In our last year system <ref type="bibr" coords="5,393.58,519.90,95.44,8.64" target="#b0">(Bonnefoy et al., 2013)</ref> we showed that even though the system has no training data for an entity, it is still able to find out some vital documents for the entity. We also show that time features as well as a dynamic model is valuable for the classification purposes although we have to improve our IR system which lake of performances.</p><p>We are currently working on improving our system to find out whether it may have any other way to update profile to improve even more precision. We also plan to investigate on a method to know when is the best moment to update a profile, whether some information must be forgotten inside the profile. Eventually, we will look at methods to improve the recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,314.23,394.49,53.97,8.74;3,425.11,394.49,33.76,8.74;3,314.23,406.84,52.00,8.74;3,434.96,406.84,14.06,8.74;3,314.23,420.60,50.35,8.74;3,391.07,417.64,6.51,6.12;3,391.07,425.63,12.91,6.12;3,406.13,420.60,97.29,9.65"><head></head><label></label><figDesc>w i , D)log 2 (p(w i , D))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.00,494.62,207.50,8.64;4,72.00,506.57,91.61,8.64;4,76.54,364.70,198.42,109.50"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Burst on different entities does not always imply vital documents.</figDesc><graphic coords="4,76.54,364.70,198.42,109.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,496.26,690.02,7.74,8.64"><head>Table 5 :</head><label>5</label><figDesc>Entity related features</figDesc><table coords="3,496.26,690.02,7.74,8.64"><row><cell>3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,296.50,156.91,207.50,192.88"><head>Table 6 :</head><label>6</label><figDesc>Time related features used for classificationWhen we update the dynamic model, we can choose to update either Vital or Vital and Useful documents which adds 2 different outputs. In total 5 outputs are computed.</figDesc><table coords="4,296.50,187.98,207.49,67.66"><row><cell>4.1 Classification</cell></row><row><cell>As a reminder of section 3.1.2, we implemented dif-</cell></row><row><cell>ferent ways to update (or not) a dynamic language</cell></row><row><cell>model:</cell></row><row><cell>-No Update: NO UPDT</cell></row></table><note coords="4,308.13,266.55,150.03,8.64;4,308.13,286.11,157.76,8.64"><p>-Update with Snippet: UPDT SNPT -Update with Document: UPDT DOC</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,287.53,207.50,239.41"><head>Table 7 :</head><label>7</label><figDesc>Methods f micro f macro Scores from system that do not update a dynamic model.</figDesc><table coords="5,72.00,300.27,207.50,226.66"><row><cell>Single</cell><cell>.400</cell><cell>.341</cell></row><row><cell>TwoStep</cell><cell>.418</cell><cell>.340</cell></row><row><cell>VitalVsOther</cell><cell>.383</cell><cell>289</cell></row><row><cell>Combine</cell><cell>.415</cell><cell>.327</cell></row><row><cell cols="3">The system that performs best on macro-average</cell></row><row><cell cols="3">point of view is the one where only vital document</cell></row><row><cell cols="3">are use to update dynamic model and the model is</cell></row><row><cell cols="3">updated with the snippet (UPDT SNIPPET VOnly)</cell></row><row><cell>(cf. table 8).</cell><cell></cell><cell></cell></row><row><cell cols="3">Ranking Methods f micro f macro</cell></row><row><cell>Single</cell><cell>.458</cell><cell>.316</cell></row><row><cell>TwoStep</cell><cell>.452</cell><cell>.303</cell></row><row><cell>VitalVsOther</cell><cell>.428</cell><cell>.273</cell></row><row><cell>Combine</cell><cell>.456</cell><cell>.305</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.00,545.02,207.49,20.59"><head>Table 8 :</head><label>8</label><figDesc>Scores from system that uses update using snippet of vital documents only.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,296.50,132.65,207.50,222.71"><head>Table 9 :</head><label>9</label><figDesc>Scores from alternative scorer to measure classifier performance on system without update.</figDesc><table coords="5,306.47,132.65,180.41,222.71"><row><cell cols="2">Scores for No Update:</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Precision Recall</cell><cell>f 1</cell></row><row><cell>Single</cell><cell>.652</cell><cell>.423</cell><cell>.513</cell></row><row><cell>TwoStep</cell><cell>.627</cell><cell>.453</cell><cell>.526</cell></row><row><cell>VitalVsOther</cell><cell>.716</cell><cell>.391</cell><cell>.506</cell></row><row><cell>Combine</cell><cell>.669</cell><cell>.434</cell><cell>.527</cell></row><row><cell cols="3">Scores for Update Snippet with Vital:</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Precision Recall</cell><cell>f 1</cell></row><row><cell>Single</cell><cell>.771</cell><cell>.422</cell><cell>.545</cell></row><row><cell>TwoStep</cell><cell>.726</cell><cell>.427</cell><cell>.537</cell></row><row><cell>VitalVsOther</cell><cell>.791</cell><cell>.386</cell><cell>.519</cell></row><row><cell>Combine</cell><cell>.750</cell><cell>.425</cell><cell>.542</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,296.50,373.44,207.50,20.59"><head>Table 10 :</head><label>10</label><figDesc>Scores from alternative scorer to measure classifier performance on system without update.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,72.00,78.99,207.49,8.64;6,81.96,89.95,197.53,8.64;6,81.96,100.73,197.53,8.82;6,81.96,111.69,107.82,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,147.86,89.95,131.64,8.64;6,81.96,100.91,163.32,8.64">A Weakly-Supervised Detection of Entity Central Documents in a Stream</title>
		<author>
			<persName coords=""><forename type="first">Ludovic</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Bouvier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,254.60,100.73,24.90,8.59;6,81.96,111.69,17.93,8.59">SIGIR 2013</title>
		<imprint>
			<date type="published" when="2013-02">2013. February</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,130.79,207.49,8.64;6,81.96,141.57,197.53,8.82;6,81.96,152.53,31.84,8.59" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,152.62,130.79,126.87,8.64;6,81.96,141.75,153.05,8.64">Large-Scale Named Entity Disambiguation Based on Wikipedia Data</title>
		<author>
			<persName coords=""><surname>Cucerzan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>EMNLP-CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.00,171.64,207.49,8.64;6,81.96,182.60,197.53,8.64;6,81.96,193.56,197.53,8.64;6,81.96,204.34,89.17,8.59" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,171.75,182.60,107.74,8.64;6,81.96,193.56,193.04,8.64">Building an Entity-Centric Stream Filtering Test Collection for TREC 2012</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>of The 21th TREC</note>
</biblStruct>

<biblStruct coords="6,72.00,223.45,207.50,8.64;6,81.96,234.23,197.53,8.82;6,81.96,245.18,15.21,8.59" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,149.94,223.45,129.56,8.64;6,81.96,234.23,197.53,8.82;6,81.96,245.18,11.41,8.59">Bursty and hierarchical structure in streams. Data Mining and Knowledge Discovery</title>
		<author>
			<persName coords=""><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
