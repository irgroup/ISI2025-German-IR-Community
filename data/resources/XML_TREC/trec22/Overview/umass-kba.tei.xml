<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,124.31,80.99,363.39,12.90;1,129.70,98.92,352.60,12.90">UMass at TREC 2013 Knowledge Base Acceleration Track: Bi-directional Entity Linking and Time-aware Evaluation</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_jHHsqsu">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_9Zrn2EF">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_M2KVt9t">
					<orgName type="full">IBM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.78,150.53,61.43,10.75"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
							<email>dietz@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.51,150.53,72.97,10.75"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
							<email>jdalton@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,124.31,80.99,363.39,12.90;1,129.70,98.92,352.60,12.90">UMass at TREC 2013 Knowledge Base Acceleration Track: Bi-directional Entity Linking and Time-aware Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0B72B55681CEC2719F2A65AE05295754</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This notebook details the participation of the University of Massachusetts Amherst in the Cumulative Citation Recommendation task (CCR) of the TREC 2013 Knowledge Base Acceleration Track. Our interest in TREC KBA is motived by our research on entity-based query expansion. Query expansion is a information retrieval technique for improving recall by augmenting the original query terms with other terms that are likely to indicate relevant documents. Such expansion terms can be inferred with pseudo-relevance feedback techniques <ref type="bibr" coords="1,72.00,397.62,117.22,8.64" target="#b4">(Lavrenko and Croft, 2001)</ref>. The resulting retrieval model can be interpreted as a weighted mixture model including the original retrieval model and retrieval models for each expansion term.</p><p>Instead of expanding the query with terms, our research is on expanding the query with relevant entities from a knowledge base. Such entities are very rich in structure, including name variants, related entities and associated text. An essential component of our entity-based query expansion is to derive a retrieval model for a given knowledge base entity, which can be incorporated into the weighted mixture model. We study the effectiveness of different entity-based retrieval models within the TREC KBA Cumulative Citation Recommendation task.</p><p>However, we do not address the novelty aspects of the task, and therefore do not distinguish between 'vital' and 'useful' documents. This year we only evaluate memoryless methods, i.e., the prediction is not influenced by predictions on previous time intervals. We segment the stream into week-long intervals which are filtered independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structured Entity Data</head><p>First we study different ways to derive a retrieval model from an entity in a knowledge base such as Wikipedia. Our methods assume access to different kinds of structured information about the entity: 1) a canonical name such as the Wikipedia title; 2) a set of alternative names with associated confidences; 3) links or relations to other entities; 4) optional free text introducing the entity.</p><p>We preprocessed a 2012 Wikipedia Wex dump to make all four kinds of data available easily (more information available in <ref type="bibr" coords="1,363.01,319.51,104.43,8.64">(Dalton and Dietz, 2013b)</ref>). Although similar information can be gathered for twitter entities as well, we did not have a twitter corpus available. Instead we vary the method only for Wikipedia entities, where all twitter entities are predicted with the "SDM" method. The evaluation in this paper only considers Wikipedia entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Retrieval Methods</head><p>We explore readily available retrieval models and study which kinds of structured entity information provide the most value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Traditional IR Models: SDM and RM3</head><p>The simplest approach is to use the canonical name as an information retrieval query. We use the sequential dependence retrieval model which scores documents by frequency of unigrams, bigrams and windowed skipbigrams of the query string, taking document length and corpus wide term statistics into account. Given the query string q, the retrieval score, log L, is computed according to Equation 1, which is also referred to via the query operator #sdm. In contrast, a query consisting of unigram terms only is represented by L unigram , which is also referred to as query likelihood.</p><formula xml:id="formula_0" coords="1,313.20,638.42,239.67,51.60">log L SDM (d|q) rank = i λ T log L unigram (d|q i ) (1) +λ B log L bigram (d|q i , q i+1 ) +λ W log L window (d|q i , q i+1 )</formula><p>The unigram model is given in Equation <ref type="formula" coords="1,484.87,701.56,4.98,8.64">2</ref>where tf referring to the term frequency of the query term q i in the Method Retrieval Model sdm #sdm(canonical name) rm w q #sdm(canonical name)+(1 -w q ) pseudoterm w pseudoterm • log L unigram (d|pseudo term) rn w q #sdm(canonical name)+(1 -w q ) name w name •#sdm(name) rt w q #sdm(canonical name)+(1 -w q ) articleterm w articleterm • log L unigram (d|article term) rtn w q #sdm(canonical name)+ </p><formula xml:id="formula_1" coords="2,224.03,288.31,74.77,28.29">tf q i ,C |C| |d| + µ (2)</formula><p>All our IR methods associate a confidence with each document which is proportional to the retrieval score. The method "sdm" issues a #sdm query with the cannonical entity name (e.g. the Wikipedia title of the query entity).</p><p>The method "rm" refers to a sequential dependence model wich is expanded with pseudo-relevance feedback. This refers to a two-pass method, where first a sequential dependence query is issued to retrieve a few top ranked documents from the stream corpus. Assuming that the retrieval score indeed captures the degree with which the document is relevant for the query, a distribution over terms is extracted as follows: A language model is build from each retrieved document to be proportional to the term frequency. Using multinomial mixture weights proportional to the exponentiated retrieval score L SDM (d|q), the language models are combined into a mixture model (cf. Equation <ref type="formula" coords="2,126.98,533.64,3.74,8.64" target="#formula_2">3</ref>, where normalization constant Z is to ensure the components sum to 1).</p><formula xml:id="formula_2" coords="2,127.44,567.16,171.36,26.88">w t = 1 Z d L SDM (d|q) tf t,d |d|<label>(3)</label></formula><p>The k most probable terms under this distribution are used to expand the sequential dependence query using the weight w t (referred to as w pseudoterm in Table <ref type="table" coords="2,255.03,627.02,3.60,8.64">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KB-based Retrieval Models: Names and Text</head><p>In the following, we extend the "sdm" methods by incorporating further names and text from the knowledge base.</p><p>The set of alternative names is exploited in the method "rn". We extract alternative names from structured data available for Wikipedia entities, including name variants from redirect pages, Freebase alternative names, and anchor text of links within Wikipedia. These names are combined into a mixture of sequential dependence retrieval models, weighted by the confidence of respective names.</p><p>We assign a disambiguation confidence for each name from anchor text. For each possible name of this entity, we derive the score as the fraction of hyper links with this name as anchor text that refer to the query entity. We also apply this scheme to Wikipedia redirects and Freebase names (which we treat as twice as trustworthy as anchor text) and compute a combined model of disambiguating names for the query entity.</p><p>The retrieval model incorporates the names with the highest disambiguation score as a mixture model of sequential dependence models for each name. The disambiguation scores are used as weights w name for each mixture component. The name model is combined with a sequential dependence model on the canonical name as in the "sdm" method. Details are given in Table <ref type="table" coords="2,505.74,437.08,3.74,8.64">1</ref>.</p><p>We further explore the use of terms extracted from the text that is associated with the entity. For method "rt", we use the text of the Wikipedia article to build a term model, after removing stopwords and normalizing punctuation. The top terms are used in a mixture model of unigram language models with the term probabilities as weights w articleterm . We notice that the text also includes mentions of the query entity under different names as well as mentions of strongly related entities.</p><p>However, we additionally explore the use of extending the canonical name with both disambiguating names and frequent terms in the method "rtn".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Knowledge Sketch Approach</head><p>As motivated in the introduction, our research goal is to retrieve relevant entities, documents and relations for a given query. As the approach is currently under submission, we omit details here, but refer the interested reader to a preliminary workshop writeup <ref type="bibr" coords="2,462.65,665.48,77.35,8.64;2,313.20,677.43,25.85,8.64">(Dalton and Dietz, 2013a)</ref>.</p><p>We apply the knowledge sketch approach in method "skq" using the canonical name as a query. The method will retrieve relevant entities, which are used to expand the original query with named of relevant (neighbor) entities to retrieve documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Converting Retrieval Scores to Confidences</head><p>We view stream filtering as a continuous task, where a user checks the pool of predicted documents in regular time intervals, for instance one a week. At every check point the user would see a ranking of the most confident top 1000 ranks and with the option to stop inspecting lower ranks, e.g. when precision sinks below a threshold. We simulate this scenario by scoring documents in a stream fashion and assign confidences that would represent the i'th rank.</p><p>We learn this mapping from document score to confidence rank by generating a document ranking on weeklong subcorpora of the training period. In particular, we choose the weeks 2011-49 and 2012-07 (given in calendar week of the year) and generated rankings across all entities. We take the maximum of the score obtained on rank 1 as an equivalent of confidence 1000 and the minimum score obtained on rank 1000 to be equivalent to confidence 1. We project retrieval scores linearly onto this confidence interval. The stream is filtered by computing the retrieval model score under each document and project it onto the confidences.</p><p>Since we expect the different retrieval models to have drastically varying scores (which are rank equivalent to unnormalized log-probabilities) we learn a different score-confidence mapping for each method. As a result, the confidence cutoffs are not comparable across our methods.</p><p>We want to point out that no training judgments are used in our process. The heuristic only requires two weeks of the training corpus to identify the range of scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Indexing</head><p>Our retrieval models are memory-less, they do not learn over time and predictions from the previous time interval do not affect the predictions of the next. We parallelize the document filtering by creating several indexes of week-long segments of the stream. We use galago 3.4 for indexing with the indexing parameters listed in Figure <ref type="figure" coords="3,72.00,598.92,3.74,8.64">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Linking back to Entities</head><p>We anticipate that the information retrieval methods may have problems distinguishing mentions of the query entity from entities with similar names. We explore the utility of our entity linking tool 1 to refine the document scores produced by the SDM method. Due to time con-1 code available at http://ciir.cs.umass.edu/~jdalton/kbbridge/ straints for the submission deadline, we simulate the method on the two top scoring documents per week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity Linking</head><p>Our entity linking method first detects named entities in the retrieved document (using Factorie's NLP Pipeline<ref type="foot" coords="3,530.21,136.53,3.49,6.05" target="#foot_0">2</ref> ). For each mention we issue a query against a search index of Wikipedia articles, which includes structured information such as linked articles and anchor text. The query is a combination of the mention and the name variants from the coreference resolution. For each mention, the top 50 Wikipedia entities are taken as candidates to be reranked with supervised learning-to-rank method (using a boosted decision tree <ref type="bibr" coords="3,399.34,233.84,67.12,8.64" target="#b3">(Friedman, 2001)</ref>). Features for the supervision include different kinds of simlarity between mention string and Wikipedia title, surrounding named entities to Wikipedia neighbors, as well as terms from the context and the Wikipedia article. Optionally, NIL classification is applied. The method is detailed in <ref type="bibr" coords="3,492.78,293.61,47.22,8.64;3,313.20,305.57,53.51,8.64">(Dalton and Dietz, 2013b)</ref>, with the retrieval method based on query and name variants ("QV"), features for the learning-torank method and NIL classification. For every mention in the document we keep the 50 retrieved candidate entities around with supervised re-ranking score and NIL prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deriving Document Score</head><p>Next we inspect all entity links in the document for links towards the query entity. We evaluate the following heuristics for deriving a score for the document:</p><p>• T2ELMax / "link": Maximum re-ranking score of the query entity for any mention, independent of the rank (inspecting all 50 candidates).</p><p>• T2ELMax_1 / "link NIL": Maximum re-ranking score of the query entity for any mention, independent of the rank, as long as it is not classified as NIL.</p><p>• T2ELMax_TO / "link Top": Maximum re-ranking score of the query entity for any mention, only if the query entity is the top ranked entity.</p><p>• T2ELMax_TO_1 / "link Top NIL": Maximum reranking score of the query entity for any mention, only if the query entity is the top ranked entity and not classified as NIL.</p><p>• t2LinkProb / "link LM": Probability under a multinomial distribution over linked Wikipedia entities; Distribution is build from top ranked (non-NIL) links per mention in the fashion of a language model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We participate in the CCR task, with the goal of predicting documents in the "useful" class.</p><p>Figure <ref type="figure" coords="4,351.13,422.67,4.98,8.64" target="#fig_2">3</ref> (top) marks our best IR run the "sdm" method, indicating that our methods perform competitively. The bottom circle marks all our entity linking runs, which (as expected) have a much lower recall, since entity links are only annotated for the the top two documents per week from the "sdm" method.</p><p>As our submission did not focus on twitter entities, nor the novelty aspects, we analyze our results with the scoring script options 'Wikipedia-only', 'vital+useful', and 'require-positives'. Figures <ref type="figure" coords="4,446.57,532.23,17.70,8.64">1a-d</ref> display the Precision/Recall/F1/SU over cutoff ranks with standard error tubes. The presented runs comprise the method "sdm", expanding with entity names ("rn"), entity linking with max score ("link"), and the entity linking language model approach ("link LM"). For comparison, 4a displays the precision of all our runs in one plot. The "sdm" method reachest a maximum of 0.57, with a decent recall of 0.75. Expansion with names and terms from Wikipedia yields better precision on high cutoffs. The entity link language model starts with a high precision that increases slightly.</p><p>We suspect that our conversion from retrieval score to confidence is flawed. To distinguish error sources, we analyze our methods via the ranking induced by confidence values. As the plots over confidence cutoffs ignore documents that were not assessed by the annotators, we omit them from the ranking as well. Figures <ref type="figure" coords="5,232.36,75.48,4.98,8.64">2</ref> and<ref type="figure" coords="5,257.28,75.48,9.96,8.64" target="#fig_4">4b</ref> present plots of precision at rank k, where rank 1 corresponds to cutoff 999, rank 2 to cutoff 998, etc.</p><p>We are glad to see that across all methods precision decreases over the ranks, indicating that on average, useful documents are located on higher ranks than not useful documents. The methods "sdm" and "skq" perform the best. We are surprised that the "sdm" method-which was originally our base line-outperformed all methods in terms of precision and also achieved a stunning recall of 0.72. The KB-based retrieval models perform worse in terms of precision, there the combination of both terms and names is slightly worse than expansion with either source. We find that the top 100 of all IR models contain about one third of documents that were not assessed by annotators. This bears the potential of changing the ranking among these methods.</p><p>The entity linking methods were intended to increase the precision for the "sdm" method, of which two documents per week were considered. It seems that this is not the case, as the precision of 0.55 in the top 10 levels out quickly to 0.46-0.48. Error analysis revealed that although only 10% of the documents in top 100 were not assessed by annotators, we only predicted on average 20 documents per entity-a number that is way too conservative to be useful in practice. Furthermore, for several entities no documents were predicted, which attributed scores of zero in the plotted macro-average; the corresponding micro-average is about 0.62. This explains why the entity linking heuristics that only consider query entities on top1 and/or if not NIL perform worse than their less restrictive counterparts.</p><p>The relevance model on pseudo-relevance feedback (method "rm") performs with worst precision and mediocre recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Time-aware Analysis</head><p>In our paper at the TAIA SIGIR 2013 workshop <ref type="bibr" coords="5,264.35,527.20,34.45,8.64;5,72.00,539.15,38.10,8.64">(Dietz et al., 2013)</ref> we suggested the use of time-aware evaluation paradigm as an alternative<ref type="foot" coords="5,177.34,549.44,3.49,6.05" target="#foot_1">3</ref> . The difference is visualized in Figure <ref type="figure" coords="5,110.30,563.06,3.74,8.64" target="#fig_5">5</ref>. Merging all weekly rankings into one overall ranking by confidence score indicates the "sdm" model as a clear winner and places name alias expansion ("rn") on the second rank.</p><p>In contrast, evaluating performance on a weekly basis demonstrates that name alias expansion is consistently worse than "sdm", while the entity linking methods outperform "sdm" on many weeks and are especially strong around ETR day 100. Macro-averaging across weeks confirms the similar performance of "sdm" (MAP 0.031)  This discrepancy is in analogy to discrepancy between micro-and macro-averages: It is more difficult (and more useful) to consistently predict good results across all weeks if they vary in difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Treatment of Unjudged Documents</head><p>We have noticed a rather large discrepancy between the findings of the official TREC KBA scorer and our timeaware evaluation method and rather low MAP scores in the time-aware analysis. It turns out that the main difference is in the way documents with missing relevant/nonrelevant judgments are treated. While our time-aware evaluation followed the general practice in information retrieval to count unjudged documents as true negatives, the official KBA scorer removed any unjudged documents before the evaluation.</p><p>Figure <ref type="figure" coords="5,351.17,665.69,4.98,8.64" target="#fig_7">6</ref> depicts this effect on methods "sdm" and "link LM" with respect to Precision@10. We see that this inverts the finding. In fact, most of the documents in the high ranks (across both entities and weeks) are unjudged, where the many judged documents are found on ranks   beyond 10. The sparsity of judgments in our high ranks comes from how documents are selected for assessment by the judges. Unlike other TREC tracks, which generate pools from contributed submissions, the KBA pools are selected by a system developed by the track organizers. The official scores represent which systems would improve the pool generating method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents the submission of the Center for Intelligent Information Retrieval at the University of Massachusetts to TREC KBA. Based on the idea of casting entity tracking as a retrieval problem we presented several methods that leverage the rich structure of knowl-edge base entities using IR, NLP, and supervised reranking. Information retrieval is used both to retrieve relevant documents and for entity linking-we refer to the combination as bi-directional entity linking. Even without entity-specific training, the retrieval methods give rise to a reasonable document filter. We were surprised that the sequential dependence model ("sdm"), originally intended as a baseline performs provides the highest precision, recall, and runtime performance.</p><p>However, we suspect that the "sdm" method yielded many non-relevant documents in the top ranks, which were effectively filtered out by the pool-generating method and are therefore not reflected in the evaluation score. In contrast, the entity linking method was intended to be a high-precision method selecting only up 6 to two documents per week and entity. Analysis in Figure 6b confirms that the entity linking method retains four times more relevant documents in the top 10 than the "sdm" method. Furthermore, our time-aware evaluation paradigm shows that sdm and entity linking are retrieving weekly rankings of equal mean-average precision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,225.73,195.01,160.55,8.64;4,191.49,226.99,117.00,87.75"><head></head><label></label><figDesc>Figure 1: P/R/F over confidence cutoffs.</figDesc><graphic coords="4,191.49,226.99,117.00,87.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,238.45,340.04,135.09,8.64;4,310.98,226.99,117.00,87.75"><head></head><label></label><figDesc>Figure 2: P/R/F over rank cutoffs.</figDesc><graphic coords="4,310.98,226.99,117.00,87.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,72.00,686.02,226.80,8.64;4,72.00,697.97,226.80,8.64;4,72.00,709.93,174.07,8.64;4,73.89,440.15,226.80,234.03"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: UMass_CIIR runs in comparison. Blue circle on top marks the "sdm" method; the blue circle at the bottom marks the "entity link LM" method.</figDesc><graphic coords="4,73.89,440.15,226.80,234.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,344.80,417.56,163.60,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision of all submitted runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,151.52,268.41,308.96,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overall MAP score (left) versus average over weekly scores (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,81.89,484.45,140.29,6.91;6,247.46,484.45,134.58,6.91"><head></head><label></label><figDesc>(a) Unjudged ignored (official KBA scorer).(b) Unjudged as negative (our evaluation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,187.71,504.32,236.58,8.64"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Treatment of documents with missing judgments.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,327.55,714.81,89.58,6.91"><p>http://factorie.cs.umass.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="5,86.35,695.88,212.45,6.91;5,72.00,705.35,226.80,6.91;5,72.00,714.81,213.33,6.91"><p>Updated code for KBA-2013 format is available at github.com/ laura-dietz/kba-y2-streameval/ with more plots and information at ciir.cs.umass.edu/~dietz/streameval/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part by <rs type="funder">IBM</rs> subcontract #<rs type="grantNumber">4913003298</rs> under DARPA prime contract #<rs type="grantNumber">HR001-12-C-0015</rs>, and in part by <rs type="funder">NSF</rs> grant #<rs type="grantNumber">IIS-0910884</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_M2KVt9t">
					<idno type="grant-number">4913003298</idno>
				</org>
				<org type="funding" xml:id="_jHHsqsu">
					<idno type="grant-number">HR001-12-C-0015</idno>
				</org>
				<org type="funding" xml:id="_9Zrn2EF">
					<idno type="grant-number">IIS-0910884</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>" f i l e t y p e " : " xz " , " p a r s e r " : { " e x t e r n a l P a r s e r s " : [ { " f i l e t y p e " : " xz " , " c l a s s " : " o r g . l e m u r p r o j e c t . g a l a g o . c o n t r i b . p a r s e . TrecKBA2013Parser " } ] } , " t o k e n i z e r " : { " f o r m a t s " : { " k b a d a t e " : " l o n g " , " k b a s t r e a m t i c k s " : " l o n g " , " k b a s t r e a m t i m e s t a m p " : " s t r i n g " } ,</p><p>" f i e l d s " : [ " t i t l e " , " k b a d a t e " , " k b a s t r e a m t i c k s " , " k b a s t r e a m t i m e s t a m p " , " k b a t y p e " ] } , " f i e l d I n d e x P a r a m e t e r s " : { " s t e m m e d P o s t i n g s " : f a l s e } , " s t e m m e d P o s t i n g s " : f a l s e } Figure <ref type="figure" coords="7,341.85,522.42,3.88,8.64">7</ref>: Index configuration parameters for Galago 3.4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,72.00,293.19,226.80,7.77;7,81.96,303.99,216.84,7.93;7,81.96,314.95,216.84,7.93;7,81.96,326.06,35.76,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,225.88,293.19,72.92,7.77;7,81.96,304.15,89.25,7.77">Constructing queryspecific knowledge bases</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,188.21,303.99,110.59,7.72;7,81.96,314.95,174.58,7.72">Proceedings of the 2013 workshop on Automated knowledge base construction</title>
		<meeting>the 2013 workshop on Automated knowledge base construction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,337.00,226.80,7.77;7,81.96,347.80,216.84,7.93;7,81.96,358.76,216.84,7.72;7,81.96,369.72,84.04,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,238.17,337.00,60.64,7.77;7,81.96,347.96,125.55,7.77">A neighborhood relevance model for entity linking</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,229.37,347.80,69.43,7.72;7,81.96,358.76,216.84,7.72;7,81.96,369.72,22.33,7.72">Proceedings of the 10th Conference on Open Research Areas in Information Retrieval</title>
		<meeting>the 10th Conference on Open Research Areas in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,380.81,226.81,7.77;7,81.96,391.77,216.84,7.77;7,81.96,402.57,216.84,7.93;7,81.96,413.53,113.81,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,277.21,380.81,21.60,7.77;7,81.96,391.77,216.84,7.77;7,81.96,402.73,26.81,7.77">Timeaware evaluation of cumulative citation recommendation systems</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,126.97,402.57,171.83,7.72;7,81.96,413.53,60.45,7.72">Proceeding of the Workshop on Time-aware Information Access</title>
		<meeting>eeding of the Workshop on Time-aware Information Access</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="10" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,424.62,226.81,7.77;7,81.96,435.43,216.84,7.93;7,81.96,446.54,20.17,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,175.07,424.62,123.74,7.77;7,81.96,435.58,92.56,7.77">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName coords=""><surname>Jerome H Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,181.89,435.43,67.11,7.72">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.00,457.47,226.80,7.77;7,81.96,468.28,216.84,7.93;7,81.96,479.24,216.84,7.72;7,81.96,490.19,216.84,7.93;7,81.96,501.31,82.92,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,238.64,457.47,60.16,7.77;7,81.96,468.43,58.42,7.77">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,157.08,468.28,141.72,7.72;7,81.96,479.24,216.84,7.72;7,81.96,490.19,130.93,7.93">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;01</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
