<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,104.06,83.07,386.91,12.22;1,172.89,114.27,249.26,12.22">Applying the Query Change Retrieval Model on Session Search -Georgetown at TREC 2013 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.43,145.47,78.14,12.22"><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street NW</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,322.80,145.47,54.79,12.22"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>huiyang@cs.georgetown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street NW</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,104.06,83.07,386.91,12.22;1,172.89,114.27,249.26,12.22">Applying the Query Change Retrieval Model on Session Search -Georgetown at TREC 2013 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">98875EFD62A5689A2965E5FEDF4D5079</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2013 Session Track, we experiment the Query Change Model (QCM) for session search on the new document collection ClueWeb12 CatB. We use structured query formulation and pseudo-relevance feedback for RL1. The QCM approach is used in RL2 and it studies query change between adjacent queries and models session search as a Markov Decision Process. We further add information from other sessions by a majority vote of cross-session clicked data to the model in RL3. Comparing the retrieval accuracy in RL1 with that in RL2 and RL3, we obtain 46.1% and 46.6% improvements, respectively. We present an analysis and discussion on the dataset difference between ClueWeb12 Cat A and Cat B, the difference between TREC2012 and TREC2013 session, and their impact on the retrieval accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Session search is the Information Retrieval task that retrieves documents for the entire session. TREC 2013 Session Track aims to retrieve relevant documents for the last query q n in a session. A session contains a sequence of queries 𝑞 ! , 𝑞 ! … 𝑞 !!! , 𝑞 ! , the ranked list of URLs for each past query, the clicked urls, and the time spent on those webpages. In TREC 2013 Session Track, there are three subtasks: RL1, a subtask that only allows using the last query 𝑞 ! and ignoring all other information in the query log; RL2, a subtask that only allows using information from the prior history in the same session; RL3, a subtask that allows using any information available in the query log, including those from other sessions.</p><p>For RL1, we use structured query formulation in combination with Lemur's language modeling approach. <ref type="foot" coords="1,129.00,477.27,3.24,5.69" target="#foot_0">1</ref> For RL2, we employ an approach based on the Query Change Model <ref type="bibr" coords="1,430.28,479.61,10.59,8.64" target="#b1">[1]</ref>, which models session search as a Markov Decision Process. For RL3, we use a majority vote of cross-session clicked data for further improvement. In the following sections, we present our methods and describe and analyze the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RL1 -The Basic Retrieval Model</head><p>Our approach for RL1 is based on the Multinomial language models with Dirichlet smoothing <ref type="bibr" coords="1,491.02,553.29,10.59,8.64" target="#b2">[2]</ref>. We calculate the term weight P(t|d) as:</p><formula xml:id="formula_0" coords="1,244.47,575.56,124.05,23.15">𝑃 𝑡 𝑑 = 𝑇𝐹 𝑡, 𝑑 + 𝜇𝑃(𝑡|𝐶) 𝑙𝑒𝑛𝑔𝑡ℎ 𝑑 + 𝜇</formula><p>where t is the term and d is the document under evaluation, length(d) is the length of the document, 𝑃 𝑡 𝐶 calculates the probability that t appears in corpus C based on the Maximum-Likelihood Estimation (MLE), and 𝜇 is the Dirichlet smoothing parameter.</p><p>We use two different methods to formulate structured queries. Both are based on query term groupings provided by quotation marks in the original query. The Strict Quotation Method always considers a quoted phrase as a single term. For example, "federal government" is grouped as one term and we write the Lemur query as #1(federal government). The Relax Quotation Method considers the quoted phrase as one single term only when the quoted parts frequently appear in the query log. In RL1, we employ the Strict Quotation Method before sending queries to Lemur. Pseudo relevance feedback is also used in RL1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RL2 -The Query Change Retrieval Model</head><p>During a session, users usually continuously modify their queries to change expressions to better describe their information needs. In our recent work <ref type="bibr" coords="2,317.25,99.69,10.59,8.64" target="#b1">[1]</ref>, we consider query change as a form of relevance feedbacks, and propose to model session search as a Markov Decision Process (MDP). Our RL2/RL3 approaches for TREC 2013 Session Track are based on this QCM model. Here we introduce how we apply this model. First, we define the query change between adjacent queries as the syntactic editing differences of q i and q i-1 : Δ𝑞 ! = 𝑞 ! -𝑞 !!! . Then, we decompose each adjacent query pair into three parts: the added terms (+Δ𝑞 ! ) the removed terms (-Δ𝑞 ! ) and the theme terms (𝑞 !!!"! ) <ref type="bibr" coords="2,471.93,157.77,10.59,8.64" target="#b1">[1]</ref>.  <ref type="table" coords="2,115.89,304.17,4.92,8.64" target="#tab_0">1</ref> shows an example of TREC 2013 Session queries and the corresponding query changes. The QCM model evaluates the relevance between a document d and the current query q i as in the following equation:</p><formula xml:id="formula_1" coords="2,165.87,354.76,281.24,20.83">𝑆𝑐𝑜𝑟𝑒(𝑞 ! , 𝑑) = 𝑃 𝑞 ! 𝑑 + γ 𝑃(𝑞 ! |𝑞 !!! , 𝐷 !!! , 𝑎) ! max ! !!! 𝑃(𝑞 !!! |𝐷 !!! )</formula><p>where γ is the discount factor. In an Markov Decision Process, this discount factor γ is usually used for future states; however, we use it in a reversed order to discount the prior queries. When γ = 1, there is no discount for the past queries.</p><p>Considering different query changes as user's feedback, we utilize four kinds of term weight adjustments based on the types of their query changes. 𝑊 !!!"! , 𝑊 !"",!" , 𝑊 !"",!"# , 𝑊 !"#$%" are the term weighting adjustments <ref type="bibr" coords="2,207.81,447.21,10.59,8.64" target="#b1">[1]</ref>. We then aggregate the relevance score between the query q i and a document d. Thus, the query to document relevance score can be calculated as:</p><formula xml:id="formula_2" coords="2,148.10,482.20,316.21,10.27">𝑆𝑐𝑜𝑟𝑒(𝑞 ! , 𝑑) = log 𝑃 𝑞 ! 𝑑 + 𝛼𝑊 !!!"! -𝛽𝑊 !"",!" + 𝜀𝑊 !"",!"# -𝛿𝑊 !"#$%"</formula><p>where log 𝑃 𝑞 ! 𝑑 is the query-document relevance scoring function in the log form. Parameters 𝛼, 𝛽, 𝜀 𝑎𝑛𝑑 𝛿 are the linear weighting coefficients for each type of query changes. We empirically set the parameters as 𝛼 = 2.2, 𝛽 = 1.8, 𝜀 = 0.07 𝑎𝑛𝑑 𝛿 = 0.4 in the submissions. The final relevance score between the last query q n and the document is calculated as:</p><formula xml:id="formula_3" coords="2,211.02,550.12,172.96,33.79">𝑆𝑐𝑜𝑟𝑒 !"!!#$% 𝑞 !, 𝑑 = 𝛾 !!! 𝑆𝑐𝑜𝑟𝑒(𝑞 !, 𝑑) ! !!!</formula><p>where 𝑆𝑐𝑜𝑟𝑒 (𝑞 ! , 𝑑) = log 𝑃 𝑞 ! 𝑑 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RL3 -Utilizing Cross-Session Information</head><p>The RL3 subtrack in this year is the first time that participants are allowed to use prior information from other sessions. Since sessions about different topics may barely contribute to each other, we only apply the cross-session information from sessions on the same topics. The topic to session mapping is provided by TREC.</p><p>Considering the SAT Clicked Documents (clicked documents that have more than 30 seconds dwell time) may show higher relevance to the information need, we take all SAT Clicks from sessions of the same topic and perform a majority vote. In each session s i , the document d relevant (s i ) that appears most as SAT Clicks in sessions sharing the same topic is considered as "relevant" and is ranked as the top on the result list:</p><formula xml:id="formula_4" coords="3,219.81,79.24,155.37,20.83">𝑑 !"#"$%&amp;' (𝑠 ! ) = argmax ! 𝑓(𝑑, 𝑠, 𝑠 ! ) !</formula><p>where d represents a document under comparison, s represents a session in the entire TREC 2013 Session query log. The voting function 𝑓 • is defined as the following:</p><formula xml:id="formula_5" coords="3,184.04,135.64,226.91,21.23">𝑓 𝑑, 𝑠, 𝑠 ! = 1, 𝑑 ∈ 𝑆𝐴𝑇 𝑠 , 𝑡𝑜𝑝𝑖𝑐(𝑠) = 𝑡𝑜𝑝𝑖𝑐(𝑠 ! ) 0, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒.</formula><p>where 𝑆𝐴𝑇 𝑠 is the set of SAT Clicked documents in session s, and 𝑡𝑜𝑝𝑖𝑐(𝑠) represents for the topic id of session s. Document d gets a vote from session s if d is an member of 𝑆𝐴𝑇(𝑠) while both session s and s i share same topic. The total votes for d is the sum of votes from each session, and the selected 𝑑 !"#"$%&amp;' (𝑠 ! ) is the document that receives the most votes.</p><p>We further classify the sessions into two types: comparison sessions and top-down sessions. We notice that 19 sessions in TREC 2013 compare two themes or concerning both sides of the same theme. On the other hand, other sessions' topics follow a top-down hierarchical structure and all queries are related to a single theme. We classify the sessions into these two session types and treat them differently during parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments 5.1 Dataset and Metrics</head><p>Our system is built on top of the Lemur search engine. We use ClueWeb12 Category B (CatB) for TREC 2013 submissions. nDCG@10 is the main evaluation metric. All parameter tuning are based on TREC 2012 and ClueWbe09 CatB. Previous research shows that the document collections like ClueWeb09 and ClueWeb12 contains many spam documents. We filter spam documents according to Waterloo Spam Ranking for both corpora.</p><p>23 All documents with spam ranking scores less than 70 are filtered out before the indexing takes place. No topic descriptions are used in the submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TREC 2013 Submissions</head><p>We submitted three runs to TREC 2013 Session Track. The run names, methods, and major parameters are listed in Table <ref type="table" coords="3,164.29,697.77,3.73,8.64">2</ref>. µμ is the Dirichlet smoothing parameter, k is the number of pseudo relevance http://www.mansci.uwaterloo.ca/~msmucker/cw12spam/  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>We classify all 87 TREC 2013 sessions into "Good", "Average", "Bad", and "Difficult" based on how our runs perform using the official TREC reported nDCG@10. Low nDCG@10 score means a bad or difficult session while high nDCG@10 score means a good session. Figure <ref type="figure" coords="4,396.88,462.81,4.92,8.64" target="#fig_1">1</ref> shows the distribution of the session difficulty levels. In total, we find 22 sessions as "Good", 34 as "Average", 19 as "Bad", and 12 as "Difficult".</p><p>Table <ref type="table" coords="4,115.92,508.65,4.92,8.64" target="#tab_1">3</ref> shows the nDCG@10 scores for the sessions just mentioned for GURun2, the TREC median run, and the TREC max run. In the "Good" sessions, our runs perform very well and have leading results. These sessions tend to show clear query editing patterns of adding and removing terms, which our model is designed for. For example, our run GURun2 gives the best performance among all groups for session 33. The four queries of session 33 are:</p><formula xml:id="formula_6" coords="4,128.88,568.84,269.82,57.07">• 𝑞 ! = 𝑔𝑒𝑛𝑒𝑟𝑎𝑙 𝑠𝑐ℎ𝑒𝑑𝑢𝑙𝑒 𝑝𝑎𝑦 𝑡𝑎𝑏𝑙𝑒𝑠 • 𝑞 ! = 𝑔𝑒𝑛𝑒𝑟𝑎𝑙 𝑠𝑐ℎ𝑒𝑑𝑢𝑙𝑒 𝑝𝑎𝑦 𝑡𝑎𝑏𝑙𝑒𝑠 2013 • 𝑞 ! = 𝑔𝑒𝑛𝑒𝑟𝑎𝑙 𝑠𝑐ℎ𝑒𝑑𝑢𝑙𝑒 𝑝𝑎𝑦 𝑡𝑎𝑏𝑙𝑒𝑠 𝑓𝑒𝑑𝑒𝑟𝑎𝑙 𝑤𝑎𝑔𝑒 𝑠𝑦𝑠𝑡𝑒𝑚 • 𝑞 ! = 𝑔𝑒𝑛𝑒𝑟𝑎𝑙 𝑠𝑐ℎ𝑒𝑑𝑢𝑙𝑒 𝑝𝑎𝑦 𝑡𝑎𝑏𝑙𝑒𝑠 "𝑓𝑒𝑑𝑒𝑟𝑎𝑙 𝑔𝑜𝑣𝑒𝑟𝑛𝑚𝑒𝑛𝑡"</formula><p>We get "Average" results in 34 sessions, for instance, session 21, where our runs perform comparable with other groups.</p><p>In the 19 "Bad" sessions, we find that one of the major reasons prevents us from getting good results is a bias of relevant documents away from CatB. One typical example is Session 83, which corresponds to topic 65. The ground truth for Session 83 has 12 relevant documents from CatA, while only 2 from CatB. This bias away from CatB makes it difficult for our runs to show good performance. This bias suggests that systems running over CatA and systems running over CatB cannot be directly compared.</p><p>Lastly, there are 12 sessions where no submitted runs receive good nDCG@10 score; we classify them as "Difficult". Examples include session 57. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good Average Bad Difficult</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results, CatB and CatA</head><p>Six groups participated in TREC 2013 Session Track: 2 groups used CatB and 4 groups used CatA. All runs are evaluated together as if they were CatA submissions. We submitted CatB runs. Table <ref type="table" coords="5,500.18,101.61,4.92,8.64" target="#tab_2">4</ref> presents the nDCG@10 scores for our submitted runs. Table <ref type="table" coords="5,116.06,455.37,4.92,8.64">5</ref> shows the statistics about both document collections. We show that these two corpora, CatB and CatA, are quite different from each other. Their major differences include but are not limited to document collection scales and coverage of the relevant documents. If only compare with our own CatB submissions among the 3 subtasks, our approaches effectively increased the retrieval result of the last query by utilizing the session prior information (RL2, by 46.1%) and the cross-session prior inforamtion (RL3, by 46.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Session Categories</head><p>All the session tasks are classified under two dimensions: a session can be either Factual (F) or Intellectual (I), and with a goal that can be either Specific (S) or Amorphous (A) <ref type="bibr" coords="5,431.77,562.89,10.59,8.64" target="#b3">[3]</ref>. Therefore, all session can be divided into four categories: F-S, F-A, I-S, and I-A. Table <ref type="table" coords="5,400.09,574.41,4.92,8.64">6</ref> shows our performance improvements from RL1 to RL2 for each of the four categories. Improvements from the TREC median runs are also listed.</p><p>Compared with the median run, we can see that our approach (QCM) is significantly more effective in search accuracy for Amorphous (A) sessions. This is because we employ previous results to guide the search engine to improve retrieval accuracy, and the search process of these Amorphous sessions have more uncertainty and rely more on previous search results. They fit well to each other. Therefore our model performs well in Amorphous sessions.</p><p>Figure <ref type="figure" coords="5,119.05,689.37,4.92,8.64" target="#fig_0">2</ref> shows the percentage of different session categories for TREC 2012 and TREC 2013. As we can see, TREC 2013 has much less Amorphous sessions (28.74%) than TREC 2012 has (43.88%). Since these Amorphous sessions are where our approach is strong about, the potential of our approach has not been fully explored in TREC 2013. On the TREC 2012 dataset, we can achieve an nDCG@10 of 0.3368, which is statistically significantly better than last year's TREC best runs. <ref type="bibr" coords="5,427.05,735.45,11.61,8.64" target="#b1">[1]</ref> Table <ref type="table" coords="6,239.76,76.65,3.73,8.64">6</ref>. Search accuracy improvement.</p><p>Task: Factual(F)/Intellectual(I), Goal: Specific(S)/Amorphous(A). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we employ the Query Change Retrieve Model on the new document collection ClueWeb12 CatB, and perform a cross-session majority vote approach based on clicked data. The evaluation results show that our method of using prior session history is effective and can bring a 46.1% improvement from RL1 to RL2, and an extra 0.34% improvement from RL2 to RL3. By keeping track of query changes, our model performs well on amorphous sessions, which show more uncertainty and rely more on the previous search results. With less amorphous sessions and more specific sessions, the strength of our model has not been fully demonstrated in TREC 2013. Our analysis on the document collections also shows the difference between the new ClueWeb12 CatB and CatA corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,159.15,420.57,294.71,8.64;3,121.22,436.36,370.67,8.75;3,210.12,451.96,192.83,8.75;3,110.94,467.85,21.55,8.64;3,191.58,467.85,18.81,8.64;3,301.77,467.85,18.81,8.64;3,426.74,467.85,18.81,8.64;3,103.99,483.93,35.49,8.64;3,167.57,483.93,66.84,8.64;3,163.75,499.72,74.47,8.75;3,267.90,483.93,86.59,8.64;3,278.31,499.53,65.69,8.64;3,270.29,515.32,81.77,8.75;3,392.87,483.93,86.59,8.64;3,384.94,499.53,102.44,8.64;3,403.28,515.13,65.69,8.64;3,395.26,530.92,81.77,8.75;3,103.99,546.81,35.49,8.64;3,167.57,546.81,66.84,8.64;3,163.75,562.60,74.47,8.75;3,267.90,546.81,86.59,8.64;3,278.31,562.41,65.69,8.64;3,276.85,578.20,68.65,8.75;3,392.87,546.81,86.59,8.64;3,384.94,562.41,102.42,8.64;3,403.28,578.01,65.69,8.64;3,401.82,593.80,68.64,8.75;3,103.99,609.69,35.49,8.64;3,167.57,609.69,66.84,8.64;3,163.75,625.48,74.47,8.75;3,267.90,609.69,86.59,8.64;3,279.14,625.29,64.08,8.64;3,276.85,641.08,68.65,8.75;3,392.87,609.69,86.59,8.64;3,403.28,625.29,65.69,8.64;3,395.26,641.08,81.77,8.75"><head>Table 2 .</head><label>2</label><figDesc>Methods and parameter settings in TREC 2013 submissions.𝝁 is the Dirichlet smoothing parameter, k is the number of pseudo relevance feedback,𝜸 is discount factor in Query Change Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,172.71,76.57,249.58,9.06"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Session difficulty levels. (Y-Axis: # of sessions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,124.87,199.21,345.27,9.06;6,252.97,213.11,109.44,106.06"><head>GUrun2Figure 2 .</head><label>2</label><figDesc>Figure 2. Session Category Differences between TREC 2013 and TREC 2012.</figDesc><graphic coords="6,252.97,213.11,109.44,106.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,89.88,183.45,398.59,129.36"><head>Table 1 . A Query Change Example (TREC 2013 Session 4).</head><label>1</label><figDesc></figDesc><table coords="2,89.88,199.53,398.59,113.28"><row><cell>Session</cell><cell>Queries</cell><cell>Query Change</cell><cell>Q theme</cell></row><row><cell>Session 4</cell><cell>q1 = heart attack details</cell><cell>+∆q ! = ∅</cell><cell>heart attack</cell></row><row><cell></cell><cell>q2 = heart attack</cell><cell>-∆q ! = details</cell><cell></cell></row><row><cell></cell><cell>q3 = heart attack statistics</cell><cell>+∆q ! = statistics</cell><cell>heart attack</cell></row><row><cell></cell><cell></cell><cell>-∆q ! = ∅</cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,89.88,244.17,415.24,165.66"><head>Table 3 . nDCG@10 for sessions with different difficulty levels.</head><label>3</label><figDesc></figDesc><table coords="4,96.87,260.25,399.15,89.04"><row><cell>Type</cell><cell></cell><cell>GUrun2</cell><cell></cell><cell></cell><cell>Median</cell><cell></cell><cell></cell><cell>Max</cell><cell></cell></row><row><cell>-Session</cell><cell>RL1</cell><cell>RL2</cell><cell>RL3</cell><cell>RL1</cell><cell>RL2</cell><cell>RL3</cell><cell>RL1</cell><cell>RL2</cell><cell>RL3</cell></row><row><cell>Good-33</cell><cell cols="9">0.2727 0.4362 0.4362 0.1005 0.0571 0.0571 0.3511 0.4362 0.4362</cell></row><row><cell cols="10">Average-21 0.4576 0.2016 0.1593 0.1486 0.2016 0.1593 0.5255 0.4959 0.4542</cell></row><row><cell>Bad-83</cell><cell>0.0249</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="3">0.4427 0.2837 0.4328</cell></row><row><cell>Difficult-57</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note coords="4,89.88,366.28,415.23,8.75;4,89.88,377.85,415.22,8.64;4,89.88,389.37,415.24,8.64;4,89.88,401.08,76.70,8.75"><p><p><p>feedback, and γ is the discount factor</p><ref type="bibr" coords="4,251.08,366.33,10.59,8.64" target="#b1">[1]</ref></p>. Four more parameters α, β, ε and δ are fixed in RL2 and RL3. Since the sessions are much longer in 2013 than in 2012, for instance, session 87 contains 21 queries, we generated runs with different discount factors. The selected submitted runs used either γ = 0.98 or γ = 1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,89.88,138.33,415.53,302.64"><head>Table 4 . nDCG@10 scores for TREC2013 runs. (RL2 improvements compared with RL1, RL3 improvements compared with RL2)</head><label>4</label><figDesc>CatB and CatA, we realize that we couldn't directly compare systems on these two collections. Although CatB is a subset from CatA, many relevant documents from CatA are not in CatB therefore a CatB system has no chance to retrieve most relevant documents. A random sample from the ground truth table (the set of all relevant documents corresponding to all sessions) shows that, only 19 out of 60 (31.7%) relevant documents from CatA are included in CatB. Based on an approximation using the hyper-geometric distribution, we get a 95% confidence interval of the actual amount of relevant documents in CatB as [683, 1491], while there are 3428 relevant documents in CatA.</figDesc><table coords="5,89.88,170.01,415.53,190.32"><row><cell></cell><cell>GUrun1</cell><cell>Improvement</cell><cell>GUrun2</cell><cell>Improvement</cell><cell>GUrun3</cell><cell>Improvement</cell></row><row><cell>RL1</cell><cell>0.1043</cell><cell>-</cell><cell>0.1013</cell><cell>-</cell><cell>0.105</cell><cell>-</cell></row><row><cell>RL2</cell><cell>0.1459</cell><cell>+ 0.0416 (39.9%)</cell><cell>0.148</cell><cell>+ 0.0467 (46.1%)</cell><cell>0.143</cell><cell>+ 0.0380 (36.2%)</cell></row><row><cell>RL3</cell><cell>0.146</cell><cell>+ 0.0001 (0.07%)</cell><cell>0.1485</cell><cell>+ 0.0005 (0.34%)</cell><cell>0.1459</cell><cell>+ 0.0029 (2.03%)</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 5. ClueWeb12 CatB vs. ClueWeb12 CatA</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ClueWeb12 CatB</cell><cell cols="2">ClueWeb12 CatA</cell></row><row><cell></cell><cell cols="2"># of Documents</cell><cell cols="2">52,343,021</cell><cell cols="2">733,019,372</cell></row><row><cell cols="3">Corpus Size Uncompressed</cell><cell cols="2">1.95TB</cell><cell></cell><cell>27.3TB</cell></row><row><cell cols="3"># of Relevant Documents</cell><cell>[683,</cell><cell></cell><cell></cell><cell>3428</cell></row><row><cell cols="3">% of Relevant Doc Included</cell><cell cols="2">[19.9%, 43.5%]</cell><cell></cell><cell>100%</cell></row><row><cell cols="3">After analyzing ClueWeb12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,97.42,761.55,92.38,8.00"><p>http://lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,98.68,775.47,170.74,8.00"><p>http://durum0.uwaterloo.ca/clueweb09spam/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="3,98.68,736.83,170.74,8.00"><p>http://durum0.uwaterloo.ca/clueweb09spam/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,94.38,493.54,64.79,10.54" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Reference</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.14,507.69,399.91,8.64;6,89.88,518.97,415.21,8.64;6,89.88,530.49,292.56,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,319.96,507.69,169.19,8.64">Utilizing query change for session search</title>
		<author>
			<persName coords=""><forename type="first">Sicong</forename><surname>Dongyi Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,89.88,518.97,415.21,8.64;6,89.88,530.49,131.54,8.64">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;13)</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,104.39,553.53,400.60,8.64;6,89.88,565.05,240.21,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,202.21,553.53,302.78,8.64;6,89.88,565.05,32.21,8.64">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,128.21,565.05,88.23,8.64">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.10,588.09,400.00,8.64;6,89.88,599.61,263.04,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,385.38,588.09,119.72,8.64;6,89.88,599.61,54.54,8.64">Overview of the TREC 2013 Session Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,162.09,599.61,186.41,8.64">the Proceedings of the TREC 2013 Conference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,469.31,365.62,2.67,7.19;6,311.18,264.65,32.68,10.79;6,311.18,275.94,31.44,10.79;6,256.84,244.13,49.20,10.79;6,256.84,255.42,31.44,10.79;6,144.79,253.94,31.44,10.79;6,195.06,277.78,32.68,10.79;6,195.06,289.07,31.44,10.79" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Specific</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>56.12%( Amorphous( 43.88%( 28.74%( Specific( 71.26%</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
