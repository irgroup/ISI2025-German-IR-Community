<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.35,72.35,465.01,16.84;1,136.61,92.27,336.51,16.84">A User-in-the-Loop Process for Investigational Search: Foreseer in TREC 2013 Microblog Track</title>
				<funder ref="#_HrKvc6w #_4rKS2DF #_75fVGWt">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_aqXx5PZ">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.72,137.97,46.96,11.06"><forename type="first">Cheng</forename><surname>Li</surname></persName>
							<email>lichengz@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information</orgName>
								<orgName type="department" key="dep2">Department of EECS</orgName>
								<orgName type="department" key="dep3">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.22,137.97,50.53,11.06"><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information</orgName>
								<orgName type="department" key="dep2">Department of EECS</orgName>
								<orgName type="department" key="dep3">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.89,137.97,67.10,11.06"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<email>qmei@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information</orgName>
								<orgName type="department" key="dep2">Department of EECS</orgName>
								<orgName type="department" key="dep3">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.35,72.35,465.01,16.84;1,136.61,92.27,336.51,16.84">A User-in-the-Loop Process for Investigational Search: Foreseer in TREC 2013 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">277536CC77E6AC06CED4FA7C81DCE0AA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditionally, ad hoc retrieval aims at satisfying an information need with a few highly relevant documents. This highprecision approach works well for simple and clear queries. When the information need becomes complex, a few topranked documents may not provide satisfactory answer. As a result, the user adapts to reformulate the query to explore more relevant information; the search engine evolves to diversify results to cover the user's real information need. In cases where the user puts emphasis on high recall of the results, the search process becomes increasingly laborious.</p><p>We propose a retrieval system that interacts with the user to obtain high precision and high recall search results, while minimizing the user effort. It iteratively explores the collection by a series of queries to optimize the recall, and refines an active learning classifier to maintain the precision. We built a prototype of the system for TREC 2013 Microblog Track. Depending on the actual query, the system converges to a stable decision on relevant/non-relevant tweets after asking for a few hundred labels, which was used to retrieve and rank 10,000 tweets (maximum allowance of the TREC API).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Traditionally, web searches are grouped into three categories: informational, navigational and transactional <ref type="bibr" coords="1,235.01,485.72,9.20,7.86" target="#b1">[1]</ref>. Users with informational search queries explore various information covering a broad topic (e.g., football). Navigational search aims at the official website or home page of a single entity (e.g., youtube). Transactional search queries are issued when users plan to perform an action on the web, like ordering a book and downloading a software.</p><p>The three categories seem to cover user intentions in most cases. However, there is no emphasis placed on these search actions where the users wants a high, or even a full coverage of relevant documents pertaining to a query, while still keeping a good precision. We call such task investigational Text Retrieval Conference TREC 2013 Microblog track search, as opposed to the three existing search classes.</p><p>There are cases when users requires more than just topranked documents and investigational search is more desirable, among which is the microblog search. Searching tweets to satisfy one's information need can be a tricky task. Consider the query "buying clothes online", where the searcher might look for online stores, coupons, discussions, suggestions, or even remote try-on technologies that are related to online apparel shopping. Since each tweet only contains very brief comment from an individual perspective, a few top-ranked tweets often deliver incomplete information and biased opinion. It is almost inevitable for the searcher to reformulate the query and search multiple times, sift through voluminous redundant and less relevant tweets, and finally put discrete pieces together into a holistic story. The process becomes increasingly laborious as the searcher proceeds to look for yet another tweet with fresh information.</p><p>There are other scenarios when information need cannot be satisfied by only a few top-ranked documents, not limited to microblog search. The challenges arise for several reasons:</p><p>Poor single-document coverage The documents are so short (or biased) that none of which independently provides useful information. Shards of texts, such as microblogs, are typical examples.</p><p>Elusive information need As the user realizes new relevant contents in top-ranked documents, he internally refines the definition of "relevance" and reformulates the query. In other words, the mature definition of information need may not exist during the composition of the initial query, but takes shape in the process of interacting with the search engine. This is evidenced by "sessions" in search logs.</p><p>Mismatched search goals Ad hoc retrieval assumes emphasis on high precision, but the user places emphasis on high recall. In other words, missing any relevant document imposes high risk. Such situations include patent search, legal search/e-discovery, scientific literature review, medical record search, etc.</p><p>Various efforts are made by IR researchers and practitioners to tackle these challenges. Query suggestion generates multiple candidate queries to help the user clarify the information need <ref type="bibr" coords="1,338.81,711.19,9.21,7.86" target="#b3">[3]</ref>. Diversified search aims to return a ranked list of documents that complement each other to provide complete coverage for a query <ref type="bibr" coords="2,142.22,68.10,9.20,7.86" target="#b4">[4]</ref>. However, due to the risk-averse nature of traditional search engines, they tend to conservatively return highly relevant results at the top, rather than probe (and agitate) the user with border cases of relevant vs. non-relevant documents. Therefore it is not ideal to use a search engine to explore the complete set of relevant information pertaining to a query.</p><p>By contrast, active learning is risk-seeking by design. A binary active learning classifier will proactively ask for clarification on uncertain/border cases, learning a decision boundary to separate the dataset into positive and negative classes <ref type="bibr" coords="2,53.80,193.63,9.20,7.86" target="#b5">[5]</ref>. Unfortunately, active learning can be ineffective when the class distribution is both extremely imbalanced and unknown a priori, which is common in IR (number of relevant vs. non-relevant documents for a query). Also, it is not typical settings for active learning when the dataset is too large to allow full access, but only allow limited access every time (e.g. via search).</p><p>In response to these challenges, we propose a retrieval system that combines ad hoc retrieval and active learning in a unified process. It interacts with the user to maximize the opportunity of obtaining as complete as possible set of documents satisfying an information need, while minimizing the user's efforts. The system alternates between inductive and deductive stages, emulating the iterative process of knowledge development. In the inductive stage, it refines information need by active learning; in the deductive stage, it converts current knowledge into a new query and searches the collection for more relevant documents.</p><p>Our user-in-the-loop system is designed for investigational purpose, where high recall is prioritized. The user is an "investigator" who would like to invest efforts to obtain coverage of relevant documents as complete as possible. Examples of such users may include legal professionals performing e-discovery tasks, patent agents searching for overlapping inventions, researchers surveying related work, physicians studying possible outcomes of a treatment, or even fans digging into every single piece of gossip about a celebrity.</p><p>The rest of the paper is organized as follows. Section 2 gives an overview of the system architecture and each component. Section 3 describes the implementation of the prototype that we built for Microblog Track, which is an instantiation of the system. Section 4 reports the evaluation and discusses about the results. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM ARCHITECTURE</head><p>In this section, we give a high-level overview of the system architecture and each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The system architecture is shown in Figure <ref type="figure" coords="2,233.59,637.96,3.58,7.86" target="#fig_0">1</ref>. At the very beginning, the investigator comes up with an initial query.</p><p>After initial retrieval, a binary classifier will proactively select a few documents and ask the oracle (e.g. human investigator) to label them as "relevant" or "nonrelevant". The active learning goes on until the classifier's prediction performance is reasonably good. The system will then prepare a new query based on the learned model, the original query, and other heuristics; if necessary, the investigator can also intervene and modify the query. The new query will retrieve another set of documents, which is merged into the retrieved document pool. At this point, a second round of active learning starts to classify the updated pool. The process continues until the binary classifier performs reasonably well without asking for labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search</head><p>This component performs ad hoc retrieval: given a query, the retrieval function evaluates documents in the collection and returns a ranked list of documents ordered by relevance. The retrieval function can take any reasonable implementation, such as Boolean retrieval model, vector space model (e.g. BM25), language model (e.g. Dirichlet prior) <ref type="bibr" coords="2,316.81,435.90,9.20,7.86" target="#b2">[2]</ref>. Many public search services, such as Google Search API <ref type="foot" coords="2,333.30,444.59,3.65,5.24" target="#foot_0">1</ref> and Yahoo! BOSS API<ref type="foot" coords="2,439.93,444.59,3.65,5.24" target="#foot_1">2</ref> , impose rate limits on the number of documents returned by one query. As in TREC 2013 Microblog Track, the ranking function is Dirichlet prior smoothing language modeling provided by TREC API, with maximum number of retrieve 10,000.</p><p>Each time the retrieved documents will be merged into the current retrieved document pool, which is the dataset for active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Active Learning</head><p>The task of this component is to learn a binary classifier to separate relevant documents from non-relevant ones, in the currently retrieved document pool. Normally, a search engine can only passively receive explicit or implicit relevance feedback from clickthrough data and estimate a relevance model. In contrast, our system uses active feedback: the active learning component proactively asks the investigator to evaluate the relevance of documents, which may include non-relevant/border cases. The promise of active learning is that by allowing the algorithm to select data to label, it will perform better with less training. Here it learns a good model of relevance with less labeling effort from the investigator.</p><p>When newly retrieved documents arrive at the unlabeled document pool, selecting documents needs special care. In the starting round, the system has no labeled documents, yet must select a subset from the unlabeled pool and ask for labels. Moreover, it is desirable to include both positive and negative examples in the subset for initial training. This is known as the cold start problem commonly seen in data modeling tasks. It is legitimate to assume that top-ranked documents are likely to be relevant, while low-ranked documents are more likely to be non-relevant. Therefore, one possible strategy is to select several top ranked documents as well as cluster centroids of the the rest documents, and ask for labeling.</p><p>In subsequent retrievals, even though the system has accumulated some labeled examples and trained an inaccurate classifier, newly retrieved documents are still unseen to the classifier. If many new and similar documents fall into the uncertain region of the classifier, active learning tends to ask labels for all these documents. To accelerate the learning rate and reduce the labeling efforts, a sensible strategy is to select random samples or cluster centroids of these uncertain documents for labeling.</p><p>At the end of each inductive stage, we have a binary classifier that automatically separates the unlabeled documents into relevant and non-relevant classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query Reformulation</head><p>When the binary classifier is perceived to be performing reasonably well, the investigator may want to proceed and retrieve another set of documents for inspection. Undoubtedly, he hopes that the newly retrieved documents are both relevant and different. To achieve this goal, a new query has to be carefully formulated.</p><p>The set of feedback documents and classifier-judged documents from the previous inductive stage can be exploited to update the query. Feedback-based query expansion is known to be effective in improving recall while maintaining precision. This component may also incorporate other query expansion strategies, such as knowledge-based query expansion. Furthermore, the investigator himself may intervene and edit the query directly.</p><p>Clearly there is exploration-exploitation trade-off in query reformulation component. Too much exploration will retrieve many non-relevant documents, increasing the burden of labeling; too much exploitation will limit the system from getting broad coverage. A good query reformulation strategy should balance these two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Stopping Criteria</head><p>The system stops whenever any of the following conditions occurs:</p><p>• The retrieval function cannot pick up anymore unseen relevant document;</p><p>• The binary classifier has become robust enough to label any unseen document;</p><p>• Labeling budget is used up.</p><p>After the system stops, the relevant document set can be identified by applying the binary classifier on all the retrieved data. It should be noted that even in the first two cases, it is not guaranteed that the system has identified the "true complete set" of relevant documents. The coverage depends on the initial query, the design of the retrieval function, selecting strategies of active learning, and query reformulation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPLEMENTATION</head><p>In this section, we describe the prototype implementation of the system for TREC 2013 Microblog Track.</p><p>The pseudo code for the prototype is displayed in Algorithm 1. As we can see, this is a very general process. Many suitable classifier can be used in the active learning process, or the inner loop. A variety of query expansion techniques could be integrated into the flow as well. This system is able to be easily adapted to any other data collection tasks as long as the API is provided. We will elaborate each step of the retrieval process in next few subsections.</p><p>Algorithm 1 User-in-the-loop process</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Initial query q0</head><p>Output: A set of relevant tweets 1: loop 2:</p><p>RetriT ← tweets collected through TREC API using query qi 3:</p><p>U nivT ← U nivT + RetriT 4:</p><p>if i = 0 then 5: Do clustering on RetriT 6:</p><p>CentT ← tweets closest to clustering centroids 7:</p><p>U ncertainT ← CentT + top 10 tweets of RetriT 8:</p><p>end if 9: repeat 10:</p><p>Request human labels for U ncertainT 11:</p><p>LabeledT ← LabeledT + U ncertainT 12:</p><p>U nlabeledT ← U nivT -LabeledT 13:</p><p>Classifier predicts labels for U nlabeledT 14:</p><p>U ncertainT is updated by classifier 15:</p><p>until Classifier performs well 16:</p><p>qi+1 ← q0+ query expansion based on current model 17: end loop</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Preprocessing</head><p>Text are preprocessed for later usage. Specifically, tweets starting with "RT" and non-English tweets are removed. Near duplicates are also removed to reduce the burden on humans and algorithms. These duplicates are added back at last if the tweet they resemble are predicted as relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clustering</head><p>Clustering is only performed after the very first outer loop. This is used to solve the cold start problem, where the classifier has no training data to build model and thus do prediction. Sampling tweets randomly for humans to label could be another easier option. To increase the diversity of tweets being selected, we do K-means clustering, and pick the cluster centroid tweets. Top tweets retrieved from TREC API are also included, with the assumption that they are mostly positive tweets. Including them could reduce the possibility that initial training data set is dominated by negative tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification</head><p>We choose SVM with linear kernel as our classifier. Krovetz stemming is performed on the text and stopwords are kept because we found that many features with stopwords included are capable of distinguishing relevant documents from irrelevant ones. The features we used are unigrams, bigrams, and trigrams. Only most frequent trigrams are kept to avoid having a high dimensionality of feature space. After training, SVM will return a model, specifying the weights of each feature. We use the linear combination of weights to compute the confidence score of the unlabeled tweets. Among these tweets, 10 positive and negative tweets with least absolute confidence values are chosen, waiting for human to label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Human Labeling</head><p>We build a web interface to facilitate human annotation. When displaying the tweets, important positive and negative features extracted by SVM are highlighted using different colors. Highlighting reduces user's process time of one tweet. However, admittedly, this could be misleading and chances are increased that unhighlighted but important words might slip away.</p><p>In addition to uncertain tweets, a random sample of confident tweets are also presented in the webpage. This helps users to monitor the progress of the classifier. When the classifier is doing well on both uncertain tweets and confident tweets, it's usually a good sign to stop the current inner loop and proceed to the next outer loop, namely, using the expanded query to retrieve more tweets through TREC API.</p><p>Expanded query, updated per each inner loop, is also shown on the interface. Users are allowed to modify the query, which is useful for domain experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Query Expansion</head><p>We use important positive features as candidates for query expansion. Since TREC API supports weighting of terms, each feature's weight provided by SVM model is directly used as the weight for query. In the future work, we should use external knowledge, such as knowledge base as a source of query expansion.</p><p>Note that the initial query is always kept in later queries. This approach alleviates the problem of semantic drift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Reranking Tweets in Submitted Runs</head><p>Since we have cast the ranking problem as a task of classification, we need to rerank the tweets for submissions. There are two quick ways to obtain a ranked list of tweets.</p><p>1. Rank tweets by the confidence score using the weights of features given by SVM 2. Rank tweets by Dirichlet model, which is already implemented by the TREC API.</p><p>With the two ranking methods at hand, the next thing is to combine them to form ranking lists. To this end, we divide the entire set of retrieved tweets into four parts.</p><p>• Note that in the four submissions, the rank of tweets are calculated separately for part (a), (c), and (d). This is to ensure that tweets from part (a) are always ranked higher than tweets from part (c) , which are in turn ranked higher than part (d).</p><p>The ways we form our four ranking lists are listed as follows:</p><p>1. FSsvm: Rank tweets in (a), (c), and (d) by SVM respectively, and concatenate them. 4. RvsDir: Tweets that are ranked high by SVM, but low by Dirichlet model. This is used to guarantee that hard tweets found by the system could be judged by TREC raters. For part (a) and (b), rank is decided by the formula rank4 = rank1 -rank2. For part (c), the formula is rank4 = rank1 + rank2. Tweets with smaller rank scores are ranked higher, even though the score could be negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND DISCUSSIONS</head><p>Performance evaluation is shown in Table <ref type="table" coords="4,492.20,606.58,3.58,7.86" target="#tab_0">1</ref>. To illustrate the set relations of NIST qrels, our labeled set, and the 4 submitted runs, Venn diagrams are drawn in Figure <ref type="figure" coords="4,529.41,627.50,4.61,7.86" target="#fig_3">2</ref><ref type="figure" coords="4,537.07,627.50,3.07,7.86" target="#fig_5">3</ref><ref type="figure" coords="4,537.07,627.50,3.07,7.86" target="#fig_6">4</ref><ref type="figure" coords="4,537.07,627.50,3.07,7.86">5</ref><ref type="figure" coords="4,543.21,627.50,3.58,7.86">6</ref>.</p><p>In total, 32,237 tweets were labeled for 60 topics, half the number of tweets (qrels) evaluated by TREC assessors. It means that on average 537 tweets were labeled to train a binary classifier for each topic, a non-trivial amount of human efforts. We argue that user-in-the-loop retrieval system targets at users who are willing to invest substantial time and efforts in the search task.</p><p>In Figure <ref type="figure" coords="5,94.99,57.64,3.58,7.86" target="#fig_3">2</ref>, Set G and H are disputes between NIST qrels and our labels. A second inspection reveals that most NISTjudged relevant tweets are indeed relevant, while many NISTjudged non-relevant tweets might also be justified as relevant. It means that our approach missed some relevant tweets in Set A. Future work will improve the components for a higher recall. It's also worth noting that a good proportion of tweets labeled as positive by our system is not included in the NIST evaluation pool at all. This implies the inadequacy of the "cut-off-at-K" pooling strategy used in the TREC evaluation when high-recall is emphasized.</p><p>SVM classifiers tend to draw the decision boundary conservatively, especially when positive training examples are few. Therefore, it maintains good precision at the retrieved set level (see Figure <ref type="figure" coords="5,121.74,214.55,3.58,7.86">5</ref>. However, pure SVM scores f (x) = w T x do not rank high-quality tweets to the top, since tweets with extremely high score tend to be overfitting cases. Such tweets merely repeat words with positive weights. Dirichlet prior smooths the document language model, promoting documents with diverse on-topic words, penalizing those overfitting tweets. However, the smoothed language model may also promote non-relevant (false positive) tweets, which will hurt precision. By averaging the ranks of SVM and Dirichlet model, we obtain improved precision at the top (     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this study, we proposed a retrieval system for investigational purposes, where high recall is prioritized. It is justified by many use cases, including e-discovery, literature review, medical record search, etc. The system brings ad hoc retrieval and active learning in a unified process, aiming to reduce the user's effort in pulling out as many relevant documents as possible.We built a prototype to demonstrate the system in TREC 2013 Microblog Track. Various parts of the system can be improved in future work, so that with less labeling effort, the system still maintains or even improves on both precision and recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,375.00,331.38,122.73,7.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,339.23,142.77,133.10,7.86;4,330.14,159.79,144.96,7.86;4,330.14,176.81,225.78,7.86;4,339.23,187.27,20.46,7.86;4,330.14,204.30,225.78,7.86;4,339.23,214.76,20.46,7.86;4,316.81,241.63,239.11,7.86"><head></head><label></label><figDesc>Part (a): Labeled positive tweets • Part (b): Labeled negative tweets • Part (c): Unlabeled tweets predicted as positive by SVM • Part (d): Unlabeled tweets predicted as negative by SVM Tweets from part (b) are discarded for all the submissions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,327.59,370.87,228.33,7.89;4,339.23,381.36,216.70,7.86;4,339.23,391.82,216.69,7.86;4,339.23,402.28,216.70,7.86;4,339.23,412.74,216.69,7.86;4,339.23,423.20,216.69,7.86;4,339.23,433.67,216.69,7.86;4,339.23,444.13,216.69,7.86;4,339.23,454.59,52.26,7.86;4,327.59,471.58,228.33,7.89;4,339.23,482.07,214.20,7.86"><head>2 . 3 .</head><label>23</label><figDesc>Direrank: Labeled positive tweets ranked by Dirichlet model, concatenated by unlabeled tweets ranked by Dirichlet model. In particular, we submit to the TREC API the final query generated by SVM and get a ranked list of tweets returned by API. If the tweets retrieved in the process are in the returned list, rank them according to the returned list. If they are not, rank them according to<ref type="bibr" coords="4,438.38,444.13,10.74,7.86" target="#b1">(1)</ref>, and append them to the returned list. Avgrank: Average rank of SVM and Dirichlet model. Tweet ranking is decided by rank3 = rank1 + rank2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,53.80,694.91,239.10,7.86;5,53.80,705.37,162.76,7.86;5,133.46,544.85,113.47,129.55"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Venn diagram of NIST qrels and our labeled set. Numbers are sizes of corresponding sets.</figDesc><graphic coords="5,133.46,544.85,113.47,129.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,316.81,244.29,239.11,7.86;5,316.81,254.75,196.68,7.86;5,396.48,94.23,113.47,129.55"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Venn diagram of NIST qrels, our labeled set, and FSsvm. Numbers are sizes of corresponding sets.</figDesc><graphic coords="5,396.48,94.23,113.47,129.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,316.81,465.83,239.11,7.86;5,358.92,476.29,162.76,7.86;5,396.48,315.77,113.47,129.55"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Venn diagram of NIST qrels, our labeled set, and Numbers are sizes of corresponding sets.</figDesc><graphic coords="5,396.48,315.77,113.47,129.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,58.10,319.16,218.66,115.75"><head>Table 1 ,</head><label>1</label><figDesc>Avgrank).</figDesc><table coords="5,69.95,342.33,206.81,92.57"><row><cell></cell><cell>R-prec MAP P@30</cell></row><row><cell>FSsvm</cell><cell>0.4882 0.4413 0.6744</cell></row><row><cell>Direrank</cell><cell>0.5172 0.4735 0.6967</cell></row><row><cell>Avgrank</cell><cell>0.5033 0.4570 0.7061</cell></row><row><cell>RvsDir</cell><cell>0.4931 0.4010 0.5822</cell></row><row><cell>Auto.: average best</cell><cell>0.5214 0.4820 0.7222</cell></row><row><cell>Auto.: average median</cell><cell>0.2721 0.2126 0.4217</cell></row><row><cell>All: average best</cell><cell>0.6086 0.5737 0.7989</cell></row><row><cell>All: average median</cell><cell>0.2834 0.2212 0.4311</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.80,447.96,239.11,39.25"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note coords="5,89.31,447.96,203.60,7.86;5,53.80,458.42,239.11,7.86;5,53.80,468.88,239.11,7.86;5,53.80,479.34,131.49,7.86"><p>Performance evaluation. Auto.: 65 automatic runs; All: all runs (65 automatic + 6 manual); average best: average of best performance per topic; average median: average of median performance per topic</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,321.42,692.53,207.23,7.47;2,316.81,701.50,94.47,7.47"><p>https://developers.google.com/custom-search/ json-api/v1/overview</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,321.42,711.83,183.25,7.47"><p>http://developer.yahoo.com/boss/search/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work is partially supported by the <rs type="funder">National Science Foundation</rs> under grant numbers <rs type="grantNumber">IIS-0968489</rs>, <rs type="grantNumber">IIS-1054199</rs>, and <rs type="grantNumber">CCF-1048168</rs>, and partially supported by the <rs type="funder">DARPA</rs> under award number <rs type="grantNumber">W911NF-12-1-0037</rs>. We thank <rs type="person">Paul Resnick</rs> and <rs type="person">Samuel Carton</rs> for their helpful suggestions in this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HrKvc6w">
					<idno type="grant-number">IIS-0968489</idno>
				</org>
				<org type="funding" xml:id="_4rKS2DF">
					<idno type="grant-number">IIS-1054199</idno>
				</org>
				<org type="funding" xml:id="_75fVGWt">
					<idno type="grant-number">CCF-1048168</idno>
				</org>
				<org type="funding" xml:id="_aqXx5PZ">
					<idno type="grant-number">W911NF-12-1-0037</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,58.28,504.83,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,517.42,220.57,7.86;6,67.99,527.88,173.45,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,113.92,517.42,105.35,7.86">A taxonomy of web search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,237.75,517.42,50.81,7.86;6,67.99,527.88,22.00,7.86">ACM SIGIR forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,539.33,186.23,7.86;6,67.99,549.79,191.38,7.86;6,67.99,560.26,185.78,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,67.99,549.79,145.90,7.86">Introduction to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,571.71,207.13,7.86;6,67.99,582.17,206.35,7.86;6,67.99,592.63,223.52,7.86;6,67.99,603.09,111.24,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,205.69,571.71,69.43,7.86;6,67.99,582.17,70.60,7.86">Query suggestion using hitting time</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,157.24,582.17,117.10,7.86;6,67.99,592.63,218.70,7.86">Proceedings of the 17th ACM conference on Information and knowledge management</title>
		<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,614.55,214.99,7.86;6,67.99,625.01,207.55,7.86;6,67.99,635.47,201.21,7.86;6,67.99,645.93,223.59,7.86;6,67.99,656.39,201.79,7.86;6,67.99,666.86,111.24,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,67.99,625.01,207.55,7.86;6,67.99,635.47,90.34,7.86">Toward whole-session relevance: Exploring intrinsic diversity in web search</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,176.81,635.47,92.39,7.86;6,67.99,645.93,223.59,7.86;6,67.99,656.39,198.21,7.86">Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;13</title>
		<meeting>the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,67.99,678.31,221.08,7.86;6,67.99,688.77,118.73,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,112.40,678.31,128.40,7.86">Active learning literature survey</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Madison</publisher>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
