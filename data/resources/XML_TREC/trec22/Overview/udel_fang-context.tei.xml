<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.93,62.28,386.13,12.93">An Opinion-aware Approach to Contextual Suggestion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,249.41,99.48,50.22,9.96"><forename type="first">Peilin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,322.33,99.48,40.27,9.96"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<email>hfang@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.93,62.28,386.13,12.93">An Opinion-aware Approach to Contextual Suggestion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FBBF038760EA43B753B44730D1500A45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our efforts for TREC Contextual Suggestion task. Our goal of this year is to evaluate the effectiveness of (1) an opinion-based method to model user profiles and rank candidate suggestions; and (2) a template-based summarization method that leverages the information from multiple resources to generate the description of a candidate suggestion. Existing approaches often uses the description or category information of the suggestions to estimate the user profile. However, one limitation of such approaches is that it may not be generalized well to other suggestions. To overcome this limitation, we propose to estimate the user profile based on the reviews of the candidate suggestions. Our preliminary results show that the proposed new method can improve the performance. Moreover, we explore a new way of generating summaries for the candidate suggestions. The main idea is that we want to leverage the information from multiple sources to generate a more comprehensive summary for a candidate suggestion. In particular, the summary includes the category information of the suggestion, meta-description of the website, reviews of the suggestion and the similar example suggestions that the user liked.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this year's Contextual Suggestion (CS) Track, we main aims are two folds: <ref type="bibr" coords="1,398.73,360.22,12.71,9.96" target="#b0">(1)</ref> to effectively model user profile <ref type="bibr" coords="1,62.36,372.17,10.50,9.96" target="#b4">[5]</ref> and <ref type="bibr" coords="1,95.57,372.17,12.71,9.96" target="#b1">(2)</ref> to explorer a new description generation method which combines multiple aspects of information.</p><p>For user profile modeling, we know from the example suggestions and the task user profile that which suggestions a user likes and dislikes, a straightforward solution is to estimate a positive user profile to model what the user likes and a negative one to model what the user dislikes <ref type="bibr" coords="1,334.33,408.04,9.95,9.96" target="#b3">[4]</ref>. Once we estimate such profiles for a user, we can then rank suggestions based on both their similarity to the positive user profile and their dissimilarity to the negative user profile. We propose a new method that models user profile based on reviews about the suggestions. It is obvious that the reviews given by the task users are not available for both example suggestions and candidate suggestions. Our method is designed to take advantage of the wisdom of the crowd. In particular, we leverage the on-line opinions posted by other real world users and use them to approximate the task user's opinion about the suggestion. If a user likes a suggestion, the positive profile of this user would be estimated based on the opinions about this suggestion from similar users, i.e., those who also like the place. The negative profile could be estimated in a similar way. We then explore two ways of estimating the profile based on the opinions. These two methods are mainly different in what kind of information from the opinions is used for profile modeling.</p><p>The description generation utilizes several sources of information to generate sentences template which cover multiple aspects of the suggestions. Category is firstly used to introduce the suggestion. The identified contents from suggestion's website are then used to show an "official" introduction. The highlighted reviews from real world users of the suggestion are picked by our review extraction algorithm and included in the description. Moreover, we add a last sentence to the description showing the example suggestions that the user liked and with the same category of the candidate suggestion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Framework</head><p>Figure <ref type="figure" coords="1,94.46,662.39,4.98,9.96" target="#fig_0">1</ref> shows our system framework. It mainly consists of three parts: (1) Useful information gathering, (2) User profile modeling, (3) Candidate suggestion ranking, (4) Description generation.</p><p>Useful information gathering component mainly crawls everything that we need to rank the candidate suggestion.</p><p>User profile modeling component basically models the user profile using reviews of example suggestions. The user profile is split into positive and negative. It also models the candidate suggestion profile in the similar way.  Candidate suggestions ranking is based on both user profile model and candidate suggestion profile model. We will introduce this component in more details later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crawl</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidates Ranking Descriptions</head><p>The description generation component mainly utilizes the positive candidate suggestion profile, categories and website of candidate suggestion, categories of some example suggestion to summarize a template-based description. We also describe this part in details later this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Useful Information Gathering</head><p>The first step of solving the contextual suggestion problem is to gather useful information. Useful information contains not only the candidate suggestions, but also their web site, reviews associated with them and etc. For candidate suggestions we use open web as they contain comprehensive information than ClueWeb12 does. We crawl the information such as candidate suggestions, main page, categories and reviews of candidates exclusively from Yelp for all contexts. The crawling is done per category. We roughly choose 16 high-level categories that cover most appropriate categories used in the CS task. Approximately 105,871 candidate suggestions are crawled for all contexts, resulting in average 2117 candidate suggestions per context. The median number of candidate suggestions for contexts is 1276. The maximum and minimum numbers are 302 and 8410 respectively. Moreover, we also crawl the web site for each candidate suggestion with maximum pages of 100. The counterpart of example suggestions in Philadelphia is also crawled. The crawled data is stored mainly in json format and is used in both candidate suggestion ranking and description generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">User Profile Modeling</head><p>Method We first describe a high level framework of our method: Given a user U and a candidate suggestion CS, our method operates as follows:</p><p>1. Build positive and negative user profile, i.e., U pos and U neg , based on the information about example suggestions and the ratings that user gives, i.e, ES(U ); 2. Build a positive and negative profile for the candidate suggestion, i.e., CS pos and CS neg ; A more detailed process of above method is described as below: We first use positive reviews of the example suggestions that the user likes to build positive user profile, and use negative reviews of example suggestions that the user dislikes to build negative user profile. The basic assumption here is that the opinion of a user about a place can be inferred by the opinions of the users who give the same rating (more accurately the similar rating since we merge some ratings into category of positive or negative) as the target user to the same place. For this year's TREC, the ratings of example suggestions ranges from 0 to 4 for both website and description. We define the example suggestion that user likes as the ratings of 3 or 4 on website. Ratings of 0 or 1 are viewed as those user dislikes. Ratings of 2, i.e. the very middle one, is simply ignored as we treat it as neutral rating. For candidate suggestions, there is a rating associated with each review. The rating ranges from 1 to 5 as scalar ratings. The reviews with rating of 4 or 5 are viewed as positive reviews and rating of 1 or 2 are viewed as negative reviews. Reviews with rating 3 are ignored for the same reason as mentioned before.</p><p>Formally, the user profiles are estimated as:</p><formula xml:id="formula_0" coords="3,200.70,87.87,210.59,51.13">U pos = esi∈ES(U ) RU (esi)=P OS REP (O pos (es i )) U neg = esi∈ES(U ) RU (esi)=N EG REP (O neg (es i ))</formula><p>where O pos (es i ) represents all positive reviews about es i , O neg (es i ) represents all negative reviews, and REP (O(es i )) denotes how to represent opinion O(es i ) in the profile. We tried two different REP (O(es i )) in our experiments.</p><p>-Use unique terms from full reviews (UFR): Use all unique terms occurred in the positive reviews about a suggestion s to build its positive profile. Similarly, use all unique terms occurred in the negative reviews of the suggestion to build its negative profile. -Use review summaries (RS): Review summaries are the terms extracted from the reviews using Opinosis Algorithm <ref type="bibr" coords="3,119.33,231.83,13.34,9.96" target="#b1">[2]</ref>.</p><p>We then follow the similar strategy and build the positive and negative profile for a candidate suggestion CS as follows:</p><formula xml:id="formula_1" coords="3,247.83,286.93,116.33,25.34">CS pos = REP (O pos (CS)) CS neg = REP (O neg (CS)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Candidate Suggestion Ranking</head><p>We estimate the similarity between a user and a candidate suggestion as follows:</p><formula xml:id="formula_2" coords="3,176.00,382.08,373.62,25.33">S(U, CS) = α × SIM (U pos , CS pos ) -β × SIM (U pos , CS neg ) -γ × SIM (U neg , CS pos ) + η × SIM (U neg , CS neg )<label>(1)</label></formula><p>where SIM (a, b) is the text similarity measurement between a and b. In our experiments, we use F2-EXP of axiomatic retrieval model <ref type="bibr" coords="3,178.35,430.65,10.50,9.96" target="#b0">[1]</ref> as SIM (a, b). α, β, γ and η are parameters that regulate the impact of the four components to the final similarity score. Their values are chosen in interval [0,1] with the pace of 0.1. Here the intuition is that the similarity score should be positively correlated with U pos , CS pos and U neg , CS neg and should be negatively correlated with U pos , CS neg and U neg , CS pos . The basic assumption is that people like or dislike a suggestion for similar reasons and such reasons are included in the reviews.</p><p>Apparently, higher similarity score between a user and a candidate suggestion means higher ranking of that candidate suggestion. The similarity score are computed for each user/context pair and then we can get all the desired results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Description Generation</head><p>We use a template to render the descriptions of candidate suggestions. The template basically consists of the following components:</p><p>1. An opening sentence. 2. The "official" introduction. 3. The highlighted reviews. 4. The concluding sentence.</p><p>The opening sentence is of the form: suggestion's name followed by the fine category of that suggestion. For example, "The big fish grocery is a shopping store." If the category of candidate suggestion is not available, we just show that "The xyz is a good place to go." We believe that the straightforward opening sentence makes the description more attractive and more readable.</p><p>Both "official" introduction and highlighted reviews depend on the extraction of aspects (features) of that candidate suggestion. This is done by first doing the part-of-speech tagging using Stanford Log-linear Part-Of-Speech Tagger <ref type="bibr" coords="3,128.68,722.17,10.50,9.96" target="#b2">[3]</ref> to the positive reviews of the candidate suggestion and then pick a set of nouns that appears most frequently. Apparently, terms in positive reviews have higher chance to be positive. We pick nouns since they are more prone to be real aspects.</p><p>After we extracted the most frequent nouns, we pick up the sentences from the web site of the candidate suggestion. We first manually generate a list of 391 positive adjectives such as "fabulous" and "wonderful". Sentences that contain larger number of positive adjectives are chosen as candidates. Sentences from web site's meta description field are also included as candidates. Finally we pick up at most 5 sentences as candidates due to the number of characters limits. We pick up highlighted reviews in the similar way as we pick up sentences from candidate suggestion's web site.</p><p>The concluding sentence is of the form: "We recommend this suggestion to you because you liked abc and xyz in example suggestions." abc and xyz are example suggestions that have the same fine category with the candidate suggestion.</p><p>The number of sentences of each component mentioned above are balanced in order to meet the requirement (less than 512 characters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs and Experiment Results</head><p>We submitted two runs: UDInfoCS1 and UDInfoCS2. UDInfoCS1 models user profiles using RS. For description, it combines the opening sentence, meta-description and the picked sentences from the web site, highlighted reviews, and the concluding sentence. UDInfoCS2 models user profiles using UFR. For description, it combines the opening sentence, meta-description (no picked sentences here) of the web site, highlighted reviews, and the concluding sentence. For parameters in the similarity function, we tune them based on the cross-validation on example suggestions. The best parameter sets for both runs are α = 0.7, β = 0.0, γ = 0.2, η = 0.1 (they are the same). As for judgments, 223 profile/context pairs were sampled and judged. There are four criteria: geographic appropriateness, interestingness of website/description, and time data (how long it took to arrive at the judgment in seconds). Geographic appropriateness has 3 scales: 0,1,2, with 0 not geographically appropriate, 1 marginally geographically appropriate, and 2 geographically appropriate. Interestingness of website/description has 5 scales: 0,1,2,3,4, with 0 strongly uninterested, 1 uninterested, 2 neutral, 3 interested, 4 strongly interested. Moreover, geographic appropriateness were judged partially by NIST accessors and partially by users. Scores from NIST accessors were used if a profile/context pair were judged by both of them.</p><p>Three evaluation measurements were reported: P@5 (precision at 5), MRR (mean reciprocal rank) and TBG (time-biased gain). For P@5 and MRR, a suggestion is labeled as relevant only if it has geographic score of 1 or 2, and description score of 3 or 4, and web site score of 3 or 4.</p><p>Figure <ref type="figure" coords="5,108.36,263.92,4.98,9.96">2</ref> and Figure <ref type="figure" coords="5,166.33,263.92,4.98,9.96">3</ref> show the performances of our two runs in terms of all evaluation measurements. The X axis consists of all profile/context pairs, ordered alphabetically in the format of "profile-context". One red point represents our result for that profile/context. Three bars corresponding to the best, median and worst results of that profile/context. We can see that our runs achieve some of the best results. Most of our results are equal or better than the median results, indicating the effectiveness of our proposed system. Table <ref type="table" coords="5,465.04,311.74,4.98,9.96" target="#tab_1">1</ref> shows the overall mean performances of our runs in terms of all evaluation measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In TREC 2013 Contextual Suggestion Track, we submitted two runs. For ranking the candidate suggestion of each profile/context, we apply the method that measures the similarity between user profile and candidate suggestions' profile. User profile is based on the positive and negative reviews of the example suggestion. Individual candidate suggestion profile is modeled in the similar way. The similarity is computed so that it is positively correlated with positive/positive negative/negative similarity between user profile and candidate suggestion profile and it is also negatively correlated with positive/negative negative/positive similarity between user profile and candidate suggestion profile. For description generation, we apply a template based summarization method. The template mainly contains an opening sentence, "official" introductions, highlighted reviews and a concluding sentence.</p><p>For our two runs, UDInfoCS1 uses review summary terms as the representation of the profiles while UD-InfoCS2 uses unique terms as the representation of the profiles. For description generation, the only difference of UDInfoCS1 and UDInfoCS2 is that UDInfoCS1 contains the meta description and picked sentences from the website as "official" introduction while UDInfoCS2 only contains the meta description.</p><p>The performance of our two submitted runs are in general better than the median performance. Some of the results are even best results, indicating the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,252.14,229.05,107.74,8.96"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,227.56,238.98,156.89,8.96"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Detailed Results of UDInfoCS1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,230.09,67.53,151.83,62.06"><head>Table 1 .</head><label>1</label><figDesc>Overall Mean Performances</figDesc><table coords="5,244.48,86.56,119.99,43.03"><row><cell></cell><cell cols="2">UDInfoCS1 UDInfoCS2</cell></row><row><cell>P@5</cell><cell>0.5094</cell><cell>0.4969</cell></row><row><cell>MRR</cell><cell>0.632</cell><cell>0.63</cell></row><row><cell>TBG</cell><cell>2.4474</cell><cell>2.431</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,65.94,576.26,483.70,8.96;5,74.51,587.21,475.12,8.96;5,74.51,598.17,175.88,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,170.07,576.26,262.53,8.96">An exploration of axiomatic approaches to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,454.98,576.56,94.66,8.65;5,74.51,587.21,446.40,8.96">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,65.94,608.30,483.70,8.96;5,74.51,619.25,475.12,8.96;5,74.51,630.21,332.05,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,212.50,608.30,337.14,8.96;5,74.51,619.25,31.90,8.96">Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,131.52,619.25,388.01,8.96">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,65.94,640.34,483.68,8.96;5,74.51,651.30,475.15,8.96;5,74.51,662.25,475.13,8.96;5,74.51,673.22,172.62,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,304.06,640.34,245.56,8.96;5,74.51,651.30,30.26,8.96">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,124.90,651.61,424.76,8.65;5,74.51,662.56,179.45,8.65;5,309.95,662.25,47.39,8.96">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>NAACL &apos;03</note>
</biblStruct>

<biblStruct coords="5,65.94,683.34,483.69,8.96;5,74.51,694.29,242.75,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,165.59,683.34,267.77,8.96">An exploration of ranking-based strategy for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,452.84,683.65,96.79,8.65;5,74.51,694.60,85.87,8.65">Proceedings of 21st Text REtrieval Conference</title>
		<meeting>21st Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2012-11">2012. November 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,65.94,704.42,483.70,8.96;5,74.51,715.38,473.71,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,172.52,704.42,257.02,8.96">Opinion-based user profile modeling for contextual suggestions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,452.92,704.72,96.72,8.65;5,74.51,715.38,251.70,8.96">Proceedings of the 2013 Conference on the Theory of Information Retrieval, ICTIR &apos;13</title>
		<meeting>the 2013 Conference on the Theory of Information Retrieval, ICTIR &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="80" to="98" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
