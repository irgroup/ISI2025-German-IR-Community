<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.13,164.85,262.99,15.12">TREC 2013 Temporal Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-02-11">February 11, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.53,197.33,65.26,10.48"><forename type="first">Javed</forename><surname>Aslam</surname></persName>
						</author>
						<author>
							<persName coords="1,234.25,197.33,74.79,10.48"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
						</author>
						<author>
							<persName coords="1,333.52,197.33,133.21,10.48"><forename type="first">Matthew</forename><surname>Ekstrand-Abueg</surname></persName>
						</author>
						<author>
							<persName coords="1,221.98,211.28,58.69,10.48"><forename type="first">Virgi</forename><surname>Pavlu</surname></persName>
						</author>
						<author>
							<persName coords="1,318.31,211.28,70.95,10.48"><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
						</author>
						<title level="a" type="main" coord="1,174.13,164.85,262.99,15.12">TREC 2013 Temporal Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-02-11">February 11, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">F5AF8BD89AD83FFAB13B18F49AB9BFD3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unexpected news events such as earthquakes or natural disasters represent a unique information access problem where traditional approaches fail. For example, immediately after an event, the corpus may be sparsely populated with relevant content. Even when, after a few hours, relevant content is available, it is often inaccurate or highly redundant. At the same time, crisis events demonstrate a scenario where users urgently need information, especially if they are directly affected by the event.</p><p>The goal of this track is to develop systems for efficiently monitoring the information associated with an event over time. Specifically, we are interested in developing systems which (1) can broadcast short, relevant, and reliable sentence-length updates about a developing event and (2) can track the value of important event-related attributes (e.g. number of fatalities).</p><p>The track has the following goals,</p><p>• to develop algorithms which detect sub-events with low latency,</p><p>• to model information reliability in the presence of a dynamic corpus,</p><p>• to understand and address the sensitivity of text summarization algorithms in an online, sequential setting, and</p><p>• to understand and address the sensitivity of information extraction algorithms in dynamic settings.</p><p>2 Task Descriptions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequential Update Summarization</head><p>During the simulation, a system should emit relevant and novel sentences to an event (exact metrics will be released in a separate document). Conceptually, a simulator should be structured as in Figure <ref type="figure" coords="1,326.35,650.48,3.87,8.74" target="#fig_0">1</ref>. The arguments to the simulator are the participant summarization system, the time-ordered corpus, the keyword  query, and the relevant time range. In line 1, we initialize the output summary to empty. In line 2, we initialize the sequential update summarization system with the event query. The system should store some representation of this query for later processing and filtering. We iterate over the corpus in temporal order (line 3), processing each document in sequence (line 5). If the document we are processing is in the event timeframe (line 7), then we check to see if adding the document resulted in the system deciding to output a set of summary sentence ids (line 9). We then add these sentence ids to the summary timestamped with the time of the decision (lines 10-12).</p><p>We have tried to present an abstract representation of sequential update summarization. There are several comments worth making. First, if a participant is interested in efficiency and does not anticipate needing documents outside of the event timeframe, then the call to S.Process(d) can be moved inside of the condition in line 7. Participants should be clear about any filtering of C. For example, a participant should note if they are just iterating over documents with a high BM25 score. However, if this is done, care must be taken to make sure that filtering out a document d does not exploit information from sources after d.Time() (e.g. retrospective IDF values).</p><p>ValueTracking(S, C, q, a, t s , t e ) S £ Participant system. C £ Time-ordered corpus. q £ Event keyword query. a £ Event attribute. </p><formula xml:id="formula_0" coords="3,133.77,199.46,226.37,200.69">t s £ Event start time. t e £ Event end time. 1 V ← {} 2 S.Initialize(q, a) 3 v ← S.InitialEstimate() 4 V.Append( v, ∅ , t s ) 5 for d ∈ C 6 do 7 S.Process(d) 8 t ← d.Time() 9 if t ∈ [t s , t e ] 10 then 11 v, s ← S.EstimateValue() 12 if v, s = ∅ 13 then 14 V.Append( v, s , t) 15 return V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Value Tracking</head><p>During the simulation, a system should emit accurate attribute value estimates for an event. Conceptually, a simulator should be structured as in Figure <ref type="figure" coords="3,469.73,487.15,3.87,8.74" target="#fig_1">2</ref>. The arguments to the simulator are the participant tracking system, the timeordered corpus, the keyword query, the attribute of interest, and the relevant time range. In line 1, we initialize the value summary to empty. In line 2, we initialize the tracking system with the event query and attribute name. The system should store some representation of this information for later processing and filtering. We then ask for an initial estimate based solely on the query and attribute. Systems are welcome to use any data that existed at or before t s . We iterate over the corpus in temporal order (line 5), processing each document in sequence (line 7). If the document we are processing is in the event timeframe (line 9), then we check to see if adding the document resulted in a change of the system's value estimate (line 11). If there is no change in the value estimate, the system should return ∅ and continue processing documents. If the estimate changes, the system should return the new value as well as the id of the supporting sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Temporal Summarization</head><p>Each event will be retrospectively analyzed for important sub-events or 'nuggets', each with a precise timestamp and text describing the sub-event. Our evaluation metrics will measure the degree to which a system can generate these nuggets in a timely manner.</p><p>A system/run update is a timestamped short text string comparable in length to a sentence. Colloquially, an update can be thought of as an SMS message or tweet. We generally denote an update as the pair (string, time): u = (u.string, u.t). For example u = ("The hurricane was upgraded to category 4", 1330169580) represents an update describing the hurricane category, now 4, pushed out by system S at UNIX time 1330169580 (i.e. 1330169580 seconds after 0:00 UTC on January 1, 1970). In this year's evaluation, the update string is chosen from the set of segmented sentences in the corpus as defined in the guidelines.</p><p>Two updates are semantically comparable using a text similarity measure or a manual annotation process applied to their string components; if two updates u and u refer to the same information (semantically matching), then we write this as u ≈ u , irrespective of their timestamps. Because two systems might deliver the same update string at different times, it is generally not the case that u.string = u .string implies u.t = u .t.</p><p>Given an event, our manual annotation process generates a set of gold standard updates called nuggets, extracted from wikipedia event pages and timestamped according to the revision history of the page. Editorial guidelines recommend that nuggets be a very short sentence, including only a single sub-event, fact, location, date, etc, associated with topic relevance. We refer to the canonical set of updates as N . This manual annotation process is retrospective and subject to error in the precision of the timestamp. As a result we might encounter situations where the timestamp of the nugget is later than the earliest matching update.</p><p>In response to an event's news coverage, a system/run broadcasts a set of timestamped updates generated in the manner described in the Guidelines. We refer to a system's set of updates as S. The set of updates received before time τ is,</p><formula xml:id="formula_1" coords="4,256.60,567.47,220.88,9.65">S τ = {u ∈ S : u.t &lt; τ } (1)</formula><p>Our goal in this evaluation is to measure the precision, recall, timeliness, and novelty of updates provided by a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Preliminaries</head><p>Our evaluation metrics are based on the following auxiliary functions.</p><p>• Nugget Relevance. Each nugget n ∈ N has an associated relevance/importance grade,</p><formula xml:id="formula_2" coords="5,287.10,161.80,190.39,8.77">R : N → [0, 1]<label>(2)</label></formula><p>R(n) measures the importance of the content (information) in the nugget.</p><p>Nugget importance was provided on a 0-3 scale by assessors (no importance to high importance). For graded relevance, we normalize on an exponential scale, since high importance nuggets are described as "of key importance to the query", whereas low importance nuggets are "of any importance to the query". When binary relevance is needed, everything of any relevance is relevant (0 is the only non-relevant grade). The actual relevance functions used are presented below; n.i denotes the nugget importance as assigned by the assessor.</p><formula xml:id="formula_3" coords="5,196.12,298.94,281.36,70.35">R graded (n) = e n.i e max n ∈N n .i Graded relevance (3) R binary (n) = 1 iff n.i &gt; 0 0 otherwise Binary relevance (4)<label>(5)</label></formula><p>Note that for graded relevance, returning exactly the nugget set as the system output updates and nothing more ("perfect system"), would usually not result in an expected gain of 1. However, using binary relevance, the perfect system would score an expected gain of 1.</p><p>The relevance can be discounted in time or in size, hence the following discounting functions.</p><p>• Latency Discount. Given a reference timestamp of a matching nugget, t * , a latency penalty function L(t * , t) is a monotonically decreasing function of t -t * . A system may return an update matching Wikipedia information before the Wikipedia information exists; thus we use a function that is smooth and decays on both the positive and negative sides.</p><p>The actual function used is presented below with arguments the nugget Wikipedia time (wiki-edit timestamp) n.t, and the update time u.t as indicated by the system. Current parameters allow the latency discount factor to vary from 0 to 2 (1 means nugget time equal to update time), and flattens at around one day(± 24 hours). Note that as a result, gain and expected gain can be greater than 1.</p><formula xml:id="formula_4" coords="5,176.02,587.15,292.98,22.31">L(n.t, u.t) = 1 - 2 π arctan( u.t -n.t α ) latency-discount<label>(</label></formula><p>• Verbosity Normalization. The task definition assumes that a user receives a stream of updates from the system. Consequently, we want to penalize systems for including unreasonably long updates, since these easily lead to significantly higher reading effort. The verbosity can be defined as a string length penalty function, monotonically increasing in the number of words of the update string. We will refer to this normalization function as V(u).</p><p>For the actual verbosity implementation, we approximate the number of extra nuggets worth of information in a given update. This is done by finding all text which did not match a nugget (as defined by the assessors), and dividing the number of words in the text by the average number of words in a nugget for that query.</p><formula xml:id="formula_5" coords="6,158.68,472.69,346.32,62.93">V(u) = 1 + |all words u | -|nuggetmatching words u | AV G n |words n | (8) = 1 + |u| -| ∪ n∈M -1 (u,S) M(n, S)| avg n∈N |n| verbosity-normalization<label>(9)</label></formula><p>where |u|, |n| are the length (in number of words) of the update u, and nugget n.</p><p>Note that if an update has all its words being part of some match to a nugget, the verbosity is V (u)=1; otherwise V (u)-1 is an approximation of the "extra non-matching words" in terms of equivalent number of nuggets.</p><p>• Update-Nugget Matching. We also define a key earliest matching function between a nugget and an update set,</p><formula xml:id="formula_6" coords="6,251.13,654.47,226.35,10.62">M(n, S) = argmin {u∈S:n≈u} u.t<label>(10)</label></formula><p>or ∅ if there is no matching update for n. M(n, S) should be interpreted as "given n, the earliest matching update in S."</p><p>We also define the set of nuggets for which u is the earliest matching update as,</p><formula xml:id="formula_7" coords="7,237.89,187.66,239.59,10.81">M -1 (u, S) = {n ∈ N : M(n, S) = u}<label>(11)</label></formula><p>Note that an update can be the earliest matching update for more than one nugget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Metrics</head><p>Using the previously defined notion of relevance, latency, verbosity, and matching we can define several measures of interest for Temporal Summarization.</p><p>Given an update u and a matching nugget n (i.e. u ≈ n), we can define the discounted gain as,</p><formula xml:id="formula_8" coords="7,227.80,325.64,249.68,8.77">g(u, n) = R(n) × discounting factor (12)</formula><p>Given the previously defined discounts, we have the following family of discounted gains,</p><formula xml:id="formula_9" coords="7,171.86,381.43,305.62,38.66">g F (u, n) = R(n) discount-free gain (13) g L (u, n) = R(n) × L(n.t, u.t) latency-discounted gain (14)<label>(15)</label></formula><p>Since an update can be the earliest to match several nuggets (u ≈ n), we define the gain of an update with respect to a system (or participant run) S as the sum of [latency-discounted] relevance of the nuggets for which it is the earliest matching update:</p><formula xml:id="formula_10" coords="7,243.07,492.46,234.41,20.64">G(u, S) = n∈M -1 (u,S) g(u, n)<label>(16)</label></formula><p>where the gain can be either of the discounted gains described earlier. Note that for an appropriate discounting function, G(u, S) ∈ [0, 1], although for the latency-discounted gain, given the imperfect nature of model timestamps,</p><formula xml:id="formula_11" coords="7,133.77,561.40,74.43,9.68">G L (u, S) ∈ [0, 2].</formula><p>One way to evaluate a system is to measure the expected gain for a system update. This is similar to traditional notions of precision in information retrieval evaluation. Over a large population of system updates, we can estimate this measure reliably. The computation of the expected update gain for system S by time τ is the average of the gain per update:</p><formula xml:id="formula_12" coords="8,211.57,147.28,265.91,90.18">EG(S) = 1 |S| u∈S G(u, S) (17) = 1 |S| u∈S n∈M -1 (u,S) g(u, n) = 1 |S| {n∈N :M(n,S) =∅} g(M(n, S), n)<label>(18)</label></formula><p>Additionally, we may penalize "verbosity" by normalizing not by the number of system updates, but by the overall verbosity of the system</p><formula xml:id="formula_13" coords="8,188.83,281.19,288.65,27.27">EG V (S) = 1 u∈S V(u) {n∈N :M(n,S) =∅} g(M(n, S), n) (19)</formula><p>Our definition of g is such that it:</p><p>• does not penalize for large a update matching several nuggets, as opposed to a few small updates each matching a nugget, due to verbosity weighting,</p><p>• penalizes for late updates (against matched nugget reference timestamp), and</p><p>• penalizes "verbosity" of updates text not matching any nuggets.</p><p>Furthermore, we have that G(u, S τ ) ∈ [0, 1] if all update timestamps are at or after matching model timestamps. Over a set of events, the mean expected gain is defined as,</p><formula xml:id="formula_14" coords="8,251.94,467.75,221.11,26.80">MEG = 1 |E| ∈E EG(S ) (<label>20</label></formula><formula xml:id="formula_15" coords="8,473.05,474.49,4.43,8.74">)</formula><p>where E is the set of evaluation events and S is the system submission for event .</p><p>Because a user interest may be concentrated immediately after an event and because a system's performance (in terms of gain) may be dependent on the time after an event, we will also consider a time-sensitive expected gain for the first τ seconds,</p><formula xml:id="formula_16" coords="8,263.31,587.46,214.16,9.68">EG τ (S) = EG(S τ )<label>(21)</label></formula><p>with MEG τ defined similarly.</p><p>In addition to good expected gain, we are interested in a system providing a comprehensive set of updates. That is, we would like the system to cover as many nuggets as possible. This is similar to traditional notions of recall in information retrieval evaluation. Given a set of system updates, S, we define the comprehensiveness (and latency-comprehensiveness) of the system as:</p><formula xml:id="formula_17" coords="9,195.43,157.29,282.05,91.40">C(S) = 1 n∈N R(n) {n∈N :M(n,S) =∅} g(M(n, S), n) (22) = 1 n∈N R(n) u∈S n∈M -1 (u,S) g(u, n) = 1 n∈N R(n) u∈S G(u, S)<label>(23)</label></formula><p>We also define a time-sensitive notion of comprehensiveness,</p><formula xml:id="formula_18" coords="9,271.57,281.82,201.48,9.68">C τ (S) = C(S τ ) (<label>24</label></formula><formula xml:id="formula_19" coords="9,473.06,281.85,4.43,8.74">)</formula><p>with an aggregated measure defined as,</p><formula xml:id="formula_20" coords="9,282.43,323.00,195.05,26.29">te ts C τ (S)dτ<label>(25)</label></formula><p>which measures how quickly a system captures nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Value Tracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Notation</head><p>Let V be the set of possible values deliverable to the user. A value is either a real number or a pair of numbers, depending on the type. A value update u refers to a timestamped value indexed such that u v ∈ V and u t is the timestamp of the update. Given a set of value updates U for an event, we assume the predicted value at time τ is the value of the most recent update before τ . We represent this as f U (τ ) ∈ V. In order for this function to be well-defined, we request that participants provide a baseline or prior prediction before having seen any evidence.</p><p>Given an event and an attribute type, our manual annotation process generates a set of timestamped target attribute values U * . In most cases, the annotation process will generate a single target value (i.e. f U * (τ ) is constant for all τ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Metrics</head><p>We evaluate a system according to the expected error with respect to the true attribute value f U * , where, for scalar attributes, the error is defined as</p><formula xml:id="formula_21" coords="9,211.04,635.56,98.06,23.23">EE(U, U * , τ ) = 1 t e -t s</formula><formula xml:id="formula_22" coords="10,234.69,339.66,242.79,11.72">Err(U, U * , τ ) = |f U * (τ ) -f U (τ )| (27)</formula><p>and for geographic attributes, we use the Vincenty distance. We gathered target attribute values from a variety of sources, including Wikipedia and official weather reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Judging</head><p>The evaluation process occurred in two phases: 1) Gold Nugget Extraction and 2) Update-Nugget Matching. The first phase defined the space of relevant information for the queries and the second phase matched this information to updates provided by participants in order to evaluate their accuracy and coverage of the information space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gold Nugget Extraction</head><p>In this first phase, assessors were asked to read all edits of the Wikipedia article for each query, manually extracting text perceived as relevant and novel for that edit. Additionally, assessors assigned an importance grade to every text fragment, or nugget, as well as noted any dependencies in the information. An example portion of the extraction interface can be seen in Figure <ref type="figure" coords="10,420.77,586.14,3.87,8.74" target="#fig_3">3</ref>. In order to simplify later matching, assessors were told to extract nuggets such that they were atomic pieces of information relevant to the query. However, knowing that information can be highly contextual, we allow for the notion of dependencies between nuggets: a nugget may be relevant to a query if and only if another nugget is also present. For evaluation purposes, a nugget was considered unmatched if it had unmatched nuggets on which it depended. Additionally, we provided a method for assessors to track updates to pieces of information. This was primarily used to allow them to collate their work and reduce redundancy, but it was also used in the matching phase to help the assessor find the closest piece of information to a match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Update-Nugget Matching</head><p>Once submissions were received, we performed a variant of depth-pooling in order to sample updates for evaluation. We sampled the top 60 updates per query and run as sorted by the provided confidence scores (highest first). Additionally, we performed near-duplicate detection among update text to increase the covered set. This resulted in sampled update counts as per Table <ref type="table" coords="11,446.06,410.19,3.87,8.74" target="#tab_1">1</ref>. One note here is that not all runs contained 60 updates per query; for the run-query pairs with less than 60 updates, all updates were sampled.</p><p>The sampled updates were presented in an interface similar to the one for extraction. Assessors examined and matched updates to nuggets by selecting portions of updates which matched a given nugget, as nuggets are atomic but updates are not. An assessor was allowed to break a nugget into two or more new nuggets to improve atomicity if desired. Note that a nugget may match multiple updates, and an update may match multiple nuggets. An example view of the matching interface can be seen in Figure <ref type="figure" coords="11,342.68,517.79,3.87,8.74" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sequential Update Summarization</head><p>The results for the sequential update summarization task are summarized in Table <ref type="table" coords="11,161.10,607.09,3.87,8.74" target="#tab_3">2</ref>. For evaluation results for individual queries, see the appendices of the TREC proceedings. It is important to note that performance on the Gain-based measures tends to be anti-correlated with the performance on the Comprehensivenessbased measures. This is expected, as these measures, like precision and recall, are difficult to simultaneously optimize. This discrepancy is partly a result of the varying number of updates provided by a group, as can be seen in Table <ref type="table" coords="12,331.99,540.06,3.87,8.74" target="#tab_1">1</ref>. The groups with higher gain have fewer updates and those with higher comprehensiveness have more updates. This shows that the task is achievable from both standpoints, but that improvements can come from attempting to rationalize these two needs or at least by prioritizing them based on user intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunID</head><p>These average scores are somewhat less indicative of the discriminative power of the measures as there was a fairly large amount of variance between queries. However, these averages are consistent with the performance across the queries and are therefore useful as overall measures of run performance. Additionally, it should be noted that the mean number of runs per update was 3.5.</p><p>Unfortunately, the submission for one group, BJUT, was in an inconsistent  format, and as such was not pooled. Therefore even with a fixed submission, the submission was less likely to match the sampled updates, and in fact matched none of them. Another group, hltcoe, submitted fixed versions of their runs shortly after the deadline, but their initial submission was included in the pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Value Tracking</head><p>We present results for value tracking in Table <ref type="table" coords="14,340.98,401.73,3.87,8.74" target="#tab_4">3</ref>. No system consistently performed strongly across attributes and several runs focused on individual attributes, omitting predictions for others. In order to provide a reference, Table <ref type="table" coords="14,133.77,437.59,4.98,8.74" target="#tab_4">3</ref> also includes the performance for a run which outputs '0' for non-location attributes. This allows us to detect runs which ignored an attribute and, for those that did not, whether the system was effective. In the cases where runs underperformed the baseline, this was often due to egregious extraction error (e.g. predicting tens of thousands of injuries instead of hundreds). Even when relatively accurate predictions were made, the expected error metric is sensitive to outliers such as these.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,187.43,389.11,236.39,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sequential update summarization simulator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,228.83,424.98,153.59,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Value tracking simulator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,133.77,276.04,343.72,8.74;10,133.77,287.99,71.18,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Extraction interface used by assessors to extract nuggets from Wikipedia edits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,135.10,248.41,341.06,8.74;11,133.77,124.80,343.71,108.49"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Matching interface used by assessors to match updates and nuggets.</figDesc><graphic coords="11,133.77,124.80,343.71,108.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,133.77,126.76,343.71,378.25"><head>Table 1 :</head><label>1</label><figDesc>µ and σ(in parens) of number of updates sampled for each run over all topics. *run not pooled.</figDesc><table coords="12,197.24,126.76,216.77,343.88"><row><cell></cell><cell># Updates</cell></row><row><cell>UWaterlooMDS-rg1</cell><cell>414.1 (113.2)</cell></row><row><cell>UWaterlooMDS-rg2</cell><cell>402.8 (113.7)</cell></row><row><cell cols="2">UWaterlooMDS-UWMDSqlec2t25 370.4 (104.6)</cell></row><row><cell cols="2">UWaterlooMDS-UWMDSqlec4t50 357.2 (107.0)</cell></row><row><cell>UWaterlooMDS-rg4</cell><cell>281.6 (116.6)</cell></row><row><cell>UWaterlooMDS-rg3</cell><cell>275 (107.9)</cell></row><row><cell>hltcoe-EXTERNAL</cell><cell>184.4 (77.8)</cell></row><row><cell>hltcoe-Baseline</cell><cell>183.9 (90.0)</cell></row><row><cell>hltcoe-BasePred</cell><cell>167.7 (75.5)</cell></row><row><cell>uogTr-uogTrEMMQ2</cell><cell>147.6 (34.7)</cell></row><row><cell>uogTr-uogTrNMM</cell><cell>140 (35.2)</cell></row><row><cell>PRIS-cluster1</cell><cell>100.9 (17.9)</cell></row><row><cell>PRIS-cluster4</cell><cell>100.7 (17.7)</cell></row><row><cell>uogTr-uogTrNMTm3FMM4</cell><cell>96.4 (29.2)</cell></row><row><cell>uogTr-uogTrNMTm1MM3</cell><cell>89.3 (29.9)</cell></row><row><cell>PRIS-cluster2</cell><cell>89 (17.3)</cell></row><row><cell>uogTr-uogTrNSQ1</cell><cell>80.7 (16.0)</cell></row><row><cell>hltcoe-TuneBasePred2*</cell><cell>74.8 (57.3)</cell></row><row><cell>wim GY 2013-SUS1</cell><cell>73.1 (15.5)</cell></row><row><cell>UWaterlooMDS-NormEgrep</cell><cell>65.3 (21.3)</cell></row><row><cell>ICTNET-run1</cell><cell>56.9 (24.9)</cell></row><row><cell>ICTNET-run2</cell><cell>55.3 (24.5)</cell></row><row><cell>hltcoe-TuneExternal2*</cell><cell>49.7 (46.8)</cell></row><row><cell>PRIS-cluster3</cell><cell>38.2 (11.7)</cell></row><row><cell>PRIS-cluster5</cell><cell>21.8 (2.6)</cell></row><row><cell>UWaterlooMDS-CosineEgrep</cell><cell>11.9 (7.5)</cell></row><row><cell>BJUT-Q0*</cell><cell>0 (0.0)</cell></row><row><cell>ALL</cell><cell>145.5 (136.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="13,133.77,534.31,343.71,20.69"><head>Table 2 :</head><label>2</label><figDesc>µ and σ of primary task metrics over all queries, sorted by Expected Latency Gain. *run not pooled.</figDesc><table coords="14,149.14,126.37,312.96,116.73"><row><cell></cell><cell>location</cell><cell>deaths</cell><cell cols="2">injuries financial impact</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(10 9 )</cell></row><row><cell>baseline</cell><cell cols="3">20038.0 195.111 473.222</cell><cell>13.3539</cell></row><row><cell>PRIS-PRISTS1</cell><cell cols="3">18101.8 37880.4 64886.5</cell><cell>9.5251</cell></row><row><cell>PRIS-PRISTS2</cell><cell cols="2">11864.1 88666.5</cell><cell>106099</cell><cell>25.1175</cell></row><row><cell>PRIS-PRISTS3</cell><cell cols="2">11796.3 88666.5</cell><cell>106375</cell><cell>42.5367</cell></row><row><cell>BJUT-Q1</cell><cell>20038.0</cell><cell>138.1</cell><cell>473.222</cell><cell>13.3539</cell></row><row><cell cols="4">ICTNET-ValueTask 20038.0 188.495 390.985</cell><cell>13.3539</cell></row><row><cell>wim GY 2013-VT1</cell><cell cols="3">14483.6 2726.06 410.092</cell><cell>13.3539</cell></row><row><cell>wim GY 2013-VT2</cell><cell cols="3">4660.76 2396.12 410.531</cell><cell>13.3539</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,133.77,255.83,343.71,32.65"><head>Table 3 :</head><label>3</label><figDesc>Value Tracking Expected Error by Attribute. The baseline run predicts '0' for all non-location attributes and achieves a maximum location distance of 20038 km.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
