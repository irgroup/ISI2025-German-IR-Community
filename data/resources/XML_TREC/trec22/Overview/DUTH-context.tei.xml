<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.64,135.09,358.73,15.12;1,485.36,132.43,5.98,10.48">DUTH at TREC 2013 Contextual Suggestion Track *</title>
				<funder ref="#_TRqDGme">
					<orgName type="full">project ATLAS (Advanced Tourism Planning)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,190.32,167.57,84.59,10.48;1,274.91,165.96,3.02,6.99"><forename type="first">George</forename><surname>Drosatos</surname></persName>
							<email>gdrosato@ee.duth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering Dept</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Language &amp; Speech Processing</orgName>
								<orgName type="department" key="dep2">Athena Research &amp; Innovation Center University Campus</orgName>
								<address>
									<addrLine>Xanthi 67 100</addrLine>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.64,167.57,109.47,10.48"><forename type="first">Giorgos</forename><surname>Stamatelatos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering Dept</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Language &amp; Speech Processing</orgName>
								<orgName type="department" key="dep2">Athena Research &amp; Innovation Center University Campus</orgName>
								<address>
									<addrLine>Xanthi 67 100</addrLine>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.24,181.52,81.02,10.48"><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering Dept</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Language &amp; Speech Processing</orgName>
								<orgName type="department" key="dep2">Athena Research &amp; Innovation Center University Campus</orgName>
								<address>
									<addrLine>Xanthi 67 100</addrLine>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.24,181.52,103.20,10.48"><forename type="first">Pavlos</forename><forename type="middle">S</forename><surname>Efraimidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering Dept</orgName>
								<orgName type="institution">Democritus University of Thrace</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Language &amp; Speech Processing</orgName>
								<orgName type="department" key="dep2">Athena Research &amp; Innovation Center University Campus</orgName>
								<address>
									<addrLine>Xanthi 67 100</addrLine>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.64,135.09,358.73,15.12;1,485.36,132.43,5.98,10.48">DUTH at TREC 2013 Contextual Suggestion Track *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B0F0B52566B3DDCE2BDF96AACA4DFEC0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this report we give an overview of our participation in the TREC 2013 Contextual Suggestion Track. We present an approach for context processing that comprises a newly designed and fine-tuned POI (Point Of Interest) data collection technique, a crowdsourcing approach to speed up data collection and two radically different approaches for suggestion processing (a k-NN based and a Rocchio-like). In the context processing, we collect POIs from three popular place search engines, Google Places, Foursquare and Yelp. The collected POIs are enriched by adding snippets from the Google and Bing search engines using crowdsourcing techniques. In the suggestion processing, we propose two methods. The first submits each candidate place as a query to an index of a user's rated examples and scores it based on the top-k results. The second method is based on Rocchio's algorithm and uses the rated examples per user profile to generate a personal query which is then submitted to an index of all candidate places. The track evaluation shows that both approaches are working well; especially the Rocchio-like approach is the most promising since it scores almost firmly above the median system and achieves the best system result in almost half of the judged contextprofile pairs. In the final TREC system rankings, we are the 2nd best group in MRR and TBG, and 3rd best group in P@5, out of 15 groups in the category we participated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC 2013 is the second year that the Contextual Suggestion Track is running. The track's goal is to investigate search techniques considering as context only the user's location, as well as, user interests via personal preferences and past history. In other words, the track focuses on one situation: a user with a mobile device with limited interaction but some sort of a user profile; who is in a strange town; and who is looking for something to do. There is no explicit query; the implicit query is: Here I am, what should I do?</p><p>The remainder of this report is organized as follows. Section 2 describes our methodology for the context processing. The proposed suggestion models with details about the steps that we  followed are presented in Section 3. Our submitted runs and the official results of TREC Contextual Suggestion Track are described in Section 4. Finally, Section 5 draws our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Context Processing</head><p>The goal of the Contextual Suggestion Track 2013 was to recommend interesting places and activities from either the open web or ClueWeb12. In our case, we used the open web because we did not obtain the ClueWeb12 dataset in time. Specifically, we followed a context processing approach similar to <ref type="bibr" coords="2,136.17,447.49,10.52,8.74" target="#b3">[4]</ref> but we enriched it with extra place search engines that provide POIs (Points of Interest) based on geographical location and a range of place types. Figure <ref type="figure" coords="2,402.40,459.45,4.98,8.74" target="#fig_0">1</ref> shows an overview of our context processing. In more detail, the steps that we followed to create a pool of POIs for every context (i.e. a primary city of 50 randomly selected metropolitan areas) are:</p><p>1. POI collection: We generate appropriate queries for every context and submit them to three place search engines, namely, Google Places<ref type="foot" coords="2,335.20,515.66,3.97,6.12" target="#foot_0">1</ref> , Foursquare<ref type="foot" coords="2,394.84,515.66,3.97,6.12" target="#foot_1">2</ref> and Yelp<ref type="foot" coords="2,442.42,515.66,3.97,6.12" target="#foot_2">3</ref> . The submitted queries have as parameters the geographical location (latitude and longitude) of the context and a set of place types that are expected to be interesting to the user. Several of the place types covered by the search engines seemed to be irrelevant to the requirements of the TREC challenge. Thus, we used the description of the TREC challenge and the example data sets to define the set of place types that are relevant to the specific search task. Each search engine has its own vocabulary for place types and consequently we selected three sets of place types, one for each search engine. The sets of place types for every engine are: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Obtaining the URLs:</head><p>The Yelp API provides only the Yelp URLs. The actual URLs of places are in the contextual list of places of Yelp; in order to retrieve them from there we use a crowdsourced distributed technique (described in Section 2.1) due to high volume of retrieval operartions that have to be executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Raw POI merging:</head><p>The partial contextual lists of places of every search engine are joined into a single one. Two items in different lists are merged if their URLs or their phone numbers are identical, and the distance between their titles is smaller than a threshold (&lt; 0.1). For the computation of distance between the titles, we use the Jaro-Winkler distance <ref type="bibr" coords="3,473.21,488.73,15.50,8.74" target="#b9">[10]</ref> that is a measure of similarity between two strings. The general rules that we follow when two places match with the one of two previous ways are:</p><p>• When two matched items differ in their URL, we keep the URL of first engine in the following order of decreasing priority: Google Places, Foursquare, Yelp.</p><p>• When two matched items differ in their title, we keep the longest title.</p><p>• The set of place types of the merged item is the union of the place types of the individual items, that is, we keep all the place types assigned by the search engines.</p><p>The merged list of places contains only places that have a website URL in accordance to the requirements of the track <ref type="bibr" coords="3,229.45,620.24,9.96,8.74" target="#b8">[9]</ref>. In Table <ref type="table" coords="3,286.67,620.24,3.87,8.74" target="#tab_2">1</ref>, we show aggregated data about the list of collected places for each context.  </p><formula xml:id="formula_0" coords="4,143.88,177.25,275.51,22.18">• • • • • • • • • • • • • • • Orlando,</formula><formula xml:id="formula_1" coords="4,458.29,148.59,6.33,68.77">           </formula><p>Sorting by places 4. Enrichment of POI descriptions: The final step is to enrich the merged list of places with descriptions. We use as descriptions the snippets returned by general purpose web search engines when the URL of a place is submitted as a search query. In particular, we use Google <ref type="foot" coords="4,146.94,314.55,3.97,6.12" target="#foot_3">4</ref> and Bing<ref type="foot" coords="4,193.60,314.55,3.97,6.12" target="#foot_4">5</ref> to collect the corresponding snippets. For retrieving the snippets we use a crowdsourcing mechanism (described in detail in Section 2.1) mainly in order to overcome the querying-limits of engines. The snippets are used to generate two versions of the description for each place. The first version, or else the "retrieval" version, of the description is used for the retrieval tasks in our contextual suggestion algorithms. The second version, or else the "presentation" version, is the description presented to the users. In merging the snippets, for the retrieval descriptions we keep only the snippet of Google (or Bing's when Google fails to retrieve any result), and for the presentation descriptions, we keep both of them if the Jaro-Winkler distance <ref type="bibr" coords="4,215.89,411.77,15.50,8.74" target="#b9">[10]</ref> of snippets is bigger than a threshold (≥ 0.25), otherwise we keep the snippet of Google (or Bing's when Google fails).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Crowdsourcing mechanism</head><p>The crowdsourcing mechanism we used to collect the necessary snippets and website URLs from Yelp is based on a distributed HTTP client-server architecture in order to overcome certain search engine limitations and exploit modern web browser heuristics on malformed HTML translation. Our client is executed in a web browser (Google Chrome) extension context, as a silent background process without the need of user interaction. At regular intervals, each client requests a new query from the service, submits it to the engines (Google, Bing or Yelp) and reports back the snippet of the most relevant result or the URL in the case of Yelp. The service prepares the queries to be submitted to the engines, distributes the workload among clients and uses a MySQL database backend to store the information retrieved by the clients. A small community of volunteers installed our client to their Chrome browsers. Overall, an average of about 25 instances of the client were used to collect the snippets of 19,825 unique URLs and the website URLs of 15,787 Yelp places, 53% of which included a website. Still, the process lasted around 20 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Suggestion Processing</head><p>In this section we present two suggestion models that are applied to the pool of POIs that are collected in the context processing (Section 2). The models are depicted in Figure <ref type="figure" coords="5,453.33,131.98,3.87,8.74" target="#fig_1">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pool with</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Suggestion Model based on k-NN Classification</head><p>The idea is to assign a rating or score to each candidate POI based on the ratings of its k semantically nearest POIs (neighbors) in the user profile. Then all candidate POIs are ranked in a decreasing order of their assigned scores. The model is implemented in three main steps:</p><p>1. Indexing the rated POIs. In order to be able to find the k semantically nearest (rated) neighbor POIs of a candidate (unrated) POI, we create an index of the POIs that are part of the user profiles and have been evaluated and rated by the users. For each rated POI we index its title, description, place types and the text of its website. The place types of the rated POIs are not provided by the track, but we retrieve them from the three place search engines that we use in context processing (as we described in Section 2). For indexing, we use Indri<ref type="foot" coords="5,494.46,611.61,3.97,6.12" target="#foot_5">6</ref> v5.5 with the default settings of this version, except that we enable the Krovetz stemmer <ref type="bibr" coords="5,487.63,625.14,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Generating queries from candidate POIs. We generate a query per candidate POI in a context. The query consists of the POI title, place types and the description of the POI that we retrieved in the context processing. From the query, we remove all punctuation and special characters.</p><p>3. Scoring candidate POIs based on their k-NNs. We submit the queries (per context) that are generated in Step 2 to the index that is created in Step 1 in order to rank the rated POIs in an increasing semantical distance. In a standard k-NN <ref type="bibr" coords="6,340.31,177.48,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="6,353.53,177.48,7.01,8.74" target="#b5">6]</ref>, a candidate POI (represented by its corresponding generated query) would be assigned the majority rating of the top-k retrieved POIs. In initial experiments, however, we found that taking into account the ranks or retrieval scores of the top-k results is beneficial. We experimented with several formulas using crossvalidation, such as linear (e.g. Borda Count) or exponential weights decreasing with the rank, and we settled for the following best-performing formula for scoring each candidate POI P :</p><formula xml:id="formula_2" coords="6,241.94,256.98,274.02,64.16">P = k i=1 s i • R i k i=1 s i , R i = R D i + R W i 2 , (<label>1</label></formula><formula xml:id="formula_3" coords="6,515.96,284.03,4.24,8.74">)</formula><p>where s i is the Indri tf-idf score of the ith ranked POI. This formula assigns to a candidate POI a score equal to the weighted average of the ratings of the k-nearest-neighbor POIs in a user profile, where weights are given by tf-idf similarity. As POI's rating R i we use the average rating of the description (R D i ) and the website (R W i ), because in Step 1 we index both the description and the text of website. The value of k that we use in our suggestions was optimized to k = 23 by using 5-folds cross-validation <ref type="bibr" coords="6,379.29,390.58,10.52,8.74" target="#b7">[8]</ref> on the example places. The scored candidate places are then ranked in a decreasing order of their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Suggestion Model Based on a Rocchio-like Method</head><p>The idea is to use the rated POIs in the user profile to generate a query using a Rocchio-like relevance feedback method <ref type="bibr" coords="6,210.74,460.58,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="6,224.26,460.58,7.01,8.74" target="#b5">6]</ref>. Then the generated query is used to score and rank all candidate POIs</p><p>The model is implemented in three main steps:</p><p>1. Indexing all candidate POIs per context. Per context, we build an index of the POIs collected during the context processing. For indexing, we use the title, description and the place types.</p><p>As noted earlier, the description used for the index is only the snippet of Google, or Bing when Google failed to retrieve any result. The POIs are indexed with the Indri v5.5, using the default settings of this version, except that we enable the Krovetz stemmer <ref type="bibr" coords="6,464.93,551.40,9.96,8.74" target="#b4">[5]</ref>.</p><p>2. Generating a personalized weighted query per user with a Rocchio-like relevance feedback method. We generate a query per user, representing her preferences, based on her rated POIs. The terms used in the query are taken from the title, the place types and the descriptions of the rated POIs. </p><formula xml:id="formula_4" coords="6,227.14,606.77,119.07,9.65">Let D i =&lt; d i,1 , • • • , d i,M &gt;</formula><formula xml:id="formula_5" coords="6,300.39,630.68,80.03,9.65">d i,j = log(1 + f i,j )</formula><p>, where the f i,j is the frequency of j term in the D i . Then, the trained weighted query of user u is given by the equation:</p><formula xml:id="formula_6" coords="7,240.66,106.00,279.54,33.68">Q u = 4 j=0   (j -2) 1 |R j,u | D∈Rj,u D  <label>(2)</label></formula><p>where the R j,u is the subset of the examples that were rated by user u with score j. In other words, we take the centroids per rating j, multiply them with the normalized rating j -2 (so that the neutral rating's centroid, i.e., for j = 2, does not contribute anything-it is zeroed), and then add the centroids in a Rocchio relevance feedback fashion. All the terms of Q u that have weight less or equal than zero are excluded from the query. The weight of every term is included in the query by using the Indri Query Language and has, e.g., the following form:</p><formula xml:id="formula_7" coords="7,210.65,234.63,215.61,9.02">#weight( 3.0 museum 2.7 art • • • 0.1 nice )</formula><p>3. Retrieving suggestions. We submit the personalized queries of the users that are generated in the second step in the index of every context (created in the first step). Our search engine is again the Indri v5.5 with the default (LM) retrieval model. The results of each query, with a possible cutoff threshold (e.g. top-50 results), are our suggestions for the corresponding user and context. In order to evaluate our methods, we developed an online tool which prompts the user to rate the description of a POI according to her own interests with a scale of 0 to 4. On the main modal of the application, the user is presented with the list of POIs that we collected, each of which contains its name and the associated description (Section 2, Step 4). Initially, the user is asked to rate the 50 example places from Philadelphia, essentially creating her profile. She also chooses a city and rates its consisting POIs using the same criteria. We then feed this profile to our models and compare the suggestions to the actual ratings that the user provided using the Pearson productmoment correlation coefficient <ref type="bibr" coords="8,229.37,274.24,9.96,8.74" target="#b2">[3]</ref>. However, due to the low number of participants (specifically 5) we managed to involve before the submission deadline, this method did not prove particularly useful. Thus, we leaned towards the use of cross-validation described in Section 3.1 (Step 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">An Attempt to Evaluate via a User Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs and Results</head><p>We submitted two runs to the TREC 2013 Contextual Suggestion Track. The first run is labeled DuTH A and uses the k-NN classification technique, while the second run (DuTH B) is based on the Rocchio-like approach (Section 3). The evaluation results according to the P@5, MRR and TBG measures over all the profiles and contexts of our two runs are reported in Table <ref type="table" coords="8,297.12,400.21,3.87,8.74" target="#tab_5">2</ref>. The definitions of these measures are:</p><p>• Precision at Rank 5 (P@5): The fraction of suggestions within the top-5 results where the user liked both the description and the geographically appropriate document.</p><p>• Mean Reciprocal Rank (MRR): One over the rank of the first suggestion where the user liked both the description and the geographically appropriate document.</p><p>• Time-Biased Gain (TBG): This measure provides a unifying framework for information retrieval evaluation, generalizing many traditional effectiveness measures while accommodating aspects of user behavior not captured by these measures. By using time as a basis for calibration against actual user data, time-biased gain can reflect aspects of the search process that directly impact user experience, including document length, near-duplicate documents, and summaries <ref type="bibr" coords="8,185.48,540.11,9.96,8.74" target="#b1">[2]</ref>.</p><p>According to Table <ref type="table" coords="8,193.16,558.95,3.87,8.74" target="#tab_5">2</ref>, DuTH B yielded better results than DuTH A in all the evaluation measures. In any case, our two runs seem very promising considering the Best, Median and Worst results of the 34 submitted runs, provided for all three measures. The comparison of our results with the Best and Median results are shown in Table <ref type="table" coords="8,285.74,594.81,3.87,8.74" target="#tab_6">3</ref>. In P@5, DuTH B performed equal or better then the Median in 209 of the 223 judged context-profile pairs and achieved 47 times the best run. In MRR, DuTH B scored equal or better than the Median in 206 of the 223 judged context-profile pairs and achieved 114 times the best run. In all the aforementioned counts, we have included the judged context-profile pairs with zero best score (11 for P@5 and MRR, and 8 for TBG). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a context processing method that we used in order to collect over 22,000 places for the TREC 2013 Contextual Suggestion Track using three popular place search engines: Google Places, Foursquare and Yelp. Furthermore, we proposed two methods for personalized suggestion of places/attractions with respect to given user preferences. The first suggestion model is based on a k-NN algorithm by using tf-idf weights in the calculations of places' scores (run DuTH B). In the second suggestion model, based on Rocchio algorithm, we proposed the generation of a weighted personal query for each user that was created by using terms from the example places and the preferences of user. Then, this personal query is used to retrieve from a context the places that are suggested to the user (run DuTH B).</p><p>In the TREC evaluation results, both approaches seem very promising. DuTH B (i.e. the Rocchiolike approach) performed better than DuTH A. Compared to other groups, DuTH B scored almost firmly above the median (in P@5 and MRR) and achieved the best results in almost half of the judged context-profile pairs (at MRR). In the final TREC system rankings, we are the 2nd best group in MRR and TBG, and 3rd best group in P@5, out of 15 groups in the category we participated. As first-time participants, we are very satisfied with these results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,185.12,313.08,241.75,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A system flowchart of the context processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,179.45,442.07,253.11,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of the proposed suggestion models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,349.84,606.77,170.35,8.74;6,116.71,618.72,403.49,9.65;6,116.71,630.68,179.06,9.65"><head></head><label></label><figDesc>be a training example, where M is the number of terms in the training set of all examples. The d i,j is the weight of j term in the D i ; we use tf-only logarithmic weighting:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,91.80,555.18,428.40,8.74;7,91.80,567.13,57.15,8.74;7,91.80,363.76,428.41,176.30"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Screenshot of a user who has completed the rating of Anniston and the 50 examples from Philadelphia.</figDesc><graphic coords="7,91.80,363.76,428.41,176.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,116.71,98.17,403.50,287.72"><head>•</head><label></label><figDesc>Google Places: Amusement Park, Aquarium, Art Gallery, Bar, Book Store, Bowling Alley, Cafe, Movie Theater, Museum, Park, Restaurant, Shopping Mall, Zoo, Grocery Store/Supermarket, Casino, Night Club, Beauty Salon, Travel Agency, Jewelry Store, Library, Church.</figDesc><table coords="3,116.71,149.97,403.50,235.92"><row><cell>• Foursquare: Arts &amp; Entertainment, Outdoors &amp; Recreation, Food, Farmers Market,</cell></row><row><cell>Smoke Shop, Mall, Gourmet Shop, Nightlife Spot, Spa/Massage, Gift Shop, Gym/Fitness</cell></row><row><cell>Center, Monument/Landmark, Tourist Information Center, Library, Spiritual Center,</cell></row><row><cell>Jewelry Store.</cell></row><row><cell>• Yelp: Arts &amp; Entertainment, Landmarks &amp; Historical Buildings, Food, Tobacco Shops,</cell></row><row><cell>Shopping Centers, Party &amp; Event Planning, Tours, Nightlife, Active Life, Restaurants,</cell></row><row><cell>Beauty &amp; Spas, Cards &amp; Stationery, Travel Services, Department Stores, Religious Or-</cell></row><row><cell>ganizations, Jewelry, Libraries.</cell></row><row><cell>To overcome the limits on the number of results returned by the APIs of the engines (Google</cell></row><row><cell>Places: 200 results per query, Foursquare: 50 results per query, Yelp: 20 results per query),</cell></row><row><cell>we split each query into subqueries with different geographic coordinates to retrieve more</cell></row><row><cell>results. The total area that we cover with the above search engines is a 6 km 2 square per</cell></row><row><cell>city. More precisely, in Foursquare and Yelp we use bounding boxes of 400 m 2 with different</cell></row><row><cell>geographic coordinates, and in Google Places we use circular areas of radius 849 m and dif-</cell></row><row><cell>ferent geographic coordinates as center. Note that a square of side 1200 m can be inscribed</cell></row><row><cell>into a circle of radius 849 m. Consequently, we submit 5 × 5 = 25 queries to Google Places</cell></row><row><cell>and 15 × 15 = 225 queries to Yelp and Foursqaure to cover the 6 km 2 area. We use wider (i.e.</cell></row><row><cell>covering larger areas) subqueries in Google Places since the corresponding API returns more</cell></row><row><cell>results than the other search engines.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,143.88,105.12,308.06,66.61"><head>Table 1 :</head><label>1</label><figDesc>Statistical information about the contextual list of places.</figDesc><table coords="4,143.88,119.15,305.17,52.57"><row><cell>Context</cell><cell cols="4">Google Foursquare Yelp Merged / Sum</cell></row><row><cell>Crestview, FL</cell><cell>103</cell><cell>33</cell><cell>38</cell><cell>131 / 174</cell></row><row><cell>Anniston, AL</cell><cell>139</cell><cell>53</cell><cell>26</cell><cell>168 / 218</cell></row><row><cell>Sumter, SC</cell><cell>147</cell><cell>52</cell><cell>40</cell><cell>173 / 239</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,100.80,105.12,410.39,119.53"><head>Table 2 :</head><label>2</label><figDesc>Mean of results over all the profiles and contexts for P@5, MRR and TBG measures.</figDesc><table coords="8,195.94,119.68,220.11,104.96"><row><cell></cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>Runs:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DuTH A</cell><cell>0.3283</cell><cell>0.4836</cell><cell>1.3109</cell></row><row><cell>DuTH B</cell><cell>0.4090</cell><cell>0.5955</cell><cell>1.8508</cell></row><row><cell>Difference:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">DuTH B vs A +24,58% +23,14% +41,19%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,102.69,105.12,406.62,99.78"><head>Table 3 :</head><label>3</label><figDesc>Number of context-profile pairs with Median-or-better and Best scores per measure.</figDesc><table coords="9,176.61,122.82,258.77,82.07"><row><cell>Runs</cell><cell cols="3">Median-or-better</cell><cell></cell><cell>Best</cell></row><row><cell></cell><cell cols="3">P@5 MRR TBG</cell><cell cols="3">P@5 MRR TBG</cell></row><row><cell cols="2">DuTH A 189</cell><cell>175</cell><cell>151</cell><cell>25</cell><cell>86</cell><cell>22</cell></row><row><cell cols="2">DuTH B 209</cell><cell>206</cell><cell>185</cell><cell>47</cell><cell>114</cell><cell>40</cell></row><row><cell></cell><cell cols="5">Total: 223 judged context-profile pairs</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,107.04,620.42,156.66,6.64"><p>https://developers.google.com/places/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,107.04,629.93,93.15,6.64"><p>https://foursquare.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,107.04,639.43,80.45,6.64"><p>http://www.yelp.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,107.04,630.97,88.92,6.64"><p>http://www.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,107.04,640.48,80.45,6.64"><p>http://www.bing.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,107.04,644.57,118.56,6.64"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The present work was partially funded by the <rs type="funder">project ATLAS (Advanced Tourism Planning)</rs>, <rs type="grantNumber">GSRT/CO-OPERATION/11SYN-10-1730</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TRqDGme">
					<idno type="grant-number">GSRT/CO-OPERATION/11SYN-10-1730</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.28,511.72,407.92,8.74;9,112.28,523.67,171.32,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,244.62,511.72,167.81,8.74">Nearest neighbor pattern classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,421.89,511.72,98.31,8.74;9,112.28,523.67,84.10,8.74">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.28,543.40,407.91,8.74;9,112.28,555.36,407.92,8.74;9,112.28,567.31,407.42,9.02;9,112.28,579.98,118.33,8.30" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,373.18,543.40,142.67,8.74">Evaluating contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings10/pdf/EVIA/09-EVIA2013-DeanHallA.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,124.99,555.36,390.02,8.74">Proceedings of the 5th International Workshop on Evaluating Information Access (EVIA)</title>
		<meeting>the 5th International Workshop on Evaluating Information Access (EVIA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.28,599.00,401.35,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,217.87,599.00,233.82,8.74">Applied statistics: Analysis of variance and regression</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.28,618.72,407.92,8.74;9,112.28,630.68,407.92,8.74;9,112.28,642.63,271.90,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,239.64,618.72,191.47,8.74">Irit at trec 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cabanac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,235.65,630.68,168.49,8.74">Text REtrieval Conference (TREC&apos; 12)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.28,98.20,407.92,8.74;10,112.28,110.15,407.92,8.74;10,112.28,122.11,276.63,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,168.96,98.20,193.32,8.74">Viewing morphology as an inference process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,385.43,98.20,134.77,8.74;10,112.28,110.15,407.92,8.74;10,112.28,122.11,51.96,8.74">Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;93)</title>
		<meeting>the 16th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;93)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.28,142.03,407.92,8.74;10,112.28,153.99,227.06,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,323.42,142.03,162.96,8.74">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.28,173.92,407.92,8.74;10,112.28,185.87,407.92,8.74;10,112.28,197.83,144.79,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,176.51,173.92,187.47,8.74">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,464.23,173.92,55.98,8.74;10,112.28,185.87,295.40,8.74">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.28,217.75,407.92,8.74;10,112.28,229.71,317.89,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,160.15,217.75,288.61,8.74">Cross-Validatory Choice and Assessment of Statistical Predictions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,458.32,217.75,61.88,8.74;10,112.28,229.71,221.10,8.74">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.28,249.63,407.92,8.74;10,112.28,262.30,328.04,8.30" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,246.80,249.63,206.00,8.74">Trec 2013 contextual suggestion track guidelines</title>
		<ptr target="https://sites.google.com/site/treccontext/trec-2013-guidelines" />
	</analytic>
	<monogr>
		<title level="m" coord="10,112.28,249.63,126.75,8.74">TREC Contextual Suggestion</title>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.28,281.51,407.92,8.74;10,112.28,293.47,407.92,8.74;10,112.28,305.42,211.90,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,187.02,281.51,333.19,8.74;10,112.28,293.47,103.59,8.74">String comparator metrics and enhanced decision rules in the fellegi-sunter model of record linkage</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,241.25,293.47,245.59,8.74">Proceedings of the Section on Survey Research Methods</title>
		<meeting>the Section on Survey Research Methods</meeting>
		<imprint>
			<publisher>American Statistical Association</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="354" to="359" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
