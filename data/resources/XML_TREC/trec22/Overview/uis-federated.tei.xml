<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,71.77,84.43,466.18,14.93">Collection and Document Language Models for Resource Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,267.17,116.75,75.38,10.37"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>krisztian.balog@uis.no</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,71.77,84.43,466.18,14.93">Collection and Document Language Models for Resource Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">694273EC5E7777E3834A16D1ADF87338</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the resource selection task of the Federated Web Search track at TREC 2013. We employ two general strategies, collection-centric and documentcentric, formulated in a language modeling framework. Results show that the document-centric approach delivers solid performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We address the resource selection task of the TREC 2013 Federated Web Search track: ranking a given set of search engines in response to an input query. Sampled search results are made available for each search engine (referred to as collections from now on). Building on prior research in federated search, we formulate two collection ranking strategies using a probabilistic retrieval framework based on language modeling techniques. According to one model (Collection-centric), each collection is represented as a term distribution, which is estimated from all sampled documents. Our second model (Document-centric) first ranks individual sampled documents, then aggregates their scores to determine collection relevance. We experimented with two type of representations for the sampled documents: snippets-only and full-text. Finally, we considered a linear combination of the Collection-centric and Document-centric methods.</p><p>The paper is organized as follows. We present our methods in Section 2. In Section 3 we report on our official runs and results, followed by a post-submission analysis in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We formulate the resource selection task in a generative probabilistic framework and rank collections based on their likelihood of containing documents relevant to an input query, P(c|q). Instead of estimating this probability directly, we apply Bayes' rule and rewrite it to P(c|q) ∝ P(q|c)P(c). Thus, the score of a collection is made up of two components: (1) query generator (P(q|c)), that is, the probability of a query being generated by collection c; this can be interpreted as the collection's relevance to the query; (2) collection prior (P(c)), that is, the a priori probability of se-lecting collection c; this tells us how likely the collection is to contain the answer to any arbitrary query. We draw upon our prior work for estimating these components <ref type="bibr" coords="1,532.68,208.91,18.59,8.64;1,316.81,220.86,78.09,8.64" target="#b1">(Neumayer et al., 2012)</ref>. Specifically, we consider two query generator models, representing two main families of collection selection strategies: lexicon-based collection selection and document-surrogate methods <ref type="bibr" coords="1,454.09,256.73,97.54,8.64" target="#b3">(Shokouhi and Si, 2011)</ref>. These two approaches also bear strong resemblance to the expert finding models (Model 1 vs. Model 2) by <ref type="bibr" coords="1,510.50,280.64,45.41,8.64;1,316.81,292.59,26.56,8.64">Balog et al. (2006)</ref> and to the blog feed search models (Large vs. Small Document Models) by <ref type="bibr" coords="1,412.60,304.55,73.94,8.64" target="#b0">Elsas et al. (2008)</ref>. Our collection prior is a simple one, based on collection size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Collection-centric Model</head><p>One of the simplest approaches to resource selection is to treat each collection as a single, large document <ref type="bibr" coords="1,526.58,378.03,29.33,8.64;1,316.81,389.98,45.97,8.64">(Callan et al., 1995;</ref><ref type="bibr" coords="1,364.73,389.98,54.99,8.64" target="#b5">Si et al., 2002)</ref>. Once such a pseudo-document is generated for each collection, we can rank collections much like documents. In a language modeling setting this ranking is based on the probability of the collection generating the query. Formally:</p><formula xml:id="formula_0" coords="1,324.74,459.87,218.89,25.38">P(q|c) = ∏ t∈q (1 -λ) ∑ d∈c P(t|d)P(d|c) + λP(t) n(t,q)</formula><p>, (1) where n(t, q) is the number of times term t is present in the query q, P(t|d) and P(t) are maximum-likelihood estimates of the probability of observing term t given the document and background language models, respectively, and λ is a smoothing parameter. The background language model is estimated form all sampled documents. We assume that all documents are equally important within a given collection, therefore, we set P(d|c) uniformly to 1/|c|, where |c| is the number of (sampled) documents in collection c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document-centric Model</head><p>Instead of creating a direct term-based representation of collections, our second approach models and queries individual sampled documents, then aggregates their relevance estimates:</p><formula xml:id="formula_1" coords="1,325.57,699.61,230.35,22.39">P(q|c) = ∑ d∈c P(d|c) ∏ t∈q (1 -λ)P(t|d) + λP(t) n(t,q) ,<label>(2)</label></formula><p>where, as before, P(t|d) and P(t) and the document and background term probabilities, λ is the smoothing parameter, and P(d|c) is the importance of the document given the collection. Additionally, we apply a rank-based cut-off and consider only the top N most relevant documents in the sample index for the computation of Eq. 2. This model resembles the ReDDE collection selection algorithm <ref type="bibr" coords="2,263.49,129.01,29.41,8.64;2,53.80,140.96,53.10,8.64" target="#b4">(Si and Callan, 2003)</ref>, but we incorporate collection size as a prior and not as part of the document score aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combination of Methods</head><p>The two strategies can be combined using a simple mixture model:</p><formula xml:id="formula_2" coords="2,94.80,233.17,198.11,9.72">P(q|c) = βP CC (q|c) + (1 -β)P DC (q|c),<label>(3)</label></formula><p>where P CC and P DC are estimated using Equations 1 and 2, respectively. For the sake of simplicity, we set β to 0.5 in our submitted runs, but we experiment with other settings in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Collection Priors</head><p>We use the number of sampled results as an approximation of collection size. Thus, we set collection priors as follows:</p><formula xml:id="formula_3" coords="2,142.27,370.72,146.76,23.95">P(c) = |c| ∑ c |c | . (<label>4</label></formula><formula xml:id="formula_4" coords="2,289.03,377.78,3.87,8.64">)</formula><p>3 Official Runs and Results</p><p>We considered two representations: snippet-only (S) and full-page (P). In both cases we indexed all the "visible" content. We applied only standard preprocessing steps. We submitted three runs, all of which were automatic. All runs employ collection priors. The smoothing parameter λ is set to 0.1.</p><p>UiSP Document-centric model based on the full page content. The relevance cut-off parameter N is set to 200. This value was chosen based on some preliminary experiments we performed on the FedWeb 2012 test collection <ref type="bibr" coords="2,103.33,565.96,83.14,8.64" target="#b2">(Nguyen et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UiSPP Linear combination of the Document-centric and</head><p>Collection-centric models.</p><p>The Document-centric model corresponds to the UiSP run; the Collectioncentric model uses a snippet-only representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UiSS Linear combination of the Document-centric and</head><p>Collection-centric models, where both use a snippetonly representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>This section reports on post-submission experiments we performed to analyze the effects of various parameter settings. First, we vary the value of the rank-based cut-off parameter for our best performing method, that is, the Documentcentric model based on full page content. Figure <ref type="figure" coords="2,515.49,304.35,4.98,8.64">1</ref> displays the NDCG@20 scores (mind the logarithmic scale on the xaxis). We observe that the plot has two peaks; the global maximum is reached as early as 10 retrieved documents (0.308), and there is a second peak that tops at N = 300 (0.285). We find N = 10 remarkable for two reasons. First, it suggests that one does not need to go beyond the first result page for effective resource selection (which also simplifies matters considerably, e.g., in terms of efficiency). Second, our document ranking method is admittedly a simple one. We hypothesize that using a more advanced document ranking approach would bring in further improvements.</p><p>Next, we compare the performance of these two settings, N = 10 vs. N = 300, on the level of individual topics. We can see on Figure <ref type="figure" coords="2,392.23,471.72,4.98,8.64">2</ref> that it is topic-dependent which of the two settings is better, i.e., some topics do benefit from considering more documents while others are hurt by that. One interesting question for further investigation is to understand why certain topics require more documents to be looked at and whether these topics can be identified automatically.</p><p>Finally, we wish to know whether the Collection-centric model can in principle add anything on top of the (best performing) Document-centric model. Therefore, we perform a sweep on the linear mixture parameter β (in Eq. 3) from 0 to 1 in 0.1 steps. The results are shown on Figure <ref type="figure" coords="2,548.44,591.27,3.74,8.64" target="#fig_1">3</ref>. The leftmost point on the plot (β = 0) corresponds to using the Document-centric model only, while the rightmost point (β = 1) corresponds to relying on the Collection-centric model only. It is clear that the combination always deteriorates the performance of the Document-centric model, no matter how low the weight on the Collection-centric model is. Note that the Collection-centric model uses a snippetonly representation. It is left to future work to test whether this finding holds if collection language models are estimated based on the full document content. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We described our participation in the TREC 2013 Federated Web Search track. Building on earlier work <ref type="bibr" coords="3,248.09,503.11,44.81,8.64;3,53.80,515.07,49.93,8.64" target="#b1">(Neumayer et al., 2012)</ref> we employed two different approaches based on language modeling techniques to the resource selection task. Initial results suggest that our Document-centric model provides a competitive baseline.</p><p>Understanding the impact of the underlying document ranking and applying rank-based cut-offs in a topic-specific manner offer interesting opportunities for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">References</head><p>Balog, K., <ref type="bibr" coords="3,98.30,642.91,156.87,8.64">Azzopardi, L., and de Rijke, M. (2006)</ref>. Formal models for expert finding in enterprise corpora. In Proceedings of SIGIR'06, pages 43-50.</p><p>Callan, J. P., <ref type="bibr" coords="3,110.05,686.91,131.17,8.64">Lu, Z., and Croft, W. B. (1995)</ref>. Searching distributed collections with inference networks. In Proceedings of SIGIR'95, pages 21-28. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,53.80,201.57,239.10,8.64;3,53.80,213.53,239.10,8.64;3,53.80,225.48,46.21,8.64;3,80.09,247.65,3.32,4.91"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: The effect of varying the rank-based cut-off parameter on the Document-centric model using full-page representation.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,330.65,201.57,211.43,8.76"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The effect of varying the mixture weight β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,674.96,239.10,44.50"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note coords="2,88.45,674.96,204.45,8.64;2,53.80,686.91,239.10,8.64;2,53.80,698.87,239.10,8.64;2,53.80,710.82,239.10,8.64"><p>displays the results. We find that the Documentcentric model performs best among our submitted runs. Combining it with the Collection-centric model does not bring in any further improvements (UiSP vs. UiSPP). It</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,316.81,64.07,239.10,120.66"><head>Table 1 :</head><label>1</label><figDesc>Results for our official resource selection runs.</figDesc><table coords="2,316.81,74.94,239.10,109.78"><row><cell>Run</cell><cell cols="4">CC DC NDCG@20 ERR@20</cell></row><row><cell>UiSP</cell><cell>-</cell><cell>P</cell><cell>0.276</cell><cell>0.020</cell></row><row><cell>UiSPP</cell><cell>S</cell><cell>P</cell><cell>0.274</cell><cell>0.020</cell></row><row><cell>UiSS</cell><cell>S</cell><cell>S</cell><cell>0.165</cell><cell>0.006</cell></row><row><cell cols="5">is also clear that the full-page representation performs sig-</cell></row><row><cell cols="5">nificantly better than the snippet-only one, at least for the</cell></row><row><cell cols="4">Document-centric model (UiSPP vs. UiSS).</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,316.81,231.71,239.10,8.64;3,326.78,243.67,229.14,8.64;3,326.78,255.45,210.09,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,366.35,243.67,189.57,8.64;3,326.78,255.62,23.94,8.64">Retrieval and feedback models for blog feed search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,369.09,255.45,98.23,8.59">Proceedings of SIGIR&apos;08</title>
		<meeting>SIGIR&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,316.81,275.55,239.10,8.64;3,326.78,287.33,229.14,8.82;3,326.78,299.28,104.79,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,531.57,275.55,24.35,8.64;3,326.78,287.50,151.34,8.64">Ranking distributed knowledge repositories</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Neumayer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,496.31,287.33,59.61,8.59;3,326.78,299.28,34.79,8.59">Proceedings of TPDL&apos;12</title>
		<meeting>TPDL&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="486" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,316.81,319.38,239.10,8.64;3,326.78,331.34,229.14,8.64;3,326.78,343.12,229.14,8.82;3,326.78,355.07,128.40,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,374.91,331.34,181.00,8.64;3,326.78,343.30,159.37,8.64">Federated search in the wild: The combined power of over a hundred search engines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,506.56,343.12,49.35,8.59;3,326.78,355.07,49.47,8.59">Proceedings of CIKM &apos;12</title>
		<meeting>CIKM &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1874" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,316.81,375.00,239.10,8.82;3,326.78,386.95,205.39,8.82" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<title level="m" coord="3,448.79,375.00,107.13,8.82;3,326.78,386.95,163.86,8.59">Federated search. Foundations and Trends in Information Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,316.81,407.06,239.10,8.64;3,326.78,418.83,229.14,8.82;3,326.78,430.79,115.93,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,430.89,407.06,125.02,8.64;3,326.78,419.01,161.22,8.64">Relevant document distribution estimation method for resource selection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,506.56,418.83,49.35,8.59;3,326.78,430.79,46.39,8.59">Proceedings of SIGIR&apos;03</title>
		<meeting>SIGIR&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,316.81,450.89,239.10,8.64;3,326.78,462.85,229.14,8.64;3,326.78,474.62,229.14,8.82;3,326.78,486.76,17.43,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,529.38,450.89,26.53,8.64;3,326.78,462.85,229.14,8.64;3,326.78,474.80,52.70,8.64">A language modeling framework for resource selection and results merging</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,401.90,474.62,99.40,8.59">Proceedings of CIKM&apos;02</title>
		<meeting>CIKM&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="391" to="397" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
