<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.95,105.61,312.10,24.25">TREC Complex Answer Retrieval Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,169.97,142.45,61.11,13.65;1,231.08,142.91,1.41,6.99"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
							<email>dietz@cs.unh.edu</email>
						</author>
						<author>
							<persName coords="1,238.23,142.45,59.64,13.65"><forename type="first">Ben</forename><surname>Gamari</surname></persName>
						</author>
						<author>
							<persName coords="1,307.71,142.45,54.68,13.65"><forename type="first">Je</forename><surname>Dalton</surname></persName>
						</author>
						<author>
							<persName coords="1,371.76,142.45,70.28,13.65"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
						</author>
						<title level="a" type="main" coord="1,149.95,105.61,312.10,24.25">TREC Complex Answer Retrieval Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B325B636DCEA8B23F5A4DA55C2353417</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This notebook gives an overview of activities, datasets, and results of the second year of TREC Complex Answer Retrieval. We lay out the tasks oered and how provided datasets are automatically derived from Wikipedia and TQA. Manual relevance assessments are created by NIST. We describe the details of the assessment procedures, inter-annotator agreement, and statistics. Nine teams submitted runs exploring interactions of entities and passages, neural as well as traditional retrieval methods. We see that combining traditional methods with learning-to-rank can outperform neural methods, even when many training queries are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating information objects, sub-document retrieval, answer aggregation, retrieving material for conversational agents are important challenges raised in the SWIRL 2012 and 2018 workshops on frontiers, challenges, and opportunities for information retrieval <ref type="bibr" coords="1,317.46,381.20,10.50,6.64" target="#b0">[1,</ref><ref type="bibr" coords="1,332.41,381.20,7.01,6.64" target="#b2">3]</ref>. These challenges share several commonalities: We desire answers to complex information needs, and wish to nd them in a single and well-organized page. Such a page may not yet exist, therefore it needs to be synthesized from multiple information sources.</p><p>Advancing the state of the art in this area is the goal of this TREC Complex Answer Retrieval track.</p><p>Algorithms that can automatically author a complex answer in response to a query would benet users that investigate new and unfamiliar topics. They would impriove information access in mobile environments with restricted interaction capabilities. In contrast to extensive work on nding the best short answer, the CAR track focuses on the retrieval of longer answersespecially answers that cover a range of dierent subtopics. We envision answers to be composed of multiple text fragments from multiple sources, recycling information about related topics, but selected to highlight insightful connections.</p><p>Retrieving high-quality comprehensive answers is challenging as it is not sucient to choose a lower rank-cuto with the same techniques as for short answers. Instead, we need new approaches for nding and organizing relevant information units of a complex answer space. Many examples of manually created complex answers exist on the Web: howstuffworks.com, travel guides, fanzines, or educational text books. These are collections of articles where each article constitutes a long answer to an information need expressed by the title of the article. Ideally, by reading an article, users will gain new information about the topic. We can measure this information gain by testing how well the text enables users to answer questions about the topic.</p><p>The fundamental task of collecting references, facts, and opinions into a single point-of-entry has traditionally been a manual process. We envision that automated information retrieval systems can relieve users from a large amount of manual work though sub-document retrieval, consolidation and organization.</p><p>Ultimately, the goal is to retrieve synthesized information rather than documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Worked Example</head><p>To motivate a brief example, consider a user wondering about how coee preparation techniques lead to dierent tastes. With this intention in mind, she enters the query Coee preparation. A possible answer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Passage Task: Given an outline for complex topic Q, retrieve for each of its sections H i , a ranking of relevant passages S.</p><p>Entity Task: Given outline for complex topic Q, retrieve for each of its sections H i , a ranking of relevant entities E. Each entity is to be supported with a passage S that motivates the why the entity is relevant for the query.</p><p>The passage S is taken from the provided passage corpus. The entity E refers to an entry in the provided knowledge base. We dene a passage or entity as relevant if the passage content or entity is appropriate to be mentioned an article about the topic Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TREC CAR Data Set (v2.1)</head><p>The 2018 Complex Answer Retrieval track uses topics and outlines that are extracted from English Wikipedia (XML dump from June 2018) and the Text Book Question Answering (TQA) dataset 2 . We refer to these subsets as Wiki-18 and TQA in the following. Paragraphs, entities and training data is extracted from previous years's English Wikipedia (XML dump from Dec 20th, 2016; referred to as Wiki-16). Wikipedia articles are split into the outline of sections and the contained paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUST be mentioned:</head><p>Many water-saving devices (such as low-ush toilets) that are useful in homes can also be useful for business water saving.</p><p>Other water-saving technology for businesses includes: CAN be mentioned:</p><p>Recycling one gallon of paint could save 13 gallons of water, 1 quart of oil, and 250,000 gallons of water pollution, 13.74 pounds of , save enough energy to power the average home for 3 hours, or cook 6 meals in a microwave oven, or blow dry someone's hair 27 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roughly on TOPIC but non-relevant:</head><p>Dual piping is a system of plumbing installations used to supply both potable and reclaimed water to a home or business.</p><p>Under this system, two completely separate water piping systems are used to deliver water to the user. This system prevents mixing of the two water supplies, which is undesirable, since reclaimed water is usually not intended for human consumption.</p><p>Paragraph IDs: left: dbcef592762b4711012041f6bdf1bd7cb5a521 center: f26730da3b7860c727411480b08ae6466dcc9a54 right: 21e6e381383e392cb7d1432200c51c095cdf3fbe All paragraphs from all articles are gathered and deduplicated to form the paragraph corpus. Due to a bug x in the Wikipedia parser, the paragraph collection for Y2 is larger and cleaner than in Y1.</p><p>Each section outline, such as depicted in Figure <ref type="figure" coords="3,72.00,399.99,3.87,6.64">1</ref>, is a description of a complex topic. By keeping the information which paragraph originates from which article and section, we have a means of providing (automatic) training data for the passage retrieval task. By preserving hyperlinks inside paragraphs that point to other Wikipedia pages (also known as entity links), we have a means of providing training data for the entity retrieval task.</p><p>Figure <ref type="figure" coords="3,118.48,495.63,4.98,6.64">3</ref> depicts the set of Wiki-18 pages selected for the benchY2test set. This allows us to study how well Wikipedia content can be recycled to populate articles on new and unseen topics. With the release of allButBenchmark, all Wiki-16 pages are made available. However, taking paragraphs and outlines from dierent collections poses signicant challenges. Only a small subset of these Wiki-18 pages in contained paragraphs available in the provided paragraph collection (derived from Wiki-16). An unfortunate consequence is that the automatic passage evaluation option used Y1 (Section 4.2) cannot be applied to passage runs in this year's evaluation.</p><p>After ltering and processing procedures described in Section 4.1, several datasets for training and evaluation are derived. The size of the datasets is given in Table <ref type="table" coords="3,331.98,591.27,3.87,6.64" target="#tab_1">1</ref>. The paragraph collection contains 29,678,367 unique paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Set Creation Pipeline</head><p>The TREC Complex Answer Retrieval benchmark (v2.1) is derived from Wikipedia so that complex topics are chosen from articles on open information needs, i.e., not people, not organizations, not events, etc.</p><p>However, any paragraph or entity on Wikipedia is a legal paragraph/entity for the retrieval task even if a person entity or a paragraph from an article on an event. The data set creation process (similar to the v1.5 data) is as follows: 5. Sections with headings that do not contain prose are discarded, for example external links, references, bibliography, notes, gallery etc.</p><p>6. Each article is separated into (1) the outline of section headings and (2) paragraphs.</p><p>7. The set of paragraphs across all of Wiki-16 are collected, and unique paragraph IDs are derived through SHA256 hashes on the text content (ignoring links).</p><p>8. The paragraphs are further deduplicated with min hashing using word embedding vectors provided by GloVe (with 50 dimensions). For each set of duplicates, one representative paragraph is chosen. 9. The collection representative paragraphs is released as the paragraphCorpus.</p><p>10. Articles are rewritten, replacing paragraphs that have duplicates with the representative paragraph.</p><p>11. The set of articles is further ltered to remove images, sections with very long (&gt;100 characters), and very short headings (&lt;3 letters). Articles with less than three remaining sections are discarded. 13. The ve folds of the training data, with separated outlines and paragraphs and extracted automatic qrels are made available as train. 14. The set of pages used for benchmarks used in Y1 (benchY1train, benchY1test, test200) were re-processed with the new Wikipedia parser, manual judgments were translated to new paragraph IDs, the re-released.</p><p>15. A manual selection of articles in Wiki-18 and the TQA corpus were only released as outlines as benchY2test.public. Ocial contributed runs were submitted on these topics. A complete bench-markY2test is released after the TREC workshop. 3 It contains:</p><p>• Manual ground truth for paragraphs (qrels)</p><p>• Automatic and manual ground truth for entities (qrels)</p><p>• Original articles</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Ground Truth</head><p>Two kinds of ground truth signals are collected: automatic and manual. For each, we release true paragraphs and true entities. While the manual ground truth is assessed after participants submit runs, the automatic ground truth is derived along with the dataset from the Wikipedia/TQA dump.</p><p>For the benchmarkY1 datasets, the automatic ground truth is derived as follows</p><p>• If a paragraph is contained in the page/section it is dened as relevant, and non-relevant otherwise.</p><p>• If the page/section contains an entity link, then the (link target) entity it is dened as relevant, and non-relevant otherwise. For benchmarkY1train, benchmarkY1test, and benchmarkY2test, the TagMe <ref type="bibr" coords="5,96.91,534.32,10.51,6.64" target="#b4">[5]</ref> entity linker was used. For the much larger train collection, entity links that were manually added by Wikipedia editors were used. Qrels are derived with several dierent levels:</p><p>• Hierarchical: Only content of leaf sections in considered.</p><p>• Article: All content of the page is considered (independent of the section). However, due to a mistake in processing, the lead text is missing.</p><p>• Top-level: Only relevance for top-levels sections is provided. All content in this section or a child section is considered relevant.</p><p>• Tree: For all page titles and headings, all content in the subtree is considered relevant. Tree qrels also contain article-level relevance assessments, although these are not part of the TREC CAR evaluation. The benchmarkY2 dataset is constructed from pages outside the Wiki-16 dump (depicted in Figure <ref type="figure" coords="6,528.38,76.98,3.87,6.64">3</ref>).</p><p>Only a small fraction of paragraphs on Wiki-18 pages already existed in Wiki-16 on a page with a dierent name. The paragraph sets from TQA and Wiki-16 are disjoint. Thus, the automatic evaluation procedure for paragraphs, used in Y1, is not applicable to the Y2 dataset.</p><p>The automatic entity ground truth is available on benchmarkY2test as it is constructed from entity links and does not rely on an overlap in paragraphs.</p><p>An overview for which benchmarks which kind of ground truth is available is given in Table <ref type="table" coords="6,491.19,148.71,3.87,6.64" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submission</head><p>For the passage ranking task, participants were asked to submit a ranking of paragraph IDs per heading in the outlines of benchY2test. For the entity ranking task, participants were asked to submit a ranking of entity IDs per heading in the outlines of benchY2test. To support assessing entity relevance, participants were asked to provide provenance to annotate each entity ID with the ID of a paragraph that explains why the entity is relevant for the corresponding section heading.</p><p>Participants were allowed to consider all headings in the outline at once, use external resources such as knowledge graphs, entity linking tools, pre-trained word embeddings, and any of the provided TREC CAR data sets. The participants were not allowed to directly use a dump of Wikipedia, as this would allow them to look up the paragraphs on the pagethe information used in the automatic ground truth.</p><p>Each participating team was allowed to submit up to three runs to the passage task and three runs to the entity task. Nine teams participated in this second year of the track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Assessment of the Manual Ground Truth</head><p>For each heading of query outlines, the top ve paragraph IDs or entity IDs of participant-contributed runs were merged to build the assessment pools. Additionally, paragraphs and entities that are relevant according to the automatic ground truth were added to the pool for verication. In the previous year, complex topics were only partially judged (to obtain a larger topical variety). In contrast, this year, all sections of complex topics were assessed. This yielded manual assessments for 65 complex topics (31 TQA and 34 Wiki-18). One section was annotated by all assessors in order to measure inter-annotator agreement across the six NIST assessors (see Section 6.2).</p><p>For the passage task, the assessor is presented with the complex topic (page title) and the headings in the outline, followed by a randomized list of paragraphs from the assessment pool. In the case of the entity task, the list displayed the canonical entity names together with the provenance paragraph if given. As not all participants submitted provenance, the list also displays an entry of the canonical entity name together with rst paragraph from the entity's Wikipedia pages as provenance. This information was intended to support the assessment process. However, the rst paragraph of the entity's Wikipedia page turned out to be generally not relevant. As only one team submitted provenance, assessors has no choice by to resort to world-knowledge.</p><p>Assessors were asked to envision writing a Wikipedia article on the given complex topic. A graded assessment scale was used based on how importantly the paragraph/entity should be mentioned in this section of the article, using grades as follows.</p><p>• MUST be mentioned The grade histogram per annotator and the overall grade distribution is given in Table <ref type="table" coords="7,471.12,401.16,4.98,6.64" target="#tab_4">4</ref> (discrepancies due to merging and cleaning). We notice that only a third of all assessments are graded as relevant, while an additional third were annotated as being on topic.</p><p>72% of passages (62% of entities) in the assessment pool were marked as non-relevant. This demonstrates that the task is feasible, but challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Inter-annotator agreement</head><p>One section (both passage and entity) was selected for annotation by all assessors to measure inter-annotator agreement in the middle of the assessment period.</p><p>We measure inter-annotator agreement using Cohen's κ for pairwise comparison and Fleiss' κ across all annotators. We analyze agreement on the derived binarized assessment and graded assessment. Subtle dierences between neighboring grades are often not reliable. Therefore, we consider the case of graded  MUST, are also counted as agreements for both p 0 and p e . We call this graded evaluation o by one, for which results are displayed in in Table <ref type="table" coords="8,242.93,357.21,3.87,6.64" target="#tab_6">6</ref>.</p><p>Inspecting Cohen's κ, we nd that pair-wise agreement is relatively similar across all pairs of assessors.</p><p>In other words, there is no odd one out which speaks to the quality of NIST's assessment procedures. As expected, the agreement for binarized paragraph judgments (Fleiss κ = 0.500) is higher than for graded judgments (Fleiss κ = 0.304). For entity judgments, we nd that Annotator 2 was an outlier, with a disproportionate high number of non-relevant assessments. After removing assessment from Annotator 2, binarized agreement is Fleiss κ = 0.416, and graded Fleiss κ = 0.374. This may sound small, yet it is comparable to previous work <ref type="bibr" coords="8,200.18,440.90,9.96,6.64" target="#b1">[2]</ref>. However, once neighboring grades are counted as agreement (o by one), the inter-annotator agreement is even higher agreement than on binarized assessments.</p><p>We conclude that, aside from subtle nuances in the grading scale, assessors agree on the whether the passage or entity should be included in the article on the complex topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Annotation Time</head><p>each, yielding a total of 240 hours. Including breaks and training, the average annotation time per passage or entity judgments depends on the annotator and ranges between 17 and 64 seconds (median: 30 seconds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Interaction between Manual and Automatic Assessments</head><p>Figure <ref type="figure" coords="8,104.74,581.26,4.98,6.64" target="#fig_1">4</ref> depicts dierences between the sets of relevant passages and entities according to manual and automatic relevant data. Diculties arise since the test queries in benchY2test are taken from Wiki-18, but the provided paragraph corpus and legal set of entities (allButBenchmark) are derived from Wiki-16. While automatic entity assessments can be extended by applying an entity linking tool (Figure <ref type="figure" coords="8,468.81,617.13,3.87,6.64" target="#fig_1">4</ref>, bottom, left), the paragraphs cannot be automatically aligned. (We experimented to re-align paragraphs using ROUGE, but were not convinced by the results.)</p><p>We asked assessors to annotate paragraphs (and entities) from the original page in addition to participant contributed paragraphs (and entities). See Figure <ref type="figure" coords="8,297.27,664.95,3.87,6.64" target="#fig_1">4</ref>, top. The motivation is to display positive examples to the assessors to retain high standards even when contributed runs were not containing relevant entries.</p><p>Only paragraphs (and entities) that were both manually assessed and contained in the paragraph corpus (and legal set of entities) were used in the evaluation.  <ref type="table" coords="9,149.38,397.80,4.98,6.64">7</ref> gives an overview over the kinds of methods contributed by participating teams. Below, detailed descriptions of submitted runs provided by teams:</p><p>• The guir team submitted two passage retrieval runs. The rst run (guir) is an approach based o prior work <ref type="bibr" coords="9,120.77,441.64,9.96,6.64" target="#b5">[6]</ref>, in which an ad-hoc neural ranking architecture is modied for the CAR task by incorporating heading frequency statistics from the training data and by incorporating separate matching phases for each heading in the query. The second run (guir-exp) uses the same approach, and adds the topscoring query expansion terms for each heading as determined by a learned match-gating mechanism.</p><p>The models were trained on the automatic relevance judgments from the train dataset, and validated using the manual relevance judgments on the benchmarkY1-test dataset.</p><p>• NYU: Lucene is the underlying retrieval engine. We train 20 query reformulators on random disjoint partitions of the training set as in Nogueira et. al., 2018 <ref type="bibr" coords="9,337.76,533.29,9.96,6.64" target="#b6">[7]</ref>. For each query, each reformulator produces a list of ranked documents. We re-rank the union of these 20 lists using a simple ranking model that scores each query-document pair using a feed-forward neural network whose input is the concatenation of the average word embeddings of the query and document. To further improve the performance of the system, we train an ensemble of 9 ranking models whose network architectures are randomly chosen. For each query, we re-rank the union of the 9 lists produced by the 9 ranking models using the best ranking model in the ensemble. All the models are trained on the rst 4 folds of TREC-CAR v2.1 queries and the last fold is used for dev/hyperparameter tuning. Y1 test queries are used as test queries. We use paragraphCorpus v2.0 to retrieve documents for train, dev, and test queries.</p><p>• CG: We developed a deep learning model for information retrieval TREC Complex Answer Retrieval (CAR) task. We used an attention based sequence-to-sequence model to rst translate content passages to outlines. Then across all extracted outlines, use sentence embedding to rank outlines based on the available query. We used attention based bidirectional LSTM model for encoder and decoder layers.</p><p>In order to capture rare words while limiting our dictionary size, we used Byte Pair Encoding subword units to tokenize sentences. The main advantage of using seq2seq model is to preform main inference bigrams and concepts induced by query terms from dierent query sections are considered. Besides, the system incorporates the Wikipedia article information or query entity mentions based on a Dirichlet prior smoothed language model. The run CUIS-F150 incorporates the Wikipedia article information.</p><p>The run "CUIS-MX5" incorporates both Wikipedia article and query entity mentions. The system is trained on benchmarkY1-train v2.0 dataset. The entity runs are derived by replacing the paragraph id with the containing Wikipedia article id.</p><p>• UMass (entityEmbedLambdaMart): For the lambdamart run we use the benchmarkY1-train.v2.0 as the training set, benchmarkY1-test-public.v2.0 as the validation set and benchmarkY2test-public.v2.1.1 evaluation topics for the submitted run. In this run, we learned a joint entity-word embedding representation based on the Wikipedia corpus. In TREC CAR topics, each query topic consists of three subtopics: Root subtopic (R), Intermediate subtopic (I) and leaf subtopic (H). We retrieve a set of documents with three baseline methods: SDM (Sequential Dependence Model), RM3 and query likelihood with the subtopic combinations of R-H, R-I-H and R-I-H, respectively. Furthermore, We represent each document and query based on their entity embeddings. Each query has ne-grained subtopic wordvector representations as well as the complete topic representation. To be more specic, we represent an entity by the average embedding vectors of entities only in Root, entities only in Leaf (H), and all of the entities in the topic. We use cosine similarity between the document vector representation and each query representation as a feature in LambdaMart learning-to-rank model as well as the retrieval scores from the base retrieval model.</p><p>• TREMA-UNH (UNH): We provided three passage runs and three entity runs that were all based on combination of low-level input runs such as BM25, Query likelihood, SDM, RM3 and Entity Context model <ref type="bibr" coords="11,127.73,343.98,10.50,6.64" target="#b3">[4]</ref> with combinations trained with coordinate ascent Learning-to-Rank optimized for MAP.</p><p>We also experimented with a new Learning-to-Walk methods for supervised graph walks. Our best performing passage run is a combination of BM25, Query Likelihood with and without RM3. All combinations were trained on benchmarkY1train, and the best three methods were selected on bench-markY1test. In our notebook also includes evaluation results on Y1 benchmarks.</p><p>• DWS-UMA: Our Trec-CAR submission is a simple unsupervised method. At query time we perform semantic query expansion in combination with term specity boosting on a Lucene Index. Our model rst represents the query by its lemmatized query terms. In the next step, for each term, the query is expanded by including the top k nearest neighbors from a semantic word embedding space. The expanded query is executed against a Lucene Index with BM25. Query terms at lower levels in the outline, i.e., more specic query terms, are boosted and receive a higher weight. For our submission we used a pre-trained embedding space and the value for k is tuned on benchmarkY1-train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Assessment Interface and Fixed Mistakes in Submitted Runs</head><p>The assessment interface was populated by a) pooling the top 5 of all submitted runs and b) paragraphs and entities on the original article (i.e., the automatic ground truth).</p><p>Several submitted runs contained mistakes. We xed those mistakes post-hoc, but we were unable to do so before the assessment. The following issues were xed post-hoc</p><p>• Team NYU assigned all ranked items the score of 1.0, all information was contained in the rank information. We derived corrected runs turning rank information into scores. As a result random 5 elements of their ranking were assessed.</p><p>• Several teams submitted illegal entity IDs. Despite recommended otherwise, participants created entity IDs from page names by replaceing spaces with %20. This will not address non-ASCII characters such as accents or umlauts. We derived corrected runs post-hoc. As a result, for aected runs entities containing accents or umlauts were not assessed.</p><p>• Team CG submitted rankings that only contained three passages. As a result, fewer elements were assessed for CG than other teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>The ocial evaluation 4 of participant-contributed is conducted with respect to four standard TREC evalua- tion measures, R-Precision (RPrec), Mean-average Precision (MAP), Reciprocal Rank (MRR), and Normalize Discounted Cumulative Gain (NDCG). Of these measures only NDCG considers the graded scale, for all other methods the positive/negative cuto indicated in Table <ref type="table" coords="12,343.28,134.67,4.98,6.64" target="#tab_3">3</ref> are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Passage Task</head><p>We evaluate participant-contributed passage runs on manual assessments, since automatic assessments are not available. As described in Section 6.4, while the manual assessments included paragraphs within and outside the paragraph collection, we evaluate participating runs only on passages that are included in the paragraph corpus.</p><p>Results for the benchY2test passage retrieval task are presented in Figure <ref type="figure" coords="12,410.53,228.64,4.98,6.64">5</ref> on the manual graded scale, and a lenient variant of the manual graded scale. Standard error bars and paired-t-test with respect to the best performing method are given for reference. All analyses across all measures are painting the same picture. Acknowledging consistent patterns in the results for dierent metrics, here the ranking of passage methods by across-the-board performance (tied methods on the same rank):</p><p>1. uog-headingrh-sdm, UNH-p-l2r</p><p>2. uog-linear-lrt-hier, UNH-p-sdm, UNH-p-mixed 3. entityEmbedLambdaMart (UMass), guir-exp, UTDHLTRI2</p><p>4. guir, uog-linear-raw-expansion, NYU-XL-f, CUIS-MX5 5. NYU-L-f, NYU-M-f, CUIS-F150 6. DWS-UMASemQueryExp, DWS-UMASemQueryExp20, DWS-UMASemQueryExp30</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CG-Seq2Seq</head><p>To study whether the dierences are due to better performance on easy queries, dicult queries, or overall, we include divide the set of all annotated topic sections into percentiles ranging from easy to dicult according to the best performing method. The results are presented in Figure <ref type="figure" coords="12,431.63,457.11,4.98,6.64">9</ref> and show a consistent ranking of methods for dicult and easy queries (interquartile ranges 25%-50%, 50%-75%, and 75%-95%).</p><p>Separating results for queries originating from Wiki-18 and TQA (Figures <ref type="figure" coords="12,409.54,481.02,10.50,6.64">5d</ref> and<ref type="figure" coords="12,442.02,481.02,8.02,6.64">5e</ref>), demonstrates that methods performing well on Wiki-18, also perform well on TQA. However, the dierence between methods is less pronounced for the Wiki-18 subset. TQA queries seem to be slightly easier, which is probably because the TQA outlines contain fewer sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Entity Task</head><p>We evaluate participant-contributed entity runs on automatic and manual assessments. While the manual assessments included entities within and outside the legal entity set (allButBenchmark), we evaluate participating runs only on entities that are included in legal set (cf. Section 6.4) We further evaluate on the automatic benchmark derived with TagMe entity links on ground truth pages. (The previous method of using only entity links included manually by Wikipedia editors does not apply to TQA articles.)</p><p>Results for the benchY2test entity retrieval task are presented in Figure <ref type="figure" coords="12,408.17,622.82,4.98,6.64">6</ref>    <ref type="figure" coords="15,104.00,258.34,3.87,6.64">7</ref>: Automatic evaluation using entity links from TagMe versus evaluation using links in Wikipedia source (Auto-Orig, only available for Wiki-18). The red arrow markes systems for which no signicant dierence to the best system could be detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CUIS-dodgeDodge, CUIS-Swift, CUIS-XTS</head><p>Comparing results under the automatic entity benchmark (using TagMe) with the automatic entity benchmark derived from entity links included by Wikipedia editors (Auto-Orig, provided as training data for the train benchmark), we see a similar pattern emerging, but some systems swap ranks.</p><p>When analyzing queries originating from Wiki-18 and TQA presented in Figure <ref type="figure" coords="15,437.02,381.48,3.87,6.64">8</ref>, we observe less sharp distinctions on Wiki-18 (with two or three systems tied for rank 1), where on TQA, the method UNH-e-L2R is consistently leading with signicant dierence to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In contrast to the previous year of TREC CAR, where neural network methods were dominating the leaderboard, in this second year we see that learning-to-rank with unsupervised retrieval models such as BM25, SDM and query expansion signicantly outperformed state-of-the-art neural methods. We see that systems that perform well in passage retrieval tasks, are also performing well in the entity retrieval task. Regarding benchmark construction for this project. In previous year, we conrmed that automatically derived relevance data for passages agrees with human-created benchmarks on the ranking of systems. In this year we further show that automatically derived relevance data for entities agrees with human-created benchmarks. This means, that we have an eective test bed for method development in TREC CAR, when the outline is provided. In the next year of TREC CAR, we will move beyond population of existing outlines and leave it to participants to also identify a suitable ordering of paragraphs to automatically populate a complete article, given only a suitable title.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,294.08,468.00,6.64;3,72.00,306.04,359.40,6.64"><head>Figure 2 :Figure 1 :</head><label>21</label><figDesc>Figure 2: Example passages and relevance for Protecting the Water Supply / Saving Water at Home (Query ID tqa:protecting%20the%20water%20supply/Saving%20Water%20at%20Home).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,72.00,270.78,468.00,6.64;9,72.00,282.73,468.00,6.64;9,72.00,294.69,468.01,6.64;9,72.00,306.64,468.00,6.64;9,72.00,318.60,45.48,6.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Manual (top) versus automatic (bottom) benchmarks for passages (left) and entities (right). The black circle marks the paragraph/entity collection, the blue area the gold articles, and the red circle markes the set of assessed paragraphs/entities. The light pink are depicts entities determined as relevant via TagMe entity links. The automatic passage collection has too few shared entries and is therefore not included in this study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,416.75,622.82,123.24,6.64;12,72.00,634.78,468.00,6.64;12,72.00,646.73,468.00,6.64;12,72.00,658.69,410.91,6.64;12,84.18,678.08,243.80,6.64;12,84.18,697.74,455.82,6.64;12,83.09,711.71,243.68,10.76;15,204.84,155.77,9.84,28.38;15,213.28,155.78,9.84,34.66;15,221.72,155.76,9.84,54.97;15,230.17,155.77,9.84,33.79;15,238.61,155.76,9.84,56.25;15,247.05,155.76,18.28,63.38;15,263.94,155.77,9.84,73.90;15,272.38,155.77,18.28,27.03;15,289.27,155.78,9.84,44.13;15,193.63,148.08,11.58,9.84;15,193.63,130.45,11.58,9.84;15,193.63,112.81,11.58,9.84;15,193.63,95.18,11.58,9.84;15,183.66,111.79,9.84,15.11;15,193.99,234.75,103.70,9.88;15,325.16,155.78,9.84,34.66;15,333.60,155.76,9.84,54.97;15,342.04,155.77,9.84,28.38;15,350.49,155.77,9.84,33.79;15,358.93,155.76,9.84,56.25;15,367.37,155.76,9.84,63.38;15,375.82,155.78,9.84,60.45;15,384.26,155.78,9.84,44.13;15,392.70,155.77,18.28,27.03;15,409.59,155.77,9.84,73.90;15,313.95,148.08,11.58,9.84;15,313.95,135.93,11.58,9.84;15,313.95,123.78,11.58,9.84;15,313.95,111.63,11.58,9.84;15,313.95,99.47,11.58,9.84;15,313.95,87.32,11.58,9.84;15,303.98,111.79,9.84,15.11;15,314.78,234.75,102.77,9.88"><head></head><label></label><figDesc>on the manual graded scale, lenient variant of the manual graded scale, and automatic assessment based on TagMe entity links, including error bars and paired-t-tests. Once more, all analyses across dierent measures are providing a coherent picture, resulting in the following ranking of entity methods (tied methods on the same rank):1. UNH-e-L2R, UNH-e-mixed, UNH-e-graph, uog-rf-ent 2. uog-linear-ltr-hier-ent, uog-heading-rh-sdm-ent, DWS-UMA-AspQLrm, DWS-UMA-EntAspBM25none4 Ocial results available http://trec-car.cs.unh.edu/results/ UNH-e-L2RUNH-e-mixed uog-paragraph-rf-ent UNH-e-graph uog-linear-ltr-hier-ent DWS-UMA-EntAspQLrm uog-heading-rh-sdm-ent DWS-Entity Automatic RprecUNH-e-mixed uog-paragraph-rf-ent UNH-e-L2R UNH-e-graph uog-linear-ltr-hier-ent uog-heading-rh-sdm-ent DWS-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="15,72.00,258.34,468.00,6.64"><head>Figure</head><label></label><figDesc>Figure 7: Automatic evaluation using entity links from TagMe versus evaluation using links in Wikipedia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,583.53,468.00,108.87"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,72.00,583.53,468.00,108.87"><row><cell cols="6">: Data set sizes in terms of articles, section, and automatic positive assessments. (**) used in this</cell></row><row><cell>years' evaluation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>benchY1train</cell><cell>benchY1test</cell><cell>train</cell><cell cols="2">benchY2test **</cell></row><row><cell></cell><cell>auto</cell><cell>manual</cell><cell>auto</cell><cell>manual</cell><cell>auto</cell></row><row><cell>number of articles (complex topics)</cell><cell>117</cell><cell>133</cell><cell>285,924</cell><cell>27</cell><cell>65</cell></row><row><cell>hierarchical sections (queries)</cell><cell>1,816</cell><cell>2,125</cell><cell>2,180,868</cell><cell>269 / 271</cell><cell>976</cell></row><row><cell>total positive paragraphs assessments</cell><cell>4,530</cell><cell>5,820</cell><cell>5,276,624</cell><cell>3,788</cell><cell>N/A</cell></row><row><cell>total positive entity assessments</cell><cell>13,031</cell><cell>15,085</cell><cell>12,310,616</cell><cell>3,173</cell><cell>17,044</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,79.20,180.13,460.80,64.03"><head>Table 2 :</head><label>2</label><figDesc>Availability of manual and automatic assessments for dierent benchmarks.</figDesc><table coords="5,79.20,213.61,460.80,30.55"><row><cell>12. The set of Wiki-16 articles is split into training and holdout data, training data is further split into</cell></row><row><cell>ve folds. To ensure uniform distribution and reproducibility, these decisions are made based on the</cell></row><row><cell>SipHash of the article title.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,86.94,611.87,224.30,60.95"><head>Table 3 :</head><label>3</label><figDesc>• SHOULD be mentioned • CAN be mentioned • Non-relevant, but roughly on TOPIC of the page • NO, non-relevant • Trash Assessment scale for manual assessments. Horizontal line: Cuto for positive/negative assessments.</figDesc><table coords="7,122.43,98.81,367.14,97.08"><row><cell></cell><cell cols="3">binary scale manual scale manual lenient scale</cell></row><row><cell>MUST be mentioned</cell><cell>1</cell><cell>3</cell><cell>5</cell></row><row><cell>SHOULD be mentioned</cell><cell>1</cell><cell>2</cell><cell>4</cell></row><row><cell>CAN be mentioned</cell><cell>1</cell><cell>1</cell><cell>3</cell></row><row><cell cols="2">Non-relevant, but roughly on TOPIC 0</cell><cell>0</cell><cell>2</cell></row><row><cell>NO, non-relevant</cell><cell>0</cell><cell>-1</cell><cell>0</cell></row><row><cell>Trash</cell><cell>0</cell><cell>-2</cell><cell>-2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,224.61,468.00,171.24"><head>Table 4 :</head><label>4</label><figDesc>Manual assessment: Grade histogram and distribution across both passage and entity task.</figDesc><table coords="7,72.00,236.85,414.39,130.92"><row><cell></cell><cell cols="7">annotator1 annotator2 annotator3 annotator4 annotator5 annotator6 Total %</cell></row><row><cell>Trash No Topic</cell><cell>40 1213 439</cell><cell>118 980 674</cell><cell>0 810 613</cell><cell>14 1124 361</cell><cell>5 801 770</cell><cell>2 1066 946</cell><cell>1 42 27</cell></row><row><cell>Can Should Must</cell><cell>241 210 145</cell><cell>489 304 214</cell><cell>469 131 65</cell><cell>261 305 288</cell><cell>212 140 191</cell><cell>115 472 51</cell><cell>13 11 7</cell></row><row><cell cols="2">6.1 Label distribution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="7,72.00,377.25,468.00,6.64;7,72.00,389.20,315.57,6.64"><p>Six assessors created 13,310 passage annotations on a total of 269 topic sections for passages. For 271 topics sections, entity assessments were created with 8415 assessments in total.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,111.96,583.27,388.09,116.30"><head>Table 5 :</head><label>5</label><figDesc>Statistics for anual assessments after merging and cleaning (as used for results).</figDesc><table coords="7,166.87,605.99,278.27,93.58"><row><cell></cell><cell>Passage</cell><cell>Entity</cell></row><row><cell>Trash No Topic Can Should Must</cell><cell>Total% TQA Wiki-18 1 25 131 43 2325 3364 28 1601 2076 12 1211 445 10 1009 374 6 400 349</cell><cell>Total% TQA Wiki-18 0 1 2 25 642 1480 37 1215 1902 13 602 450 13 630 458 12 585 448</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,72.00,73.56,468.00,266.37"><head>Table 6 :</head><label>6</label><figDesc>Inter annotator agreement according to Cohen's κ on graded scale, counting grades that are o by</figDesc><table coords="8,72.00,87.34,433.22,208.20"><row><cell>one as agreement.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(a) Passage assessment.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">annotator1 annotator2 annotator3 annotator4 annotator5 annotator6</cell></row><row><cell>annotator1 annotator2 annotator3 annotator4 annotator5 annotator6</cell><cell>0.687 0.430 0.781 0.628 0.449</cell><cell>0.687 0.568 0.632 0.644 0.871</cell><cell>0.430 0.568 0.409 0.422 0.859</cell><cell>0.781 0.632 0.409 0.407 0.740</cell><cell>0.628 0.644 0.422 0.407 0.512</cell><cell>0.449 0.871 0.859 0.740 0.512</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) Entity assessment.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">annotator1 annotator2 annotator3 annotator4 annotator5 annotator6</cell></row><row><cell>annotator1 annotator2 annotator3 annotator4 annotator5 annotator6</cell><cell>0.342 0.542 0.708 0.705 0.134</cell><cell>0.342 0.526 0.096 0.500 0.514</cell><cell>0.542 0.526 0.422 0.716 0.457</cell><cell>0.708 0.096 0.422 0.643 0.467</cell><cell>0.705 0.500 0.716 0.643 0.408</cell><cell>0.134 0.514 0.457 0.467 0.408</cell></row></table><note coords="8,72.00,333.30,468.00,6.64"><p>assessment where assessments that dier by no more than one grade step, e.g., grades SHOULD and</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,87.24,692.18,203.19,8.47"><p>https://en.wikipedia.org/wiki/Coffee_preparation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,87.24,700.61,165.08,9.88"><p>Available at http://data.allenai.org/tqa/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="6,72.00,685.62,468.00,6.64;6,72.00,697.58,93.51,6.64;6,83.09,711.71,173.97,10.41"><p>The grade trash is assigned to paragraphs/entities of low quality which therefore would not be relevant for any topic imaginable.<ref type="bibr" coords="6,83.09,711.71,3.65,7.30" target="#b2">3</ref> http://trec-car.cs.unh.edu/datareleases/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We express our gratitude for many suggestions of several experts in the eld, who helped to make this track successful. We thank the <rs type="institution">University of New Hampshire</rs> for providing computational resources and web servers. We are deeply thankful for Ellen Voorhees' experience, patience, and persistence in running the assessment process. Finally we thank all our participants for their contributions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity Task X X X X Table <ref type="table" coords="10,215.72,327.10,3.87,6.64">7</ref>: Participant contributed runs in CAR Year 2.</p><p>computation oine and only use the model to re-rank all outlines based on the query in inference time which is much faster compare to recent neural IR models. Using attention based model also provides position-dependent information required to assess the relevance of a snippet of a document to a given query. Attention signals illustrate term dependencies between query and given passage. The model is trained on TREC V2 data set which has 50% of Wikipedia Articles and test on TREC benchmarkY1 which is ocial evaluation topics for the TREC CAR task.</p><p>• UTD: We extended our approach from last year to create the TRANSformer Complex Answer PAragraph Retrieval (TRANS-CAPAR) system to perform complex answer retrieval consisting of the fol- • uog: Our 2018 runs study entity-aware expansion models tailored to the TREC CAR task. In particular, we use ne-grained expansion features based on heading components of the TREC CAR query topics. We employ query entity linking, entity retrieval, and performed expansion over diverse matching vocabularies (words, entity IDs, aliases). For entity retrieval we experimented with using feedback on the paragraph collection. All methods parameters (hyper-parameters and LTR models) used Y1Train (hierarchical qrels) and runs were selected by performance on Y1Test (tree qrels).</p><p>• CUIS: Team CUIS provided two passage runs and three entity runs. The system consists of two stages.    0%-5% 5%-25% 25%-50% 50%-75% 75%-95% 95%-100% difficulty percentile according to uog-heading-rh-sdm  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="17,87.50,438.77,452.51,6.64;17,87.50,450.73,452.50,6.64;17,87.50,462.27,452.50,7.19;17,87.50,471.78,222.39,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,407.45,438.77,132.55,6.64;17,87.50,450.73,157.46,6.64">Frontiers, challenges, and opportunities for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>James Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Moat</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sanderson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2215676.2215678</idno>
		<ptr target="http://doi.acm.org/10.1145/2215676.2215678" />
	</analytic>
	<monogr>
		<title level="m" coord="17,255.14,450.73,284.85,6.64;17,87.50,462.68,123.04,6.64">Report from SWIRL 2012 the second strategic workshop on information retrieval in Lorne</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">232</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,87.50,494.04,452.50,7.19;17,87.50,505.99,220.47,7.19" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,253.49,494.45,222.13,6.64">Using crowdsourcing for trec relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">Omar</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,487.93,494.04,52.07,7.19;17,87.50,505.99,112.81,7.19">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">10531066</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,87.50,526.22,452.50,6.64;17,87.50,537.76,452.51,7.19;17,87.50,549.72,195.40,7.19" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,352.59,526.22,187.41,6.64;17,87.50,538.18,376.47,6.64">Research frontiers in information retrieval: Report from the third strategic workshop on information retrieval in lorne (swirl 2018)</title>
		<author>
			<persName coords=""><forename type="first">Shane</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,485.05,537.76,54.95,7.19;17,87.50,549.72,26.07,7.19">ACM SIGIR Forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">3490</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,87.50,569.95,452.50,6.64;17,87.50,581.49,452.50,7.19;17,87.50,593.45,216.99,7.19" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,302.90,569.95,237.10,6.64;17,87.50,581.91,19.18,6.64">Entity query feature expansion using knowledge base links</title>
		<author>
			<persName coords=""><forename type="first">Jerey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,130.00,581.49,410.00,7.19;17,87.50,593.45,89.41,7.19">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">365374</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,87.50,613.68,452.50,6.64;17,87.50,625.22,452.51,7.19;17,87.50,637.17,172.35,7.19" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,242.07,613.68,297.93,6.64;17,87.50,625.63,34.16,6.64">Tagme: on-the-y annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,142.50,625.22,397.50,7.19;17,87.50,637.17,33.83,7.19">Proceedings of the 19th ACM international conference on Information and knowledge management</title>
		<meeting>the 19th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">16251628</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,87.50,657.40,452.50,6.64;17,87.50,668.94,452.50,7.19;17,87.50,680.90,408.00,7.19" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,125.20,669.36,256.14,6.64">Characterizing question facets for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,401.88,668.94,138.12,7.19;17,87.50,680.90,377.85,7.19">Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,87.50,701.13,452.50,6.64;17,87.50,713.08,279.73,6.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="17,374.89,701.13,165.11,6.64;17,87.50,713.08,248.24,6.64">Learning to coordinate multiple reinforcement learning agents for diverse query reformulation</title>
		<author>
			<persName coords=""><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
