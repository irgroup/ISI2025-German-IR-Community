<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.32,164.85,314.61,15.12">Overview of the TREC 2018 CENTRE Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.64,197.33,62.75,10.48"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua Tetsuya Sakai Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.49,197.33,62.92,10.48"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua Tetsuya Sakai Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.06,197.33,73.55,10.48"><forename type="first">Maria</forename><surname>Maistro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua Tetsuya Sakai Waseda University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.32,164.85,314.61,15.12">Overview of the TREC 2018 CENTRE Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22D9B74CD11A2BFBA24B91C2A8EFA31C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CLEF-NTCIR-TREC Reproducibility track (CENTRE) is a research replication and reproduction effort spanning three major information retrieval evaluation venues. In the TREC edition, CENTRE participants were asked to reproduce runs from either the TREC 2016 clinical decision support track, the 2013 web track, or the 2014 web track. Only one group participated in the track, and unfortunately the track will not continue in 2019.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the cross-campaign CENTRE effort was to develop and tune a reproducibility evaluation protocol. In particular, we targeted two specific objectives:</p><p>• Replicability: different team, same experimental setup;</p><p>• Reproducibility: different team, different experimental setup.</p><p>Organizers selected, among the methods/systems submitted to the CLEF, NTCIR, and TREC ad-hoc tasks over the years, the top performing and most impactful ones. Each participating group was challenged to replicate and/or reproduce one or more of the selected systems using standard open source IR systems, like Lucene, Terrier, and others. Each participating group submitted one or more runs representing the output of their reproduced systems. They also developed and integrated into the open source IR system all the missing components and resources needed to replicate/reproduce the selected systems. Finally, they were asked to contribute back to open source all the developed components, resources, and configuration via a common repository.</p><p>Therefore, the goal of CENTRE was to run a joint CLEF/NTCIR/TREC task that challenges participants:</p><p>• to reproduce best results of best/most interesting systems in previous editions of CLEF, NTCIR, TREC by using standard open source IR systems;</p><p>• to contribute back to the community the additional components and resources developed to reproduce the results in order to improve existing open source systems.</p><p>The CENTRE tracks<ref type="foot" coords="2,244.78,167.12,3.97,6.12" target="#foot_0">1</ref> took place at the three major evaluation venues, TREC, NTCIR, and CLEF. <ref type="bibr" coords="2,255.81,180.64,19.99,8.74" target="#b11">[12,</ref><ref type="bibr" coords="2,280.53,180.64,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="2,293.00,180.64,7.75,8.74" target="#b3">4]</ref> For the TREC edition of the track, a particular goal was to develop and refine a report format that would serve as a template to other reproduction efforts in the IR community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>The TREC CENTRE track had three tasks:</p><p>1. Merck at TREC 2016 Clinical Decision Support: Replicating runs from the Merck group participation in the TREC 2016 Clinical Decision Support (CDS) track. <ref type="bibr" coords="2,255.78,299.38,16.17,8.74" target="#b10">[11,</ref><ref type="bibr" coords="2,276.75,299.38,7.75,8.74" target="#b5">6]</ref> The CDS task was to retrieve biomedical journal articles relevant to a patient represented by a de-identified clinical note. The Merck system employed a number of advanced features including word embeddings, pseudo-relevance feedback, sophisticated query parsing, and learning-to-rank.</p><p>2. Delaware at TREC 2013 Web: Replicating runs from the University of Delaware (Fang) group in the TREC 2013 Web track. <ref type="bibr" coords="2,405.35,377.53,11.82,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,420.67,377.53,12.73,8.74" target="#b13">14]</ref> The 2013 Web Track task was to retrieve web pages from the ClueWeb12 collection in response to single-and multi-faceted topics. The Delaware system uses an axiomatic retrieval model.</p><p>3. Glasgow at TREC 2014 Web: Replicating runs from the University of Glasgow group in the TREC 2014 Web track. <ref type="bibr" coords="2,375.41,443.72,11.82,8.74">[3,</ref><ref type="bibr" coords="2,391.46,443.72,7.75,8.74" target="#b8">9]</ref> The TREC 2014 task was the same as in the 2013 Web Track, with a new topic set. The Glasgow system uses a risk-sensitive learning to rank approach.</p><p>In each task, participating groups were asked to attempt to reproduce the results of the target group/system. Whether the reproduction was successful would be measured using the original relevance assessments and comparison of the ranking of the reproduction system against the original systems. If resources allowed, we hoped to do additional relevance assessments to explore bias.</p><p>To facilitate the tasks, the respective topics, collections, relevance judgments, system outputs, overview paper, and track guidelines were collected on the track homepage. <ref type="foot" coords="2,179.71,566.56,3.97,6.12" target="#foot_1">2</ref>The track had one participating group, the Anserini team from the University of Waterloo. <ref type="bibr" coords="2,219.51,592.05,17.80,8.74" target="#b14">[15]</ref> That group submitted six runs to task 2 (2013 Web Track, UDel Fang), three attempts for each original UDel Fang group run. Additionally, the University of Padua submitted unofficial runs for task 1 after the track deadline, with three variations on the stoplist for each of the original three Merck runs. <ref type="bibr" coords="2,184.61,639.87,16.52,8.74" target="#b9">[10]</ref> 3 Task 1: Clinical 2016</p><p>The University of Padua submitted unofficial runs for task 1, which were accepted as no new relevance assessments were planned for task 1. The Merck 2016 runs identified for reproduction were MRKUmlsSolr, MRKSumCln, and MRKPrfNote, which explored different sources of terms for query expansion. <ref type="bibr" coords="3,464.50,185.65,12.98,8.74" target="#b5">[6]</ref> The Padua group submitted three runs covering various instantiations of the stoplist used in pseudo-relevance feedback, which was not specified in the original work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Task 2: Web 2013</head><p>The 2013 Web Track collection was constructed using very shallow pools of either depth 10 or 20 depending on the topic. <ref type="bibr" coords="5,317.95,458.44,11.62,8.74" target="#b1">[2]</ref> Because of this, the collection was mostly useful for measuring the 2013 participating runs on precision-oriented metrics, and it could be quite likely that a system developed later would retrieve many unjudged documents and therefore its effectiveness would be uncertain. Since CENTRE asks participants to reproduce an existing system (and so, in some sense a well-measured approach) but using open-source software (that would quite likely differ in many internal aspects such as segmentation and tokenization), we planned early in the TREC year to devote some assessment resources for the CENTRE track. These assessments could only be made for tasks 2 and 3, which did not require assessors with a medical background.</p><p>Since in the end we only had participation in task 2, we decided to revisit the pooling for that task entirely. In 2013, some topics were pooled to depth 10 and some to depth 20. For the CENTRE track, we re-pooled all the 2013 web track runs to depth 30, and included the Anserini runs. The assessors judged a random sample of 10% of the previously-judged documents as well as the unjudged documents in the pool. This gave us two sets of relevance assessments we could use to study the Anserini runs: the original relevance assessments from 2013 and the combined new set of judgments. Furthermore, we could study the agreement between the 2013 and 2018 judgments in the agreement sample. <ref type="foot" coords="6,460.77,126.39,3.97,6.12" target="#foot_2">3</ref>Figure <ref type="figure" coords="6,180.30,139.92,4.98,8.74" target="#fig_0">1</ref> shows the pool sizes. These multiple sets of relevance assessments would allow us to compute inter-annotator agreeement between the 2013 and the 2018 assessments, to make a reasonable comparison of the Anserini and original UDel Fang runs, to examine whether the Anserini runs would have been unfairly measured using only the original judgments, and to see if the new judgments imply a different run ranking than the original judgments had in 2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Agreement</head><p>We measured agreement between the original assessors and the CENTRE track assessors. Agreement is not a strong concern for ranking systems generally <ref type="bibr" coords="6,461.98,257.79,15.50,8.74" target="#b12">[13]</ref> but here there is a special concern because we are pooling and judging with different assessors to measure specific runs which can introduce bias. Since the 2013 web track judgments are on a graded relevance scale (junk, not relevant, relevant, highly relevant, key, navigational query target), simple overlap measures are not suitable. We used Krippendorff's alpha measure <ref type="bibr" coords="6,406.16,317.57,9.96,8.74" target="#b6">[7]</ref>, which allows for multiple raters and categorical scales in a principled way. <ref type="foot" coords="6,401.22,327.95,3.97,6.12" target="#foot_3">4</ref> Alpha lies on a [-1, 1] scale where 1.0 is perfect agreement, 0.0 is no relationship between the assessors, and negative values indicate systematic disagreement.</p><p>Figure <ref type="figure" coords="6,180.71,365.39,4.98,8.74">2</ref> shows that assessor agreement on the sample varies quite widely, perhaps due to the sample size compared with the number of categories on the relevance scale. We designated two sets of relevance judgments: the union of original and new judgments for all topics ("centre"), and the union for topics where α &gt; 0.2 ("pruned").</p><p>We then measured all the runs from the TREC 2013 track plus the new runs from Anserini using three sets of relevance judgments: the originals from 2013, the "centre" set, and the "pruned" set. The main track metrics for the adhoc task were expected reciprocal rank (ERR) <ref type="bibr" coords="6,345.11,461.03,10.52,8.74" target="#b0">[1]</ref> and Normalized Discounted Cumulative Gain (nDCG) <ref type="bibr" coords="6,251.26,472.99,9.96,8.74" target="#b7">[8]</ref>. The Kendall's tau correlations among the three system rankings are (left column pair): with Anserini without Anserini ERR@10 nDCG@10 ERR@10 nDCG@10 τ We conclude that the new assessments rank the systems quite differently, even when the new judgments are pruned to topics with (relatively) high agreement. Comparing the correlations where we alternately include and leave out the Anserini runs indicates that the original pools are biased against the new We then deepened those pools and included the Anserini submissions (centre). 10% of the documents from the original runs were randomly selected for re-assessment (rejudge).</p><p>runs, but that making new judgments based on the new runs can bias the measurement in the other direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>Figures 3 and 4 compare the ERR and nDCG scores using the original and augmented relevance judgments. The Anserini and UDel runs are called out in the plots. Table <ref type="table" coords="8,205.42,210.10,4.98,8.74">2</ref> gives the ERR scores for all runs against each set of relevance judgments.</p><p>In the augmented relevance judgements, ERR scores increase for all runs, and additionally the values are a bit more spread out, helping visually distinguish some points that were very close together originally. For nDCG, all scores decrease, which is sensible since the centre relevance judgments have essentially lengthened the ideal gain vector for everyone.</p><p>The critical point is to observe the relative placement of the Anserini and UDel WEB1 and WEB2 runs. The orders are not consistent between UDel and Anserini: for UDel, WEB2 is quite a bit higher-scoring than WEB1, whereas for Anserini the reverse is true. This indicates that the Anserini submission is not perfectly reproducing the original runs.  <ref type="table" coords="10,187.54,138.72,3.87,8.74">2</ref>: ERR@10 scores for all runs against all three sets of relevance judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>With only two participants (one official) and in the midst of the first CENTRE cycle, we are hesitant to report conclusions beyond the limited experience presented here. Perhaps the strongest point we can make is that, indeed, replicating and reproducing experiments is hard. We supposed that the fairly rigorous and regular methodology of test collection experiments would help, and indeed it has, to the extent that the data used and the procedure followed are well defined. However, this perspective allowed us to more closely examine results, to consider the effect of assessor disagreement and test collection mismatch, and even to consider what it means to actually reproduce a result: are we looking for scores, or for effects, or for rank agreement, or something else?</p><p>There are formidable social challenges to reproducibility beyond the technical ones. Reproducing experiments is hard, but not very publishable, since without careful presentation no new research is done. The CENTRE tracks tried to substitute a social environment to encourage reproduction, with it must be said poor results in the TREC edition. We note that there are a wide array of social incentives being created for reproduction, from special tracks in conferences and journals to lauding open source reproductions to increase their social capital. Perhaps we are at an early stage of a long journey.</p><p>Reproduction of results with test collections that have not been built to handle recall is fraught. In the Web track task in TREC CENTRE, the participant paid close attention to detail, and effect sizes and rank orders seem to indicate success. But the runs found many unjudged documents, and reassessing documents five years after the original collection yielded low assessor agreement. This meant that we are not very confident of the evaluation either with the original relevance judgments (due to poor coverage) or with the augmented judgments (due to poor agreement). In order to support reproducing experiments on large data sets, we either need to solve the agreement problem, solve the pooling problem in large data sets, or come with another solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>topic.subtopic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Krippendorff alpha</head><p>Figure <ref type="figure" coords="11,166.72,555.89,3.87,8.74">2</ref>: Values of Krippendorff's alpha for the 10% sample of documents judged by both the 2013 and the 2018 assessors. The blue line is the α &gt; 0.2 threshold used for selecting the topics to include in the pruned relevance judgment set. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Anserini-UDInfolabWEB1-Anserini-UDInfolabWEB1-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UDInfolabWEB2 UDInfolabWEB2R</head><p>Anserini-UDInfolabWEB2-Anserini-UDInfolabWEB2-3</p><p>Anserini-UDInfolabWEB1-Anserini-UDInfolabWEB1-3</p><p>Anserini-UDInfolabWEB1-Anserini-UDInfolabWEB1-1</p><p>Anserini-UDInfolabWEB2-Anserini-UDInfolabWEB2-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UDInfolabWEB1 UDInfolabWEB1R</head><p>Anserini-UDInfolabWEB2-Anserini-UDInfolabWEB2-1 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Anserini-UDInfolabWEB1-Anserini-UDInfolabWEB1 Second Text REtrieval Conference (TREC 2013), Gaithersburg, MD, November 2013.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,133.77,555.61,343.71,8.74;7,133.77,567.56,343.71,8.74;7,133.77,579.52,343.71,8.74;7,133.77,591.47,323.06,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The number of documents assessed for the 2013 web track task. Documents marked trec22 are from the original 2013 pools. We then deepened those pools and included the Anserini submissions (centre). 10% of the documents from the original runs were randomly selected for re-assessment (rejudge).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,133.77,567.56,343.71,8.74;12,133.77,579.52,156.36,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ERR@10 scores, based on the TREC 2013 qrels (x-axis) and the augmented CENTRE qrels (y-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,133.77,567.56,343.71,8.74;13,133.77,579.52,156.36,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: nDCG@10 scores, based on the TREC 2013 qrels (x-axis) and the augmented CENTRE qrels (y-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,133.77,221.51,343.71,447.89"><head>Table 1 :</head><label>1</label><figDesc>Table1shows the inferred average precision scores for the new Padua runs (in red) and the original runs (in black) from the track. Note that the rank ordering of the respective Merck runs is by and large reproduced in the Padua runs.</figDesc><table coords="3,193.53,277.70,224.20,391.70"><row><cell>run</cell><cell>infAP</cell></row><row><cell>ManualRun</cell><cell>0.0535</cell></row><row><cell>AutoSummary1</cell><cell>0.0454</cell></row><row><cell>SumES</cell><cell>0.0321</cell></row><row><cell>CCNUSUMR1</cell><cell>0.0316</cell></row><row><cell>cbnus1</cell><cell>0.0316</cell></row><row><cell>MrkUmlsXgb</cell><cell>0.0315</cell></row><row><cell>ECNUrun5</cell><cell>0.0313</cell></row><row><cell>UDelInfoCDS5</cell><cell>0.0311</cell></row><row><cell>DUTHsaRPF</cell><cell>0.0302</cell></row><row><cell>udelSRef</cell><cell>0.0302</cell></row><row><cell>ECNUrun1</cell><cell>0.0296</cell></row><row><cell>cbnus2</cell><cell>0.0295</cell></row><row><cell>nkuRun1</cell><cell>0.0289</cell></row><row><cell>SDPHBo1NE</cell><cell>0.0287</cell></row><row><cell>nkuRun3</cell><cell>0.0286</cell></row><row><cell>MRKUmlsSolr</cell><cell>0.0285</cell></row><row><cell>udelSB</cell><cell>0.0283</cell></row><row><cell>DUTHmaRPF</cell><cell>0.0281</cell></row><row><cell>nkuRun5</cell><cell>0.0276</cell></row><row><cell>ECNUrun3</cell><cell>0.0276</cell></row><row><cell>UWM1</cell><cell>0.0274</cell></row><row><cell>ETHSummRR</cell><cell>0.0272</cell></row><row><cell>MRKSumCln</cell><cell>0.0272</cell></row><row><cell>udelSDI</cell><cell>0.0263</cell></row><row><cell>ETHSumm</cell><cell>0.0261</cell></row><row><cell>AutoSummary</cell><cell>0.0258</cell></row><row><cell>sacmmf</cell><cell>0.0257</cell></row><row><cell>DAdescTM</cell><cell>0.0255</cell></row><row><cell>ETHNoteRR</cell><cell>0.0254</cell></row><row><cell>DAsummTM</cell><cell>0.0253</cell></row><row><cell>mayoas</cell><cell>0.0252</cell></row><row><cell>Smart ims unipd-MRKUmlsSolr</cell><cell>0.0244</cell></row></table><note coords="5,199.67,341.96,248.29,8.74;5,159.96,353.91,288.00,8.74;5,159.96,365.87,287.99,8.74;5,159.96,377.82,159.21,8.74"><p>Runs from the 2016 Clinical Decision Support task, with the University of Padua reproductions (red) of the Merck runs, scored by inferred average precision. Runs with infAP scores less that 0.01 have been elided for space.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,149.01,658.37,114.82,6.64"><p>http://www.centre-eval.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,149.01,667.88,195.27,6.64"><p>http://www.centre-eval.org/trec2018/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,149.01,629.41,328.47,6.99;6,133.77,638.87,343.71,6.99"><p>Relevance assessment in information retrieval typically exhibits low agreement, but in practice differences in relevance do not significantly affect the relative rankings of systems.<ref type="bibr" coords="6,462.77,638.87,14.71,6.99" target="#b12">[13]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,149.01,648.37,328.47,6.99;6,133.77,657.84,343.71,7.21;6,133.77,667.30,341.74,6.99"><p>The paper by Hayes and Krippendorff<ref type="bibr" coords="6,299.10,648.37,8.47,6.99" target="#b6">[7]</ref>, available freely from his website (afhayes. com/public/cmm2007.pdf), gives a succinct discussion of all the major agreement measures proposed since the 1950s, describes Krippendorff's α, and gives an implementation in SPSS.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,154.25,596.89,323.23,8.74;10,154.25,608.85,323.23,8.74;10,154.25,620.80,323.24,8.74;10,154.25,632.76,217.32,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,462.12,596.89,15.36,8.74;10,154.25,608.85,193.18,8.74">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,373.78,608.85,103.70,8.74;10,154.25,620.80,319.36,8.74">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,154.25,652.68,323.24,8.74;10,154.25,664.64,323.23,8.74;11,146.54,476.10,10.34,5.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,200.91,664.64,137.22,8.74">TREC 2013 web track overview</title>
		<author>
			<persName coords=""><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charlie</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<idno>0.5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,360.00,664.64,112.39,8.74">Proceedings of the Twenty</title>
		<meeting>the Twenty</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,159.84,323.24,8.74;14,154.25,171.80,323.23,8.74;14,154.25,183.75,323.23,8.74;14,154.25,195.71,92.60,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,210.07,171.80,149.50,8.74">TREC 2014 web track overview</title>
		<author>
			<persName coords=""><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,393.67,171.80,83.81,8.74;14,154.25,183.75,252.30,8.74">Proceedings of the Twenty-Third Text REtrieval Conference (TREC 2014)</title>
		<meeting>the Twenty-Third Text REtrieval Conference (TREC 2014)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,215.63,323.23,8.74;14,154.25,227.59,323.23,8.74;14,154.25,239.54,323.23,8.74;14,154.25,251.50,22.69,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,278.50,227.59,194.46,8.74">Sequel in the systematic reproducibility realm</title>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Maistro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Centre</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,167.07,239.54,114.67,8.74">Proceedings of CLEF 2019</title>
		<meeting>CLEF 2019</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">11696</biblScope>
			<biblScope unit="page" from="287" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,271.42,323.23,8.74;14,154.25,283.38,323.23,8.74;14,154.25,295.33,323.23,8.74;14,154.25,307.29,22.69,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,425.80,271.42,51.68,8.74;14,154.25,283.38,318.71,8.74">Overview of CENTRE@CLEF2018: a first tale in the systematic reproducibility realm</title>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Maistro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,167.34,295.33,115.41,8.74">Proceedings of CLEF 2018</title>
		<meeting>CLEF 2018</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11018</biblScope>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,327.21,323.24,8.74;14,154.25,339.17,323.23,8.74;14,154.25,351.12,323.24,8.74;14,154.25,363.08,273.88,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,245.84,339.17,231.63,8.74;14,154.25,351.12,89.36,8.74">Semi-supervised information retrieval system for clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claudia</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Megaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,268.23,351.12,209.25,8.74;14,154.25,363.08,110.15,8.74">Proceedings of the Twenty-Fifth Text REtrieval Conference (TREC 2016)</title>
		<meeting>the Twenty-Fifth Text REtrieval Conference (TREC 2016)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,383.00,323.23,8.74;14,154.25,394.96,323.23,8.74;14,154.25,406.91,74.16,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,336.78,383.00,140.70,8.74;14,154.25,394.96,144.84,8.74">Answering the call for a standard reliability measure for coding data</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">F</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,306.46,394.96,166.28,8.74">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,426.84,323.23,8.74;14,154.25,438.79,310.13,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,332.58,426.84,144.90,8.74;14,154.25,438.79,69.42,8.74">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,232.47,438.79,97.04,8.74">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002-10">October 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,458.72,323.23,8.74;14,154.25,470.68,323.24,8.74;14,154.25,482.63,323.24,8.74;14,154.25,494.59,323.23,8.74;14,154.25,506.54,92.60,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,249.92,482.63,140.35,8.74">TREC 2014 web track overview</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Romain</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M-Dyaa</forename><surname>Albakour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stuart</forename><surname>Mackie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thibaut</forename><surname>Thonet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bekir</forename><surname>Taner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dinçer</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,415.27,482.63,62.21,8.74;14,154.25,494.59,255.67,8.74">Proceedings of the Twenty-Third Text REtrieval Conference (TREC 2014)</title>
		<meeting>the Twenty-Third Text REtrieval Conference (TREC 2014)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,526.47,323.23,8.74;14,154.25,538.42,323.24,8.74;14,154.25,550.38,323.23,8.74;14,154.25,562.33,92.60,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,371.02,526.47,106.46,8.74;14,154.25,538.42,211.62,8.74">The University of Padua IMS research group at CENTRE@TREC 2018</title>
		<author>
			<persName coords=""><forename type="first">Giorgio</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Marchesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,395.68,538.42,81.80,8.74;14,154.25,550.38,254.29,8.74">Proceedings of the Twenty-Seventh Text REtrieval Conference (TREC 2018)</title>
		<meeting>the Twenty-Seventh Text REtrieval Conference (TREC 2018)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,582.26,323.23,8.74;14,154.25,594.21,323.24,8.74;14,154.25,606.17,323.23,8.74;14,154.25,618.12,155.87,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,190.19,594.21,266.10,8.74">Overview of the TREC 2016 clinical decision support track</title>
		<author>
			<persName coords=""><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,154.25,606.17,318.70,8.74">Proceedings of the Twenty-Fifth Text REtrieval Conference (TREC 2016)</title>
		<meeting>the Twenty-Fifth Text REtrieval Conference (TREC 2016)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.25,638.05,323.24,8.74;14,154.25,650.00,323.23,8.74;14,154.25,661.96,172.30,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,223.58,650.00,183.71,8.74">Overview of the NTCIR-14 CENTRE task</title>
		<author>
			<persName coords=""><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaohao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Maistro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,427.82,650.00,49.65,8.74;14,154.25,661.96,55.52,8.74">Proceedings of NTCIR-14</title>
		<meeting>NTCIR-14</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="494" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,154.25,127.96,323.23,8.74;15,154.25,139.92,323.23,8.74;15,154.25,151.87,323.24,8.74;15,154.25,163.83,300.80,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,238.26,127.96,239.22,8.74;15,154.25,139.92,103.27,8.74">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,280.53,139.92,196.94,8.74;15,154.25,151.87,323.24,8.74;15,154.25,163.83,76.29,8.74">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,154.25,183.75,323.23,8.74;15,154.25,195.71,323.24,8.74;15,154.25,207.66,273.88,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,279.48,183.75,197.99,8.74;15,154.25,195.71,91.84,8.74">Evaluating the effectiveness of axiomatic approaches in web track</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,265.77,195.71,211.72,8.74;15,154.25,207.66,110.15,8.74">Proceedings of the Twenty-Second Text REtrieval Conference (TREC 2013)</title>
		<meeting>the Twenty-Second Text REtrieval Conference (TREC 2013)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,154.25,227.59,323.23,8.74;15,154.25,239.54,323.23,8.74;15,154.25,251.50,273.88,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,283.69,227.59,193.78,8.74;15,154.25,239.54,90.11,8.74">Anserini at TREC 2018: CENTRE, common core, and news tracks</title>
		<author>
			<persName coords=""><forename type="first">Peiling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,263.26,239.54,214.22,8.74;15,154.25,251.50,110.15,8.74">Proceedings of the Twenty-Seventh Text REtrieval Conference (TREC 2018)</title>
		<meeting>the Twenty-Seventh Text REtrieval Conference (TREC 2018)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
