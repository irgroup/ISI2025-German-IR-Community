<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,73.27,94.24,448.74,17.54;1,54.99,123.15,485.29,17.54;1,150.43,152.07,294.41,17.54">Neural Networks and Support Vector Machine based Approach for Classifying Tweets by Information Types at TREC 2018 Incident Streams Task</title>
				<funder ref="#_YFrS5YC">
					<orgName type="full">MEXT KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.12,197.72,128.79,10.68"><roleName>Umme</roleName><forename type="first">Abu</forename><forename type="middle">Nowshed</forename><surname>Chy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology Toyohashi</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.91,197.72,78.20,10.68"><forename type="first">Aymun</forename><surname>Siddiqua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology Toyohashi</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,384.40,197.72,64.75,10.68"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology Toyohashi</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,73.27,94.24,448.74,17.54;1,54.99,123.15,485.29,17.54;1,150.43,152.07,294.41,17.54">Neural Networks and Support Vector Machine based Approach for Classifying Tweets by Information Types at TREC 2018 Incident Streams Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4FE976E3A14F876549D7BBB6EB290F9C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC incident streams</term>
					<term>microblog retrieval</term>
					<term>disasters</term>
					<term>TREC-IS</term>
					<term>deep neural network (DNN)</term>
					<term>DeepMoji</term>
					<term>attention mechanism</term>
					<term>SVM</term>
					<term>supervised learning</term>
					<term>hand-crafted features</term>
					<term>information types classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Microblog, especially twitter, is treated as an important source to serve the situational information needs during a disaster period. Monitoring and producing the curated tweets based on different information types from massive twitter posts provide enormous opportunities to different public safety personnel or used for post-incident analysis. In this paper, we present our approach to addressing the problem defined in the TREC 2018 incident streams (TREC-IS) task. The task is to classify the tweets in each event/incident's stream into different high-level information types within the incident ontology. In our approach, we employ different deep neural network (DNN) classifiers in combination with a multi-class support vector machine (SVM) classifier and a rule-based classifier. We consider a rich set of hand-crafted features to train our multi-class SVM classifier, whereas a pre-trained word2vec model is used for the DNN based classifiers. Moreover, we introduce a set of rules based on the language of tweets, exploiting indicator terms, and WH-orientation of tweets for our rule-based classifier. Experimental results showed that our proposed KDEIS4 DM method obtained the second position among the participants in TREC-IS task and outperforms the participant median by more than 8% and 5% in terms of F1 Score and accuracy, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Microblog sites such as twitter, tumblr, sina weibo, etc. are rapidly moving towards a platform for mass collaboration in usergenerated information production. Twitter has become the most popular among the microblog services. Everyday lots of people turning to this online platform to share their views, opinions, breaking news as well as fulfill their diverse information needs. The real-time nature of twitter plays an important role during a disaster period, such as earthquake, floods, wildfires, and typhoons. Because the user-generated twitter posts during such events might be useful to serve the situational information needs <ref type="bibr" coords="1,71.18,681.45,9.79,8.23" target="#b0">[1]</ref>. However, due to the brevity of the tweets and noisy tweet contents, information retrieval in twitter is regarded as a challenging IR problem. To address the general real-time information seeking behaviors, TREC was introduced the microblog ad-hoc search task in 2011 <ref type="bibr" coords="1,147.71,742.69,9.79,8.23" target="#b1">[2]</ref>. In contrast, this year <ref type="bibr" coords="1,241.25,742.69,45.05,8.23">TREC-2018</ref> introduces an incident streams (TREC-IS) task designed specif-ically to tackle the microblog retrieval during a disaster period.</p><p>The main task for the 2018 TREC-IS track was to categorize the tweets in each event/incident's stream into different high-level information types defined in the TREC-IS incident ontology.</p><p>In this paper, we present our participation in the 2018 TREC-IS task. We combine different types of classifiers in our proposed approach. We define a set of rules for the rule-based classifier by focusing on the language of tweets, exploiting indicator terms from the training corpus, and WH-orientation of tweets. We consider lexical and content relevance features, incident and event related features, sentiment aware, and twitter specific features to train our multi-class SVM classifier, whereas a pre-trained word2vec model is used for the deep neural network (DNN) classifiers.</p><p>The rest of the paper is structured as follows: We will introduce our proposed TREC-IS framework in Section 2. Section 3 includes experiments and evaluation to show the effectiveness of our proposed methods. Some concluded remarks and future directions of our work described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiclass SVM with RBF Kernel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble based Prediction Combine Prediction Results</head><p>Training Data Data Preprocessing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes No</head><p>Fig. <ref type="figure" coords="2,210.02,337.94,3.90,6.89">1</ref> Proposed TREC incident streams (TREC-IS) framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed TREC-IS Framework</head><p>In this section, we describe the details of our proposed framework. Given a query related to an event/incident and a set of tweets, the goal of our proposed system is to categorize the tweet into the different high-level information types. The overview of our proposed framework depicted in Fig. <ref type="figure" coords="2,200.06,452.56,3.46,8.23">1</ref>.</p><p>At first, our system fetches a query and the corresponding tweet set as a single batch and indexed them for further processing. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Preprocessing</head><p>Data preprocessing stage is initiated with tokenization. As tweets are informal user generated contents, people use lots of non-English characters and symbols in tweets. Since meaningful English words do not contain these characters, we remove these characters from tweets. Moreover, the short length constraint of the tweet makes characters expensive. To overcome this constraint, people are utilizing twitter specific syntaxes such as #hashtag to express their thoughts concisely. For #hashtag removal, we segment each #hashtag by using a hashtag segmentation technique similar to Siddiqua et al. <ref type="bibr" coords="2,471.26,577.59,10.75,8.23" target="#b2">[3]</ref> and replaced the hashtag with the segmented words.</p><p>Moreover, tweets often contain non-standard word forms and domain-specific entities. For example, people usually use "earthquakeeeee" instead of "earthquake," "addiquate" instead of "adequate," "appt" instead of "appointment," etc. We utilized two lexical normalization dictionaries collected from <ref type="bibr" coords="2,495.94,669.44,10.75,8.23" target="#b3">[4]</ref> and <ref type="bibr" coords="2,526.86,669.44,10.75,8.23" target="#b4">[5]</ref> to normalize such non-standard words into their canonical forms.</p><p>In incident streams task, stopwords play a negative role because they do not carry any incident-oriented information and may actually damage the performance of the classifiers. For stopword removal, we applied the Indri's standard stoplist * 1 .</p><p>Table <ref type="table" coords="3,249.68,64.97,3.90,6.89">1</ref> List of features used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Type Feature Name</head><p>Lexical and Content Relevance Features</p><p>( 1 ) TF-IDF <ref type="bibr" coords="3,286.23,94.71,9.09,6.96" target="#b5">[6]</ref> similarity score between an incident query and a tweet.</p><p>( 2 ) Okapi BM25 <ref type="bibr" coords="3,302.91,105.77,9.09,6.96" target="#b6">[7]</ref> similarity score between an incident query and a tweet.</p><p>( 3 ) Language model with Dirichlet smoothing <ref type="bibr" coords="3,395.95,116.83,9.09,6.96" target="#b7">[8]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>In our proposed framework, we extract a set of 19 features broadly grouped into 4 different categories, including lexical and content relevance features, incident and event related features, sentiment aware features, and twitter specific features. The feature extraction processes are described in Table <ref type="table" coords="3,224.98,557.02,3.46,8.15">1</ref>.</p><p>The first 3 lexical and content relevance features are used to estimate the similarity between a given incident query and a tweet.</p><p>In this regard, we generate the incident query by combining the query title and narrative and perform the minimal preprocessing as described in Section 2.1. We also extract 6 incident event related features that seem to be important during the disaster situation. We utilize the Stanford named entity recognizer (NER) tool <ref type="bibr" coords="3,63.58,679.53,10.75,8.23" target="#b8">[9]</ref> to extract the location count, organization count, and person count features. Along with this direction, a publicly available library is utilized to estimate the phone number count feature.</p><p>We also use the CMU ARK POS tagger <ref type="bibr" coords="3,196.25,725.46,15.35,8.23" target="#b9">[10]</ref> to identify the noun POS of each tokenized word which is required to extract the noun count feature. To estimate the sentiment polarity of a tweet, we use a publicly available package SentiStrength <ref type="bibr" coords="3,480.10,480.51,14.13,8.23" target="#b10">[11]</ref>. We construct the positive and negative sentiment bearing word lexicons as described in <ref type="bibr" coords="3,347.94,511.13,14.13,8.23" target="#b11">[12]</ref>. We utilize these lexicons to estimate the lexicon based sentiment aware features. For emoticon count feature, we use a publicly available library to identify the emoticon. Other features are extracted as described in Table <ref type="table" coords="3,471.87,557.05,3.46,8.23">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rule-Based Classifier</head><p>In a rule-based classifier, we usually construct a set of rules that determine a certain combination of patterns, which are most likely to be related to the different classes or information types.</p><p>Each rule consists of an antecedent part and a consequent part.</p><p>The antecedent part corresponds to the patterns and the consequent part corresponds to a class label. We can define a rule as follows:</p><formula xml:id="formula_0" coords="3,360.98,712.65,134.66,27.92">R j : if x 1 is A j1 and ........ x n is A jn then Class = C j , j = 1, ......, N</formula><p>where R j is a rule label, j is a rule index, A j1 is an antecedent set, C j is a consequent class, and N is the total number of rules.</p><p>Our unsupervised rule-based classifier casts the TREC incident streams (TREC-IS) task as a multi-class classification problem and labeled each tweet to the corresponding information types assigned by the rules. To achieve this, we define a set of rules based on the tweets language, indicator terms within tweets, and WH-orientation of the tweet. Descriptions of each set of rules are presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Language Related Rule</head><p>Even though twitter is a multilingual microblog service, we only consider English tweets as relevant in this research. Therefore, we define a rule based on the language of a tweet that is if the language of a tweet is not English, we classify the tweet as</p><p>Irrelevant information type. To identify the non-English tweets from the given tweet set, we apply a language detection library * 2 in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Indicator Terms based Rule</head><p>A tweet may contain some highly influential indicator terms related to a high-level information type which may useful to categorize the tweet into that information type. In this regard, we </p><formula xml:id="formula_1" coords="4,47.08,356.74,25.46,8.23">exploit</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">WH-Orientation based Rule</head><p>Since people usually use WH sentence to know more about the incident, we use the regular expression to identify the WHorientation of a tweet and categorize the tweet into the Informa-tionWanted information type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">An Ensemble of Learning Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Support Vector Machine (SVM) Classifier</head><p>We use the SVM multiclass with RBF kernel from <ref type="bibr" coords="4,238.09,583.84,14.13,8.23" target="#b12">[13]</ref>. It uses the multi-class formulation described in <ref type="bibr" coords="4,199.44,599.15,14.13,8.23" target="#b13">[14]</ref>  where C is the usual regularization parameter that trades off margin size and training error. We estimate the optimal value of C using cross-validation. (y n , y) is the loss function that returns 0 if y n equals y, and 1 otherwise. To solve this optimization problem, SVM multiclass uses an algorithm based on structural SVMs.</p><p>For the training, we use the features described in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Deep Learning based Classifiers</head><p>Besides feature based multi-class SVM classifier, we employ the deep neural network (DNN) based classifier models because traditional bag-of-words based methods cannot perform well due to the curse of dimensionality and the loss of word order information. However, to train the deep learning models effectively, it is important to represent the tweets as meaningful features. To achieve this goal, we apply the CLSTM architecture inspired by the proposal of Zhou et al. <ref type="bibr" coords="4,408.54,280.19,14.13,8.23" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN LSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Vector</head><p>Dense Layer RT @BBCWorld: Three killed in Italy earthquake ….</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Layer</head><p>Output (Probability Score of Each Information Types)</p><p>Fig. <ref type="figure" coords="4,337.63,491.18,3.90,6.89">2</ref> Convolutional long short-term memory (CLSTM) network.</p><p>In our CLSTM architecture as depicted in Fig. <ref type="figure" coords="4,499.25,516.38,3.45,8.15">2</ref>, the higher level representations of CNN are fed into the LSTM to learn long-term dependencies. The CNN is constructed on top of the pre-trained word vectors from fastText <ref type="bibr" coords="4,454.91,562.34,15.35,8.23" target="#b15">[16]</ref> to learn higher-level representations of n-grams. The feature maps of CNN are then organized as sequential window features to serve as the input of LSTM to learn sequential correlations from higher-level sequence representations. The LSTM transition functions are defined as follows: φ is the hyperbolic tangent activation function that has an output in [1, -1] and is the element-wise multiplication.</p><formula xml:id="formula_2" coords="4,379.23,657.00,98.71,101.12">i t = σ(W i • [h t-1 , x t ] + b i ) f t = σ(W f • [h t-1 , x t ] + b f ) u t = φ(W u • [h t-1 , x t ] + b u ) c t = f t c t-1 + i t u t o t = σ(W o • [h t-1 , x t ] + b o ) h t =</formula><p>At the last time step of LSTM, the output of the hidden state is regarded as the final tweet representation and passed to a fully connected softmax layer. The output of the softmax layer is the probability distribution over all the information types. To learn the model parameter, we utilize the stochastic gradient descent (SGD) and adopt the Adam optimizer <ref type="bibr" coords="5,188.36,218.96,14.13,8.23" target="#b16">[17]</ref>. In our ACBLSTM architecture as depicted in Fig. <ref type="figure" coords="5,240.31,752.69,3.45,8.15" target="#fig_2">3</ref>, the higher level representations of CNN are fed into the bidirectional LSTM to learn long-term dependencies. In order to amplify the contribution of important elements in the final representation of bidirectional LSTM, we employ a recently introduced attention mechanism <ref type="bibr" coords="5,334.18,111.79,14.13,8.23" target="#b17">[18]</ref>, <ref type="bibr" coords="5,355.02,111.79,15.35,8.23" target="#b18">[19]</ref> to aggregate all the hidden states according to their relative importance.</p><p>In addition, we utilize the stacked bidirectional LSTM instead of a single bidirectional LSTM in our ACSBLSTM architecture.</p><p>Our stacked bidirectional LSTM is comprised of N = 15 bidirectional LSTM layers, where each layer provides a sequence output to the next layer as depicted in Fig. <ref type="figure" coords="5,440.26,203.61,3.45,8.15">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Bi-LSTM#N</head><p>Feature Vector Dense Layer RT @BBCWorld: Three killed in Italy earthquake …. Next, we employ the state-of-the-art deep learning architecture DeepMoji (DM), proposed by Felbo et al. <ref type="bibr" coords="5,482.53,525.63,14.13,8.23" target="#b19">[20]</ref>. We train the DeepMoji architecture without loading the pre-trained weights.</p><p>As depicted in Fig. <ref type="figure" coords="5,381.41,556.21,3.45,8.15" target="#fig_4">5</ref>, DeepMoji uses an embedding layer of 256 dimensions to project each word into a vector space. Two bidirectional LSTM layers with 1024 hidden units in each (512 in each direction) are applied to capture the context of each word.</p><p>Finally, an attention layer takes all of these layers as input using skip-connections. The representation vector obtained from the attention layer is sent to the softmax layer for classification. To select the optimal value for the anchoring parameter α, we swept the parameter between {0.1, ...., 0.9}. Information type that gets the highest probability score will be assigned to the label of the tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Combining the Classifiers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Collection</head><p>The TREC incident streams (TREC-IS) task at TREC-2018 provides a benchmark dataset to evaluate the performance of the proposed systems. The dataset contains 21 query topics along with the relevant tweets sampled from several disaster events such as earthquake, typhoon, shooting, etc. Among the 21 query topics, the training set contains 6 query topics and the test set contains 15 query topics. The number of tweets in the training set is around 1300, whereas the number of tweets in the test set is around 20,000. The organizer also provides an ontology of information types, which contains 25 information types or class label broadly grouped into Request, Report, CallToAction, and Other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Measures</head><p>To evaluate the performance of our proposed systems, we applied the evaluation measure used in the TREC-IS task. According to the benchmark of the 2018 TREC-IS task, participant systems were tasked to assign one most representative information type per-tweet. However, during the ground truth generation, the human assessors were allowed to select as many information types as appropriate for a single tweet. Therefore, to evaluate the performance of a TREC-IS system the organizer used two ways referred to as multi-type and any-type.</p><p>In the multi-type evaluation, the categorization performance per information type is estimated in a 1 vs. All manner. If both the system and human assessor selected the corresponding category then the system is considered to correctly categorize a tweet. Whereas, in the any-type evaluation, a system is considered to correctly categorize a tweet if it assigned any of the categories that the human assessor selected for that tweet. However, any-type evaluation criteria is used to estimate the overall performance of a 2018 TREC-IS system. Four standard evaluation metrics including precision, recall, F1 score, and accuracy were used in both the multi-type and any-type evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results with Different Experimental Settings</head><p>We now evaluate the performance of our proposed methods in this section. We describe the experimental settings of each method and the summarized evaluation results were presented in Table <ref type="table" coords="6,342.44,581.25,3.46,8.15" target="#tab_5">2</ref>.</p><p>At first, our rule-based classifier is applied to classify the tweet into corresponding information type and tweets that are not clas- Results showed that our KDEIS4 DM setting achieved the best performance in both the multi-type and any-type evaluation criteria in terms of primary evaluation measure F1 score. For the any-type evaluation criteria, our system outperformed the participant median by more than 8% in terms of F1 Score and by more than 5% in terms of Accuracy.</p><p>We also reported the results of the top 5 performing systems in TREC-IS 2018 in Table <ref type="table" coords="7,151.98,341.40,3.46,8.15" target="#tab_6">3</ref>. It showed that our KDEIS4 DM achieved the second position among the participants in terms of primary evaluation measure F1 score for the any-type evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Directions</head><p>In this paper, we presented our approach to the TREC 2018 incident streams (TREC-IS) task. We tackled the problem by employing an ensemble of classifiers. Along with a rule-based classifier and SVM classifier, four different deep neural network (DNN) models were employed in several experimental settings.</p><p>Among our proposed methods, KDEIS4 DM achieved the second best performance (F1 Score = 0.5603 for any-type evaluation) among the participant systems.</p><p>There is much room left to further improve our methods. Shortage of training dataset for our deep learning approach is the main problem. In the future, we have a plan to overcome this limitation by incorporating more training samples collected in an unsupervised manner. We also have a plan to exploit the more sophisticated techniques in our deep learning approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,47.08,498.48,239.22,8.23;2,47.08,513.79,239.22,8.23;2,47.08,529.10,239.22,8.23;2,47.08,544.41,239.22,8.23;2,47.08,559.72,239.22,8.23;2,47.08,575.03,239.22,8.23;2,47.08,590.34,239.22,8.23;2,47.08,605.65,239.22,8.23;2,47.08,620.96,239.22,8.23;2,47.08,636.27,239.22,8.23;2,47.08,651.58,239.22,8.23;2,47.08,666.89,239.22,8.23;2,47.08,682.20,239.22,8.23;2,47.08,697.51,239.22,8.23;2,47.08,712.82,239.22,8.23;2,47.08,728.13,239.22,8.23;2,47.08,743.44,239.22,8.23;2,47.08,758.75,239.22,8.23;2,308.98,365.80,239.22,8.23;2,308.98,381.11,239.22,8.23;2,308.98,396.42,209.61,8.23"><head></head><label></label><figDesc>the data preprocessing stage, we perform the tokenization, lexical normalization to the tokenized words, stop-word removal, special character removal, and hashtag segmentation. Next, our proposed rule-based classifier is applied to classify the tweets into the corresponding high-level information types. For the tweets that are not classified by the rule-based classifier, we consider the combined weighted prediction score from multi-class support vector machine (SVM) classifier and several deep neural network (DNN) classifiers. SVM classifier is trained with our extracted features. We extract several effective features broadly grouped into four different categories, including lexical and content relevance features, incident and event related features, sentiment aware features, and twitter specific features. For extracting sentiment aware features, we construct strong sentiment lexicons by combining several publicly available sentiment lexicons. To scale the feature values, we make use of the Min-Max normalization technique. For DNN based classifiers, a pre-trained word2vec model is applied. Tweets are labeled to the information type that gets the highest prediction score. Results of both the rule-based classifier and the ensemble of SVM and DNN classifiers are then combined and the set of labeled tweets return to the user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,74.94,356.74,211.36,8.23;4,47.08,372.05,239.22,8.23;4,47.08,387.36,239.22,8.23;4,47.08,402.67,239.22,8.23;4,47.08,417.98,239.22,8.23;4,47.08,433.29,239.22,8.23;4,47.08,448.60,200.01,8.23"><head></head><label></label><figDesc>the indicator terms given in the training data. We prepare two curated indicator terms lexicon based on the given indicator terms of several information types. One for the MultimediaShare category and other for the Donations category. If a tweet contains words from these lexicons, it is classified to the corresponding information type. The priority of the information type is determined by the number of lexicon words available in the tweet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,47.08,717.03,239.22,6.99;5,73.38,728.12,17.89,6.96"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Attention based convolutional bidirectional LSTM (ACBLSTM) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,308.98,470.79,239.22,6.99;5,335.28,481.88,57.96,6.96"><head>1 ⋮Fig. 4</head><label>14</label><figDesc>Fig. 4 Attention based convolutional stacked bidirectional LSTM (ACS-BLSTM) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,47.08,230.77,239.22,6.99;6,73.38,241.85,57.81,6.96;6,47.08,263.41,239.22,8.23;6,47.08,278.72,239.22,8.23;6,47.08,294.03,239.22,8.23;6,47.08,309.34,49.40,8.23"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 DeepMoji (DM) network, where T is the tweet length and C is the number of classes. classifier we consider the weighted ensemble based prediction from multi-class SVM classifier and several deep neural network models. The prediction score is computed using the following Equation 2.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,110.72,116.83,378.18,338.87"><head></head><label></label><figDesc>Person Count Feature: Number of person information available in a tweet. ( 4 ) Noun Count Feature: Number of noun POS available in a tweet. ( 5 ) Phone Number Count Feature: Number of phone number available in a tweet. ( 6 ) Known Already Count Feature: Number of previously posted tweets that are closely matched (based on Cosine Similarity) with the corresponding tweet. Sentiment Polarity Feature: A binary feature that is assigned to 1 if a tweet has the positive or negative sentiment polarity and 0 otherwise. ( 2 ) Positive Word Count Feature: Number of positive words available in a tweet based on the lexicon. ( 3 ) Negative Word Count Feature: Number of negative words available in a tweet based on the lexicon. ( 4 ) Emoticon Count Feature: Number of emoticons available in a tweet.</figDesc><table coords="3,110.72,116.83,378.18,291.22"><row><cell>score between an incident</cell></row><row><cell>query and a tweet.</cell></row><row><cell>( 4 ) Tweet Length Feature: Number of words available in a tweet.</cell></row><row><cell>( 5 ) Average Word Length Feature: Average length of the words available in a</cell></row><row><cell>tweet.</cell></row><row><cell>( 1 ) Location Count Feature: Number of locations name available in a tweet.</cell></row><row><cell>( 2 ) Organization Count Feature: Number of organizations name available in</cell></row><row><cell>a tweet.</cell></row><row><cell>Incident and Event Related Features ( 3 ) Sentiment Aware Features ( 1 ) Twitter Specific Features</cell></row></table><note coords="3,239.47,366.71,249.44,6.96;3,260.02,377.76,82.50,6.96;3,239.47,388.82,223.79,6.96;3,239.47,399.88,249.44,6.96;3,260.02,410.93,68.86,6.96;3,239.47,421.99,249.44,6.96;3,260.02,433.05,132.94,6.96;3,159.26,448.73,15.83,6.96;3,347.52,448.73,36.16,6.96"><p><p><p>( 1 ) Hashtag Feature: A binary feature that is assigned to 1 if a tweet contains a hashtag and 0 otherwise. ( 2 ) Hashtag Count Feature: Number of hashtags available in a tweet.</p>( 3 ) URL Feature: A binary feature that is assigned to 1 if a tweet contains a URL and 0 otherwise. ( 4 ) Retweet Feature: A binary feature that is assigned to 1 if a tweet is a retweet of the other tweet and 0 otherwise.</p>Total 19 Features</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,47.08,599.15,239.22,101.21"><head></head><label></label><figDesc>. For a training set (x 1 , y 1 )...(x n , y n ) with labels y i in[1..k], it finds the solution of the</figDesc><table coords="4,47.08,629.77,182.78,70.59"><row><cell cols="3">following optimization problem during training:</cell></row><row><cell>min1/2</cell><cell>i=1..k w i  *  w i + C/n</cell><cell>i=1..n ξ i</cell></row><row><cell cols="2">s.t. for all y in [1..k] :</cell><cell></cell></row><row><cell>[x 1  *  w</cell><cell></cell><cell></cell></row></table><note coords="4,110.80,690.66,133.83,9.70"><p>yi ] &gt;= [x 1 * w y ] + 100 * (y i , y) -ξ 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,77.54,642.42,440.20,127.89"><head>Table 2</head><label>2</label><figDesc>Performance (Precision, Recall, F1 Score, and Accuracy; higher is better) and priority estimation error (mean squared error; lower is better) on TREC-IS 2018 test set for various experimental settings. The best results are highlighted in boldface.</figDesc><table coords="6,77.54,681.47,440.20,88.84"><row><cell></cell><cell></cell><cell cols="2">Multi-type (Macro)</cell><cell></cell><cell></cell><cell cols="2">Any-type (Micro)</cell><cell></cell><cell>Priority</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Estimation</cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell><cell>Accuracy</cell><cell>Error</cell></row><row><cell>KDEIS1 CLSTM</cell><cell>0.1388</cell><cell>0.0607</cell><cell>0.0620</cell><cell>0.8929</cell><cell>0.2575</cell><cell>0.9783</cell><cell>0.4077</cell><cell>0.2580</cell><cell>0.0842</cell></row><row><cell>KDEIS2 ACBLSTM</cell><cell>0.1512</cell><cell>0.0689</cell><cell>0.0703</cell><cell>0.8890</cell><cell>0.2089</cell><cell>0.9734</cell><cell>0.3440</cell><cell>0.2098</cell><cell>0.0842</cell></row><row><cell>KDEIS3 ACSBLSTM</cell><cell>0.1209</cell><cell>0.0577</cell><cell>0.0482</cell><cell>0.8933</cell><cell>0.2630</cell><cell>0.9788</cell><cell>0.4147</cell><cell>0.2635</cell><cell>0.0842</cell></row><row><cell>KDEIS4 DM</cell><cell>0.1482</cell><cell>0.0708</cell><cell>0.0734</cell><cell>0.9035</cell><cell>0.3914</cell><cell>0.9856</cell><cell>0.5603</cell><cell>0.3908</cell><cell>0.0842</cell></row><row><cell>Participant Median</cell><cell>0.1827</cell><cell>0.0784</cell><cell>0.0825</cell><cell>0.8993</cell><cell>0.3978</cell><cell>0.6164</cell><cell>0.4775</cell><cell>0.3385</cell><cell>0.0933</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,47.08,394.77,239.22,123.27"><head>Table 3</head><label>3</label><figDesc>Top 5 performing systems (Precision, Recall, F1 Score, and Accuracy; higher is better) in TREC-IS 2018. Boldfaced one is our proposed system.</figDesc><table coords="7,64.55,435.66,204.28,82.37"><row><cell></cell><cell></cell><cell cols="2">Any-type (Micro)</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell><cell>Accuracy</cell></row><row><cell>cbnuS2</cell><cell>0.4559</cell><cell>0.7780</cell><cell>0.5749</cell><cell>0.4213</cell></row><row><cell>KDEIS4 DM</cell><cell>0.3914</cell><cell>0.9856</cell><cell>0.5603</cell><cell>0.3908</cell></row><row><cell>umdhcilfasttext</cell><cell>0.4534</cell><cell>0.7260</cell><cell>0.5582</cell><cell>0.4022</cell></row><row><cell>cbnuS1</cell><cell>0.4472</cell><cell>0.7402</cell><cell>0.5575</cell><cell>0.4064</cell></row><row><cell>NHK run2</cell><cell>0.4483</cell><cell>0.7143</cell><cell>0.5509</cell><cell>0.3997</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research was supported by <rs type="funder">MEXT KAKENHI</rs>, <rs type="grantName">Grant-in-Aid for Scientific Research</rs> (B), Grant Number <rs type="grantNumber">17H01746</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YFrS5YC">
					<idno type="grant-number">17H01746</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,330.83,134.01,217.37,6.96;7,330.83,142.51,217.37,6.96;7,330.83,151.02,217.36,6.96;7,330.83,159.52,84.89,6.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,460.44,134.01,87.76,6.96;7,330.83,142.51,194.32,6.96">Tweet analysis for real-time event detection and earthquake reporting system development</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,531.31,142.57,16.89,6.81;7,330.83,151.07,186.34,6.81">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="919" to="931" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,168.73,217.37,6.96;7,330.83,177.24,217.37,6.96;7,330.83,185.74,107.76,6.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,496.23,168.73,51.97,6.96;7,330.83,177.24,83.32,6.96">Overview of the trec-2011 microblog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,420.84,177.29,127.36,6.81;7,330.83,185.74,83.08,6.96">Proceedings of the 20th Text REtrieval Conference (TREC), NIST</title>
		<meeting>the 20th Text REtrieval Conference (TREC), NIST</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,194.95,217.37,6.96;7,330.83,203.46,217.37,6.96;7,330.83,211.96,217.37,6.96;7,330.83,220.46,22.73,6.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,472.73,194.95,75.47,6.96;7,330.83,203.46,217.37,6.96;7,330.83,212.01,142.90,6.81">Stance detection on microblog focusing on syntactic tree representation, International Conference on Data Mining and Big Data (DMBD)</title>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">A</forename><surname>Siddiqua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Chy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="478" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,229.68,217.37,6.96;7,330.83,238.18,217.36,6.96;7,330.83,246.74,217.37,6.81;7,330.83,255.24,217.37,6.81;7,330.83,263.69,217.37,6.96;7,330.83,272.19,36.38,6.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,454.68,229.68,93.52,6.96;7,330.83,238.18,128.60,6.96">Automatically constructing a normalisation dictionary for microblogs</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,467.04,238.23,81.15,6.81;7,330.83,246.74,217.37,6.81;7,330.83,255.24,217.37,6.81;7,330.83,263.74,23.58,6.81">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,281.41,217.37,6.96;7,330.83,289.91,217.36,6.96;7,330.83,298.47,217.37,6.81;7,330.83,306.92,147.32,6.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,432.04,281.41,116.16,6.96;7,330.83,289.91,92.95,6.96">A broad-coverage normalization system for social media language</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,429.20,289.96,119.00,6.81;7,330.83,298.47,217.37,6.81;7,330.83,306.97,21.66,6.81">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,316.13,217.37,6.96;7,330.83,324.63,141.75,6.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<title level="m" coord="7,489.18,316.18,59.01,6.81;7,330.83,324.69,24.45,6.81">Mining of massive datasets</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,333.85,217.37,6.96;7,330.83,342.35,217.37,6.96;7,330.83,350.85,168.48,6.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,520.90,333.85,27.30,6.96;7,330.83,342.35,196.48,6.96">Okapi at TREC-7: automatic ad hoc, filtering, VLC and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,532.17,342.40,16.03,6.81;7,330.83,350.91,70.31,6.81">NIST Special Publication SP</title>
		<imprint>
			<biblScope unit="issue">500</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,360.07,217.37,6.96;7,330.83,368.57,217.37,6.96;7,330.83,377.13,217.36,6.81;7,330.83,385.58,85.01,6.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,429.76,360.07,118.44,6.96;7,330.83,368.57,57.40,6.96">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,392.88,368.62,155.32,6.81;7,330.83,377.13,217.36,6.81;7,330.83,385.58,14.55,6.96">Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval, ACM</title>
		<meeting>the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval, ACM</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,394.79,217.37,6.96;7,330.83,403.29,217.37,6.96;7,330.83,411.85,217.37,6.81;7,330.83,420.30,217.37,6.96;7,330.83,428.80,63.67,6.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,473.99,394.79,74.21,6.96;7,330.83,403.29,213.98,6.96">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,330.83,411.85,217.37,6.81;7,330.83,420.35,75.05,6.81">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL)</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,438.02,217.37,6.96;7,330.83,446.52,217.37,6.96;7,330.83,455.02,217.37,6.96;7,330.83,463.53,57.94,6.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,378.02,446.52,170.18,6.96;7,330.83,455.02,88.91,6.96">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics (ACL</note>
</biblStruct>

<biblStruct coords="7,330.83,472.74,217.37,6.96;7,330.83,481.24,217.37,6.96;7,330.83,489.75,217.36,6.96;7,330.83,498.25,22.73,6.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,476.74,472.74,71.46,6.96;7,330.83,481.24,78.21,6.96">Sentiment strength detection for the social web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,415.11,481.30,133.10,6.81;7,330.83,489.80,126.37,6.81">Journal of the American Society for Information Science and Technology (JASIST)</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,507.46,217.37,6.96;7,330.83,515.97,217.37,6.96;7,330.83,524.47,217.36,6.96;7,330.83,532.98,217.36,6.96;7,330.83,541.48,51.97,6.96" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,472.25,507.46,75.95,6.96;7,330.83,515.97,217.37,6.96;7,330.83,524.47,217.36,6.96;7,330.83,533.03,178.10,6.81">Combining a rule-based classifier with ensemble of feature sets and machine learning techniques for sentiment analysis on microblog, 19th International Conference on Computer and Information Technology (ICCIT</title>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">A</forename><surname>Siddiqua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Chy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="304" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,550.69,217.37,6.96;7,330.83,559.20,217.37,6.96;7,330.83,567.70,217.37,6.96;7,330.83,576.20,123.78,6.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,533.47,550.69,14.73,6.96;7,330.83,559.20,217.37,6.96;7,330.83,567.70,19.11,6.96">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,355.16,567.75,193.04,6.81;7,330.83,576.26,52.63,6.81">Proceedings of the 21st International Conference on Machine Learning (ICML)</title>
		<meeting>the 21st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,585.42,217.37,6.96;7,330.83,593.92,214.90,6.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,426.98,585.42,121.22,6.96;7,330.83,593.92,53.67,6.96">On the algorithmic implementation of multi-class SVMs</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Krammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,390.83,593.97,65.39,6.81">Proceedings of JMLR</title>
		<meeting>JMLR</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="265" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,603.13,217.37,6.96;7,330.83,611.64,217.37,6.96;7,330.83,620.14,174.96,6.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,488.21,603.13,59.99,6.96;7,330.83,611.64,94.09,6.96">A C-LSTM neural network for text classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
		</author>
		<idno>abs/1511.08630</idno>
		<ptr target="http://arxiv.org/abs/1511.08630" />
	</analytic>
	<monogr>
		<title level="j" coord="7,430.67,611.69,16.46,6.81">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,629.35,217.37,6.96;7,330.83,637.86,217.37,6.96;7,330.83,646.41,217.37,6.81;7,330.83,654.86,102.65,6.96" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<title level="m" coord="7,330.83,637.86,217.37,6.96;7,330.83,646.41,217.37,6.81;7,330.83,654.92,34.22,6.81">Advances in Pre-Training Distributed Word Representations, Proceedings of the International Conference on Language Resources and Evaluation</title>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.83,664.08,217.37,6.96;7,330.83,672.58,124.23,6.96" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="7,405.66,664.08,139.36,6.96">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,330.83,681.79,217.37,6.96;7,330.83,690.30,217.36,6.96;7,330.83,698.80,81.64,6.96" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="7,432.41,681.79,115.79,6.96;7,330.83,690.30,163.15,6.96">Feed-forward networks with attention can solve some long-term memory problems</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08756</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,330.83,708.01,217.37,6.96;7,330.83,716.51,217.37,6.96;7,330.83,725.02,217.37,6.96;7,330.83,733.52,84.61,6.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,439.84,708.01,108.36,6.96;7,330.83,716.51,217.37,6.96;7,330.83,725.02,78.81,6.96">YNU-HPCC at IJCNLP-2017 Task 4: Attention-based Bi-directional GRU Model for Customer Feedback Analysis Task of English</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,415.68,725.07,104.39,6.81">Proceedings of the IJCNLP 2017</title>
		<meeting>the IJCNLP 2017</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="174" to="179" />
		</imprint>
	</monogr>
	<note>Shared Tasks</note>
</biblStruct>

<biblStruct coords="7,330.83,742.73,217.37,6.96;7,330.83,751.24,217.37,6.96;7,330.83,759.74,217.36,6.96;7,330.83,768.25,199.05,6.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="7,536.94,742.73,11.26,6.96;7,330.83,751.24,217.37,6.96;7,330.83,759.74,140.33,6.96">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,476.54,759.80,71.66,6.81;7,330.83,768.30,141.66,6.81">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
