<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,115.96,345.83,12.62;1,225.45,133.89,164.45,12.62">The University of Padua IMS Research Group at CENTRE@TREC 2018</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,210.09,172.03,74.99,8.74"><forename type="first">Giorgio</forename><forename type="middle">Maria</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,325.20,172.03,80.07,8.74"><forename type="first">Stefano</forename><surname>Marchesin</surname></persName>
							<email>stefano.marchesin@unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,115.96,345.83,12.62;1,225.45,133.89,164.45,12.62">The University of Padua IMS Research Group at CENTRE@TREC 2018</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A227BAA733E3152779427155F7758A2F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Precision medicine</term>
					<term>query expansion</term>
					<term>reproducibility</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our participation in one of the tasks of the CENTRE@TREC 2018 Track: the Clinical Decision Support task. We describe the steps of the original paper we wanted to reproduce, identifying the elements of ambiguity that may affect the reproducibility of the results. The experimental results we obtained follow a similar trend to those presented in the original paper: using clinical trials' "note" field decreases the retrieval performances significantly, while the pseudo-relevance feedback approach together with query expansion achieves the best results across different measures. In the experimental results we find out that the choice of the stoplist is fundamental to achieve a reasonable level of reproducibility. However, stoplist creation is not described sufficiently well in the original paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CENTRE@TREC challenges TREC participants to reproduce the results of the most interesting systems submitted in previous editions of TREC. Track organizers select these top performing experiments and each participating group is challenged to replicate and/or reproduce one or more of the selected systems by only using standard open source IR systems <ref type="bibr" coords="1,343.96,524.14,9.96,8.74" target="#b3">[4]</ref>.</p><p>In 2018, CENTRE@TREC had three tasks: replicating runs of the TREC 2016 Clinical Decision Support track; replicating runs of the TREC 2013 and TREC 2014 Web track. In this paper, we focus on the first task: the retrieval of biomedical articles relevant for answering clinical questions based on patient records. The target document collection contains 1,251,954 full-text articles, a snapshot of the Open Access Subset of PubMed Central (PMC) from March 28, 2016. The list of topics comprises 30 clinical notes with the following structure: summary, description and note. The paper to reproduce combines pseudorelevance feedback and semantic query expansion <ref type="bibr" coords="1,359.59,632.21,9.96,8.74" target="#b2">[3]</ref>. In our experiments, we were able to partially reproduce three of the four runs described in the original paper.</p><p>This paper is organized as follows. Section 2 lists elements of the original paper that may be ambiguous or not clear from an experimental point of view. Section 3 describes our experimental setting and the choices that we made in order to reproduce the results. Section 4 analyzes and compares the results of the original paper with the results of our runs. Finally, Section 5 concludes the paper with some general comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Analysis of the Methods</head><p>Below, we analyze the description of the model components from the original paper. First, a quote from the paper is reported to highlight the component. Then, a description of how we reproduced -or we tried to reproduce -the component follows.</p><p>The document collection for 2016 consists of 1,251,954 PMC articles in NXML format that were indexed with Solr (version 5.5.2).</p><p>In our experiments, we used the latest version of Solr (version 7.5.0). One thing that is unclear is whether the authors have used the Data Import Handler (DIH) of Solr to import and index the content. In our case, we decided to parse the collection into a Solr XML data structure that can be used with a curl command to update the index.</p><p>Fields used for indexing were PMCID, title, abstract, body, conclusion, journal title and journal type. Indexing was performed with standard Solr settings which include tokenization, stemming and stopword removal. A master stopword list was constructed which is a combination of standard English stopwords and a list of stopwords constructed manually based on most frequent terms occurring in the document collection. This is a crucial point in the description of the model. There is no field named "PMCID". We believe that the "pmc" field of the article metadata is the correct mapping. Besides, the most problematic field is "conclusion" as there is no explicit field named conclusion. Therefore, in our experiments, we used pmc, title, abstract, body, journal title and journal type as fields.</p><p>Furthermore, the lack of a detailed description for the stoplist used makes the reproduction of the results extremely difficult. According to <ref type="bibr" coords="2,391.39,560.54,9.96,8.74" target="#b1">[2]</ref>, "the most prominent effects are those of stop lists and IR models, as well as their interactions, while stemmers and n-grams play a smaller role". Following this consideration, we used three 'standard' stoplists (described in the experimental section) and we tried to find the optimal number of 'most frequent terms' that were manually added to the list.</p><p>Okapi BM25 was the similarity measure used for querying and retrieval from the index.</p><p>We used the BM25 model implemented in Solr 7.5.0. According to the Solr documentation, the implementation of the BM25 model for version 5.5.2 (original paper) and version 7.5.0 (our implementation) is the same.</p><p>For query expansion, the authors of the original paper used MetaMap <ref type="bibr" coords="3,458.17,154.86,9.96,8.74" target="#b0">[1]</ref>:</p><p>The MetaMap program was applied for the identification of UMLS concepts in topics. Since UMLS has over 100 semantic types, mappings were restricted to only the following semantic types [...] For all the mapped concepts, synonyms were extracted from UMLS knowledge sources which were thereafter used for query expansion.</p><p>We used MetaMap in the same way described in the original paper, Finally, the description of the Pseudo-Relevance Feedback approach is as follows:</p><p>For a given query, Pseudo-Relevance Feedback (PRF) was implemented to collect words from titles of top k retrieved documents. Stopwords were eliminated and the remaining words were added to the initial query in order to generate a new query that can be reused for searching and retrieval. [...] Although, the value of k had only minor impact on retrieval performance, k = 30 was chosen which was observed to deliver the best results. For the runs that leveraged on UMLS query expansion, terms in the topic after stopword removal in combination with UMLS terms (i.e. expanded queries) were used as initial query.</p><p>The paper is clear on the number of k documents to use for PRF. However, the lack of description for the stoplist used -especially regarding the manual selection of frequent medical terms -makes it hard to replicate the performances of PRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setting</head><p>We tried to reproduce the following steps from the original paper: pre-processing, indexing and query expansion. A description of each step is presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-processing</head><p>In order to parse the collection of documents, we created an XML file containing the schema used by the update handler of Solr for adding documents to the index. The structure of the file is shown in Listing 1.1, and the code used to produce all the XML files is openly available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing</head><p>For indexing, we tried three different stoplists: the default Solr stoplist (33 terms), the SMART stoplist (571 terms)<ref type="foot" coords="4,311.29,304.12,3.97,6.12" target="#foot_1">2</ref> , the SMART stoplist together with a list of the 300 most frequent words<ref type="foot" coords="4,284.61,316.07,3.97,6.12" target="#foot_2">3</ref> in the collection . The union of the SMART stoplist with the most frequent words produced a stoplist of 731 words. We name the three variants in the experimental results as: Lucene, SMART, and SMART+.</p><p>The stoplists can be found at the GitHub repository. <ref type="foot" coords="4,380.30,363.89,3.97,6.12" target="#foot_3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Expansion</head><p>For query expansion, we followed the steps described in the paper. We used MetaMap<ref type="foot" coords="4,180.40,421.28,3.97,6.12" target="#foot_4">5</ref> to identify UMLS concepts for each query and for each title of the first 30 documents retrieved. First, we restricted the mappings of UMLS concepts to only those semantic types listed in <ref type="bibr" coords="4,301.99,446.76,9.96,8.74" target="#b2">[3]</ref>. Then, we extracted all the synonyms of the matched concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparison of Results</head><p>In this section, we analyze the results obtained with our experiments by comparing the impact of the three stoplists with the original runs. We replicated three out of four runs of the original paper: MRKPrfNote, MRKSumCln, MRKUmlsSolr.</p><p>In Tables <ref type="table" coords="4,178.55,545.99,9.96,8.74" target="#tab_1">1a</ref> and<ref type="table" coords="4,211.22,545.99,8.86,8.74" target="#tab_1">1b</ref>, we report the values of the infNDCG and infAP measures obtained in <ref type="bibr" coords="4,187.41,557.94,9.96,8.74" target="#b2">[3]</ref>, along with the performances of each of our three runs based on the different stoplists (i.e. Lucene, SMART and SMART+). We also add the performance of each run in terms of Rprec in Table <ref type="table" coords="4,362.58,581.85,8.12,8.74" target="#tab_1">1c</ref>.</p><p>The results we obtained are close to those reported in the original paper. Regardless of the stoplist used, we found that: • Using the "note" field of clinical trials decreases the performance significantly (MRKPrfNote run); • The query expansion-based approach (MRKSumCln run) increases performance significantly over the plain BM25 approach (the values of the performance for plain BM25 are not presented in the original paper nor in this paper); • The PRF approach (MRKUmlsSolr run) increases the performance over the simple query expansion approach (MRKSumCln run).</p><p>A deeper analysis shows that we were not able to match the scores of the original paper. However, by increasing the size of the stoplist we were able to get closer and closer to the values reported in the original paper.</p><p>In Figure <ref type="figure" coords="5,193.85,560.48,3.87,8.74" target="#fig_1">1</ref>, we show a topic by topic analysis of the difference between the reproduced run MRKUmlsSolr and the original one for the three performance measures. A negative value (red bars below zero) means that the original run performed better than the reproduced run. For most of the topics, the difference in the inferred measures, infNDCG in Figure <ref type="figure" coords="5,337.88,608.30,9.96,8.74" target="#fig_1">1a</ref> and infAP in 1b, is in favour of the original run. On the other hand, for RPrec we observe a more evenly distributed difference across topics. A paired t-test confirms that there is no statistical significant difference between the two runs (the reproduced run and the original one).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented the results of our participation in the CENTRE@TREC 2018 Track. Our objective was the reproduciblity of a paper aiming to retrieve biomedical articles relevant for answering clinical questions based on patient records. In particular, we wanted to reproduce the query expansion and pseudorelevance feedback components.</p><p>We analyzed the steps of the original paper we wanted to reproduce, and we described the elements of ambiguity that may affect the reproducibility of the results. We provided a detailed list of the choices we made for each step and, in particular, the selection of terms for the stoplist.</p><p>The experimental results we obtained follow a similar trend to those of the original paper: using the "note" field of clinical trials decreases the retrieval performances in a significant way, while pseudo-relevance feedback and query expansion obtain the best results across different measures.</p><p>We were able to get close to the performances of the best run reported in <ref type="bibr" coords="6,457.34,298.32,10.52,8.74" target="#b2">[3]</ref> by increasing the number of terms in the stoplist. The results showed that the best results for some performance measures (such as the inferred NDCG and inferred Average Precision) are more difficult to achieve compared to other measures (such as RPrec).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,267.37,284.75,80.62,7.86;7,274.44,474.04,66.48,7.86;7,274.36,663.33,66.63,7.86"><head></head><label></label><figDesc>(a) infNDCG values (b) infAP values (c) RPrec values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,694.23,345.82,8.74;7,134.77,706.18,160.34,8.74;7,203.93,494.41,207.50,163.39"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Topic by topic comparison of the difference of values for the MRKUmlsSolr reproduced run and the original one.</figDesc><graphic coords="7,203.93,494.41,207.50,163.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,117.78,345.82,255.03"><head>Table 1 :</head><label>1</label><figDesc>A comparison of different performance measures among the original runs and the reproduced runs with different stoplists.</figDesc><table coords="5,205.04,117.78,203.75,231.23"><row><cell></cell><cell cols="2">infNDCG</cell><cell></cell></row><row><cell>run</cell><cell cols="3">original Lucene SMART SMART+</cell></row><row><cell>MRKPrfNote</cell><cell>0.150 0.125</cell><cell>0.133</cell><cell>0.138</cell></row><row><cell>MRKSumCln</cell><cell>0.222 0.187</cell><cell>0.197</cell><cell>0.202</cell></row><row><cell cols="2">MRKUmlsSolr 0.226 0.190</cell><cell>0.205</cell><cell>0.199</cell></row><row><cell></cell><cell cols="2">(a) infNDCG values</cell><cell></cell></row><row><cell></cell><cell cols="2">infAP</cell><cell></cell></row><row><cell>run</cell><cell cols="3">original Lucene SMART SMART+</cell></row><row><cell>MRKPrfNote</cell><cell>0.018 0.012</cell><cell>0.014</cell><cell>0.014</cell></row><row><cell>MRKSumCln</cell><cell>0.027 0.021</cell><cell>0.023</cell><cell>0.024</cell></row><row><cell cols="2">MRKUmlsSolr 0.029 0.022</cell><cell>0.024</cell><cell>0.024</cell></row><row><cell></cell><cell>(b) infAP values</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">RPrec</cell><cell></cell></row><row><cell>run</cell><cell cols="3">original Lucene SMART SMART+</cell></row><row><cell>MRKPrfNote</cell><cell>0.102 0.084</cell><cell>0.093</cell><cell>0.093</cell></row><row><cell>MRKSumCln</cell><cell>0.156 0.144</cell><cell>0.147</cell><cell>0.147</cell></row><row><cell cols="2">MRKUmlsSolr 0.168 0.151</cell><cell>0.153</cell><cell>0.156</cell></row><row><cell></cell><cell>(c) RPrec values</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,657.44,184.08,7.47"><p>https://github.com/gmdn/CENTRE-TREC2018</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,624.57,202.42,7.47"><p>http://members.unine.ch/jacques.savoy/clef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,634.88,291.91,7.86"><p>We used the Solr Terms Component to extract the most frequent terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,646.48,184.08,7.47"><p>https://github.com/gmdn/CENTRE-TREC2018</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,657.44,127.10,7.47"><p>https://metamap.nlm.nih.gov</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,401.62,342.25,7.86;6,146.91,412.58,333.68,7.86;6,146.91,423.51,333.68,7.89;6,146.91,435.14,207.12,7.47" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,265.33,401.62,215.27,7.86;6,146.91,412.58,60.87,7.86">An overview of metamap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1136/jamia.2009.002733</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pubmed/20442139" />
	</analytic>
	<monogr>
		<title level="j" coord="6,214.59,412.58,266.00,7.86">Journal of the American Medical Informatics Association : JAMIA</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010-06">May-Jun 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,445.45,342.24,7.86;6,146.91,456.39,333.68,8.14;6,146.91,468.02,98.85,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,238.93,445.45,237.36,7.86">Toward an anatomy of IR system component performances</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Silvello</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.23910</idno>
		<ptr target="https://doi.org/10.1002/asi.23910" />
	</analytic>
	<monogr>
		<title level="j" coord="6,146.91,456.41,31.87,7.86">JASIST</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="200" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,478.33,342.25,7.86;6,146.91,489.29,333.68,7.86;6,146.91,500.25,333.68,7.86;6,146.91,511.21,333.68,8.12;6,146.91,522.81,75.81,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,415.95,478.33,64.65,7.86;6,146.91,489.29,231.38,7.86">Semi-supervised information retrieval system for clinical decision support</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Megaro</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec25/papers/MERCKKGAA-CL.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="6,401.36,489.29,79.23,7.86;6,146.91,500.25,166.68,7.86">Proceedings of The Twenty-Fifth Text REtrieval Conference</title>
		<meeting>The Twenty-Fifth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-15">2016. November 15-18, 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,533.12,342.24,7.86;6,146.91,544.08,333.68,7.86;6,146.91,555.04,320.19,8.12" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,279.52,533.12,163.04,7.86">Overview of the TREC 2018 centre trask</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/" />
	</analytic>
	<monogr>
		<title level="m" coord="6,463.04,533.12,17.56,7.86;6,146.91,544.08,236.66,7.86">Proceedings of The Twenty-Seventh Text REtrieval Conference</title>
		<meeting>The Twenty-Seventh Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">November 14-16, 2018 (2018</date>
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
