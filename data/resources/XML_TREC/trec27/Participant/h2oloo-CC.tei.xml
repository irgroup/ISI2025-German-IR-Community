<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,113.46,84.23,13.15,15.44;1,126.61,90.04,5.79,11.03;1,132.90,84.23,365.64,15.44;1,160.59,104.15,290.80,15.44">H 2 oloo at TREC 2018: Cross-Collection Relevance Transfer for the Common Core Track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,215.80,130.29,45.50,10.59"><forename type="first">Ruifan</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.42,130.29,47.90,10.59"><forename type="first">Yuhao</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,346.04,130.29,51.16,10.59"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.40,146.39,73.47,8.83"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,113.46,84.23,13.15,15.44;1,126.61,90.04,5.79,11.03;1,132.90,84.23,365.64,15.44;1,160.59,104.15,290.80,15.44">H 2 oloo at TREC 2018: Cross-Collection Relevance Transfer for the Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05B7A7DC9A51F9235021A8BEF6C65206</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The h2oloo team at the University of Waterloo participated in the Common Core Track in TREC 2018. Our main effort involved reproducing the cross-collection relevance transfer technique of Grossman and Cormack <ref type="bibr" coords="1,146.84,237.46,10.68,7.94" target="#b7">[8]</ref> from the TREC 2017 Common Core Track, as captured in their WCrobust0405 run. Their idea was relatively simple: for information needs (topics) that are shared across more than one test collection, it is possible to train (per topic) relevance classifiers using one or more test collections (in their case, from the TREC 2004 and 2005 Robust Tracks) and apply the classifiers to a new document collection (in their case, the New York Times collection used in the TREC 2017 Common Core Track) to improve ranking effectiveness. Each classifier, in essence, learns a relevance model for a specific information need, and the experiments of <ref type="bibr" coords="1,88.83,347.05,53.90,7.94">Grossman and</ref> Cormack demonstrate that such models can generalize across document collections.</p><p>According to the TREC 2017 Common Core Track overview paper <ref type="bibr" coords="1,67.34,379.93,9.27,7.94" target="#b1">[2]</ref>, WCrobust0405 ranked third in terms of average precision of runs that contributed to the judgment pools. The two runs that were more effective than WCrobust0405 involved humans who interactively searched the target collection to find relevant documents. In other words, the relevance transfer technique yielded effectiveness levels that approach human searchers.</p><p>Not only is the technique of Grossman and Cormack effective, it is also simple: According to their paper, a logistic regression classifier for each topic was trained on the union of relevance judgments from the TREC 2004 and 2005 Robust Tracks. Documents were represented using word-level tf-idf features and each logistic regression classifier was learned using Sofia-ML 1 and then applied to the entire Common Core collection. The top 10000 scoring documents (per topic), in decreasing order of classifier score, was submitted as the final run.</p><p>We set out to reproduce the work of Grossman and Cormack described above, but with three main differences:</p><p>• Reranking search results. Instead of applying relevance classifiers over the entire collection, we reranked only the top k = 10000 hits from an initial retrieval run. • Incorporating retrieval scores. In WCrobust0405, documents were simply sorted by their classifier scores. Since we were reranking an initial pool of candidate documents, it made sense to combine classifier scores with the retrieval scores, which we accomplished via linear interpolation. Our full reproduction effort is detailed in a forthcoming ECIR paper <ref type="bibr" coords="1,332.15,237.77,13.29,7.94" target="#b13">[14]</ref>. Our successful reimplementation was then applied to the Common Core Track in TREC 2018. Overall, our work involved a collaboration with the Anserini team, who participated in the CENTRE, Common Core, and News Tracks separately. Although the team composition had some overlap, work on expanding the capabilities of Anserini involved researchers beyond the University of Waterloo. Thus, those efforts are documented in a separate TREC report <ref type="bibr" coords="1,476.12,314.48,13.36,7.94" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RESULTS</head><p>We submitted a total of ten runs for the Common Core Track, based on the code developed for the reproduction effort described in Yu et al. <ref type="bibr" coords="1,327.74,372.45,13.23,7.94" target="#b13">[14]</ref>. Our reranking approach built on runs generated by Anserini. Of the 50 topics in the evaluation, 25 topics overlapped with topics from the TREC 2004 Robust Track (Robust04) and the TREC 2017 Common Core Track (Core17). Of those 25 topics, 15 overlapped with topics from the TREC 2005 Robust Track (Robust05). For these topics, we used all available relevance judgments from the previous test collections. For the remaining topics, we simply used the output of Anserini, unmodified.</p><p>The exact configurations of our submitted runs are shown in Table 1. There are three main degrees of freedom in our experimental design: First, we can vary the source of the candidate documents on which we apply our classifiers. This is shown in the column denoted "Base", where we use either BM25 with axiomatic semantic term matching <ref type="bibr" coords="1,372.83,514.91,9.23,7.94" target="#b4">[5,</ref><ref type="bibr" coords="1,384.16,514.91,11.47,7.94" target="#b9">10]</ref> or BM25 with RM3 <ref type="bibr" coords="1,467.60,514.91,9.37,7.94" target="#b0">[1]</ref>; additional details can be found in the Anserini overview paper <ref type="bibr" coords="1,474.08,525.87,13.49,7.94" target="#b12">[13]</ref>. In both cases, we retrieved the top k = 10000 documents from the collection. The second is the weight we place on the classifier score when integrating evidence with the retrieval score (via linear interpolation). This is shown in the column denoted "Weight". Details of weight tuning are described in Yu et al. <ref type="bibr" coords="1,409.67,580.67,13.36,7.94" target="#b13">[14]</ref>.</p><p>The third degree of freedom is the actual classifier that we deployed, shown in Table <ref type="table" coords="1,404.61,602.58,4.15,7.94" target="#tab_1">1</ref> under the column "Classifier". We experimented with different classifiers and ensembles, but in all cases each classifier was trained using all available data from Robust04, Robust05, and Core17, where the feature vectors are terms with tfidf weights. The feature space is shown under the column "Vocabulary": we tried using only the vocabulary of the Robust04 collection as well as the combined vocabulary of all three collections (All). For logistic regression, we actually experimented with two different configurations, what we call LR1 and LR2. The primary difference is that LR2 uses the so-called "balanced" mode in scikit-learn to automatically adjust class weights to be inversely proportional to class frequencies. The first three rows of Table <ref type="table" coords="2,172.35,338.80,4.25,7.94" target="#tab_1">1</ref> describe submissions that used only one classifier (LR2) to rerank the documents. The second block of the table (rows 4-8) describes submissions that deployed both LR1 and LR2 as part of a seven-classifier ensemble. The seven classifiers are as follows: LR1, LR2, SVM, gradient boosting trees, stochastic gradient descent classifier, stochastic gradient descent regressor, and ridge regressor. Finally, we tried a three-classifier ensemble, with only LR2, SVM, and gradient boosting trees; this is shown in the third block of the table (rows 8-10). For the ensembles, the score from each classifier is averaged and then interpolated with the original retrieval score.</p><p>For each configuration, Table <ref type="table" coords="2,170.05,459.35,4.09,7.94" target="#tab_1">1</ref> also shows effectiveness in terms of standard ranked retrieval metrics. The final column denotes whether or not the run contributed to the judgment pool. We see that classifier ensembles yield better effectiveness over using only logistic regression, but it is unclear whether the seven-classifier ensemble beats the three-classifier ensemble, since the scores are all quite close. Nevertheless, given the greater complexity of a seven-classifier ensemble, the three-classifier ensemble should be preferred. In terms of the initial retrieval, axiomatic semantic term matching and RM3 yield comparable end-to-end results, although we achieve a higher AP (by a small margin) with BM25 + RM3. We do not believe any firm conclusions can be drawn about the relative merits of these query expansion techniques due to the lack of parameter tuning.</p><p>For reference, the final block of Table <ref type="table" coords="2,211.05,612.77,4.25,7.94" target="#tab_1">1</ref> reports results from baseline Anserini runs: BM25, BM25 with axiomatic semantic term matching, and BM25 with RM3 (respectively). We see that relevance transfer leads to large gains in effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REPEATABILITY ANALYSIS</head><p>Given growing interest in reproducibility in information retrieval <ref type="bibr" coords="2,285.80,687.18,9.23,7.94" target="#b2">[3,</ref><ref type="bibr" coords="2,53.80,698.14,6.11,7.94" target="#b5">6,</ref><ref type="bibr" coords="2,62.15,698.14,6.11,7.94" target="#b6">7,</ref><ref type="bibr" coords="2,70.51,698.14,6.17,7.94" target="#b8">9]</ref>, here we document a case study that highlights some of the challenges researchers face today. To provide a common vocabulary, we adopt the definitions of the terms repeatable, replicable, and reproducible as follows, per recent ACM guidelines: <ref type="foot" coords="2,504.92,314.74,3.38,6.44" target="#foot_1">3</ref>• Repeatable: a researcher can reliably repeat her own computation.</p><p>• Replicable: an independent group can obtain the same result using the author's own artifacts. • Reproducible: an independent group can obtain the same result using artifacts which they develop completely independently.</p><p>Although one might imagine repeatability to be achievable in a fairly straightforward manner, in practice it is non-trivial and involves quite a number of complexities and nuances (nevermind replicability or reproducibility). The fundamental problem is that computational artifacts are always evolving, and even if they remains static, their execution environments can change in unanticipated ways. We detail our experiences below:</p><p>The deadline of the TREC 2018 Common Core Track was in August 2018, and our submitted runs were generated before then. Since Anserini is an open-source project, all code changes are publicly documented; however, code for relevance transfer was kept in a separate private repository. While it would have been possible to snapshot the code that generated our official submitted runs (e.g., by commit ids), unfortunately we did not do this.</p><p>The relevance transfer code was not contributed to the Anserini code repository until December 2018. However, the Anserini codebase itself had evolved from the summer, such that the contributed code targeted the most recent state of the codebase at the time (as opposed to the state of the code in August). At commit acf4c87 (dated 12/15/2018), when we were ready to repeat the previously submitted TREC runs, we obtained different results. This point is worth emphasizing:</p><p>We, as the authors, were unable to repeat the results of our own submitted runs! In other words, the exact state of the computational artifact that generated our official TREC runs had been lost forever and cannot Run AP (official) AP <ref type="bibr" coords="3,208.09,88.69,3.56,7.94">(</ref> be recreated. We did more rigorously document the relevance transfer code that was actually contributed to the Anserini repository, <ref type="foot" coords="3,290.33,182.01,3.38,6.44" target="#foot_2">4</ref>and in Table <ref type="table" coords="3,100.60,195.11,4.12,7.94" target="#tab_2">2</ref> we provide two points of comparison. Our attempts to recreate h2oloo_enrm30.6, our most effective submitted run, yielded AP shown in the third column, under 12/15. For reference, we also provide AP scores for the comparable baseline with only logistic regression, h2oloo_LR2_rm3. We see that, for reasons that we were not able to identify, the effectiveness improved-likely as the result of general improvements to the codebase. Shortly after incorporating the relevance transfer code, we upgraded the underlying Lucene dependency of Anserini from version 6.3 to version 7.6 (commit e71df7a, 12/18). This changed the effectiveness of our runs again, which is shown in the rightmost column in Table <ref type="table" coords="3,85.38,315.66,3.07,7.94" target="#tab_2">2</ref>. Effectiveness increased again (slightly).</p><p>Our story has a happy ending because in each case, improved. However, imagine the alternative where the effectiveness decreased. Would it have been "legitimate" (for example, from an ethical perspective) to report a result that is no longer achievable even though it had been gotten under some unknown conditions? By definition, an unrepeatable result is unscientific. We shudder to ponder how many results "enshrined" in the literature are not repeatable, but have gone unnoticed.</p><p>Although to some extent we are at fault for this state of affairs: for example, better record keeping could have allowed us to recover exactly the code that was used to submit the runs. However, even meticulous documentation efforts might not have been sufficient. For example, changes to underlying libraries such as scikit-learn might cause our runs to be non-repeatable. While it is possible to document and capture underlying libraries, where do we stop? In the context of neural question answering, Crane <ref type="bibr" coords="3,250.35,491.00,10.68,7.94" target="#b3">[4]</ref> recently documented a litany of details that complicate repeatability, down to compliance issues of math libraries with the IEEE 754 floating point specification. Increasingly low-level capture techniques (e.g., virtual environments, Docker, virtual machines, etc.) can address more and more of these issues, but at greater costs, slowing down progress. As an example, there are known hardware differences that affect floating point computations, <ref type="foot" coords="3,192.65,565.57,3.38,6.44" target="#foot_3">5</ref> and hence might introduce minor perturbations in the ranked lists that in turn yield small differences in scores. How do we deal with such issues? There is a tension between repeatability (no doubt desirable) and the pace of progress. The optimal balance is difficult to find.</p><p>The fundamental challenge is that computational artifacts and execution environments are never static. It's not merely a matter of "document everything", because "everything" involves an unreasonable number of variables. Of course, we only want to document the variables that have a direct bearing on the experiment at hand, but often we don't actually know what they are-since that's the point of the inquiry to begin with. Furthermore, variables can change between experimental trials unbeknownst to researchers (for example, a system-wide upgrade of a core library by an administrator). Repeatability, as it turns out, isn't trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>This report documents our experiences reproducing the crosscollection relevance transfer technique proposed by Grossman and Cormack <ref type="bibr" coords="3,352.58,199.35,10.43,7.94" target="#b7">[8]</ref> and then applying it in TREC 2018. Along the way, we identified repeatability challenges, highlighting issues that likely affect other researchers in the computational sciences. We hope that our experiences contribute lessons on "how to do good science" (both positive and negative).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,77.40,88.69,456.90,188.25"><head>Table 1 :</head><label>1</label><figDesc>The configuration and effectiveness of our submitted runs; results from Anserini provided for reference.</figDesc><table coords="2,81.11,88.69,447.53,168.36"><row><cell></cell><cell>Run</cell><cell>Base</cell><cell cols="2">Weight Classifier</cell><cell cols="2">Vocabulary AP</cell><cell>NDCG P@10</cell><cell>Pool</cell></row><row><cell>1</cell><cell cols="2">h2oloo_LR2AX0.6 BM25 + Ax</cell><cell>0.6</cell><cell>LR2</cell><cell>Robust04</cell><cell cols="2">0.3256 0.6145 0.5800 N</cell></row><row><cell>2</cell><cell>h2oloo_LR2_rm3</cell><cell cols="2">BM25 + RM3 0.6</cell><cell>LR2</cell><cell>Robust04</cell><cell cols="2">0.3273 0.6113 0.5800 N</cell></row><row><cell>3</cell><cell>h2oloo_LRax0.6</cell><cell>BM25 + Ax</cell><cell>0.6</cell><cell>LR2</cell><cell>All</cell><cell cols="2">0.3227 0.6123 0.5800 Y</cell></row><row><cell>4</cell><cell>h2oloo_e7ax0.6</cell><cell>BM25 + Ax</cell><cell>0.6</cell><cell cols="2">7 classifier ensemble All</cell><cell cols="2">0.3310 0.6215 0.5840 Y</cell></row><row><cell>5</cell><cell>h2oloo_e7ax0.7</cell><cell>BM25 + Ax</cell><cell>0.7</cell><cell cols="2">7 classifier ensemble All</cell><cell cols="2">0.3274 0.6209 0.5880 N</cell></row><row><cell>6</cell><cell cols="3">h2oloo_e7rm30.6 BM25 + RM3 0.6</cell><cell cols="2">7 classifier ensemble All</cell><cell cols="2">0.3333 0.6143 0.5820 N</cell></row><row><cell>7</cell><cell cols="3">h2oloo_e7rm30.7 BM25 + RM3 0.7</cell><cell cols="2">7 classifier ensemble All</cell><cell cols="2">0.3361 0.6177 0.5940 N</cell></row><row><cell>8</cell><cell>h2oloo_enax0.6</cell><cell>BM25 + Ax</cell><cell>0.6</cell><cell cols="2">3 classifier ensemble All</cell><cell cols="2">0.3341 0.6233 0.5860 Y</cell></row><row><cell>9</cell><cell>h2oloo_enax0.7</cell><cell>BM25 + Ax</cell><cell>0.7</cell><cell cols="2">3 classifier ensemble Robust04</cell><cell cols="2">0.3351 0.6218 0.5920 Y</cell></row><row><cell cols="4">10 h2oloo_enrm30.6 BM25 + RM3 0.6</cell><cell cols="2">3 classifier ensemble All</cell><cell cols="2">0.3382 0.6193 0.5920 Y</cell></row><row><cell></cell><cell>anserini_bm25</cell><cell>BM25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.2284 0.5064 0.4500 Y</cell></row><row><cell></cell><cell>anserini_ax</cell><cell>BM25 + Ax</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.2734 0.5582 0.4960 Y</cell></row><row><cell></cell><cell>anserini_rm3</cell><cell cols="2">BM25 + RM3 -</cell><cell>-</cell><cell>-</cell><cell cols="2">0.2680 0.5422 0.4680 Y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.50,88.69,240.73,65.20"><head>Table 2 :</head><label>2</label><figDesc>Results of our repeatability analysis, comparing our official submissions with code at two other points in time.</figDesc><table coords="3,64.39,88.69,216.82,34.35"><row><cell></cell><cell></cell><cell cols="2">12/15) AP (12/18)</cell></row><row><cell>h2oloo_LR2_rm3</cell><cell>0.3273</cell><cell>0.3539</cell><cell>0.3569</cell></row><row><cell cols="2">h2oloo_enrm30.6 0.3382</cell><cell>0.3652</cell><cell>0.3670</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="1,321.00,702.18,50.08,6.18"><p>http://anserini.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,321.00,701.65,191.74,6.18"><p>https://www.acm.org/publications/policies/artifact-review-badging</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,56.72,677.40,238.46,6.18;3,53.80,685.63,9.04,6.18"><p>https://github.com/castorini/Anserini/blob/master/docs/runbook-trec2018-h2oloo. md</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="3,56.84,694.04,217.95,6.18;3,53.80,702.26,116.67,6.18"><p>https://stackoverflow.com/questions/11832428/windows-intel-and-ios-armdifferences-in-floating-point-calculations</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC) of Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,333.39,322.20,225.58,6.18;3,333.16,330.17,226.22,6.18;3,333.39,338.14,224.81,6.18;3,333.39,346.08,173.15,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,425.73,338.14,120.17,6.18">UMass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName coords=""><forename type="first">Nasreen</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leah</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Courtney</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,333.39,346.08,135.79,6.23">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,354.08,225.99,6.18;3,333.39,362.05,224.81,6.18;3,333.39,369.99,173.15,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,502.03,354.08,57.35,6.18;3,333.39,362.05,66.20,6.18;3,423.00,362.05,123.95,6.18">Christophe Van Gysel, and Ellen Voorhees</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,333.39,369.99,135.79,6.23">Proceedings of the 26th Text REtrieval Conference</title>
		<meeting>the 26th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>TREC 2017 Common Core Track Overview</note>
</biblStruct>

<biblStruct coords="3,333.39,377.99,225.88,6.18;3,333.39,385.96,225.58,6.18;3,333.39,393.90,213.91,6.23" xml:id="b2">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,351.10,385.96,207.87,6.18;3,333.39,393.93,111.26,6.18">Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,401.90,225.99,6.18;3,333.39,409.84,224.81,6.23;3,333.39,417.81,134.63,6.23" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,386.21,401.90,173.17,6.18;3,333.39,409.87,135.75,6.18">Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results</title>
	</analytic>
	<monogr>
		<title level="j" coord="3,474.31,409.84,83.89,6.23;3,333.39,417.81,82.51,6.23">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="241" to="252" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>Matt Crane</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,425.81,225.99,6.18;3,333.39,433.75,224.81,6.23;3,333.39,441.72,224.81,6.23;3,332.97,449.69,122.17,6.23" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,440.33,425.81,119.05,6.18;3,333.39,433.78,92.78,6.18">Semantic Term Matching in Axiomatic Approaches to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,437.83,433.75,120.37,6.23;3,333.39,441.72,224.81,6.23;3,332.97,449.69,17.71,6.23">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,457.69,224.81,6.18;3,333.28,465.66,224.92,6.18;3,333.39,473.60,224.81,6.23;3,333.39,481.57,73.13,6.23" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,387.59,465.66,170.61,6.18;3,333.39,473.63,201.35,6.18">Increasing Reproducibility in IR: Findings from the Dagstuhl Seminar on &quot;Reproducibility of Data-Oriented Experiments in e-Science</title>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Lippold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,542.72,473.60,15.48,6.23;3,333.39,481.57,17.89,6.23">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="68" to="82" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,489.57,224.81,6.18;3,333.39,497.51,149.49,6.23" xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diane</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,434.95,489.57,123.25,6.18;3,333.39,497.51,97.49,6.23">SIGIR Initiative to Implement ACM Artifact Review and Badging. SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,505.51,224.81,6.18;3,333.06,513.45,225.84,6.23;3,333.39,521.42,164.07,6.23" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="3,494.57,505.51,63.63,6.18;3,333.06,513.48,202.17,6.18">MRG_UWaterloo and WaterlooCormack Participation in the TREC 2017 Common Core Track</title>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,547.53,513.45,11.36,6.23;3,333.39,521.42,126.71,6.23">Proceedings of the 26th Text REtrieval Conference</title>
		<meeting>the 26th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,529.42,225.58,6.18;3,333.28,537.39,226.10,6.18;3,333.13,545.36,225.07,6.18;3,333.39,553.31,225.88,6.23;3,333.39,561.30,61.23,6.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,549.63,537.39,9.75,6.18;3,333.13,545.36,214.59,6.18">Toward Reproducible Baselines: The Open-Source IR Reproducibility Challenge</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ishan</forename><surname>Chattopadhyaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grant</forename><surname>Ingersoll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,333.39,553.31,190.72,6.23">Proceedings of the 38th European Conference on Information Retrieval</title>
		<meeting>the 38th European Conference on Information Retrieval<address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="408" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,569.27,224.81,6.18;3,333.15,577.22,225.05,6.23;3,332.97,585.19,35.61,6.23" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="3,434.12,569.27,124.08,6.18;3,333.15,577.24,73.45,6.18">Evaluating the Effectiveness of Axiomatic Approaches in Web Track</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,419.07,577.22,139.13,6.23">Proceedings of the 22nd Text REtrieval Conference</title>
		<meeting>the 22nd Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,593.18,224.81,6.18;3,333.39,601.13,224.81,6.23;3,333.39,609.10,224.81,6.23;3,333.39,617.07,135.21,6.23" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="3,470.21,593.18,87.99,6.18;3,333.39,601.15,123.07,6.18">Anserini: Enabling the Use of Lucene for Information Retrieval Research</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,469.73,601.13,88.47,6.23;3,333.39,609.10,224.81,6.23;3,333.39,617.07,24.51,6.23">Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,625.06,224.81,6.18;3,333.39,633.01,225.58,6.23;3,333.15,641.00,29.24,6.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="3,465.07,625.06,93.13,6.18;3,333.39,633.03,66.78,6.18">Anserini: Reproducible Ranking Baselines Using Lucene</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,406.45,633.01,115.97,6.23">Journal of Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,648.97,224.81,6.18;3,333.39,656.92,224.81,6.23;3,333.39,664.89,16.10,6.23" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="3,431.99,648.97,126.21,6.18;3,333.39,656.94,62.53,6.18">Anserini at TREC 2018: CENTRE, Common Core, and News Tracks</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,407.53,656.92,131.77,6.23">Proceedings of the 27th Text REtrieval Conference</title>
		<meeting>the 27th Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,333.39,672.89,225.99,6.18;3,333.39,680.83,224.81,6.23;3,333.39,688.80,159.63,6.23" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="3,471.50,672.89,87.88,6.18;3,333.39,680.86,87.40,6.18">Simple Techniques for Cross-Collection Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">Ruifan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,433.36,680.83,124.84,6.23;3,333.39,688.80,67.73,6.23">Proceedings of the 41th European Conference on Information Retrieval</title>
		<meeting>the 41th European Conference on Information Retrieval<address><addrLine>Cologne, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
