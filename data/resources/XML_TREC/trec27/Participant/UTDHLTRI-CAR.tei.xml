<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.46,71.55,384.64,12.90">UTD HLTRI at TREC 2018: Complex Answer Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.18,100.96,98.64,10.75"><forename type="first">Ramon</forename><surname>Maldonado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Human Language Technology Research Institute</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.08,100.96,107.29,10.75"><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Human Language Technology Research Institute</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.46,71.55,384.64,12.90">UTD HLTRI at TREC 2018: Complex Answer Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6DE4EE6F7C7A1EBB4AA002E14626DDD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Complex Question</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding answers to complex questions within a corpus of Wikipedia paragraphs needs to account for (a) the similarity between questions and paragraphs as well as (b) their shared semantics. In our participation in the 2018 TREC CAR track, we focused on developing a novel neural paragraph ranking model in our existing CAPAR system, developed for the 2017 TREC CAR track. The new system TRANS-CAPAR, takes advantage of the recently introduced Transformer architecture to encode information from the question and to semantically decode it in each paragraph. The results obtained during the official evaluations indicate that TRANS-CAPAR makes good use of both discourse context and similarity when ranking paragraphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our second year of participation in the Text REtrieval Conference Complex Answer Retrieval (TREC CAR) track we have focused only on the paragraph retrieval task, which enabled us to benefit from our Complex Answer PAragraph Retrieval (CAPAR) system designed for our participation in last year's TREC CAR track. Finding relevant paragraphs that answer a complex question is essential for satisfying the information need expressed by the complex question (CQ). Complex questions, as illustrated in Figure1, have the format of a title and a table of contents, with entries having a path using the words from the title, section and corresponding subsection. For example, the entry for subsection 2.5 in the complex question from Figure1 is represented by the path Analytics/Applications of analytics/Digital analytics. When complex questions are processed, de-Title:</p><p>Analytics Table <ref type="table" coords="1,343.34,241.34,9.09,9.46">of</ref>   ciding which paragraph is relevant for each entry of the CQ's Table of Contents (TOC) needs to take into account not only the path of the TOC entry, but also additional relevance and discourse signals. We believe that discourse cohesion and coherence contributes to the relevance of a paragraph to a CQ's TOC entry. For example, coreference, as a form of discourse cohesion, holds multiple paragraph "together" in answering an entry of the TOC. Moreover, coherence relations, such as elaboration, also account for the ability to inform the answer, and thus decide on the ranking of paragraphs.</p><p>As any possible cues to inter-paragraph cohesion and coherence are removed when an enormous "bag of paragraphs" are presented for selection and ranking for each entry of a complex question's TOC, we asked ourselves if discourse semantics alone could be used and thus use a more sophisticated neural pairwise ranker, operating between pairs of Wikipedia paragraphs would be capable of learning a ranking function that can account for similarity but also "pay attention" to patterns signaled from the discourse context of each paragraph. While many neural attention mechanisms are available, we chose to use the recently introduced Transformer architecture <ref type="bibr" coords="2,237.50,391.60,52.77,9.46;2,72.00,405.15,42.05,9.46" target="#b6">(Vaswani et al., 2017)</ref> as we believed that its multi-headed attention could capture the complex context patterns within paragraphs. The remainder of the paper is organized as follows: Section 2 presents the new architecture of our system, Section 3 provides results, which are analyzed in Section 4 while Section 5 summarizes the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Approach</head><p>The paragraph retrieval task of TREC CAR requires teams to retrieve a ranked list of paragraphs for each section of an article outline representing aspects of a complex topic. To address this task, we developed the TRANSformer Complex Answer PAragraph Retrieval (TRANS-CAPAR) system having the architecture illustrated in Figure <ref type="figure" coords="2,282.09,625.68,4.09,9.46" target="#fig_1">2</ref>. TRANS-CAPAR is an extension of our complex answer retrieval system from last year, CA-PAR <ref type="bibr" coords="2,96.12,666.33,111.98,9.46">(Maldonado et al., 2017)</ref> that leverages the Transformer architecture <ref type="bibr" coords="2,183.29,679.88,95.65,9.46" target="#b6">(Vaswani et al., 2017)</ref> to improve paragraph ranking. The TRANS-CAPAR architecture consists of the same five modules as CAPAR:</p><p>1. Query Processing Module. Modules 1-4 operate exactly as in the CA-PAR system <ref type="bibr" coords="2,366.88,643.31,112.11,9.46">(Maldonado et al., 2017)</ref>, however the Paragraph Ranking Module has been replaced with a novel neural relevance model that combines strategies that were shown to be successful last year with improved semantic relevance matching via the Transformer model. Therefore, we describe next only the new paragraph ranking module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paragraph Ranking</head><p>To create our new paragraph rankings, we trained The Siamese Transformer Network (STNet) for Pairwise Ranking, within a neural Learning-to-Rank (L2R) system. Given a section s and a set of paragraphs P , STNet produces a relevance score for each paragraph p ∈ P and the final rankings are produced by sorting P by relevance score. Since the L2R system is the same as in our CA-PAR system <ref type="bibr" coords="3,130.37,188.61,110.09,9.46">(Maldonado et al., 2017)</ref>, we detail next only the Siamese Transformer Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Siamese Transformer Network for Pairwise Ranking</head><p>The Siamese Transformer Network (STNet) for Pairwise Ranking is a deep neural network that learns to rank pairs of paragraphs according to their relevance to a Wikipedia article section. In order to produce a paragraph ranking, STNet learns to calculate a relevance score for a query expressed as a section heading path against paragraphs that are relevant to it. STNet is a neural architecture using in a novel way an encoder, a decoder and an attention mechanism. Typically, encoder-decoder architectures using attention are used in sequence-to-sequence models to translate an input sequence into an output sequence. Instead of translating an input sequence into an output sequence, we cast the encoder as having the role of capturing a representation of the words used in the section heading path while the decoder generates a semantic representation of how well a paragraph matches the query. The encoder-decoder architecture is implemented as a Transformer <ref type="bibr" coords="3,188.13,537.40,97.45,9.46" target="#b6">(Vaswani et al., 2017)</ref>, which uses both self-attention and traditional attention to (a) generate contextualized representations of both the query and paragraph and (b) semantically match words from the query to words from the paragraph.</p><p>In addition to the Transformer, we have used (1) a feature encoder to capture dynamic features between the query and paragraph and (2) a neural similarity module that implements PACRR <ref type="bibr" coords="3,270.27,661.02,20.00,9.46;3,72.00,674.57,54.12,9.46" target="#b2">(Hui et al., 2017)</ref> based on the top-performing system from the 2017 TREC CAR evaluation <ref type="bibr" coords="3,255.74,688.12,28.77,9.46;3,72.00,701.67,83.03,9.46" target="#b4">(MacAvaney et al., 2018)</ref>. We hypothesized that combining a similarity-matrix-based approach with the sophisticated representation provided by a the Transformer Seq2seq model should improve results as the two approaches should encode com-plimentary information from the input text.</p><p>Given the text and features of two paragraphs along with the text of a section heading path, STNet determines which of the two paragraphs is more relevant to the section using the following five main components illustrated in Figure <ref type="figure" coords="3,494.72,134.82,12.36,9.46" target="#fig_3">3(a</ref> STNet is a Siamese network that shares weights among the Feature Encoders, Neural Similarity Modules, Transformer Encoders and Decoders, and Scorers for both input paragraphs. We trained the network by optimizing the pairwise hinge loss between the scores from the two halves of the Siamese network:</p><formula xml:id="formula_0" coords="3,307.28,671.34,225.73,10.63">l(s, p 1 , p 2 ) = [1+τ (p 1 , p 2 ) (φ (s, p 1 ) -φ (s, p 2 ))] +</formula><p>(1) where [•] + is the hinge loss and</p><formula xml:id="formula_1" coords="3,308.56,725.84,214.01,26.07">τ (p 1 , p 2 ) = 1 if p 1 is more relevant than p 2 -1 otherwise.</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max Pooling</head><p>Transformer Encoder STNet was trained on the train-v2.0 dataset, using the test200.v2.0 dataset for validation. We used the same training regime as described in our previous work <ref type="bibr" coords="4,351.19,747.96,109.78,9.46">(Maldonado et al., 2017)</ref> to create pair-wise training data from train-v2.0 using pairs of articles with differing relevance values. To train the model we used the Adam Optimizer (Kingma and Ba, 2014) with default parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Details of the STNet</head><p>The Transformer Encoder. In order to facilitate the semantic soft-matching between words from a section heading path and words from a paragraph, the Transformer Encoder, depicted in i . However, it is not sufficient to use the GloVe embedding as a representation for the words in a section heading path since the same word may have a completely different meaning depending on its context (e.g. the word "turtle" in the Wikipedia articles "Sea Turtle" and "Turtle Ship"). Moreover, if a word is from the article title of a section heading path, it plays a different role when determining relevance than if it were from a section heading. To capture this sort of context information in our section path encoding, we use three separate Transformer Encoders <ref type="bibr" coords="5,238.39,476.33,51.88,9.46;5,72.00,489.88,42.08,9.46" target="#b6">(Vaswani et al., 2017)</ref> to produce an encoding of each section heading word that takes context information into account using self-attention. We train three separate Transformer Encoders to learn separate encodings of words from (1) a section heading; (2) an intermediate heading; and (3) an article title.</p><p>Each Transformer Encoder learns a contextualized encoding, o s i , for each word, w s i using a stack of 6 Transformer Encoder Blocks, T E k for k ∈ [1, . . . , 6]. The output of the k th block for the i th word, denoted as ebl k i , is given by:</p><formula xml:id="formula_2" coords="5,75.43,649.41,214.84,62.76">ebl k i = ebl k-1 i + T E k (ebl k-1 ) i (3) T E K (ebl k-1 ) = LN (F F N ( Ô) + Ô) (4) Ô = LN (M HS(ebl k-1 ) + ebl k-1 ) (5)</formula><p>where LN (•) is the Layer Normalization function <ref type="bibr" coords="5,92.51,742.31,71.24,9.46" target="#b0">(Ba et al., 2016)</ref>, F F N (•) is a feed-forward network and M HS(•) is MultiHead self-attention.</p><p>The feed-forward network, F F N , is a simple twolayer network comprised of ReLU and linear units:</p><formula xml:id="formula_3" coords="5,319.07,101.49,206.47,15.42">F F N (x) = W f 1 ReLU (W f 2 x + b f 2 ) + b f 1 (6)</formula><p>where W f 1 , W f 2 ∈ R n×n are weight matrices and b f 1 , b f 2 , ∈ R n are bias vectors. The MultiHead selfattention function, M HS(•), allows the model to jointly attend to information from different representation subspaces at different positions of a section heading path. This is accomplished by computing, in parallel, h different self-attention functions -referred to as attention heads -and combining the results of each head as follows:</p><p>M HS(X) = [head 1 , . . . , head h ]W M (7)</p><formula xml:id="formula_4" coords="5,324.15,281.35,201.40,44.27">head i = Attn(XW Q i , XW K i , XW V i ) (8) Attn(Q, K, V ) = softmax QK √ n V<label>(9)</label></formula><p>where</p><formula xml:id="formula_5" coords="5,307.28,337.82,218.27,27.55">W M ∈ R n×n , W Q i , W K i , W V i ∈ R n× n h for i ∈ [1, . . . , h] are weight matrices, [•]</formula><p>is the concatenation operation, and n is hidden size of the model. In this work, we use h = 10 attention heads.</p><p>The final encoding of word w s i is given by o s i = ebl 6 i and the input to the encoder blocks is given by the word embeddings combined with a positional mask, ebl 0 i = e(w s i ) + P E i , where:</p><formula xml:id="formula_6" coords="5,321.50,472.97,204.05,38.93">P E ij =    sin i 10000 j/n if j is even cos i 10000 j/n if j is odd (10)</formula><p>The final section path encoding is produced by concatenating the encodings of (1) the section heading, o s 1 , (2) the intermediate headings, o s 2 , and (3) the article title, o s 3 :</p><formula xml:id="formula_7" coords="5,429.95,563.87,84.29,11.52">o s = [o s 1 , o s 2 , o s 3 ].</formula><p>The Transformer Decoder. The role of the Transformer Decoder, illustrated in Figure <ref type="figure" coords="5,470.95,593.27,4.34,9.46" target="#fig_3">3</ref>.c is to perform a semantic soft-matching between each word from the paragraph text, p, and each word from the section heading path, s, using attention and to represent this soft matching into a single vector, a(s, p). The Transformer Decoder makes use of attention to determine which pairs of words help indicate relevance between a section heading path and a paragraph, even if the words are not the same (i.e. a hard match). Moreover, self-attention over the paragraph words can be used to contextualize the representations of each word, as in the Transformer Encoder.</p><p>Like the Transformer Encoder, the Transformer Decoder is comprised of a stack of 6 identical blocks, T D k for k ∈ [1, ldots, 6], where the output of the k t h block for the i t h paragraph word, denoted as dbl k i , is given by:</p><formula xml:id="formula_8" coords="6,83.12,142.78,207.15,67.63">dbl k i = dbl k-1 i + T D k (dbl k ) i (11) T D k (dbl k-1 ) = LN (F F N ( Ô2 ) + Ô2 ) (12) Ô2 = LN (M HA( Ô1 , O s ) + Ô1 ) (13) Ô1 = LN (M HS(dbl k-1 ) + dbl k-1 ) (14)</formula><p>where LN (•), F F N (•), and M HS(•) correspond to Layer Normalization, feed-forward network, and MultiHead Self-attention as defined in the Transformer Encoder section, M HA(•, •) is Mul-tiHead Attention, and O s = o s : is a matrix comprised of the full section heading path encoding. MultiHead Attention computes an h attention weights over each pair of words from the section heading path and paragraph, one for each attention head. Like MultiHead Self-attention, M HA uses multiple attention heads to capture different aspects of the semantic matching in parallel. M HA is computed similarly to M HS, however it incorporates the section heading path encoding, O s : M HA(X, O s ) = [head 1 , . . . , head h ]W M (15)</p><formula xml:id="formula_9" coords="6,93.81,441.95,196.46,15.55">head i = Attn(XW Q i , XW K i , O s W V i ) (16)</formula><p>where where <ref type="bibr" coords="6,156.79,485.92,23.87,9.57">. . , h]</ref> are weight matrices and Attn(•) is defined in Equation ( <ref type="formula" coords="6,208.43,499.82,3.86,9.46" target="#formula_4">9</ref>). Again, as in the Transformer Encoder, the input to the first Transformer Decoder block is given by the word embeddings of the paragraph words combined with a positional mask: dbl 0 i = e(w p i ) + P E i where P E is defined in Equation <ref type="formula" coords="6,170.18,567.56,9.09,9.46">10</ref>.</p><formula xml:id="formula_10" coords="6,72.00,469.07,218.27,27.03">W M ∈ R n×n , W Q i , W K i , W V i ∈ R n× n h for i ∈ [1, .</formula><p>In order to create the semantic matching vector, a(s, p), we combine the vectors produced by the final block of the Transformer Decoder using max pooling over the paragraph dimension and pass the pooled vector through a ReLU layer:</p><formula xml:id="formula_11" coords="6,115.86,657.43,174.41,28.84">P = maxpool 0 (dbl 6 : ) (17) a(s, p) = ReLU (W a P + b a )<label>(18)</label></formula><p>where W a ∈ R n×n is a weight matrix, b a ∈ R n is a bias vector, and maxpool 0 (•) denotes max pooling over the 0 dimension. The intuition behind max pooling over the paragraph dimension is to select the strongest relevance signals among every paragraph word.</p><p>The Neural Similarity Module. The Neural Similarity module is based on the deep neural relevance model of the top-performing system from the 2017 TREC-CAR track which uses PACRR <ref type="bibr" coords="6,345.07,134.41,75.23,9.46" target="#b2">(Hui et al., 2017)</ref>. PACRR operates in 4 steps: (1) it constructs a similarity matrix given a section path and a paragraph; (2) passes the similarity matrix through several convolutional layers and max-pooling layers, constructing a vector for each word in the section path representing the strongest similarity signals that word had with the entire paragraph;</p><p>(3) concatenates each section path word vector with a contextual vector; and (4) combines each section path word vector into a single vector using a LSTM.</p><p>Given a section heading path, s, and a paragraph, p, the Neural Similarity Module will calculate a vector ψ(s, p) representing the similarity between s and p. First, we construct a similarity matrix, sim ∈ [-1, 1] |s|×|p| , where sim ij is the cosine similarity between the word embeddings of the i th word of the section heading path and the j th word of the paragraph. We use g -1 different convolutional layers with square kernel sizes 2 × 2, 3 × 3, . . . , g × g corresponding to bi-gram, tri-gram, . . . , g-gram matching. All convolutional layers use f c filters and have a stride size of (1, 1), resulting in g -1 tensors, C g ∈ R |s|×|p|×fc . Two pooling layers are used to highlight the strongest signals for each query term: max pooling over the filter dimension, and k-max pooling over the paragraph-word dimension, resulting in a single tensor C p ∈ R |s|×g-1×k . This tensor is flattened into a series of (k + g -1)-dimensional vectors, one for each word in the section heading path, s. Each of these vectors is concatenated with a contextual vector consisting of a three dimensional 1hot encoding of what kind of heading a section path word belonged to (section heading, intermediate heading, or article title) along with the normalized IDF of the word. Finally, the vectors are passed through a LSTM with hidden size n. The similarity encoding between section heading path s and paragraph p is given by the final hidden state of the LSTM, ψ(s, p) ∈ R n .</p><p>The Feature Encoder. To incorporate features extracted between a section s and a paragraph p, STNet encodes a set of features into a single vector, f (s, p), called the feature encoding using the same module as SANet from the CAPAR sys- tem <ref type="bibr" coords="7,92.61,172.31,111.11,9.46">(Maldonado et al., 2017)</ref>. The Feature Encoder module, fully defined in <ref type="bibr" coords="7,213.85,185.86,76.42,9.46;7,72.00,199.41,27.27,9.46">Maldonado et al. (2017)</ref>, composes the feature encoding, f (s, p) by using a series of 1-dimensional convolutional layers to combine a set of feature vectors for each word and entity in section heading path and the paragraph.</p><p>The Scorer. The Scorer learns a function that combines the feature encoding, f (s, p), similarity encoding, ψ(s, p), and the semantic matching vector, a(s, p) to produce a real valued score representing how relevant a paragraph, p is to the section, s. Formally, the score φ(s, p) for a section s and a paragraph p is given by the following equation:</p><formula xml:id="formula_12" coords="7,85.35,383.28,204.92,25.85">φ(s, p) = elu v T s [a(s, p), f (s, p), ψ(s, p)]<label>(19)</label></formula><p>where elu(•) is the Exponential Linear Unit activation function <ref type="bibr" coords="7,144.22,426.78,90.60,9.46" target="#b1">(Clevert et al., 2015</ref>) and v T s ∈ R |a(s,p)|+|f (s,p)|+|ψ(s,p)| is a learned weight vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The results for TRANS-CAPAR in the 2018 TREC CAR evaluation are summarized in Table <ref type="table" coords="7,282.09,498.43,4.09,9.46" target="#tab_2">1</ref>. Two evaluation protocols are used to evaluate the paragraph retrieval task: (1) Manual Evaluation, and (2) Lenient Evaluation. The top 100 results for participant systems were pooled, and judgments were created by National Institute for Standards and Technology (NIST) judges assigning each paragraph a score of 3 (MUST BE MENTIONED), 2 (SHOULD BE MENTIONED), 1 (COULD BE MEN-TIONED), 0 (ROUGHLY ON TOPIC), -1 (NON-RELEVANT), or -2 (TRASH). In the lenient evaluation a paragraph is considered relevant if it is determined to be at least ROUGHLY ON TOPIC. However, in the manual evaluation paragraphs are only relevant if their relevance is at least COULD BE MENTIONED. For both the manual and lenient evaluations a paragraph is considered irrelevant if it has not been manually assessed for relevance.</p><p>In order to test our hypothesis that the Transformer and Neural Similarity module capture se-mantic and similarity information from the question and paragraph, we evaluated the STNet relevance model against two alternate configurations in which we omit either the Transformer Encoder and Decoder (-TRANS) or the Neural Similarity module (-SIM). The three STNet configurations are evaluated using three standard IR metrics: Mean Average Precision (MAP), R-Precision (R-Prec), and Normalized Discounted Cumulative Gain (NDCG). Clearly, the STNet relevance model performs best on all metrics for both manual and lenient evaluations. Interestingly, the two alternate configurations of STNet perform remarkably similar to one another, with absolute differences of 0.0002, 0.004, and 0.006 in MAP, R-Precision, and NDCG, respectively, on the manual evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Complex answer retrieval is a challenging task and it is clear from these results that further research is necessary to address its complexities. However, with a 26.6% relative increase in MAP over the top performing system in TREC CAR 2017, TRANS-CAPAR shows promise.</p><p>The fact that STNet that uses both the Transformer Encoder/Decoder and the Neural Similarity module outperforms both alternate configurations that use one or the other suggests that the two approaches are indeed complimentary. We believe this is because, when used together, the Neural Similarity module is able to capture similarity signals while the Transformer captures semantic information and discourse context. For example, consider the test topic "Air pollution/The Clean Air Act". For this query, the top result of the STNet-TRANS configuration which uses the Neural Similarity module, but not the Transformer, is a paragraph about 'New Source Performance Standards' because that paragraph contains the n-gram "Clean Air Act" several times. In contrast, the top ranked result of the STNet-SIM configuration which uses the Transformer, but not the Neural Similarity module is a relevant paragraph that contains the n-gram "Clean Air Act" only once, but it is clear from the discourse context that it is the subject of the paragraph. Conversely, for the test topic "Relative ages of rocks/Law of Lateral Continuity", the top ranked result returned by STNet-SIM about the continuation of Roman law through the ages, but STNet-TRANS is able to correctly rank a relevant paragraph by identifying the similarity between the word 'rocks' from the question and the words 'geologic' and 'outcrop' in the paragraph. For both of these questions, STNet using both the Transformer and Neural Similarity module is able to correctly rank the better paragraph in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The TRANS-CAPAR system used in the official evaluation of the 2018 TREC CAR track considered that answering complex questions by providing a ranked list of paragraphs performs best when taking into account both the similarity between the question paths and the paragraphs as well as the semantics that they share. However, capturing semantics between questions and paragraphs is notoriously difficult. We developed a neural architecture that encodes the semantics of the question and decodes it against the paragraph while also taking into account their similarity. The neural architecture used the encoder and decoder implemented by the Transformer <ref type="bibr" coords="8,147.85,480.84,100.07,9.46" target="#b6">(Vaswani et al., 2017)</ref> architecture to account for "deep" semantics. The results demonstrate that both similarity and deep semantics are useful in ranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,307.28,440.79,218.27,9.46;1,307.28,454.34,218.27,9.46;1,307.28,467.89,218.27,9.46;1,307.28,481.44,74.84,9.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Example of complex question as the topic "Analytics". Each complex question is represented by (1) the title of an article and (2) its table of contents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,169.57,249.97,258.41,9.46"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Architecture of the TRANS-CAPAR system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,507.07,134.82,8.24,9.46;3,315.93,158.54,209.62,9.88;3,329.09,172.52,196.45,9.46;3,329.09,186.07,196.45,9.46;3,329.09,199.62,196.45,9.46;3,329.09,213.17,196.45,9.46;3,329.09,226.72,69.87,9.46;3,315.93,250.44,209.61,9.88;3,329.09,264.42,196.45,9.46;3,329.09,277.97,196.45,9.46;3,329.09,291.51,196.45,9.46;3,329.09,305.06,43.64,9.46;3,315.93,328.79,209.61,9.88;3,329.09,342.76,196.45,9.46;3,329.09,356.31,196.45,9.46;3,329.09,369.86,196.45,9.46;3,329.09,383.41,196.45,9.46;3,329.09,396.96,196.45,9.46;3,329.09,410.51,80.30,9.46;3,315.93,434.23,209.61,9.88;3,329.09,448.21,196.45,9.46;3,329.09,461.76,101.64,9.46;3,315.93,485.48,209.61,9.88;3,329.09,499.46,196.45,9.46;3,329.09,512.66,196.45,9.81;3,329.09,526.21,196.45,9.81;3,329.09,539.76,27.54,9.81"><head>): 1 .</head><label>1</label><figDesc>The Transformer Encoder, detailed in, encodes the sequence of words in a section heading path into a sequence of vector encodings that can be used to perform semantic soft-matching against the words of the two paragraphs; 2. The Transformer Decoder, detailed in Figure 3(c), determines which pairs of paragraph and section words indicate a relevance match and produces a vector representing this softmatching;3. The Neural Similarity Module, detailed in Figure3(d), computes a cosine similarity matrix between a section path and a paragraph, uses CNN layers to extract n-gram similarity patterns, and produces a vector representation representing the strongest similarity signals using a RNN;4. The Feature Encoder encodes the raw features for each section-paragraph pair into a single vector encoding;5. The Scorer combines the feature encoding, the similarity encoding, and the attention vector into a single real valued score φ(s, p) representing the relevance of paragraph p to section s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,72.00,674.49,453.54,9.46;4,72.00,688.04,453.55,9.46;4,72.00,701.59,243.65,9.46"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The Siamese Attention Network for Pairwise Ranking. The full network is illustrated in (a), the Transformer Encoder and Decoder are depicted in (b) and (c) respectively, the Neural Similarity module is shown in (d) and the Feature Encoder is shown in (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,272.08,191.18,18.19,9.46;5,72.00,204.73,218.27,9.46;5,72.00,217.93,218.27,10.68;5,72.00,231.48,118.14,9.81;5,190.43,229.53,3.92,6.99;5,190.14,236.41,4.23,6.99;5,194.87,231.48,27.21,9.57;5,222.37,229.53,3.92,6.99;5,222.08,236.72,6.72,6.99;5,230.14,231.83,60.12,9.46;5,72.00,245.03,7.81,9.57;5,80.10,243.07,3.92,6.99;5,79.81,250.09,2.88,6.99;5,87.67,245.03,202.60,9.81;5,72.00,258.58,218.27,9.81;5,72.00,272.13,49.40,9.81;5,121.69,270.17,3.92,6.99;5,121.40,277.19,2.88,6.99;5,126.10,272.13,28.03,10.18;5,154.13,270.17,5.14,6.99;5,159.77,272.47,130.51,9.46;5,72.00,286.02,218.27,9.46;5,72.00,299.57,218.27,9.46;5,72.00,312.77,108.90,9.81;5,181.20,310.82,3.92,6.99"><head></head><label></label><figDesc>Figure 3.b produces a sequence of vector encodings, o 1 , . . . , o N representing the sequence of words in a section heading path, w s 1 , . . . w s N . Each word w s i in a section heading path has an associated ndimensional word embedding (n = 300 in this work), e(w s i ) ∈ R n , learned by GloVe (Pennington et al., 2014) on the full text of Wikipedia that captures fine-grained semantic and syntactic information about the word w s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,64.34,453.55,84.18"><head>Table 1 :</head><label>1</label><figDesc>Results for TRANS-CAPAR using Mean Average Precision (MAP), R-Precision (R-Prec), and Normalized Discounted Cumulative Gain (NDCG). Highest scores in bold.</figDesc><table coords="7,153.98,64.34,289.58,48.42"><row><cell></cell><cell></cell><cell>Manual</cell><cell></cell><cell>Lenient</cell></row><row><cell>Paragraph Ranker</cell><cell>MAP</cell><cell>R-Prec NDCG</cell><cell>MAP</cell><cell>R-Prec NDCG</cell></row><row><cell>STNet</cell><cell cols="4">0.2545 0.2789 0.4280 0.3125 0.3404 0.5293</cell></row><row><cell>STNet(-TRANS)</cell><cell cols="4">0.2152 0.2644 0.3774 0.2625 0.3321 0.4565</cell></row><row><cell>STNet(-SIM)</cell><cell cols="4">0.2150 0.2604 0.3780 0.2697 0.3398 0.4626</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,323.42,757.13,88.49,7.77"><p>http://lucene.apache.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,579.71,218.27,8.64;8,82.91,590.50,207.36,8.81;8,82.91,601.46,75.27,8.58" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m" coord="8,137.92,590.67,81.02,8.64">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,72.00,624.02,218.27,8.64;8,82.91,634.98,207.36,8.64;8,82.91,645.77,207.36,8.81;8,82.91,656.73,110.05,8.58" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,161.89,634.98,128.38,8.64;8,82.91,645.94,172.97,8.64">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName coords=""><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,72.00,679.29,218.27,8.64;8,82.91,690.25,207.36,8.64;8,82.91,701.04,207.36,8.81;8,82.91,712.00,75.27,8.58" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melo</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03940</idno>
		<title level="m" coord="8,170.97,690.25,119.30,8.64;8,82.91,701.21,140.61,8.64">Pacrr: A position-aware neural ir model for relevance matching</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,72.00,734.56,218.27,8.64;8,82.91,745.35,207.36,8.81;8,82.91,756.31,70.29,8.58" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="8,251.19,734.56,39.08,8.64;8,82.91,745.52,139.44,8.64">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,307.28,67.28,218.27,8.64;8,318.19,78.24,207.36,8.64;8,318.19,89.20,207.36,8.64;8,318.19,100.16,129.23,8.64;8,464.43,99.99,61.11,8.58;8,318.19,110.95,75.27,8.58;8,307.28,131.04,28.78,8.64;8,353.12,131.04,47.87,8.64;8,421.72,131.04,23.80,8.64;8,462.58,131.04,27.86,8.64;8,511.16,131.04,14.39,8.64;8,318.19,142.00,207.36,8.64;8,318.19,152.79,207.36,8.81;8,318.19,163.75,110.99,8.58" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,398.70,89.20,126.84,8.64;8,318.19,100.16,125.72,8.64;8,455.33,142.00,70.22,8.64;8,318.19,152.96,162.25,8.64">Characterizing question facets for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00791</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,509.50,152.79,16.04,8.58;8,318.19,163.75,86.09,8.58">Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">Ramon</forename><surname>Maldonado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stuart</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Harabagiu</forename><surname>Sanda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2018. 2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Utd hltri at trec 2017: Complex answer retrieval track</note>
</biblStruct>

<biblStruct coords="8,307.28,183.84,218.27,8.64;8,318.19,194.80,207.36,8.64;8,318.19,205.59,207.36,8.81;8,318.19,216.55,207.36,8.58;8,318.19,227.51,148.59,8.81" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,394.21,194.80,131.34,8.64;8,318.19,205.76,55.00,8.64">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,394.72,205.59,130.82,8.58;8,318.19,216.55,207.36,8.58;8,318.19,227.51,68.38,8.58">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,307.28,247.61,218.27,8.64;8,318.19,258.56,207.36,8.64;8,318.19,269.52,207.36,8.64;8,318.19,280.31,207.35,8.81;8,318.19,291.27,140.84,8.81" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,465.42,269.52,60.12,8.64;8,318.19,280.48,35.06,8.64">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,374.36,280.31,151.18,8.58;8,318.19,291.27,61.60,8.58">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
