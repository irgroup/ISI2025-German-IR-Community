<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.66,83.69,378.20,15.44;1,239.91,103.61,132.18,15.44">Anserini at TREC 2018: CENTRE, Common Core, and News Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,239.43,129.79,54.99,10.59"><forename type="first">Peilin</forename><surname>Yang</surname></persName>
							<email>yangpeilyn@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.39,129.79,51.16,10.59"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.66,83.69,378.20,15.44;1,239.91,103.61,132.18,15.44">Anserini at TREC 2018: CENTRE, Common Core, and News Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">739C8DA4B4E20BF9C2053699A63F9FB7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Anserini is an open-source information retrieval toolkit built on Lucene <ref type="bibr" coords="1,83.87,203.13,9.44,7.94" target="#b2">[3,</ref><ref type="bibr" coords="1,96.64,203.13,6.29,7.94" target="#b3">4]</ref>. The goal of our effort is to support information retrieval research using the popular open-source Lucene search library by allowing researchers to easily replicate results with modern ranking models on diverse test collections.</p><p>Although there are many open-source search engines developed and maintained by academic research groups, most of them are designed primarily to facilitate the publication of research papers, and as such, they often suffer from poor usability, incomplete documentation, and a host of other issues. The growing complexity of modern software ecosystems and the diverse capabilities that are required to build useful end-to-end search applications places academic research groups at a huge disadvantage relative to Lucene. Except for a handful of commercial web search engines that deploy custom infrastructure, Lucene has become the de facto platform in industry for building production search applications-used by organizations as diverse as Twitter, Reddit, Bloomberg, and Target. It has an active developer base, diverse features and capabilities, and lies at the center of a vibrant ecosystem. However, Lucene lacks systematic support for information retrieval research-in particular, ad hoc experimentation using standard test collections. This is where Anserini comes in: we enable cutting-edge information retrieval research using Lucene.</p><p>At TREC 2018, we participated in the CENTRE, Common Core, and News Tracks. Each is described in its own section below. Our development efforts centered around the v0.1.0 release of Anserini, which is based on Lucene 6.3.0 (not the latest release).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CENTRE TRACK</head><p>To accomplish the objectives discussed in the introduction, we are engaged in an ongoing effort to reproduce effective retrieval ranking models in Anserini, which will provide researchers a solid foundation to build on. For TREC, our primary objective was to reimplement axiomatic semantic term matching <ref type="bibr" coords="1,232.01,557.24,10.57,7.94" target="#b0">[1]</ref> in our toolkit. Although the paper of Yang and Hui <ref type="bibr" coords="1,192.22,568.20,9.52,7.94" target="#b1">[2]</ref>, which applied the technique to web collections, was selected as one of the reproduction targets of the CENTRE Track, we were not particularly interested in optimizing for the track's metrics (i.e., Root Mean Square Error with respect to the original submission). Since the original implementation was in Indri, there will inevitably be differences in document processing (e.g., tokenization, stopwords, stemming, etc.) that, while important, are not interesting from the perspective of reproducibility.</p><p>In a recent policy, ACM defines a number of terms related to reproducibility as follows: 1 repeatable ("a researcher can reliably 1 https://www.acm.org/publications/policies/artifact-review-badging repeat her own computation"), replicable ("an independent group can obtain the same result using the author's own artifacts"), and reproducible ("an independent group can obtain the same result using artifacts which they develop completely independently"). Our work might be characterized as "self-reproducibility"; although one of the authors of the original paper led the Anserini implementation effort, we go beyond repeatability because a completely different codebase was developed from scratch. Because of this, and the fact that we had access to the original code, we did not encounter any challenges during the process. To facilitate future reuse of the Anserini implementation, we have built an extensive regression framework that ensures others will be able to repeat our results.</p><p>Although the derivations of the models underlying Yang and Hui <ref type="bibr" coords="1,334.15,320.93,10.65,7.94" target="#b1">[2]</ref> are couched within the framework of axiomatic retrieval, the approach is operationalized as query expansion: A "working set" is constructed from an initial ranking, from which expansion terms are computed. These expansion terms are then combined with the original query to yield the final ranking in a second retrieval stage. This procedure means that semantic term matching can be applied to any "base" ranking model. That is, we can use any ranking function to aid in the construction of the working set (i.e., for gathering pseudo-relevant documents), and use the same ranking function (or even a different one) for the expanded query. Our implementation in Anserini makes such explorations easy.</p><p>Another feature of our implementation worth mentioning concerns how the working set is populated. In the original formulation, non-relevant documents are sampled from a particular source, and the non-determinism of this sampling process makes runs nonrepeatable. We address this by allowing the developer to supply a specific random seed as a configuration parameter. Repeatability can be ensured as long as these seeds are carefully recorded and tracked (just like any other experimental setting).</p><p>Yang and Hui <ref type="bibr" coords="1,383.99,529.15,10.68,7.94" target="#b1">[2]</ref> described different combinations of target collections and sources for building the working sets. The target collection is what we search against for the initial and expanded query, and the composition of the working set affects the score of the expansion terms. One might think that the target collection is simply the collection for the task, in this case, ClueWeb12. However, this was, in fact, not what Yang and Hui did in their TREC 2013 experiments. Due to reasons of computational efficiency, they first created a smaller sub-collection from ClueWeb12, which then served as the target collection on which subsequent experiments were performed. To quote:</p><p>[W]e first use Indri's default language model to retrieve 10,000 top ranked documents for each query and then filter out the documents that have spam score less than -130. The filtered documents are used to build a much smaller index. This reduced sub-collection is referred to as CW12Lite2013. Fortunately, we still have access to the exact docids that comprise this sub-collection, and hence can attempt to replicate results based on it. Using this sub-collection as the target collection, we experimented with three different working sets: using CW12Lite2013 itself, using web snippets from 2013 (which we call SP13),<ref type="foot" coords="2,242.94,450.05,3.38,6.44" target="#foot_0">2</ref> and using a Wikipedia dump from June 20, 2018 (which we call Wiki). These three combinations are shown as the first three rows of Table <ref type="table" coords="2,290.00,474.12,4.25,7.94" target="#tab_0">1</ref> (rows numbered 1 to 3). Using each combination, we can then apply our implementation of axiomatic semantic term matching from Anserini. In particular, the first two conditions are directly comparable to the UDInfolabWEB1 and UDInfolabWEB2 runs, respectively, expect with our Anserini implementation.</p><p>Although first creating a sub-collection was primarily an efficiency optimization made in TREC 2013, we were interested in the effects of sub-collection creation with more modern tools. To create what we call the CW12Lite2018 target collection, we used Lucene's query likelihood with Dirichlet smoothing implementation to fetch the top 10,000 documents (from all 50 queries). We did not, however, perform spam score filtering, as in CW12Lite2013. Using this sub-collection as the target collection, we experimented with the combinations shown in the middle three rows in Table <ref type="table" coords="2,290.05,627.55,4.21,7.94" target="#tab_0">1</ref> (rows numbered 4 to 6). We also crawled new web snippets in July 2018, which we call SP18.</p><p>Finally, we eliminated the sub-collection creation step and used the entire ClueWeb12 collection as the target. With vastly improved hardware since 2013, Lucene makes such experiments practical; we are able to build a single monolithic index over the entire collection (without partitioning) and run experiments with acceptable (albeit non-interactive) latencies. Working set combinations with all of ClueWeb12 as the target collection (CW12) is shown in the four rows on the bottom in Table <ref type="table" coords="2,403.60,441.24,4.17,7.94" target="#tab_0">1</ref> (rows numbered 7 to 10).</p><p>Given these combinations, we have one more variable in our experimental design: the retrieval model for both gathering the initial ranked list and for retrieval using the expanded query. The TREC 2013 experiments used F2Log, but here we additionally tried BM25 and query likelihood (QL) with Dirichlet smoothing.</p><p>Out of all the possible experimental conditions, we selected the six shown in Table <ref type="table" coords="2,403.37,517.96,4.22,7.94" target="#tab_1">2</ref> as our final CENTRE Track submissions. Detailed commands for replicating these runs, including building the index from scratch and generating the run files, are documented in a runbook checked into the Anserini code repository. <ref type="foot" coords="2,522.44,548.68,3.38,6.44" target="#foot_1">3</ref>We used the newly released qrels file to evaluate the submitted runs and the results are presented in Table <ref type="table" coords="2,475.93,572.75,3.07,7.94" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COMMON CORE TRACK</head><p>We submitted a total of ten runs to the Common Core Track, described as follows:</p><p>• anserini_bm25: BM25 baseline.</p><p>• anserini_sdm: BM25 + sequential dependence model. • anserini_rm3: BM25 + RM3. • anserini_ax: BM25 + axiomatic semantic term matching, with working set from the Washington Post collection itself. • anserini_ax17: BM25 + axiomatic semantic term matching, with working set from the New York Times collection used in Core17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>• anserini_ql: QL baseline.</p><p>• anserini_qlsdm: QL + sequential dependence model. • anserini_qlrm3: QL + RM3.</p><p>• anserini_qlax: QL + axiomatic semantic term matching, with working set from the Washington Post collection itself. • anserini_qlax17: QL + axiomatic semantic term matching, with working set from the New York Times collection used in Core17.</p><p>A detailed runbook checked into our Anserini code repository describes how these runs can be replicated. 4  The effectiveness of our submitted runs in terms of standard retrieval metrics are shown in Table <ref type="table" coords="3,188.17,489.79,3.08,7.94" target="#tab_2">3</ref>. The final column indicates whether or not the run contributed to the judgment pools. The final four rows in the table show mean of per topic median and max scores across two different categories of runs: automatic runs (Auto) and all runs (All). These values are based on summary statistics provided by NIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEWS TRACK</head><p>We participated in the background linking task of the News Track, submitting five runs. Our main focus was to explore different approaches to constructing a keyword query from a query article.</p><p>• anserini_1000w: Each term in the query article is assigned a weight equal to its tf-idf score. The query is constructed by selecting up to 1000 terms based on these scores. The selected terms comprise a weighted query for searching the collection using BM25 to generate the final ranked list. • anserini_nsdm: We selected the first 1000 terms from the query article and used those as the "query" to search the collection 4 See similar note with respect to the CENTRE runbook. using the sequential dependence model with BM25. Due to limits on the number of clauses in a Lucene query, we were not able to use the entire document as the query. • anserini_nax: We ran BM25 with axiomatic semantic term matching using the query article as the "query". That is, we constructed an initial query using up to the first 1000 terms from the query article without weights. BM25 is used to perform an initial retrieval, and then axiomatic semantic term matching is used to select up to 20 expansion terms. The expanded query (original terms from the query article and the expansion terms) is used to search the collection with BM25 to produce the final ranked list, with β = 0.4 to control the importance of the semantic term match weights. • anserini_sdmp: Instead of treating the entire query article as a single query, we selected the five longest paragraphs and formed five individual queries, each comprising up to 1000 terms (from the paragraph). These were used as input to the sequential dependence model to query the collection using BM25. The final ranking is generated by selecting results of the paragraph queries in a round-robin fashion. • anserini_axp: Similar to anserini_sdmp, but we used BM25 with axiomatic semantic term matching for each of the individual paragraph queries. We selected up to 20 expansion terms (each), with β = 0.4 to control the importance of the semantic term match weights. All of our background linking runs contained a post-processing step to eliminate duplicate articles. Results from the evaluation are shown in Table <ref type="table" coords="3,374.36,531.63,3.01,7.94" target="#tab_3">4</ref>. It appears that our simplest approach of selecting terms from the query article to form a weighted query is the most effective in terms of mean NDCG@5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,61.44,88.72,486.89,263.84"><head>Table 1 :</head><label>1</label><figDesc>Different possible combinations of target collections and working sets for axiomatic semantic term matching.</figDesc><table coords="2,61.44,88.72,486.89,263.84"><row><cell></cell><cell cols="2">Target Collection Working Set</cell><cell>Comments</cell></row><row><cell>1</cell><cell>CW12Lite2013</cell><cell cols="2">CW12Lite2013 directly to UDInfolabWEB1</cell></row><row><cell>2</cell><cell>CW12Lite2013</cell><cell>SP13</cell><cell>directly comparable to UDInfolabWEB2</cell></row><row><cell>3</cell><cell>CW12Lite2013</cell><cell>Wiki</cell><cell></cell></row><row><cell>4</cell><cell>CW12Lite2018</cell><cell cols="3">CW12Lite2018 similar to UDInfolabWEB1, but using Anserini to create sub-collection</cell></row><row><cell>5</cell><cell>CW12Lite2018</cell><cell>SP18</cell><cell cols="2">similar to UDInfolabWEB2, but using Anserini to create sub-collection + 2018 web snippets</cell></row><row><cell>6</cell><cell>CW12Lite2018</cell><cell>Wiki</cell><cell></cell></row><row><cell>7</cell><cell>CW12</cell><cell>CW12</cell><cell cols="2">similar to UDInfolabWEB1, but without creating sub-collection first</cell></row><row><cell>8</cell><cell>CW12</cell><cell>SP13</cell><cell cols="2">similar to UDInfolabWEB2, but without creating sub-collection first</cell></row><row><cell>9</cell><cell>CW12</cell><cell>SP18</cell><cell cols="2">similar to above, except with 2018 web snippets</cell></row><row><cell cols="2">10 CW12</cell><cell>Wiki</cell><cell></cell></row><row><cell></cell><cell>Run</cell><cell></cell><cell>Description</cell><cell>AP NDCG@20 ERR@20</cell></row><row><cell></cell><cell cols="3">Anserini-UDInfolabWEB1-1 #7, using BM25 and β = 0.5</cell><cell>0.1329</cell><cell>0.19754</cell><cell>0.20112</cell></row><row><cell></cell><cell cols="3">Anserini-UDInfolabWEB1-2 #9, using BM25 and β = 0.5</cell><cell>0.2172</cell><cell>0.25322</cell><cell>0.23460</cell></row><row><cell></cell><cell cols="4">Anserini-UDInfolabWEB1-3 #10, using BM25 and β = 0.5 0.1233</cell><cell>0.18550</cell><cell>0.16617</cell></row><row><cell></cell><cell cols="3">Anserini-UDInfolabWEB2-1 #1, using QL and β = 1.7</cell><cell>0.0880</cell><cell>0.15602</cell><cell>0.15770</cell></row><row><cell></cell><cell cols="3">Anserini-UDInfolabWEB2-2 #4, using QL and β = 1.7</cell><cell>0.1096</cell><cell>0.16391</cell><cell>0.18595</cell></row><row><cell></cell><cell cols="3">Anserini-UDInfolabWEB2-3 #6, using BM25 and β = 0.5</cell><cell>0.1119</cell><cell>0.17599</cell><cell>0.16384</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,211.61,364.57,188.48,7.70"><head>Table 2 :</head><label>2</label><figDesc>Submitted runs to the CENTRE Track.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.50,88.72,242.15,205.65"><head>Table 3 :</head><label>3</label><figDesc>Anserini results in the Common Core Track, compared to summary statistics provided by NIST.</figDesc><table coords="3,73.25,88.72,199.11,174.83"><row><cell></cell><cell>AP NDCG</cell><cell cols="2">P@10 Pool</cell></row><row><cell>anserini_bm25</cell><cell cols="2">0.2284 0.5064 0.4500</cell><cell>Y</cell></row><row><cell>anserini_sdm</cell><cell>0.2364 0.5127</cell><cell></cell><cell>Y</cell></row><row><cell>anserini_rm3</cell><cell cols="2">0.2680 0.5422 0.4680</cell><cell>Y</cell></row><row><cell>anserini_ax</cell><cell cols="2">0.2734 0.5582 0.4960</cell><cell>Y</cell></row><row><cell>anserini_ax17</cell><cell cols="2">0.2059 0.4942 0.4060</cell><cell>Y</cell></row><row><cell>anserini_ql</cell><cell cols="2">0.2294 0.5059 0.4660</cell><cell>N</cell></row><row><cell>anserini_qlsdm</cell><cell cols="2">0.2326 0.5071 0.4740</cell><cell>N</cell></row><row><cell>anserini_qlrm3</cell><cell cols="2">0.2501 0.5359 0.4660</cell><cell>N</cell></row><row><cell>anserini_qlax</cell><cell cols="2">0.2749 0.5484 0.4780</cell><cell>N</cell></row><row><cell cols="3">anserini_qlax17 0.2039 0.4875 0.4280</cell><cell>N</cell></row><row><cell>Median (Auto)</cell><cell cols="2">0.1745 0.4483 0.3340</cell><cell>-</cell></row><row><cell>Max (Auto)</cell><cell cols="2">0.3958 0.6849 0.7280</cell><cell>-</cell></row><row><cell>Median (All)</cell><cell cols="2">0.2112 0.4925 0.4140</cell><cell>-</cell></row><row><cell>Max (All)</cell><cell cols="2">0.5353 0.7792 0.8340</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,317.66,88.72,240.54,119.97"><head>Table 4 :</head><label>4</label><figDesc>Anserini results in the background linking task of the News Track, compared to summary statistics provided by NIST.</figDesc><table coords="3,369.46,88.72,135.00,78.18"><row><cell></cell><cell cols="2">NDCG@5</cell></row><row><cell>Run</cell><cell cols="2">mean median</cell></row><row><cell cols="2">anserini_1000w 0.3529</cell><cell>0.3433</cell></row><row><cell>anserini_nsdm</cell><cell cols="2">0.3347 0.3456</cell></row><row><cell>anserini_nax</cell><cell>0.2948</cell><cell>0.2402</cell></row><row><cell>anserini_sdmp</cell><cell>0.3271</cell><cell>0.3435</cell></row><row><cell>anserini_axp</cell><cell>0.3030</cell><cell>0.2832</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,56.84,702.46,166.07,6.18"><p>Fortunately, we have retained the web snippets from 2013.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,321.00,685.46,237.20,6.18;2,317.96,693.43,240.43,6.18;2,317.96,701.65,128.30,6.18"><p>Available from the main Anserini repo, http://anserini.io; given the impermanence of web links, we strive to keep Anserini documentation up to date and intuitively organized in lieu of providing a specific URL.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,330.15,586.73,228.05,6.18;3,329.90,594.65,229.00,6.23;3,330.15,602.62,228.05,6.23;3,329.72,610.59,122.17,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,445.01,586.73,113.19,6.18;3,329.90,594.70,105.31,6.18">Semantic Term Matching in Axiomatic Approaches to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,447.42,594.65,111.49,6.23;3,330.15,602.62,228.05,6.23;3,329.72,610.59,17.71,6.23">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,330.15,618.61,229.23,6.18;3,330.15,626.53,228.75,6.23;3,330.15,634.50,120.95,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,425.10,618.61,134.28,6.18;3,330.15,626.58,62.99,6.18">Evaluating the Effectiveness of Axiomatic Approaches in Web Track</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,405.34,626.53,153.56,6.23;3,330.15,634.50,11.92,6.23">Proceedings of the Twenty-Second Text REtrieval Conference</title>
		<meeting>the Twenty-Second Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,330.15,642.52,228.05,6.18;3,330.15,650.44,228.05,6.23;3,330.15,658.41,228.05,6.23;3,330.15,666.38,89.51,6.23" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,454.61,642.52,103.59,6.18;3,330.15,650.49,95.26,6.18">Anserini: Enabling the Use of Lucene for Information Retrieval Research</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,437.49,650.44,120.71,6.23;3,330.15,658.41,209.27,6.23">Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,330.15,674.40,228.05,6.18;3,330.15,682.32,228.82,6.23;3,329.90,690.34,29.24,6.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,464.56,674.40,93.64,6.18;3,330.15,682.37,67.26,6.18">Anserini: Reproducible Ranking Baselines Using Lucene</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,404.54,682.32,117.16,6.23">Journal of Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
