<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.16,164.85,336.93,15.12;1,216.35,186.77,178.55,15.12">TREMA-UNH at TREC 2018: Complex Answer Retrieval and News Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.75,219.25,93.96,10.48"><forename type="first">Sumanta</forename><surname>Kashyapi</surname></persName>
						</author>
						<author>
							<persName coords="1,243.21,219.25,103.43,10.48"><forename type="first">Shubham</forename><surname>Chatterjee</surname></persName>
						</author>
						<author>
							<persName coords="1,355.72,219.25,83.89,10.48"><forename type="first">Jordan</forename><surname>Ramsdell</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,449.11,219.25,61.13,10.48"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
							<email>dietz@cs.unh.edu</email>
						</author>
						<title level="a" type="main" coord="1,137.16,164.85,336.93,15.12;1,216.35,186.77,178.55,15.12">TREMA-UNH at TREC 2018: Complex Answer Retrieval and News Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4CA1B78B1C191B95AA81383D9334F28E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This notebook describes the submission of team TREMA-UNH to the TREC Complex Answer Retrieval track and the TREC News track in 2018. Our methods focus on passage retrieval, entity-aware passage retrieval, and entity retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Users expectations of search engines continue grow with advancements of search engines and retrieval models. We explore "search engines of the future" that not only rank documents according to relevance, but also present all relevant information in a compact manner from which users are able to synthesize knowledge easily. To accomplish this task, we train retrieval models to have a better understanding relevance for natural language.</p><p>The Complex Answer Retrieval (CAR) <ref type="bibr" coords="1,310.93,459.81,15.10,8.74" target="#b0">[1]</ref> track at the Text Retrieval Conference (TREC)<ref type="foot" coords="1,192.09,470.19,3.97,6.12" target="#foot_0">1</ref> aims to address this scenario. Current retrieval systems provide good solutions towards passage retrieval for simple fact and entity-centric information needs. In contrast, CAR is about answering more complex information needs with longer answers. The formal task statement includes the two following tasks: CAR Passage Task: Given an article stub Q, retrieve for each of its sections H i , a ranking of relevant passages P . The passage P is taken from a provided paragraph corpus. CAR Entity Task: Given an article stub Q, retrieve for each of its sections H i , a ranking of relevant entities E. The entity E is taken from a provided Wikipedia corpus. Additionally for each entity E, a support passage from the passage corpus is to be identified that explains why the entity is relevant for the heading H i on the stub.</p><p>We further participate in the NEWS track entity ranking task.</p><p>NEWS Entity Task: Given a news article with title and content that is annotated with entity links to a set of entities E = {E1, E2, ...En}, the task is to rank the the given entities by importance for the article (i.e., saliency).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overarching Approach</head><p>For each page, we use the section headings from the page's outline as queries. The headings are usually very short. Hence, using retrieval models without query expansion (such as BM25) may not yield good results since headings have potentially little textual overlap with their passages. To alleviate this problem, we experiment with various entity-and word-based query expansion methods. We observe the performance of each of the methods individually. However, since any single unsupervised method may not provide best performance on its own, we also experiment with machine-learned combinations of methods using learning-to-rank. <ref type="foot" coords="2,206.61,306.68,3.97,6.12" target="#foot_1">2</ref>A knowledge graph (KG) is used to enhance a retrieval models's results with information gathered from a variety of sources. The KG provides access to knowledge about entities and is derived from external sources such as Wikipedia.</p><p>Here we use a Wikipedia dump<ref type="foot" coords="2,272.31,354.50,3.97,6.12" target="#foot_2">3</ref> provided by the TREC CAR organizers that does not include pages from test queries. Such knowledge graphs hold a wealth of information which can be leveraged to solve information retrieval problems.</p><p>In Section 3.1 we first describe which unsupervised retrieval and expansion models were used for the passage task, and in Section 3.2 for the entity task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Low-level Retrieval Features</head><p>Each of our approaches are based on a variety of unsupervised retrieval models and document indexes that we describe in the following.</p><p>Indexes: We create the following indexes for use with our retrieval methods.</p><p>• A paragraph index out of the text in passages of the paragraphCorpus.</p><p>• A page index out of all visible text on Wikipedia pages in allButBenchmark.</p><p>• An entity index out of the first paragraph, anchor text, and category info of Wikipedia in allButBenchmark.</p><p>Query models: Given a stub with page title T and a tree-shaped outline of headings H1, H1.1, H1.2, H1.2.1, H2,... there are different ways to derive queries for retrieval with BM25, QL, etc for text of a particular heading, such as H1.2. The simplest approach is what we call section path queries, which concatenates the page title, T , with all parent headings (in this example H1), along with the heading itself (H1.2), to derive the query. However, more options are possible such, as as:</p><p>• Section Path: Concatenation of page title, parent headings, and heading (Example: T, H1, H1.2).</p><p>• Title: only the page title (T )</p><p>• Leaf: only the heading itself (H1.2)</p><p>• Internal: concatenation of the parent heading(s) (H1)</p><p>• All: concatenation of page title and all headings in the stub ( T, H1, H1.1, H1.2, H1.2.1, H2, . . . )</p><p>• Subtree: concatenation of all headings in the subtree rooted by the heading (H1.2, H1.2.1)</p><p>The section path model is very popular among the TREC CAR participants and usually works best as a standalone method. However, it has been argued that learning a weighted combination between title, internal, and leaf query models could give potentially even better performance <ref type="bibr" coords="3,373.84,367.06,9.96,8.74" target="#b1">[2]</ref>.</p><p>Retrieval and expansion models: Given a query model to transform the stub into a keyword query and an index, and we use different retrieval models to obtain low-level rankings (which we will combine with different learning to rank methods in the following). In particular we use as retrieval models:</p><p>• BM25, as implemented in Lucene (using default parameters).</p><p>• Query likelihood with Dirichlet smoothing, as implemented in Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We use as expansion models</head><p>• None: No expansion. Just uses the initial ranking.</p><p>• RM3: RM Relevance model expansion <ref type="bibr" coords="3,329.42,532.44,10.52,8.74" target="#b2">[3]</ref> using 20 top paragraphs/pages to expand with 20 terms. This is an RM1-style ranking that is intended to be combined with the basic ranking model (to yield RM3).</p><p>• ECM: A variant of RM3 representing a document as a bag-of-entity-linktargets. Uses top 100 paragraphs/pages. Ecm uses the entity context model in a non-standard way, where the whole page is used instead of short passages: After retrieving a feedback run of paragraphs or Wikipedia pages with BM25, each page is represented as bag-of-entitylink-targets. Using the relevance model <ref type="bibr" coords="4,312.86,266.64,9.96,8.74" target="#b2">[3]</ref>, we compute the distribution over expansion entities P (E|Q). We discuss two variants, in ecm-passage we expand the query with these entities to retrieve a new ranking. In ecm we use the distribution directly to produce a ranking of entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>score(E|Q) =</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D∈ranking p(D|Q)p(E|D)</head><p>The name ecm is in attribution to the entity context model <ref type="bibr" coords="4,414.07,355.20,9.96,8.74" target="#b3">[4]</ref>, which uses passages with contained entity mentions for new expansion models. Where Dalton et al. <ref type="bibr" coords="4,193.91,379.11,10.52,8.74" target="#b3">[4]</ref> uses passages surrounding entity links in a feedback run, the ecm-paragraph index allows us to directly retrieve relevant passages with entitycentric information.</p><p>We provide all low-level runs for benchmarkY1train, benchmarkY1test and benchmarkY2test online. <ref type="foot" coords="4,242.33,425.35,3.97,6.12" target="#foot_3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Low-level Paragraph Retrieval Features</head><p>We use the following pararaph ranking and expansion models to derive features for learning-to-rank for the passage task (decribed in the following sections).</p><p>• sectionPath-bm25-none: Using section path queries, BM25 retrieval model (no expansion)</p><p>• sectionPath-ql-none: Using section path queries, Query likelihood retrieval model (no expansion)</p><p>• sectionPath-bm25-rm: Using section path queries, BM25-based RM3</p><p>• sectionPath-ql-rm: Using section path queries, query likelihood-based RM3</p><p>The results of low-level retrieval methods are presented in Table <ref type="table" coords="4,432.45,608.70,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low-level Entity Retrieval Features</head><p>We use the following ranking and expansion models to derive features for learningto-rank for the entity task (decribed in the following sections).</p><p>• sectionPath-bm25-ecm: Using section path queries, BM25 ranking with ECM ranking based on BM25 page retrieval.</p><p>• sectionPath-ql-ecm: Same as above, but based on Query likelihood.</p><p>• all-bm25-ecm: Constructing a query from title and all headings on the outline, ECM ranking based on BM25 page.</p><p>• all-ql-ecm: Same as above, but based on Query likelihood.</p><p>These were then combined using learning-to-rank and a combined ranking was obtained. This model is trained on benchmarkY1-train, using the tree-level ground truth for the passage task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNH-p-l2r</head><p>Interpreting the set of rank scores (using Section 3.1) for a particular paragraph as features, we learn how to optimally combine these features into a weighted combination. We train the model using the coordinate ascent algorithm of RankLib's learning to rank implementation, optimized for mean average precision.</p><p>We train combinations of:</p><p>• sectionPath-bm25-none: BM25 retrieval model</p><p>• sectionPath-ql-none: Query likelihood retrieval model (no expansion)</p><p>• sectionPath-bm25-rm: BM25-based relevance feedback</p><p>• sectionPath-ql-rm: Query likelihood-based relevance feedback</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">UNH-e-l2r</head><p>Using Lucene indexes for entity, page and paragraph (as described in Section 3). We create features for each index by combining query-level, retrieval-model, and expansion-model methods as follows:</p><p>We train combinations of (across all choices of entity, page and paragraph indexes):</p><p>• sectionPath-bm25-ecm: Section-path BM25 retrieval model with ecm entity ranking</p><p>• sectionPath-ql-ecm: Same as above but with Query likelihood instead of BM25</p><p>• all-bm25-ecm: Query from title and all headings using BM25 retrieval model with ecm entity ranking</p><p>• all-ql-ecm: Same as above but with Query likelihood instead of BM25</p><p>These were then combined using learning-to-rank and a combined ranking was obtained. This model is trained on benchmarkY1-train, using the tree-level ground truth for the entity task.</p><p>The rankings were annotated with the highest ranked paragraph from an additional paragraph ranking as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">UNH-p-SDM</head><p>We use Lucene to index the TREC CAR 2017 paragraph corpus after stemming and removing stopwords. The UNH-p-SDM method is inspired by the Sequential Dependence model <ref type="bibr" coords="6,220.17,306.26,10.52,8.74" target="#b4">[5]</ref> in that it combines unigrams, bigrams, and windowedbigrams. However, where the original approach by Metzler et al. used language models (such as query likelihood) as features, here we use BM25 as a basis for scoring the relevance of documents given a query. We do so by indexing the unigrams, bigrams, and unordered windowed-bigrams (with a window size of eight words) as document fields using Lucene. We tokenize and stem documents using Lucene's English analyzer. We then consider three features that score document relevance with respect to these fields:</p><p>• Unigram: Unigrams in the query are used to retrieve and score passages via their unigram fields.</p><p>• Bigram: Bigrams in the query are used to retrieve and score passages via their bigram fields with BM25/QL.</p><p>• Windowed-bigram: Unordered windowed bigrams in the query are used to retrieve and score passages via their windowed bigram fields.</p><p>We train a weighted combination of these three scores that best predicts the relevance of a passage. We do so by using RankLib with coordinate ascent, optimized for best MAP performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Joint Entity-Passage Methods</head><p>In this paper, we explore features that can be used to jointly score entities and passages. We do so using the following approaches.</p><p>Passage retrieval using entity features. Let f e be an entity relevance feature. For each passage, p, let E be the set of entities contained in p. Then a feature that scores the relevance of p given the relevance entities can be represented as f p (p) = e∈E f e (e). We are free to combine this feature with other passage features to rank retrieved passages according to relevance.</p><p>Entity retrieval using passage features. Let f p be a passage relevance feature. For each entity, e, let P be the set of passages (retrieved with UNH-p-SDM) that contain at least one instance of e. We may sum over the scores of passages that contain e to produce a score for e: f e (e) = p∈P f p (p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Description of Passage Features</head><p>We consider the following passage features for use in our joint entity-passage methods:</p><p>UNH-p-SDM. We directly use the scores of passages obtained by the UNHp-SDM (see section 6)) passage retrieval method as a passage feature.</p><p>SDM. Passages are scored using a standard SDM model under query likelihood, in the spirit of Metzler et al. <ref type="bibr" coords="7,294.94,269.88,9.96,8.74" target="#b4">[5]</ref>.</p><p>We convert these passage features into into entity features as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Description of Entity Features</head><p>We consider the following entity features for use in our joint entity-passage methods:</p><p>Entity Link Frequency. We score an entity with respect to the relative frequency at which it was linked to by passages in a candidate set of passage. We do so by summing over the total number of times an entity was linked by candidate passages, and then normalizing by the total number of entity links among all candidate passages.</p><p>Fielded Queries. Because our entity index represents a collection of Wikipedia pages, we can consider page attributes as document fields. We store the following page attributes as unigrams in document fields: (enwiki) categories, inlinks, outlinks, section headers, page name disambiguations, and page name redirects. We score the relevance of entities with respect to the query by using the standard BM25 model. This feature can also be transformed into a passage feature.</p><p>Global Entity Context. Whenever a passage links to an entity in Wikipedia, we create a pseudo-document that contains the unigrams, bigrams, and windowed bigrams derived from that passage. We consider the collection of all such pseudo-documents belonging to an entity as that entity's "context" in which it is mentioned. This differs from query-specific context models <ref type="bibr" coords="7,415.65,543.31,9.96,8.74" target="#b3">[4]</ref>, in that we use one global entity context model derived from the corpus. For each query, we construct a unigram, bigram, and windowed bigram query that is used to retrieve pseudo-documents from the global entity context index.</p><p>Pseudo-documents are scored with respect to the standard BM25 model. Each pseudo-document represents a particular context in which an entity is mentioned in a passage, and we let an entity's score be equal to the highest scored context in which it was mentioned. We obtain one such score for each of the retrieval methods (unigrams, bigrams, and windowed bigrams). We treat each score as a separate entity feature.</p><p>All these features are used either directly as an entity feature, or they are transformed to a passage feature by summing over the relative entity frequencies of entities contained within a passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">UNH-e-SDM</head><p>For our entity retrieval task, we derive an entity ranking (the UNH-e-SDM) from the scores of candidate passages using the UNH-p-SDM passage feature as described in Section 7.1. We score each candidate entity by summing over the scores of candidate passages that are linked to this entity. We retrieve candidate entities by retrieving entities linked to the top 100 candidate passages ranked using the UNH-p-SDM passage retrieval method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">UNH-p-Mixed</head><p>The UNH-p-mixed method is a passage retrieval method that utilizes all of the passage features described in section 7.1 (UNH-p-SDM and SDM). This method also uses all of the entity features described in section 7.2 (entity link frequency, the six fielded query features, and the three entity context features), which are transformed into passage features. As previously described, we score passages using entity features by summing over the scores of entities contained within each passage.</p><p>The top 100 candidate passages are retrieved using the UNH-p-SDM passage retrieval method, and all entities linked to these passages are used as candidate entities. Candidate entities and passages are scored using the above features. Finally, we learn a weighted combination of these passage features and transformed features using learning to rank (where queries and query relevance is derived from the section path model described previously). We find that UNHp-SDM feature receives the highest weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">UNH-e-Mixed</head><p>The UNH-e-mixed method is an entity retrieval method that utilizes all of the entity features described in section 7.2. These include entity link frequency, the six fielded queries (enwiki categories, inlinks, outlinks, section headers, page name disambiguations, page name redirects), the three entity context features, and all of the passage features described in section 7.1 (UNH-p-SDM and SDM). The passage features are transformed into entity features such that the score of each entity is equal to the sum of the scores of passages that link to the entity, as mentioned in Section 7.</p><p>Our candidate passages and entities are retrieved using the approach described in Section 7.4. We again use learning to rank (and the section path query model) to learn a linear combination of entity features and transformed passage features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">UNH-e-graph</head><p>We consider a graph where entities are nodes and paragraphs form edges between all nodes that are contained in the paragraph. Degree centrality, PageRank, personalized PageRank, HITS, or SALSA could be applied to this graph to identify important nodes. However, unsupervised graph walk methods have no knowledge about which edges are relevant for the query, and therefore suffer from concept drift.</p><p>We explore a novel, unpublished method for "Learning to Walk", where nodes and edges are associated with feature vectors that quantify their relevance for the query-these are derived by unsupervised rank scores such as BM25 or relevance models. It is possible to derive an optimization algorithm to optimize weight parameters for node and edge features, so that the entity ranking produced by applying degree centrality to the graph obtains the best MAP performance.</p><p>Similar to traditional learning to rank approaches, features are derived from different unsupervised ranking functions. We use indexes described in Section 2. Rankings of entities provide a feature vector for nodes; rankings of paragraphs provide a feature vector for edges. The Learning-to-Walk algorithm is used to train weight parameters to optimize for MAP on entity rankings. The Learningto-Walk is trained with mini-batched coordinate ascent using five restarts on benchmarkY1train.</p><p>In contrast to our other methods, this one is trained with a custom version of a tree-qrels, created as follows: For all queries in the benchmarkY1train and benchmarkY1test we obtain the Wikipedia page. These pages are entity linked with DBpedia Spotlight. Then all predicted entity links are compared to link targets that were manually included in the page by Wikipedia editors. Spotlight links are only retained if the Wikipedia editor had included a link to the same target (anywhere on the page).</p><p>This process is very similar to how the official automatic qrels were created by the track organizers, with the difference that official qrels are based on entity links created by Wikipedia editors, where we extend this set with our spotlightbased heuristic. The main concern is that due to Wikipedia's editorial policies, only the first mention of an entity of a page is annotated with a hyperlink to the entity. However, central entities may get mentioned several more times on the page, possibly in other sections. Without fixing this ground truth, the concern was that when relevant entities are included in the ranking, we don't want the qrels file to (falsely) indicate that the entity is non-relevant.</p><p>We train several variations of this method and select the best variation for submission as "UNH-e-graph". We found in post-mortem experiments that including ecm-expansion would lead to even better performance. However, we were concerned that this feature would not generalize well to non-Wikipedia collections and did not include it in the variant submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Support Passage for Entity Rankings</head><p>The entity ranking is augmented with support passages that are supposed to explain to a user how this entity is relevant for the heading H i .</p><p>We first compute an entity ranking without support passages, then extend it as follows. We use a paragraph ranking obtained using one of our paragraph ranking methods. For every entity in the ranking, we return the first paragraph in the ranking which contains this entity. We use the entity-passage pairs to produce a run file in the TREC CAR entity ranking task format. The score of a paragraph given a query and entity in this combined run file is the original score of the entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Evaluation on TREC Complex Answer Retrieval</head><p>We evaluate the performance of our passage and entity retrieval methods with the following qrels files (all from the TREC CAR v1.2 data set) <ref type="bibr" coords="10,415.57,330.08,9.96,8.74" target="#b0">[1]</ref>:</p><p>• Y1 Train: benchmarkY1train, automatic tree-qrels, using 5-fold CV.</p><p>• Y1 Test: benchmarkY1test, automatic tree-qrels, trained on Y1 Train.</p><p>• Y2 Test: benchmarkY2test, manual assessments, trained on Y1 Train, best methods selected on Y1 Test.</p><p>For each method, the top 100 paragraphs/entities were retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Results</head><p>We compare the performance of our passage retrieval methods using three benchmarks (Y1 Train, Y1 Test, and Y2 Test) in Table <ref type="table" coords="10,376.47,483.95,3.87,8.74" target="#tab_2">3</ref>. Highest MAP values are depicted in bold. Results are presented in Table <ref type="table" coords="10,363.58,495.91,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="10,391.24,495.91,3.87,8.74" target="#tab_2">3</ref>. Among the submitted runs, the runs produced by UNH-p-l2r and UNH-e-l2r work best on Y2 Test. This is especially surprising for UNH-p-l2r for two reasons: (1) the method is fairly simple, including only BM25 and Query likelihood with and without RM3 expansion; (2) both were not necessarily performing best in terms of MAP or Rprec on automatic Y1 train and Y1 test benchmarks-in contrast to findings of the track organizers last year, that automatic and manual assessments lead to nearly the same system rankings.</p><p>On the automatic benchmarks Y1 Train and Y1 Test, UNH-p-mixed and UNH-e-mixed methods have the highest performance with respect to RPrec and MAP. Interestingly, their performance is the lowest among all our submitted methods with respect to the manual benchmark (Y2 Test). We speculate that the NDCG measure on the automatic benchmarks are better correlated with good performance on manual assessments of Y2 test.    We participate in the NEWS track entity ranking task. As no training data was provided, we participate with with three variations of our low-level entity retrieval features as described in Section 3.2.</p><p>In this task, we are given a set of entities that we have to re-rank by importance for the article. Our approach is to use a part of the given news article (title or first paragraph) to construct a query. This query is used to retrieve entities from the entity index built from pages made available in CAR's allBut-Benchmark collection. We use BM25 with a whitelist to retrieve a re-ranking Wiki pages and rank the set of given entities according to their pages.</p><p>• UNH-TitleBm25: Using the article title as a query, use BM25 to retrieve from the page index.</p><p>• UNH-ParaBM25: Using the first paragraph (or at least 200 characters) of the news article's content as a query, use BM25 to retrieve from the page index.</p><p>• UNH-ParaBM25Ecm: Using the first paragraph of the news article as The results are presented in Table <ref type="table" coords="13,297.73,161.83,3.87,8.74" target="#tab_3">4</ref>. We see that retrieving entities with the first paragraph (no ecm expansion) works best. For reference we include median and best run as provided by the NEWS organizers. Our best run performs above the median and slightly below the best run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Conclusion</head><p>In this notebook we experiment with methods of document retrieval that utilize the context of entity links for passage and entity retrieval.</p><p>In the case of CAR entity retrieval, we use the context in which an entity occurs in a passage as a means of indicating the relevance of an entity with respect to a query. This results in a significant improvement over just using features that score the relevance of an entity using fielded queries (such as the title, inlinks, or outlinks fields associated with a Wikipedia entity).</p><p>While we were unable to show a similar improvement by retrieving passages using information pertaining to the entities it contains, we conclude that part of the reason for this not working was because entities are less "specific' indicators of relevance than passages. For example, while the entity "United States" may be relevant to a query, this does not necessarily indicate that all passages that contain the entity "United States" are relevant. This suggests potential future work in understanding which entities are most important (salient) in indicating the relevance of passages that contain them. We show that under the right conditions (e.g., Lucene with English stemmer) a combination of traditional retrieval methods with learning to rank results in best performance.</p><p>On the NEWS entity ranking task we only retrieved entities from a Wikipedia dump, as no training data was provided. We see that using the first paragraph of the news article is as good as its title.</p><p>In comparison to other participating teams in CAR and NEWS track, our methods placed either best or second-best.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,152.91,233.63,13.38,72.05;11,159.56,233.64,13.38,34.77;11,166.21,233.62,13.38,62.35;11,172.86,233.65,13.38,47.29;11,179.51,233.64,13.38,40.47;11,186.16,233.63,13.38,29.44;11,192.81,233.64,13.38,89.86;11,199.46,233.62,13.38,40.04;11,206.11,233.63,13.38,13.85;11,212.76,233.61,13.38,90.35;11,219.41,233.63,13.38,31.18;11,226.06,233.64,13.38,36.82;11,232.71,233.64,13.38,34.71;11,239.36,233.64,13.38,26.34;11,246.01,233.63,13.38,28.50;11,252.66,233.65,13.38,93.06;11,259.31,233.65,20.03,102.06;11,272.61,233.65,13.38,44.15;11,144.58,223.17,11.25,13.38;11,144.58,198.37,11.25,13.38;11,144.58,173.56,11.25,13.38;11,144.58,148.75,11.25,13.38;11,131.02,173.80,13.38,20.56;11,179.81,343.43,62.59,6.99;11,344.37,235.18,13.38,38.61;11,355.86,235.18,24.87,47.16;11,378.83,235.16,13.38,76.52;11,390.32,235.16,13.38,74.78;11,401.80,235.17,13.38,86.22;11,413.29,235.17,13.38,100.54;11,424.78,235.18,13.38,82.23;11,436.26,235.19,13.38,60.04;11,447.75,235.18,24.87,36.77;11,333.62,224.72,11.25,13.38;11,333.62,197.39,11.25,13.38;11,333.62,170.07,11.25,13.38;11,333.62,142.75,11.25,13.38;11,320.07,175.34,13.38,20.56;11,371.25,343.43,57.79,6.99"><head></head><label></label><figDesc>uog-linear-ltr-hier-ent uog-paragraph-rf-ent uog-heading-rh-sdm-ent DWS-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,133.77,363.23,343.71,8.74;11,133.77,375.19,197.92,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: TREC CAR 2018 results for all participated systems, using the manual benchmark and Rprec as evaluation measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,148.71,608.15,328.77,20.69"><head>Table 1 :</head><label>1</label><figDesc>Paragraph feature results, Y1 Train tree qrels</figDesc><table coords="3,148.71,608.15,328.77,20.69"><row><cell>• ECM-psg: Like ECM expansion but expands the keyword query with top</cell></row><row><cell>100 expansion entities.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,133.77,553.09,343.71,115.74"><head>Table 2 :</head><label>2</label><figDesc>Performance of entity retrieval methods. Y1 Train and Y2 Test benchmarks are derived from an automatically generated ground truth. The Y2 Test benchmark contains the TREC 2018 manual assessments.</figDesc><table coords="11,137.94,600.93,333.05,67.90"><row><cell>Method</cell><cell></cell><cell>Y1 Train</cell><cell></cell><cell></cell><cell>Y1 Test</cell><cell></cell><cell></cell><cell>Y2 Test</cell><cell></cell></row><row><cell></cell><cell cols="9">MAP RPREC NDCG MAP RPREC NDCG MAP RPREC NDCG</cell></row><row><cell>UNH-e-l2r</cell><cell>0.15</cell><cell>0.17</cell><cell>0.42</cell><cell>0.17</cell><cell>0.18</cell><cell>0.44</cell><cell>0.31</cell><cell>0.31</cell><cell>0.51</cell></row><row><cell cols="2">UNH-e-graph 0.14</cell><cell>0.15</cell><cell>0.34</cell><cell>0.14</cell><cell>0.16</cell><cell>0.34</cell><cell>0.27</cell><cell>0.28</cell><cell>0.48</cell></row><row><cell>UNH-e-sdm</cell><cell>0.16</cell><cell>0.17</cell><cell>0.38</cell><cell>0.17</cell><cell>0.18</cell><cell>0.40</cell><cell>0.26</cell><cell>0.28</cell><cell>0.46</cell></row><row><cell cols="2">UNH-e-mixed 0.16</cell><cell>0.17</cell><cell>0.38</cell><cell>0.17</cell><cell>0.18</cell><cell>0.40</cell><cell>0.27</cell><cell>0.28</cell><cell>0.44</cell></row><row><cell>bm25-ecm</cell><cell>0.11</cell><cell>0.12</cell><cell>0.32</cell><cell>0.10</cell><cell>0.12</cell><cell>0.32</cell><cell>0.18</cell><cell>0.19</cell><cell>0.42</cell></row><row><cell>ql-ecm</cell><cell>0.11</cell><cell>0.13</cell><cell>0.33</cell><cell>0.11</cell><cell>0.13</cell><cell>0.32</cell><cell>0.17</cell><cell>0.19</cell><cell>0.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,133.77,126.37,343.71,171.15"><head>Table 3 :</head><label>3</label><figDesc>Performance of passage retrieval methods (submitted and low-level sectionpath baselines). Y1 Train and Y1 Test benchmarks are derived from an automatically generated ground truth. The Y2 Test benchmark contains the TREC 2018 manual assessments.</figDesc><table coords="12,133.77,186.15,337.25,111.37"><row><cell>Method</cell><cell></cell><cell>Y1 Train</cell><cell></cell><cell></cell><cell>Y1 Test</cell><cell></cell><cell></cell><cell>Y2 Test</cell><cell></cell></row><row><cell></cell><cell cols="9">MAP RPREC NDCG MAP RPREC NDCG MAP RPREC NDCG</cell></row><row><cell>UNH-p-l2r</cell><cell>0.12</cell><cell>0.09</cell><cell>0.20</cell><cell>0.13</cell><cell>0.11</cell><cell>0.27</cell><cell>0.34</cell><cell>0.33</cell><cell>0.58</cell></row><row><cell>UNH-p-SDM</cell><cell>0.15</cell><cell>0.12</cell><cell>0.25</cell><cell>0.14</cell><cell>0.13</cell><cell>0.24</cell><cell>0.30</cell><cell>0.31</cell><cell>0.47</cell></row><row><cell cols="2">UNH-p-mixed 0.16</cell><cell>0.14</cell><cell>0.25</cell><cell>0.15</cell><cell>0.13</cell><cell>0.25</cell><cell>0.29</cell><cell>0.32</cell><cell>0.47</cell></row><row><cell>bm25-none</cell><cell>0.14</cell><cell>0.11</cell><cell>0.26</cell><cell>0.13</cell><cell>0.10</cell><cell>0.25</cell><cell>0.30</cell><cell>0.30</cell><cell>0.54</cell></row><row><cell>ql-none</cell><cell>0.13</cell><cell>0.10</cell><cell>0.25</cell><cell>0.13</cell><cell>0.10</cell><cell>0.25</cell><cell>0.31</cell><cell>0.32</cell><cell>0.55</cell></row><row><cell>ql-rm</cell><cell>0.08</cell><cell>0.05</cell><cell>0.24</cell><cell>0.08</cell><cell>0.06</cell><cell>0.20</cell><cell>0.23</cell><cell>0.24</cell><cell>0.47</cell></row><row><cell>bm25-rm</cell><cell>0.11</cell><cell>0.08</cell><cell>0.24</cell><cell>0.11</cell><cell>0.09</cell><cell>0.24</cell><cell>0.29</cell><cell>0.29</cell><cell>0.52</cell></row><row><cell cols="7">11 Evaluation on TREC News</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,133.77,529.09,343.71,139.49"><head>Table 4 :</head><label>4</label><figDesc>Performance of entity re-ranking methods on NEWS. marks methods with significant difference to best performing method according to a paired-ttest with α = 0.05. CAR paragraphs. The ecm entitt ranking is derived from these paragraphs (using entity links provided by CAR organizers).</figDesc><table coords="12,162.00,578.48,283.44,90.10"><row><cell>Method</cell><cell></cell><cell>NEWS</cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>RPREC</cell><cell>NDCG@5</cell></row><row><cell>UNH-ParaBm25</cell><cell>0.82 ± 0.03</cell><cell>0.72 ± 0.04</cell><cell>0.74 ± 0.03</cell></row><row><cell>UNH-TitleBm25</cell><cell>0.81 ± 0.03</cell><cell>0.71 ± 0.04</cell><cell>0.72 ± 0.03</cell></row><row><cell cols="2">UNH-ParaBm25Ecm 0.71 ± 0.04</cell><cell>0.63 ± 0.05</cell><cell>0.55 ± 0.04</cell></row><row><cell>median run</cell><cell>N/A</cell><cell>0.66</cell><cell>0.62</cell></row><row><cell>best run</cell><cell>N/A</cell><cell>0.76</cell><cell>0.82</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,149.01,652.67,88.92,6.64"><p>https://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,149.01,649.51,148.20,6.64"><p>http://lemurproject.org/ranklib.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,149.01,658.44,307.09,7.21"><p>unprocessedAllButBenchmark.v2.1.tar.xz, Wikipedia dump from December 2016</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,149.01,627.63,222.10,7.21"><p>Available at http://trec-car.cs.unh.edu/runs/TREMA-UNH</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,149.27,558.29,328.21,8.74;13,149.27,570.25,136.57,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="13,359.03,558.29,118.44,8.74;13,149.27,570.25,106.70,8.74">Trec car 2.1: A data set for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Gamari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,149.27,590.17,328.22,8.74;13,149.27,602.13,328.22,8.74;13,149.27,614.08,310.27,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,309.11,602.13,168.37,8.74;13,149.27,614.08,88.99,8.74">Overcoming low-utility facets for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,246.53,614.08,129.32,8.74">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,149.27,634.01,328.21,8.74;13,149.27,645.96,264.01,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,315.27,634.01,143.90,8.74">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,149.27,645.96,84.68,8.74">ACM SIGIR Forum</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,149.27,127.96,328.21,8.74;14,149.27,139.92,328.21,8.74;14,149.27,151.87,328.21,8.74;14,149.27,163.83,120.43,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,368.39,127.96,109.09,8.74;14,149.27,139.92,150.46,8.74">Entity query feature expansion using knowledge base links</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,319.30,139.92,158.18,8.74;14,149.27,151.87,324.37,8.74">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,149.27,183.75,328.21,8.74;14,149.27,195.71,328.22,8.74;14,149.27,207.66,328.21,8.74;14,149.27,219.62,120.43,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,324.05,183.75,153.43,8.74;14,149.27,195.71,79.62,8.74">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,253.58,195.71,223.90,8.74;14,149.27,207.66,324.37,8.74">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
