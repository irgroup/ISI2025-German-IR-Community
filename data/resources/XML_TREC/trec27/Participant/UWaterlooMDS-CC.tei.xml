<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,80.70,104.50,357.41,12.55">UWaterlooMDS at the TREC 2018 Common Core Track</title>
				<funder ref="#_nhqjhPy">
					<orgName type="full">University of Waterloo</orgName>
				</funder>
				<funder ref="#_CJHh5Z7">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_bHz6fNC">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,80.70,132.49,119.60,10.63"><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.68,132.49,121.09,10.63"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.17,132.49,95.31,10.63"><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.87,132.49,81.62,10.63"><forename type="first">Amira</forename><surname>Ghenai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,80.70,146.44,122.15,10.63"><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.22,146.44,115.63,10.63"><forename type="first">Shahin</forename><surname>Rahbariasl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.22,146.44,105.58,10.63"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,480.64,146.44,50.67,10.63;1,80.70,160.38,40.79,10.63"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,80.70,104.50,357.41,12.55">UWaterlooMDS at the TREC 2018 Common Core Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7819D9EE3C960C174315FD29FF6543F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is year we applied dynamic sampling (DS) <ref type="bibr" coords="1,255.68,231.35,10.54,7.95" target="#b3">[4]</ref> to create a sampled set of relevance judgments. One goal was to test the e ectiveness and e ciency of this technique with a set of non-expert, secondary relevance assessors. We consider NIST assessors to be the experts and the primary assessors. Another goal was to make available to other researchers a sampled set of relevance judgments (prels) and thus allow the estimation of retrieval metrics that have the potential to be more robust than the standard NIST provided relevance judgments (qrels). In addition to creating the prels, we also submi ed several runs based on our manual judging and the models produced by our HiCAL system <ref type="bibr" coords="1,366.12,286.14,9.33,7.95" target="#b0">[1,</ref><ref type="bibr" coords="1,377.69,286.14,6.22,7.95" target="#b5">6]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">DYNAMIC SAMPLING</head><p>While we detail the steps of dynamic sampling (DS) in Algorithm 1, in this section, we rst give a general overview of the process followed and then later give the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview</head><p>Dynamic sampling (DS) <ref type="bibr" coords="1,176.88,383.75,11.58,8.84" target="#b3">[4]</ref> is a technique that creates a strati ed sample of relevance judgments for test collection construction. DS is performed for each topic in a test collection. In our implementation of DS, we start by creating a "zeroth" stratum that consists of judged documents found from a mixture of continuous active learning (CAL) <ref type="bibr" coords="1,511.03,407.66,10.26,8.84" target="#b1">[2,</ref><ref type="bibr" coords="1,523.19,407.66,8.11,8.84" target="#b2">3]</ref> and interactive searching and judging (ISJ). Because these documents are found without any sampling, they are all given an inclusion probability of 1.0. e authors performed all judging themselves.</p><p>With a set of relevant and non-relevant documents forming our zeroth stratum, we use these documents, plus a random selection of unjudged documents assumed to be non-relevant, to train a classi er. We then rank all documents, that are not yet a member of a stratum, using the classi er. To rank the documents, we rst divide each document into non-overlapping paragraphs. We then rank all paragraphs and select B unique documents with the highest scoring paragraphs. ese B documents form the next stratum. From the stratum, we then use simple random sampling to sample n documents for judging. Each of these documents will have an inclusion probability of n/B. When we judge documents for relevance, we select the paragraph with the highest probability of relevance as per our classi er and judge this paragraph. We do not view the whole document. Past research has shown that judging paragraphs is more e cient than judging full documents, and the judgments have comparable quality <ref type="bibr" coords="1,111.65,551.13,10.37,8.84" target="#b6">[7,</ref><ref type="bibr" coords="1,124.51,551.13,6.91,8.84" target="#b7">8]</ref>.</p><p>Both the stratum size B and the number of documents to sample n vary for the strata. Each stratum is larger than the previous one, and as relevant documents are found, the inclusion probability decreases. In addition to the di erences between the strata, a er each stratum is sampled and judged, the classi er is trained anew and thus should be be er at nding relevant documents for formation of the next stratum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Details</head><p>Algorithm 1 details the steps of our implementation of dynamic sampling. In Step 1, one of the authors used a live, human-in-the-loop, AutoTAR CAL implementation to assess in total 3648 documents over 50 topics, 1419 of them are relevant (38.9%). is author spent a total of 11.1 hours judging documents (13.2 minutes per topic). e same CAL implementation was used for relevance assessments for the MRG UWaterloo submission in TREC Common Core Track 2017 <ref type="bibr" coords="2,187.73,130.97,10.30,8.84" target="#b4">[5]</ref>. ese assessments in the initial train set were used to build the classi er in Step 7.</p><p>Among the assessments for 50 topics, we found 20 topics had less than 10 documents judged as relevant. In order to provide a be er prime model for dynamic sampling, we used interactive search and judging to augment the relevance assessments on those topics in Step 2. We conducted ad-hoc searches using the search model of the HiCAL system <ref type="bibr" coords="2,160.76,178.79,10.58,8.84" target="#b0">[1]</ref>. e interfaces of the search model remained the same but we replaced the back-end Indri search engine with Anserini<ref type="foot" coords="2,220.76,188.89,3.38,6.45" target="#foot_0">1</ref> . Five authors used this search engine and tried to nd at least 10 relevant documents on those 20 topics. e search engine returned 50 results by default. We reformulated queries as many times as we wanted. We allocated a maximum of 30 minutes of judging per each of these 20 topics.</p><p>We merged the relevance assessments from the AutoTAR CAL judgments and from interactive search and judging (ISJ). In some cases, the same document was judged by both the CAL process and the ISJ process. If a document was found to be relevant by either process, it was considered relevant. A er merging these two sets of assessments, we had in total 4161 judgments for 50 topics, in which 1645 of them were relevant (39.5%). ese assessed documents form an initial seed set (the zeroth stratum) and were not shown to assessors again in the dynamic sampling process.</p><p>A er having the initial seed documents in steps 1 and 2, we started the dynamic sampling process. We made the judgments on the CAL model of HiCAL system through step 6 to 14. In each iteration of dynamic sampling, the system displayed the selected paragraphs to assessors. ere was no option to view the full document. Each document was assessed only once and not shown to assessors again. e assessors judged 300 paragraphs for each topic and then the dynamic sampling process stopped. In Step 4, we set N = 25 in our experiment.</p><p>We randomized the 50 topics and assigned them to ve authors. ree assessors judged 24 topics in the rst pass. We found there existed a bug in our code. erefore, we rejudged those 24 topics in the second pass and nished all 50 topics. ere was no time limit for assessing documents for each topic. e assessments were nished within one week and averaged 33 minutes per topic. In total, we spent about one hour per topic to produce our relevance judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELEVANCE JUDGMENTS</head><p>e output of dynamic sampling is a set of sampled documents with inclusion probabilities. For each of these sampled documents, we have a relevance judgment. e judgments plus inclusion probabilities are called prels.</p><p>e prels contains ve elds: topic, assessed document id, stratum number, the inclusion probability of the assessed document, and relevance judgment of the document.</p><p>We provided our prels to NIST, who then had NIST assessors judge the same documents. With the NIST judgments and the inclusion probabilities, NIST was able to estimate the number of relevant documents in the collection for each topic.</p><p>As presented at TREC, at h p://cormack.uwaterloo.ca/sample/, we provide our relevance judgments along with a new evaluation program, DynEval, wri en by Gordon Cormack that can use either traditional trec eval qrels or xinfAP irels. In addition to our judgments, we also provide irels that combine our work with the judging done by NIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SUBMITTED RUNS</head><p>We submi ed four runs to the Common CORE track 2018 for evaluation. All these runs were manual runs. As per agreement with NIST, none of these runs were part of the pooling. For each topic, we built a nal classi er using all the judgments from the initial training set (stratum 0) and the dynamic sampling iterations (stratum 1, 2, 3, . . . ). e classi er was built on document features and was used to score all the documents in the collection.</p><p>UWaterMDS DS A: is run is composed by all documents we judged as relevant, ordered by the nal classi er. If we found fewer than 10 relevant documents, we then appended all documents we judged as nonrelevant, ordered by the classi er.</p><p>UWaterMDS DS B: is run is composed by all documents we judged as relevant, in reverse order by the nal classi er. If we found fewer than 10 relevant documents, we then appended all documents we judged as non-relevant, ordered by the classi er.</p><p>UWaterMDS Rank: We use the nal classi er to rank all documents and include the top 10,000 documents. UWaterMDS SEQ: e run is generated based on the order of our judgments. For the stratum 0, we rst put the AutoTAR CAL judged relevant documents and then the ISJ judged relevant documents, ordered by time of discovery. For the remaining strata, if a stratum had all documents sampled (inclusion probability 1.0), we put all judged relevant documents (no non-relevant ones) from that stratum by the order discovered. For the strata with documents having inclusion probabilities smaller than 1.0, we put all documents (including non-relevant and unjudged documents), ordered by the classi er.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,80.37,108.91,265.35,7.96;3,80.70,123.55,229.87,8.84;3,80.70,135.51,434.21,8.84;3,85.68,147.46,172.63,8.84;3,80.70,160.41,155.04,8.85;3,80.70,173.36,254.50,8.85;3,80.70,186.31,209.47,8.85;3,80.70,198.27,400.94,8.84;3,85.68,210.22,141.01,8.84;3,80.70,223.18,389.52,8.84;3,80.70,236.13,224.65,8.84;3,80.70,248.08,435.66,8.85;3,85.68,260.04,115.01,8.84;3,80.70,272.34,73.80,9.96;3,162.86,270.77,12.70,6.80;3,166.85,278.45,4.20,6.49;3,185.32,272.81,210.37,9.33;3,80.70,285.25,416.32,8.85;3,85.68,296.25,119.61,9.96;3,80.70,310.16,227.85,8.84;3,80.70,323.41,88.97,8.85;3,178.64,321.21,4.38,6.49;3,177.65,328.61,6.76,6.45;3,189.90,323.41,2.35,8.84;3,80.70,335.88,289.60,9.33;3,80.70,347.36,320.91,9.96"><head>ALGORITHM 1 :</head><label>1</label><figDesc>Dynamic sampling algorithm used in this experiment. Step 1. Use CAL to discover and label initial training set; Step 2. Use interactive search and judging to augment the initial training set for the topics on which Step 1 yielded fewer than 10 relevant documents; Step 3. Set the initial batch size B to 1; Step 4. Set the initial decay threshold T to hyper-parameter N ; Step 5. Set the initial number of assessments A to 0; Step 6. Temporarily augment the training set by adding 100 random document from the collection, temporarily labeled "not relevant"; Step 7. Construct a classi er from the training set and score all the paragraphs in the collection; Step 8. Remove the random documents added in step 6; Step 9. From the documents not yet part of a sampled stratum, select B documents such that they contain the highest scoring paragraphs.; Step 10. Draw n = B •N T ≤ B random documents from the Step 9 documents; Step 11. Assess relevance of the n documents based on viewing the highest scoring paragraph for each document. Update A = A + n; Step 12. Add the assessed documents to the training set; Step 13. Increase B by B 10 ; Step 14. If the number of assessed relevant documents R ≥ T , double T ; Step 15. Repeat step 6 through 14 until A = 300 documents have been assessed.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,83.93,649.30,121.47,7.07"><p>h ps://github.com/castorini/Anserini</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENTS is work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs> (Grants <rs type="grantNumber">CRDPJ 468812-14</rs>, <rs type="grantNumber">RGPIN-2017-04239</rs>, and <rs type="grantNumber">RGPIN-2014-03642</rs>), and in part by the <rs type="funder">University of Waterloo</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bHz6fNC">
					<idno type="grant-number">CRDPJ 468812-14</idno>
				</org>
				<org type="funding" xml:id="_CJHh5Z7">
					<idno type="grant-number">RGPIN-2017-04239</idno>
				</org>
				<org type="funding" xml:id="_nhqjhPy">
					<idno type="grant-number">RGPIN-2014-03642</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,98.77,121.31,432.54,7.07;4,98.77,131.28,432.53,7.07;4,98.28,141.24,73.45,7.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,500.62,121.31,30.69,7.07;4,98.77,131.28,108.49,7.07">A System for E cient High-Recall Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,233.81,131.28,297.49,7.06;4,98.28,141.25,30.31,7.06">41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR &apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1317" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,98.77,151.20,432.53,7.07;4,98.77,161.16,433.41,7.07;4,98.58,171.13,28.36,7.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,259.00,151.20,272.30,7.07;4,98.77,161.16,29.94,7.07">Evaluation of machine-learning protocols for technology-assisted review in electronic discovery</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,143.62,161.17,364.07,7.06">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,98.77,181.09,432.53,7.07;4,98.77,191.05,222.18,7.07" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,265.70,181.09,265.60,7.07;4,98.77,191.05,21.47,7.07">Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno>arxiv.org/abs/1504.06868</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,98.77,201.01,432.82,7.07;4,98.77,210.98,430.57,7.07" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210119</idno>
		<title level="m" coord="4,271.42,201.01,260.16,7.07;4,98.77,210.98,150.92,7.06">Beyond Pooling. In e 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR &apos;18)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1169" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,98.77,220.94,432.53,7.07;4,98.77,230.90,433.41,7.07;4,98.77,240.87,15.91,7.06" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,266.51,220.94,264.79,7.07;4,98.77,230.90,35.35,7.07">MRG UWaterloo and WaterlooCormack Participation in the TREC 2017 Common Core Track</title>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,148.45,230.91,186.56,7.06">Proceedings of e Twenty-Sixth Text REtrieval Conference</title>
		<meeting>e Twenty-Sixth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-15">2017. 2017. November 15-17, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,98.77,250.83,433.77,7.07;4,98.77,260.79,260.90,7.07" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angshuman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<title level="m" coord="4,228.90,260.79,104.59,7.07">TREC 2017 Common Core Track</title>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,98.77,270.75,432.54,7.07;4,98.77,280.72,432.54,7.07;4,98.77,290.68,391.14,7.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,503.17,270.75,28.13,7.07;4,98.77,280.72,180.78,7.07">E ective User Interaction for High-Recall Retrieval: Less is More</title>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3269206.3271796</idno>
		<ptr target="p://dx.doi.org/10.1145/3269206.3271796" />
	</analytic>
	<monogr>
		<title level="m" coord="4,294.16,280.72,237.14,7.06;4,98.77,290.69,113.57,7.06">Proceedings of the 27th ACM International Conference on Information and Knowledge Management (CIKM &apos;18)</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management (CIKM &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,98.77,300.64,432.75,7.07;4,98.77,310.60,250.44,7.07" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08988</idno>
		<title level="m" coord="4,379.66,300.64,151.85,7.07;4,98.77,310.60,120.79,7.07">Evaluating Sentence-Level Relevance Feedback for High-Recall Information Retrieval</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
