<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,180.26,115.96,254.83,12.62">htw saar @ TREC 2018 News Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.96,153.63,68.64,8.74"><forename type="first">Agra</forename><surname>Bimantara</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.81,153.63,57.78,8.74"><forename type="first">Michelle</forename><surname>Blau</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.54,153.63,75.50,8.74"><forename type="first">Kevin</forename><surname>Engelhardt</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.00,153.63,76.65,8.74"><forename type="first">Johannes</forename><surname>Gerwert</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.25,165.58,77.28,8.74"><forename type="first">Tobias</forename><surname>Gottschalk</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.39,165.58,63.16,8.74"><forename type="first">Philipp</forename><surname>Lukosz</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.57,165.58,50.06,8.74"><forename type="first">Shenna</forename><surname>Piri</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.75,165.58,77.07,8.74"><forename type="first">Nima</forename><forename type="middle">Saken</forename><surname>Shaft</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,272.55,177.54,70.26,8.74"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
							<email>klaus.berberich@htwsaar</email>
							<affiliation key="aff0">
								<address>
									<addrLine>htw saar, Goebenstrasse 40</addrLine>
									<postCode>66117</postCode>
									<settlement>Saarbruecken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,180.26,115.96,254.83,12.62">htw saar @ TREC 2018 News Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16691FAAE1C54B029A30B517EE6E3795</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the background linking task of the TREC 2018 News Track. We explored four different methods to address the task. All of our methods largely rely on offthe-shelf open-source components (e.g., Apache Lucene for indexing the documents). The methods differ in how they analyze the given input document to obtain a query (e.g., by keyword extraction or named entity recognition) and to what extent the returned results are re-ranked taking meta data of the documents (e.g., publication dates) into account.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The News Track is a new track first run in TREC 2018. It considers two tasks, namely background linking and entity retrieval. While the latter focuses on ranking named entities mentioned in a given document, the former starts from a document and seeks to retrieve other documents published earlier that can provide background information to the user to help her understanding. The two tasks are tested on a collection consisting of more than 590, 000 news articles published by The Washington Post.</p><p>Saarbr√ºcken University of Applied Sciences (htw saar) participated in the background linking task of the TREC 2018 News Track. In this paper, we describe our approach to address this task. The proposed methods were designed and implemented by two teams of undergraduate students, who had little prior knowledge about Information Retrieval, as parts of a semester project and thesis work, respectively. We tested and submitted runs for four different methods, which were implemented largely relying on off-the-shelf open-source components such as Apache Lucene for indexing the document collection.</p><p>In the rest of the paper, we provide details on how the provided document collection was preprocessed in Section 2. Details about our methods are described in Section 3. Following that, in Section 4 we discuss the results as reported back by TREC. Finally, we draw conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset and Preprocessing</head><p>In this section, we describe the document collection provided by TREC and preprocessing steps that we applied to it prior to indexing.</p><p>The document collection provided by TREC consists of more than 590, 000 news articles published by The Washington Post either in print or as blog posts. After its initial release, the document collection was revised to remove documents having the same identifier. Even after this revision, we observed duplicate documents, with different identifiers but identical content, in our results. As discussed below, our methods eliminate such duplicates either in a preprocessing step, when indexing the document collection, or in a result filtering step, i.e. post retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We next describe the four methods that produced our submitted runs (htwsaar1-4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method 1 (htwsaar1)</head><p>Methods 1 and 2 rely on Elasticsearch <ref type="bibr" coords="2,303.64,315.57,10.52,8.74" target="#b0">[1]</ref> as an indexing platforms. The revised document collection provided by TREC was indexed, treating all fields from the JSON documents separately. Elasticsearch builds on-top of Lucene and we used the default settings for tokenization and normalization, i.e., words were lowercased but not stemmed. Also for the retrieval model, we rely on the default setting, which is a variation of the vector space model <ref type="bibr" coords="2,373.54,375.34,9.96,8.74" target="#b2">[3]</ref>.</p><p>Our first method analyzes the input document by performing a simple keyword extraction. More precisely, terms contained in the input document are ranked according to their value</p><formula xml:id="formula_0" coords="2,252.60,432.90,101.63,22.31">tf (v, d) ‚Ä¢ log 1 + |D| df (v)</formula><p>with tf (v, d) as the frequency of term v in the input document d, |D| as the size of the document collection, and df (v) as the document frequency of term v. The twenty terms achieving the highest value are selected and used as a query. In addition, a date filter is applied to allow only documents published on or before the publication date of the input document.</p><p>The result returned by Elasticsearch are re-ranked taking their publication dates into account. The score attached to every result document is modified by a factor of e -Œª(|tq-t|)</p><p>with t q as the publication date of the input document and t as the publication date of the considered result document. Here, publication dates are represented as UNIX timestamps in milliseconds and Œª = 10 -12 was employed as a parameter setting. As an additional post-filtering step, duplicate documents having the same title, the same authors, and published on the same date are removed. Moreover, based on the task instructions, all documents published in the sections "Opinion", "Letters to the Editor", and "The Post's View" are also filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method 2 (htwsaar2)</head><p>Our second method operating on the index built using Elasticsearch relies on named entity recognition to construct a query for the given input document.</p><p>Stanford CoreNLP <ref type="bibr" coords="3,222.20,162.82,10.52,8.74" target="#b4">[5]</ref> is used to analyze the title of the document and spot mentions of named entities. Terms belonging to such a mentioned of a named entity are concatenated to yield the query that is issued. If the title of the document does not contain any mention of a named entity, the method falls back to using the entire title as a query. The returned results undergo the same temporal re-ranking and post-filtering as described above for Method 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Method 3 (htwsaar3)</head><p>Methods 3 and 4 were implemented in a separate system, relying on Apache Lucene <ref type="bibr" coords="3,168.91,284.33,10.52,8.74" target="#b1">[2]</ref> as an indexing platform. As a pre-processing step, based on our observation that duplicate documents were still present in the collection, we applied a simple heuristic to remove suspected duplicates. More precisely, when processing input files in lexicographic order of their file name, we check for every document whether a document with the same title, written by the same author, and published on the same date has already been seen. If this is the case for a document, it is ignored and not added to our index. Moreover, based on task instructions, all documents published in the sections "Opinion", "Letters to the Editor", and "The Post's View" are also filtered out. This leaves us with a total of 480, 627 documents. From the JSON documents provided by TREC we extracted the author, publication date, URL, title, and contents and indexed them as separate fields. For the title and content fields we relied on Lucene's default tokenization and normalization, i.e., words were lowercased but not stemmed. The other fields were indexed as-is, in the case of publication date as a numerical value to allow filtering based on date ranges.</p><p>Our first method operating on the thus created index relies on a two-step approach. In a first step, the given input document is analyzed to construct a query that can be issued against Lucene. The returned results, in a second step, are then re-ranked taking into account meta data of the input documents. In the following, we describe these two steps in more detail.</p><p>Query Construction by Document Analysis Not all words contained in an input document are equally useful as query terms -at least that was our initial intuition. To identify salient words contained in the input document, more specifically its title and content, our third method relies on key-phrase extraction and named entity recognition. We make use of TextRank <ref type="bibr" coords="3,419.30,589.18,10.52,8.74" target="#b3">[4]</ref> to identify key phrases contained in the title or content of the input document, relying on the implementation<ref type="foot" coords="3,220.00,611.51,3.97,6.12" target="#foot_0">1</ref> provided by one of the original authors. Likewise, to spot mentions of named entities (specifically persons, organization, and locations), we again use Stanford CoreNLP <ref type="bibr" coords="3,278.23,637.00,10.52,8.74" target="#b4">[5]</ref> with its default models for English.</p><p>Having extracted key phrases and named entities, we next determine how often each of them is mentioned in the input document and rank them accordingly. Thus we obtain four ranked lists of key phrases, persons, organizations, and locations in descending order of their frequency in the document. Let m 1 , . . . , m n denote one such ranked list, we boost the terms contained in the mention m i at rank i as max</p><formula xml:id="formula_1" coords="4,275.71,190.72,78.16,9.65">(1.1, C T -0.1 ‚Ä¢ i)</formula><p>with C T as a constant that is chosen for each of the four types of mentions that we consider. For key phrases C K = 3.0, for persons C P = 3.0, for organizations C O = 2.1, and locations C L = 1.8. These choices reflect our initial expectation regarding the importance of the different types of mentions. To illustrate this with an example, assume that michael jordan was found to be the second most frequent named entity mentioned in the input document. In this case, the terms michael and jordan will be assigned a boost of 2.8. If a term is found in different types of mentions in the document (e.g., as a person and in a key phrase), the largest boost determined is used. Boosting query terms in Lucene effectively inflates their term frequency in any document by a factor corresponding to the specified boost. As a result, these terms end up having a stronger influence on the result, which is determined using a variation of the vector space model <ref type="bibr" coords="4,470.07,339.13,10.52,8.74" target="#b2">[3]</ref> in Lucene. The query that is issued against Lucene then consists of all terms that we found in any key phrase or named entity boosted as described above. In addition, a date filter is applied, allowing only such documents into the result that were published on or before the publication date of the input document.</p><p>Re-Ranking based on Meta Data Once result documents have been obtained for the query constructed in the first step, we re-rank them taking into account their author and publication date. For the publication date, we reckon that documents published at a time closer to the publication date of the input document are preferable. Thus, we determine how many weeks w a document has been published before the input document and boost its score as returned by Lucene by a factor of 1 + 0.7 ‚àö 1 + w .</p><p>A document published in the same week as the input article thus obtains a boost of 1.7, whereas a document published three weeks before the input article only obtains a boost of 1.35. As a second criterion for re-ranking we check whether a document was written by the same author as the input document. If this is the case, its score is boosted by a factor of 1.2 and remains unaltered otherwise.</p><p>The idea here is that authors tend to specialize on certain topics or events, so that documents by the same author should be more likely to be related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Method 4 (htwsaar4)</head><p>Our last method relies on the same approach for indexing documents and removing duplicates as Method 2. In contrast, it uses a much simpler approach to derive a query from the input document and does not apply any re-ranking based on content or meta data. It simply forms a verbose query by concatenating the title and the body of the input document and issues it against Lucene. Again, a date filter is applied to allow only documents published on or before the publication date of the input document. This was the first method developed and served as a baseline for internal testing. Quite surprisingly, it turned out to be the best performing among our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We now discuss our findings regarding the performance of our four methods based on the results provided by TREC. Since, at the time of writing, runs for other participants are not yet available, we compare our methods against a hypothetical competitor (median) that achieves median performance for all topics.</p><p>For our discussion, we focus on nDCG@5 as a primary effectiveness measure, in line with the task description.</p><p>Table <ref type="table" coords="5,164.29,344.58,4.13,7.89">1</ref>. nDCG@5 for a hypothetical competitor that achieves median performance for all topics (median) and our four runs (htwsaar1-4)</p><formula xml:id="formula_2" coords="5,195.64,389.38,275.59,7.47">median htwsaar1 htwsaar2 htwsaar3 htwsaar4</formula><p>nDCG@5 0.3448 0.4150 0.1957 0.4609 0.4619</p><p>Overall Performance Table <ref type="table" coords="5,269.76,464.84,4.98,8.74">1</ref> lists nDCG@5 across all topics. As can be seen from the figures, our second method htwsaar2, which relies on extracting named entities from the title of the input document, can not outperform the hypothetical competitor. This can be explained by the fact that, if the title contains only few named entities, the method ends up generating a very short query, arguably loosing the gist of the input document. All of our other methods (i.e., htwsaar1, htwsaar3, and htwsaar4) do outperform the hypothetical competitor by a reasonable margin. Among them, quite surprisingly, the simplest method htwsaar4, which uses the entire input document as a query, achieves the best performance. The two methods htwsaar1 and htwsaar3 relying on an analysis of the input document and re-ranking based on meta data follow closely. Our take home from these results is that verbose queries, retaining many or all of the terms from the input document, seem favorable for the task. Whether our simple baseline (htwsaar4) with its very verbose queries would be viable in a production setting is questionable. The two methods (htwsaar1 and htwsaar3) conducting an analysis of the entire document, yielding shorter queries, may be more appropriate, even though they sacrifice a bit of retrieval effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gains and Losses</head><p>To shed some light on in which cases our methods work and in which cases they don't, we conduct a gains and losses analysis. For each of our four methods, we identify the topic for which it achieves the highest absolute gain or loss in terms of nDCG@5 relative to the hypothetical competitor. Table <ref type="table" coords="7,475.61,154.86,4.98,8.74">2</ref> lists the identified topics. Interestingly, two of our methods (htwsaar1 and htwsaar4) achieve their biggest gains for the same topic 811. Inspecting the document, we observe that it mentions a fair amount of named entities and technical terms, which can be picked up by our methods when constructing the query. Our method htwsaar2 achieves its biggest gain for the topic 378, for which it manages to recognize the two key named entities Germans and Greece from the title. Topic 646, for which our method htwsaar3 achieves its biggest gain, mentions a fair amount of locations and our method is able to pick up key phrases such as food stamps and SNAP benefits.</p><p>Looking at biggest losses, we observe that our methods suffer them for different topics. Topic 455, which our method htwsaar1 looses on, corresponds to a relatively short document, which makes it harder for the method to extract meaningful key words. Our method htwsaar2 looses on topic 805, a document for which only Potomac is recognized as a named entity in the title, resulting in a very unspecific query. Topic 341 is the one for which our method htwsaar3 suffers its biggest loss. Inspecting the document, we observe that many of the named entities therein are related to the U.S., shifting the focus of the generated query away from the terror attacks in Brussels. Finally, our method htwsaar4 looses on topic 818. The document about dietary recommendations related to cholesterol contains many general terms, leading to a diluted query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we described our methods that participated in the TREC 2018 News Track. Based on the results available at this time, we conducted an initial analysis. Interestingly, the simplest method that we considered achieved the best results in terms of nDCG@5 among our methods. More sophisticated methods, relying on an analysis of the input document, got close in terms of nDCG@5 and may be more viable in practice than using the entire input document as a query. We also observed that relying only on the title of the input document is insufficient, our method based on this strategy performed worst among our methods.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,133.43,7.86"><p>https://github.com/boudinfl/pke</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,142.56,71.07,7.89;8,146.91,153.55,298.69,7.86" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Elasticsearch</surname></persName>
		</author>
		<ptr target="https://www.elastic.co/products/elasticsearch" />
		<imprint>
			<date type="published" when="2018-10-26">2018/10/26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,164.48,81.52,7.89;8,146.91,175.46,212.15,7.86" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Apache</forename><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org" />
		<imprint>
			<date type="published" when="2018-10-26">2018/10/26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,186.40,126.57,7.89;8,146.91,197.38,315.95,7.86" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Apache</forename><surname>Lucene -Scoring</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/350/scoring.html" />
		<imprint>
			<date type="published" when="2018-10-26">2018/10/26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,208.34,342.24,7.86;8,146.91,219.30,333.67,7.86;8,146.91,230.26,333.68,7.86;8,146.91,241.22,333.67,7.86;8,146.91,252.18,194.82,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,317.23,208.34,163.35,7.86;8,146.91,219.30,102.90,7.86">Topicrank: Graph-based topic ranking for keyphrase extraction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bougouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daille</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/I/I13/I13-1062.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,277.51,219.30,203.07,7.86;8,146.91,230.26,100.94,7.86;8,204.39,241.22,243.12,7.86">Sixth International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10-14">2013. October 14-18, 2013. 2013</date>
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing / ACL</note>
</biblStruct>

<biblStruct coords="8,138.35,263.14,342.24,7.86;8,146.91,274.09,333.68,7.86;8,146.91,285.05,333.68,7.86;8,146.91,296.01,210.31,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,163.62,274.09,247.05,7.86">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P14/P14-5010" />
	</analytic>
	<monogr>
		<title level="m" coord="8,434.41,274.09,46.18,7.86;8,146.91,285.05,252.62,7.86">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
