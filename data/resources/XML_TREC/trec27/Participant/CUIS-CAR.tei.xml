<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.79,84.23,282.92,15.44">Entity Retrieval in the Knowledge Graph with Hierarchical Entity Type and Content</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2018-09-10">2018-09-10</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.71,110.40,50.96,10.59"><forename type="first">Xinshi</forename><surname>Lin</surname></persName>
							<email>xslin@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.54,110.40,43.56,10.59"><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<email>wlam@se.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kwun</forename><forename type="middle">Ping</forename><surname>Lai</surname></persName>
						</author>
						<title level="a" type="main" coord="1,164.79,84.23,282.92,15.44">Entity Retrieval in the Knowledge Graph with Hierarchical Entity Type and Content</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval</title>
						<meeting>the 2018 ACM SIGIR International Conference on Theory of Information Retrieval						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<date type="published" when="2018-09-10" />
						</imprint>
					</monogr>
					<idno type="MD5">AB71D2430A7A6DDC6556F9602668B371</idno>
					<idno type="DOI">10.1145/3234944.3234963</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the Complex Answer Retrieval(CAR) Track at TREC 2018. We propose a Markov random field based framework concerning unigrams, bigrams and concepts from differnt query sections. Besides, we employ a language modelling framework facilitating the Wikipedia article information and query entity mentions. Our best passage run achieves NDCG@5 of 0.3503 and MAP of 0.1715.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The TREC 2018 Complex Answer Retrieval (CAR) Track provides a forum for soliciting works in meeting complex information needs with long answers. The task and related dataset are based on the assumption that each Wikipedia page represents a complex topic, with further details under each sections. The goal of the task is presented as such: given an outline of a page (in the form of the page title and hierarchical section headings), retrieve a ranking of paragraphs for each section <ref type="bibr" coords="1,156.89,347.60,9.37,7.94" target="#b1">[2]</ref>. A complex query can be long and composed of several query sections. Each complex query indicate one or several aspects of the article stub. For example, the complex query "new york yankees/rivalries" concerns the competition between baseball teams. This brings challenges to existing approaches to model term dependencies between different query sections and differentiate their importance.</p><p>We propose a Markov random field based model where unigrams, bigrams and phrases generated from different query sections. Besides, our system incorporates the Wikipedia article information and query entity mentions based on a Dirichlet prior smoothed language modeling framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL DESCRIPTION 2.1 Overview</head><p>Given a complex query Q and a candidate paragraph D, we propose a MRF-based framework considering D and its corresponding Wikipedia article H . The overall ranking function F (Q, D) is formulated as follows:</p><formula xml:id="formula_0" coords="1,116.72,563.26,177.33,38.13">F (Q, D) = w par a f par a (Q, D) + w wiki f wiki (Q, H ) + w el r f el r (E (Q) , E (D))<label>(1)</label></formula><p>where f par a (D, Q) and f wiki (Q, H ) are ranking functions for a candidate paragraph D, its corresponding Wikipedia article H respectively. The function E () returns the set of entity mentions given a segment of text. The ranking function f el r (E (Q) , E (D)) scores the paragraph D concerning the occurrences of query entity mentions E (Q) in the paragraph. w par a , w wiki and w el r are parameters satisfying that w par a + w wiki + w el r = 1.</p><p>A Markov Random Field based Paragraph Ranking Model. Assuming that a complex query is in the form of Q = s 1 s 2 ...s n where s i Figure <ref type="figure" coords="1,346.51,237.34,3.45,7.70">1</ref>: An example of the concept expansion scheme for the complex query "Green sea turtle/conservation/threats". Query phrases are generated by the terms connected with lines sharing the same color. The generated concepts for this query are "Green conservation", "Green threats", "conservation threats", "sea conservation", "sea threats", "turtle conservation", "turtle threats" and "conservation threats".</p><p>denote the i-th query section. The query section s i contains m i query terms q i 1 q i 2 ...q i m i . Since the sequential dependence model cannot model the term dependencies between long distance dependence. We consider query concept expansion by sampling query terms from different query sections. Figure <ref type="figure" coords="1,478.64,379.43,4.21,7.94">1</ref> demonstrates an example of generating query phrases from the query "Green sea turtle/conservation/threats". As shown in the figure, query terms from different query sections within a distance are considered to be relevant in context. We generate new query phrases from these relevant terms. Thus, the paragraph ranking function f par a (Q, D) is formulated as follows:</p><formula xml:id="formula_1" coords="1,356.50,460.54,201.71,76.55">f par a (Q, D) =λ T q ∈Q f T (q, D)+ λ O q ix ,q jy ∈Q f O q i x , q j y , D + λ U q ix ,q jy ∈Q f U q i x , q j y , D<label>(2)</label></formula><p>where i and j are the query section numbers satisfying |i -j | ≤ k.</p><p>x and y are the indices of query terms that enumerate integer in 1..m i and 1..m j respectively. f T , f O and f U are feature functions for matching query terms, ordered bigrams and unordered bigrams within a window of 8 words. They can formulated in a uniform way as follows:</p><formula xml:id="formula_2" coords="1,362.20,616.26,196.00,27.06">f {T ,O ,U } (W , D) = log t f W ,D + µ cf W |C | |D| + µ<label>(3)</label></formula><p>where W generalizes to query terms, phrases and concepts. t f and c f denote term frequencies and collection frequencies of W respectively. µ is the Dirichlet prior.</p><p>Exploiting Wikipedia Article Information. A Wikipedia article is organized by sections. It can be represented as a tree where each node corresponds to a section and its children represent subsections. We use the notation H to denote the tree representation of a corresponding Wikipedia article for the candidate paragraph D. We consider the occurrences of each query term q i in each sequence of sections and find the one that has the most relevant sections to the query. The scoring function is formulated as follows according to <ref type="bibr" coords="2,63.39,153.55,9.50,7.94" target="#b3">[4]</ref>:</p><formula xml:id="formula_3" coords="2,107.19,165.14,186.86,21.43">f wiki (Q, H ) = max p ∈H q i ∈Q hT (q i , H, p)<label>(4)</label></formula><p>hT</p><formula xml:id="formula_4" coords="2,96.65,194.21,197.40,30.94">(q i , H, p) = log s j ∈p β j • t f q i ,s j + µ d c f q i ,C d |C d | s j ∈p β j • |s j | + µ d<label>(5)</label></formula><p>where p enumerates each directed path in H that starts from a leaf and ends at an immediate child of the root. s j denotes the jth section in p. t f q i ,s j is the raw frequency of q i in s j . C d is the collection of all Wikipedia articles from the knowledge sources.</p><p>The smoothing parameter µ d is set to the average document length in C d . β is the reduction coefficient.</p><p>Exploiting Query Entity Mentions. We employ a Dirichlet prior smoothed entity language model to score the paragraph D. The ranking function f elr is formulated as follows:</p><formula xml:id="formula_5" coords="2,88.02,339.67,206.03,31.57">f el r (E (Q) , E (D)) = e ∈E(Q ) log t f e,E(D) + µ c f e |C | |E (D) | + µ<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENT 3.1 Experimental Setup</head><p>We use the TREC CAR v2.1 benchmark dataset <ref type="bibr" coords="2,221.65,413.06,10.43,7.94" target="#b0">[1]</ref> with corresponding Wikipedia dump as our knowledge base. We use Lucene to implement our model and construct the index. Each paragraph in the index is represented with three fields: id, wiki article id and content, which denote the paragraph identifier, the Wikipedia article the paragraph belongs to and the paragraph content. All index terms are stemmed using the Snowball stemmer and filtered with NLTK stopword list. We use Tagme <ref type="bibr" coords="2,183.99,489.78,10.47,7.94" target="#b2">[3]</ref> that indexes the Wikipedia dump on 2016-10 to annotate entity mentions in queries and paragraphs. Illegal entity mentions are removed. Our system consists of two stages. The first stage chooses top 1000 candidate passages based on Lucene's BM25 method. The second stage reranks those passages based on our proposed framework. We submitted the two passages runs and three entity runs. Each run is named after a popular vehicle in North America. The details of runs are listed as follows:</p><p>• • CUIS-Swift: simply transform the passage ranking results in the run "CUIS-MX5". Less than top 100 candidate entities are reported for each query. • CUIS-XTS: simply transform the passage ranking results in the run "CUIS-F150". Less than top 100 candidate entities are reported for each query. • CUIS-dogeDodge: simply transform the passage ranking results in the run "CUIS-MX5". The results are complemented by a Dirichlet prior smoothed language model on the collection of Wikipedia articles if there are less than 100 original candidate entities for each query. For our model, the parameters λ T , λ O and λ U for SDM and FSDM are set to 0.8, 0.1 and 0.1 respectively. The parameter k is set to 4. For the Wikipedia article scoring function, the parameter β is set to 1. We train our framework on the benchmarkY1-testpublic.v2.0 dataset to obtain the best values of w par a , w wiki and w elr . The Dirichlet prior µ in the ranking functions are set to the average length of paragraphs, sections or number of entities in the collection.</p><p>After we had submitted the runs to the TREC 2018, we examined our implementation and runs. We found bugs in the implementation of the scoring function f wiki that results in empty output for all queries. In other words, our submitted runs were generated using the ranking system whose f wiki is always zero. Here we report new runs based on the corrected ranking framework reporting top 1000 candidates. These runs are denoted with the identifier 'fix'.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Result and Discussion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,317.66,406.64,240.54,98.78"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results for passage ranking runs based on manual judgments.</figDesc><table coords="2,338.09,440.68,199.97,64.73"><row><cell>run name</cell><cell cols="2">NDCG@5 NDCG R-Prec MAP</cell></row><row><cell>CUISPR</cell><cell>0.3704</cell><cell>0.5368 0.3230 0.3079</cell></row><row><cell>CUIS-F150</cell><cell>0.3503</cell><cell>0.3113 0.2094 0.1715</cell></row><row><cell>CUIS-MX5</cell><cell>0.3406</cell><cell>0.4218 0.1955 0.1771</cell></row><row><cell cols="2">CUIS-F150-fix 0.3764</cell><cell>0.4933 0.2512 0.2462</cell></row><row><cell cols="2">CUIS-MX5-fix 0.3753</cell><cell>0.5012 0.2499 0.2451</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,317.66,531.13,240.54,87.02"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for entity ranking runs based on automatic judgments.</figDesc><table coords="2,332.40,565.18,211.36,52.98"><row><cell>run name</cell><cell cols="2">Automatic NDCG@5 NDCG R-Prec MAP</cell></row><row><cell>CUIS-Swift</cell><cell>0.0635</cell><cell>0.0957 0.0409 0.0256</cell></row><row><cell>CUIS-XTS</cell><cell>0.0664</cell><cell>0.0887 0.0426 0.0264</cell></row><row><cell cols="2">CUIS-dogeDodge 0.0632</cell><cell>0.0954 0.0406 0.0254</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="2,317.53,635.74,240.68,73.70"><head>Table 1</head><label>1</label><figDesc>reports the results of our submitted passage runs and the new runs. As shown in the table, our model that only incorporates Wikipedia article information has better one-page performance. It has NDCG@5 of 0.3503 and R-Prec of 0.2094. However, the full model that incorporates both Wikipedia article information and query entity mentions outperform the previous one in terms of NDCG of 0.4218 and MAP of 0.1771. The gap between CUIS-F150</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,53.50,85.73,240.55,176.94"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results for entity ranking runs based on manual judgments. in terms of NDCG are due to different number of top candidate paragraphs output for evaluation. Table2and 3 report the results of our entity runs. The submitted three runs achieve roughly equal performance. This is because most retrieved relevant paragraphs belong to the same Wikipedia article. Our proposed method cannot differentiate the importance of different Wikipedia articles.</figDesc><table coords="3,53.80,119.77,225.80,77.14"><row><cell>run name</cell><cell cols="2">Manual NDCG@5 NDCG R-Prec MAP</cell></row><row><cell>CUIS-Swift</cell><cell>0.1654</cell><cell>0.2287 0.1300 0.1026</cell></row><row><cell>CUIS-XTS</cell><cell>0.1654</cell><cell>0.2109 0.1259 0.0988</cell></row><row><cell cols="2">CUIS-dogeDodge 0.1688</cell><cell>0.2311 0.1309 0.1042</cell></row><row><cell>and other variants</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4">CONCLUSION</head><p>We participated in the Complex Answer Retrieval (CAR) Track at TREC 2018. We propose a Markov random field based framework concerning unigrams, bigrams and phrases induced by query terms from different query sections. Besides, we employ a language modeling framework facilitating the Wikipedia article information and query entity mentions. Our best passage run achieves NDCG@5 of 0.3503 and MAP of 0.1715. Future work includes the investigation of a new framework that can differentiate the importance of query concepts and an individual entity ranking model without the help of passage ranking results.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="3,330.15,197.69,228.19,6.18;3,330.15,205.66,46.61,6.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="3,438.76,197.69,119.57,6.18;3,330.15,205.66,22.90,6.18">Trec car: A data set for complex answer retrieval</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gamari</forename><surname>Ben</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,330.15,213.63,228.05,6.18;3,330.15,221.55,228.05,6.23;3,329.72,229.52,21.48,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,541.22,213.63,16.98,6.18;3,330.15,221.60,103.92,6.18">TREC Complex Answer Retrieval Overview</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,417.98,213.63,98.81,6.18;3,446.61,221.55,111.59,6.23;3,329.72,229.52,18.41,6.23">Proceedings of Text REtrieval Conference (TREC)</title>
		<meeting>Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Filip Radlinski, and Nick Craswell</note>
</biblStruct>

<biblStruct coords="3,330.15,237.54,229.23,6.18;3,330.15,245.46,228.05,6.23;3,330.15,253.43,228.82,6.23;3,330.15,261.45,228.05,6.18;3,329.94,269.42,162.80,6.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,526.62,237.54,32.76,6.18;3,330.15,245.51,145.90,6.18">On the Reproducibility of the TAGME Entity Linking System</title>
		<author>
			<persName coords=""><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Svein</forename><forename type="middle">Erik</forename><surname>Bratsberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,488.70,245.46,69.50,6.23;3,330.15,253.43,23.91,6.23">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Giorgio</forename><surname>Maria</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Di</forename><surname>Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gianmaria</forename><surname>Silvello</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="436" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,330.15,277.39,228.05,6.18;3,330.15,285.31,228.06,6.23;3,330.15,293.28,228.82,6.23;3,330.15,301.30,202.69,6.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,463.42,277.39,94.78,6.18;3,330.15,285.36,137.61,6.18">Entity Retrieval in the Knowledge Graph with Hierarchical Entity Type and Content</title>
		<author>
			<persName coords=""><forename type="first">Xinshi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kwun Ping</forename><surname>Lai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3234944.3234963</idno>
		<ptr target="https://doi.org/10.1145/3234944.3234963" />
	</analytic>
	<monogr>
		<title level="m" coord="3,479.97,285.31,78.24,6.23;3,330.15,293.28,208.79,6.23">Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR &apos;18)</title>
		<meeting>the 2018 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="211" to="214" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
