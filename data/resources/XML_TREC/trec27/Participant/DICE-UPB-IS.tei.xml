<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.89,116.06,345.58,12.68;1,143.13,133.99,329.10,12.68">DICE @ TREC-IS 2018: Combining Knowledge Graphs and Deep Learning to Identify Crisis-Relevant Tweets</title>
				<funder ref="#_PR4YQ85">
					<orgName type="full">BMVI</orgName>
				</funder>
				<funder ref="#_UPwJy9q">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder ref="#_4jFGbZG">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.47,171.64,77.18,8.90"><forename type="first">Hamada</forename><forename type="middle">M</forename><surname>Zahera</surname></persName>
							<email>hamada.zahera@uni-paderborn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Data Science Group</orgName>
								<orgName type="institution">Paderborn University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.62,171.64,52.29,8.90"><forename type="first">Rricha</forename><surname>Jalota</surname></persName>
							<email>rricha.jalota@uni-paderborn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Data Science Group</orgName>
								<orgName type="institution">Paderborn University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.75,171.64,63.91,8.90"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
							<email>ricardo.usbeck@uni-paderborn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Data Science Group</orgName>
								<orgName type="institution">Paderborn University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.89,116.06,345.58,12.68;1,143.13,133.99,329.10,12.68">DICE @ TREC-IS 2018: Combining Knowledge Graphs and Deep Learning to Identify Crisis-Relevant Tweets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D30FBDB6AECEA34FE5A450E18C5B481</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our submissions to the TREC Incident Stream (TREC-IS) challenge 2018. We investigated different machine learning approaches to classify crisis-related tweets into different information types. We incorporated knowledge graphs as features into this social media analysis, in addition to bag of words, word embeddings, time data, and event-types. Further, we evaluate stateof-the-art classification models on 31 generated features sets. Our TREC-IS results indicate that a model based on combining knowledge graphs (i.e., Babelfy), word embeddings and textual features outperformes classical machine learning models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today, social media is playing a significant role in disaster management and communication <ref type="bibr" coords="1,162.13,387.73,10.58,8.90" target="#b7">[8]</ref>. During emergencies, the availability of real-time information is critical for effective disaster relief and preparedness. Recent studies leverage shared information on social media in mitigating disasters impact and delivering faster responses <ref type="bibr" coords="1,439.03,411.64,10.79,8.90" target="#b2">[3,</ref><ref type="bibr" coords="1,452.97,411.64,7.19,8.90" target="#b4">5]</ref>. For an instance, Sakaki et al. <ref type="bibr" coords="1,238.31,423.59,11.62,8.90" target="#b6">[7]</ref> built an earthquake-detection system for Japan using the real-time situational tweets posted by the twitter users. Recently, deep learning models have been successfully applied in this domain. For example, Burel et al. <ref type="bibr" coords="1,425.73,447.50,11.62,8.90" target="#b0">[1]</ref> developed a semantic-deep model to classify information categories in crisis-related social media. Their results showed an increased accuracy by incorporating semantic features (e.g., entity representation) compared to statistical and non-semantic features.</p><p>However, existing tools to effectively monitor social media are not sufficient due to large volume of information and the need to categorize, cross-refer and verify them. The TREC-IS task aims to classify social media posts, i.e. Twitter tweets, into information types. These information types are modeled as multi-layer (hierarchical) ontologies (Top→ High→ Low) based on existing crisis management ontologies such as MOAC <ref type="foot" coords="1,474.12,541.55,3.49,6.23" target="#foot_0">1</ref> . For instance, a tweet could be categorized in top-level as report information type. Further, tweets at high-level might include information about emerging threats, significant event change or available services. This year, TREC-IS was centered around classifying tweets based on information types only at high-level. More details about TREC datasets including events, tweets and information types are discussed in section 4.2.</p><p>In this paper, we followed the intuition that combining Knowledge Graph data with novel statistical information retrieval and novel machine learning techniques can outperform more simplistic models <ref type="bibr" coords="1,265.03,638.79,10.58,8.90" target="#b8">[9]</ref>. To solve the TREC-IS task, we extracted different features from the tweets and their meta-data including textual features (bag of words), knowledge graph features (bag of concept) from external sources as Babelfy <ref type="bibr" coords="2,446.87,131.03,10.58,8.90" target="#b5">[6]</ref>, sentiments polarities, date-time and word-embedding features from pretrained models. Then, we combined all possible sets of features (31 features, in total) and investigated various classical machine learning models (e.g., Logistic Regression, Support Vector Machine, etc) and deep learning techniques.</p><p>To obtain the best training results, we built a feature pyramid (all vs all) to evaluate and compare the performance of all feature sets against all classifiers. Our experimental results showed that features trained by deep learning model achieved the highest class accuracy compared to the classical machine learning models which in turn achieved higher micro f-measure.</p><p>The rest of this paper is organized as follows: we first explore the analysis of TREC training dataset in section 2. Then we describe the details of our approach and official results in sections 3, 4 respectively. In section 5, we conclude the paper with some discussion about future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Exploratory Data Analysis</head><p>TREC-IS provided us with a training data comprising 1335 tweets spread over 23 information types. There were no tweets for two categories -GoodsServices and SearchAn-dRescue. Furthermore, the data was unevenly distributed among the 23 given classes with majority of tweets belonging to Continuing News, Sentiment, Irrelevant, Factoid, Multimedia Share and knownAlready information types. The dataset was largely unbalanced and hence, posed a challenge in efficient classification of the tweets. Figure <ref type="figure" coords="2,475.61,402.97,4.98,8.90" target="#fig_0">1</ref> shows the (partly skewed) distribution of tweets over 23 information types in the training data and 25 information types in the test data. In addition, we present some statistics about datasets in Table <ref type="table" coords="2,228.04,438.84,3.74,8.90" target="#tab_0">1</ref>. For this year's challenge, we regarded this task as a timeless classification task and aimed to distinguish patterns in the tweet to information-type mapping. For this purpose, we generated features based on tweet time, event-type, sentiment and underlying semantics of tweets. Although, the training data was insufficient to train sophisticated models using classical machine learning or deep learning techniques, we were interested in uncovering the capabilities of these classification approaches by enriching them  with 31 different features sets generated from the tweets and their meta-data. Our intention was to perform classification with minimal human effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>To account for the two missing classes, we manually augmented the dataset by adding 28 labelled tweets and their meta-data to the training set. We then preprocessed the dataset by normalizing the tweets to contain only the keywords. The normalization process involved lower-casing and lemmatization of text, decryption of emojis to natural language text. Moreover, we removed usernames, numbers, special symbols, urls, extra whitespaces, stopwords and punctuation from tweets. Also, we expanded the contracted words in tweets, for efficient removal of unimportant words (e.g., "how'd'y" to "how do you", "that'd" to "that would"). Table2 shows an example of tweet-preprocessing. We then extracted five features from tweets and their meta-data, and generated all combinations of them. The process of feature-generation has been described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Engineering</head><p>In the following, we describe the features used to train the different models:</p><p>-Bag of Words -We generated a bag-of-words feature from tweets by training scikit-learn's<ref type="foot" coords="4,203.17,169.28,3.49,6.23" target="#foot_1">2</ref> CountVectorizer on the keywords generated from the preprocessing of tweets in addition to the indicator terms that were given in the dataset. This trained model was then used to extract bag-of-words from tweets in the test dataset. -Bag of Concepts -For representing the underlying semantics of each tweet, we used a knowledge graph-based semantic framework, Babelfy<ref type="foot" coords="4,401.39,217.10,3.49,6.23" target="#foot_2">3</ref> which uses entity linking and word sense disambiguation to detect all potential meanings of the recognized text fragments. It selects the best candidate meaning for each fragment and returns its corresponding 'synset' by creating a dense sub-graph from the candidate meanings of the linkable tokens. For example, given an input text -"Shots fired, airport evacuated at LAX is what im seeing all over my timeline.", Babelfy returns the following as output: {'Shots': 'bn:00071206n','fired':'bn:00088225v','airport':'bn:00001676n', 'evacuated':'bn:00087757v','seeing': bn:00082813v','timeline': 'bn:00077308n'} For each tweet, we used one-hot encoding for the concepts (synsets) that were retrieved from Babelfy. The tweets were sent to Babelfy's API only after expanding word-contractions, removing '#' and 'RT' and substituting emojis with text using the python library Emojipedia<ref type="foot" coords="4,362.05,360.56,3.49,6.23" target="#foot_3">4</ref> . -Word Embeddings -Here, we used the pretrained Glove word-embeddings <ref type="foot" coords="4,459.25,372.52,3.49,6.23" target="#foot_4">5</ref> and computed a weighted average of word2vec vectors for each tweet. -Sentiment -By performing sentiment analysis on each tweet, a polarity score S ∈ [-1, +1] was computed using TextBlob text processing library <ref type="foot" coords="4,440.63,408.38,3.49,6.23" target="#foot_5">6</ref> . We did not performed any preprocessing on the tweet text to keep its context for sentiment analysis. -Event Type and Date Time -Since the challenge was aimed at categorizing the tweets based on information type of events, we used the event-type of the tweets as a feature and grouped it together with a date-time attribute from tweets meta-data to create this feature. We first sorted the dataset on event-type and then on time to get an event-wise and time-based representation of the information-types. Finally, we normalized it into range [0, 1].</p><p>Feature Pyramid -Finally, from the above-mentioned single features, we generated all possible combination of features (in total, 31 sets of features) and trained our models for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifiers</head><p>For the actual classification task, we employed both classical machine learning as well as deep-learning models. For classical machine learning models, we used the python library scikit-learn <ref type="foot" coords="5,209.41,258.94,3.49,6.23" target="#foot_6">7</ref> and performed hyper-parameter optimization using an automated machine learning library called tpot <ref type="foot" coords="5,276.98,270.90,3.49,6.23" target="#foot_7">8</ref> .</p><p>On the other hand, for the deep learning approach, we trained a deep neural network model with two dense layers. The first layer has 32 units with ReLU activation and the second layer computed the output probabilities using the standard Softmax function. To estimate the prediction error of the deep learning model, we used the loss function, categorical cross entropy <ref type="bibr" coords="5,237.07,332.27,11.62,8.90" target="#b1">[2]</ref> The learning parameters were then optimized using ADAM gradient optimizer <ref type="bibr" coords="5,210.58,344.23,10.58,8.90" target="#b3">[4]</ref>. In order to evaluate the performance of the classical and deep-learning approaches, we estimated our models based on two evaluation metrics: categorical accuracy and micro F1-measure. For classical models, we performed a Stratified K-fold cross validation, with K = 10. For the deep model, we performed a train-test split on the dataset, keeping 90% of the dataset for training and 10% to benchmark the model performance.</p><p>We, then investigated and compared the performance of models by trying different settings of the preprocessing module and training dataset (original and augmented version with 28 tweets related to the missing information types). The results showed that the models that were trained on the original dataset had a better classification accuracy than the ones trained on the augmented dataset. Hence, while training the models for final submission we used the original training dataset from TREC and re-ran our evaluation. Figure2 depicts a detailed comparison of all the classifiers against all the feature-sets based on the micro F1-measure.</p><p>Based on the classification accuracy, we shortlisted the top four performing models for final TREC submission and generated the submission files using the corresponding feature-sets, namely, {embedding},{embedding, bag-of-words, bag-of-concepts}, {bagof-words, bag-of-concepts} and {bag-of-words}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset</head><p>The TREC-IS dataset contains 1335 tweets for training and 19,784 for testing, which has been curated from different types of events. For each event, the tweets were collected using hashtags and keywords. Human annotators have labeled tweets into the multi-layer ontology of information types. In our experiments, we combined tweets from all the events provided for training, into one dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The official results are evaluated based on the micro-averaged performance metrics of (Accuracy, Precision, Recall, F1-score). Table <ref type="table" coords="6,335.84,524.69,4.98,8.90" target="#tab_4">4</ref> shows the results of our four submissions (runs) and the median scores. The run, UPB DICE2, generated from training the deep model with {embedding, bag-of-words, bag-of-concepts} feature-set performed better than our other submitted runs, with an F1-score of 0.369 which is somewhat close to the median (0.477) overall participants. In particular, it achieved a higher recall score (0.868) than the median (0.616).</p><p>Figure <ref type="figure" coords="6,178.38,596.42,9.40,8.90">3a</ref> shows the performance of our submissions against all information types. It depicts that we got a higher precision and recall for categories (ContinuingNews, Sentiment, MultimediaShare, Factoid) that were better represented in the training dataset than the ones with fewer examples (GoodServices, SearchAndRescue, CleanUp, Volunteer, etc). This clearly shows that our models learned the more prominent information types better and could not generalize well over others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we described our four submissions to the TREC-IS 2018 track. Our approach extracts textual features from tweets (e.g., bag of words) and incorporates con- ceptual features from knowledge graphs (i.e., Babelfy) to enrich social media analysis.</p><p>In addition, we also extracted word embedding features from tweets using pretrained Glove embedding model and also took into account the sentiment, time and event-type of the tweets. We then evaluated all the possible combinations of these features against classical machine learning and deep learning models in terms of categorical accuracy and micro-F1 measure. In the future, we plan to re-evaluate our approach with more training data from the TREC-IS challenge as well as on other relevant datasets. Also, we will investigate more deep models with different architectures (e.g. Recurrent Neural Model) and semi-supervised classification approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,145.06,429.17,325.24,8.90"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Percentages of Information Types per Tweets in Training and Test Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,138.22,302.47,169.46,8.01;8,138.22,313.43,48.31,8.01;8,307.68,302.01,169.46,8.01;8,307.68,312.97,104.50,8.01"><head></head><label></label><figDesc>(a) F1 Scores per Information Type for our run UPB DICE2. (b) Mean Squared Error on Predicted Priority Levels per Information Type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,198.01,481.11,219.34,83.32"><head>Table 1 :</head><label>1</label><figDesc>TREC-IS Dataset.</figDesc><table coords="2,198.01,496.59,219.34,67.84"><row><cell>#</cell><cell cols="2">Train Test</cell></row><row><cell>Total No. of events</cell><cell>6</cell><cell>15</cell></row><row><cell>Total No. of tweets</cell><cell cols="2">1335 19784</cell></row><row><cell>No. of Information Types</cell><cell>23</cell><cell>25</cell></row><row><cell>Average No. of tweets/event</cell><cell cols="2">267 1318</cell></row><row><cell>Average No. tweets/Information Type</cell><cell>53</cell><cell>1736</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,162.12,512.93,291.12,83.32"><head>Table 2 :</head><label>2</label><figDesc>An Example of Preprocessed Tweets.</figDesc><table coords="4,162.12,528.41,291.12,67.84"><row><cell>Input text</cell><cell>Preprocessed text</cell></row><row><cell>RT @indigojourney: You dont have 2</cell><cell>rt ask permission u prob-</cell></row><row><cell>ask permission, U see problem &amp;amp;</cell><cell>lem amp solution encourage</cell></row><row><cell>solution...encourage 'leaderhsip' roles</cell><cell>leaderhsip role floodrelief</cell></row><row><cell>#occupy #floodrelief #coflood ht?</cell><cell>coflood ht</cell></row><row><cell>Praying for West Texas</cell><cell>pray west texas red heart</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,155.15,387.40,316.47,203.87"><head>Table 3 :</head><label>3</label><figDesc>ML Classifiers with Their Hyper-parameters.</figDesc><table coords="5,155.15,402.88,316.47,188.39"><row><cell>Model name</cell><cell>Hyper-parameters</cell></row><row><cell>Logistic Regression</cell><cell>solver='newton-cg'</cell></row><row><cell>KNeighborsClassifier</cell><cell>n neighbors=5, weights='distance'</cell></row><row><cell>Linear SVM</cell><cell>kernel="linear", C=0.025</cell></row><row><cell>RBF SVM</cell><cell>gamma=2, C=1</cell></row><row><cell>Linear SVM (squared loss)</cell><cell>C=0.1,dual=True,loss='squared hinge',</cell></row><row><cell></cell><cell>penalty='l2',tol=0.0001</cell></row><row><cell>DecisionTreeClassifier</cell><cell>max depth=7,criterion='gini',</cell></row><row><cell></cell><cell>min samples leaf=2,min samples split=12</cell></row><row><cell>RandomForestClassifier</cell><cell>max depth=5, n estimators=10</cell></row><row><cell>MLPClassifier</cell><cell>alpha=1</cell></row><row><cell>GradientBoostingClassifier</cell><cell>learning rate=0.01,max depth=10,</cell></row><row><cell></cell><cell>max features=0.35,min samples leaf=10,</cell></row><row><cell></cell><cell>min samples split=6,n estimators=100,</cell></row><row><cell></cell><cell>subsample=0.75</cell></row><row><cell>Deep Model (DNN)</cell><cell>epochs=100,batch size=100,activation='relu',</cell></row><row><cell></cell><cell>optimizer='adam',loss='categorical crossentropy'</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,138.99,121.60,327.44,469.20"><head>Table 4 :</head><label>4</label><figDesc>Overall Performance (Micro Average) in All Submissions.</figDesc><table coords="7,138.99,121.60,327.44,469.20"><row><cell>boc</cell><cell>0.35</cell><cell>0.28</cell><cell>0.23</cell><cell>0.52</cell><cell>0.5</cell><cell>0.31</cell><cell>0.24</cell><cell>0.41</cell><cell>0.38</cell><cell>0.43</cell><cell>0.37</cell></row><row><cell>boc_date</cell><cell>0.34</cell><cell>0.28</cell><cell>0.23</cell><cell>0.53</cell><cell>0.5</cell><cell>0.3</cell><cell>0.24</cell><cell>0.42</cell><cell>0.38</cell><cell>0.45</cell><cell>0.47</cell></row><row><cell>boc_sent</cell><cell>0.34</cell><cell>0.28</cell><cell>0.23</cell><cell>0.52</cell><cell>0.51</cell><cell>0.3</cell><cell>0.24</cell><cell>0.41</cell><cell>0.38</cell><cell>0.45</cell><cell>0.47</cell></row><row><cell>boc_sent_time</cell><cell>0.35</cell><cell>0.29</cell><cell>0.23</cell><cell>0.53</cell><cell>0.5</cell><cell>0.3</cell><cell>0.23</cell><cell>0.42</cell><cell>0.38</cell><cell>0.46</cell><cell>0.47</cell></row><row><cell>bow</cell><cell>0.42</cell><cell>0.45</cell><cell>0.21</cell><cell>0.61</cell><cell>0.6</cell><cell>0.36</cell><cell>0.31</cell><cell>0.5</cell><cell>0.49</cell><cell>0.51</cell><cell>0.5</cell></row><row><cell>bow_boc</cell><cell>0.36</cell><cell>0.49</cell><cell>0.21</cell><cell>0.61</cell><cell>0.61</cell><cell>0.37</cell><cell>0.27</cell><cell>0.51</cell><cell>0.49</cell><cell>0.52</cell><cell>0.43</cell></row><row><cell>bow_boc_embedding</cell><cell>0.49</cell><cell>0.56</cell><cell>0.21</cell><cell>0.56</cell><cell>0.57</cell><cell>0.34</cell><cell>0.34</cell><cell>0.56</cell><cell>0.49</cell><cell>0.57</cell><cell>0.47</cell></row><row><cell>bow_boc_embedding_time</cell><cell>0.49</cell><cell>0.56</cell><cell>0.2</cell><cell>0.55</cell><cell>0.56</cell><cell>0.33</cell><cell>0.35</cell><cell>0.54</cell><cell>0.49</cell><cell>0.59</cell><cell>0.55</cell></row><row><cell>bow_boc_time</cell><cell>0.36</cell><cell>0.5</cell><cell>0.21</cell><cell>0.62</cell><cell>0.61</cell><cell>0.36</cell><cell>0.3</cell><cell>0.51</cell><cell>0.49</cell><cell>0.53</cell><cell>0.47</cell></row><row><cell>bow_date</cell><cell>0.41</cell><cell>0.45</cell><cell>0.21</cell><cell>0.61</cell><cell>0.6</cell><cell>0.36</cell><cell>0.3</cell><cell>0.5</cell><cell>0.49</cell><cell>0.52</cell><cell>0.4</cell></row><row><cell>bow_sent</cell><cell>0.42</cell><cell>0.45</cell><cell>0.21</cell><cell>0.61</cell><cell>0.6</cell><cell>0.36</cell><cell>0.3</cell><cell>0.49</cell><cell>0.49</cell><cell>0.51</cell><cell>0.46</cell></row><row><cell>bow_sent_boc</cell><cell>0.37</cell><cell>0.5</cell><cell>0.21</cell><cell>0.62</cell><cell>0.61</cell><cell>0.36</cell><cell>0.27</cell><cell>0.51</cell><cell>0.49</cell><cell>0.52</cell><cell>0.42</cell></row><row><cell>bow_sent_boc_time</cell><cell>0.36</cell><cell>0.49</cell><cell>0.2</cell><cell>0.6</cell><cell>0.6</cell><cell>0.37</cell><cell>0.29</cell><cell>0.51</cell><cell>0.49</cell><cell>0.53</cell><cell>0.55</cell></row><row><cell>bow_sent_time</cell><cell>0.41</cell><cell>0.45</cell><cell>0.21</cell><cell>0.62</cell><cell>0.6</cell><cell>0.36</cell><cell>0.31</cell><cell>0.49</cell><cell>0.49</cell><cell>0.52</cell><cell>0.44</cell></row><row><cell>date_sent</cell><cell>0.16</cell><cell>0.19</cell><cell>0.2</cell><cell>0.19</cell><cell>0.19</cell><cell>0.21</cell><cell>0.22</cell><cell>0.19</cell><cell>0.13</cell><cell>0.22</cell><cell>0</cell></row><row><cell>datetime</cell><cell>0.13</cell><cell>0.19</cell><cell>0.19</cell><cell>0.19</cell><cell>0.19</cell><cell>0.18</cell><cell>0.19</cell><cell>0.19</cell><cell>0.2</cell><cell>0.16</cell><cell>0</cell></row><row><cell>embedding</cell><cell>0.5</cell><cell>0.55</cell><cell>0.22</cell><cell>0.48</cell><cell>0.5</cell><cell>0.31</cell><cell>0.38</cell><cell>0.54</cell><cell>0.46</cell><cell>0.52</cell><cell>0.5</cell></row><row><cell>embedding_boc</cell><cell>0.5</cell><cell>0.55</cell><cell>0.21</cell><cell>0.51</cell><cell>0.53</cell><cell>0.34</cell><cell>0.33</cell><cell>0.53</cell><cell>0.38</cell><cell>0.55</cell><cell>0.5</cell></row><row><cell>embedding_boc_time</cell><cell>0.51</cell><cell>0.55</cell><cell>0.21</cell><cell>0.52</cell><cell>0.53</cell><cell>0.33</cell><cell>0.35</cell><cell>0.53</cell><cell>0.38</cell><cell>0.55</cell><cell>0.48</cell></row><row><cell>embedding_bow</cell><cell>0.49</cell><cell>0.56</cell><cell>0.21</cell><cell>0.56</cell><cell>0.56</cell><cell>0.33</cell><cell>0.37</cell><cell>0.55</cell><cell>0.49</cell><cell>0.57</cell><cell>0.49</cell></row><row><cell>embedding_bow_time</cell><cell>0.49</cell><cell>0.56</cell><cell>0.21</cell><cell>0.55</cell><cell>0.57</cell><cell>0.33</cell><cell>0.37</cell><cell>0.56</cell><cell>0.49</cell><cell>0.59</cell><cell>0.41</cell></row><row><cell>embedding_date</cell><cell>0.5</cell><cell>0.55</cell><cell>0.21</cell><cell>0.49</cell><cell>0.5</cell><cell>0.3</cell><cell>0.39</cell><cell>0.52</cell><cell>0.46</cell><cell>0.52</cell><cell>0.51</cell></row><row><cell>embedding_sent</cell><cell>0.5</cell><cell>0.55</cell><cell>0.22</cell><cell>0.48</cell><cell>0.51</cell><cell>0.31</cell><cell>0.39</cell><cell>0.52</cell><cell>0.46</cell><cell>0.52</cell><cell>0.46</cell></row><row><cell>embedding_sent_boc</cell><cell>0.5</cell><cell>0.55</cell><cell>0.21</cell><cell>0.51</cell><cell>0.53</cell><cell>0.34</cell><cell>0.35</cell><cell>0.54</cell><cell>0.38</cell><cell>0.54</cell><cell>0.47</cell></row><row><cell>embedding_sent_boc_time</cell><cell>0.49</cell><cell>0.55</cell><cell>0.21</cell><cell>0.5</cell><cell>0.51</cell><cell>0.35</cell><cell>0.33</cell><cell>0.53</cell><cell>0.38</cell><cell>0.54</cell><cell>0.5</cell></row><row><cell>embedding_sent_bow</cell><cell>0.49</cell><cell>0.56</cell><cell>0.21</cell><cell>0.55</cell><cell>0.56</cell><cell>0.33</cell><cell>0.39</cell><cell>0.56</cell><cell>0.49</cell><cell>0.57</cell><cell>0.46</cell></row><row><cell>embedding_sent_bow_boc</cell><cell>0.49</cell><cell>0.56</cell><cell>0.21</cell><cell>0.56</cell><cell>0.58</cell><cell>0.33</cell><cell>0.37</cell><cell>0.56</cell><cell>0.49</cell><cell>0.58</cell><cell>0.45</cell></row><row><cell>embedding_sent_bow_boc_time</cell><cell>0.49</cell><cell>0.57</cell><cell>0.21</cell><cell>0.55</cell><cell>0.55</cell><cell>0.33</cell><cell>0.35</cell><cell>0.55</cell><cell>0.5</cell><cell>0.58</cell><cell>0.52</cell></row><row><cell>embedding_sent_bow_time</cell><cell>0.49</cell><cell>0.56</cell><cell>0.21</cell><cell>0.54</cell><cell>0.55</cell><cell>0.32</cell><cell>0.36</cell><cell>0.56</cell><cell>0.49</cell><cell>0.58</cell><cell>0.47</cell></row><row><cell>embedding_sent_time</cell><cell>0.5</cell><cell>0.55</cell><cell>0.21</cell><cell>0.49</cell><cell>0.51</cell><cell>0.31</cell><cell>0.37</cell><cell>0.52</cell><cell>0.46</cell><cell>0.53</cell><cell>0.54</cell></row><row><cell>sentiment</cell><cell>0.18</cell><cell>0.19</cell><cell>0.2</cell><cell>0.19</cell><cell>0.19</cell><cell>0.21</cell><cell>0.21</cell><cell>0.19</cell><cell>0.13</cell><cell>0.21</cell><cell>0</cell></row><row><cell></cell><cell>Nearest Neighbors</cell><cell>Linear SVM</cell><cell>RBF SVM</cell><cell>Linear SVM (squared loss)</cell><cell>Logistic Regression</cell><cell>Decision Tree</cell><cell>Random Forest</cell><cell>Neural Net</cell><cell>Naive Bayes</cell><cell>Gradient Boost</cell><cell>Deep Model</cell></row><row><cell></cell><cell cols="9">Fig. 2: F1 Scores of classifiers for All Feature-sets.</cell><cell></cell><cell></cell></row><row><cell>RUN ID</cell><cell></cell><cell></cell><cell></cell><cell cols="8">Precision Recall F1 Accuracy Priority Est. Err.</cell></row><row><cell>UPB DICE1</cell><cell></cell><cell></cell><cell></cell><cell>0.271</cell><cell cols="4">0.569 0.367 0.228</cell><cell></cell><cell>0.091</cell><cell></cell></row><row><cell cols="2">UPB DICE2 (Best)</cell><cell></cell><cell></cell><cell>0.234</cell><cell cols="4">0.868 0.369 0.229</cell><cell></cell><cell>0.092</cell><cell></cell></row><row><cell>UPB DICE3</cell><cell></cell><cell></cell><cell></cell><cell>0.194</cell><cell cols="4">0.415 0.265 0.180</cell><cell></cell><cell>0.094</cell><cell></cell></row><row><cell>UPB DICE4</cell><cell></cell><cell></cell><cell></cell><cell>0.215</cell><cell cols="4">0.62 0.319 0.203</cell><cell></cell><cell>0.087</cell><cell></cell></row><row><cell cols="2">Median Scores</cell><cell></cell><cell></cell><cell>0.397</cell><cell cols="4">0.616 0.477 0.338</cell><cell></cell><cell>0.093</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,152.70,656.87,131.38,8.01"><p>http://observedchange.com/moac/ns/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,152.70,613.03,78.16,8.01"><p>http://scikit-learn.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,152.70,623.99,64.13,8.01"><p>http://babelfy.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,152.70,634.95,172.98,8.01"><p>https://github.com/bcongdon/python-emojipedia</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,152.70,645.91,138.49,8.01"><p>https://nlp.stanford.edu/projects/glove/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,152.70,656.87,136.54,8.01"><p>https://textblob.readthedocs.io/en/dev/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,152.70,645.91,78.16,8.01"><p>http://scikit-learn.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,152.70,656.87,115.19,8.01"><p>http://epistasislab.github.io/tpot/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been supported by the <rs type="funder">BMVI</rs> projects <rs type="projectName">LIMBO</rs> (project no. <rs type="grantNumber">19F2029C</rs>), and also by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> within '<rs type="projectName">KMU-innovativ: Forschung für die zivile Sicherheit' in particular 'Forschung für die zivile Sicherheit</rs>' and the project <rs type="projectName">SOLIDE</rs> (no. <rs type="grantNumber">13N14456</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_PR4YQ85">
					<idno type="grant-number">19F2029C</idno>
					<orgName type="project" subtype="full">LIMBO</orgName>
				</org>
				<org type="funded-project" xml:id="_UPwJy9q">
					<orgName type="project" subtype="full">KMU-innovativ: Forschung für die zivile Sicherheit&apos; in particular &apos;Forschung für die zivile Sicherheit</orgName>
				</org>
				<org type="funded-project" xml:id="_4jFGbZG">
					<idno type="grant-number">13N14456</idno>
					<orgName type="project" subtype="full">SOLIDE</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,151.36,170.88,329.22,8.90;9,151.36,182.83,329.23,8.90;9,151.36,194.79,196.15,8.90" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,337.07,170.88,143.52,8.90;9,151.36,182.83,217.45,8.90">Semantic wide and deep learning for detecting crisis-information categories on social media</title>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Burel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,388.74,182.90,91.85,8.71;9,151.36,194.86,62.96,8.71">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="138" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,206.75,329.22,8.90;9,151.36,218.70,329.23,8.90;9,151.36,230.66,22.42,8.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,473.39,206.75,7.19,8.90;9,151.36,218.70,141.66,8.90">A tutorial on the cross-entropy method</title>
		<author>
			<persName coords=""><forename type="first">Pieter-Tjerk De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,300.64,218.77,116.96,8.71">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,242.61,329.22,8.90;9,151.36,254.57,329.22,8.90;9,151.36,266.52,329.22,8.90;9,151.36,278.48,329.22,8.90;9,151.36,290.43,114.19,8.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,336.02,266.52,144.57,8.90;9,151.36,278.48,325.26,8.90">Social media and disasters: a functional framework for social media use in disaster planning, response, and research</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mildred</forename><forename type="middle">F</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eun</forename><forename type="middle">Hae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marlo</forename><surname>Goldstein Hode</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Michael R Halliwell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rachel</forename><surname>Turner Mcgowen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shivani</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Vaid</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mcelderry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.36,290.50,36.03,8.71">Disasters</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,302.39,329.22,8.90;9,151.36,314.34,154.60,8.90" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="9,298.20,302.39,178.32,8.90">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,151.36,326.30,329.23,8.90;9,151.36,338.25,311.54,8.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,326.17,326.30,117.12,8.90">Social media in disaster relief</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Landwehr</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,460.67,326.37,19.93,8.71;9,151.36,338.32,178.67,8.71">Data mining and knowledge discovery for big data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="225" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,350.21,329.22,8.90;9,151.36,362.16,291.32,8.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,397.57,350.21,83.02,8.90;9,151.36,362.16,185.38,8.90">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,344.67,362.23,20.40,8.71">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,374.12,329.22,8.90;9,151.36,386.07,329.23,8.90;9,151.36,398.03,296.35,8.90" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,382.57,374.12,98.01,8.90;9,151.36,386.07,211.08,8.90">Earthquake shakes twitter users: real-time event detection by social sensors</title>
		<author>
			<persName coords=""><forename type="first">Takeshi</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Makoto</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,383.91,386.14,96.69,8.71;9,151.36,398.10,174.50,8.71">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,409.98,329.22,8.90;9,151.36,421.94,329.23,8.90;9,151.36,433.89,193.50,8.90" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,359.53,409.98,121.06,8.90;9,151.36,421.94,226.17,8.90">Socializing in emergencies-a review of the use of social media in emergency situations</title>
		<author>
			<persName coords=""><forename type="first">Tomer</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avishay</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruria</forename><surname>Adini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,384.67,422.01,95.92,8.71;9,151.36,433.96,99.46,8.71">International Journal of Information Management</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="619" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,445.85,329.22,8.90;9,151.36,457.80,329.23,8.90;9,151.36,469.83,329.22,8.71;9,151.36,481.71,166.68,8.90" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,221.69,445.85,258.90,8.90;9,151.36,457.80,125.58,8.90">Combining linked data and statistical information retrieval -next generation information systems</title>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,299.18,457.87,181.42,8.71;9,151.36,469.83,118.90,8.71;9,176.27,481.78,47.53,8.71">The Semantic Web: Trends and Challenges -11th International Conference</title>
		<meeting><address><addrLine>Anissaras, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-25">2014. May 25-29, 2014. 2014</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
