<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.90,77.31,375.03,20.09">CBNU at TREC 2018 Incident Streams Track</title>
				<funder ref="#_sR2yPQU">
					<orgName type="full">Ministry of Education</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea</orgName>
				</funder>
				<funder ref="#_DJB7RJK">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_s7t2mBV">
					<orgName type="full">ITRC</orgName>
				</funder>
				<funder>
					<orgName type="full">MSIT(Ministry of Science and ICT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.74,113.34,69.04,12.75"><forename type="first">Won-Gyu</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Computer Science and Engineering</orgName>
								<orgName type="institution">CAIIT Chonbuk National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.69,113.34,74.89,12.75"><forename type="first">Seung-Hyeon</forename><surname>Jo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Computer Science and Engineering</orgName>
								<orgName type="institution">CAIIT Chonbuk National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.76,113.34,78.89,12.75"><forename type="first">Kyung-Soon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Computer Science and Engineering</orgName>
								<orgName type="institution">CAIIT Chonbuk National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.90,77.31,375.03,20.09">CBNU at TREC 2018 Incident Streams Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2F17C6587C6DDA1CD4F93CBD11DB3D9C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conceptual representation</term>
					<term>Incident Streams</term>
					<term>Convolutional neural networks</term>
					<term>Class activation mapping</term>
					<term>One-shot learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the CBNU team at the TREC Incident Streams Track 2018. For tweet representation, crisis-related terms are represented as conceptual entities. For tweet classification, we have compared support vector machines and our deep learning model which combines class activation mapping with one-shot learning in convolutional neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>TREC Incident Streams Track (TREC-IS) 2018 is a task for participant systems to categorize the tweets in each event/incident's stream by information type in order to automatically process social media streams during emergency situations <ref type="bibr" coords="1,158.82,360.67,10.51,11.09" target="#b0">[1]</ref>. In our participation to TREC-IS Track 2018, we focus on conceptual representation for crisis-related terms and combining deep learning models for information type classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Conceptual Representation for Crisis-Related Tweets</head><p>In order to classify a stream of tweets related to the incident, the terms in each tweet are represented as conceptual entities such as event entities, category indicator entities, information type entities, URL entities, and user entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Event Entities</head><p>In TREC-IS training data, hashtags and keywords used for collecting a stream of tweets related to each incident have been provided. For conceptual representation, the hashtags and keywords are represented as event entities: &lt;EventLoc&gt;, &lt;EventCri&gt;, and &lt;EventClu&gt; according to the information in such as a location, an incident and a clue. The followings are examples of event entity representation for each hashtag or keyword.</p><p> #COfire: &lt;EventLoc&gt;, &lt;EventCri&gt;  LAX shooting: &lt;EventLoc&gt;, &lt;EventCri&gt;  #Philippines: &lt;EventLoc&gt;  sismo: &lt;EventCri&gt;  #StrongerPH: &lt;EventClu&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Category Indicator Entities</head><p>In a training data, category indicator terms are provided for each tweet by human assessors. We have used the indicator terms for each category indicator entity. The number of categories in TREC-IS and category indicator entities are 25 such as &lt;InformationWanted&gt;, &lt;ServiceAvailable&gt;, &lt;Official&gt; and etc.</p><p>The followings are examples of category indicator entity representation for each term.</p><p> &lt;InformationWanted&gt;: does anyone know, report?, what is going on, …  &lt;ServiceAvailable&gt;: heavy air tanker, animal hospital, offering housing, …  &lt;Official&gt;: tsunami warning issued, briefing, emergency declaration, …</p><p>The terms for 5 category indicator entities such as volunteering, donation, multimedia, and sentiment are expanded using terms in thesaurus.com and relatedwords.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Information Type Entities</head><p>In order to represent information type for each term, information type entities are defined as followings: &lt;Question&gt;, &lt;Communication&gt;, &lt;Organization&gt;, &lt;Evacuation&gt;, &lt;Location&gt;, &lt;Emergency&gt;, &lt;Person1st&gt;, &lt;Informal&gt;, &lt;Emotion/reaction&gt;, &lt;Narration&gt;, &lt;News&gt;, &lt;Disaster/threat&gt;, &lt;Change&gt;, &lt;Service&gt;, &lt;Provider&gt;, &lt;Fact&gt;, &lt;Unit&gt;, &lt;Government&gt;, &lt;Advice&gt;, &lt;Pastdate&gt;, &lt;Pastverb&gt;</p><p>The followings are term lists for each information type entity. The seed terms we have defined are expanded using terms in thesaurus.com and relatedwords.org. We have assumed that each information type entity would give additional information for classification. For example, the entities such as &lt;Government&gt; and &lt;Organization&gt; would be helpful for the category 'Official', an entity &lt;News&gt; for the category 'ContinuingNews', an entity &lt;Disaster/threat&gt; for the category 'EmergingThreats', and an entity &lt;Evacuation&gt; for the category 'MovePeople'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">URL Entities</head><p>A URL itself has information as represented by the URL. We have defined URL entities such as &lt;URLVideo&gt;, &lt;URLPhoto&gt;, &lt;URLNews&gt;, &lt;URLWeather&gt;, &lt;URLOrganization&gt;, &lt;URLDonation&gt;, &lt;URLDisaster&gt; and &lt;URLBlogMagazine&gt;.</p><p>We have assumed that each URL entity would give information for a category. For example, the entity &lt;URLDonation&gt; would be helpful for the category 'Donations', an entity &lt;URLWeather&gt; for the category 'Weather', an entity &lt;URLOrganization&gt; for the category 'Official', an entity &lt;URLNews&gt; for the categories 'ContinuingNews' or 'PastNews', an entity &lt;URLDisasterInfo&gt; for the category 'EmergingThreats', and the entities &lt;URLVideo&gt; and &lt;URLPhoto&gt; for the category 'MultimediaShare'.</p><p>In order to extract useful URLs, the TREC-IS training data and CrisisT26 dataset are used. For pre-processing, a short URL is converted to an original URL. The URLs with more than frequency 10 have been defined as 8 entities. The followings are term lists for each URL entity.</p><p> &lt;URLPhoto&gt;: instagram.com, twitpic.com, facebook.com/photo.php, …  &lt;URLOrganization&gt;: usgs.gov, rfs.nsw.gov.au, redcross.org, …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">User Entities</head><p>Based on a user's characteristics, we have defined user entities such as &lt;UserNews&gt;, &lt;UserWeather&gt;, &lt;UserOrganization&gt;, &lt;UserDonation&gt;, &lt;UserDisasterInfo&gt;, and &lt;UserMultimedia&gt;.</p><p>In order to extract useful user information in tweets, the TREC-IS training data and CrisisT26 dataset are used. The users with more than frequency 10 have been defined as 6 user entities as the characteristics. We have assumed that a user entity &lt;UserDonation&gt; would be helpful for the category 'Donations', a user entity &lt;UserWeather&gt; for the category 'Weather', an entity &lt;UserNews&gt; for the categories 'Factoid', 'ContinuingNews' or 'PastNews', an entity &lt;UserOrganization&gt; for the category 'Official', and an entity &lt;UserMultimedia&gt; for the category 'MultimediaShare'.</p><p>The followings are tweet user lists for each user entity.</p><p> &lt;UserWeather&gt;: @dost_pagasa, @breakingstorm  &lt;UserDisasterInfo&gt;: @NewEarthquake, @INGVterremoti User Entity &lt;UserNews&gt; @BreakingNews,@ChannelNewsAsia,@CBSNews, … 6 &lt;UserOrganization&gt; @LarimerSheriff, @HumaneSociety, @RedCross, …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Learning Models for Crisis-Related Tweet Classification</head><p>In this experiment, we have used combining deep learning models which combine class activation mapping with one-shot learning in convolutional neural networks. The class activation mapping is generated using global average pooling in Convolutional Neural Networks(CNNs). Then, one-shot learning is applied in CNNs. The expected effect using combining deep learning models is that crisis-related terms and conceptual entities would be emphasized for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generation of Class Activation Mapping for Text Classification</head><p>In image localization, class activation mapping highlights the class-specific discriminative regions <ref type="bibr" coords="3,483.77,383.23,10.50,11.09" target="#b1">[2]</ref>. Class activation mapping can identify the importance of the image regions most relevant to the particular category. We expect that the class activation map for text classification represent the importance of terms.</p><p>In this paper, class activation mapping is applied for text classification. The class activation mapping is generated by multiplying the weight from the convolution layer and the weight used for classification using the global average pooling in CNNs. As shown in Fig. <ref type="figure" coords="3,275.75,449.23,3.74,11.09" target="#fig_1">1</ref>. This allows us to check the importance of terms in the input data. Through learning, global average pooling creates class activation mapping and represents terms. The proposed method readjusts the terms weight in order of importance and re-inputs them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">One-shot Learning in CNNs</head><p>There has been research using One-shot learning in LSTM (Long-Short Term Memory). It improves classification accuracy using One-shot learning. And it fixes the problem that the number of data is too small to learn <ref type="bibr" coords="3,485.78,719.23,10.67,11.09" target="#b2">[3,</ref><ref type="bibr" coords="3,496.45,719.23,7.11,11.09" target="#b3">4]</ref>. We apply this method in CNNs and expect effectiveness.</p><p>CNNs consists of a convolutional layer, a maximum pooling layer, and a fully connected layer <ref type="bibr" coords="4,470.45,85.75,10.53,11.09" target="#b4">[5]</ref>. One-shot learning uses CNN's document vectors that come from fully connected layer and category vectors that represent each category. One-shot learning finds a category vector corresponding to the matched category, learns the index of similarity of the document vector and sets the value to 1. In the other case of finding, the vector corresponding to the unmatched category, it sets the value of index of similarity to zero. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Class Activation Mapping with One-shot Learning in CNNs</head><p>The class activation mapping is used to represent terms with high importance of classification. We use a weighted readjustment of terms in order of importance and input them to CNNs model applying One-shot learning. The terms are continuously changed through the learning of the weights of the terms having high importance, and the readjusting of the terms is also performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SUBMITTED RUNS</head><p>We have compared conceptual representations for crisis-related tweets as follows:</p><p> Conceptual Representation 1: Tweets are represented by terms, event entities, category indicator entities, and information type entities.</p><p> Conceptual Representation 2: Tweets are represented by terms and all conceptual entities we have defined such as event entities, category indicator entities, information type entities, URL entities, and user entities.</p><p>We have compared classification methods for our conceptual representation as follows:</p><p> Support vector machines (SVM): a set of supervised learning methods used for classification <ref type="bibr" coords="4,466.61,521.71,10.50,11.09" target="#b5">[6]</ref>.</p><p> Combining deep learning models: class activation mapping with one-shot learning in CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Run Description</head><p>The submitted runs are described as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>The experimental results of information type categorization for multi-type and any-type are shown in Table <ref type="table" coords="4,518.37,719.71,5.01,11.09" target="#tab_1">2</ref> and Table <ref type="table" coords="4,114.91,731.71,3.77,11.09" target="#tab_2">3</ref>, respectively. Table <ref type="table" coords="4,204.57,731.71,5.01,11.09" target="#tab_3">4</ref> shows experimental results per each information type performance for multitype. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented the conceptual representation for crisis-related tweet classification. Experimental results show that the conceptual representation (run cbnuS2) is effective for classification with SVMs and our combining deep learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,86.40,235.68,174.46,11.16;2,86.22,247.68,254.03,11.16;2,86.22,259.68,206.35,11.16"><head></head><label></label><figDesc>&lt;Communication&gt;: Tweet, DM, email, …  &lt;Disaster-threat&gt;: Power is out, No power, People trapped, …  &lt;Multimedia&gt;: photo, instagram, visualization, …</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,186.96,635.01,221.51,8.83;3,181.14,507.06,233.76,115.50"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Generation of class activation mapping for text classification</figDesc><graphic coords="3,181.14,507.06,233.76,115.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,240.54,271.53,114.35,8.83;4,193.86,158.46,220.86,112.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. One-shot learning in CNNs</figDesc><graphic coords="4,193.86,158.46,220.86,112.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,86.40,611.64,199.28,11.16;4,86.40,629.64,199.28,11.16;4,86.40,647.64,306.45,11.16;4,86.40,665.64,306.45,11.16"><head> cbnuS1: SVMs for Conceptual Representation 1  2  1 </head><label>121</label><figDesc>cbnuS2: SVMs for Conceptual Representation cbnuC1: Combining deep learning models for Conceptual Representation cbnuC2: Combining deep learning models for Conceptual Representation 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,74.10,85.39,446.47,145.21"><head>Table 1 .</head><label>1</label><figDesc>Examples of Conceptual Representation</figDesc><table coords="3,74.10,104.60,446.47,126.00"><row><cell>Conceptual Entities</cell><cell></cell><cell>Examples</cell><cell># of Entities</cell></row><row><cell>Event Entity</cell><cell>&lt;EventLoc&gt; &lt;EventCri&gt;</cell><cell>#colorado, #costarica, #LAX, … #Wildfire, airport shooting, #Typhoon Bopha, …</cell><cell>3</cell></row><row><cell>Category Indicator Entity</cell><cell>&lt;Movepeople&gt; &lt;Weather&gt;</cell><cell>evacuation, leave town, pre-evacuation notice, … #weather, forecast, satellite image, …</cell><cell>25</cell></row><row><cell>Information Type Entity</cell><cell>&lt;Unit&gt; &lt;Evacuation&gt;</cell><cell>magnitude, dollars, acres leave NOW, evacuation orders, closed the highway, …</cell><cell>21</cell></row><row><cell>URL Entity</cell><cell>&lt;URLNews&gt; &lt;URLDonation&gt;</cell><cell>yahoo.com/news, reuters.com, cbsnews.com, … worldvision.org.ph, prizeo.com, secure.redcross.ca</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,138.42,85.39,318.55,101.71"><head>Table 2 .</head><label>2</label><figDesc>Experimental results for Information Type Categorization (Multi-type)</figDesc><table coords="5,181.50,104.60,235.26,82.50"><row><cell>Run ID</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Accuracy</cell></row><row><cell>cbnuS1</cell><cell>0.219</cell><cell>0.116</cell><cell>0.125</cell><cell>0.905</cell></row><row><cell>cbnuS2</cell><cell>0.267</cell><cell>0.112</cell><cell>0.126</cell><cell>0.906</cell></row><row><cell>cbnuC1</cell><cell>0.239</cell><cell>0.102</cell><cell>0.115</cell><cell>0.901</cell></row><row><cell>cbnuC2</cell><cell>0.231</cell><cell>0.102</cell><cell>0.116</cell><cell>0.903</cell></row><row><cell>Median</cell><cell>0.183</cell><cell>0.078</cell><cell>0.082</cell><cell>0.899</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,140.94,208.33,313.53,104.71"><head>Table 3 .</head><label>3</label><figDesc>Experimental results for Information Type Categorization (Any-type)</figDesc><table coords="5,181.50,227.60,235.26,85.44"><row><cell>Run ID</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Accuracy</cell></row><row><cell>cbnuS1</cell><cell>0.447</cell><cell>0.740</cell><cell>0.558</cell><cell>0.406</cell></row><row><cell>cbnuS2</cell><cell>0.456</cell><cell>0.778</cell><cell>0.575</cell><cell>0.421</cell></row><row><cell>cbnuC1</cell><cell>0.506</cell><cell>0.480</cell><cell>0.492</cell><cell>0.354</cell></row><row><cell>cbnuC2</cell><cell>0.532</cell><cell>0.533</cell><cell>0.533</cell><cell>0.389</cell></row><row><cell>Median</cell><cell>0.398</cell><cell>0.616</cell><cell>0.477</cell><cell>0.338</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,63.30,343.33,468.76,423.15"><head>Table 4 .</head><label>4</label><figDesc>Information Type Categorization (Multi-type) per Information Type Performance</figDesc><table coords="5,63.30,363.51,468.76,402.97"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Run ID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>cbnuS1</cell><cell></cell><cell></cell><cell>cbnuS2</cell><cell></cell><cell></cell><cell>cbnuC1</cell><cell></cell><cell></cell><cell>cbnuC2</cell><cell></cell></row><row><cell></cell><cell>Category</cell><cell>Support</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell></row><row><cell></cell><cell>GoodsServices</cell><cell>126</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Request</cell><cell>SearchAndRescue</cell><cell>286</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>InformationWanted</cell><cell>172</cell><cell cols="9">0.05 0.01 0.01 0.17 0.01 0.02 0.25 0.01 0.01</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>Volunteer</cell><cell>116</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>CallToAction</cell><cell>Donations</cell><cell>804</cell><cell cols="12">0.38 0.58 0.46 0.48 0.47 0.47 0.64 0.43 0.52 0.62 0.46 0.53</cell></row><row><cell></cell><cell>MovePeople</cell><cell>27</cell><cell cols="12">0.06 0.26 0.10 0.05 0.19 0.07 0.23 0.11 0.15 0.05 0.04 0.04</cell></row><row><cell></cell><cell>FirstPartyObservation</cell><cell>3807</cell><cell cols="7">0.31 0.01 0.02 0.37 0.01 0.01 0.12</cell><cell>0</cell><cell>0</cell><cell>0.47</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>ThirdPartyObservation</cell><cell>4160</cell><cell>0.25</cell><cell>0</cell><cell>0</cell><cell>0.30</cell><cell>0</cell><cell>0</cell><cell>0.5</cell><cell>0</cell><cell>0</cell><cell>0.25</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>Weather</cell><cell>1325</cell><cell cols="6">0.51 0.17 0.25 0.53 0.17 0.25</cell><cell>0.6</cell><cell cols="5">0.11 0.18 0.62 0.15 0.24</cell></row><row><cell></cell><cell>EmergingThreats</cell><cell>686</cell><cell cols="7">0.06 0.01 0.02 0.07 0.02 0.03 0.05</cell><cell>0</cell><cell>0</cell><cell cols="3">0.07 0.01 0.01</cell></row><row><cell></cell><cell>SignificantEventChange</cell><cell>415</cell><cell cols="7">0.03 0.01 0.02 0.02 0.01 0.01 0.02</cell><cell>0</cell><cell>0</cell><cell>0.03</cell><cell>0</cell><cell>0.01</cell></row><row><cell>Report</cell><cell>MultimediaShare</cell><cell>3974</cell><cell cols="6">0.41 0.25 0.32 0.40 0.46 0.43</cell><cell>0.4</cell><cell cols="5">0.22 0.28 0.48 0.26 0.34</cell></row><row><cell></cell><cell>ServiceAvailable</cell><cell>1076</cell><cell cols="12">0.39 0.09 0.15 0.37 0.09 0.15 0.61 0.03 0.07 0.57 0.03 0.06</cell></row><row><cell></cell><cell>Factoid</cell><cell>2383</cell><cell cols="9">0.40 0.18 0.25 0.38 0.18 0.25 0.38 0.14 0.21</cell><cell>0.4</cell><cell cols="2">0.14 0.21</cell></row><row><cell></cell><cell>Official</cell><cell>403</cell><cell cols="6">0.14 0.04 0.07 0.18 0.05 0.08</cell><cell>0.2</cell><cell cols="5">0.09 0.13 0.18 0.07 0.10</cell></row><row><cell></cell><cell>CleanUp</cell><cell>62</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>Hashtags</cell><cell>3363</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>PastNews</cell><cell>1351</cell><cell cols="4">0.59 0.01 0.01 0.45</cell><cell>0</cell><cell>0.01</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>ContinuingNews</cell><cell>4871</cell><cell cols="12">0.40 0.35 0.37 0.43 0.31 0.36 0.46 0.26 0.33 0.50 0.28 0.36</cell></row><row><cell></cell><cell>Advice</cell><cell>1209</cell><cell cols="12">0.17 0.10 0.13 0.18 0.05 0.08 0.23 0.05 0.08 0.27 0.06 0.09</cell></row><row><cell>Other</cell><cell>Sentiment Discussion</cell><cell>6952 2060</cell><cell cols="12">0.73 0.44 0.55 0.72 0.42 0.53 0.68 0.42 0.52 0.64 0.48 0.55 0.25 0.04 0.07 0.23 0.04 0.07 0.21 0.03 0.06 0.20 0.02 0.04</cell></row><row><cell></cell><cell>Irrelevant</cell><cell>2605</cell><cell cols="12">0.20 0.25 0.22 0.21 0.23 0.22 0.16 0.31 0.21 0.17 0.31 0.22</cell></row><row><cell></cell><cell>Unknown</cell><cell>77</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="6">0.01 0.26 0.02 0.01 0.19 0.01</cell></row><row><cell></cell><cell>KnownAlready</cell><cell>1101</cell><cell cols="12">0.13 0.10 0.12 0.13 0.10 0.11 0.24 0.08 0.12 0.26 0.07 0.11</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This research was supported by the <rs type="funder">MSIT(Ministry of Science and ICT)</rs>, <rs type="funder">Korea</rs>, under the <rs type="funder">ITRC</rs>(<rs type="programName">Information Technology Research Center) support program</rs> (<rs type="grantNumber">IITP-2017-2015-0-00378</rs>) supervised by the <rs type="institution">IITP(Institute for Information &amp; communications Technology Promotion)</rs>, and this research was supported by <rs type="programName">Basic Science Research Program</rs> through the <rs type="funder">National Research Foundation of Korea(NRF)</rs> funded by the <rs type="funder">Ministry of Education</rs>(<rs type="grantNumber">NRF-2017R1D1A1B03036275</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_s7t2mBV">
					<idno type="grant-number">IITP-2017-2015-0-00378</idno>
					<orgName type="program" subtype="full">Information Technology Research Center) support program</orgName>
				</org>
				<org type="funding" xml:id="_DJB7RJK">
					<orgName type="program" subtype="full">Basic Science Research Program</orgName>
				</org>
				<org type="funding" xml:id="_sR2yPQU">
					<idno type="grant-number">NRF-2017R1D1A1B03036275</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,90.00,293.72,433.49,9.96;6,90.00,302.72,148.62,9.96" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buntain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<ptr target="http://dcs.gla.ac.uk/~richardm/TREC_IS/" />
		<title level="m" coord="6,308.35,293.72,207.60,9.96">TREC 2018 Incident Streams Track Guidelines</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,314.24,433.53,9.96;6,90.00,323.24,380.65,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,299.78,314.24,205.72,9.96">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,90.00,323.24,295.84,9.96">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,337.10,433.54,9.96;6,90.00,349.10,226.45,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,303.29,337.10,152.21,9.96">Matching networks for one shot learning</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,477.38,337.10,46.16,9.96;6,90.00,349.10,141.51,9.96">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,363.62,433.55,9.96;6,90.00,375.62,247.42,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,294.47,363.62,151.26,9.96">Label-embedding for image classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,455.29,363.62,68.26,9.96;6,90.00,375.62,163.03,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1425" to="1438" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,390.14,433.51,9.96;6,90.00,402.14,270.90,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,127.35,390.14,211.34,9.96">Convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,359.29,390.14,164.22,9.96;6,90.00,402.14,148.99,9.96">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.00,416.60,433.57,9.96;6,90.00,428.60,433.46,9.96;6,90.00,440.60,338.12,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,444.46,428.60,79.00,9.96;6,90.00,440.60,67.76,9.96">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,163.99,440.60,171.26,9.96">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
