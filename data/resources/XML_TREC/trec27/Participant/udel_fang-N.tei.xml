<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,84.82,118.16,442.37,18.14;1,234.80,143.07,142.41,18.14">Paragraph as Lead -Finding Background Documents for News Articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,225.22,182.87,61.83,12.58"><forename type="first">Kuang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<postCode>19716</postCode>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.53,182.87,56.77,12.58"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<postCode>19716</postCode>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,84.82,118.16,442.37,18.14;1,234.80,143.07,142.41,18.14">Paragraph as Lead -Finding Background Documents for News Articles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">63446D2D769826D9B2EAC20933F4BB99</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When reading a news article, it is very useful that articles about the background of various aspects of the story are provided so that the readers can better understand the story. In this year's News Track, we tried to use paragraphs as leads to find background articles since they tend to cover different aspects of the main story <ref type="bibr" coords="1,410.87,321.79,10.91,9.57" target="#b2">[3]</ref>. More specifically, the keywords in the paragraphs were extracted and used as queries to find background articles. Entities were also leveraged to improve the retrieval performances of the keyword queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The News track is introduced this year with the goal of identifying the search needs of readers and journalists <ref type="foot" coords="1,152.86,455.21,4.23,6.99" target="#foot_0">1</ref> . Two potential needs are identified. The first need is, for a news article, to link a list of other articles that a user should read next in order to get the context and background information of the article. This would help readers better understand the story and evaluate the trustworthiness it. The second need is identified as ranking the entities in a news piece based on whether the external information, such as Wikipedia pages, would help readers better understand the piece.</p><p>Based on these needs, two tasks are introduced for the news track, which are background linking and entity ranking. In this year's News Track, we focus on the first task and propose a method that uses paragraphs as leads to find the background articles. More specifically, for each query article, we extract paragraphs for them and identify keywords from the paragraphs using a probabilistic mode. The keywords are used to retrieve articles related to the paragraphs. The retrieval process is improved by using entities to reduce ambiguity. The articles retrieved for different paragraphs are then merged and ranked to generate the final background article list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Paragraph-Based Background Article Discovery</head><p>As mentioned before, we used paragraphs as leads to find background articles. This method is proposed based on our understanding of what background information a reader needs and the unique structure of news articles. A news article usually has a main story as well as its different aspects. An example would be for a news piece about an earthquake, the event itself is reported and other aspects of it such as its aftermath, disaster relief efforts, and earthquake history of the region are also likely to be covered. The background articles for this article could be some other general reports of the same event, or articles that cover in details the aspects mentioned in the original article. Thus, these aspects as well as the main story can be used as leads to find the background articles. Then the problem becomes to how they can be extracted from the article. Luckily, the structure of the news may certainly accelerate such process. According to Fox <ref type="bibr" coords="2,345.64,253.76,11.71,10.48" target="#b2">[3]</ref>, a news article often starts with a summary of the main story, following by detailed coverage of major aspects of the story. The aspects are reported in the order of importance and the unity and completeness of the writing of the aspects are ensured so that even latter an editor decides to cutoff a part of the article from the end, which are aspects that are least important, the story itself will not damaged. Therefore, logical breaks can be inserted into the article so that it is divided into the coverage of various aspects of the article. These aspects then can be used as leads to find background articles. The physical breaks between paragraphs, we argue, can serve as the logic breaks. The information inside a paragraph usually is about only one aspect of the story and therefore the breaks between aspects can only happen at the breaks between paragraphs. As a result, paragraphs can be treated as a text unit of an aspect of the main story that we can use to find background articles about.</p><p>It is important to note that the report of an aspect of the main story can span multiple paragraphs <ref type="bibr" coords="2,133.70,448.77,11.71,10.48" target="#b2">[3]</ref>. However, we argue that paragraphs about the same aspect will result in similar background articles, and through the final merging and ranking step, this redundancy may not be a problem.</p><p>Having paragraphs as leads for finding background articles, the next question that needs to be answered is how should the leads be used. We argue that the idea of relatedness between a paragraph and a background article is similar as the relevance concept in IR. Therefore, it is tackled as a retrieval problem. However, paragraphs are often too long and therefore directly using them for retrieval might lead to noisy results, as it is observed for verbose query retrieval <ref type="bibr" coords="2,152.36,571.56,11.71,10.48" target="#b3">[4]</ref>. Thus, we need a way to find keywords, which are the words that are most representative of the meaning of the paragraphs, so that they can instead be used as the queries. We define keywords as the words that maximize the probability of P (p|w) in which p is a paragraph and w is a word in the paragraph. This is essentially the generation probability of a paragraph given a word. Using Bayes' theorem and bag-of-words assumption it can be re-written as:</p><formula xml:id="formula_0" coords="2,183.18,675.42,356.82,32.38">P (p|w) = P (w|p)P (p) P (w) ∝ P (w|p) = w i ∈p P (w|w i )<label>(1)</label></formula><p>where we assume P (w i ) follows a uniform distribution and ∝ denotes rank equivalent. P (w|w i ) can be easily computed using word co-occurrence and therefore this equation can help us effectively extract keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs</head><p>In this section, we describe how the data was processed and how our runs for the background linking task were generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Processing</head><p>In this year's News Track, the collection is shared by Washington Post, which contains five years of their news articles from 2012 to 2017. All articles in the collection were processed in the same way in which only document id, title, timestamp, and the text of each paragraph were kept. The text of paragraphs of the same article is merged to form a "body" field whereas the title of the article is used as the "title" field of the document. These two fields were then used for indexing and retrieval. A document was constructed in this way so that the "title" field can be prioritized in the retrieval phase. Using the documents, two different types of indices were created in terms of how the text was tokenized. The first type of index tokenized the text normally and used space as the separators of different terms. However, we treated entities, either single-term or multi-term, as single words for the second index type. Entities have been used for improving the retrieval performances, since they are less ambiguous and generally more important than other terms in queries <ref type="bibr" coords="3,431.19,429.97,11.71,10.48" target="#b4">[5]</ref>. More specifically, the entities in the text were identified using DBpedia Spotlight <ref type="bibr" coords="3,416.82,444.42,12.36,10.48" target="#b0">[1]</ref> which automatically annotates DBpedia entities from text. The entities were subsequently replaced by their canonical forms (e.g. the form appears in DBpedia), which are also provided by the toolkit. For example, "the Red Planet" and "Mars" are all mapped to the canonical form "Mars". These canonical forms were used as a single word for indexing. We hope that such way of indexing would help us to more accurately match the entities in the query articles and other articles, which may certainly leads to more effective background article discovery. The parameter "Confidence" of the tool is set to 0.5.</p><p>For query articles, document id, timestamp and the text of each paragraph were kept. However, the paragraph text is not merged since the paragraphs were used separately for retrieving background articles. The text was tokenized using the same two ways as the text of other articles was tokenized. This resulted in two query types. Different query types were used with the corresponding index types. These two types are referred to as Normal and Entity for the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Background Article Retrieval and Result Merging</head><p>Keywords were extracted from the two types of queries using Equation 1. F2exp <ref type="bibr" coords="4,504.14,105.18,12.36,10.48" target="#b1">[2]</ref> was employed to retrieval background articles with keywords from the paragraphs of queries. More specifically, the "title" field of article was given the weight of 0.7 and the "body" field was given the weight of 0.3. The score of a document is the sum of the scores for the two fields. We only retrieved 5 documents for each paragraph. This number as well as the field weights were set empirically according to the test results on the three example queries provided by the organizers.</p><p>After retrieving documents, the final step is to merge the results of each paragraph. We used three ways of merging results. The first one is simply to use the maximum score a document receives among different paragraphs to rank the documents. We call this method score-order. The second one is to use the "voting" of the paragraphs. A document that appears in more paragraphs' results is ranked higher. If there is a vote tie, it is broken by the highest scores that the document receives. This way of ranking documents is called vote-order. The third way of merging results ranks the top documents (e.g. the ones that ranked first) for the results of different paragraphs based on their scores. This process is repeated for other ranking positions from 2 to 5. The final results is the concatenation of the results of this process and the sub-result for the higher ranking positions are put higher in the final list. This merging method is named ladder-order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Run Description</head><p>As discussed earlier, there are two variants of our system, which are the index/query type and the result merging method. We generated 5 runs based on these variants to investigate the effects of them.</p><p>• UDInfolab kweh: Entity index and queries were used to retrieval documents and score-order was used for merging.</p><p>• UDInfolab kwh: Normal index and queries were used to retrieval documents and score-order was used for merging.</p><p>• UDInfolab kwef: Entity index and queries were used to retrieval documents and ladder-order was used for merging.</p><p>• UDInfolab kwf: Normal index and queries were used to retrieval documents and ladder-order was used for merging.</p><p>• UDInfolab kwev: Entity index and queries were used to retrieval documents and vote-order was used for merging. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>The effectiveness of the submitted runs as well as the TREC median are reported in Table <ref type="table" coords="5,534.15,261.30,5.85,10.48" target="#tab_0">1</ref> as NDCG@5. When only comparing submitted runs there are two observations. First, our hypothesis that using entity helps identifying background articles seems to be supported.</p><p>When the merging methods are the same, runs using Entity index and queries (UDInfolab kweh, UDInfolab kwef) outperform their counterparts using Normal index and queries (UDInfolab kwh, UDInfolab kwf). On the other hand, when comparing result merging methods, it seems that the vote-order method is the best. However, when comparing to TREC median, all of the submitted runs do not seem to perform well.</p><p>After obtaining the judgement data, we conducted experiments and error analysis to further investigate our methods. First, we investigate the core part of our runs, which is to use paragraph as lead to find background articles. More specifically, we gathered the retrieval results from the first retrieval step and computed the best possible NDCG@5 can be obtained from this pool of articles. The optimal NDCG@5 for using Entity and Normal are 0.7156 and 0.6552 respectively. Moreover, 10 out 50 queries received perfect NDCG@5 when Entity index and queries were used. Another important detail of the article pool is that since only 5 articles were retrieved for each paragraph, in average the pool size for a query document is only 119. Therefore, it can be concluded that our paragraph-based method can identify background articles effectively.</p><p>We then performed error analysis on the document pools mentioned above to discover why in some cases our methods failed. Two causes were discovered. First, same weights were given for each keyword of a paragraph, which leads to topic drifting. Second, we found that when computing keywords, only paragraphs were taken into account. However, a background article should be related to not only the paragraph, but also the whole document. The error analysis shows potential ways of improving our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this year's News Track, we investigated using keywords in different paragraphs to find background news articles. Moreover, the effectiveness of named entities and different result merging techniques were also tested. The effectiveness of our runs are not very satisfying. However, for article retrieval step, analysis illustrates that using paragraphs can effectively identify background articles. Limitations of the keyword generation and keyword weighting were also discovered through error analysis, which can guide us to improve the proposed methods. Besides, result merging was done using very simple method. It is reasonable to anticipate better results if more sophisticated methods are used, such considering paragraph importance of the query document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,157.38,82.10,297.25,119.56"><head>Table 1 :</head><label>1</label><figDesc>NDCG@5 for submitted runs and TREC median</figDesc><table coords="5,224.64,102.07,162.71,99.59"><row><cell>Run</cell><cell>NDCG@5</cell></row><row><cell>U DInf olab kweh</cell><cell>0.2511</cell></row><row><cell>U DInf olab kwh</cell><cell>0.1818</cell></row><row><cell>DInf olab kwef</cell><cell>0.1938</cell></row><row><cell>DInf olab kwf</cell><cell>0.1444</cell></row><row><cell>DInf olab kwev</cell><cell>0.2693</cell></row><row><cell>T REC median</cell><cell>0.2792</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,89.93,674.66,57.67,8.74"><p>trec-news.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,90.21,243.13,449.78,10.48;6,90.21,257.57,449.78,10.48;6,90.21,272.02,200.49,10.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,352.20,243.13,187.80,10.48;6,90.21,257.57,150.80,10.48">Improving efficiency and accuracy in multilingual entity extraction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,272.07,257.57,267.93,10.48;6,90.21,272.02,158.82,10.48">Proceedings of the 9th International Conference on Semantic Systems (I-Semantics</title>
		<meeting>the 9th International Conference on Semantic Systems (I-Semantics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.21,293.69,449.79,10.48;6,90.21,308.14,225.05,10.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,191.46,293.69,326.15,10.48">An exploration of axiomatic approaches to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,90.21,308.14,188.64,10.48">Proceedings of SIGIR &apos;05. SIGIR &apos;05</title>
		<meeting>SIGIR &apos;05. SIGIR &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.21,329.80,362.79,10.48" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,139.46,329.80,87.32,10.48">Writing the News</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Iowa State University Press</publisher>
		</imprint>
	</monogr>
	<note>3 edn.</note>
</biblStruct>

<biblStruct coords="6,90.21,351.47,449.78,10.48;6,90.21,365.92,205.50,10.48" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,229.27,351.47,207.78,10.48">Information retrieval with verbose queries</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-03">March 2015</date>
			<publisher>ACM -Association for Computing Machinery</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.21,387.59,449.79,10.48;6,90.21,402.03,271.09,10.48" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,186.23,387.59,353.78,10.48;6,90.21,402.03,36.48,10.48">Latent Entity Space: A Novel Retrieval Approach for Entity-Bearing Queries</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,135.79,402.03,110.39,10.48">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="473" to="503" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
