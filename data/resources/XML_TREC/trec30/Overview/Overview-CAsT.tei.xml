<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,53.28,84.23,505.63,15.44">TREC CAsT 2021: The Conversational Assistance Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.11,109.24,68.19,11.96"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
							<email>jeff.dalton@glasgow.ac.uk</email>
						</author>
						<author>
							<persName coords="1,258.49,109.24,76.68,11.96"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<email>chenyan.xiong@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,366.35,109.24,61.77,11.96"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow 1</orgName>
								<address>
									<addrLine>Microsoft Research 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,53.28,84.23,505.63,15.44">TREC CAsT 2021: The Conversational Assistance Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F41F3B16C0CE3C892B56F4744DE863F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>CAsT 2021 is the third year of the Conversational Assistance Track. The techniques for conversational search continue to evolve as the task becomes more challenging. Proven neural query rewriting and ranking approaches based on pre-trained language models continue to improve with new large-scale datasets. As there is increased dependence on long result history, models that discriminatively select relevant parts of the conversation history are increasingly important. The traditional NLP approaches continue to be used, but generative approaches based on large-scale pre-trained language models are most widely used. One important development this year is the use of dense retrieval approaches. The results show that these models are complementary to traditional search approaches and appear to improve recall, but still usually require a multi-pass neural re-ranking model to be most effective.</p><p>Based on participant feedback, CAsT 2021 task is similar to previous years. The task is to identify relevant content for conversational queries that evolve through a trajectory of a discussion on a topic. For 2021, the collection evolved to be based on documents rather than passages to facilitate more complex types of discourse. The collection is similar to previous years and includes MS MARCO documents, an updated dump of Wikipedia from the KILT benchmark, and the Washington Post V4 collection. The collection content is similar to previous years, but with recent content from Wikipedia and WaPo included.</p><p>One important change for CAsT 2021 is that every turn has a single manually selected canonical response passage result representing a previous system response. This evolved from last year when only some turns were manually selected and others automatically added from baselines. This year's manual results provide consistency between automatic and manual runs. The canonical results are used more, with greater query dependence on previous system responses.</p><p>Another minor change to make the conversations more realistic compared with previous years is that the turns introduce simple forms of user revealment, reformulation, and explicit feedback if the previous canonical response is not relevant. This makes the task a bit more realistic by having varying types of user interactions.</p><p>Similar to previous years, the topics in 2021 are based on real user needs from information-seeking sessions in Bing sessions <ref type="bibr" coords="1,282.97,585.52,9.34,8.97" target="#b5">[6]</ref>. The organizers manually reviewed and filtered sessions to ensure they have meaningful trajectories that are then manually rewritten to make them conversational. The topics reflect diverse types of exploratory information needs while also being grounded in real information needs that have content available in the target collection. We detail topic construction in Section 2. Year three continued to have strong participation from more than a dozen teams worldwide. There remains a large gap in effectiveness between manual and automatic systems indicating that there is significant headroom for improving query understanding. In particular, turns with long-distance dependence across multiple turns and queries are very challenging.</p><p>We see CAsT continue to evolve as systems become more capable. This year presented a shift towards retrieving fixed passages in the context of a document. We envision conversations that are more natural with the use of initiative to support more varied types of results and more realistic discourse for complex information tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK, DATA, AND RESOURCES</head><p>The core of the CAsT 2021 task remains mostly unchanged from previous years. The goal of the task is to satisfy a user's complex information need expressed through multi-turn conversational queries/utterances (ùë¢) for each turn ùëá = {ùë¢ 1 , ...ùë¢ ùëñ ...ùë¢ ùëõ }. The change is that these passages come from a document corpus by retrieving and ranking documents and passages from MS MARCO <ref type="bibr" coords="1,546.50,590.07,9.52,8.97" target="#b0">[1]</ref>, Wikipedia -the KILT dump <ref type="bibr" coords="1,425.24,601.03,9.52,8.97" target="#b4">[5]</ref>, and news from the Washington Post V4 collection. Previous CAsT overview papers detail the previous versions of the passage collections. <ref type="bibr" coords="1,469.78,622.94,9.33,8.97" target="#b2">[3,</ref><ref type="bibr" coords="1,481.36,622.94,6.22,8.97" target="#b3">4]</ref>.</p><p>CAsT 2021 has 26 information needs (topics) with an average length of 9.2 utterances, for a total of 239 turns. In comparison, the CAsT 2020 topics are slightly shorter with an average of 8.6 utterances per topic. An example of a 2021 topic is shown in Table <ref type="table" coords="1,553.56,666.78,3.01,8.97" target="#tab_0">1</ref>.</p><p>The rest of this section focuses on the major changes for the third year: the more diverse types of interactions and the increased dependence on previous system responses.</p><p>Information Needs. The high-level method for constructing and filtering topics remains the same. Information needs are based on long sessions from a commercial search engine. Once sessions are filtered, the organizer interacts with a baseline CAsT system that includes rewriting and neural re-ranking (made available to participants) and selects a canonical response passage. This is selected to create challenging conversational trajectories. As a result, each turn has a manually selected passage that can be referred to later in the conversation.</p><p>Collection. Similar to previous years, the collection includes MS MARCO documents, an updated dump of Wikipedia from the KILT benchmark, and the Washington Post V4 collection.</p><p>Due to the presence of duplicates within and across the corpora we identify similar documents using SimHashing <ref type="bibr" coords="2,235.07,229.39,10.52,8.97" target="#b1">[2]</ref> with a 64-bit hash. After initial experimentation, we set the duplicate threshold to be less than 5 bits. We post-process the identified duplicates to remove false positives based on a Jaccard similarity greater than 0.85. The result are duplicates that are excluded from the collection / assessment. Furthermore, to facilitate passage retrieval from the document corpus, we split each document into passages of at most 250 words using version 3.0.6 of the spaCy toolkit with the en_core_web_sm-3.0.0 model. We provide participants with the duplicates list for their de-duplication efforts and tools for passage segmentation.</p><p>Interactive CAsT (iCAsT) System. The baseline interactive system<ref type="foot" coords="2,78.82,358.91,3.38,7.27" target="#foot_1">1</ref> features a web interface built on top of the well-performing re-writing and retrieval systems from previous years. The system re-writes queries with a T5-based query re-writer fine-tuned on the CANARD dataset <ref type="foot" coords="2,131.46,391.79,3.38,7.27" target="#foot_2">2</ref> . This uses all previous turn queries and the three previous turn canonical passage responses as context, subject to length constraints. For turns with fewer than three previous turns, the re-writer uses all the available previous turn queries and canonical responses (none for the first turn). When context becomes too large (i.e more than the 512 tokens), the re-writer truncates content from the previous turn passages and queries (oldest first) to accommodate context.</p><p>For search, the system retrieves an initial 50 (for interaction) candidate documents using BM25 from the collection. It then segments candidate documents into passages for re-ranking based on user-specified parameters from the web interface. Passage are reranked using a T5 based passage re-ranker trained on MS MARCO and available through Pygaggle <ref type="foot" coords="2,172.99,534.25,3.38,7.27" target="#foot_3">3</ref> . The passages are clustered by document in order of max passage score.</p><p>Canonical Response. Introduced last year, the interactive nature of the response dependence requires including a fixed system response as part of the benchmark. A key difference is that every turn has a manually selected passage. In year two this was only a subset of turns.</p><p>In year three the organizers provide a single canonical passage response manually selected for all turns. The response is often selected from results in the baseline results that the organizer finds engaging. The organizers use the baseline system to check the number and nature of answer passages and also ensure challenging conversational structure. The canonical results are usually at least partially relevant, although when the baseline fails the organizer decides to select an irrelevant result or a passage from a different source to advance the conversation. There is only one canonical response for both manual and automatic queries.</p><p>Interactive topic development with manual canonical results required significant time commitment from the organizers. It took approximately six hours per topic to iteratively develop and refine the turn trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Result Dependence Considerations</head><p>The introduction of response dependence more closely models the challenges faced in conversational search systems, but the offline nature of this benchmark and our goal to ensure re-usability introduces challenges and limitations.</p><p>The issue remains that there is only one response provided. Because there is often dependence on the result as a building block for later turns, there is a greater need for the result to be at least partially related to the conversational trajectory. As a result, the canonical responses may not truly reflect the quality of an automatic system.</p><p>There are three categories in this year's runs based on the data used in the testing phrase:</p><p>(1) Manual: Runs that use the manually rewritten (resolved) context-free queries, and/or manual canonical responses. (2) Automatic-Canonical: Automatic runs that use the provided automatic canonical system responses. (3) Automatic-Raw: Runs that only use the provided raw conversational queries, automatic baseline results, automatic query re-writes, and/or other data sources that do only contain manual or automatic-canonical information.</p><p>This year all Automatic runs are reported together. The increased level of result dependence means that automatic systems that don't use the canonical response (Raw) are at a significant disadvantage by ignoring previous conversation context.</p><p>Feedback and Revealment In combination with fixed canonical responses, this year also introduced feedback and revealment discourse types. These were added by the organizers and not present in the original search sessions.</p><p>For feedback, this year includes turns with explicit relevance feedback on the canonical result. The results from the baseline system may or may not be relevant to the previous user request. In these cases, the organizer had a choice to introduce a feedback turn ("What? No, I want to know..") to give a hint to the system, carry on from the result provided ("No, I meant the funny car. But, that's interesting... "), or change topics ("That's not what I wanted. How about recent developments.."). In all cases, the feedback or reformulation was explicit and fully contained in a single turn.</p><p>This year also introduced simplistic forms of user revealment. In some (rare) cases, the simulated user revealed information that is part of a turn, "I live in Seattle and have a big lawn." or "I'm a runner and I've been feeling tired.", or "I'd like a more scientific explanation. ". Some of these elements were required for subsequent turns in the conversation. Generated Baseline Runs. The organizers generate seven baseline runs this year including ones that use traditional sparse retrieval and those using dense retrieval. These are described in detail below:</p><p>(1) org_auto_bm25_t5: Based on the baseline interactive system, this uses the T5-based query re-writer fine-tuned on the CANARD dataset for generative query rewriting on the raw utterances. As with the interactive system, the rewriter uses all previous turn queries and the three previous canonical passage responses as context. In addition, where turns had fewer than three previous turns (e.g turn (3) org_auto_convdr_bert: This performs re-ranking of the top 100 passages from convdr. For re-ranking, it uses a BERTbased pointwise ranker pre-trained on MS MARCO and further fine-tuned on CAsT 2020 using joint supervision of manual utterances and relevance labels. (4) org_manual_bm25: This uses BM25 from Pyserini to perform passage retrieval on the collection with the default configuration and manual utterances for retrieval. (5) org_manual_bm25_t5: This follows the same setup as org_-auto_bm25_t5 (document retrieval then passage segmentation and re-ranking) using the manually rewritten utterances. <ref type="bibr" coords="3,328.87,516.51,9.51,8.97" target="#b5">(6)</ref> org_manual_ance: This uses an ANCE checkpoint trained on MS MARCO Passages to encode passages and manually rewritten utterances and retrieves the most similar. (7) org_manual_ance_bert: A fine-tuned BERT-base re-ranks the top 100 retrieved passages from org_manual_ance.</p><p>Other Data Resources. The organizers released similar resources released as year two <ref type="bibr" coords="3,427.72,588.69,10.68,8.97" target="#b3">[4]</ref> including canonical results and baseline runs. In addition, the organizers release the contextual dependence labels and types of queries developed during topic curation. These were created during topic construction and verified, similar to the previous year. We use these annotations to analyze the influence of contextual dependence discussed later.</p><p>Evaluation. The overlap among submissions was high again this year. The evaluation was similar to last year, except that judgments were performed at the document level, as described below.</p><p>The organizers opted to build shallow pools from a larger number of topics. The original pools were formed using the top ten passages from up to four runs per group (a total of 57 runs). Based on assessing speeds last year, it seemed likely that it would be possible to fully assess at least 20 topics. The initial plan was to assess all queries for all topics.</p><p>Just before assessing, NIST discovered that some runs contained invalid passage identifiers. A brief investigation determined that different versions or configurations of spaCy produced differing segmentations of documents. Seven groups had at least one run with invalid passage ids. As a result, NIST converted the passageoriented pools to document-oriented pools. Passage identifiers are truncated to remove the passage id and a max passage algorithm was used to convert passage runs to document runs. Duplicate retrieved documents are removed from the rankings. The top seven documents from each run are used to construct pools from 50 participant runs and 7 organizer runs.</p><p>Because assessing documents is slower than passages, pools are shallower and fewer topics/turns assessed. The final qrels were built from depth-7 pools across all submissions (plus the seven baseline runs from the organizers). Nineteen topics have at least some turns judged, most of which were judged through turn eight. Some topics had additional turns judged and a few have less than eight turns judged (125, 128). Turn 117_7 was removed due to only one relevant doc. Topics 109, 114, 120, 122, 126, and 130 were not assessed due to resource constraints.</p><p>The CAsT organizers thank Ian Soboroff and Ellen Voorhees for responding quickly and gracefully to the unanticipated, last-minute changes that the document segmentation issues introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARTICIPANTS</head><p>CAsT received 50 run submissions from 15 teams shown in Table <ref type="table" coords="4,289.41,413.90,3.01,8.97" target="#tab_1">2</ref>. Participants provided metadata and descriptions of their runs.</p><p>Similar to last year, many teams used a multi-step pipeline consisting of 1) conversational rewriting (most incorporating the previous canonical responses), 2) retrieval using traditional IR or dense model, and 3) re-ranking with a neural language model fine-tuned for pointwise (mono) and pairwise (duo) ranking. Almost all teams leverage pre-trained Transformer-based language models for rewriting (BART, T5) and ranking (BERT, ALBERT, T5). Some teams also perform document expansion with generated queries (doc2query). Multiple teams use dense retrieval for passage retrieval and experiment with conversationally encoding queries with result context rather than relying solely on generative query rewriting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OVERALL RESULTS</head><p>In this section, we present the results of the submitted runs. We include seven organizer baselines (prefixed org) described above that are available in the public CAsT Github repository.</p><p>The main results are turn-level macro-averaged response effectiveness. We use four standard evaluation measures: Recall, Mean Average Precision (MAP@500), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG@500). The primary measure continues to be NDCG@3 to focus on high-precision and quality responses in the top ranks. Note that we use 500 instead of the typical 1000 because of the conversion from passages to documents. Consistent with previous years, we threshold the official results using a relevance cutoff of two as positive for binary measures, because the value of one is marginal in the guidelines. Automatic run results. Table <ref type="table" coords="4,447.20,678.34,4.25,8.97" target="#tab_3">3</ref> shows the results for the 37 automatic runs with a median NDCG@3 score of 0.377. The top two runs use a pipeline that includes mono-duo T5 re-ranking after a first pass retrieval. The most successful first pass retrieval pipelines leverage both generative conversational query rewriting and dense retrieval (on its own or in combination with a traditional retrieval). It's noteworthy that cqe and cqe-t5 use only conversational understanding combined with dense retrieval approaches without further re-ranking and outperform the organizer t5 re-ranking baseline due to their improvements in recall effectiveness. All the other topperforming runs use a combination of a mono/duo re-ranking with a neural language model. We observe that while many of the new conversational dense retrieval approaches perform competitively in recall (t5colbert, DPH-auto-rye), most appear to benefit significantly from further re-ranking for high precision in the top ranks. Note that the organizer convdr runs use the top 100 passages.</p><p>Manual run results. Table <ref type="table" coords="5,163.61,229.39,4.09,8.97" target="#tab_4">4</ref> shows the results for the 13 manual runs with a median NDCG@3 value of 0.554. Three runs outperform the organizer bm25_t5 re-ranking. These runs both use forms of query and/or document expansion. The best performing run, clarke_manual performs query expansion on an external collection, dense passage retrieval with ANCE, and a mono/duo re-ranking with T5. This approach achieves very high recall (92.7%), indicating that there is headroom in the re-ranking phase even with manually rewritten queries. The runs from CFDA_CLIP both also include document expansion from doc2query.</p><p>Overall. The best automatic runs achieve high recall, approximately 85%, with manual runs only 9% more effective than automatic. The improvement in NDCG@3 between the best manual and automatic runs is 22%, greater than in previous years. This indicates that while the automatic methods identify candidates reasonably well the conversational query understanding remains challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results by Topic</head><p>Figure <ref type="figure" coords="5,78.82,435.82,4.09,8.97" target="#fig_1">1</ref> provides a per-topic analysis comparing the two classes of systems across topics. It uses data from all submitted runs. As with last year, the results show that the topic difficulty varies widely across topics. We observe that there is a large absolute gap in approximately half the topics overall. The hardest topics for automatic systems are 113 and 117. This is also indicated in the structure of Topic 113, which includes multiple feedback turns when the baseline system fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results by turn depth</head><p>In this section, we discuss how systems perform over the course of the conversation and as turn depth increases. Due to the small sample size, turns beyond nine are truncated. Figure <ref type="figure" coords="5,250.12,576.49,4.24,8.97" target="#fig_2">2</ref> shows the average NDCG@3 at each turn depth for the different categories of systems. To focus on strong systems, the figure only shows the data for runs that perform at or above the median NDCG@3.</p><p>For the automatic runs, the results show a steady turn-by-turn drop in system effectiveness from turn one to five. Although there is a slight increase between turns five and eight, system effectiveness drops by 49% from the beginning. In contrast, the Manual runs appear relatively consistent across turn depth, with even a slight increase in effectiveness (except turn 9). The most noticeable pattern is the steady degradation in effectiveness up to turn 5 in automatic runs. Although the gap between automatic and manual systems appears to widen as turn depth increases, the fact that they follow similar behavior may also indicate the discourse structure challenges in the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Explicit Query and Turn Dependence</head><p>In this section, we study the effect of context. In the organizers' dependence annotations 60 queries depend on a query from a previous turn in the conversation. There are 86 queries that depend on results from previous turns. Note that a turn may depend on multiple previous queries, results, or a combination of both.</p><p>Figure <ref type="figure" coords="5,354.18,348.69,4.25,8.97" target="#fig_4">3</ref> shows the breakdown of these dependencies by turn depth according to the source turn that is referenced. Compared to year two, there is a less dependence on the first turn query, and increased result dependencies across all turns. The trend of strong dependence on the previous turns continues this year, with the majority of query dependencies being to the immediate previous turn. About a fourth of the query dependencies are hard, defined to reference not to the first or immediate preceding turn.</p><p>Table <ref type="table" coords="5,351.14,436.36,4.25,8.97" target="#tab_5">5</ref> shows the results broken down by different types of conversational contexts. Some turns (for example all first turns) do not rely on a previous turn at all. We label this None and they represent approximately 20% of turns. Others only depend on previous utterances Query. Less than 40% of all turns depend on a previous utterance with some of these being the first turn. We also further split the dependence into a Query hard subset (approximately 10% of turns) where there is a dependence on a previous query that is not the first turn and not the immediately preceding turn. The bottom two rows focus on result dependence. The Result type has 54% of turns and Result Hard has 11% of turns. Similar to queries, the hard variant for results is result dependence beyond the first or immediate preceding turn.</p><p>The results show that turns without dependence are the easiest and systems perform the best on this subset. Systems perform around the same on the Query subset as they do on all turns. Interestingly, systems perform slightly better on queries with harder context (Query hard) than they do on the Query subset, which bears further investigation.</p><p>As with last year, turns with result dependence perform are harder than query dependence -an 8% relative reduction compared with query dependence. And the hard subset of result dependencies are even more challenging, with a further 11% relative drop in system effectiveness. As with last year, this continues to highlight areas for further research.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The third year of TREC CAsT continued developing resources for studying conversational information seeking and added to the community's understanding of the topic. Conversations had more varied types of discourse with feedback turns and elements of user revealment with greater dependence on canonical system response.  ‚Ä¢ Conversational Dense Retrieval. To overcome issues of increasing sequence length and issues of lexical mismatch the use of dense retrieval for the task was seen for the first time.</p><p>The results show they can be highly effective for recall but are most effective when combined with neural re-ranking. ‚Ä¢ Conversational Language Understanding. Sequence-tosequence generative rewriting methods continue to be widely used and models are improving to handle more complex types of dependence.</p><p>‚Ä¢ Conversational Context. Clean, resolved context remains advantageous for manual runs as the results show that they maintain or even improve in effectiveness over the course of the conversation. In contrast, automatic systems still suffer from degradation in their effectiveness as conversations become longer. ‚Ä¢ Ranking. The use of pre-trained neural language models for ranking continues to be widely used in the most effective systems. The results show that there is still significant headroom for precision in the early ranks. ‚Ä¢ Conversational structure. More systems used the canonical responses because this year dependence on system response was very important for effectiveness. The use of the iCAsT interactive system for topic development made canonical responses for all turns possible. We note that injecting canonical responses into systems can lead to artificial behavior. To overcome this, new methods for scalably and interactively developing conversational trajectories on the same topic is an area for the future. After the success of year three, we look forward to year four.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,317.83,343.26,17.78,6.74;6,94.51,227.05,6.74,27.08;6,94.51,192.21,6.74,32.83;6,112.45,315.98,18.19,6.74;6,112.45,266.24,18.19,6.74;6,112.45,216.50,18.19,6.74;6,112.45,166.76,18.19,6.74;6,112.45,117.02,18.19,6.74;6,141.91,323.86,369.43,6.74;6,252.67,99.18,51.31,6.74;6,327.22,99.18,42.84,6.74"><head></head><label></label><figDesc>110 111 112 113 115 116 117 118 119 121 124 125 127 128 129 131 Automatic Runs Manual Runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,184.09,378.31,243.81,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NDCG@3 aggregated for each topic across all runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,53.80,587.35,240.25,7.70;6,53.80,598.31,193.76,7.70;6,53.80,402.17,252.20,171.19"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NDCG@3 at varying conversation turn depth. We report the average across runs median or better.</figDesc><graphic coords="6,53.80,402.17,252.20,171.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,317.96,572.11,240.24,7.70;6,317.96,583.07,233.90,7.70"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Statistics on the source of contextual information in query and response (used by later turns) by turn depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,323.78,154.18,228.61,207.32"><head>Table 1 :</head><label>1</label><figDesc>CAsT 2021 Topic 112.</figDesc><table coords="1,323.78,176.40,228.61,185.11"><row><cell cols="2">Title: Steroid use in US sports</cell></row><row><cell cols="2">Description: The history of steroid use in US sports.</cell></row><row><cell cols="2">Turn Conversation Utterances</cell></row><row><cell>1</cell><cell>What's the history of steroid use in sports in the US?</cell></row><row><cell>2</cell><cell>What were Ziegler's improvements?</cell></row><row><cell>3</cell><cell>Why are they banned?</cell></row><row><cell>4</cell><cell>Are there visible signs?</cell></row><row><cell>5</cell><cell>That sounds easy to spot. How do they get away with</cell></row><row><cell></cell><cell>it?</cell></row><row><cell>6</cell><cell>What is the NFL policy?</cell></row><row><cell>7</cell><cell>Isn't that speed?</cell></row><row><cell>8</cell><cell>What is the difference between the two policies?</cell></row><row><cell>9</cell><cell>I heard it even affects card players. Didn't bridge also</cell></row><row><cell></cell><cell>have a problem?</cell></row><row><cell>10</cell><cell>I know what bridge is. I heard there was a drug scandal</cell></row><row><cell></cell><cell>recently.</cell></row><row><cell>11</cell><cell>Does the article have more about it?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,132.85,85.73,346.30,279.70"><head>Table 2 :</head><label>2</label><figDesc>Participants and their runs.</figDesc><table coords="3,132.85,108.97,346.30,256.46"><row><cell>Group</cell><cell>Run ID</cell><cell cols="2">Run Type Group</cell><cell>Run ID</cell><cell>Run Type</cell></row><row><cell cols="2">CFDA_CLIP CFDA_CLIP_ARUN1</cell><cell>canonical</cell><cell>MLIA-LIP6</cell><cell>t5colbert</cell><cell>canonical</cell></row><row><cell cols="2">CFDA_CLIP CFDA_CLIP_ARUN2</cell><cell>canonical</cell><cell>RUIR</cell><cell>RUIR1_TURN-FT</cell><cell>manual</cell></row><row><cell cols="2">CFDA_CLIP CFDA_CLIP_MRUN1</cell><cell>manual</cell><cell>RUIR</cell><cell>RUIR2_TURN</cell><cell>manual</cell></row><row><cell cols="2">CFDA_CLIP CFDA_CLIP_MRUN2</cell><cell>manual</cell><cell>RUIR</cell><cell>RUIR4_HIST</cell><cell>manual</cell></row><row><cell>CMU-LTI</cell><cell>LTI-entity-g</cell><cell>manual</cell><cell>TKB48</cell><cell>bm25_automatic</cell><cell>raw</cell></row><row><cell>CMU-LTI</cell><cell>LTI-rewriter-5q</cell><cell>canonical</cell><cell>TKB48</cell><cell>dense_manual</cell><cell>manual</cell></row><row><cell>CMU-LTI</cell><cell>LTI-rewriter-g</cell><cell>canonical</cell><cell>TKB48</cell><cell>hybrid_manual</cell><cell>manual</cell></row><row><cell>CMU-LTI</cell><cell>LTI-rewriter-tc</cell><cell>canonical</cell><cell>TKB48</cell><cell>sparse_manual</cell><cell>manual</cell></row><row><cell>CNR</cell><cell>CNR-run1</cell><cell>raw</cell><cell>UAmsterdam</cell><cell>astypalaia256</cell><cell>canonical</cell></row><row><cell>CNR</cell><cell>CNR-run2</cell><cell>canonical</cell><cell>UAmsterdam</cell><cell>historyonly</cell><cell>raw</cell></row><row><cell>CNR</cell><cell>CNR-run3</cell><cell>raw</cell><cell>UAmsterdam</cell><cell>historyonlyKILT</cell><cell>raw</cell></row><row><cell>CNR</cell><cell>CNR-run4</cell><cell>canonical</cell><cell>UiS</cell><cell>UiS_raft</cell><cell>manual</cell></row><row><cell>h2oloo</cell><cell>cqe</cell><cell>canonical</cell><cell>UMD</cell><cell>umd2021_run1</cell><cell>canonical</cell></row><row><cell>h2oloo</cell><cell>cqe-t5</cell><cell>canonical</cell><cell>UMD</cell><cell cols="2">umd2021_run2doc canonical</cell></row><row><cell>h2oloo</cell><cell>mono-duo-rerank</cell><cell>canonical</cell><cell>UMD</cell><cell>umd2021_run3rrf</cell><cell>canonical</cell></row><row><cell>h2oloo</cell><cell>t5</cell><cell>canonical</cell><cell>UMD</cell><cell cols="2">umd2021_run4den canonical</cell></row><row><cell>HBKU</cell><cell>HBKU_CQR_POS</cell><cell>canonical</cell><cell>uogTr</cell><cell>uogTrADT</cell><cell>raw</cell></row><row><cell>HBKU</cell><cell>HBKU_CQR_TC</cell><cell>canonical</cell><cell>uogTr</cell><cell>uogTrMDT</cell><cell>manual</cell></row><row><cell>HBKU</cell><cell>HBKU_CQR-HC</cell><cell>raw</cell><cell>uogTr</cell><cell>uogTrTCT</cell><cell>canonical</cell></row><row><cell>HBKU</cell><cell cols="2">HBKU_CQRHC_BM25 canonical</cell><cell>uogTr</cell><cell>uogTrTDT</cell><cell>canonical</cell></row><row><cell>IITD-DBAI</cell><cell>IITD-RAW_U_T5_1</cell><cell>raw</cell><cell>V-Ryerson</cell><cell>DPH-auto-rye</cell><cell>canonical</cell></row><row><cell>IITD-DBAI</cell><cell>IITD-RAW_U_T5_2</cell><cell>raw</cell><cell>V-Ryerson</cell><cell>DPH-manual-rye</cell><cell>manual</cell></row><row><cell>MLIA-LIP6</cell><cell>Rewritt5_monot5</cell><cell>canonical</cell><cell cols="2">WaterlooClarke clarke-auto</cell><cell>raw</cell></row><row><cell>MLIA-LIP6</cell><cell>t5_doc2query</cell><cell>canonical</cell><cell cols="2">WaterlooClarke clarke-cc</cell><cell>canonical</cell></row><row><cell>MLIA-LIP6</cell><cell>t5_monot5</cell><cell>canonical</cell><cell cols="2">WaterlooClarke clarke-manual</cell><cell>manual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,64.71,501.02,230.86,206.23"><head></head><label></label><figDesc>This uses ConvDR to encode the conversational query and passages in the collection. It performs Maximum Inner Product Search to retrieve 100 candidate responses. The query encoder is trained on TREC CAsT 2020 data and the encoder uses the concatenation of historical queries, the last system response, and the current query. The passage encoder is from ANCE.</figDesc><table coords="3,64.71,501.02,230.86,140.47"><row><cell>1), the</cell></row><row><cell>re-writer uses all the available previous turn queries and</cell></row><row><cell>canonical responses. No extra considerations are made for</cell></row><row><cell>instances where the context might have been too large (e.g</cell></row><row><cell>deep turns, long paragraphs). For retrieval, BM25 (k1=4.46,</cell></row><row><cell>b=0.82) is used to collect the top 1000 documents from the</cell></row><row><cell>collection. These are segmented into sentence-based pas-</cell></row><row><cell>sages (using spaCy's SentenceRecongnizer model) with a</cell></row><row><cell>maximum length of 250 words. The passages are re-ranked</cell></row><row><cell>with a pointwise (mono) T5 passage ranker trained on MS</cell></row><row><cell>MARCO. The run file returns the top thousand passages for</cell></row><row><cell>each query.</cell></row><row><cell>(2) org_auto_convdr:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,317.66,85.73,240.54,344.22"><head>Table 3 :</head><label>3</label><figDesc>Automatic response retrieval results. Evaluation at retrieval cutoff of 500 with a binary relevance threshold of 2.</figDesc><table coords="4,321.42,119.26,233.32,310.69"><row><cell>Group</cell><cell>Run</cell><cell cols="4">Recall MAP MRR NDCG NDCG@3</cell></row><row><cell>h2oloo</cell><cell>mono-duo-rerank</cell><cell>0.850</cell><cell>0.376 0.679</cell><cell>0.636</cell><cell>0.526</cell></row><row><cell cols="2">WaterlooClarke clarke-cc</cell><cell>0.869</cell><cell>0.362 0.684</cell><cell>0.640</cell><cell>0.514</cell></row><row><cell>h2oloo</cell><cell>cqe-t5</cell><cell>0.846</cell><cell>0.342 0.644</cell><cell>0.618</cell><cell>0.488</cell></row><row><cell>HBKU</cell><cell>HBKU_CQR_TC</cell><cell>0.696</cell><cell>0.310 0.632</cell><cell>0.540</cell><cell>0.477</cell></row><row><cell>HBKU</cell><cell>HBKU_CQRHC_BM25</cell><cell>0.598</cell><cell>0.287 0.622</cell><cell>0.490</cell><cell>0.471</cell></row><row><cell>HBKU</cell><cell>HBKU_CQR_POS</cell><cell>0.588</cell><cell>0.283 0.616</cell><cell>0.487</cell><cell>0.451</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_ARUN1</cell><cell>0.697</cell><cell>0.308 0.613</cell><cell>0.539</cell><cell>0.444</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_ARUN2</cell><cell>0.652</cell><cell>0.301 0.608</cell><cell>0.518</cell><cell>0.439</cell></row><row><cell>h2oloo</cell><cell>cqe</cell><cell>0.791</cell><cell>0.289 0.603</cell><cell>0.557</cell><cell>0.438</cell></row><row><cell>MLIA-LIP6</cell><cell>t5_doc2query</cell><cell>0.761</cell><cell>0.290 0.585</cell><cell>0.548</cell><cell>0.436</cell></row><row><cell>-</cell><cell>org_auto_bm25_t5</cell><cell>0.636</cell><cell>0.291 0.607</cell><cell>0.504</cell><cell>0.436</cell></row><row><cell>UMD</cell><cell>umd2021_run3rrf</cell><cell>0.723</cell><cell>0.298 0.611</cell><cell>0.539</cell><cell>0.425</cell></row><row><cell>-</cell><cell>org_convdr_bert</cell><cell>0.426</cell><cell>0.236 0.607</cell><cell>0.398</cell><cell>0.423</cell></row><row><cell>uogTr</cell><cell>uogTrADT</cell><cell>0.661</cell><cell>0.278 0.581</cell><cell>0.501</cell><cell>0.417</cell></row><row><cell>UMD</cell><cell>umd2021_run2doc</cell><cell>0.613</cell><cell>0.262 0.558</cell><cell>0.478</cell><cell>0.399</cell></row><row><cell>HBKU</cell><cell>HBKU_CQR-HC</cell><cell>0.531</cell><cell>0.236 0.531</cell><cell>0.422</cell><cell>0.392</cell></row><row><cell>UMD</cell><cell>umd2021_run1</cell><cell>0.613</cell><cell>0.250 0.544</cell><cell>0.464</cell><cell>0.389</cell></row><row><cell>MLIA-LIP6</cell><cell>t5_monot5</cell><cell>0.360</cell><cell>0.190 0.571</cell><cell>0.337</cell><cell>0.388</cell></row><row><cell>IITD-DBAI</cell><cell>IITD-RAW_U_T5_2</cell><cell>0.327</cell><cell>0.175 0.515</cell><cell>0.316</cell><cell>0.380</cell></row><row><cell>h2oloo</cell><cell>t5</cell><cell>0.364</cell><cell>0.176 0.534</cell><cell>0.336</cell><cell>0.377</cell></row><row><cell>UMD</cell><cell>umd2021_run4den</cell><cell>0.735</cell><cell>0.265 0.521</cell><cell>0.512</cell><cell>0.377</cell></row><row><cell cols="2">WaterlooClarke clarke-auto</cell><cell>0.721</cell><cell>0.260 0.524</cell><cell>0.487</cell><cell>0.375</cell></row><row><cell>IITD-DBAI</cell><cell>IITD-RAW_U_T5_1</cell><cell>0.312</cell><cell>0.166 0.509</cell><cell>0.303</cell><cell>0.371</cell></row><row><cell>MLIA-LIP6</cell><cell>Rewritt5_monot5</cell><cell>0.361</cell><cell>0.184 0.549</cell><cell>0.332</cell><cell>0.370</cell></row><row><cell>CMU-LTI</cell><cell>LTI-rewriter-g</cell><cell>0.465</cell><cell>0.209 0.521</cell><cell>0.386</cell><cell>0.369</cell></row><row><cell>CMU-LTI</cell><cell>LTI-rewriter-tc</cell><cell>0.465</cell><cell>0.211 0.528</cell><cell>0.387</cell><cell>0.367</cell></row><row><cell>-</cell><cell>org_convdr</cell><cell>0.426</cell><cell>0.197 0.505</cell><cell>0.372</cell><cell>0.361</cell></row><row><cell>CNR</cell><cell>CNR-run3</cell><cell>0.190</cell><cell>0.123 0.472</cell><cell>0.222</cell><cell>0.349</cell></row><row><cell>CNR</cell><cell>CNR-run4</cell><cell>0.187</cell><cell>0.116 0.477</cell><cell>0.220</cell><cell>0.333</cell></row><row><cell>uogTr</cell><cell>uogTrTDT</cell><cell>0.557</cell><cell>0.216 0.491</cell><cell>0.408</cell><cell>0.332</cell></row><row><cell>uogTr</cell><cell>uogTrTCT</cell><cell>0.562</cell><cell>0.214 0.473</cell><cell>0.414</cell><cell>0.323</cell></row><row><cell>TKB48</cell><cell>bm25_automatic</cell><cell>0.623</cell><cell>0.173 0.474</cell><cell>0.405</cell><cell>0.317</cell></row><row><cell>CNR</cell><cell>CNR-run2</cell><cell>0.167</cell><cell>0.107 0.444</cell><cell>0.202</cell><cell>0.304</cell></row><row><cell>CNR</cell><cell>CNR-run1</cell><cell>0.164</cell><cell>0.101 0.406</cell><cell>0.196</cell><cell>0.298</cell></row><row><cell>CMU-LTI</cell><cell>LTI-rewriter-5q</cell><cell>0.392</cell><cell>0.158 0.428</cell><cell>0.319</cell><cell>0.296</cell></row><row><cell>UAmsterdam</cell><cell>astypalaia256</cell><cell>0.453</cell><cell>0.120 0.364</cell><cell>0.304</cell><cell>0.236</cell></row><row><cell>V-Ryerson</cell><cell>DPH-auto-rye</cell><cell>0.624</cell><cell>0.145 0.367</cell><cell>0.360</cell><cell>0.232</cell></row><row><cell>UAmsterdam</cell><cell>historyonlyKILT</cell><cell>0.288</cell><cell>0.084 0.314</cell><cell>0.214</cell><cell>0.196</cell></row><row><cell>UAmsterdam</cell><cell>historyonly</cell><cell>0.252</cell><cell>0.077 0.317</cell><cell>0.198</cell><cell>0.195</cell></row><row><cell>MLIA-LIP6</cell><cell>t5colbert</cell><cell>0.589</cell><cell>0.076 0.270</cell><cell>0.314</cell><cell>0.154</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,317.66,447.12,242.15,187.24"><head>Table 4 :</head><label>4</label><figDesc>Manual retrieval results. These runs used the manually resolved queries and/or manual canonical results. Evaluation at retrieval cutoff of 500 with a binary relevance threshold of 2.</figDesc><table coords="4,321.33,502.53,233.50,131.83"><row><cell>Group</cell><cell>Run</cell><cell cols="4">Recall MAP MRR NDCG NDCG@3</cell></row><row><cell cols="2">WaterlooClarke clarke-manual</cell><cell>0.927</cell><cell>0.473 0.793</cell><cell>0.727</cell><cell>0.644</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_MRUN1</cell><cell>0.806</cell><cell>0.434 0.800</cell><cell>0.669</cell><cell>0.628</cell></row><row><cell>CFDA_CLIP</cell><cell>CFDA_CLIP_MRUN2</cell><cell>0.863</cell><cell>0.438 0.792</cell><cell>0.687</cell><cell>0.626</cell></row><row><cell>-</cell><cell>org_manual_bm25_t5</cell><cell>0.796</cell><cell>0.419 0.780</cell><cell>0.649</cell><cell>0.595</cell></row><row><cell>uogTr</cell><cell>uogTrMDT</cell><cell>0.831</cell><cell>0.424 0.777</cell><cell>0.665</cell><cell>0.592</cell></row><row><cell>UiS</cell><cell>UiS_raft</cell><cell>0.761</cell><cell>0.397 0.746</cell><cell>0.637</cell><cell>0.579</cell></row><row><cell>RUIR</cell><cell>RUIR1_TURN-FT</cell><cell>0.796</cell><cell>0.378 0.717</cell><cell>0.618</cell><cell>0.554</cell></row><row><cell>RUIR</cell><cell>RUIR2_TURN</cell><cell>0.796</cell><cell>0.390 0.737</cell><cell>0.626</cell><cell>0.554</cell></row><row><cell>-</cell><cell>org_manual_ance</cell><cell>0.539</cell><cell>0.308 0.727</cell><cell>0.485</cell><cell>0.548</cell></row><row><cell>-</cell><cell>org_manual_ance_bert</cell><cell>0.539</cell><cell>0.329 0.702</cell><cell>0.498</cell><cell>0.540</cell></row><row><cell>RUIR</cell><cell>RUIR4_HIST</cell><cell>0.796</cell><cell>0.325 0.664</cell><cell>0.593</cell><cell>0.493</cell></row><row><cell>CMU-LTI</cell><cell>LTI-entity-g</cell><cell>0.492</cell><cell>0.264 0.632</cell><cell>0.434</cell><cell>0.462</cell></row><row><cell>TKB48</cell><cell>hybrid_manual</cell><cell>0.710</cell><cell>0.197 0.601</cell><cell>0.467</cell><cell>0.438</cell></row><row><cell>TKB48</cell><cell>dense_manual</cell><cell>0.624</cell><cell>0.181 0.580</cell><cell>0.428</cell><cell>0.417</cell></row><row><cell>TKB48</cell><cell>sparse_manual</cell><cell>0.747</cell><cell>0.237 0.595</cell><cell>0.510</cell><cell>0.407</cell></row><row><cell>-</cell><cell>org_manual_bm25</cell><cell>0.471</cell><cell>0.213 0.594</cell><cell>0.400</cell><cell>0.407</cell></row><row><cell>V-Ryerson</cell><cell>DPH-manual-rye</cell><cell>0.188</cell><cell>0.074 0.386</cell><cell>0.170</cell><cell>0.252</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,317.66,85.73,240.54,108.30"><head>Table 5 :</head><label>5</label><figDesc>Dependence results for automatic runs. We report the average across automatic runs median or better.</figDesc><table coords="5,359.04,120.00,158.09,74.02"><row><cell cols="3">Dependence Turns Auto. NDCG@3</cell></row><row><cell>All turns</cell><cell>158</cell><cell>0.433</cell></row><row><cell>None</cell><cell>31</cell><cell>0.513</cell></row><row><cell>Query</cell><cell>60</cell><cell>0.429</cell></row><row><cell>Query (hard)</cell><cell>16</cell><cell>0.440</cell></row><row><cell>Result</cell><cell>86</cell><cell>0.393</cell></row><row><cell>Result (hard)</cell><cell>17</cell><cell>0.348</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,53.80,685.28,91.85,6.23;1,53.32,693.42,113.01,6.97"><p>TREC'21, November 2021, Virtual ¬© 2022 Copyright held by the author(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="2,56.72,685.21,129.57,6.97"><p>https://github.com/grill-lab/Interactive-CAsT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="2,56.84,693.62,135.75,6.97"><p>https://huggingface.co/castorini/t5-base-canard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="2,56.84,702.03,107.56,6.97"><p>https://github.com/castorini/pygaggle</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Paul Owoicho</rs> for his extensive work with all aspects of the track: interactive baseline system development, topic development, and creation of baseline runs. We thank <rs type="person">Shi Yu</rs> for contributing the ConvDR dense retrieval baselines and helping with topic construction. We also are deeply thankful for Ellen Voorhees' experience, patience, and persistence in running the assessment process. Finally, we thank all our participants.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,330.15,231.28,229.23,6.97;7,330.15,239.25,228.05,6.97;7,330.15,247.22,201.84,6.97" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mc-Namara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m" coord="7,444.63,239.25,113.58,6.97;7,330.15,247.22,87.82,6.97">Ms marco: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,330.15,255.19,228.05,6.97;7,329.59,263.16,27.23,6.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,367.46,255.19,161.36,6.97">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.15,271.13,228.24,6.97;7,330.15,279.10,228.06,6.97;7,330.15,287.07,227.99,6.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,422.32,271.13,136.06,6.97;7,330.15,279.10,24.47,6.97">Cast 2019: The conversational assistance track overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,368.66,279.10,189.54,6.97;7,330.15,287.07,11.06,6.97">The Twenty-Eighth Text REtrieval Conference Proceedings (TREC 2019</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.15,295.04,228.05,6.97;7,330.15,303.01,229.23,6.97;7,330.15,310.98,229.13,6.97;7,329.90,318.95,33.84,6.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,451.45,295.04,106.75,6.97;7,330.15,303.01,56.77,6.97">Cast-19: A dataset for conversational information seeking</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,400.92,303.01,158.46,6.97;7,330.15,310.98,182.04,6.97">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1985" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.15,326.92,228.82,6.97;7,329.95,334.89,228.26,6.97;7,330.15,342.86,157.33,6.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,461.52,334.89,96.68,6.97;7,330.15,342.86,68.46,6.97">Kilt: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rocktaschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno>ArXiv abs/2009.02252</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.15,350.83,228.88,6.97;7,330.15,358.80,228.05,6.97;7,329.94,366.77,136.84,6.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,330.15,358.80,172.57,6.97">Leading conversational search by suggesting useful questions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,516.50,358.80,41.70,6.97;7,329.94,366.77,59.77,6.97">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1160" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
