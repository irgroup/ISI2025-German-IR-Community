<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.42,135.46,311.16,9.73">TREC 2021 Podcasts Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,80.26,174.28,81.76,6.76"><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,174.12,174.28,66.95,6.76"><forename type="first">Rosie</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.18,174.28,71.22,6.76"><forename type="first">Ben</forename><surname>Cartere</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.63,174.28,45.13,6.76"><forename type="first">Ann</forename><surname>Cli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,432.90,174.28,90.32,6.76"><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.23,192.22,101.64,6.76"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.98,192.22,88.67,6.76"><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.75,192.22,80.76,6.76"><forename type="first">Edgar</forename><surname>Tanaka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.93,246.01,83.09,6.76"><forename type="first">Eric</forename><surname>Clarin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.42,135.46,311.16,9.73">TREC 2021 Podcasts Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E2FA2B5324AD5C7903D5BE51C9526647</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Podcasts Track is intended to facilitate research in language technologies applied to podcasts specifically by lowering the barrier-to-entry for data-oriented research for podcasts and for diverse spoken documents in general. This year, 2021, is the second year of the track. A more general overview of some of the challenges is given in last year's Podcasts Track Overview. The track this year consisted of two shared tasks: segment retrieval and summarisation, both based on a dataset of over 100,000 podcast episodes (metadata, audio, and automatic transcripts) which was released concurrently with the track. The tasks were slightly elaborated this year to encourage participants to use audio analysis. This paper gives an overview of the tasks and the results of the participants' experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Podcasts Track was launched in 2020 to facilitate research in language technologies applied to podcasts by lowering the barrier-to-entry for data-oriented research for podcasts and for diverse spoken documents in general. A more general overview of some of the starting points is given in the 2021 Podcasts Track Overview <ref type="bibr" coords="1,236.28,575.88,55.54,5.14;1,65.06,589.43,28.81,5.14">(Jones et al., 2021a)</ref> and some of the challenges are detailed in separate publications <ref type="bibr" coords="1,161.69,602.98,82.81,5.14">(Jones et al., 2021b;</ref><ref type="bibr" coords="1,246.43,602.98,45.39,5.14;1,65.06,616.52,48.54,5.14" target="#b2">Cartere e et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Data</head><p>The data distributed by the track organisers consisted of just over 100,000 episodes of Englishlanguage podcasts. Each episode comes with full audio, a transcript which was automatically generated using Google's Speech-to-Text API as of early 2020, and a description and metadata provided by the podcast creator, along with the RSS feed content for the show. The data set is described in greater detail in <ref type="bibr" coords="1,394.28,509.04,83.53,5.14" target="#b3">Cli on et al. (2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Participation</head><p>In 2020, the Podcasts Track a racted a great deal of a ention with more than 200 registrations to participate. Most registrants did not submit experimental runs for assessment. In 2021, the number of participants who have registered for the Podcast track decreased, while the number of submitted runs stayed comparable, cf. Table <ref type="table" coords="2,234.62,193.12,3.74,5.14" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Tasks</head><p>The Podcasts Track o ered two tasks: (1) topical retrieval of fixed two-minute segments and (2) textual summarisation of episodes. In 2020, both tasks were possible to complete on the automatic transcripts of episodes, without using the audio data at all. In 2021, we adjusted the tasks somewhat, without changing the overall task formulation, to nudge participants to make more use of the audio material. The segment retrieval, besides the topical retrieval, requested the participants to also rerank the results in additional sets sorted by the segments being entertaining, subjective, or containing discussion of the topic. In addition the two topic types "known-item" and "refinding" from 2020 were merged to "known-item" since we found no practical reason to keep them separate. The summarisation task added the request to submit an audio clip representative of the episode, with slightly more emphasis given in the instructions on the use case of helping a listener decide whether to listen to the episode or not.</p><p>2 Segment Retrieval Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Previous Work on Retrieval of Spoken Content</head><p>There is longstanding interest in spoken content retrieval. TREC organised the Spoken Document Retrieval Track which ran at TREC in the years 1997-2000 <ref type="bibr" coords="2,90.45,663.48,102.86,5.14" target="#b4">(Garofolo et al., 2000)</ref> and which focussed on broadcast news. CLEF organised the Cross-Language Speech Retrieval (CL-SR) task which ran at CLEF in the years <ref type="bibr" coords="2,175.29,704.12,19.65,5.14">2005</ref><ref type="bibr" coords="2,199.85,704.12,19.65,5.14" target="#b5">-2007</ref><ref type="bibr" coords="2,65.06,717.67,23.56,5.14" target="#b5">(Pecina et al., 2008) )</ref> and which focussed on retrieval from a large archive of oral history. NTCIR organised a spoken content retrieval task in the years 2010-2016, which focussed on search of Japanese language lectures and technical presentations, including using spoken queries <ref type="bibr" coords="2,373.39,131.86,79.73,5.14" target="#b6">(Akiba et al., 2013;</ref><ref type="bibr" coords="2,455.34,131.86,21.64,5.14" target="#b7">2016)</ref>. MediaEval organised the Rich Speech Retrieval and Search and Hyperlinking tasks in the years 2011-2015 which worked with the non-professional video content and the professional broadcast TV material <ref type="bibr" coords="2,513.28,186.05,33.67,5.14;2,320.17,199.60,49.59,5.14" target="#b8">(Larson et al., 2011;</ref><ref type="bibr" coords="2,372.49,199.60,91.41,5.14" target="#b9">Eskevich et al., 2012;</ref><ref type="bibr" coords="2,466.63,199.60,21.64,5.14">2015)</ref>. While none of this existing work has focused on podcast material, the various content archives used raise many of the same issues that can be observed in podcasts in terms of content diversity, use of domain specific vocabularies, and issues relating to potential absence of entity mentions in conversational podcasts. A more complete overview of research in spoken content retrieval from its beginnings in the early 1990s to today can be found in Jones (2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Definition</head><p>The retrieval task was defined as the problem of finding relevant segments from the episodes for a set of search queries which were provided in traditional TREC topic format. Given a retrieval topic (a phrase, sentence or set of words) and a set of ranking criteria, retrieve and rank relevant two-minute segments from the data.</p><p>The provided transcripts have word-level timestamps on a granularity of 0.1s which allows retrieval systems to index the contents by time osets. A segment is defined to be a two-minute chunk starting on the minute; e.g. [0.0-119.9] seconds, [60-179.9] seconds, [120-239.9] seconds, etc. Segments overlap with each other by one minuteany segment except for the first and last segment is covered by the preceding and following segments. The rationale for creating overlapping segments is to account for the case where a phrase or sentence is split across the imposed segment boundaries. This creates 3.4M segments in total from the document collection with an average word count of 340 Â± 70 per segment.</p><p>Topics consist of a topic number, keyword query, a query type, and a description of the user's information need. In 2021, the queries are 40 of type "topical" and 10 of type "known-item". In 2020, TREC 2021 Podcasts Track Overview -Page 2 Podcasts Track Overview there were 35 of type "topical", 8 of type "refinding", and 7 of type "known-item". Eight topics were given at the outset for the participants to practice on, six of type "topical", and one each of "refinding" and "known-item". Example topics are given in Figure <ref type="figure" coords="3,107.59,158.95,3.74,5.14">1</ref>.</p><p>The lists of segments for each topical query are to be submi ed in four separately ranked lists: one ranked list of topically relevant segments, and three reranked lists of those same topically relevant segments. Reranking is not relevant for the knownitem topics, where the objective is to find one specific segment. <ref type="foot" coords="3,126.68,251.26,3.71,3.75" target="#foot_0">1</ref>The reranking criteria are:</p><p>Adhoc topical retrieval (QR): the segment is topically relevant to the topic description.</p><p>Entertaining (QE): the segment is topically relevant to the topic description AND the topic is presented in a way which the speakers intend to be amusing and entertaining to the listener, rather than informative or evaluative.</p><p>Subjective (QS): the segment is topically relevant to the topic description AND the speaker or speakers explicitly and clearly express a polar opinion about the query topic, so that the approval or disapproval of the speaker is evident in the segment.</p><p>Discussion (QD): the segment is topically relevant to the topic description AND includes more than one speaker participating with non-trivial topical contribution (e.g. mere grunts, expressions of agreement, or discourse management cues ("go on", "right", "well, I don't know . . . " etc) are not su icient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submissions</head><p>6 participants submi ed 23 experiments for the retrieval task. For an overview summary of the submission see Table <ref type="table" coords="3,147.23,686.59,3.74,5.14" target="#tab_2">2</ref>. All runs were 'automatic', i.e, without human intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation</head><p>Submi ed two-minute length segments were judged by NIST assessors for their topical relevance to the topic description. Each relevant segment will also be assessed for adherence to the reranking criteria. NIST assessors had access to both the automatically generated transcript (including text before and a er the text of the two-minute segment, which can be used as context) as well as the corresponding audio segment. Assessments were made on the PEGFB graded scale (Perfect, Excellent, Good, Fair, Bad) as approximately follows:</p><p>Perfect (4): this grade is intended to be used only for "known item" topics but was used across the board for all topics. It reflects the segment that is the earliest entry point into the intended segment of the intended episode.</p><p>Excellent (3): the segment conveys highly relevant information, is an ideal entry point for a human listener, and is fully on topic. An example would be a segment that begins at or very close to the start of a discussion on the topic, immediately signalling relevance and context to the user.</p><p>Good (2): the segment conveys highly-tosomewhat relevant information, is a good entry point for a human listener, and is fully to mostly on topic. An example would be a segment that is a few minutes "o " in terms of position, so that while it is relevant to the user's information need, they might have preferred to start two minutes earlier or later.</p><p>Fair (1): the segment conveys somewhat relevant information, but is a sub-par entry point for a human listener and may not be fully on topic. Examples would be segments that switch from non-relevant to relevant (so that the listener is not able to immediately understand the relevance of the segment), segments that start well into a discussion without providing enough context for understanding, etc.  The primary metrics for evaluation are mean nDCG, with normalization based on an ideal ranking of all relevant segments, nDCG at the top thirty retrieved items, and precision at ten retrieved items. Note that a single episode may contribute one or more relevant segments, some of which may be overlapping, but these are treated as independent items for the purpose of nDCG computation.</p><formula xml:id="formula_0" coords="5,71.03,605.89,424.48,63.81">-B Baseline BM25-Q Q BM25 QL-Q Q query likelihood BM25-D D BM25 QL-D D query likelihood</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Search Baselines</head><p>Four baseline segment retrieval runs on transcripts are included using standard information retrieval methods (BM25 and ery Likelihood, both as implemented in the Pyserini package<ref type="foot" coords="6,218.29,304.39,3.71,3.75" target="#foot_1">2</ref> ), each using either the query field only or using both the query and the description fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Relevance Assessment</head><p>Figures <ref type="figure" coords="6,101.21,388.44,5.07,5.14" target="#fig_0">2</ref> and<ref type="figure" coords="6,130.13,388.44,5.07,5.14" target="#fig_1">3</ref> show the number of topically relevant segments per test topic for topical queries. This demonstrates that all topics had some relevant segments retrieved by participants and assessed by assessors.</p><p>eries are classified by somewhat arbitrarily chosen thresholds as "hard" if less than 20 relevant segments are found among the assessed ones and "easy" if 50 or more are found, as shown in Table <ref type="table" coords="6,102.82,496.83,3.74,5.14" target="#tab_3">3</ref>. The queries for 2021 appear to be somewhat more challenging than the 2020 ones. Table <ref type="table" coords="6,286.75,510.38,5.07,5.14" target="#tab_4">4</ref> shows the distribution of number of relevant segments over the relevance scores and Table <ref type="table" coords="6,256.12,537.48,5.07,5.14">6</ref> shows the most "hard" and the most "easy" topic of the 2021 training set.</p><p>The reranking criteria-entertaining, subjective, and discussion-are variously frequent over the topics. Table <ref type="table" coords="6,110.70,605.29,5.07,5.14">5</ref> shows the number of segments on average per query for the topics, and demonstrates that both "hard" and "easy" topics have on average segments of all three types. Examining the topics individually, we find that whereas several topics have no entertaining segments at all, all topics have numerous subjective and discussion segments. Table <ref type="table" coords="6,91.84,700.14,5.07,5.14">6</ref> gives examples of topics with many or few segments assessed to be entertaining, subjective, or discussion. The examples conform to expectation, in that e.g. indeed one might expect many subjective segments for a topic which explicitly asks for argumentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Search Results</head><p>Tables 7-10 give an overview of the scores for the submi ed experiments for each reranking scheme. Scoring only the top 30 items or the top 10 items of the list promotes some reranking approaches to the top of the list, illustrating the e ect of use casemotivated evaluation metrics on system comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summarization Task</head><p>The user task for summarization is to provide a short description of the podcast episode to help the user decide whether to listen to a podcast. This user task is the background for both the assessment of the text snippet and the audio clip. In particular, participants were required to produce, for a given podcast episode:</p><p>1. A short text snippet capturing the most important information in the content of episode, in grammatical u erances of significantly shorter length than the input episode itself.</p><p>2. An audio file of up to one minute duration selected from the podcast to give the user a sense of what the podcast sounds like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Previous Work on Summarization of Spoken Material</head><p>Research in summarization has traditionally focused on text in the news domain (eg <ref type="bibr" coords="7,505.58,339.79,41.36,5.14;7,320.17,353.34,75.54,5.14" target="#b13">Mihalcea and Tarau (2004)</ref>  <ref type="formula" coords="9,266.76,91.21,16.71,5.14">2020</ref>), suggesting that summarization models may also have the ability to recover from noisy input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Data</head><p>No ground truth summaries are provided for training or evaluation. The closest proxies are the show and episode descriptions provided by the podcast creators which are included in the released dataset. As described in the 2020 track overview <ref type="bibr" coords="9,65.06,239.67,82.43,5.14">Jones et al. (2021a)</ref>, these descriptions vary widely in scope, and not all are intended as summaries of the episode. However, most of the submissions using supervised approaches that relied on the creator descriptions as target summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Submissions</head><p>5 participants submi ed 11 experiments (Table <ref type="table" coords="9,276.00,347.48,7.91,5.14" target="#tab_10">11</ref>), in comparison to 22 experiments submi ed by 8 participants in 2020. All the participants used deep learning, and 3 of the 5 submi ed at least one run producing extractive summaries. This is in contrast to 2020, where abstractive summarization dominated -this could be motivated by the additional task to submit a representative audio segment.</p><p>As organizers, we provided one baseline: the transcript of first one minute of the episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation</head><p>NIST assessors evaluated 193 of the episodes. Summaries are judged on a four-step scale, as per the following instructions to the assessors.</p><p>Excellent: the summary accurately conveys all the most important a ributes of the episode, which could include topical content, genre, and participants. In addition to giving an accurate representation of the content, it contains almost no redundant material which is not needed when deciding whether to listen. It is also coherent, comprehensible, and has no grammatical errors.</p><p>Good: the summary conveys most of the most important a ributes and gives the reader a reasonable sense of what the episode contains with li le redundant material which is not needed when deciding whether to listen. Occasional grammatical or coherence errors are acceptable.</p><p>Fair: the summary conveys some a ributes of the content but gives the reader an imperfect or incomplete sense of what the episode contains. It may contain redundant material which is not needed when deciding whether to listen and may contain repetitions or broken sentences.</p><p>Bad: the summary does not convey any of the most important content items of the episode or gives the reader an incorrect or incomprehensible sense of what the episode contains.</p><p>It may contain a large amount of redundant information that is not needed when deciding whether to listen to the episode.</p><p>As in the 2020 task, we devised a set of boolean a ributes that a desirable podcast summary might contain.</p><p>1. names: Does the summary include names of the main people (hosts, guests, characters) involved or mentioned in the podcast?</p><p>2. bio: Does the summary give any additional information about the people mentioned (such as their job titles, biographies, personal background, etc)?</p><p>3. topics: Does the summary include the main topic(s) of the podcast?</p><p>4. format: Does the summary tell you anything about the format of the podcast; e.g. whether it's an interview, whether it's a chat between friends, a monologue, etc? 5. title-context: Does the summary give you more context on the title of the podcast?</p><p>6. redundant: Does the summary not contain redundant information?</p><p>7. english: Is the summary wri en in good English?</p><p>8. sentence: Are the start and end of the summary good sentence and paragraph start and end points?</p><p>Finally, the assessors also gave a binary rating to each submi ed audio segment for whether it conveyed a sense of the sound and feel of the podcast episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summarization Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table</head><p>shows the scores for the 193 assessed episodes. Overall quality scores were significantly lower than in the 2020 task, where the highest mean quality scores were greater than 2.0. Whether that discrepancy is due to the particular test sets, the methods employed, or the annotators is an open question. On the whole, audio segment acceptability scores were high. Consistent with 2020, abstractive systems tended to score higher than extractive ones, though not uniformly so. The first one minute baseline, although simple, proves to be relatively strong.</p><p>As in the 2020 task, all a ributes were found to be significantly correlated with the aggregate quality score (Figure <ref type="figure" coords="10,137.83,437.36,4.17,5.14" target="#fig_2">4</ref>) with 'Does the summary include the main topic(s) of the podcast?' being the most correlated. The audio segment assessment is only weakly correlated with the summary quality. Some episodes proved to be easier to summarize than others, with higher aggregate quality scores across systems. Figure <ref type="figure" coords="10,199.49,717.67,3.74,5.14" target="#fig_3">5</ref>, the distribution of episode-wise aggregate quality scores, shows that summaries on a minority of episodes get consistently high scores.   <ref type="table" coords="18,94.81,485.65,9.17,5.63" target="#tab_2">12</ref>: Overview of manual assessment results from submi ed summarization experiments. A denotes abstractive and E extractive systems. The quality score is aggregated from the EGFB assessments by assigning E=4, G=2, F=1, B=0 and averaging; i.e., the scale of the quality score is from 0 to 4. The audio segment acceptability is a binary assessment where the scale is from 0 to 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,320.17,236.22,226.78,5.63;6,320.17,250.66,226.77,5.63;6,320.17,265.11,226.78,5.63;6,320.17,279.55,226.78,5.63;6,320.17,294.00,103.18,5.63;6,325.56,168.03,215.98,53.71"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of topically relevant segments assessed for both 2020 and 2021 topical queries. Red bars for highly relevant segments (scores 3 and 4); blue bars for less relevant (scores 1 and 2).</figDesc><graphic coords="6,325.56,168.03,215.98,53.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,320.17,401.35,226.78,5.63;6,320.17,415.79,226.78,5.63;6,320.17,430.24,226.77,5.63;6,320.17,444.68,221.31,5.63;6,325.56,304.92,216.00,81.94"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of topically relevant segments assessed for 2021 topical queries. Red bars for highly relevant segments (scores 3 and 4); blue bars for less relevant (scores 1 and 2).</figDesc><graphic coords="6,325.56,304.92,216.00,81.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,65.06,626.18,226.77,5.63;10,65.06,640.63,226.77,5.63;10,65.06,655.07,119.32,5.63;10,65.06,503.18,237.60,108.52"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pearson correlation of a ributes with the aggregate EGFB quality score across all submi ed baseline runs.</figDesc><graphic coords="10,65.06,503.18,237.60,108.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,320.17,466.67,226.77,5.63;10,320.17,481.11,226.77,5.63;10,320.17,495.56,226.77,5.63;10,320.17,510.00,226.77,5.63;10,320.17,524.45,30.33,5.63;10,325.56,315.38,216.00,136.80"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Histogram of EGFB aggregate score averaged across systems per episode. The y axis shows the number of episodes whose average summary quality lies within the given range.</figDesc><graphic coords="10,325.56,315.38,216.00,136.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,326.15,562.60,260.10,169.43"><head>Table 1 :</head><label>1</label><figDesc>Participation statistics</figDesc><table coords="1,326.15,562.60,260.10,142.89"><row><cell>Name</cell><cell cols="2">2020 2021</cell></row><row><cell>Email list sign-ups</cell><cell>285</cell><cell>173</cell></row><row><cell cols="2">Joined TREC slack channel #podcasts-2020 194</cell><cell>67</cell></row><row><cell>Registered for TREC podcasts track</cell><cell>213</cell><cell>42</cell></row><row><cell>Signed data sharing agreement</cell><cell>77</cell><cell>191</cell></row><row><cell>Downloaded transcripts</cell><cell>64</cell><cell>377</cell></row><row><cell>Downloaded audio</cell><cell>18</cell><cell>26</cell></row><row><cell cols="2">Downloaded test audio for summarization N/A</cell><cell></cell></row><row><cell>Participated in Search task</cell><cell>7</cell><cell></cell></row><row><cell>Participated in Summarization task</cell><cell>8</cell><cell></cell></row><row><cell>Participated in Both tasks</cell><cell>2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,176.80,688.48,258.41,81.89"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note coords="5,217.54,688.48,217.67,5.63;5,187.88,764.75,222.35,5.63;6,65.06,46.08,123.66,5.63;6,65.06,89.34,168.17,9.55"><p>Technologies employed for the retrieval task TREC 2021 Podcasts Track Overview -Page 5 Podcasts Track Overview Bad (0): the segment is not relevant.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,185.08,88.88,241.84,129.86"><head>Table 3 :</head><label>3</label><figDesc>Number of hard vs easy queries.</figDesc><table coords="7,346.76,88.88,80.16,5.63"><row><cell>2020 2021 all</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,117.22,237.52,377.56,5.63"><head>Table 4 :</head><label>4</label><figDesc>Number of Relevant segments among those assessed for 2021 topics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,65.06,353.34,481.89,417.04"><head></head><label></label><figDesc>Ponzi scheme or a scam without elaborating on how are not relevant -to be relevant they need to describe the scam and how it works. Many Entertaining 66 how to handle failing a job interview: Any material on how a job interview fails or how a candidate was rejected due to the interview: tips, advice, or personal anecdotes and testimonials are all relevant. If a rejection is not about the interview, even if the segment mentions an interview, it is not relevant. I want to understand what topics others consider to be taboo. To be relevant, the segment must mention that a topic is o limits and be clear about what the topic in question is. A mention that some topics are not on is not su icient. A very general mention that some topics are taboo, e.g. "sexuality" is partially relevant. Few Discussion 95 limericks: I want to hear limericks. Discussion about limericks is not relevant if a limerick is not included in the segment.</figDesc><table coords="7,65.06,353.34,481.89,417.04"><row><cell cols="2">Podcasts Track Overview Podcasts Track Overview</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hard</cell><cell cols="4">86 best drum solo: I want to hear about great drummers and especially</cell></row><row><cell></cell><cell cols="4">their solos. A segment is relevant if it names the drummer appre-</cell></row><row><cell></cell><cell cols="4">ciatively and mentions them playing a solo in some song or in some</cell></row><row><cell></cell><cell>concert.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Easy</cell><cell cols="4">62 descriptions of ponzi schemes and other financial scams: I want</cell></row><row><cell></cell><cell cols="4">to find stories about Ponzi schemes or similar scams. Segments that</cell></row><row><cell cols="5">; Nenkova and McKeown (2011); Hermann et al. (2015)). However, more recently, summarization of other types of content such as dialogues Gliwa et al. (2019) have come into the claim that something (e.g. bitcoin investing) is a Few Entertaining 85 personality disorders: Discussions about any personality disor-</cell></row><row><cell></cell><cell cols="4">forefront, and summarization tasks have moved be-der are relevant. Passing claims that someone (e.g. a criminal, a</cell></row><row><cell></cell><cell cols="4">yond text to encompass transcripts of spoken audio celebrity, the speakers themselves) has a personality disorder with-</cell></row><row><cell></cell><cell cols="4">such as meetings Zhong et al. (2021) and media in-out discussing the disorder itself are not.</cell></row><row><cell>Many Subjective</cell><cell cols="4">terviews Zhu et al. (2021). By and large, current re-67 pros and cons of ubi: I want to find arguments for and against uni-</cell></row><row><cell></cell><cell>versal basic income.</cell><cell cols="3">search relies on human generated transcripts; how-</cell></row><row><cell>Few Subjective</cell><cell cols="4">ever, we believe that automatically generated tran-71 roman empire: I am looking to learn something about the history of</cell></row><row><cell></cell><cell>the Roman Empire</cell><cell cols="3">scripts are a promising input domain. Previous</cell></row><row><cell cols="5">work Spina et al. (2017) has demonstrated that summaries generated using automatically gener-ated transcripts can be comparable in terms of usability to summaries generated using error-free manual transcripts. Modern models like transform-ers have been shown to be e ective at 'correcting' Entertaining Subjective Discussion 457 1081 729 78 taboo topics: Table 6: Topics with many or few segments assessed to be topically relevant, entertaining, subjec-All Many Discussion</cell></row><row><cell>tive, or discussion</cell><cell>average per query</cell><cell>11</cell><cell>27</cell><cell>18</cell></row><row><cell></cell><cell>Hard topics</cell><cell>89</cell><cell>115</cell><cell>99</cell></row><row><cell></cell><cell>average per query</cell><cell>8</cell><cell>10</cell><cell>9</cell></row><row><cell></cell><cell>Easy topics</cell><cell>118</cell><cell>476</cell><cell>291</cell></row><row><cell></cell><cell>average per query</cell><cell>11</cell><cell>43</cell><cell>26</cell></row><row><cell cols="5">Table 5: Number of Entertaining, Subjective, Discussion segments among those assessed for 2021</cell></row><row><cell>topics.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">TREC 2021 Podcasts Track Overview -Page 7 TREC 2021 Podcasts Track Overview -Page 8</cell><cell></cell></row></table><note coords="9,65.06,91.21,201.71,5.14"><p><p>speech recognition errors</p>Hrinchuk et al. (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,90.90,198.82,430.20,571.56"><head>Table 7 :</head><label>7</label><figDesc>Overview of results from submi ed topical (QR) segment retrieval experiments.</figDesc><table coords="13,161.70,198.82,288.61,571.56"><row><cell></cell><cell cols="3">nDCG nDCG at 30 precision at 10</cell></row><row><cell>osc tok vec</cell><cell>0.39</cell><cell>0.35</cell><cell>0.41</cell></row><row><cell>tp mt5 f1</cell><cell>0.51</cell><cell>0.37</cell><cell>0.40</cell></row><row><cell>tp mt5 f2</cell><cell>0.51</cell><cell>0.37</cell><cell>0.40</cell></row><row><cell>tp mt5</cell><cell>0.51</cell><cell>0.37</cell><cell>0.40</cell></row><row><cell>TUW hybrid cat</cell><cell>0.53</cell><cell>0.34</cell><cell>0.39</cell></row><row><cell>TUW hybrid ws</cell><cell>0.53</cell><cell>0.33</cell><cell>0.38</cell></row><row><cell>TUW tasb cat</cell><cell>0.50</cell><cell>0.33</cell><cell>0.39</cell></row><row><cell>osc vec tok</cell><cell>0.35</cell><cell>0.32</cell><cell>0.38</cell></row><row><cell>f b25 tct</cell><cell>0.50</cell><cell>0.31</cell><cell>0.37</cell></row><row><cell>TUW tasb192 ann</cell><cell>0.43</cell><cell>0.30</cell><cell>0.37</cell></row><row><cell>f coil tct</cell><cell>0.48</cell><cell>0.31</cell><cell>0.35</cell></row><row><cell>s tct</cell><cell>0.41</cell><cell>0.29</cell><cell>0.35</cell></row><row><cell>osc vector</cell><cell>0.31</cell><cell>0.28</cell><cell>0.34</cell></row><row><cell>f b25 coil</cell><cell>0.46</cell><cell>0.28</cell><cell>0.33</cell></row><row><cell>s tasb</cell><cell>0.39</cell><cell>0.25</cell><cell>0.32</cell></row><row><cell>osc token</cell><cell>0.34</cell><cell>0.28</cell><cell>0.32</cell></row><row><cell>ms mt5</cell><cell>0.44</cell><cell>0.27</cell><cell>0.28</cell></row><row><cell>UCL audio 2</cell><cell>0.29</cell><cell>0.24</cell><cell>0.28</cell></row><row><cell>UCL audio 1</cell><cell>0.29</cell><cell>0.24</cell><cell>0.28</cell></row><row><cell>Webis pc bs</cell><cell>0.36</cell><cell>0.20</cell><cell>0.25</cell></row><row><cell>Webis pc cola</cell><cell>0.36</cell><cell>0.20</cell><cell>0.25</cell></row><row><cell>Webis pc co rob</cell><cell>0.36</cell><cell>0.20</cell><cell>0.25</cell></row><row><cell>Webis pc rob</cell><cell>0.36</cell><cell>0.20</cell><cell>0.25</cell></row><row><cell>Baseline BM25-D</cell><cell>0.42</cell><cell>0.25</cell><cell>0.29</cell></row><row><cell>Baseline QL-D</cell><cell>0.43</cell><cell>0.25</cell><cell>0.30</cell></row><row><cell>Baseline BM25-Q</cell><cell>0.41</cell><cell>0.24</cell><cell>0.27</cell></row><row><cell>Baseline QL-Q</cell><cell>0.41</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell cols="4">TREC 2021 Podcasts Track Overview -Page 13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="14,78.79,614.23,454.42,156.15"><head>Table 8 :</head><label>8</label><figDesc>Overview of results from submi ed entertaining (QE) segment retrieval experiments.</figDesc><table coords="14,185.10,764.75,227.91,5.63"><row><cell>TREC 2021 Podcasts Track Overview -Page 14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="15,84.53,614.23,442.95,156.15"><head>Table 9 :</head><label>9</label><figDesc>Overview of results from submi ed subjective (QS) segment retrieval experiments.</figDesc><table coords="15,185.10,764.75,227.91,5.63"><row><cell>TREC 2021 Podcasts Track Overview -Page 15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="16,79.82,614.23,452.37,156.15"><head>Table 10 :</head><label>10</label><figDesc>Overview of results from submi ed discussion (QD) segment retrieval experiments.</figDesc><table coords="16,185.10,764.75,227.91,5.63"><row><cell>TREC 2021 Podcasts Track Overview -Page 16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="17,157.38,617.05,297.24,153.33"><head>Table 11 :</head><label>11</label><figDesc>Technologies employed for the summarization task TREC 2021 Podcasts Track Overview -Page 17</figDesc><table coords="18,65.06,46.08,123.66,5.63"><row><cell>Podcasts Track Overview</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,82.99,719.71,463.95,4.69;3,65.06,731.67,416.94,4.69"><p>We made use of the 0 field in the classic TREC retrieval submission format to distinguish between the various reranking schemes. To our knowledge this is the first time that field has been used for anything at all.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,82.99,718.28,463.96,8.30;6,65.06,731.67,133.45,4.69"><p>https://github.com/castorini/pyserini -a Python front end to the Anserini open-source information retrieval toolkit (Yang et al. (2017))</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,65.06,115.71,226.77,5.14;11,76.76,129.26,215.06,5.14;11,76.76,142.81,215.06,5.14;11,76.76,156.36,215.06,5.14;11,76.76,167.88,215.07,9.71;11,76.76,181.43,215.06,9.71;11,76.76,194.98,91.49,9.71" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,266.13,142.81,25.69,5.14;11,76.76,156.36,141.34,5.14;11,113.43,197.01,19.33,5.14">TREC 2020 Podcasts Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Cartere E</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Cli On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,240.43,167.88,51.39,9.71;11,76.76,181.43,215.06,9.71;11,76.76,194.98,29.09,9.71">Proceedings of the Twenty-Ninth Text REtrieval Conference (TREC)</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Angela</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>NIST</note>
</biblStruct>

<biblStruct coords="11,65.06,219.85,226.77,5.14;11,76.76,233.40,215.06,5.14;11,76.76,246.95,215.06,5.14;11,76.76,260.49,215.06,5.14;11,76.76,274.04,215.06,5.14;11,76.76,287.59,215.06,5.14;11,76.76,299.11,215.07,9.71;11,76.76,312.66,215.06,9.71;11,76.76,326.21,215.01,9.71" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,76.76,274.04,215.06,5.14;11,76.76,287.59,215.06,5.14;11,76.76,301.14,75.34,5.14">Hugues Bouchard, and Ben Cartere e. Current Challenges and Future Directions in Podcast Information Access</title>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ching-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Cli On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helia</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zahra</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oguz</forename><surname>Semerci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,174.47,299.11,117.36,9.71;11,76.76,312.66,215.06,9.71;11,76.76,326.21,180.04,9.71">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,65.06,351.08,226.77,5.14;11,76.76,364.63,215.06,5.14;11,76.76,378.18,215.06,5.14;11,76.76,391.73,215.06,5.14;11,76.76,403.25,215.07,9.71;11,76.76,416.79,215.06,9.71;11,76.76,430.34,215.06,9.71;11,76.76,443.89,134.86,9.71" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,256.46,378.18,35.37,5.14;11,76.76,391.73,215.06,5.14;11,76.76,405.27,139.05,5.14">Podcast metadata and content: Episode relevance and a ractiveness in ad hoc search</title>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Ben Cartere E</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Cli On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soboro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,240.43,403.25,51.39,9.71;11,76.76,416.79,215.06,9.71;11,76.76,430.34,215.06,9.71;11,76.76,443.89,25.83,9.71">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2247" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,65.06,468.76,226.77,5.14;11,76.76,482.31,215.06,5.14;11,76.76,495.86,215.06,5.14;11,76.76,509.41,215.06,5.14;11,76.76,522.96,215.06,5.14;11,76.76,534.48,215.06,9.71;11,76.76,548.03,202.44,9.71" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,252.76,509.41,39.07,5.14;11,76.76,522.96,193.98,5.14">000 Podcasts: A Spoken English Document Corpus</title>
		<author>
			<persName coords=""><forename type="first">Ann</forename><surname>Cli On</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongze</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aasish</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rezvaneh</forename><surname>Rezapour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamed</forename><surname>Bonab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosie</forename><surname>Cartere</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,76.76,534.48,215.06,9.71;11,76.76,548.03,171.83,9.71">Proceedings of the 28th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 28th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,65.06,572.89,226.77,5.14;11,76.76,586.44,215.06,5.14;11,76.76,599.99,215.06,5.14;11,76.76,611.51,215.07,9.71;11,76.76,625.06,215.06,9.71;11,76.76,638.23,215.06,7.55;11,76.76,654.19,136.61,5.14" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,176.90,586.44,114.93,5.14;11,76.76,599.99,210.31,5.14">The TREC spoken document retrieval track: A success story (RIAO)</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cedric</forename><forename type="middle">G P</forename><surname>Auzanne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,88.84,611.51,202.99,9.71">Content-Based Multimedia Information Access</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Le Centre de Hautes Ãtudes Internationales d&apos;Informatique Documentaire</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,65.06,677.03,226.77,5.14;11,76.76,690.57,215.06,5.14;11,76.76,704.12,215.06,5.14;11,76.76,715.64,215.06,9.71;11,76.76,729.19,215.06,9.71;11,331.88,89.18,215.06,9.71;11,331.88,102.73,215.07,9.71;11,331.88,118.31,22.69,5.14" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,76.76,704.12,215.06,5.14;11,76.76,717.67,104.54,5.14">Overview of the CLEF-2007 Cross-Language Speech Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petra</forename><surname>Ho MannovÃ¡</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,211.99,715.64,79.83,9.71;11,76.76,729.19,215.06,9.71;11,331.88,89.18,215.06,9.71;11,331.88,102.73,48.39,9.71">Advances in Multilingual and Multimodal Information Retrieval: Eighth Workshop of the Cross-Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2007">2007. 2008</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="11,320.17,141.99,226.77,5.14;11,331.88,155.54,215.06,5.14;11,331.88,169.09,215.06,5.14;11,331.88,182.64,215.06,5.14;11,331.88,194.16,215.06,9.71;11,331.88,207.71,215.06,9.71;11,331.88,221.26,114.04,9.71" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,436.75,182.64,110.19,5.14;11,331.88,196.19,94.03,5.14">Overview of the NTCIR-10 SpokenDoc-2 Task</title>
		<author>
			<persName coords=""><forename type="first">Tomoyosi</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiromitsu</forename><surname>Nishizaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kiyoaki</forename><surname>Aikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinhui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshiaki</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seiichi</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroaki</forename><surname>Nanjo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoichi</forename><surname>Yamashita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,446.37,194.16,100.57,9.71;11,331.88,207.71,215.06,9.71;11,331.88,221.26,84.17,9.71">Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies</title>
		<meeting>the 12th NTCIR Conference on Evaluation of Information Access Technologies</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,320.17,246.97,226.77,5.14;11,331.88,260.52,215.06,5.14;11,331.88,274.07,215.06,5.14;11,331.88,285.59,215.06,9.71;11,331.88,299.14,215.06,9.71;11,331.88,312.69,122.16,9.71;11,471.95,312.69,74.99,9.71;11,331.88,326.24,192.14,9.71" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,490.06,260.52,56.88,5.14;11,331.88,274.07,193.22,5.14">Overview of the NTCIR-12 Spoken ery &amp; Doc-2 Task</title>
		<author>
			<persName coords=""><forename type="first">Tomoyosi</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiromitsu</forename><surname>Nishizaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroaki</forename><surname>Nanjo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,331.88,285.59,215.06,9.71;11,331.88,299.14,215.06,9.71;11,331.88,312.69,122.16,9.71;11,471.95,312.69,74.99,9.71;11,331.88,326.24,162.37,9.71">Proceedings of the 9th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, estion Answering and Cross-Lingual Information Access</title>
		<meeting>the 9th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, estion Answering and Cross-Lingual Information Access</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,320.17,351.95,226.77,5.14;11,331.88,365.50,215.06,5.14;11,331.88,379.05,215.06,5.14;11,331.88,392.60,215.06,5.14;11,331.88,404.12,215.07,9.71;11,331.88,417.67,213.27,9.71" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,442.38,379.05,104.56,5.14;11,331.88,392.60,215.06,5.14;11,331.88,406.15,16.93,5.14">Overview of mediaeval 2011 rich speech retrieval task and genre tagging task</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roeland</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Schmiedeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,366.27,404.12,180.68,9.71;11,331.88,417.67,182.73,9.71">Working Notes Proceedings of the MediaEval 2011 Multimedia Benchmark Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,320.17,443.38,226.77,5.14;11,331.88,456.93,215.06,5.14;11,331.88,470.48,215.06,5.14;11,331.88,482.00,215.07,9.71;11,331.88,495.55,198.01,9.71" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,331.88,470.48,210.52,5.14">Search and hyperlinking task at mediaeval 2012</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shu</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roeland</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,345.24,482.00,201.71,9.71;11,331.88,495.55,167.47,9.71">Working Notes Proceedings of the MediaEval 2012 Multimedia Benchmark Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,320.17,521.26,226.77,5.14;11,331.88,534.81,215.06,5.14;11,331.88,548.36,215.06,5.14;11,331.88,559.88,215.06,9.71;11,331.88,573.43,215.06,9.71;11,331.88,586.98,71.54,9.71" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,331.88,548.36,215.06,5.14;11,331.88,561.91,77.21,5.14">SAVA at Mediaeval 2015: Search and anchoring in video archives</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Eskevich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roeland</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">N</forename><surname>Racca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,438.71,559.88,108.23,9.71;11,331.88,573.43,215.06,9.71;11,331.88,586.98,41.00,9.71">Working Notes Proceedings of the MediaEval 2015 Multimedia Benchmark Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,320.17,612.69,226.77,5.14;11,331.88,624.21,215.06,9.71;11,331.88,637.76,215.06,9.71;11,331.88,651.31,204.77,9.71" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,407.76,612.69,139.18,5.14;11,331.88,624.21,215.06,9.71;11,331.88,637.76,215.06,9.71;11,331.88,651.31,131.62,9.71">About sound and vision: CLEF beyond text retrieval tasks. In Information Retrieval Evaluation in a Changing World -Lessons Learned from 20 Years of CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,320.17,677.02,226.77,5.14;11,331.88,690.57,215.06,5.14;11,331.88,702.09,215.07,9.71;11,331.88,715.64,215.06,9.71;11,331.88,729.19,155.90,9.71" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,488.94,677.02,58.00,5.14;11,331.88,690.57,215.06,5.14;11,331.88,704.12,35.64,5.14">Anserini: Enabling the use of lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,387.62,702.09,159.33,9.71;11,331.88,715.64,215.06,9.71;11,331.88,729.19,126.43,9.71">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.06,91.21,226.77,5.14;12,76.76,102.73,215.07,9.71;12,76.76,116.28,215.06,9.71;12,76.76,129.83,139.98,9.71" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,208.17,91.21,83.65,5.14;12,76.76,104.76,62.38,5.14">Textrank: Bringing order into text</title>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,158.09,102.73,133.73,9.71;12,76.76,116.28,215.06,9.71;12,76.76,129.83,109.09,9.71">Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing (EMNLP). ACL</title>
		<meeting>the 2004 conference on Empirical Methods in Natural Language Processing (EMNLP). ACL</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.06,152.44,226.77,9.71;12,76.76,165.99,185.60,9.71" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,247.14,152.44,44.68,9.71;12,76.76,165.99,62.27,9.71">Automatic summarization</title>
		<author>
			<persName coords=""><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.06,190.64,226.77,5.14;12,76.76,204.19,215.06,5.14;12,76.76,217.73,215.06,5.14;12,76.76,231.28,215.06,5.14;12,76.76,244.83,24.99,5.14" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">TomÃ¡s</forename><surname>KociskÃ½</surname></persName>
		</author>
		<title level="m" coord="12,76.76,231.28,215.06,5.14;12,76.76,244.83,19.99,5.14">Teaching machines to read and comprehend</title>
		<editor>
			<persName><forename type="first">Lasse</forename><surname>Grefenste E</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Will</forename><surname>Espeholt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mustafa</forename><surname>Kay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Phil</forename><surname>Suleyman</surname></persName>
		</editor>
		<editor>
			<persName><surname>Blunsom</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.59,242.80,165.23,9.71;12,76.76,258.38,19.71,5.14;12,131.47,256.82,160.36,9.09;12,76.76,270.37,263.11,9.09" xml:id="b16">
	<monogr>
		<ptr target="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend" />
		<title level="m" coord="12,144.15,242.80,18.94,9.71">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.06,294.55,226.77,5.14;12,76.76,308.10,215.06,5.14;12,76.76,321.64,215.06,5.14;12,76.76,333.16,215.06,9.71" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Gliwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iwona</forename><surname>Mochol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maciej</forename><surname>Biesek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aleksander</forename><surname>Wawer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12237</idno>
		<title level="m" coord="12,168.57,308.10,123.25,5.14;12,76.76,321.64,215.06,5.14;12,76.76,335.19,46.21,5.14">Samsum corpus: A humanannotated dialogue dataset for abstractive summarization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,65.06,357.81,226.77,5.14;12,76.76,371.36,215.06,5.14;12,76.76,384.91,215.06,5.14;12,76.76,398.46,215.06,5.14;12,331.88,91.21,215.06,5.14;12,331.88,102.73,215.07,9.71;12,331.88,116.28,215.07,9.71;12,331.88,131.86,22.69,5.14" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,239.43,398.46,52.40,5.14;12,331.88,91.21,215.06,5.14;12,331.88,104.76,106.44,5.14">QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mutethia</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,460.48,102.73,86.46,9.71;12,331.88,116.28,209.80,9.71">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,320.17,157.49,226.77,5.14;12,331.88,171.04,215.06,5.14;12,331.88,182.56,215.07,9.71;12,331.88,196.11,136.46,9.71" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mediasum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06410</idno>
		<title level="m" coord="12,425.57,171.04,121.37,5.14;12,331.88,184.59,182.03,5.14">A large-scale media interview dataset for dialogue summarization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,320.17,223.78,226.77,5.14;12,331.88,237.33,215.06,5.14;12,331.88,250.88,215.06,5.14;12,331.88,262.40,215.06,9.71;12,331.88,275.95,215.06,9.71;12,331.88,291.52,22.69,5.14" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,482.42,237.33,64.52,5.14;12,331.88,250.88,215.06,5.14;12,331.88,264.43,59.38,5.14">Extracting audio summaries to support e ective spoken document search</title>
		<author>
			<persName coords=""><forename type="first">Damiano</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanne</forename><forename type="middle">R</forename><surname>Trippas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,401.80,262.40,145.15,9.71;12,331.88,275.95,184.24,9.71">Journal of the Association for Information Science and Technology (JASIST)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,320.17,317.16,226.77,5.14;12,331.88,330.71,215.06,5.14;12,331.88,344.26,215.06,5.14;12,331.88,355.78,215.07,9.71;12,331.88,369.33,215.06,9.71;12,331.88,382.88,215.07,9.71;12,331.88,396.44,159.08,9.67" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,366.43,330.71,180.51,5.14;12,331.88,344.26,215.06,5.14;12,331.88,357.81,25.17,5.14">Correction of automatic speech recognition with transformer sequence-to-sequence model</title>
		<author>
			<persName coords=""><forename type="first">Oleksii</forename><surname>Hrinchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariya</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP40776.2020.9053051</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,379.35,355.78,167.60,9.71;12,331.88,369.33,215.06,9.71;12,331.88,382.88,72.55,9.71">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7074" to="7078" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
