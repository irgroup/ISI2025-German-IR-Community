<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,104.61,114.94,402.78,15.12">Overview of the TREC 2021 Health Misinformation Track</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder ref="#_7q9XRM2">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">facilities of Compute Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.53,148.84,105.90,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.31,148.84,73.55,10.48"><forename type="first">Maria</forename><surname>Maistro</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.51,148.84,90.23,10.48"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,104.61,114.94,402.78,15.12">Overview of the TREC 2021 Health Misinformation Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">07FA3076BE59BCA8719995F948B4467E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC 2021 was the third year for the Health Misinformation track, which was named the Decision Track in 2019 <ref type="bibr" coords="1,140.20,284.54,10.91,9.57" target="#b0">[1]</ref>. In 2021, the track had an ad-hoc retrieval task. In each year, the track has used a crawl for its document collection. In 2019 and 2021, we used web crawls, and in 2020, we used a web crawl restricted to news sites.</p><p>By focusing on health-related ad-hoc web search, the track brings new challenges to the web retrieval task. The most striking difference is that for health search, documents containing incorrect information are considered to be harmful and not merely non-relevant. As such, retrieval systems need to actively work to avoid including or ranking this incorrect, harmful information highly in the results. For relevant documents that contain correct information, we prefer sources with higher credibility.</p><p>This year, each topic's description was expressed as a question, for example "Should I apply ice to a burn?". A topic also has a query, for example "put ice on a burn", that represents what a user might enter if they do not ask a full question. All topics concern themselves with determining the efficacy of a treatment for a health issue. Based on a credible source of information, we declare a stance for a topic as either helpful or unhelpful. We provide an evidence URL link to the source we used to determine the stance. Each topic is also supplied with a narrative providing additional clarification to the assessors.</p><p>Automatic runs could only make use of the topic's query or description. If a run used the narrative, stance, or evidence, it had to be considered a manual run. A challenge of health-related search is determining what is correct information, i.e., determining the correct stance for a topic.</p><p>Based on the assessors' judgments, we establish a preference ordering for documents considered to be helpful as well as for documents considered to be harmful. Helpful documents are supportive of helpful treatments or try to dissuade the reader from using unhelpful treatments. Harmful documents encourage use of unhelpful treatments or dissuade the reader from using helpful treatments. Whether a treatment is considered helpful or unhelpful is based on our provided stance.</p><p>Submitted runs are evaluated based on their compatibility <ref type="bibr" coords="1,365.15,609.72,11.52,9.57" target="#b3">[4,</ref><ref type="bibr" coords="1,379.73,609.72,8.49,9.57" target="#b4">5]</ref> with both a preference ordering for helpful documents as well as a preference ordering for harmful documents. The best runs have high compatibility with the helpful preference ordering and low compatibility with the harmful ordering. The preference orderings take into consideration the usefulness, correctness, and credibility of the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topics</head><p>We created 50 topics this year with half of them having a stance of helpful and half with a stance of unhelpful. Figure <ref type="figure" coords="2,171.28,113.41,5.45,9.57" target="#fig_0">1</ref> shows an example of a topic.</p><p>NIST was only able to provide assessments for 35 of the 50 topics. Of these 35 topics, no harmful documents were found for topics 127, 133, and 145. We have excluded these three topics from the analysis in this paper. &lt;topic&gt; &lt;number&gt;105&lt;/number&gt; &lt;query&gt;put ice on a burn&lt;/query&gt; &lt;description&gt;Should I apply ice to a burn?&lt;/description&gt; &lt;narrative&gt;Many people commonly put on ice on burns in an attempt to stop the burning and pain. A very useful document would discuss the effectiveness of using ice to treat burns. A useful document would help a user decide if putting ice on burns is a recommended treatment by providing information on recommended treatments for burns and may not discuss ice as a treatment, or a useful document may discuss benefits or concerns for application of ice to skin.&lt;/narrative&gt; &lt;disclaimer&gt;We do not claim to be providing medical advice, and medical decisions should never be made based on the stance we have chosen. Consult a medical doctor for professional advice.&lt;/disclaimer&gt; &lt;stance&gt;unhelpful&lt;/stance&gt; &lt;evidence&gt;https://www.uwhealth.org/news/the-right-way-to-treat-burns&lt;/evidence&gt; &lt;/topic&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Collection</head><p>This year we used the noclean version of the C4 dataset<ref type="foot" coords="2,339.90,496.09,4.23,6.99" target="#foot_0">1</ref> used by Google to train their T5 model. The collection is comprised of plain text extracted from the April 2019 snapshot of the Common Crawl and contains over 1 billion English web pages. The noclean version of C4 was used rather than the clean version to provide the full text of a web page. We observed many cases where the clean version of C4 removes section headers and important material. The clean version of C4 is designed for training a language model, which is a different purpose than retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>Seven groups submitted 71 runs to the adhoc retrieval task. The UWaterlooMDS group submitted a BM25 baseline run, baselineBM25, which used the topic's query field and default parameters from Anserini (k 1 = 0.9, b = 0.4, Porter stemming, stopword removal). Table <ref type="table" coords="2,438.87,651.61,5.45,9.57" target="#tab_0">1</ref> reports an overview of the participating groups and the number of runs submitted by each group. Next we present a brief summary of the approach adopted by each group. CiTIUS <ref type="bibr" coords="3,135.60,239.49,11.52,9.57" target="#b6">[7]</ref> used BM25 as base ranker and different strategies to perform passage re-ranking of the top 100 documents. Passage re-ranking was performed with respect to the original topic or hand-crafted expressions generated from the topic's fields. RoBERTa was used to represent sentences and compute the similarity of the passages within the top 100 documents and the topic. An additional classifier for passage reliability was trained on data from past editions. Finally, scores from different components were merged with CombSUM or Borda Count.</p><p>DigiLab <ref type="bibr" coords="3,134.93,320.79,16.97,9.57" target="#b11">[12]</ref> implemented a two-step ranking approach that includes a standard retrieval phase, based on the BM25 model, and a re-ranking phase, with a pipeline of models to estimate (1) usefulness, (2) supportiveness, and (3) credibility. The usefulness ranking was generated with a set of transformer-based language models fine-tuned on the MS MARCO corpus. The supportiveness ranking was generated with BERT-based models fine-tuned on scientific and Wikipedia corpora. The credibility ranking was generated with a random forest model trained on the Microsoft Credibility dataset combined with a list of credible sites. The resulting ranked lists were fused with Reciprocal Rank Fusion (RRF).</p><p>h2oloo used Pyserini's default BM25 as base ranker. Re-ranking was performed with a combination of different T5 models (mono and duo) and Vera <ref type="bibr" coords="3,342.43,442.73,16.97,9.57" target="#b9">[10]</ref> with different topic fields.</p><p>UPV <ref type="bibr" coords="3,119.58,456.28,16.97,9.57" target="#b10">[11]</ref> also used Pyserini's default BM25 as base ranker. Usefulness scores were estimated as the similarity between documents represented with Bio Sentence BERT and the topic's description. Credibility was estimated with cosine similarity between documents represented with RoBERTa and a reference document satisfying 4 different credibility criteria. Finally, BM25 scores, usefulness scores, and credibility scores were fused in a single ranking.</p><p>UWaterlooMDS <ref type="bibr" coords="3,180.36,524.03,11.52,9.57" target="#b1">[2]</ref> submitted 19 runs (5 automatic and 14 manual). One of their runs, base-lineBM25 was used by the track organizers as a baseline for the task. Their automatic runs focused on experimenting with creating subcollections of higher quality and then performing retrieval over these subcollections. Their manual runs used 3 different approaches. The first manual approach applied continuous active learning (CAL) to find relevant documents over one of their filtered subcollections. The second approach reranked the output of CAL with RoBERTa, fine tuned on BoolQ dataset, to detect the stance of the document. The final approach used BM25 as the base ranker and then used the T5-large model to re-rank the top 3K results. T5 was fine-tuned on a balanced subset of 2019 qrels to predict the stance of each document, and different strategies were used to fuse BM25 and T5 scores.</p><p>Waterloo Cormack trained a logistic classifier with search results returned by Google or medline BM25. In some cases the term "Pubmed" was added to the topic's query as additional search term. Reciprocal rank fusion (RRF) was used to fuse different combinations of the above.</p><p>Webis <ref type="bibr" coords="3,124.70,700.17,11.52,9.57" target="#b2">[3]</ref> exploited Anserini's BM25 and PyGaggle's default MonoT5 model to create two base-line rankings. Then the top 20 documents of each baseline ranked list were re-ranked according to 3 argumentative axioms with different weighting schemes for queries that seem to be argumentative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Runs were evaluated by using a script<ref type="foot" coords="4,255.13,145.83,4.23,6.99" target="#foot_2">3</ref> to compute the compatibility measure <ref type="bibr" coords="4,450.56,147.78,11.52,9.57" target="#b3">[4,</ref><ref type="bibr" coords="4,465.98,147.78,7.68,9.57" target="#b4">5]</ref>. We derive a qrels file to use with compatibility from the original NIST qrels file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">qrels (query-relevance files)</head><p>NIST used the track's relevance assessing guidelines <ref type="foot" coords="4,326.53,209.19,4.23,6.99" target="#foot_3">4</ref> to generate the track's qrels. The format adopted for NIST qrels file is as follows:</p><p>topic_id 0 doc_id usefulness-judgment supportiveness-judgment credibility-judgment</p><p>where the columns are space separated. Documents were assessed by NIST assessors with respect to 3 criteria, which were recorded in the NIST qrels as follows:</p><p>• Usefulness: does the document contain material that the search user might find useful in answering the topic's question? Usefulness was assessed on 3-point scale: 0 if the document is not useful, 1 if the document is useful, and 2 if the document is very useful. This is column 4 of the qrels file.</p><p>• Supportiveness: does the document contain information that supports/dissuades the use of the treatment in the question? Supportiveness could be assessed to be one of three values: 0 means that the document dissuades the use of the treatment, 1 means that the document neither dissuades or supports the use of the treatment (neutral), and 2 means that the document supports the use of the treatment. This is column 5 of the qrels file.</p><p>• Credibility: how credible is the document? Credibility was assessed on a on 3-point scale: 0 if the document has low credibility, 1 if the document has good credibility, but does not exhibit the highest quality and credibility, and 2 if the document is excellent, i.e., it exhibits the highest quality and most credible information source. This is column 6 of the qrels file.</p><p>Notes:</p><p>• The assessors were not to refer to the topic's stance while judging, and usefulness judgements do not depend on the credibility of the source.</p><p>• Credibility is judged based on the assessor's expert opinion. A set of guidelines was developed to guide assessors in judging credibility (reported in the assessing guidelines).</p><p>• When a document was judged as not useful, it was not judged for its supportiveness nor for its credibility (value -1). In some cases, a useful document was accidentally not judged for its answer or credibility, i.e., a "skip" (value -2).</p><p>• Even by reducing the pool depth to 20, NIST assessors were only able to judge 35 out of the 50 topics due to lack of time. The missing topics are: 113, 116, 119, 123, 124, 125, 126, 130, 135, 138, 141, 142, 147, 148, 150. Possible reasons for the increase of time needed for judgement might be: 1) differently from previous editions <ref type="bibr" coords="5,379.58,116.16,11.52,9.57" target="#b0">[1,</ref><ref type="bibr" coords="5,394.91,116.16,7.68,9.57" target="#b5">6]</ref>, credibility was judged with a 3-point scale instead of a binary label; 2) the documents in the C4 dataset are difficult to read as text extracts, and 3) usefulness was possibly too broadly defined this year. This issue needs to be further investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Derived qrels</head><p>We took the NIST qrels and generated derived qrels for the various evaluation measures. We describe this next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Preference Levels</head><p>For the compatibility measure, we converted the 3 aspects judged for documents (usefulness, supportiveness, and credibility) into a basic preference ordering as reported in Table <ref type="table" coords="5,466.78,283.12,4.24,9.57" target="#tab_1">2</ref>. A document with a higher preference value is preferred to a document with a lower preference value. To define the preference order among tuples of labels we decided to favour credibility over usefulness (assuming the same correctness label). For example, in Table <ref type="table" coords="5,331.73,323.77,4.24,9.57" target="#tab_1">2</ref>, one could consider to rank 11 above 10 or to swap them: both documents are correct, but one is more useful while the other is more credible. Since we favour credibility, the more credible document comes first. Assessors judged useful and very useful documents for their supportiveness towards the health treatment. A document's supportiveness could be judged as supports, neutral, or dissuades. Assessors did not judge the supportiveness or the credibility of not-useful documents.</p><p>A document is correct if it is supportive of helpful treatments or dissuades unhelpful treatments. A document is incorrect if it dissuades from helpful treatments and is supportive of unhelpful treatments. Note that neutral documents are neither correct nor incorrect.</p><p>It is tempting to use the preference values as scores to compute Normalized Discounted Cumulated Gain (nDCG), but that ignores the incorrect information, which is critical to understanding the quality of results. In addition, we do not have a notion of gain here, which is a critical component of nDCG. We only know that we prefer certain documents to other documents. Of particular note, we prefer not-useful documents to incorrect documents.</p><p>We use the preference ordering to create a set of helpful and harmful preference qrels. We create helpful qrels by taking all preference values greater than zero. To create the harmful qrels, we use only the absolute value of the negative scores. Thus, the most harmful documents are those that are judged to be very useful or useful, incorrect, and have excellent credibility.</p><p>With helpful and harmful preference orderings, we can compute a run's compatibility with helpful and harmful documents. A run wants high compatibility with helpful documents and low compatibility with harmful documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Graded and Binary Relevance</head><p>We created a series of qrels files in the standard qrels format for graded and binary relevance effectiveness measures. These files can be used to compute Convex Aggregating Mean (CAM) <ref type="bibr" coords="5,528.48,657.71,11.52,9.57" target="#b7">[8]</ref> and Multidimensional Measure (MM) <ref type="bibr" coords="5,258.61,671.26,10.91,9.57" target="#b8">[9]</ref>. We recommend use of compatibility with helpful and harmful documents as the primary measures. • Binary Usefulness. Same as the above, but usefulness is mapped to binary labels with a lenient mapping: if the document is useful or very useful, then it is mapped to 1; not useful documents are still mapped to 0.</p><p>• Useful and credible. Note that a document cannot be judged credible unless it is judged useful. A document is credible if only judged to have good or high credibility, otherwise it is not credible.</p><p>• Useful and correct. Note that a document cannot be judged correct unless it is judged useful.</p><p>• Useful and correct and credible.</p><p>• Incorrect. A document is incorrect if it is useful and is against the topic's given stance (a neutral document is not incorrect).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Multiple Aspect qrels</head><p>We created three aspect qrels as follows. The correctness column is mapped to 1, if the document's supportiveness aligns with the topic stance, and to 0 otherwise (no distinction for not judged or neutral). The credibility column is the same except that a -1 (not judged) is mapped to 0 (not credible). We also created two aspect qrels but only consider usefulness and one of the other two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Measures</head><p>We evaluate runs by their compatibility with helpful and harmful results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Tables <ref type="table" coords="7,105.69,154.83,5.45,9.57" target="#tab_2">3</ref> and<ref type="table" coords="7,134.77,154.83,5.45,9.57" target="#tab_3">4</ref> report the results for automatic and manual runs. Figure <ref type="figure" coords="7,415.63,154.83,5.45,9.57" target="#fig_1">2</ref> plots runs' compatibility with helpful and harmful results. For two runs with the same level of compatibility with helpful results, the run with the lower compatibility with harmful results is to be preferred. Thus the vera mdt5 0.5 and vera mt5 0.5 runs are notable for having high compatibility with helpful and low compatibility with harmful results. The baselineBM25 run had a helpful compatibility of 0.122 and a harmful compatibility of 0.144. It seems that when no effort is made to prefer correct documents over incorrect documents, search results can be a mix of both, which can have negative consequences for people looking to search engines to help them make a decision about the efficacy of a health treatment.</p><p>The best scoring automatic runs (mt5, all use sup cre, WatSAE-BM25) have helpful compatibility ranging from 0.137 to 0.195 and harmful compatibility from 0.095 to 0.153. In comparison to the BM25 baseline, the mt5 run boosted its compatibility with helpful results, but mt5 also increased its compatibility with harmful results. Both all use sup cre and WatSAE-BM25 were able to boost helpful compatibility and reduce harmful compatibility in comparison to baselineBM25. The mt5 run is a T5 (MedMonoT5) reranking of BM25 results. The all use sup cre run used a fusion of runs produced using separate models for usefulness, supportiveness, and credibility. The WatSAE-BM25 run is BM25 run over a curated collection that aims to contain credible health documents.</p><p>In comparison to the automatic runs, the best manual runs (vera mdt5 0.5 and vera mt5 0.5) have a helpful compatibility of around 0.298 and a low compatibility with harmful results of around 0.040. These runs are produced by first manually reformulating the description field to align with the topic's stance and then reranking BM25 results using T5 (mono-duo-T5 and mono-T5, respectively) along with a reranking based on the stance (Vera). Thus, we may be able to infer that the vera mt5 0.5's performance gains over mt5 come from the ability to promote correct results and demote incorrect results.</p><p>The WatSMC-Correct run had human assessors interactively search and use a continuous active learning (CAL) tool for finding correct documents, which they placed at their top ranks. While this run did not consider credibility in its ranking of documents, it represents a reasonable standard of what humans can achieve in a limited time. As such, the vera mdt5 0.5 and vera mt5 0.5 runs are impressive in their ability to exceed the performance of reasonable human effort based on our current evaluation. Unknown is what human performance would be if a human generated ranking also ordered documents based on credibility in addition to correctness. For a given level of helpfulness, a run with less harm is to be preferred. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,117.58,425.41,376.85,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a topic for the TREC 2021 Health Misinformation track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,72.00,632.11,468.00,9.57;8,117.18,645.66,340.64,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Compatibility of runs with helpful and harmful results. A good run is helpful and notFor a given level of helpfulness, a run with less harm is to be preferred.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,79.05,74.03,453.91,137.33"><head>Table 1 :</head><label>1</label><figDesc>Overview of the groups participating in the TREC Health Misinformation track 2021.</figDesc><table coords="3,112.02,101.47,384.32,109.89"><row><cell>Group Name</cell><cell>Organization</cell><cell># Submitted Runs</cell></row><row><cell>CiTIUS</cell><cell cols="2">University of Santiago de Compostela 10</cell></row><row><cell>DigiLab</cell><cell>University of Geneva</cell><cell>7</cell></row><row><cell>h2oloo</cell><cell>University of Waterloo (Lin)</cell><cell>10</cell></row><row><cell>UPV</cell><cell>Valencia Polytechnic University</cell><cell>10</cell></row><row><cell>UWaterlooMDS 2</cell><cell>University of Waterloo (Smucker)</cell><cell>19</cell></row><row><cell cols="2">Waterloo Cormack University of Waterloo (Cormack)</cell><cell>9</cell></row><row><cell>Webis</cell><cell>Bauhaus University, Weimar</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,74.03,468.00,332.67"><head>Table 2 :</head><label>2</label><figDesc>Preference ordering for documents.</figDesc><table coords="6,72.00,102.01,457.53,253.01"><row><cell>Preference Value</cell><cell>Usefulness</cell><cell>Correctness</cell><cell>Credibility</cell></row><row><cell>12</cell><cell>Very Useful</cell><cell>Correct</cell><cell>Excellent</cell></row><row><cell>11</cell><cell>Useful</cell><cell>Correct</cell><cell>Excellent</cell></row><row><cell>10</cell><cell>Very Useful</cell><cell>Correct</cell><cell>Good</cell></row><row><cell>9</cell><cell>Useful</cell><cell>Correct</cell><cell>Good</cell></row><row><cell>8</cell><cell>Very Useful</cell><cell>Correct</cell><cell>Low or Not Judged</cell></row><row><cell>7</cell><cell>Useful</cell><cell>Correct</cell><cell>Low or Not Judged</cell></row><row><cell>6</cell><cell>Very Useful</cell><cell>Neutral or Not Judged</cell><cell>Excellent</cell></row><row><cell>5</cell><cell>Useful</cell><cell>Neutral or Not Judged</cell><cell>Excellent</cell></row><row><cell>4</cell><cell>Very Useful</cell><cell>Neutral or Not Judged</cell><cell>Good</cell></row><row><cell>3</cell><cell>Useful</cell><cell>Neutral or Not Judged</cell><cell>Good</cell></row><row><cell>2</cell><cell>Very Useful</cell><cell cols="2">Neutral or Not Judged Low or Not Judged</cell></row><row><cell>1</cell><cell>Useful</cell><cell cols="2">Neutral or Not Judged Low or Not Judged</cell></row><row><cell>0</cell><cell>Not Useful</cell><cell>Not Judged</cell><cell>Not Judged</cell></row><row><cell>-1</cell><cell>Very Useful or Useful</cell><cell>Incorrect</cell><cell>Low or Not Judged</cell></row><row><cell>-2</cell><cell>Very Useful or Useful</cell><cell>Incorrect</cell><cell>Good</cell></row><row><cell>-3</cell><cell>Very Useful or Useful</cell><cell>Incorrect</cell><cell>Excellent</cell></row></table><note coords="6,88.40,382.85,451.60,10.91;6,99.27,397.13,249.13,9.57"><p>• Usefulness. Ignores answer correctness and document credibility. Obtained from NIST qrels by dropping supportiveness and credibility columns.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,118.13,122.60,375.75,534.87"><head>Table 3 :</head><label>3</label><figDesc>Automatic run results.</figDesc><table coords="9,394.60,122.60,84.12,8.74"><row><cell>Avg. Compatibility</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,235.15,582.64,136.85,9.57"><head>Table 4 :</head><label>4</label><figDesc>Manual run results. expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.59,700.43,197.71,7.47"><p>https://huggingface.co/datasets/allenai/c4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.59,710.74,104.32,7.86"><p>Includes one baseline run.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,88.59,658.88,245.78,7.47"><p>https://github.com/trec-health-misinfo/Compatibility</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,88.59,669.84,451.41,7.47;4,72.00,680.79,98.04,7.47"><p>https://trec-health-misinfo.github.io/docs/TREC-2021-Health-Misinformation-Track-Assessing-Gui delines Version-2.pdf</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>Thanks to <rs type="person">Mustafa Abualsaud</rs>, <rs type="person">Irene XiangYi Chen</rs>, <rs type="person">Kamyar Ghajar</rs>, <rs type="person">Linh Nhi Phan Minh</rs>, <rs type="person">Amir Vakili Tahami</rs>, and <rs type="person">Dake Zhang</rs> for their contributions to the running of the track.</p><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, in part by <rs type="funder">Google</rs>, in part by the <rs type="funder">facilities of Compute Canada</rs>, and in part by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Sk lodowska-Curie</rs> grant agreement No. <rs type="grantNumber">893667</rs>. Any opinions, findings and conclusions or recommendations</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7q9XRM2">
					<idno type="grant-number">893667</idno>
					<orgName type="grant-name">Marie Sk lodowska-Curie</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,94.43,147.78,445.58,9.57;11,94.42,161.33,445.58,9.57;11,94.42,174.88,445.58,9.57;11,94.42,188.43,241.13,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,430.74,147.78,109.26,9.57;11,94.42,161.33,92.36,9.57">Overview of the TREC 2019 Decision Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maistro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,382.20,161.33,157.81,9.57;11,94.42,174.88,170.77,9.57;11,94.42,188.43,115.48,9.57">The Twenty-Eigth Text REtrieval Conference Proceedings (TREC 2019</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 1250</note>
</biblStruct>

<biblStruct coords="11,94.43,210.95,445.58,9.57;11,94.42,224.49,445.57,9.57;11,94.42,238.04,445.57,9.57;11,94.42,251.59,445.58,9.57;11,94.42,265.14,24.85,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,149.62,224.49,313.57,9.57">UWaterlooMDS at the TREC 2021 Health Misinformation Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ghajar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">N L</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V</forename><surname>Tahami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno>Special Publication 500-335</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,195.71,238.04,290.13,9.57">The Thirtieth REtrieval Conference Proceedings (TREC 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.43,287.66,445.58,9.57;11,94.42,301.21,445.58,9.57;11,94.42,314.76,445.58,9.57;11,94.42,328.30,445.58,9.57;11,94.42,341.85,255.67,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,284.26,301.21,255.75,9.57;11,94.42,314.76,149.98,9.57">Webis at TREC 2021: Deep Learning, Health Misinformation, and Podcasts Tracks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gohsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schwerter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno>Special Publication 500-335</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,425.64,314.76,114.36,9.57;11,94.42,328.30,170.77,9.57">The Thirtieth REtrieval Conference Proceedings (TREC 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.43,364.37,445.58,9.57;11,94.42,377.92,445.58,9.57;11,94.42,391.47,445.57,9.57;11,94.42,405.02,351.62,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,337.24,364.37,202.76,9.57;11,94.42,377.92,94.49,9.57">Offline Evaluation by Maximum Similarity to an Ideal Ranking</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vtyurina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,133.50,391.47,406.50,9.57;11,94.42,405.02,128.08,9.57">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management (CIKM 2020)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>D'aquin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Dietze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Curry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cudré-Mauroux</surname></persName>
		</editor>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management (CIKM 2020)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.43,427.53,445.58,9.57;11,94.42,441.08,445.58,9.57;11,94.42,454.63,445.58,9.57;11,94.42,468.18,214.94,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,359.09,427.53,156.27,9.57">Offline Evaluation without Gain</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vtyurina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,472.00,441.08,68.00,9.57;11,94.42,454.63,440.61,9.57">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR 2020)</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Berberich</surname></persName>
		</editor>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR 2020)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.43,490.70,445.57,9.57;11,94.42,504.25,445.58,9.57;11,94.42,517.79,445.57,9.57;11,94.42,531.34,282.33,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,406.40,490.70,133.60,9.57;11,94.42,504.25,101.82,9.57">Overview of the TREC 2020 Misinformation Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,403.21,504.25,136.79,9.57;11,94.42,517.79,206.35,9.57;11,135.63,531.34,115.49,9.57">The Twenty-Ninth Text REtrieval Conference Proceedings (TREC 2020</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication 1266</note>
</biblStruct>

<biblStruct coords="11,94.43,553.86,445.58,9.57;11,94.42,567.41,445.58,9.57;11,94.42,580.96,445.57,9.57;11,94.42,594.51,375.36,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,503.03,553.86,36.97,9.57;11,94.42,567.41,236.01,9.57">CiTIUS at the TREC 2021 Health Misinformation Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fernandez-Pichel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Prada-Corral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Pichel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gamallo</surname></persName>
		</author>
		<idno>Special Publication 500-335</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,521.60,567.41,18.40,9.57;11,94.42,580.96,276.47,9.57">The Thirtieth REtrieval Conference Proceedings (TREC 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.42,617.02,445.57,9.57;11,94.42,630.57,445.57,9.57;11,94.42,644.12,445.58,9.57;11,94.42,657.67,277.80,9.57" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,298.47,617.02,241.53,9.57;11,94.42,630.57,75.12,9.57">Evaluation Measures for Relevance and Credibility in Ranked Lists</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,94.42,644.12,445.58,9.57">Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<meeting>the ACM SIGIR International Conference on Theory of Information Retrieval<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,94.42,680.19,445.58,9.57;11,94.42,693.73,445.57,9.57;12,94.42,75.51,445.58,9.57;12,94.42,89.06,445.58,9.57;12,94.42,102.61,297.82,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,287.74,680.19,252.26,9.57;11,94.42,693.73,110.73,9.57">MM: A new Framework for Multidimensional Evaluation of Search Engines</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">W</forename><surname>Paton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Labrinidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,519.08,75.51,20.92,9.57;12,94.42,89.06,445.58,9.57;12,94.42,102.61,63.37,9.57">Proceedings of the 27th ACM International Conference on Information &amp; Knowledge Management (CIKM 2018)</title>
		<meeting>the 27th ACM International Conference on Information &amp; Knowledge Management (CIKM 2018)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1699" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,94.42,125.12,445.57,9.57;12,94.42,138.67,445.57,9.57;12,94.42,152.22,445.58,9.57;12,94.42,165.77,445.58,9.57;12,94.42,179.32,53.94,9.57" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,332.08,125.12,207.92,9.57;12,94.42,138.67,245.85,9.57">Vera: Prediction Techniques for Reducing Harmful Misinformation in Consumer Health Search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,248.98,152.22,291.02,9.57;12,94.42,165.77,271.26,9.57">The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Diaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Suel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2066" to="2070" />
		</imprint>
	</monogr>
	<note>SIGIR 2021)</note>
</biblStruct>

<biblStruct coords="12,94.42,201.83,445.57,9.57;12,94.42,215.38,445.58,9.57;12,94.42,228.93,445.57,9.57;12,94.42,242.48,375.36,9.57" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,358.64,201.83,181.36,9.57;12,94.42,215.38,272.28,9.57">UPV at TREC Health Misinformation Track 2021 Ranking with SBERT and Quality Estimators</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Schlicht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Magnossao De Paula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<idno>Special Publication 500-335</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,94.42,228.93,286.42,9.57">The Thirtieth REtrieval Conference Proceedings (TREC 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,94.42,265.00,445.57,9.57;12,94.42,278.55,445.58,9.57;12,94.42,292.10,445.57,9.57;12,94.42,305.65,445.57,9.57;12,94.42,319.19,117.03,9.57" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,373.10,265.00,166.90,9.57;12,94.42,278.55,440.70,9.57">DS4DH at TREC Health Misinformation 2021: Multi-Dimensional Ranking Models with Transfer Learning and Rank Fusion</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jaume-Santero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teodoro</surname></persName>
		</author>
		<idno>Special Publication 500-335</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.95,292.10,271.05,9.57;12,94.42,305.65,19.86,9.57">The Thirtieth REtrieval Conference Proceedings (TREC 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ellis</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
