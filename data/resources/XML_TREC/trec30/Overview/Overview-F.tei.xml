<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.71,112.05,336.58,15.12">Overview of the TREC 2021 Fair Ranking Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-14">February 14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.05,144.34,106.39,10.68"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
							<email>michaelekstrand@boisestate.edu</email>
						</author>
						<author>
							<persName coords="1,363.61,144.34,98.43,10.68"><forename type="first">Graham</forename><surname>Mcdonald</surname></persName>
							<email>graham.mcdonald@glasgow.ac.uk</email>
						</author>
						<author>
							<persName coords="1,365.31,178.09,71.49,10.68"><forename type="first">Isaac</forename><surname>Johnson</surname></persName>
						</author>
						<title level="a" type="main" coord="1,137.71,112.05,336.58,15.12">Overview of the TREC 2021 Fair Ranking Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-14">February 14, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">D171DC7805F5454C964558B6358F474E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors.</p><p>The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. <ref type="bibr" coords="1,146.30,351.93,3.97,6.16" target="#b0">1</ref> WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups [3, 5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>The 2021 Fair Ranking Track used an ad hoc retrieval protocol. Participants were provided with a corpus of documents (a subset of the English language Wikipedia) and a set of queries. A query was of the form of a short list of search terms that represent a WikiProject. Each document in the corpus was relevant to zero to many WikiProjects and associated with zero to many fairness categories.</p><p>There were two tasks in the 2021 Fair Ranking Track. In each of the tasks, for a given query, participants were to produce document rankings that are:</p><p>1. Relevant to a particular WikiProject.</p><p>2. Provide a fair exposure to articles that are associated to particular protected attributes.</p><p>The tasks shared a topic set, the corpus, the basic problem structure and the fairness objective. However, they differed in their target user persona, system output (static ranking vs. sequences of rankings) and evaluation metrics. The common problem setup was as follows:</p><p>• Queries were provided by the organizers and derived from the topics of existing or hypothetical WikiProjects.</p><p>• Documents were Wikipedia articles that may or may not be relevant to any particular WikiProject that is represented by a query.</p><p>• Rankings were ranked lists of articles for editors to consider working on.</p><p>• Fairness of exposure was achieved with respect to the geographic location of the articles (geographic location annotations were provided). For the evaluation topics, in addition to geographic fairness, to the extent that biographical articles are relevant to the topic, the rankings should have also been fair with respect to an undisclosed demographic attribute of the people that the biographies cover, which was gender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task 1: WikiProject Coordinators</head><p>The first task focused on WikiProject coordinators as users of the search system; their goal is to search for relevant articles and produce a ranked list of articles needing work that other editors can then consult when looking for work to do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>The output for this task was a single ranking per query, consisting of 1000 articles.</p><p>Evaluation was a multi-objective assessment of rankings by the following two criteria:</p><p>• Relevance to a WikiProject topic. Relevance assessments were provided for articles for the training queries derived from existing Wikipedia data; evaluation query relevance were assessed by NIST assessors. Ranking relevance was computed with nDCG, using binary relevance and logarithmic decay.</p><p>• Fairness with respect to the exposure of different fairness categories in the articles returned in response to a query. Section 4.2 contains details on the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 2: Wikipedia Editors</head><p>The second task focused on individual Wikipedia editors looking for work associated with a project. The conceptual model is that rather than maintaining a fixed work list as in Task 1, a WikiProject coordinator would create a saved search, and when an editor looks for work they re-run the search. This means that different editors may receive different rankings for the same query, and differences in these rankings may be leveraged for providing fairness.</p><p>Output: The output of this task is 100 rankings per query, each consisting of 50 articles.</p><p>Evaluation was a multi-objective assessment of rankings by the following three criteria:</p><p>• Relevance to a WikiProject topic. Relevance assessments were provided for articles for the training queries derived from existing Wikipedia data; evaluation query relevance was assessed by NIST assessors. Ranking relevance was computed with nDCG.</p><p>• Work needed on the article (articles needing more work preferred). We provided the output of an article quality assessment tool for each article in the corpus; for the purposes of this track, we assumed lower-quality articles need more work.</p><p>• Fairness with respect to the exposure of different fairness categories in the articles returned in response to a query.</p><p>The goal of this task was not to be fair to work-needed levels; rather, we consider work-needed and topical relevance to be two components of a multi-objective notion of relevance, so that between two documents with the same topical relevance, the one with more work needed is more relevant to the query in the context of looking for articles to improve.</p><p>This task used expected exposure to compare the exposure article subjects receive in result rankings to the ideal (or target) exposure they would receive based on their relevance and work-needed [1]. This addresses fundamental limits in the ability to provide fair exposure in a single ranking by examining the exposure over multiple rankings.</p><p>For each query, participants provided 100 rankings, which we considered to be samples from the distribution realized by a stochastic ranking policy (given a query q, a distribution π q over truncated permutations of the documents). Note that this is how we interpret the queries, but it did not mean that a stochastic policy is how the system should have been implemented -other implementation designs were certainly possible. The objective was to provide equitable exposure to documents of comparable relevance and work-needed, aggregated by protected attribute. Section 4.3 has details on the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>This section provides details of the format of the test collection, topics and ground truth. Further details about data generation and limitations can be found in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Obtaining the Data</head><p>The corpus and query data set is distributed via Globus, and can be obtained in two ways. First, it can be obtained via Globus, from our repository at https://boi.st/TREC2021Globus. From this site, you can log in using your institution's Globus account or your own Google account, and synchronize it to your local Globus install or download it with Globus Connect Personal. <ref type="bibr" coords="3,333.04,377.82,3.97,6.16" target="#b1">2</ref> This method has robust support for restarting downloads and dealing with intermittent connections. Second, it can be downloaded directly via HTTP from: https://data.boisestate.edu/library/Ekstrand-2021/TRECFairRanking2021/.</p><p>The runs and evaluation qrels will be made available in the ordinary TREC archives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Corpus</head><p>The corpus consisted of articles from English Wikipedia. We removed all redirect articles, but left the wikitext (markup Wikipedia uses to describe formatting) intact. This was provided as a JSON file, with one record per line, and compressed with gzip (trec corpus.json.gz). Each record contains the following fields:</p><p>id The unique numeric Wikipedia article identifier.</p><p>title The article title.</p><p>url The article URL, to comply with Wikipedia licensing attribution requirements.</p><p>text The full article text.</p><p>The contents of this corpus were prepared in accordance with, and licensed under, the CC BY-SA 3.0 license. <ref type="bibr" coords="3,103.05,611.40,3.97,6.16" target="#b2">3</ref> The raw Wikipedia dump files used to produce this corpus are available in the source directory; this is primarily for archival purposes, because Wikipedia does not publish dumps indefinitely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Topics</head><p>Each of the track's training topics is based on a single Wikiproject. The topic is also GZIP-compressed JSON lines (file trec topics.json.gz), with each record containing: id A query identifier (int) title The Wikiproject title (string) keywords A collection of search keywords forming the query text (list of str) scope A textual description of the project scope, from its project page (string) homepage The URL for the Wikiproject. This is provided for attribution and not expected to be used by your system as it will not be present in the evaluation data (string)</p><p>rel docs A list of the page IDs of relevant pages (list of int)</p><p>The keywords are the primary query text. The scope is there to provide some additional context and potentially support techniques for refining system queries.</p><p>In addition to topical relevance, for Task 2: Wikipedia Editors (Section 2.2), participants were also expected to return relevant documents that need more editing work done more highly than relevant documents that need less work done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Annotations</head><p>NIST assessors annotated the retrieved documents with binary relevance score for given topics. We provided additional options like unassessable and skip if the document-topic pair is difficult to assess or the assessor is not familiar with the topic. The annotations are incomplete, for reasons including:</p><p>• Task 2 requires sequence of rankings which results a large number of dataset, thus it was not possible to annotate all the retrieved documents.</p><p>• Some documents were not complete and did not have enough information to match with the topic.</p><p>We obtained assessments through tiered pooling, with the goal of having assessments for a coherent subset of rankings that are as complete as possible. We have assessments for the following tiers:</p><p>• The first 20 items of all rankings for Task 1 (all queries).</p><p>• The first 5 items of the first 25 rankings from every submission to Task 2 (about 75% of the queries).</p><p>Details are included with the annotations and metric code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Metadata and Fairness Categories</head><p>For training data, participants were provided with a geographical fairness ground truth. For the evaluation data, submitted systems were evaluated on how fair their rankings are to the geographical fairness category and an undisclosed personal demographic attribute (gender).</p><p>We also provided a simple Wikimedia quality score (a float between 0 and 1 where 0 is no content on the page and 1 is high quality) for optimizing for work-needed in Task 2. Work-needed was operationalized as the reverse-i.e. 1 minus this quality score. The discretized quality scores were used as work-needed for final system evaluation.</p><p>This data was provided together in a metadata file (trec metadata.json.gz), in which each line is the metadata for one article represented as a JSON record with the following keys: geographic locations Continents that are associated with the article topic. Zero or many of: Africa, Antarctica, Asia, Europe, Latin America and the Caribbean, Northern America, Oceania (list of string)</p><p>gender For articles with a gender, the gender of the article's subject, obtained from WikiData.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Output</head><p>For Task 1, participants outputted results in rank order in a tab-separated file with two columns: </p><formula xml:id="formula_0" coords="5,72.00,258.89,9.55,8.77">id</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Metrics</head><p>Each task was evaluated with its own metric designed for that task setting. The goal of these metrics was to measure the extent to which a system (1) exposed relevant documents, and (2) exposed those documents in a way that is fair to article topic groups, defined by location (continent) and (when relevant) the gender of the article's subject. This faces a problem in that Wikipedia itself has well-documented biases: if we target the current group distribution within Wikipedia, we will reward systems that simply reproduce Wikipedia's existing biases instead of promoting social equity. However, if we simply target equal exposure for groups, we would ignore potential real disparities in topical relevance. Due to the biases in Wikipedia's coverage, and the inability to retrieve documents that don't exist to fill in coverage gaps, there is not good empirical data on what the distribution for any particular topic should be if systemic biases did not exist in either Wikipedia or society (the "world as it could and should be" [2]). Therefore, in this track we adopted a compromise: we averaged the empirical distribution of groups among relevant documents with the world population (for location) or equality (for gender) to derive the target group distribution.</p><p>Code to implement the metrics is found at https://github.com/fair-trec/trec2021-fair-public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries</head><p>The tasks were to retrieve documents d from a corpus D that are relevant to a query q. r q ∈ [0, 1] |D| is a vector of relevance judgements for query q. We denote a ranked list by L; L i is the document at position i (starting from 1), and L -1 d is the rank of document d. For Task 1, each system returned a single ranked list; for Task 2, it returned a sequence of rankings L.</p><p>We represented the group alignment of a document d with an alignment vector a d ∈ [0, 1] |G| . a dg is document d's alignment with group g. A ∈ [0, 1] |D|×|G| is the alignment matrix for all documents. a world denotes the distribution of the world. <ref type="bibr" coords="6,234.89,97.45,3.97,6.16" target="#b3">4</ref> We considered fairness with respect to two group sets, G geo and G gender . We operationalized this intersectional objective by letting G = G geo × G gender , the Cartesian product of the two group sets. Further, alignment under either group set may be unknown; we represented this case by treating "unknown" as its own group (g ? ) in each set. In the product set, a document's alignment may be unknown for either or both groups.</p><p>In all metrics, we use log discounting to compute attention weights:</p><formula xml:id="formula_1" coords="6,264.95,189.36,80.90,24.83">v i = 1 log 2 max(i, 2)</formula><p>Task 2 also considered the work each document needs, represented by w d ∈ {1, 2, 3, 4}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 1: WikiProject Coordinators (Single Rankings)</head><p>For the single-ranking Task 1, we adopted attention-weighted rank fairness (AWRF), first described by Sapiezynski et al.</p><p>[6] and named by Raj et al. [4]. AWRF computes a vector d L of the cumulated exposure a list gives to each group, and a target vector d * q ; we then compared these with the Jenson-Shannon divergence:</p><formula xml:id="formula_2" coords="6,143.09,323.93,396.91,88.20">d L = i v i a Li cumulated attention d L = d L d L 1 normalize to a distribution d * q = 1 2 A T r q + a world AWRF(L) = 1 -d JS (d L , d * q )<label>(1)</label></formula><p>For Task 1, we ignored documents that are fully unknown for the purposes of computing d L and d * q ; they do not contribute exposure to any group.</p><p>The resulting metric is in the range [0, 1], with 1 representing a maximally-fair ranking (the distance from the target distribution is minimized). We combined it with an ordinary nDCG metric for utility:</p><formula xml:id="formula_3" coords="6,234.14,489.68,305.86,35.71">NDCG(L) = i v i r qd ideal (2) M 1 (L) = AWRF(L) × NDCG(L)<label>(3)</label></formula><p>To score well on the final metric M 1 , a run must be both accurate and fair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Task 2: Wikipedia Editors (Multiple Rankings)</head><p>For Task 2, we used Expected Exposure [1] to compare the exposure each group receives in the sequence of rankings to the exposure it would receive in a sequence of rankings drawn from an ideal policy with the following properties:</p><p>• Relevant documents come before irrelevant documents</p><p>• Relevant documents are sorted in nonincreasing order of work needed</p><p>• Within each work-needed bin of relevant documents, group exposure is fairly distributed according to the average of the distribution of relevant documents and the distribution of global population (the same average target as before).</p><p>We have encountered some confusion about whether this task is requiring fairness towards work-needed; as we have designed the metric, work-needed is considered to be a part of (graded) relevance: a document is more relevant if it is relevant to the topic and needs significant work. In the Expected Exposure framework, this combined relevance is used to derive the target policies.</p><p>To apply expected exposure, we first define the exposure d a document d receives in sequence L:</p><formula xml:id="formula_4" coords="7,270.63,184.79,269.37,27.47">d = 1 |L| L∈L w L -1 d (4)</formula><p>This forms an exposure vector ∈ R |D| . It is aggregated into a group exposure vector γ, including "unknown" as a group:</p><formula xml:id="formula_5" coords="7,286.27,253.66,253.73,11.41">γ = A T<label>(5)</label></formula><p>Our implementation rearranges the mean and aggregate operations, but the result is mathematically equivalent.</p><p>We then compare these system exposures with the target exposures * for each query. This starts with the per-document ideal exposure; if m w is the number of relevant documents with work-needed level w ∈ {1, 2, 3, 4}, then according to Diaz et al. [1] the ideal exposure for document d is computed as:</p><formula xml:id="formula_6" coords="7,261.94,338.97,278.07,34.72">* d = 1 m w d m ≥w d i=m&gt;w d +1 v i (6)</formula><p>We use this to compute the non-averaged target distribution γ * :</p><formula xml:id="formula_7" coords="7,282.46,401.00,257.54,11.41">γ * = A T * (7)</formula><p>Since we include "unknown" as a group, we have a challenge with computing the target distribution by averaging the empirical distribution of relevant documents and the global population -global population does not provide any information on the proportion of relevant articles for which the fairness attributes are relevant. Our solution, therefore, is to average the distribution of known-group documents with the world population, and re-normalize so the final distribution is a probability distribution, but derive the proportion of known-to unknown-group documents entirely from the empirical distribution of relevant documents. Extended to handle partially-unknown documents, this procedure proceeds as follows:</p><p>• Average the distribution of fully-known documents (both gender and location are known) with the global intersectional population (global population by location and equality by gender).</p><p>• Average the distribution of documents with unknown location but known gender with the equality gender distribution.</p><p>• Average the distribution of documents with unknown gender but known location with the world population.</p><p>The result is the target group exposure γ * . We use this to measure the expected exposure loss: Lower M 2 is better. It decomposes into two submetrics, the expected exposure disparity (EE-D) that measures overall inequality in exposure independent of relevance, for which lower is better; and the expected exposure relevance (EE-L) that measures exposure/relevance alignment, for which higher is better [1].</p><formula xml:id="formula_8" coords="7,225.73,632.82,314.27,56.56">M 2 (L q ) = γ -γ * 2 (8) = γ • γ -2γ • γ * + γ * • γ * EE-D(L q ) = γ * • γ * (9) EE-R(L q ) = γ • γ *<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This year four different teams submitted a total of 24 runs. All four teams participated in Task 1: Single Rankings (13 runs total), while only three of the four groups participated in Task 2: Multiple Rankings (11 runs total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task 1: WikiProject Coordinators (Single Rankings)</head><p>Approaches for Task 1 included:</p><p>• RoBERTa model to compute embeddings for text fields.</p><p>• A filtering approach to select top ranked documents from either competing rankers or the union of rankers.</p><p>• BM25 ranking from pyserini and re-ranked using MMR implicit diversification (without explicit fairness groups). Lambda varied between runs.</p><p>• BM25 initial ranking with iterative reranking using fairness calculations to select documents to add to the ranking.</p><p>• Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversification plus data fusion.</p><p>• Optimisation to consider a protected group's distribution in the background collection and the total predicted relevance of the group in the candidate results set.</p><p>• Allocating positions in the generated ranking to a protected group proportionally with respect to the total relevance score of the group within the candidate results set. • Relevance-only approaches.</p><p>Table <ref type="table" coords="9,114.65,379.11,4.98,9.96" target="#tab_1">1</ref> shows the submitted systems ranked by the official Task 1 metric M 1 and its component parts nDCG and AWRF. Figure <ref type="figure" coords="9,190.47,391.07,4.98,9.96" target="#fig_1">1</ref> plots the runs with the component metrics on the x and y axes. Notably, each of the approaches from a participating team are clustered in terms of the component metrics and the official M 1 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task 2: Wikipedia Editors (Multiple Rankings)</head><p>Approaches for Task 2 included:</p><p>• A randomized method with BERT and a two-staged Plackett-Luce sampling where relevance scores are combined with work needed.</p><p>• An iterative approach that uses RoBERTa and computes a score for each of the top-K documents in the current state, based on the expected exposure of each group so far and the original estimated relevance score, integrating an article's quality score.</p><p>• BM25 plus re-ranking iteratively selecting documents by combining relevance, fairness and quality scores.</p><p>• Relevance ranking using Terrier plus a fairness component that aims to be fair to both the geographic location attribute and an inferred demographic attribute through tailored diversification plus data fusion to prioritise highly relevant documents while matching the distributions of the protected groups in the generated ranking to their distributions in the background population.</p><p>• Minimising the predicted divergence, or skew, in the distributions of the protected groups over all of the rankings within a sequence, compared to the background population.</p><p>• Minimising the disparity between a group's expected and actual exposures and learning the importance of the group relevance and background distributions. • Relevance-only ranking.</p><p>Table <ref type="table" coords="10,114.12,286.73,4.98,9.96" target="#tab_2">2</ref> shows the submitted systems ranked by the official Task 2 metric EE-L and its component parts EE-D and EE-R. Figure <ref type="figure" coords="10,183.94,298.68,4.98,9.96" target="#fig_2">2</ref> plots the runs with the component metrics on the x and y axes. Overall, the submitted systems generally performed better for one of the component metrics than they did for the other. There are, however, a cluster of four points in Figure <ref type="figure" coords="10,309.49,322.59,4.98,9.96" target="#fig_2">2</ref> that make headway in the trade-off between EE-D and EE-L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>The data and metrics in this task address a few specific types of unfairness, and do so partially. This is fundamentally true of any fairness intervention, and does not in any way diminish the value of the effortit is impossible for any data set, task definition, or metric to fully capture fairness in a universal way, and all data and analyses have limitations.</p><p>Some of the limitations of the data and task include:</p><p>• Fairness criteria -Geography: For each Wikipedia article, we ascertained which, if any, continents are relevant to the content. <ref type="bibr" coords="10,171.04,489.98,3.97,6.16" target="#b4">5</ref> This was determined by directly looking up several community-maintained (Wikidata) structured data statements about the article. These properties were checked for the presence of countries, which were then mapped to continents via the United Nation's geoscheme. <ref type="bibr" coords="10,504.56,513.89,3.97,6.16">6</ref> While this data must meet Wikidata's verifiability guidelines,<ref type="foot" coords="10,353.28,525.85,3.97,6.16" target="#foot_6">7</ref> it does suffer from varying levels of incompleteness. For example, only 73% of people on Wikidata have a country of citizenship property.<ref type="foot" coords="10,535.53,537.80,3.97,6.16" target="#foot_7">8</ref> Furthermore, structured data is itself limited-e.g., country of citizenship does not appropriately capture people who are considered stateless though these people may have many strong ties to a country. It is not easy to evaluate whether this data is missing at random or biased against certain regions of the world. Care should be taken when interpreting the absence of associated continents in the data. Further details can be found in the code repository.<ref type="foot" coords="10,446.90,597.58,3.97,6.16" target="#foot_8">9</ref>  -Gender: For each Wikipedia article, we also ascertained whether it is a biography, and, if so, which gender identity can be associated with the person it is about. 10 This data is also directly determined via Wikidata based on the instance-of property indicating the article is about a human (P31:Q5 in Wikidata terms) and then collecting the value associated with the sex-orgender property (P21). Coverage here is much higher at 99.98% of biographies on Wikipedia having associated gender data on Wikidata.</p><p>Assigning gender identities to people is not a process without errors, biases, and ethical concerns.</p><p>Since we are using it to calculate aggregate statistics, we judged it to be less problematic than it would be if we were making decisions about individuals. The process for assigning gender is subject to some community-defined technical limitations 11 and the Wikidata policy on living people 12 . While a separate project, English Wikipedia's policies on gender identity 13 likely inform how many editors handle gender; in particular, this policy explicitly favors the most recent reliably-sourced self-identification for gender, so misgendering a biography subject is a violation of Wikipedia policy; there may be erroneous data, but such data seems to be a violation of policy instead of a policy decision. Wikidata:WikiProject LGBT has documented some clear limitations of gender data on Wikidata and a list of further discussions and considerations. <ref type="bibr" coords="11,422.60,542.66,7.94,6.16">14</ref> In our analysis (see Appendix A), we handle nonbinary gender identities by using 4 gender categories: unknown, male, female, and third. We advise great care when working with the gender data, particularly outside the immediate context of the TREC task (either its original instance or using the data to evaluate comparable systems). • Relevance Criteria -WikiProject Relevance: For the training queries, relevance was obtained from page lists for existing WikiProjects. While WikiProjects have broad coverage of English Wikipedia and we selected for WikiProjects that had tagged new articles in the recent months in the training data as a proxy for activity, it is certain that almost all WikiProjects are incomplete in tagging relevant content (itself a strong motivation for this task). While it is not easy to measure just how incomplete they are, it should not be assumed that content that has not been tagged as relevant to a WikiProject in the training data is indeed irrelevant. <ref type="bibr" coords="12,382.21,165.20,7.94,6.16">15</ref> Evaluation query relevance was assessed by NIST assessors, but the large sets of relevant documents and limited budget for working through the pool mean these lists are also incomplete.</p><p>-Work-needed: Our proxy for work-needed is a coarse proxy. It is based on just a few simple features (page length, sections, images, and references) and does not reflect the nuances of the work needed to craft a top-quality Wikipedia article. <ref type="bibr" coords="12,357.09,228.96,7.94,6.16">16</ref> A fully-fledged system for supporting Wikiprojects would also include a more nuanced approach to understanding the work needed for each article and how to appropriately allocate this work.</p><p>• Task Definition -Existing Article Bias: The task is limited to topics for which English Wikipedia already has articles. These tasks are not able to counteract biases in the processes by which articles come to exist (or are deleted [7])-recommending articles that should exist but don't is an interesting area for future study.</p><p>-Fairness constructs: we focus on gender and geography in this challenge as two metrics for which there is high data coverage and clearer expectations about what "fairer" or more representative coverage might look like. That does not mean these are the most important constructs, but others-e.g., religion, sexuality, culture, race-generally are either more challenging to model or map to fairness goals [5].</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Alignments</head><p>This appendix provides further details on how the page alignments and target distributions are computed.</p><p>It is a Jupyter notebook analyzes page alignments and prepares metrics for final use. It needs to be run to create the serialized alignment data files the metrics require; it is available in the code that goes with the appendix. Its final output is pickled metric objects: an instance of the Task 1 and Task 2 metric classes, serialized to a compressed file with binpickle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Setup</head><p>We begin by loading necessary libraries:  <ref type="figure" coords="14,72.00,587.00,57.53,9.96;14,150.46,587.00,104.61,9.96">---------------------------</ref> </p><formula xml:id="formula_9" coords="13,72.00,322.98,20.92,9.96">from</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Query Relevance</head><p>We now need to get the qrels for the topics. This is done by creating frames with entries for every relevant document; missing documents are assumed irrelevant (0).</p><p>In the individual metric evaluation files, we will truncate each run to only the assessed documents (with a small amount of noise), so this is a safe way to compute.</p><p>First the training topics: </p><formula xml:id="formula_10" coords="17,72.00,374.16,66.53,9.96">train qrels =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Page Alignments</head><p>All of our metrics require page "alignments": the protected-group membership of each page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Geography</head><p>Let's start with the straight page geography alignment for the public evaluation of the training queries. The page metadata has that; let's get the geography column.  , 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], ..., [1., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.]]) Coordinates:</p><p>* page (page) int64 12 25 39 290 ... 67268663 67268668 67268699 67268751 * geography (geography) object 'Unknown' 'Africa' ... 'Oceania' binarysize(page geo xr.nbytes) '385.50 MiB'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Gender</head><p>The "undisclosed personal attribute" is gender. Not all articles have gender as a relevant variable -articles not about a living being generally will not.</p><p>We're going to follow the same approach for gender: We need to do a little targeted repair -there is an erroneous record of a gender of "Taira no Kiyomori" is actually male. Replace that: page gender = page gender.loc[page gender[ gender ] != Taira no Kiyomori ] Now, we're going to do a little more work to reduce the dimensionality of the space. Points:</p><formula xml:id="formula_11" coords="19,72.00,327.99,20.92,9.96">page</formula><p>1. Trans men are men 2. Trans women are women 3. Cisgender is an adjective that can be dropped for the present purposes</p><p>The result is that we will collapse "transgender female" and "cisgender female" into "female". The downside to this is that trans men are probabily significantly under-represented, but are now being collapsed into the dominant group. pgcol = page gender[ gender ] pgcol = pgcol.str.replace(r (?:tran|ci)sgender\s+((?:fe)?male) , r \1 , regex=True) Now, we're going to group the remaining gender identities together under the label 'third'. As noted above, this is a debatable exercise that collapses a lot of identity.  </p><p>..., [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]],</p><p>[[1., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], ..., [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]],</p><p>[[1., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], ..., [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]]) Coordinates:</p><p>* page (page) int64 12 25 39 290 ... 67268663 67268668 67268699 67268751 * geography (geography) object 'Unknown' 'Africa' ... 'Oceania' * gender (gender) object 'unknown' 'female' 'male' 'third' Intersectional is a little harder to do, because things can be intersectionally unknown: we may know gender but not geography, or vice versa. To deal with these missing values for Task 1, we're going to ignore totally unknown values, but keep partially-known as a category.</p><p>We also need to ravel our tensors into a matrix for compatibility with the metric code. Since 'unknown' is the first value on each axis, we can ravel, and then drop the first column.  q am = q am / q am.sum() # compute fractions in each section q fk all = q am[1:, 1:].sum() q fk geo = q am[1:, :1].sum() q fk gen = q am[:1, 1:].sum() # known average q am[1:, 1:] *= 0.5 q am[1:, 1:] += int tgt * 0.5 * q fk all # known-geo average q am[1:, :1] *= 0.5 q am[1:, :1] += geo tgt xa * 0.5 * q fk geo # known-gender average q am[:1, 1:] *= 0.5 q am[: This is our group exposure target distributions for each query, for the geographic data. We're now ready to set up the matrix.</p><p>train geo qtgt = q geo tgt.loc[train topics [ id ]] eval geo qtgt = q geo tgt.loc[eval topics [ id ]] t2 train geo metric = metrics.Task2Metric(train qrels.set index( id ), page geo align, page work, train geo qtgt) binpickle.dump(t2 train geo metric, task2-train-geo-metric.bpk , codec=codec) INFO:binpickle.write:pickled 2018 bytes with 9 buffers t2 eval geo metric = metrics.Task2Metric(eval qrels.set index( id ), page geo align, page work, eval geo qtgt) binpickle.dump(t2 eval geo metric, task2-eval-geo-metric.bpk , codec=codec) INFO:binpickle.write:pickled 2014 bytes with 9 buffers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.3 Intersectional Alignment</head><p>Now we need to compute the intersectional targets for Task 2. We're going to take a slightly different approach here, based on the intersectional logic for Task 1, because we've come up with better ways to write the code, but the effect is the same: only known aspects are averaged.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.00,74.49,160.60,9.96;5,72.00,94.42,468.00,9.96;5,96.91,106.37,122.67,9.96;5,72.00,126.30,468.00,9.96;5,96.91,138.25,231.29,9.96"><head></head><label></label><figDesc>page id Unique page identifier (int) quality score Continuous measure of article quality with 0 representing low quality and 1 representing high quality (float in range [0, 1]) quality score disc Discrete quality score in which the quality score is mapped to six ordinal categories from low to high: Stub, Start, C, B, GA, FA (string)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,72.00,320.45,468.00,9.96;9,72.00,332.40,73.17,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Task 1 submissions by individual component metrics (NDCG and AWRF). Higher values are better for both metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,72.00,320.45,468.00,9.96;11,72.00,332.40,29.09,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Task 2 submissions by expected exposure subcomponents. Lower EE-D is better; higher EE-R is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="18,72.00,388.44,10.46,9.96;18,207.99,388.44,15.69,9.96;18,249.83,388.44,15.69,9.96;18,312.60,388.44,15.69,9.96;18,343.98,388.44,15.69,9.96;18,385.82,388.44,15.69,9.96;18,72.00,400.39,10.46,9.96;18,207.99,400.39,15.69,9.96;18,249.83,400.39,15.69,9.96;18,312.60,400.39,15.69,9.96;18,343.98,400.39,15.69,9.96;18,385.82,400.39,15.69,9.96;18,72.00,412.35,10.46,9.96;18,207.99,412.35,15.69,9.96;18,249.83,412.35,15.69,9.96;18,312.60,412.35,15.69,9.96;18,343.98,412.35,15.69,9.96;18,385.82,412.35,15.69,9.96;18,72.00,424.30,15.69,9.96;18,207.99,424.30,15.69,9.96;18,249.83,424.30,15.69,9.96;18,312.60,424.30,15.69,9.96;18,343.98,424.30,15.69,9.96;18,385.82,424.30,15.69,9.96;18,72.00,436.26,15.69,9.96;18,207.99,436.26,15.69,9.96;18,249.83,436.26,15.69,9.96;18,312.60,436.26,15.69,9.96;18,343.98,436.26,15.69,9.96;18,385.82,436.26,15.69,9.96;18,72.00,460.17,387.05,9.96;18,72.00,472.12,36.61,9.96;18,72.00,484.08,10.46,9.96;18,333.52,484.08,15.69,9.96;18,427.66,484.08,15.69,9.96;18,72.00,496.03,10.46,9.96;18,333.52,496.03,15.69,9.96;18,427.66,496.03,15.69,9.96;18,72.00,507.99,10.46,9.96;18,333.52,507.99,15.69,9.96;18,427.66,507.99,15.69,9.96;18,72.00,519.94,15.69,9.96;18,333.52,519.94,15.69,9.96;18,427.66,519.94,15.69,9.96;18,72.00,531.90,15.69,9.96;18,333.52,531.90,15.69,9.96;18,427.66,531.90,15.69,9.96;18,72.00,555.81,151.68,9.96;18,72.00,567.76,36.61,9.96;18,72.00,579.72,10.46,9.96;18,207.99,579.72,15.69,9.96;18,72.00,591.67,10.46,9.96;18,207.99,591.67,15.69,9.96;18,72.00,603.63,10.46,9.96;18,207.99,603.63,15.69,9.96;18,72.00,615.58,15.69,9.96;18,207.99,615.58,15.69,9.96;18,72.00,627.54,15.69,9.96;18,207.99,627.54,15.69,9.96;18,86.94,649.11,253.49,9.96;18,72.00,670.69,360.27,9.96;18,72.00,682.64,54.60,9.96;19,72.00,74.49,251.06,9.96;19,72.00,86.45,52.30,9.96"><head></head><label></label><figDesc>And convert this to an xarray for multidimensional usage: page geo xr = xr.DataArray(page geo align, dims=[ page , geography ]) page geo xr &lt;xarray.DataArray (page: 6023415, geography: 8)&gt; array([[1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="20,72.00,74.49,251.06,9.96;20,72.00,86.45,188.29,9.96;20,86.94,108.36,247.21,9.96;20,72.00,130.28,150.22,9.96;20,72.00,142.24,220.51,9.96;20,86.94,164.15,353.84,9.96;20,72.00,186.07,485.80,9.96;20,72.00,198.03,201.05,9.96;20,72.00,209.98,459.64,9.96;20,72.00,221.94,112.14,9.96;20,72.00,243.86,31.38,9.96;20,119.07,243.86,146.45,9.96;20,72.00,255.81,36.61,9.96;20,72.00,267.77,10.46,9.96;20,139.99,267.77,15.69,9.96;20,181.84,267.77,15.69,9.96;20,213.22,267.77,15.69,9.96;20,249.83,267.77,15.69,9.96;20,72.00,279.72,10.46,9.96;20,139.99,279.72,15.69,9.96;20,181.84,279.72,15.69,9.96;20,213.22,279.72,15.69,9.96;20,249.83,279.72,15.69,9.96;20,72.00,291.68,10.46,9.96;20,139.99,291.68,15.69,9.96;20,181.84,291.68,15.69,9.96;20,213.22,291.68,15.69,9.96;20,249.83,291.68,15.69,9.96;20,72.00,303.63,15.69,9.96;20,139.99,303.63,15.69,9.96;20,181.84,303.63,15.69,9.96;20,213.22,303.63,15.69,9.96;20,249.83,303.63,15.69,9.96;20,72.00,315.59,15.69,9.96;20,139.99,315.59,15.69,9.96;20,181.84,315.59,15.69,9.96;20,213.22,315.59,15.69,9.96;20,249.83,315.59,15.69,9.96;20,86.94,337.50,196.90,9.96;20,72.00,359.42,288.51,9.96;20,72.00,381.34,31.38,9.96;20,72.00,393.30,36.61,9.96;20,129.53,393.30,47.07,9.96;20,72.00,405.25,20.92,9.96;20,129.53,405.25,47.07,9.96;20,72.00,417.21,31.38,9.96;20,134.76,417.21,41.84,9.96;20,72.00,429.16,26.15,9.96;20,150.46,429.16,26.15,9.96;20,72.00,441.12,73.22,9.96;20,86.94,463.03,113.83,9.96;20,72.00,484.95,355.04,9.96;20,72.00,496.91,59.84,9.96;20,72.00,518.82,235.37,9.96;20,72.00,530.78,94.15,9.96"><head></head><label></label><figDesc>genders = [ unknown , male , female , third ] pgcol[˜pgcol.isin(genders)] = third Now put this column back in the frame and deduplicate. page gender[ gender ] = pgcol page gender = page gender.drop duplicates() And make an alignment matrix (reordering so 'unknown' is first for consistency): page gend align = page gender.assign(x=1).pivot(index= page id , columns= gender , values= x ) page gend align.fillna(0, inplace=True) page gend align = page gend align.reindex(columns=[ unknown , female , male , third ]) page gend align.head(Let's see how frequent each of the genders is: page gend align.sum(axis=0).sort values(ascending=False) And convert to an xarray: page gend xr = xr.DataArray(page gend align, dims=[ page , gender ]) page gend xr &lt;xarray.DataArray (page: 6023415, gender: 4)&gt; array([[1., 0., 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="23,72.00,333.51,124.06,9.96;23,72.00,345.47,183.06,9.96;23,72.00,357.42,485.80,9.96;23,72.00,369.38,85.99,9.96;23,108.61,391.29,5.23,9.96;23,134.76,391.29,5.23,9.96;23,160.92,391.29,5.23,9.96;23,187.07,391.29,5.23,9.96;23,213.22,391.29,5.23,9.96;23,239.37,391.29,5.23,9.96;23,265.52,391.29,5.23,9.96;23,291.68,391.29,5.23,9.96;23,317.83,391.29,5.23,9.96;23,343.98,391.29,5.23,9.96;23,364.90,391.29,15.69,9.96;23,396.28,391.29,10.46,9.96;23,422.43,391.29,10.46,9.96;23,448.59,391.29,26.15,9.96;23,72.00,403.25,20.92,9.96;23,364.90,403.25,15.69,9.96;23,72.00,415.20,10.46,9.96;23,103.38,415.20,355.66,9.96;23,72.00,427.16,10.46,9.96;23,103.38,427.16,355.66,9.96;23,72.00,439.11,10.46,9.96;23,103.38,439.11,355.66,9.96;23,72.00,451.07,15.69,9.96;23,103.38,451.07,355.66,9.96;23,72.00,463.02,15.69,9.96;23,103.38,463.02,355.66,9.96;23,108.61,486.93,10.46,9.96;23,134.76,486.93,10.46,9.96;23,160.92,486.93,10.46,9.96;23,187.07,486.93,10.46,9.96;23,213.22,486.93,10.46,9.96;23,239.37,486.93,10.46,9.96;23,265.52,486.93,10.46,9.96;23,72.00,498.89,20.92,9.96;23,72.00,510.84,10.46,9.96;23,103.38,510.84,172.60,9.96;23,72.00,522.80,10.46,9.96;23,103.38,522.80,172.60,9.96;23,72.00,534.75,10.46,9.96;23,103.38,534.75,172.60,9.96;23,72.00,546.71,15.69,9.96;23,103.38,546.71,172.60,9.96;23,72.00,558.66,15.69,9.96;23,103.38,558.66,172.60,9.96;23,72.00,582.57,109.84,9.96;23,86.94,604.49,239.79,9.96;23,72.00,626.40,168.21,9.96;23,72.00,652.93,151.19,8.77;23,72.00,670.69,468.00,9.96;23,72.00,682.64,336.77,9.96"><head>A. 5 . 1</head><label>51</label><figDesc>xshp = page xalign.shape xshp = (xshp[0], xshp[1] * xshp[2]) page xa df = pd.DataFrame(page xalign.values.reshape(xshp), index=page xalign.indexes[ page ]) page xa df.head() And drop unknown, to get our page alignment vectors: page kia = page xa df.iloc[:, 1:] Geographic AlignmentWe'll start with the metric configuration for public training data, considering only geographic alignment. We configure the metric to do this for both the training and the eval queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="41,108.61,74.49,5.23,9.96"><head>[</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,526.72,679.05,13.28,9.96"><head>Table 1 :</head><label>1</label><figDesc>Task 1 runs. Higher score is better (for all metrics).</figDesc><table coords="7,526.72,679.05,13.28,9.96"><row><cell>10)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,150.34,76.48,308.00,170.39"><head>Table 2 :</head><label>2</label><figDesc>Task 2 runs. Lower EE-L is better.</figDesc><table coords="10,150.34,76.48,308.00,146.47"><row><cell></cell><cell>EE-R</cell><cell>EE-D</cell><cell>EE-L</cell><cell>EE-L 95% CI</cell></row><row><cell>RUN task2</cell><cell cols="4">9.5508 4.1557 14.9007 (12.303, 19.946)</cell></row><row><cell>pl control 0.6</cell><cell cols="4">8.8091 3.2733 15.5017 (12.552, 20.477)</cell></row><row><cell>UoGTrRelT2</cell><cell cols="4">11.8281 9.4609 15.6514 (13.057, 20.148)</cell></row><row><cell>pl control 0.8</cell><cell cols="4">8.6654 3.2550 15.7708 (12.746, 21.251)</cell></row><row><cell>pl control 0.92</cell><cell cols="4">8.4802 3.1486 16.0348 (12.820, 21.158)</cell></row><row><cell>PL IRLab 07</cell><cell cols="4">5.2790 1.5327 20.8213 (16.283, 28.089)</cell></row><row><cell>PL IRLab 05</cell><cell cols="4">4.9331 1.4029 21.3832 (16.579, 28.293)</cell></row><row><cell>UoGTrDivPropT2</cell><cell cols="4">4.9372 7.1005 27.0726 (21.098, 35.870)</cell></row><row><cell>UoGTrDRelDiT2</cell><cell cols="4">3.4770 5.5891 28.4816 (22.366, 37.739)</cell></row><row><cell>UoGTrDExpDisT2</cell><cell cols="4">3.7459 6.1356 28.4903 (22.571, 37.548)</cell></row><row><cell>UoGTrLambT2</cell><cell cols="4">2.2447 3.4644 28.8216 (22.799, 37.718)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,72.00,74.49,468.00,65.75"><head></head><label></label><figDesc>6] P. Sapiezynski, W. Zeng, R. E Robertson, A. Mislove, and C. Wilson. Quantifying the impact of user attentionon fair group representation in ranked lists. In Companion Proceedings of The 2019 World Wide Web Conference, pages 553-562, 2019. [7] F. Tripodi. Ms. categorized: Gender, notability, and inequality on wikipedia. New Media &amp; Society, page 14614448211023772, 2021.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="24,72.00,75.13,520.94,591.14"><head></head><label></label><figDesc>Now we need to apply similar logic, but for the intersectional (geography * gender) alignment.As noted as above, we need to carefully handle the unknown cases.</figDesc><table coords="24,72.00,75.13,520.94,591.14"><row><cell>Training Queries eval qalign = eval qrels.join(page kga, on= page id ).drop(columns=[ page id ]).groupby( id ).sum() ...,</cell></row><row><cell>eqa sums = eval qalign.sum(axis=1) [0., 0., 0., 0.], train qalign = train qrels.join(page kga, on= page id ).drop(columns=[ page id ]).groupby( id ).sum() eval qalign = eval qalign.divide(eqa sums, axis=0) [0., 0., 0., 0.], tqa sums = train qalign.sum(axis=1) t1 eval metric = metrics.Task1Metric(eval qrels.set index( id ), page kga, eval qtarget) train qalign = train qalign.divide(tqa sums, axis=0) eval qtarget = (eval qalign + world pop) * 0.5 [0., 0., 0., 0.]],</cell></row><row><cell>binpickle.dump(t1 eval metric, task1-eval-geo-metric.bpk , codec=codec) [[1., 0., 0., 0.], train qalign.head() Africa Antarctica Asia [0., 0., 0., 0.], INFO:binpickle.write:pickled 337312643 bytes with 5 buffers [0., 0., 0., 0.], Europe Latin America and the Caribbean \ ..., id 1 0.049495 0.00000 0.121886 0.356566 A.5.2 Intersectional Alignment ... 0.031650 2 0.013388 0.00000 0.112008 0.574026 ..., 0.026105 3 0.109664 0.00000 0.125529 0.456033 [0., 0., 0., 0.], 0.100040 4 0.062495 0.00025 0.116161 0.327272 [0., 0., 0., 0.], 0.079514 5 0.000835 0.00000 0.065433 0.010149 0.064755 Demo To demonstrate how the logic works, let's first work it out in cells for one query (1). [0., 0., 0., 0.]],</cell></row><row><cell>Northern America What are its documents? [[1., 0., 0., 0.], Oceania 0.261616 0.178788 0.228715 0.045758 [0., 0., 0., 0.], qdf = qrels[qrels[ id ] == 1] id 1 2 3 [0., 0., 0., 0.], qdf.name = 1 ..., qdf [0., 0., 0., 0.], 0.158419 0.050316 4 0.369277 0.045032 5 0.850192 0.008636 id [0., 0., 0., 0.], page_id 0 1 572 [0., 0., 0., 0.]],</cell></row><row><cell>train qtarget = (train qalign + world pop) * 0.5 train qtarget.head() Africa Antarctica Asia Europe \ id 1 0.102283 7.721200e-08 0.361044 0.230115 2 0.084229 7.721200e-08 0.356105 0.338845 3 0.132367 7.721200e-08 0.362866 0.279848 4 0.108783 1.250113e-04 0.358182 0.215468 5 0.077953 7.721200e-08 0.332818 0.056906 1 1 627 2 1 [[1., 0., 0., 0.], 903 3 1 [0., 0., 0., 0.], 1193 4 1 [0., 0., 0., 0.], 1542 ... .. ..., ... 6959 [0., 0., 0., 0.], 1 67066971 6960 [0., 0., 0., 0.], 1 67075177 6961 [0., 0., 0., 0.]]]) 1 67178925 6962 Coordinates: 1 67190032 6963 * page (page) int64 572 627 903 1193 ... 67178925 67190032 67244439 1 67244439 * geography (geography) object 'Unknown' 'Africa' ... 'Oceania'</cell></row><row><cell>Latin America and the Caribbean Northern America [6964 rows x 2 columns] * gender (gender) object 'unknown' 'female' 'male' 'third' Oceania</cell></row><row><cell>id 1 2 3 q am = q xa.sum(axis=0) Summing over the first axis ('page') will produce an alignment matrix: 0.058874 We can use these page IDs to get its alignments: 0.155616 0.092068 0.056101 0.139166 0.025553 0.093069 0.104018 0.027832 q xa = page xalign.loc[qdf[ page id ].values, :, :] q xa q am</cell></row><row><cell>4 5 t1 train metric = metrics.Task1Metric(train qrels.set index( id ), page kga, train qtarget) 0.082806 0.209447 0.025190 0.075427 0.449904 0.006992 And we can prepare a metric and save it: binpickle.dump(t1 train metric, task1-train-geo-metric.bpk , codec=codec) INFO:binpickle.write:pickled 337312647 bytes with 5 buffers &lt;xarray.DataArray (geography: 8, gender: 4)&gt; &lt;xarray.DataArray (page: 6964, geography: 8, gender: 4)&gt; array([[3767., 52., 200., 0.], array([[[1., 0., 0., 0.], [ 128., 12., 7., 0.], [0., 0., 0., 0.], [ 0., 0., 0., 0.], [0., 0., 0., 0.], [ 322., 11., 29., 0.], ..., [ 940., 23., 96., 0.], [0., 0., 0., 0.], [ 79., 8., 7., 0.], [0., 0., 0., 0.], [ 618., 28., 131., 0.], [0., 0., 0., 0.]], [ 484., 6., 41., 0.]])</cell></row><row><cell>Eval Queries Do the same thing for the eval data for a geo-only eval metric: Coordinates: [[1., 0., 0., 0.], * geography (geography) object 'Unknown' 'Africa' ... 'Oceania' [0., 0., 0., 0.], [0., 0., 0., 0.], * gender (gender) object 'unknown' 'female' 'male' 'third'</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="29,72.00,277.73,254.20,412.45"><head>1 Work and Target Exposure The</head><label></label><figDesc>Now with that function, we can compute the alignment vector for each query. Do the same for eval: eval qtarget = eval qrels.groupby( id ).apply(query xalign) t1 eval metric = metrics.Task1Metric(eval qrels.set index( id ), page kia, eval qtarget) binpickle.dump(t1 eval metric, task1-eval-metric.bpk , codec=codec) first thing we need to do to prepare the metric is to compute the work-needed for each topic's pages, and use that to compute the target exposure for each (relevant) page in the topic. This is because an ideal ranking orders relevant documents in decreasing order of work needed, followed by irrelevant documents. All relevant documents at a given work level should receive the same expected exposure.First, look up the work for each query page ('query page work', or qpw):Now we need to convert this into target exposure levels. This function will, given a series of counts for each work level, compute the expected exposure a page at that work level should receive.We can now merge the relevant document work categories with this exposure, to compute the target exposure for each relevant document:</figDesc><table coords="29,72.00,277.73,254.20,412.45"><row><cell></cell><cell>1.095016e-03 GA 240 Unknown Africa</cell><cell>Antarctica</cell><cell>Asia</cell><cell>Europe \</cell></row><row><cell>29 q_id</cell><cell>6.526425e-03 ...</cell><cell></cell><cell></cell></row><row><cell cols="5">30 150 Start 3.311463e-06 138 1 0.575338 0.043635 3.278897e-08 0.153851 0.098450</cell></row><row><cell cols="5">dtype: float64 C 2 0.173889 0.069608 6.378567e-08 0.294269 0.280798 127</cell></row><row><cell>3</cell><cell cols="4">B 0.234897 0.101882 5.907510e-08 0.278161 0.215027 35</cell></row><row><cell cols="5">GA 0.312664 0.076008 8.262075e-05 0.246140 0.145192 16 INFO:binpickle.write:pickled 1493808200 bytes with 5 buffers 4 FA 8 5 0.182143 0.063760 6.314834e-08 0.273795 0.046710 train qtarget = train qrels.groupby( id ).apply(query xalign) Name: page_id, Length: 636, dtype: int64 ... ... ... ... ... ... train qtarget A.6 Task 2 Metric Preparation 146 0.292441 0.090378 5.463208e-08 0.299627 0.067556</cell></row><row><cell cols="5">1, 1:] += gender tgt xa * 0.5 * q fk gen # and return the result return pd.Series(q am.values.ravel()[1:]) query xalign(qdf) 0 2.742706e-02 1 5.039417e-02 2 3.910615e-04 3 8.173284e-02 4 6.615024e-03 5 5.839108e-03 6 9.601669e-05 7 6.161144e-08 8 4.733009e-09 9 4.733009e-09 10 9.561635e-11 11 2.894353e-01 12 2.010289e-02 13 2.289618e-02 14 3.716338e-04 15 1.872315e-01 16 6.746451e-03 17 1.807482e-02 18 6.418665e-05 19 4.661047e-02 20 3.880320e-03 21 3.725136e-03 22 5.331020e-05 23 1.156990e-01 24 5.865852e-03 25 2.184971e-02 26 27 B 610 7.724241e-02 13 2.260124e-05 C 1603 3.072172e-05 0 1 2 3 id 1 0.027427 0.050394 0.000391 0.081733 0.006615 0.005839 0.000096 4 5 6 2 147 0.434276 0.060053 4.368069e-08 0.195520 0.130625 Task 2 requires some different preparation. 148 0.637050 0.033542 2.802409e-08 0.233693 0.045680 We're going to start by computing work-needed information: page work = pages.set index( page id ).quality score disc.astype(pd.CategoricalDtype(ordered=True)) \ 149 0.370828 0.061724 4.857964e-08 0.243518 0.172170 def qw tgt exposure(qw counts: pd.Series) -&gt; pd.Series: if id == qw counts.index.names[0]: 150 0.414091 0.062031 4.523918e-08 0.208319 0.131270 0.012235 0.032073 0.000232 0.073168 0.003571 0.003669 0.000070 3 0.022553 0.035541 0.000292 0.023527 0.040981 0.059556 0.000574 4 0.012472 0.029112 0.000209 0.094840 0.004409 0.004901 0.000086 5 0.023416 0.063398 0.000436 0.020521 0.024932 0.025194 0.000504 6 id 1 3.311463e-06 2 2.401088e-06 3 4 2.960754e-06 5 6 7 8 9 10 5.600064e-06 11 1.234124e-05 12 2.253246e-05 id 1 Stub Start 2822 Name: tgt_exposure, Length: 636, dtype: float32 1527 FA 0.118126 quality GA 0.118827 1.909121e-05 qwork B 0.120441 3.745849e-05 qwork = qpw.groupby([ id , quality ])[ page id ].count() C 0.127359 2.226348e-05 150 Start 0.154202 2.797145e-05 And now use that to compute the number of documents at each work level: ... 1.737119e-05 GA 0.078702 [2199077 rows x 3 columns] B 0.079298 1.941043e-05 page work = page work.cat.reorder categories(work order) page work.name = quality A.6.qpw = qrels.join(page work, on= page id ) qpw 3 1 1193 B 4 1 1542 GA ... ... ... ... 2199072 150 63656179 Start 2199073 150 63807245 NaN 2199074 150 64614938 2199075 150 64716982 2199076 150 65355704 C 0.081146 C Start 0.087373 C 1 Stub 0.114738 C qw counts = qw counts.reset index(level= id , drop=True) Latin America and the Caribbean Northern America Oceania qwc = qw counts.reindex(work order, fill value=0).astype( i4 ) q_id tot = int(qwc.sum()) da = metrics.discount(tot) qwp = qwc.shift(1, fill value=0) qwc s = qwc.cumsum() qwp s = qwp.cumsum() res = pd.Series( [np.mean(da[s:e]) for (s, e) in zip(qwp s, qwc s)], index=qwc.index ) return res We'll then apply this to each topic, to determine the per-topic target exposures: C:\Users\michaelekstrand\Miniconda3\envs\wptrec\lib\site-packages\numpy\core\fromnumeric.py:3440: Runtim return _methods._mean(a, axis=axis, dtype=dtype, C:\Users\michaelekstrand\Miniconda3\envs\wptrec\lib\site-packages\numpy\core\_methods.py:189: RuntimeWar ret = ret.dtype.type(ret / rcount) id quality 1 0.025042 0.065388 0.038296 2 0.046323 0.115193 0.019920 3 0.071196 0.077784 0.021053 4 0.058319 0.143947 0.017648 5 0.061549 0.366345 0.005697 ... ... ... ... 146 0.045686 0.178497 0.025815 147 0.061604 0.091005 0.026916 148 0.018613 0.025322 0.006099 149 0.040886 0.073876 0.036999 150 0.042203 0.116868 0.025218 0.126820 0.201558 0.001691 0.000021 0.028097 0.030215 0.000519 30 id page_id quality 0 1 572 C 1 1 627 2 1 903 C qw pp target FA qw pp target = qwork.groupby( id ).apply(qw tgt exposure) qw pp target.name = tgt exposure [106 rows x 8 columns]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="41,72.00,74.49,413.20,618.11"><head></head><label></label><figDesc>5.32652519e-02, 2.51534798e-03, 9.59370956e-03, 1.36109402e-05], [3.48679417e-02, 4.71346052e-04, 2.95512391e-03, 1.46710935e-06]]) Coordinates: * geography (geography) object 'Unknown' 'Africa' ... 'Oceania' * gender (gender) object 'unknown' 'female' 'male' 'third' .017108 0.032738 0.000250 0.033631 0.031692 0.024843 147 0.380085 0.025582 0.026067 0.001304 0.028472 0.017849 0.014999 148 0.620663 0.005550 0.010755 0.000082 0.031143 0.001188 0.001188 149 0.365415 0.002870 0.002516 0.000027 0.060143 0.000783 0.000783 150 0.228180 0.057917 0.127065 0.000930 0.014522 0.021052 0.026136 .349054e-08 1.046506e-08 ... 0.009259 0.000118 0.164611 147 0.000207 2.317530e-08 1.019747e-08 ... 0.018809 0.000115 0.050914 148 0.000024 2.563480e-08 1.182699e-09 ... 0.000659 0.000013 0.020264 149 0.000016 4.700522e-08 7.793387e-10 ... 0.002786 0.000009 0.071839 150 0.000320 1.332948e-08 1.579530e-08 ... 0.016691 0.000178 0.040721 .004670 0.000008 0.003416 0.000041 0.002642 8.274784e-07 149 0.000250 0.001781 0.000005 0.036944 0.000027 0.000027 5.452665e-07</figDesc><table coords="41,72.00,142.84,413.20,537.81"><row><cell></cell><cell>And let's go!</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">q xtgt = qrels.groupby( id ).progress apply(query xideal)</cell><cell></cell></row><row><cell cols="2">q xtgt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">{"model id":"","version major":2,"version minor":0}</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>\</cell></row><row><cell>id</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="8">0.540211 0.012290 0.022661 0.000176 0.038091 0.002908 0.002593</cell></row><row><cell>2</cell><cell cols="8">0.135109 0.010633 0.027958 0.000201 0.063400 0.003032 0.003115</cell></row><row><cell>3</cell><cell cols="8">0.185923 0.018891 0.029817 0.000245 0.018607 0.033486 0.049321</cell></row><row><cell>4</cell><cell cols="8">0.283620 0.008665 0.020234 0.000145 0.069568 0.003021 0.003361</cell></row><row><cell>5</cell><cell cols="8">0.102865 0.021347 0.057531 0.000396 0.017768 0.022647 0.022888</cell></row><row><cell>..</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell cols="2">...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell cols="2">146 0.242344 07</cell><cell>8</cell><cell></cell><cell>9</cell><cell>...</cell><cell>22</cell><cell>23</cell><cell>24 \</cell></row><row><cell>id</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="8">0.000043 2.855279e-08 2.096911e-09 ... 0.001652 0.000024 0.053265</cell></row><row><cell>2</cell><cell cols="8">0.000059 5.789347e-08 2.916166e-09 ... 0.002811 0.000033 0.099662</cell></row><row><cell>3</cell><cell cols="8">0.000471 1.300894e-08 2.280358e-08 ... 0.032680 0.000257 0.019061</cell></row><row><cell>4</cell><cell cols="8">0.000059 8.261490e-05 2.894759e-09 ... 0.002071 0.000033 0.127457</cell></row><row><cell>5</cell><cell cols="8">0.000458 1.758741e-08 2.255280e-08 ... 0.034300 0.000254 0.104245</cell></row><row><cell>..</cell><cell>...</cell><cell>...</cell><cell></cell><cell cols="2">... ...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell cols="2">146 0.000212 325</cell><cell>26</cell><cell>27</cell><cell cols="2">28</cell><cell>29</cell><cell>30</cell><cell>31</cell></row><row><cell>id</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="8">0.002515 0.009594 0.000014 0.034868 0.000471 0.002955 1.467109e-06</cell></row><row><cell>2</cell><cell cols="8">0.002826 0.012684 0.000019 0.017811 0.000468 0.001639 2.040304e-06</cell></row><row><cell>3</cell><cell cols="8">0.018746 0.039812 0.000164 0.004908 0.005947 0.010179 1.595459e-05</cell></row><row><cell>4</cell><cell cols="8">0.002601 0.013870 0.000019 0.015398 0.000279 0.001968 2.025326e-06</cell></row><row><cell>5</cell><cell cols="8">0.012692 0.249255 0.000146 0.002342 0.000883 0.002456 1.577913e-05</cell></row><row><cell>..</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell cols="2">...</cell><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell cols="9">146 0.010455 0.003362 0.000068 0.013121 0.012324 0.000362 7.321910e-06</cell></row><row><cell cols="9">147 0.022379 0.017457 0.000066 0.016122 0.005498 0.005220 7.134685e-06</cell></row><row><cell cols="2">148 0.000380 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,680.79,173.60,6.71"><p>https://en.wikipedia.org/wiki/WikiProject</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,87.24,644.40,195.77,6.71"><p>https://www.globus.org/globus-connect-personal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,87.24,653.91,199.50,6.71"><p>https://creativecommons.org/licenses/by-sa/3.0/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,87.24,668.57,434.01,7.26"><p>Obtained from https://en.wikipedia.org/wiki/List_of_continents_and_continental_subregions_by_population</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="10,87.24,618.08,374.36,7.26"><p>Code: https://github.com/geohci/wiki-region-groundtruth/blob/main/wiki-region-data.ipynb</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="10,87.24,628.13,228.64,6.71"><p>https://en.wikipedia.org/wiki/United_Nations_geoscheme</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="10,87.24,637.64,220.17,6.71"><p>https://www.wikidata.org/wiki/Wikidata:Verifiability</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="10,87.24,647.14,195.77,6.71"><p>https://humaniki.wmcloud.org/gender-by-country</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="10,87.24,656.64,208.47,6.71"><p>https://github.com/geohci/wiki-region-groundtruth</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9" coords="12,87.24,631.55,452.76,7.26;12,72.00,641.57,211.71,6.71"><p>Current Wikiproject tags were extracted from the database tables maintained by the PageAssessments extension: https: //www.mediawiki.org/wiki/Extension:PageAssessments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10" coords="12,87.24,650.52,106.29,7.04;12,209.74,651.07,330.26,6.71;12,72.00,660.53,119.05,6.71"><p>For further details, see: https://meta.wikimedia.org/wiki/Research:Prioritization_of_Wikipedia_Articles/ Language-Agnostic_Quality#V1</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Coordinates: * geography (geography) object 'Africa' 'Antarctica' ... 'Oceania' * gender (gender) object 'female' 'male' 'third'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Task 1 Metric Preparation</head><p>Now that we have our alignments and qrels, we are ready to prepare the Task 1 metrics. Task 1 ignores the "unknown" alignment category, so we're going to create a kga frame (for Known Geographic Alignment), and corresponding frames for intersectional alignment. Now we need to do reset the (0,0) coordinate (full unknown), and normalize to a proportion.</p><p>q am[0, 0] = 0 q am = q am / q am.sum() q am &lt;xarray.DataArray (geography: 8, gender: Ok, now we have to -very carefully -average with our target modifier. There are three groups:</p><p>• known (use intersectional target)</p><p>• known-geo (use geo target)</p><p>• known-gender (use gender target)</p><p>For each of these, we need to respect the fraction of the total it represents. Let's compute those fractions:</p><p>q fk all = q am[1:, 1:].sum() q fk geo = q am[1:, :1].sum() q fk gen = q am[:1, 1:].sum() q fk all, q fk geo, q fk gen (&lt;xarray.DataArray ()&gt; array(0.12383613), &lt;xarray.DataArray ()&gt; array(0.79795158), &lt;xarray.DataArray ()&gt; array(0.07821229))</p><p>And now do some surgery. Weighted-average to incorporate the target for fully-known:</p><p>q tm = q am.copy() q tm[1:, 1:] *= 0.5 q tm[1:, 1:] += int tgt * 0.5 * q fk all q tm &lt;xarray. DataArray  And for known-geo:</p><p>q tm[1:, :1] *= 0.5 q tm[1:, :1] += geo tgt xa * 0.5 * q fk geo And known-gender:</p><p>q tm[:1, 1:] *= 0.5 q tm[: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.2 Geographic Alignment</head><p>Now that we've computed per-page target exposure, we're ready to set up the geographic alignment vectors for computing the per-group expected exposure with geographic data.</p><p>We're going to start by getting the alignments for relevant documents for each topic: Now we need to compute the per-query target exposures. This starst with aligning our vectors:</p><p>qp geo exp, qp geo align = qp exp.align(qp geo align, fill value=0)</p><p>And now we can multiply the exposure vector by the alignment vector, and summing by topic -this is equivalent to the matrix-vector multiplication on a topic-by-topic basis.</p><p>qp aexp = qp geo align.multiply(qp geo exp, axis=0) q geo align = qp aexp.groupby( q id ).sum() Now things get a little weird. We want to average the empirical distribution with the world population to compute our fairness target. However, we don't have empirical data on the distribution of articles that do or do not have geographic alignments.</p><p>Therefore, we are going to average only the known-geography vector with the world population. This proceeds in N steps:</p><p>1. Normalize the known-geography matrix so its rows sum to 1. 2. Average each row with the world population. 3. De-normalize the known-geography matrix so it is in the original scale, but adjusted w/ world population 4. Normalize the entire matrix so its rows sum to 1 Let's go. q geo tgt = q geo tgt.divide(q geo tgt.sum(axis=1), axis=0) q geo tgt We'll write a function very similar to the one for Task # and we multiply! q xa = q xa * p exp # normalize into a matrix (this time we don t clear) q am = q xa.sum(axis=0) q am = q am / q am.sum() # compute fractions in each section -combined with q am[0,0], this should be about 1 q fk all = q am[1:, 1:].sum() q fk geo = q am[1:, :1].sum() q fk gen = q am[:1, 1:].sum() # known average q am[1:, 1:] *= 0.5 q am[1:, 1:] += int tgt * 0.5 * q fk all # known-geo average q am[1:, :1] *= 0.5 q am[1:, :1] += geo tgt xa * 0.5 * q fk geo # known-gender average q am[:1, 1:] *= 0.5 q am[: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,87.50,448.06,452.50,9.96;12,87.50,460.02,397.71,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,384.18,448.06,155.82,9.96;12,87.50,460.02,77.99,9.96">Evaluating stochastic rankings with expected exposure</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.13157" />
	</analytic>
	<monogr>
		<title level="m" coord="12,186.92,460.62,69.37,8.80">Proc. CIKM &apos;20</title>
		<meeting>CIKM &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,479.94,452.50,9.96;12,87.50,491.90,452.50,9.96;12,87.50,503.85,452.50,9.96;12,87.50,515.81,171.63,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,386.31,479.94,153.69,9.96;12,87.50,491.90,118.81,9.96">Algorithmic fairness: Choices, assumptions, and definitions</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lum</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-statistics-042720-125902</idno>
		<ptr target="https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-042720-125902" />
	</analytic>
	<monogr>
		<title level="j" coord="12,221.69,492.50,217.71,8.80">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,535.73,452.50,9.96;12,87.50,547.69,452.51,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,268.38,535.73,147.23,9.96">Discrimination-aware data mining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pedreshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,438.79,536.34,101.21,8.80;12,87.50,548.30,353.89,8.80">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="560" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,567.61,452.50,9.96;12,87.50,579.57,164.91,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,320.10,567.61,136.40,9.96">Comparing fair ranking metrics</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Montoly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2009.01311" />
		<imprint>
			<date type="published" when="2020-09">Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,87.50,599.49,452.50,9.96;12,87.50,611.45,265.69,9.96;22,260.29,117.02,138.54,9.96;22,72.00,138.28,118.83,9.96;22,72.00,159.55,230.14,9.96;22,72.00,171.50,313.82,9.96;22,108.61,183.46,277.21,9.96;22,108.61,195.41,277.21,9.96;22,108.61,207.37,277.21,9.96;22,108.61,219.32,277.21,9.96;22,108.61,231.28,277.21,9.96;22,108.61,243.23,277.21,9.96;22,108.61,255.19,219.67,9.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12314</idno>
		<title level="m" coord="12,340.64,599.49,199.35,9.96;12,87.50,611.45,34.40,9.96;22,260.29,117.02,138.54,9.96;22,72.00,138.28,118.83,9.96;22,72.00,159.55,230.14,9.96;22,72.00,171.50,26.15,9.96">we have intersectional numbers: page xalign.sum(axis=0) &lt;xarray.DataArray (geography: 8, gender: 4)&gt; array</title>
		<imprint>
			<date type="published" when="2020-02">2020. 2.06922e+06, 8.21940e+04, 4.05772e+05, 1.85000e+02</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A taxonomy of knowledge gaps for wikimedia projects. 7.76580e+04, 1.04830e+04, 4.34670e+04, 8.00000e+00], [9.62500e+03, 0.00000e+00, 1.00000e+00, 0.00000e+00. 4.27422e+05, 3.79980e+04, 1.35310e+05, 2.10000e+01], [7.65203e+05, 9.67970e+04, 4.27747e+05, 6.30000e+01. 1.01464e+05, 1.61660e+04, 6.77640e+04, 4.00000e+00], [7.21244e+05, 8.25430e+04, 3.30205e+05, 1.59000e+02], [9.26820e+04, 1.45240e+04, 5.07260e+04, 2</note>
</biblStruct>

<biblStruct coords="22,72.00,267.14,62.76,9.96;22,82.46,279.10,334.74,9.96;22,82.46,291.05,41.84,9.96;22,150.46,291.05,256.29,9.96;22,86.94,312.32,259.28,9.96;22,72.00,333.58,211.52,9.96;22,72.00,354.84,230.14,9.96;22,72.00,366.80,292.90,9.96;22,108.61,378.76,256.29,9.96;22,108.61,390.71,256.29,9.96;22,108.61,402.67,256.29,9.96;22,108.61,414.62,256.29,9.96;22,108.61,426.58,256.29,9.96;22,108.61,438.53,183.06,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="22,72.00,267.14,62.76,9.96;22,82.46,279.10,334.74,9.96;22,82.46,291.05,41.84,9.96;22,150.46,291.05,256.29,9.96;22,86.94,312.32,259.28,9.96;22,72.00,333.58,211.52,9.96;22,72.00,354.84,36.61,9.96">Coordinates: * geography (geography) object &apos;Unknown&apos; &apos;Africa&apos; ... &apos;Oceania&apos; * gender (gender) object &apos;unknown&apos; &apos;female&apos; &apos;male&apos; &apos;third&apos; And make sure combination with targets work as expected: (page xalign.sum(axis=0) + int tgt) * 0.5 &lt;xarray</title>
	</analytic>
	<monogr>
		<title level="j" coord="22,113.84,354.84,47.07,9.96">DataArray</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>geography: 7, gender: 3)&gt; array([[5.24153838e+03, 2.17335384e+04, 4.00077535e+00. 3.82199400e-08, 5.00000038e-01, 7.72120000e-10. 1.89991486e+04, 6.76551486e+04, 1.05030010e+01. 4.83985257e+04, 2.13873526e+05, 3.15005183e+01. 8.08302131e+03, 3.38820213e+04, 2.00043049e+00. 4.12715123e+04, 1.65102512e+05, 7.95002481e+01], [7.26200132e+03, 2.53630013e+04</note>
</biblStruct>

<biblStruct coords="34,92.92,457.06,62.76,9.96;34,72.00,469.01,83.69,9.96;34,72.00,480.97,83.69,9.96;34,72.00,492.92,83.69,9.96;34,72.00,504.88,83.69,9.96;34,72.00,516.83,83.69,9.96;34,72.00,528.79,83.69,9.96;34,72.00,540.74,83.69,9.96;34,72.00,552.70,83.69,9.96;34,72.00,564.65,83.69,9.96;34,72.00,576.61,83.69,9.96;34,72.00,588.56,83.69,9.96;34,72.00,612.47,115.07,9.96;34,86.94,631.88,42.68,9.96;34,72.00,651.28,467.18,9.96;34,72.00,663.24,363.20,9.96;34,72.00,682.64,313.82,9.96;42,72.00,74.49,402.74,9.96;42,72.00,98.40,120.30,9.96;42,72.00,120.32,220.51,9.96;42,72.00,132.27,210.05,9.96;42,72.00,154.19,339.35,9.96;42,270.75,166.15,110.67,9.96;42,270.75,178.10,56.07,9.96;42,72.00,190.06,363.20,9.96;42,72.00,211.98,282.44,9.96;42,72.00,233.89,328.89,9.96;42,265.52,245.85,110.67,9.96;42,265.52,257.80,50.84,9.96;42,72.00,269.76,352.74,9.96;42,72.00,291.68,282.44,9.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="34,86.94,631.88,42.68,9.96;34,72.00,651.28,467.18,9.96;34,72.00,663.24,363.20,9.96;42,86.23,154.19,325.12,9.96;42,270.75,166.15,110.67,9.96;42,270.75,178.10,56.07,9.96;42,72.00,190.06,363.20,9.96;42,72.00,211.98,282.44,9.96;42,72.00,233.89,241.43,9.96">train metric = metrics.Task2Metric(train qrels.set index( id ), page xa df, page work, train qtgt) binpickle.dump(t2 train metric, task2-train-metric.bpk , codec=codec) INFO:binpickle.write:pickled 1879 bytes with 9 buffers t2 eval metric = metrics.Task2Metric(eval qrels</title>
		<idno>INFO:binpickle.write:pickled 1493808204 bytes with 5 buffers 150 0.017971 0.058074 0.000103 0.009900 0.000547 0.013698 1.071915e-03</idno>
		<imprint/>
	</monogr>
	<note>And save: t1 train metric = metrics.Task1Metric(train qrels.set index( id ), page kia, train qtarget) binpickle.dump(t1 train metric, task1-train-metric.bpk , codec=codec). set index( id ), page xa df, page work, eval qtgt) binpickle.dump(t2 eval metric, task2-eval-metric.bpk , codec=codec) INFO:binpickle.write:pickled 1875 bytes with 9 buffers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
