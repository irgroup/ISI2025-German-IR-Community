<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,219.56,112.60,172.88,14.93">Overview of TREC 2021</title>
				<funder>
					<orgName type="full">NIST</orgName>
				</funder>
				<funder>
					<orgName type="full">TREC Overview</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,275.77,144.91,60.46,10.37"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
						</author>
						<author>
							<persName coords="2,78.38,189.78,43.96,7.77;2,342.28,189.78,40.21,7.77"><forename type="first">Elmdoc</forename><forename type="middle">B V</forename><surname>Technische</surname></persName>
						</author>
						<author>
							<persName coords="2,384.72,189.78,62.19,7.77"><forename type="first">Hochschule</forename><surname>Koln</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group Sabanci University Assoc para</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Investigacao em Valor e Inovacao Tecnologica em Saude Siena College Institute for Artificial Intelligence Bosch Center for AI Spotify CMU Language Technologies Institute State</orgName>
								<orgName type="institution">University of Campinas</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">HES-SO Charles University TDMINER Chinese Information Processing Lab TU</orgName>
								<orgName type="institution">CNR Swiss Institute of Bioinformatics</orgName>
								<address>
									<settlement>Wien</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Industrial Research Organisation</orgName>
								<orgName type="institution">Commonwealth Scientific</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Technical University of Valencia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">RMIT/ITTC AI MedTech IBM Research Universidad del Pais Vasco IHS Markit University College Dublin Indian Institute of Technology Delhi University College London Information Technologies Institute CERTH University Hospital Essen Jeonbuk</orgName>
								<orgName type="laboratory">Fernuniversitat in Hagen The University of Queensland Gonzaga University U. of Applied Sciences &amp; Arts of Western Switzerland Hamad Bin Khalifa University UniMelb</orgName>
								<orgName type="institution">National University University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="laboratory">L3S Research Center</orgName>
								<orgName type="institution" key="instit1">Leibniz University University of Cincinnati College of Medicine MLIA team -LIP6 -Sorbonne</orgName>
								<orgName type="institution" key="instit2">Universite University of Duisburg</orgName>
								<address>
									<settlement>Essen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<orgName type="institution">University of Geneva Middlebury College University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">Institute of Technology University of North Texas OpenSource Connections</orgName>
								<orgName type="laboratory">Lab University of Maryland Naver Labs Europe University of Milano-Bicocca New</orgName>
								<orgName type="institution">National Institute of Standards and Technology R&amp;R Group University of La Rochelle National Taiwan University NLP</orgName>
								<address>
									<country key="JE">Jersey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">University of Padova</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="department">de Compostela Politecnico di Torino</orgName>
								<orgName type="institution">PingAn Smart Health University of Santiago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">University of Stavanger Poznan University of Technology University of Tsukuba</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">University University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="laboratory">Weimar Radboud University Wilfrid Laurier University Research Ctr for IT Innovation</orgName>
								<orgName type="institution">RMIT University Webis@Halle</orgName>
								<address>
									<settlement>Leipzig</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="institution">Academia Sinica York University Ryerson University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,219.56,112.60,172.88,14.93">Overview of TREC 2021</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9DECE3AA82B9B97A652DED97688E243</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC 2021 is the thirtieth edition of the Text REtrieval Conference (TREC). The main goal of TREC is to create the evaluation infrastructure required for large-scale testing of information retrieval (IR) technology. This includes research on best methods for evaluation as well as development of the evaluation materials themselves. "Retrieval technology" is broadly interpreted to include a variety of techniques that enable and/or facilitate access to information that is not specifically structured for machine use. The TREC 2021 meeting was held at the National Institute of Standards and Technology (NIST) November 15-19, 2021.</p><p>Each TREC is organized around a set of focus areas called "tracks". A track has a motivating use case, which is generally an abstraction of a user task. TREC 2021 contained eight tracks: Clinical Trials: The Clinical Trials track looks to focus research on matching patient health records to suitable clinical trials for that patient. Conversational Assistance: The Conversation Assistance (CAsT) track looks to build systems that engage users in open-domain, information-centric, conversational dialogues. Deep Learning: The Deep Learning track focuses on IR tasks where a large training set is available, allowing us to compare a variety of retrieval approaches including deep neural networks and strong non-neural approaches, to see what works best in a large-data regime.</p><p>Fair Ranking: The Fair Ranking track focuses on building two-sided systems that offer fair exposure for producers of ranked content while ensuring high results quality for ranking consumers.</p><p>Health Misinformation: The Health Misinformation track aims to (1) provide a venue for research on retrieval methods that promote better decision making with search engines, and (2) develop new online and offline evaluation methods to predict the decision making quality induced by search results. Consumer health information is used as the domain of interest in the track.</p><p>Incident Streams: The Incident Streams track is designed to develop technologies to automatically process social media streams during emergency situations, with the aim of categorizing information and aid requests made on social media for emergency service operators.</p><p>News: The News track is run in partnership with The Washington Post, it looks to develop test collections that support the search needs of news readers and news writers in the current news environment.</p><p>Podcast: The Podcasts track seeks to develop methods for information retrieval and content understanding from opendomain podcast transcripts and audio.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Seventy-three groups from twenty-seven different countries participated in TREC 2021. Table <ref type="table" coords="1,469.36,623.85,4.98,8.64">1</ref> lists the participating organizations.</p><p>This paper serves as an introduction to the research described in detail in the remainder of the proceedings. The next section provides a summary of the retrieval background knowledge that is assumed in the other papers. Section 3 presents a short description of each track-a more complete description of a track can be found in that track's overview paper in the proceedings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test collections</head><p>Text retrieval has a long history of using retrieval experiments on test collections to advance the state of the art <ref type="bibr" coords="3,515.87,97.74,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="3,529.21,97.74,7.19,8.64" target="#b7">8]</ref>, and TREC continues this tradition. A test collection is an abstraction of an operational retrieval environment that provides a means for researchers to explore the relative benefits of different retrieval strategies in a laboratory setting. Test collections consist of three parts: a set of documents, a set of information needs (called topics in TREC), and relevance judgments, an indication of which documents should be retrieved in response to which topics. We call the result of a retrieval system executing a task on a test collection a run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Documents</head><p>The document set of a test collection should be a sample of the kinds of texts that will be encountered in the operational setting of interest. It is important that the document set reflect the diversity of subject matter, word choice, literary styles, document formats, etc. of the operational setting for the retrieval results to be representative of the performance in the real task. The initial TREC test collections contained 2 to 3 gigabytes of text and 500,000 to 1,000,000 documents. The document sets used in various tracks throughout the years have been smaller and larger than these initial sets depending on the needs of the track and the availability of data, but the general trend has been toward ever-larger document sets to enhance the realism of the evaluation tasks. Similarly, the initial TREC document sets consisted mostly of newspaper or newswire articles, but later document sets have included a much broader spectrum of document types (such as recordings of speech, web pages, scientific documents, blog posts, email messages, and business documents). Each document is assigned an unique identifier called the DOCNO. For most document sets, high-level structures within a document are tagged using a mark-up language such as SGML or HTML, or broken into fields of a JSON object. In keeping with the spirit of realism, the text is kept as close to the original as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Topics</head><p>TREC distinguishes between a statement of information need (the topic) and the data structure that is actually given to a retrieval system (the query). The TREC test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of the criteria that make a document relevant. What is now considered the "standard" format of a TREC topic statement-a topic id, a title, a description, and a narrative-was established in TREC-5 <ref type="bibr" coords="3,166.20,427.26,24.90,8.64">(1996)</ref>. But topic formats vary in support of the task, and few current TREC tasks use topics in this traditional format.</p><p>Participants are (usually) free to use any method they wish to create queries from the topic statements. TREC distinguishes among two major categories of query construction techniques, automatic methods and manual methods. An automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. The definition of manual query construction methods is very broad, ranging from simple tweaks to an automatically derived query, through manual construction of an initial query, to multiple query reformulations based on the document sets retrieved. Since these methods require radically different amounts of (human) effort, care must be taken when comparing manual results to ensure that the runs are truly comparable.</p><p>TREC topics are generally constructed specifically for the task they are to be used in. When outside resources such as search engine logs are used as a source of topics the sample selected for inclusion in the test set is vetted to insure there is a reasonable match with the document set (i.e., neither too many nor too few relevant documents). Topics developed at NIST are created by the NIST assessors, the set of people hired to both create topics and make relevance judgments. Most of the NIST assessors are retired intelligence analysts. The assessors receive track-specific training by NIST staff for both topic development and relevance assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Relevance judgments</head><p>Relevance judgments turn a set of documents and topics into a test collection. Given a set of relevance judgments, the ad hoc retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. Most of the traditional measures of retrieval effectiveness treat relevance judgments as binary indicators-either a document is relevant to the topic or it is not-and the judgments themselves are binary in the original TREC collections. Use of evaluation measures that incorporate different levels (or grades) of relevance has become much more prevalent in recent TRECs, and today relevance judgments are generally made on a graded scale to support the use of these measures.</p><p>Relevance is inherently subjective. Relevance judgments are known to differ across judges and for the same judge at different times <ref type="bibr" coords="4,141.58,123.30,10.58,8.64" target="#b5">[6]</ref>. Furthermore, a set of static relevance judgments makes no provision for the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. Despite the idiosyncratic nature of relevance, test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments <ref type="bibr" coords="4,305.19,159.16,10.58,8.64" target="#b8">[9]</ref>.</p><p>The relevance judgments in the first retrieval test collections were complete. That is, a relevance decision was made for every document in the collection for every topic. The size of the TREC document sets makes complete judgments infeasible, so by necessity TREC collections are created by judging only a subset of the document collection for each topic and then estimating the effectiveness of retrieval results from the judged sample.</p><p>"Pooling" is the technique used in early TRECs for selecting the sample of documents for the human assessor to judge <ref type="bibr" coords="4,96.48,230.89,10.58,8.64" target="#b6">[7]</ref>. In pooling, the top results from a set of runs are combined to form the pool and only those documents in the pool are judged. Runs are subsequently evaluated assuming that all unpooled (and hence unjudged) documents are not relevant. In more detail, the TREC pooling process proceeds as follows. When participants submit their retrieval runs to NIST, they rank their runs in the order they prefer them to be judged. NIST chooses a number of runs to be merged into the pools, and selects that many runs from each participant respecting the preferred ordering. For each selected run, the top X documents per topic are added to the topics' pools.</p><p>The critical factor in pooling is that unjudged documents are assumed to be not relevant when computing traditional evaluation scores such as mean average precision (MAP). This treatment is a direct result of the original premise of pooling: that by taking top-ranked documents from sufficiently many, diverse retrieval runs, the pool will contain the vast majority of the relevant documents in the document set. If this is true, then the resulting relevance judgment sets will be "essentially complete", and the evaluation scores computed using the judgments will be very close to the scores that would have been computed had complete judgments been available.</p><p>Various studies have examined the validity of pooling's premise in practice. Harman <ref type="bibr" coords="4,428.54,374.36,11.62,8.64" target="#b4">[5]</ref> and Zobel <ref type="bibr" coords="4,485.86,374.36,16.60,8.64" target="#b9">[10]</ref> independently showed that early TREC collections in fact had unjudged documents that would have been judged relevant had they been in the pools. But, importantly, the distribution of those "missing" relevant documents was highly skewed by topic (a topic that had lots of known relevant documents had more missing relevant), and uniform across runs. Zobel demonstrated that these "approximately complete" judgments produced by pooling were sufficient to fairly compare retrieval runs. Using the leave-out-uniques (LOU) test, he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. For the TREC-5 ad hoc collection, he found that using the unique relevant documents increased a run's 11 point average precision score by an average of 0.5 %. The maximum increase for any run was 3.5 %. The average increase for the TREC-3 ad hoc collection was somewhat higher at 2.2 %.</p><p>As document sets continue to grow, the proportion of documents contained in standard-sized pools shrinks. At some point, pooling's premise must become invalid. The test collection created in the Robust and HARD tracks in TREC 2005 showed that this point is not at some absolute pool size, but rather when pools are shallow relative to the number of documents in the collection <ref type="bibr" coords="4,230.14,541.73,10.58,8.64" target="#b1">[2]</ref>. With shallow pools, the sheer number of documents of a certain type fill up the pools to the exclusion of other types of documents. This produces judgments sets that are biased against runs that retrieve the less popular document type, resulting in an invalid evaluation.</p><p>Several TREC tracks have investigated new ways of sampling from very large documents sets to obtain judgment sets that support fair evaluations. The primary goal of the Terabyte track that was part of TRECs 2004-2006 was to investigate new pooling strategies to build reusable, fair collections at a reasonable cost despite collection size. The Million Query track (TRECs 2007-2009) was a successor to the Terabyte track in that it had the same goal, but a different approach. The Common Core track of TREC 2017 and 2018 used multi-arm bandit optimization techniques to select documents to be judged. TRECs since 2019 have experimented with a different approach based on the University of Waterloo's HiCAL <ref type="bibr" coords="4,208.97,649.33,11.62,8.64" target="#b0">[1]</ref> system to select the judgment set in the Deep Learning track. Each of these methods reduces the number of relevance judgments made, but can bias the test collection more towards submitted systems, introduce logistical challenges, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>Retrieval runs on a test collection can be evaluated in a number of ways. In TREC, ad hoc tasks that use pooling are evaluated using the trec eval package <ref type="bibr" coords="5,277.29,109.70,10.58,8.64" target="#b2">[3]</ref>. This package reports about 85 different numbers for a run, including recall and precision at various cut-off levels plus single-valued summary measures that are derived from recall and precision. Precision is the proportion of retrieved documents that are relevant (number-retrieved-andrelevant/number-retrieved), while recall is the proportion of relevant documents that are retrieved (number-retrievedand-relevant/number-relevant). A cut-off level is a rank that defines the retrieved set; for example, a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. The trec eval program reports the scores as averages over the set of topics where each topic is equally weighted. (An alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. Evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important.)</p><p>Precision reaches its maximal value of 1.0 when only relevant documents are retrieved, and recall reaches its maximal value (also 1.0) when all the relevant documents are retrieved. Note, however, that these theoretical maximum values are not obtainable as an average over a set of topics at a single cut-off level because different topics have different numbers of relevant documents. For example, a topic that has fewer than ten relevant documents will have a precision score at ten documents retrieved less than 1.0 regardless of how the documents are ranked. Similarly, a topic with more than ten relevant documents must have a recall score at ten documents retrieved less than 1.0. For a single topic, recall and precision at a common cut-off level reflect the same information, namely the number of relevant documents retrieved. At varying cut-off levels, recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TREC 2021 Tracks</head><p>TREC's track structure began in <ref type="bibr" coords="5,206.16,362.85,61.84,8.64">TREC-3 (1994)</ref>. The tracks serve several purposes. First, tracks act as incubators for new research areas: the first running of a track often defines what the problem really is, and a track creates the necessary infrastructure (test collections, evaluation methodology, etc.) to support research on its task. The tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. Finally, the tracks make TREC attractive to a broader community by providing tasks that match the research interests of more groups. Table <ref type="table" coords="5,233.82,422.63,4.98,8.64" target="#tab_0">2</ref> lists the different tracks that were in each TREC, the number of groups that submitted runs to that track, and the total number of groups that participated in each TREC.</p><p>This section describes the tasks performed in the TREC 2021 tracks. See the track reports later in these proceedings for a more complete description of each track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clinical Trials</head><p>The Clinical Trials track is the successor to Precision Medicine, and is new in 2021. The goal is to identify appropriate clinical trials given a (mock) patient's electronic health record. The vast majority of clinical trials fail to recruit sufficient patients or to recruit them in time for the study. If patients can be matched to appropriate clinical trials efficiently then more trials may be run. There is already sizeable research on the trial matching problem using structured health record data, and so the Clinical Trials track focuses on what can be done with natural language text appearing in the record.</p><p>The document collection for this track is a snapshot of the clinicaltrials.gov registry from April, 2021, with 375,000 clinical trial descriptions in XML format. The seventy-five topics each consist of a 5-10 sentence patient case description that mimics an admission statement that would be found in an electronic health record. These case descriptions were composed by medical clinicians and not taken from real people's records. Figure <ref type="figure" coords="5,461.56,616.00,4.98,8.64" target="#fig_0">1</ref> shows an example topic.</p><p>Assessments for this track were done by the Department of Medical Informatics at Oregon Health and Science University. The four highest-priority runs from each team were pooled to a depth of 10, for a total of 35,832 relevance judgments. Retrieved trials were judged as either "eligible", meaning that the patient met the inclusion criteria and did not meet any exclusion criteria; "excluded", meaning that the patient met the inclusion criteria but was excluded by one or more exclusion criteria; or "not relevant". The main metric for Clinical Trials is nDCG at ranks 5 and 10, with a gain value of 2 for "eligible" documents and 1 for "excluded" documents. Precision at ranks 5 and 10, R-Precision, and mean reciprocal rank were also reported, counting only "eligible" documents as relevant. This reflects the behavior of real users, who are extremely dissatisfied when shown trials that they are explicitly excluded from. 113 runs were submitted to the Clinical Trials track from 26 Patient is a 45-year-old man with a history of anaplastic astrocytoma of the spine complicated by severe lower extremity weakness and urinary retention s/p Foley catheter, high-dose steroids, hypertension, and chronic pain. The tumor is located in the T-L spine, unresectable anaplastic astrocytoma s/p radiation. Complicated by progressive lower extremity weakness and urinary retention. Patient initially presented with RLE weakness where his right knee gave out with difficulty walking and right anterior thigh numbness. MRI showed a spinal cord conus mass which was biopsied and found to be anaplastic astrocytoma. Therapy included field radiation t10-l1 followed by 11 cycles of temozolomide 7 days on and 7 days off. This was followed by CPT-11 Weekly x4 with Avastin Q2 weeks/ 2 weeks rest and repeat cycle. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conversational Assistance</head><p>The Conversational Assistance track (CAsT) started in 2019. Its focus is on systems that support conversational information seeking such as automated personal assistants. Such systems need to be able to maintain information about the state of the dialogue ("context") to properly interpret the current information need.</p><p>The system task is operationalized as a passage retrieval task where each topic consisted of a series of questions and systems returned a ranked list of passages for each question ( "turn") in the series. The topics were developed by the track organizers, who provided a raw version of the series, which was the version required to be used for automatic runs; a manually rewritten version that made the implicit context explicit in each question; and an automatically rewritten version using a T5 transformer. Figure <ref type="figure" coords="7,269.88,372.23,4.98,8.64">2</ref> shows three turns with all three question versions for an example topic from the test set. The manual version of the series was used by the assessors during judging and by some manual runs submitted to the track, and can be used as training data for future systems. The test set consisted of 25 topics with a mean of nine turns per topic.</p><p>The document set used in the track was the union of MS MARCO, <ref type="foot" coords="7,356.13,418.39,3.49,6.05" target="#foot_0">1</ref> the KILT benchmark version of Wikipedia,<ref type="foot" coords="7,536.02,418.39,3.49,6.05" target="#foot_1">2</ref> and the TREC Washington Post collection, version 4. <ref type="foot" coords="7,283.95,430.34,3.49,6.05" target="#foot_2">3</ref> Systems returned a ranked list of up to 1000 passages for each question.</p><p>A problem was uncovered at the start of assessing: the track-provided tool to break the collections into passages produced different output depending on the version of a library used. Since canonical passage numbering is essential for valid evaluation and there was no way of knowing which numbering was used in which runs, we were forced to assess documents rather than passages. Assessing documents takes more time, which in turn forced us to use smaller pools than originally planned. The final relevance judgments (qrels) are based on depth-7 pools across all submissions, including baselines provided by the organizers. Not all questions in all topics were able to be judged; please see the Conversational Assistance track overview for details.</p><p>NIST assessors made judgments for nineteen topics, with most of those topics judged through question eight.</p><p>Each passage was judged on the 5-point scale used in 2020:</p><p>4 Fully Meets: The passage is a perfect answer for the turn. It includes all of the information needed to fully answer the turn in the conversation context. It focuses only on the subject and contains little extra information.</p><p>3 Highly Meets: The passage answers the question and is focused on the turn. It would be a satisfactory answer if Google Assistant or Alexa returned this passage in response to the query. It may contain limited extraneous information.</p><p>Turn 1:</p><p>Raw utterace : "I just had a breast biopsy for cancer. What are the most common types?"</p><p>Manual Rewrite : "I just had a breast biopsy for cancer. What are the most common types of breast cancer?" Automatic Rewrite : "What are the most common types of cancer in regards to breast biopsy?" Turn 2:</p><p>Raw utterace : "Once it breaks out, how likely is it to spread?"</p><p>Manual Rewrite : "Once it breaks out, how likely is lobular carcinoma breast cancer to spread?"</p><p>Automatic Rewrite : "Once the cancer breaks out, how likely is it to spread?" Turn 3:</p><p>Raw utterace : "How deadly is it?"</p><p>Manual Rewrite : "How deadly is lobular carcinoma in situ?"</p><p>Automatic Rewrite : "How deadly is LCIS?" Figure <ref type="figure" coords="8,101.66,348.06,3.88,8.64">2</ref>: An example question sequence from the CAsT test set. Automatic systems chose to process either the raw questions or the automatically-rewritten questions. Manual systems could use the manually-rewritten questions.</p><p>Assessors judged against the manually-rewritten questions.</p><p>2 Moderately Meets: The passage answers the turn, but is focused on other information that is unrelated to the question. The passage may contain the answer, but users will need extra effort to pick the correct portion. The passage may be relevant, but it may only partially answer the turn, missing a small aspect of the context.</p><p>1 Slightly Meets: The passage includes some information about the turn, but does not directly answer it. Users will find some useful information in the passage that may lead to the correct answer, perhaps after additional rounds of conversation (better than nothing).</p><p>0 Fails to Meet: The document is not relevant to the question and is unrelated to the target query.</p><p>The rubric was adjusted to refer to documents. The main effect of this was that concerns regarding "extraneous information" were ignored.</p><p>Each submission was processed such that the first occurrence of a document in a ranked list was retained and all other occurrences removed. Removing a document moved subsequent documents higher in the ranking and shortened the overall list to fewer than the allowed maximum number retrieved (1000). The de-duplicateds ranked lists were used for pooling and were the lists evaluated.</p><p>The main evaluation measures for the track are nDCG@3, nDCG@5, NDCG@500, and AP@500. (500 was used instead of 1000 due to the shortening of ranked lists by de-duping.)</p><p>The track received 50 runs from 15 participating teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Learning</head><p>The Deep Learning track focuses on a traditional ad hoc retrieval task in an environment where there is a large, labeled training set. The goal is to explore the trade-offs between neural-and dense-retrieval approaches on the one hand and traditional rankers on the other. In 2021, the track refreshed the corpus to unify the document and passage datasets.  This presented a significant challenge to participants as the new data was made available only a month before the submission deadline.</p><p>The track had two tasks, Document Ranking and Passage Ranking. For each task, participants could either do their own retrieval from the full collection or re-rank an initial retrieved set provided by the track organizers. Both tasks used the same set of 477 test questions, a sample of which are shown in Figure <ref type="figure" coords="9,388.02,294.44,3.74,8.64" target="#fig_1">3</ref>.</p><p>Both tasks used version 2 of the MS MARCO dataset. <ref type="foot" coords="9,312.81,304.73,3.49,6.05" target="#foot_3">4</ref> For the Passage Ranking task, the document set was about 140 million passages extracted from web pages using an algorithm to try to identify the most promising passage independent of a query. The Document Ranking task set was around 11 million documents. There was a set of training queries and relevance judgments, two sets of development queries and judgments, and the queries and judgments from TREC 2019 and 2020 as validation data.</p><p>Partly because of the shift from the data used on the official MS MARCO leaderboard to the new version of the collection, there were new rules about what data was permitted to be used by participants. The passage-document mapping was permitted. The ORCAS click data <ref type="foot" coords="9,265.09,388.41,3.49,6.05" target="#foot_4">5</ref> was prohibited, as well as any information that mapped documents and passages in the new collection back to the old collection. Aside from ORCAS, the topics and relevance judgments from previous DL track were permitted. Participants were prohibited from using other MS MARCO resources, such as the QnA or NLGEN data. A segmented document collection and an augmented passage collection were provided by the organizers.</p><p>One goal of the track was to create traditional ad hoc test sets from this data. To this end, NIST assessors created much more dense judgments for much smaller sets of questions. NIST originally selected 57 questions from the test set of 477 to be judged for both tasks. The topics were selected by observing the behavior of submitted Document Ranking task runs on the entire test set when using the sparse MS MARCO judgments to evaluate runs. Test questions that had median (across submissions) mean reciprocal rank (MRR) scores of 0.0 or 1.0 were eliminated. Queries for assessment were randomly selected from the remainder such that half were long queries (ten or more words) and half were short.</p><p>After question selection, we pooled the top 10 results across all runs in the task. The assessor judged these pool documents first, then another 20-25 documents selected to be judged using the University of Waterloo's CAL <ref type="bibr" coords="9,528.39,545.50,11.62,8.64" target="#b0">[1]</ref> system. <ref type="foot" coords="9,102.17,555.79,3.49,6.05" target="#foot_5">6</ref> CAL uses the current set of judgments to build a relevance model using very basic tokenization and logistic regression, and then selects the unjudged document most likely to be relevant as the next document to judge. A topic continued to be assessed using CAL if</p><p>• more than half of the documents judged for the topic so far had been judged relevant; or • if fewer than 150 documents had been judged so far; or • if the topic had not yet had any documents suggested by CAL judged (that is, each topic had at least one CAL iteration); or</p><p>• if more than 20% (10% in early iterations) of the documents in the most recent previous iteration had been judged relevant.</p><p>Once a topic exited the process, it was never restarted. Assessment for the passage ranking task followed the same procedure and started with the same topics. All 57 topics completed the process for document ranking, but for passage ranking, four were dropped as they had less than five relevant passages. Documents in the Document Ranking task were judged on a four-point scale of Irrelevant (0), Relevant (1), Highly Relevant (2), and Perfect (3) where all but Irrelevant were treated as relevant in CAL and in computing binaryrelevance-based measures. For the Passage Ranking task, passages were judged on a four-point scale of Irrelevant (0), Related (the passage is on-topic but does not answer the question) (1), Highly Relevant (2), and Perfect (3). In this task, only Highly and Perfectly Relevant were considered to be relevant for binary measures and by CAL, though nDCG scores did use a gain value of 1 for the Related passages.</p><p>Most topics have very many relevant (presumably because the document set is much larger than in previous years), and as a result the collection is likely not a reusable collection. A variant of the "Leave-Out-Uniques" test <ref type="bibr" coords="10,505.49,258.79,16.60,8.64" target="#b9">[10]</ref> that accommodates the use of CAL shows that some Document Ranking task submissions experience large swings in evaluated effectiveness if their team's uniquely-retrieved relevant documents are removed from the top-10 pools. This suggests future systems may not be evaluated fairly with that collection, and similar concerns are appropriate for the Passage Ranking task collection, too. Evaluation results that might be biased can be detected by the presence of many unjudged passages in a ranking. This does not affect track submissions for high-precision measures such as P@10 or ndcg@10 since all submissions contributed to the depth-10 pools. But so many relevant means these high-precision measures obtain very high scores. While that can be gratifying, it also makes the evaluation less stable because the effective number of topics that are distinguishing the runs from one another in the average is reduced.</p><p>Twenty teams participated in the Deep Learning track, submitting 63 Passage Ranking runs and 66 Document Ranking runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fair Ranking</head><p>The Fair Ranking track recognizes that search systems affect both consumers and producers of information, and looks to build systems that provide fair exposure of producers while still returning relevant information. The initial iterations of the track used academic papers from Semantic Scholar, and sought to encourage the development of systems that would rank search results fairly with respect to pre-specified groups of authors, and then in 2020 against arbitrary group definitions.</p><p>For 2021, the track initiated a partnership with the Wikimedia Foundation. The track considered two types of users in the Wikipedia environment. The first type of user is a WikiProject coordinator; this is a person or group of people who shepherd a project such as Wikipedia, Wikidata, or any of the other projects hosted at the Foundation. Coordinators keep an eye on the overall direction and quality of the project, and part of that is keeping tabs on pages that need to be worked on. This user was the model for the track's first task. Task two represented a second type of user, a Wikipedia editor who may be looking for pages that would benefit from their effort.</p><p>Both tasks shared the same documents, topics, and definition of fairness. The documents were English Wikipedia articles, and the topics focused on topics in individual WikiProjects that would have relevant pages in Wikipedia. Fairness of exposure was with respect to geographic focus and (in the case of biography pages) the gender of the subject of the page. An automated measure of article quality was also provided as a characteristic that should not receive fair exposure.</p><p>For the Coordinators task, the output of the search was a single ranking of pages that were both relevant to the project and fair with respect to exposure across the fairness categories. For the Editors task, the output was a sequence of rankings where systems needed to identify relevant pages that needed work, in addition to the fairness criterion. The notion of a set of rankings comes from the idea that a project would have a "standing query" representing information relevant to the project, and different editors would receive different rankings targeted to them, and that whole set of rankings was a candidate for exposure fairness. id a topic identifier title the WikiProject title keywords a collection of search keywords forming the query text scope a textual description of the project scope, from its project page homepage the URL for the WikiProject. This is provided for attribution only and is not intended for system use. NIST assessors reviewed pooled Wikipedia pages for relevance. There were two sets of pools: a depth-20 pool of Task 1 rankings, and a depth-5 pool of the first 25 Task 2 rankings from a submission, for 75% of the queries. Quality, gender, and geography categories were sourced automatically.</p><p>The metrics for the Fair Ranking track attempt to measure the degree to which the system retrieved relevant documents, and exposed documents in a way that is fair across the fairness categories (geography and gender). One difficulty discussed in the overview paper is that Wikipedia itself has biases in these categories, and as such measuring fairness here is difficult.</p><p>Task 1 used attention-weighted rank fairness as a metric, which accumulates exposure across fairness groups. This metric was multiplied by nDCG for relevance to produce the final metric. Task 2 computed expected exposure. Metrics for fairness are an active area of research and we refer the reader to the track overview for more details.</p><p>The track received 24 runs from four participating teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Health Misinformation</head><p>This is the third year of the Health Misinformation track, a track that investigates how search engines can support sound decision making. <ref type="foot" coords="11,166.95,366.59,3.49,6.05" target="#foot_6">7</ref> The track used search for consumer health information over web documents as its use case since this is a common, important scenario. As operationalized in the track, the search task is similar to a traditional ad hoc search task, with the difference that retrieving on-topic but incorrect information is more strongly penalized than simply retrieving non-relevant information.</p><p>The document set used in the track was HuggingFace's "noclean" version of Google's C4 dataset <ref type="foot" coords="11,480.49,414.42,3.49,6.05" target="#foot_7">8</ref> . As a typical web collection, it contains documents of widely varying credibility and quality, including documents that actively try to mislead.</p><p>Fifty topics were created by track organizers. Two example topics are shown in Figure <ref type="figure" coords="11,448.02,451.95,3.74,8.64" target="#fig_3">5</ref>. The topic format is similar to traditional TREC topics except for the addition of three fields, "stance", "evidence", and "disclaimer". The stance indicates a position on whether the treatment is helpful or unhelpful for the health issue. The evidence gives support for that stance. The disclaimer reads, "We do not claim to be providing medical advice, and medical decisions should never be made based on the stance we have chosen. Consult a medical doctor for professional advice." The set of topics was selected to have equal representation of helpful and unhelpful issue-intervention pairs. Document judging was performed by NIST assessors. Initial judgment pools were created over all runs submitted to the track to depth 35. Documents were judged on three separate dimensions. The first dimension was Usefulness, where documents were judged on a three-point scale of not-useful (0), useful (1) , and very-useful (2) based on whether the document provided information about the health issue, the treatment, or both that would help the user make a decision about the search topic. Provided the usefulness judgment was either useful or very useful, the assessor then judged the Supportiveness and Credibility dimensions. Supportive was judged with respect to what the document itself claimed-regardless of any knowledge the assessor might have to the contrary-and was judged as either dissuades (0), neutral (1), or supportive (2). Credibility was judged as either low (0), good (1), or excellent (2) and was based on the assessor's opinion of the document taking into consideration its source and the supporting evidence it contained. <ref type="foot" coords="11,113.23,629.61,3.49,6.05" target="#foot_8">9</ref> The assessors were able to complete judgments for 35 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number:</head><p>105 Query:</p><p>put ice on a burn Description: Should I apply ice to a burn? Narrative:</p><p>Many people commonly put on ice on burns in an attempt to stop the burning and pain. A very useful document would discuss the effectiveness of using ice to treat burns. A useful document would help a user decide if putting ice on burns is a recommended treatment by providing information on recommended treatments for burns and may not discuss ice as a treatment, or a useful document may discuss benefits or concerns for application of ice to skin. Disclaimer:</p><p>We do not claim to be providing medical advice, and medical decisions should never be made based on the stance we have chosen. Vitiligo is characterized by discolored patches on the skin because the skin loses its pigment cells (melanocytes). A very useful document would discuss the effectiveness of vitamin b12 and sun exposure together for vitiligo. A useful document help a user make a decision about using the combination of vitamin B12 and sunlight to treat vitiligo, and would provide information on recommended treatments for vitiligo, use of vitamin b12 and sun exposure, or both. Disclaimer:</p><p>We do not claim to be providing medical advice, and medical decisions should never be made based on the stance we have chosen. Consult a medical doctor for professional advice. These judgments were used to create six different derived qrels files, based on different combinations of usefulness, correctness, and credibility. The resulting split is complex, because an answer is "correct" if it supports a helpful treatment or if it dissuades the use of an unhelpful treatment. In addition to reporting nDCG, precision at 10, and average precision using these qrels, the track computed a "compatibility" metric, described in the track overview and at https://github.com/claclark/Compatibility.</p><p>Seven participating teams submitted 71 runs to the Health Misinformation track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Incident Streams</head><p>The Incident Streams track is motivated by the rise of social media use during emergency situations. Emergency service operators are now expected to monitor various social media channels and respond to requests found there. However, the emergency services do not have the tools or manpower to categorize, cross-reference, and verify the information spread across multiple platforms. The track is designed to focus research on technologies to automatically process social media streams during emergencies. The track debuted in TREC 2018, where systems categorized tweets into a set of high-level information types for 15 different events. In subsequent editions, the track has had two evaluation phases, A and B, adding more annotated events in each edition. In 2021, the track distributed a test set with 26 crisis events. 2021-A produced partial assessments for a subset of events, which supported a leaderboard, and 2021-B reviewed deeper pools for more of the High-level Type Description Request-GoodsServices</p><p>The user is asking for a particular service or physical good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Request-SearchAndRescue</head><p>The user is requesting a rescue (for themselves or others).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Request-InformationWanted</head><p>The user is requesting information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CallToAction-Volunteer</head><p>The user is asking people to volunteer to help the response effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CallToAction-Donations</head><p>The user is asking people to donate goods/money.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CallToAction-MovePeople</head><p>The user is asking people to leave an area or go to another area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-FirstPartyObservation</head><p>The user is giving an eye-witness account. Report-ThirdPartyObservation The user is reporting information from someone else.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-Weather</head><p>The user is providing a weather report (current or forecast).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-EmergingThreats</head><p>Report of a potential problem that may cause future loss of life or damage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-MultimediaShare</head><p>The user is sharing images or video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-ServiceAvailable</head><p>The user is reporting that someone is providing a service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-Factoid</head><p>The user is reporting some facts, typically numerical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-Official</head><p>An official report by a government or public safety representative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-CleanUp</head><p>A report of the clean up after an event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-Hashtags</head><p>Reporting which hashtags correspond to each event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-NewSubEvent</head><p>The user is reporting a new occurrence that public safety officers need to respond to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report-Location</head><p>The post contains information about the user or observer location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other-News</head><p>The post provides/links to continuous coverage of the event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other-Advice</head><p>The author is providing some advice to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other-Sentiment</head><p>The post is expressing some sentiment about the event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other-Discussion</head><p>Users are discussing the event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other-Irrelevant</head><p>The post is irrelevant, containing no information about the event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other-ContextualInformation</head><p>The post is generic news, e.g., reporting that the event occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other-OriginalEvent</head><p>The responder already knows this information. test events. Systems were given the ontology of high-level information types shown in Figure <ref type="figure" coords="13,423.17,436.51,4.98,8.64" target="#fig_4">6</ref> plus a time-ordered stream of tweets for each event. The systems' task was to label each tweet with all of the information types that applied to it. This categorization was required to be done on-line; that is, each stream needed to be processed in order as if the event were occurring in real time, and the labels for the current tweet had to be assigned before the next tweet was processed.</p><p>NIST assessors processed each stream as well, assigning all applicable labels to each tweet. The assessors also indicated categories for images present in a tweet: whether the image was a satellite image, a weather forecast, an infographic, a photo of a weather event, if the image depicted victims, damage, crowds, or recovery efforts, if the image was a screenshot, and whether the image was informative.</p><p>Assessors also assigned an information priority level to each tweet. Runs are evaluated by how closely the set of tags the run assigned to each tweet match the set of labels the assessors assigned to that tweet, conditioned on actionable and non-actionable categories. In addition, the information priority level assigned by assessors is used to define a new metric, Accumulated Alert Worth, that aims to capture how well systems perform as an alerting service.</p><p>Five groups participated in the Incident Streams track, submitting 14 runs for the 2021-A deadline, and 17 runs for the leaderboard and 2021-B. This is the last year of the Incident Streams track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">News</head><p>The News track is coordinated in partnership with The Washington Post and looks to study the search needs of news readers and writers in the current news environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">:</head><p>The document provides essential useful background ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">:</head><p>The document must be linked otherwise critical context is missing. The primary metric for the task is nDCG@5 with the gain values as 2 r where r is the value of the relevance label except that non-relevant documents (label 0) contribute no gain.</p><p>The track only received wikification task submissions from one group, so NIST staff reviewed top-10 pools. For wikification, a pool entry was composed of an extent in the article and a target link, which could either point to a Wikipedia page or another document in the Washington Post collection. Each anchor-link pair was judged according to the following rubric:</p><p>• Is the anchor sensible, that is, does it break on word boundaries and seem reasonable lexically?</p><p>• Is the anchor useful, that is, does it identify an extent which might be usefully linked to another resource to help the reader better understand the article?</p><p>• Does the target match, that is, is the linked article what a user would reasonably expect to be linked from that anchor?</p><p>• Is the target helpful, that is, does the linked article actually provide useful background that would help the reader better understand the article?</p><p>A retrieved anchor-link pair needed to answer 'yes' to all four questions to count as relevant for the task. The track received 47 runs from 8 participants: 22 runs for background linking, 20 for the subtopics subtask, and 5 for wikification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Podcast</head><p>The Podcast track began in 2020 to explore search and summarization tasks in the podcast domain. Podcasts are audio programs distributed as digital files. A podcast comprises a series of episodes, may or may not be on a focused topic, and may have guests or just be a monologue performance by the podcast creator. Podcasts range in quality from professional audio productions to a pair of teenagers with a phone, and in topic from journalism to scientific research to improvised dialogue.</p><p>The dataset for the Podcast track was provided by Spotify, and consists of over 100,000 English-language podcast episodes. The episodes are available as audio files or as an ASR transcript generated through Google's Speech-to-Text API. Metadata includes descriptions of the podcast and of the episode, provided by the podcast creator.</p><p>The track featured two tasks, retrieval and summarization. The retrieval task topics included "query" and "description" fields. Topics were also of two types: topical, meaning a user was interested in finding podcast episodes about a topic, and known-item, where they were looking for a specific podcast episode. Figure <ref type="figure" coords="15,419.17,495.56,4.98,8.64" target="#fig_5">8</ref> shows three examples.</p><p>The episodes were automatically marked with boundaries at each minute, and systems were to retrieve two-minute segments by indicating the start of the segment. The retrieved segments were pooled to depth 20 and judged by NIST assessors for relevance along a scale of perfect-excellent-good-fair-bad. The perfect level was reserved for knownitem topics for segments that returned a perfect entry point to the desired podcast episode. For topical topics, the level indicated both the relevance of the result and the quality of the entry point.</p><p>A new twist to the retrieval task for 2021 was that systems were required to return four rankings for each topic: a normal "relevance ranking", and three re-orderings of the relevance ranking along the following criteria:</p><p>Entertaining: the segment is topically relevant and the topic is presented in a way which the speakers tend to be entertaining to the listener, rather than informative or evaluative.</p><p>Subjective: the segment is topically relevant to the topic description and the speaker or speakers explicitly and clearly express a polar opinion about the query topic, so that the approval or disapproval of the speaker is evident in the segment.</p><p>Discussion: the segment is topically relevant to the topic description and includes more than one speaker participating with non-trivial topical contribution.  All four rankings were included in the depth-20 pool. Assessors judged every pool segment against the four criteria, so as a practical matter the pool depth was more than 20 for both relevance and the re-ranking criteria.</p><p>For the summarization task, 1000 podcast episodes were provided with both audio and transcripts, and systems were required to return a textual summary as well as a one-minute audio clip. The intended use of both the summary and the clip were to help the user decide if they would want to listen to the podcast episode. No training summaries were provided, but podcasts and episodes included descriptions which served as baselines in the 2020 track. For 2021, the baseline was the transcript and audio of the first minute of the episode.</p><p>The 11 teams participating in the Podcast track submitted 27 retrieval runs and 12 summarization runs.</p><p>4 Future TREC will continue in 2022. The News track is ending, the Podcast track is taking a hiatus, and the Incident Streams track is changing to "Crisis Facts", a turn back to its temporal summarization roots but with some added twists. In addition, TREC 2022 will have a new cross-language retrieval task. Planning sessions for all tracks were included in their presentation block during the virtual program.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,178.31,206.59,255.39,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The first topic from the 2021 Clinical Trials collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,93.61,204.61,424.77,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example questions from the set of questions NIST assessors judged for the Deep Learning track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,220.96,144.84,170.08,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Structure of Fair Ranking topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,162.78,430.76,286.44,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example topics from the Health Misinformation track test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,72.00,380.55,468.00,8.64;13,72.00,392.51,267.44,8.64"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: High-level information types used in the Incident Streams 2021 track. Systems categorized tweets in an event's tweet stream by assigning one or more labels to each tweet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="16,95.18,286.11,421.65,8.64"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Three example Podcast track topics. In the test set, 40 are topical and 10 are known-item topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,76.19,82.27,463.81,505.31"><head>Table 2 :</head><label>2</label><figDesc>Number of participants per track and total number of distinct participants in each TREC</figDesc><table coords="6,76.19,103.03,463.81,484.55"><row><cell>Track</cell><cell cols="2">'92 '93 '94 '95 '96 '97 '98 '99 '00 '01 '02 '03 '04 '05 '06 '07 '08 '09 '10 '11 '12 '13 '14 '15 '16 '17 '18 '19 '20 '21</cell></row><row><cell>Ad Hoc</cell><cell>18 24 26 23 28 31 42 41</cell><cell></cell></row><row><cell>Routing</cell><cell>16 25 25 15 16 21</cell><cell></cell></row><row><cell>Interactive</cell><cell>3 11 2 9 8 7 6 6 6</cell><cell></cell></row><row><cell>Spanish</cell><cell>4 10 7</cell><cell></cell></row><row><cell>Confusion</cell><cell>4 5</cell><cell></cell></row><row><cell>Merging</cell><cell>3 3</cell><cell></cell></row><row><cell>Filtering</cell><cell>4 7 10 12 14 15 19 21</cell><cell></cell></row><row><cell>Chinese</cell><cell>9 12</cell><cell></cell></row><row><cell>NLP</cell><cell>4 2</cell><cell></cell></row><row><cell>Speech</cell><cell>13 10 10 3</cell><cell></cell></row><row><cell>Xlingual</cell><cell>13 9 13 16 10 9</cell><cell></cell></row><row><cell>High Prec</cell><cell>5 4</cell><cell></cell></row><row><cell>VLC</cell><cell>7 6</cell><cell></cell></row><row><cell>Query</cell><cell>2 5 6</cell><cell></cell></row><row><cell>QA</cell><cell>20 28 36 34 33 28 33 31 28</cell><cell>14 16 5</cell></row><row><cell>Web</cell><cell>17 23 30 23 27 18</cell><cell>26 24 16 12 15 10</cell></row><row><cell>Video</cell><cell>12 19</cell><cell></cell></row><row><cell>Novelty</cell><cell>13 14 14</cell><cell></cell></row><row><cell>Genomics</cell><cell>29 33 41 30 25</cell><cell></cell></row><row><cell>HARD</cell><cell>14 16 16</cell><cell></cell></row><row><cell>Robust</cell><cell>16 14 17</cell><cell></cell></row><row><cell>Terabyte</cell><cell>17 19 21</cell><cell></cell></row><row><cell>Enterprise</cell><cell>23 25 20 16</cell><cell></cell></row><row><cell>Spam</cell><cell>13 9 12</cell><cell></cell></row><row><cell>Legal</cell><cell cols="2">6 14 15 14 17 11</cell></row><row><cell>Blog</cell><cell cols="2">16 24 25 11 16</cell></row><row><cell>Mlln Query</cell><cell cols="2">11 7 8</cell></row><row><cell>Feedback</cell><cell cols="2">15 20 7</cell></row><row><cell>Chemical</cell><cell></cell><cell>8 4 9</cell></row><row><cell>Entity</cell><cell></cell><cell>13 16 10</cell></row><row><cell>Session</cell><cell></cell><cell>10 13 10 6 11</cell></row><row><cell>Crowd</cell><cell></cell><cell>11 8 4</cell></row><row><cell>Medical</cell><cell></cell><cell>29 24</cell></row><row><cell>Microblog</cell><cell></cell><cell>58 36 20 23 16</cell></row><row><cell>Contextual</cell><cell></cell><cell>13 19 17 12 13</cell></row><row><cell>KBA</cell><cell></cell><cell>11 13 11</cell></row><row><cell>Temporal Summ</cell><cell></cell><cell>7 6 12</cell></row><row><cell>Federated</cell><cell></cell><cell>11 12</cell></row><row><cell>Clinical</cell><cell></cell><cell>26 36 26</cell></row><row><cell>Dynamic Domain</cell><cell></cell><cell>7 6 3</cell></row><row><cell>Tasks</cell><cell></cell><cell>5 4 2</cell></row><row><cell>Recall</cell><cell></cell><cell>10 5</cell></row><row><cell>RTS</cell><cell></cell><cell>19 18</cell></row><row><cell>OpenSearch</cell><cell></cell><cell>9 3</cell></row><row><cell>CAR</cell><cell></cell><cell>7 7</cell></row><row><cell>Core</cell><cell></cell><cell>14</cell></row><row><cell>Precision Medicine</cell><cell></cell><cell>32 15 16</cell></row><row><cell>CENTRE</cell><cell></cell><cell></cell></row><row><cell>Incident Streams</cell><cell></cell><cell>15 6 5</cell></row><row><cell>News</cell><cell></cell><cell>13 10 8</cell></row><row><cell>CAsT</cell><cell></cell><cell>21 15 15</cell></row><row><cell>Misinfo</cell><cell></cell><cell>4 9 7</cell></row><row><cell>Deep Learning</cell><cell></cell><cell>16 25 20</cell></row><row><cell>Fair Ranking</cell><cell></cell><cell>5 6 4</cell></row><row><cell>Podcast</cell><cell></cell><cell>14 11</cell></row><row><cell>Clinical Trials</cell><cell></cell><cell>26</cell></row><row><cell>Participants</cell><cell cols="2">22 31 33 36 38 51 56 66 69 87 93 93 103 117 107 95 56 67 75 121 83 60 75 87 74 67 66 84 74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,78.38,75.28,458.73,116.23"><head></head><label></label><figDesc>168329 does light intensity or concentration of carbon dioxide have a higher rate of photosynthesis 300025 how many whales are caught in fishing nets off california 505390 supartz injections what does it made of 615176 what court has appellate jurisdiction circuits based on population distribution 818583 what is the difference between the range rover and the range rover sport 935353 when and where did the battle of manassas take place 952284 when is the best time to fish with a crawfish color bait 1006728 which cerebral lobe of the brain is involved in our ability to see? 1109840 what law is concerned with the safety and health conditions in the majority of private-sector industries? 1121909 what are the instruments in a woodwind quintet</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="16,133.89,74.28,344.23,188.76"><head></head><label></label><figDesc>to hear stories about smuggling. General discussion about smuggling without reference to actual events are not relevant. that Chef Samin Nosrat makes a surprise appearance on an episode of The Cut and I want to find it.</figDesc><table coords="16,133.89,74.28,344.23,176.81"><row><cell>id</cell><cell>97</cell></row><row><cell>query</cell><cell>smuggling</cell></row><row><cell>type</cell><cell>topical</cell></row><row><cell cols="2">description I want id 98</cell></row><row><cell>query</cell><cell>nobel prize laureates</cell></row><row><cell>type</cell><cell>topical</cell></row><row><cell cols="2">description I want to hear about Nobel prize laureates. Biographies including both</cell></row><row><cell></cell><cell>personal and professional life is relevant. The segment must name the</cell></row><row><cell></cell><cell>laureate to be relevant.</cell></row><row><cell>id</cell><cell>99</cell></row><row><cell>query</cell><cell>samin nosrat</cell></row><row><cell>type</cell><cell>known-item</cell></row><row><cell cols="2">description I heard</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,86.35,645.29,105.21,5.61"><p>http://www.msmarco.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,86.35,655.02,196.06,5.61"><p>https://github.com/facebookresearch/KILT/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,86.35,664.75,162.59,5.61"><p>https://trec.nist.gov/data/wapost/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="9,86.35,640.53,453.65,6.91;9,72.00,649.99,178.42,6.91"><p>http://www.msmarco.org, see https://microsoft.github.io/msmarco/TREC-Deep-Learning for details. Version 2 is not the same as the MS MARCO leaderboard dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="9,86.35,659.72,395.96,6.91"><p>ORCAS: Open Resource for Click Analysis in Search, https://microsoft.github.io/msmarco/ORCAS.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="9,86.35,669.45,397.97,6.91"><p>"CAL" refers to the active learning document selection process; "HiCAL" is a web-based tool that uses CAL under the hood.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="11,86.35,650.50,254.40,6.91"><p>In 2019 this track started as the "Decision" track, but changed its name in 2020.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="11,86.35,660.98,200.85,5.61"><p>https://huggingface.co/datasets/allenai/c4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="11,86.35,669.96,387.49,6.91"><p>There are some cases where the assessor missed a criterion by mistake; those "judgments" are indicated in the qrels as -2.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Much of the text and all of the format of this paper were taken from <rs type="person">Ellen Voorhees</rs>' 2019 <rs type="funder">TREC Overview</rs>. I am grateful to be allowed to reuse her text. Any errors in this document are my own. The descriptions of the tracks' goals and tasks were adapted from the track guidelines and overview papers authored by the track coordinators. My thanks to the coordinators who by their volunteer efforts make the variety of different tasks addressed in TREC possible.</p></div>
<div><head>Disclaimer</head><p>Certain companies or commercial products are identified in various papers in the TREC proceedings, including this one, in order to describe the TREC process adequately. Such identification is not intended to imply recommendation or endorsement by <rs type="funder">NIST</rs>, nor is it intended to imply that the companies or products identified are necessarily the best available for the purpose.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topic: 936 Title: Nora Ephron dies at 71 Desc: I'm looking for information on the passing of author and screenwriter Nora Ephron. Narr: Please provide information as to when author and screenwriter Nora Ephron died and of what causes. Biographical details of her life and career are all relevant. Reactions to her passing by friends and the media are relevant as well. Subtopics:</p><p>1. Find details of Nora Ephron's life and accomplishments.</p><p>2. What are Nora Ephron's most well-known works? 3. Did Nora Ephron know that Mark Felt was "Deep Throat" of Watergate fame?</p><p>Target article: [f831cae6-bfa4-11e1-9ce8-ff26651238d0, https://...] Nora Ephron, prolific author and screenwriter, dies at age 71 "Take notes," Nora Ephron's mother advised her as a child. "Everything is copy." Her mother, a Broadway playwright and Hollywood screenwriter, imbued Ms. Ephron with a razorsharp self-awareness and the ambition to transform workaday absurdities, cultural idiosyncrasies, romantic foibles and even marital calamity into essays, novels and films brimming with invitingly mordant wit. She credited her mother with bestowing "this kind of terrific ability, not to avoid pain but to turn it over and recycle it as soon as possible." Nora Ephron, who gained a devoted following for her perceptive, deeply personal essays and parlayed that renown into a screenwriting career of wistful romantic comedies such as "When Harry Met Sally" and "You've Got Mail," the marital exposé "Heartburn" and the whistleblower drama "Silkwood," died June 26 at a hospital in New York. She was 71. The track focuses on two tasks, background linking and wikification. 10 The document collection for both tasks is the TREC Washington Post collection. 11 Both tasks also used the same set of 51 topics. For background linking, a topic is an article from the document collection that is designated as the target article. Systems retrieve other articles that provide important context or background information for the target. The goal is to help a reader understand or learn more about the story or the main issues of the target article using the best possible sources. For wikification, the systems were to identify spans of text in the target article and propose links from those spans to either other Washington Post articles or Wikipedia. The links should ideally be those that would help the reader understand the broader context of the article; this is in contrast to other wikification tasks that seek to link all entities in a document.</p><p>The Washington Post itself makes recommendations like these, but they are manually curated. Articles frequently have interstitial links to other articles on the same story. In addition, the Post creates story-specific link boxes that appear at the end of some articles, with links to different aspects of the story. To capture this latter idea, the track included title/description/narrative blocks, and a small set of subtopics. The subtopics were not used for diversity ranking; rather, systems submitting results to the subtopics subtask of the background linking task returned a separate ranked list for each subtopic. Figure <ref type="figure" coords="14,218.87,549.25,4.98,8.64">7</ref> shows a news track topic statement with a portion of the target article.</p><p>For the background linking task (main and subtopics subtasks), systems returned up to 100 documents per topic. The top 20 documents per run were pooled for assessment, with assessors marking each pooled article on the following scale: 0 : The linked document provides little or no useful background information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">:</head><p>The linked document provides some useful background or contextual information that would help the user understand the broader story context of the query article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">:</head><p>The document provides significantly useful background ... 10 Wikification is automatically linking segments of text to other pages, in the style of Wikipedia. 11 https://trec.nist.gov/data/wapost/</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="17,93.58,97.74,446.42,8.64;17,93.58,109.52,446.42,8.82;17,93.58,121.47,350.04,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,142.77,109.70,170.42,8.64">A system for efficient high-recall retrieval</title>
		<author>
			<persName coords=""><forename type="first">Mustafa</forename><surname>Abualsaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nimesh</forename><surname>Ghelani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,334.67,109.52,205.33,8.59;17,93.58,121.47,246.21,8.82">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR &apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1317" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,141.58,446.42,8.64;17,93.58,153.36,215.69,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,377.88,141.58,162.12,8.64;17,93.58,153.53,41.85,8.64">Bias and the limits of pooling for large collections</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darrin</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,142.82,153.36,85.07,8.59">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="491" to="508" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,173.46,446.42,8.64;17,93.58,186.35,80.20,7.01" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="17,174.74,173.46,122.97,8.64">trec eval IR evaluation package</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<ptr target="https://github.com/usnistgov/trec_eval.git" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,205.34,446.42,8.64;17,93.58,217.30,141.21,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,278.87,205.34,233.02,8.64">Factors determining the performance of indexing systems</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Keen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,522.64,205.34,17.36,8.64;17,93.58,217.30,31.55,8.64">Two volumes</title>
		<imprint>
			<date type="published" when="1968">1968</date>
			<pubPlace>Cranfield, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,237.04,446.42,8.82;17,93.58,249.00,446.42,8.82;17,93.58,261.13,35.70,8.64" xml:id="b4">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,159.63,237.22,238.41,8.64;17,503.94,237.04,36.06,8.59;17,93.58,249.00,221.32,8.59">Proceedings of the Fourth Text REtrieval Conference (TREC-4)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Fourth Text REtrieval Conference (TREC-4)</meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="page" from="500" to="236" />
		</imprint>
	</monogr>
	<note>Overview of the fourth Text REtrieval Conference (TREC-4)</note>
</biblStruct>

<biblStruct coords="17,93.58,280.88,446.42,8.82;17,93.58,293.01,60.05,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,165.84,281.06,143.25,8.64">Relevance and information behavior</title>
		<author>
			<persName coords=""><forename type="first">Linda</forename><surname>Schamber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,317.62,280.88,218.03,8.59">Annual Review of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3" to="48" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,312.94,446.43,8.64;17,93.58,324.89,446.42,8.64;17,93.58,336.85,71.67,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,259.87,312.94,280.14,8.64;17,93.58,324.89,55.21,8.64">Report on the need for and provision of an &quot;ideal&quot; information retrieval test collection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,159.04,324.89,205.24,8.64">British Library Research and Development Report</title>
		<imprint>
			<biblScope unit="volume">5266</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,356.59,341.46,8.82" xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Spärck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,178.78,356.59,191.19,8.82">Information Retrieval Experiment. Butterworths</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,376.52,446.42,8.82;17,93.58,388.47,213.54,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,174.75,376.70,321.88,8.64">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,504.58,376.52,35.42,8.59;17,93.58,388.47,131.11,8.59">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,93.58,408.58,446.42,8.64;17,93.58,420.35,446.42,8.82;17,93.58,432.31,446.42,8.82;17,93.58,444.44,240.10,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,150.09,408.58,308.84,8.64">How reliable are the results of large-scale information retrieval experiments?</title>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,414.48,420.35,125.52,8.59;17,93.58,432.31,376.66,8.59">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Wilkinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Melbourne, Australia; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
